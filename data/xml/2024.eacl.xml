<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.eacl">
  <volume id="long" ingest-date="2024-03-03" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</booktitle>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Matthew</first><last>Purver</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St. Julian’s, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="29a5ddc6">2024.eacl-long</url>
      <venue>eacl</venue>
    </meta>
    <frontmatter>
      <url hash="94792eae">2024.eacl-long.0</url>
      <bibkey>eacl-2024-european-chapter-association-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement</title>
      <author><first>Xin</first><last>Quan</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Louise</first><last>Dennis</last><affiliation>University of Manchester, University of Manchester</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>University of Manchester</affiliation></author>
      <pages>1-22</pages>
      <abstract>An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models’ reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.</abstract>
      <url hash="066f368a">2024.eacl-long.1</url>
      <bibkey>quan-etal-2024-enhancing</bibkey>
    </paper>
    <paper id="2">
      <title>Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions</title>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Danilo</first><last>Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>University of Manchester</affiliation></author>
      <pages>23-34</pages>
      <abstract>Natural language definitions possess a recursive, self-explanatory semantic structure that can support representation learning methods able to preserve explicit conceptual relations and constraints in the latent space. This paper presents a multi-relational model that explicitly leverages such a structure to derive word embeddings from definitions. By automatically extracting the relations linking defined and defining terms from dictionaries, we demonstrate how the problem of learning word embeddings can be formalised via a translational framework in Hyperbolic space and used as a proxy to capture the global semantic structure of definitions. An extensive empirical analysis demonstrates that the framework can help imposing the desired structural constraints while preserving the semantic mapping required for controllable and interpretable traversal. Moreover, the experiments reveal the superiority of the Hyperbolic word embeddings over the Euclidean counterparts and demonstrate that the multi-relational approach can obtain competitive results when compared to state-of-the-art neural models, with the advantage of being intrinsically more efficient and interpretable</abstract>
      <url hash="89559947">2024.eacl-long.2</url>
      <attachment type="software" hash="5008b672">2024.eacl-long.2.software.zip</attachment>
      <bibkey>valentino-etal-2024-multi</bibkey>
    </paper>
    <paper id="3">
      <title>Anisotropy Is Inherent to Self-Attention in Transformers</title>
      <author><first>Nathan</first><last>Godey</last></author>
      <author><first>Éric</first><last>Clergerie</last></author>
      <author><first>Benoît</first><last>Sagot</last><affiliation>INRIA</affiliation></author>
      <pages>35-48</pages>
      <abstract>The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations tend to demonstrate that anisotropy might actually be inherent to Transformers-based models.</abstract>
      <url hash="6b88a441">2024.eacl-long.3</url>
      <bibkey>godey-etal-2024-anisotropy</bibkey>
    </paper>
    <paper id="4">
      <title>Generating Benchmarks for Factuality Evaluation of Language Models</title>
      <author><first>Dor</first><last>Muhlgay</last><affiliation>AI21 Labs</affiliation></author>
      <author><first>Ori</first><last>Ram</last></author>
      <author><first>Inbal</first><last>Magar</last></author>
      <author><first>Yoav</first><last>Levine</last></author>
      <author><first>Nir</first><last>Ratner</last></author>
      <author><first>Yonatan</first><last>Belinkov</last><affiliation>Technion, Technion</affiliation></author>
      <author><first>Omri</first><last>Abend</last><affiliation>Hebrew University of Jerusalem, Technion</affiliation></author>
      <author><first>Kevin</first><last>Leyton-Brown</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Amnon</first><last>Shashua</last><affiliation>Hebrew University, Hebrew University of Jerusalem and Hebrew University, Hebrew University of Jerusalem</affiliation></author>
      <author><first>Yoav</first><last>Shoham</last></author>
      <pages>49-66</pages>
      <abstract>Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM’s propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create three benchmarks: Wiki-FACTOR, News-FACTOR and Expert-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score and perplexity do not always agree on model ranking; (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.</abstract>
      <url hash="9af62840">2024.eacl-long.4</url>
      <attachment type="software" hash="28858435">2024.eacl-long.4.software.zip</attachment>
      <attachment type="note" hash="b521cf2d">2024.eacl-long.4.note.zip</attachment>
      <bibkey>muhlgay-etal-2024-generating</bibkey>
    </paper>
    <paper id="5">
      <title>Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source <fixed-case>LLM</fixed-case>s</title>
      <author><first>Simone</first><last>Balloccu</last><affiliation>, Charles University Prague</affiliation></author>
      <author><first>Patrícia</first><last>Schmidtová</last></author>
      <author><first>Mateusz</first><last>Lango</last><affiliation>Charles University and Poznan University of Technology</affiliation></author>
      <author><first>Ondrej</first><last>Dusek</last><affiliation>Charles University, Prague</affiliation></author>
      <pages>67-93</pages>
      <abstract>Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of indirect data leaking, where modelsare iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI’s GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI’s data usage policy, we extensively document the amount of data leaked to these models during the first year after the model’s release. We report that these models have been globally exposed to ∼4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.</abstract>
      <url hash="59105272">2024.eacl-long.5</url>
      <bibkey>balloccu-etal-2024-leak</bibkey>
    </paper>
    <paper id="6">
      <title>Archer: A Human-Labeled Text-to-<fixed-case>SQL</fixed-case> Dataset with Arithmetic, Commonsense and Hypothetical Reasoning</title>
      <author><first>Danna</first><last>Zheng</last></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Jeff</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>94-111</pages>
      <abstract>We present Archer, a challenging bilingual text-to-SQL dataset specific to complex reasoning, including arithmetic, commonsense and hypothetical reasoning. It contains 1,042 English questions and 1,042 Chinese questions, along with 521 unique SQL queries, covering 20 English databases across 20 domains. Notably, this dataset demonstrates a significantly higher level of complexity compared to existing publicly available datasets. Our evaluation shows that Archer challenges the capabilities of current state-of-the-art models, with a high-ranked model on the Spider leaderboard achieving only 6.73% execution accuracy on Archer test set. Thus, Archer presents a significant challenge for future research in this field.</abstract>
      <url hash="9a7935c2">2024.eacl-long.6</url>
      <bibkey>zheng-etal-2024-archer</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>GEAR</fixed-case>: Augmenting Language Models with Generalizable and Efficient Tool Resolution</title>
      <author><first>Yining</first><last>Lu</last></author>
      <author><first>Haoping</first><last>Yu</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>112-138</pages>
      <abstract>Augmenting large language models (LLM) to use external tools enhances their performance across a variety of tasks. However, prior works over-rely on task-specific demonstration of tool use that limits their generalizability and computational cost due to making many calls to large-scale LLMs. We introduce GEAR, a computationally efficient query-tool grounding algorithm that is generalizable to various tasks that require tool use while not relying on task-specific demonstrations. GEAR achieves better efficiency by delegating tool grounding and execution to small language models (SLM) and LLM, respectively; while leveraging semantic and pattern-based evaluation at both question and answer levels for generalizable tool grounding. We evaluate GEAR on 14 datasets across 6 downstream tasks, demonstrating its strong generalizability to novel tasks, tools and different SLMs. Despite offering more efficiency, GEAR achieves higher precision in tool grounding compared to prior strategies using LLM prompting, thus improving downstream accuracy at a reduced computational cost. For example, we demonstrate that GEAR-augmented GPT-J and GPT-3 outperform counterpart tool-augmented baselines because of better tool use.</abstract>
      <url hash="80f455cc">2024.eacl-long.7</url>
      <bibkey>lu-etal-2024-gear</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>LLM</fixed-case> Comparative Assessment: Zero-shot <fixed-case>NLG</fixed-case> Evaluation through Pairwise Comparisons using Large Language Models</title>
      <author><first>Adian</first><last>Liusie</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Potsawee</first><last>Manakul</last></author>
      <author><first>Mark</first><last>Gales</last><affiliation>University of Cambridge</affiliation></author>
      <pages>139-151</pages>
      <abstract>Current developments in large language models (LLMs) have enabled impressive zero-shot capabilities across various natural language tasks. An interesting application of these systems is in the automated assessment of natural language generation (NLG), a highly challenging area with great practical benefit. In this paper, we explore two options for exploiting the emergent abilities of LLMs for zero-shot NLG assessment: absolute score prediction, and comparative assessment which uses relative comparisons between pairs of candidates. Though comparative assessment has not been extensively studied in NLG assessment, we note that humans often find it more intuitive to compare two options rather than scoring each one independently. This work examines comparative assessment from multiple perspectives: performance compared to absolute grading; positional biases in the prompt; and efficient ranking in terms of the number of comparisons. We illustrate that LLM comparative assessment is a simple, general and effective approach for NLG assessment. For moderate-sized open-source LLMs, such as FlanT5 and Llama2-chat, comparative assessment is superior to prompt scoring, and in many cases can achieve performance competitive with state-of-the-art methods. Additionally, we demonstrate that LLMs often exhibit strong positional biases when making pairwise comparisons, and we propose debiasing methods that can further improve performance.</abstract>
      <url hash="4ce6a306">2024.eacl-long.8</url>
      <attachment type="software" hash="e9ead08e">2024.eacl-long.8.software.zip</attachment>
      <bibkey>liusie-etal-2024-llm</bibkey>
    </paper>
    <paper id="9">
      <title>Parameter-Efficient Conversational Recommender System as a Language Processing Task</title>
      <author><first>Mathieu</first><last>Ravaut</last></author>
      <author><first>Hao</first><last>Zhang</last></author>
      <author><first>Lu</first><last>Xu</last></author>
      <author><first>Aixin</first><last>Sun</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yong</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>152-165</pages>
      <abstract>Conversational recommender systems (CRS) aim to recommend relevant items to users by eliciting user preference through natural language conversation. Prior work often utilizes external knowledge graphs for items’ semantic information, a language model for dialogue generation, and a recommendation module for ranking relevant items. This combination of multiple components suffers from a cumber-some training process, and leads to semantic misalignment issues between dialogue generation and item recommendation. In this paper, we represent items in natural language and formulate CRS as a natural language processing task. Accordingly, we leverage the power of pre-trained language models to encode items, understand user intent via conversation, perform item recommendation through semantic matching, and generate dialogues. As a unified model, our PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without relying on non-textual metadata such as a knowledge graph. Experiments on two benchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of PECRS on recommendation and conversation. Our code is available at: https://github.com/Ravoxsg/efficient_unified_crs.</abstract>
      <url hash="ba4a938d">2024.eacl-long.9</url>
      <bibkey>ravaut-etal-2024-parameter</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>O</fixed-case>pen<fixed-case>PI</fixed-case>2.0: An Improved Dataset for Entity Tracking in Texts</title>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Hainiu</first><last>Xu</last></author>
      <author><first>Abhinav</first><last>Kommula</last></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>Allen Institute for Artificial Intelligence and University of Pennsylvania</affiliation></author>
      <author><first>Niket</first><last>Tandon</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>166-178</pages>
      <abstract>Much texts describe a changing world (e.g., procedures, stories, newswires), and understanding them requires tracking how entities change. An earlier dataset, OpenPI, provided crowdsourced annotations of entity state changes in text. However, a major limitation was that those annotations were free-form and did not identify salient changes, hampering model evaluation. To overcome these limitations, we present an improved dataset, OpenPI2.0, where entities and attributes are fully canonicalized and additional entity salience annotations are added. On our fairer evaluation setting, we find that current state-of-the-art language models are far from competent. We also show that using state changes of salient entities as a chain-of-thought prompt, downstream performance is improved on tasks such as question answering and classical planning, outperforming the setting involving all related entities indiscriminately. We offer OpenPI2.0 for the continued development of models that can understand the dynamics of entities in text.</abstract>
      <url hash="c7e59488">2024.eacl-long.10</url>
      <attachment type="software" hash="ecafe860">2024.eacl-long.10.software.zip</attachment>
      <attachment type="note" hash="ecafe860">2024.eacl-long.10.note.zip</attachment>
      <bibkey>zhang-etal-2024-openpi2</bibkey>
    </paper>
    <paper id="11">
      <title>A Comparative Multidimensional Analysis of Empathetic Systems</title>
      <author><first>Andrew</first><last>Lee</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Jonathan</first><last>Kummerfeld</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Larry</first><last>Ann</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>179-189</pages>
      <abstract>Recently, empathetic dialogue systems have received significant attention.While some researchers have noted limitations, e.g., that these systems tend to generate generic utterances, no study has systematically verified these issues. We survey 21 systems, asking what progress has been made on the task. We observe multiple limitations of current evaluation procedures. Most critically, studies tend to rely on a single non-reproducible empathy score, which inadequately reflects the multidimensional nature of empathy. To better understand the differences between systems, we comprehensively analyze each system with automated methods that are grounded in a variety of aspects of empathy. We find that recent systems lack three important aspects of empathy: specificity, reflection levels, and diversity. Based on our results, we discuss problematic behaviors that may have gone undetected in prior evaluations, and offer guidance for developing future systems.</abstract>
      <url hash="df025edc">2024.eacl-long.11</url>
      <bibkey>lee-etal-2024-comparative</bibkey>
    </paper>
    <paper id="12">
      <title>Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering</title>
      <author><first>Mingda</first><last>Chen</last><affiliation>Meta AI</affiliation></author>
      <author><first>Xilun</first><last>Chen</last><affiliation>Meta AI</affiliation></author>
      <author><first>Wen-tau</first><last>Yih</last><affiliation>Meta Platforms, Inc.</affiliation></author>
      <pages>190-208</pages>
      <abstract>Few-shot learning for open domain multi-hop question answering typically relies on the in-context learning capability of large language models (LLMs). While powerful, these LLMs usually contain tens or hundreds of billions of parameters, making them rather inefficient at inference time. To improve performance of smaller language models, we propose a data synthesis framework for multi-hop question answering that requires less than 10 human-annotated question answer pairs. Our framework depends only on rich, naturally-occurring relationships among documents and is built upon the data generation functions parameterized by LLMs and prompts. We synthesize millions of multi-hop questions and claims to finetune language models, evaluated on popular benchmarks for multi-hop question answering and fact verification. Empirically, our approach improves model performance significantly, allowing the finetuned models to be competitive with GPT-3.5 based approaches while being almost one-third the size in parameter count.</abstract>
      <url hash="e98ef159">2024.eacl-long.12</url>
      <bibkey>chen-etal-2024-shot</bibkey>
    </paper>
    <paper id="13">
      <title>Language Models as Inductive Reasoners</title>
      <author><first>Zonglin</first><last>Yang</last></author>
      <author><first>Li</first><last>Dong</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Xinya</first><last>Du</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Hao</first><last>Cheng</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Jianfeng</first><last>Gao</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>209-225</pages>
      <abstract>Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language models as ”reasoners”. Moreover, we provide the first and comprehensive analysis of how well pretrained language models can induce natural language rules from natural language facts. We also propose a new framework drawing insights from philosophy literature for this task, which we show in the experiment section that surpasses baselines in both automatic and human evaluations. We discuss about our future perspectives for inductive reasoning in Section 7. Dataset and code are available at https://github.com/ZonglinY/Inductive_Reasoning.</abstract>
      <url hash="6a54cd0d">2024.eacl-long.13</url>
      <attachment type="software" hash="59dd943e">2024.eacl-long.13.software.zip</attachment>
      <attachment type="note" hash="59dd943e">2024.eacl-long.13.note.zip</attachment>
      <bibkey>yang-etal-2024-language</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>SIB</fixed-case>-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects</title>
      <author><first>David</first><last>Adelani</last></author>
      <author><first>Hannah</first><last>Liu</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Xiaoyu</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <author><first>Nikita</first><last>Vassilyev</last></author>
      <author><first>Jesujoba</first><last>Alabi</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Yanke</first><last>Mao</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Haonan</first><last>Gao</last></author>
      <author><first>En-Shiun</first><last>Lee</last></author>
      <pages>226-245</pages>
      <abstract>Despite the progress in building multilingual language models, evaluation is often limited to a few languages with available datasets which excludes a large number of low-resource languages. In this paper, we create SIB-200—a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 204 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages. We found that languages unseen during the pre-training of multilingual language models, languages from under-represented families (like Nilotic and Altantic-Congo), and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on our topic classification dataset. We hope our dataset %will encourages a more inclusive evaluation of multilingual language models on a more diverse set of languages.</abstract>
      <url hash="674a9719">2024.eacl-long.14</url>
      <attachment type="software" hash="dcb0841f">2024.eacl-long.14.software.zip</attachment>
      <attachment type="note" hash="dcb0841f">2024.eacl-long.14.note.zip</attachment>
      <bibkey>adelani-etal-2024-sib</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>F</fixed-case>in<fixed-case>BPM</fixed-case>: A Framework for Portfolio Management-based Financial Investor Behavior Perception Model</title>
      <author><first>Zhilu</first><last>Zhang</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Procheta</first><last>Sen</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Zimu</first><last>Wang</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Ruoyu</first><last>Sun</last><affiliation>Xi’an Jiaotong-Liverpool University</affiliation></author>
      <author><first>Zhengyong</first><last>Jiang</last><affiliation>Xi’an Jiaotong-Liverpool University</affiliation></author>
      <author><first>Jionglong</first><last>Su</last></author>
      <pages>246-257</pages>
      <abstract>The goal of portfolio management is to simultaneously maximize the accumulated return and also to control risk. In consecutive trading periods, portfolio manager needs to continuously adjust the portfolio weights based on the factors which can cause price fluctuation in the market. In the stock market, the factors affecting the stock price can be divided into two categories. The first is price fluctuations caused by irrational investment of the speculators. The second is endogenous value changes caused by operations of the company. In recent years, with the advancement of artificial intelligence technology, reinforcement learning (RL) algorithms have been increasingly employed by scholars to address financial problems, particularly in the area of portfolio management. However, the deep RL models proposed by these scholars in the past have focused more on analyzing the price changes caused by the investment behavior of speculators in response to technical indicators of actual stock prices. In this research, we introduce an RL-based framework called FinBPM, which takes both the factor pertaining to the impact on operations of the company and the factor of the irrational investment of the speculator into consideration. For our experimentation, we randomly selected twelve stocks from the Dow Jones Industrial Index to construct our portfolio. The experimental results reveal that, in comparison to conventional reinforcement learning methods, our approach with at least 13.26% increase over other methods compared. Additionally, it achieved the best Sharpe ratio of 2.77, effectively maximizing the return per unit of risk.</abstract>
      <url hash="0d1fae97">2024.eacl-long.15</url>
      <bibkey>zhang-etal-2024-finbpm</bibkey>
    </paper>
    <paper id="16">
      <title>Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance to Ask Clarification Questions</title>
      <author><first>Alberto</first><last>Testoni</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Raquel</first><last>Fernández</last><affiliation>University of Amsterdam and University of Amsterdam</affiliation></author>
      <pages>258-275</pages>
      <abstract>Clarification questions are an essential dialogue tool to signal misunderstanding, ambiguities, and under-specification in language use. While humans are able to resolve uncertainty by asking questions since childhood, modern dialogue systems struggle to generate effective questions. To make progress in this direction, in this work we take a collaborative dialogue task as a testbed and study how model uncertainty relates to human uncertainty—an as yet under-explored problem. We show that model uncertainty does not mirror human clarification-seeking behavior, which suggests that using human clarification questions as supervision for deciding when to ask may not be the most effective way to resolve model uncertainty. To address this issue, we propose an approach to generating clarification questions based on model uncertainty estimation, compare it to several alternatives, and show that it leads to significant improvements in terms of task success. Our findings highlight the importance of equipping dialogue systems with the ability to assess their own uncertainty and exploit in interaction.</abstract>
      <url hash="d8992014">2024.eacl-long.16</url>
      <bibkey>testoni-fernandez-2024-asking</bibkey>
    </paper>
    <paper id="17">
      <title>Like a Good Nearest Neighbor: Practical Content Moderation and Text Classification</title>
      <author><first>Luke</first><last>Bates</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>276-297</pages>
      <abstract>Few-shot text classification systems have impressive capabilities but are infeasible to deploy and use reliably due to their dependence on prompting and billion-parameter language models. SetFit (Tunstall, 2022) is a recent, practical approach that fine-tunes a Sentence Transformer under a contrastive learning paradigm and achieves similar results to more unwieldy systems. Inexpensive text classification is important for addressing the problem of domain drift in all classification tasks, and especially in detecting harmful content, which plagues social media platforms. Here, we propose Like a Good Nearest Neighbor (LaGoNN), a modification to SetFit that introduces no learnable parameters but alters input text with information from its nearest neighbor, for example, the label and text, in the training data, making novel data appear similar to an instance on which the model was optimized. LaGoNN is effective at flagging undesirable content and text classification, and improves SetFit’s performance. To demonstrate LaGoNN’s value, we conduct a thorough study of text classification systems in the context of content moderation under four label distributions, and in general and multilingual classification settings.</abstract>
      <url hash="1e90f053">2024.eacl-long.17</url>
      <attachment type="software" hash="40e95628">2024.eacl-long.17.software.zip</attachment>
      <attachment type="note" hash="af4a194c">2024.eacl-long.17.note.zip</attachment>
      <bibkey>bates-gurevych-2024-like</bibkey>
    </paper>
    <paper id="18">
      <title>Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon</title>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Tilman</first><last>Beck</last></author>
      <author><first>Zeerak</first><last>Talat</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>298-320</pages>
      <abstract>Improving multilingual language models capabilities in low-resource languages is generally difficult due to the scarcity of large-scale data in those languages. In this paper, we relax the reliance on texts in low-resource languages by using multilingual lexicons in pretraining to enhance multilingual capabilities. Specifically, we focus on zero-shot sentiment analysis tasks across 34 languages, including 6 high/medium-resource languages, 25 low-resource languages, and 3 code-switching datasets. We demonstrate that pretraining using multilingual lexicons, without using any sentence-level sentiment data, achieves superior zero-shot performance compared to models fine-tuned on English sentiment datasets, and large language models like GPT–3.5, BLOOMZ, and XGLM. These findings are observable for unseen low-resource languages to code-mixed scenarios involving high-resource languages.</abstract>
      <url hash="c55df9c7">2024.eacl-long.18</url>
      <bibkey>koto-etal-2024-zero</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>CEAN</fixed-case>: Contrastive Event Aggregation Network with <fixed-case>LLM</fixed-case>-based Augmentation for Event Extraction</title>
      <author><first>Zihao</first><last>Meng</last></author>
      <author><first>Tao</first><last>Liu</last></author>
      <author><first>Heng</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Kai</first><last>Feng</last></author>
      <author><first>Peng</first><last>Zhao</last><affiliation>Alibaba Group</affiliation></author>
      <pages>321-333</pages>
      <abstract>Event Extraction is a crucial yet arduous task in natural language processing (NLP), as its performance is significantly hindered by laborious data annotation. Given this challenge, recent research has predominantly focused on two approaches: pretraining task-oriented models for event extraction and employing data augmentation techniques. These methods involve integrating external knowledge, semantic structures, or artificially generated samples using large language models (LLMs). However, their performances can be compromised due to two fundamental issues. Firstly, the alignment between the introduced knowledge and event extraction knowledge is crucial. Secondly, the introduction of data noise during the augmentation is unavoidable and can mislead the model’s convergence. To address these issues, we propose a Contrastive Event Aggregation Network with LLM-based Augmentation to promote low-resource learning and reduce data noise for event extraction. Different from the existing methods introducing linguistic knowledge into data augmentation, an event aggregation network is established to introduce event knowledge into supervised learning by constructing adaptively-updated semantic representation for trigger and argument. For LLM-based augmentation, we design a new scheme including a multi-pattern rephrasing paradigm and a data-free composing paradigm. Instead of directly using augmentation samples in the supervised task, we introduce span-level contrastive learning to reduce data noise. Experiments on the ACE2005 and ERE-EN demonstrate that our proposed approach achieves new state-of-the-art results on both of the two datasets.</abstract>
      <url hash="bb74697a">2024.eacl-long.19</url>
      <bibkey>meng-etal-2024-cean</bibkey>
    </paper>
    <paper id="20">
      <title>How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?</title>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>334-348</pages>
      <abstract>Customizing machine translation models to comply with desired attributes (e.g., formality or grammatical gender) is a well-studied topic. However, most current approaches rely on (semi-)supervised data with attribute annotations. This data scarcity bottlenecks democratizing such customization possibilities to a wider range of languages, particularly lower-resource ones. This gap is out of sync with recent progress in pretrained massively multilingual translation models. In response, we transfer the attribute controlling capabilities to languages without attribute-annotated data with an NLLB-200 model as a foundation. Inspired by techniques from controllable generation, we employ a gradient-based inference-time controller to steer the pretrained model. The controller transfers well to zero-shot conditions, as it is operates on pretrained multilingual representations and is attribute- rather than language-specific. With a comprehensive comparison to finetuning-based control, we demonstrate that, despite finetuning’s clear dominance in supervised settings, the gap to inference-time control closes when moving to zero-shot conditions, especially with new and distant target languages. The latter also shows stronger domain robustness. We further show that our inference-time control complements finetuning. Moreover, a human evaluation on a real low-resource language, Bengali, confirms our findings. Our code is in the supplementary material.</abstract>
      <url hash="3b6ada37">2024.eacl-long.20</url>
      <attachment type="software" hash="4d9af2bc">2024.eacl-long.20.software.zip</attachment>
      <bibkey>liu-niehues-2024-transferable</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>MUC</fixed-case>: Multilingual Template Filling on <fixed-case>MUC</fixed-case>-4</title>
      <author><first>William</first><last>Gantt</last><affiliation>Department of Computer Science, University of Rochester</affiliation></author>
      <author><first>Shabnam</first><last>Behzad</last></author>
      <author><first>Hannah</first><last>An</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Yunmo</first><last>Chen</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Aaron</first><last>White</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Mahsa</first><last>Yarmohammadi</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>349-368</pages>
      <abstract>We introduce MultiMUC, the first multilingual parallel corpus for template filling, comprising translations of the classic MUC-4 template filling benchmark into five languages: Arabic, Chinese, Farsi, Korean, and Russian. We obtain automatic translations from a strong multilingual machine translation system and manually project the original English annotations into each target language. For all languages, we also provide human translations for key portions of the dev and test splits. Finally, we present baselines on MultiMUC both with state-of-the-art template filling models for MUC-4 and with ChatGPT. We release MUC-4 and the supervised baselines to facilitate further work on document-level information extraction in multilingual settings.</abstract>
      <url hash="c4b6f876">2024.eacl-long.21</url>
      <bibkey>gantt-etal-2024-multimuc</bibkey>
    </paper>
    <paper id="22">
      <title>Align and Augment: Generative Data Augmentation for Compositional Generalization</title>
      <author><first>Francesco</first><last>Cazzaro</last><affiliation>Universidad Politécnica de Cataluna</affiliation></author>
      <author><first>Davide</first><last>Locatelli</last><affiliation>TORTUS.ai</affiliation></author>
      <author><first>Ariadna</first><last>Quattoni</last></author>
      <pages>369-383</pages>
      <abstract>Recent work on semantic parsing has shown that seq2seq models find compositional generalization challenging. Several strategies have been proposed to mitigate this challenge. One such strategy is to improve compositional generalization via data augmentation techniques. In this paper we follow this line of work and propose Archer, a data-augmentation strategy that exploits alignment annotations between sentences and their corresponding meaning representations. More precisely, we use alignments to train a two step generative model that combines monotonic lexical generation with reordering. Our experiments show that Archer leads to significant improvements in compositional generalization performance.</abstract>
      <url hash="757ffe9e">2024.eacl-long.22</url>
      <bibkey>cazzaro-etal-2024-align</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>UNSEE</fixed-case>: Unsupervised Non-contrastive Sentence Embeddings</title>
      <author><first>Ömer</first><last>Çağatan</last></author>
      <pages>384-393</pages>
      <abstract>In this paper, we introduce UNSEE, which stands for Unsupervised Non-Contrastive Sentence Embeddings. UNSEE demonstrates better performance compared to SimCSE in the Massive Text Embedding (MTEB) benchmark. We begin by highlighting the issue of representation collapse that occurs with the replacement of contrastive objectives with non-contrastive objectives in SimCSE. Subsequently, we introduce a straightforward solution called the target network to mitigate this problem. This approach enables us to harness non-contrastive objectives while ensuring training stability and achieving performance improvements similar to those seen with contrastive objectives. We have reached peak performance in non-contrastive sentence embeddings through extensive fine-tuning and optimization. These efforts have resulted in superior sentence representation models, emphasizing the importance of careful tuning and optimization for non-contrastive objectives.</abstract>
      <url hash="66089413">2024.eacl-long.23</url>
      <bibkey>cagatan-2024-unsee</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>EXPLORER</fixed-case>: Exploration-guided Reasoning for Textual Reinforcement Learning</title>
      <author><first>Kinjal</first><last>Basu</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Keerthiram</first><last>Murugesan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Subhajit</first><last>Chaudhury</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Murray</first><last>Campbell</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Kartik</first><last>Talamadupula</last></author>
      <author><first>Tim</first><last>Klinger</last></author>
      <pages>394-405</pages>
      <abstract>Text-based games (TBGs) have emerged as an important collection of NLP tasks, requiring reinforcement learning (RL) agents to combine natural language understanding with reasoning. A key challenge for agents attempting to solve such tasks is to generalize across multiple games and demonstrate good performance on both seen and unseen objects. Purely deep-RL-based approaches may perform well on seen objects; however, they fail to showcase the same performance on unseen objects. Commonsense-infused deep-RL agents may work better on unseen data; unfortunately, their policies are often not interpretable or easily transferable. To tackle these issues, in this paper, we present EXPLORER which is an exploration-guided reasoning agent for textual reinforcement learning. EXPLORER is neuro-symbolic in nature, as it relies on a neural module for exploration and a symbolic module for exploitation. It can also learn generalized symbolic policies and perform well over unseen data. Our experiments show that EXPLORER outperforms the baseline agents on Text-World cooking (TW-Cooking) and Text-World Commonsense (TWC) games.</abstract>
      <url hash="869cec73">2024.eacl-long.24</url>
      <bibkey>basu-etal-2024-explorer</bibkey>
    </paper>
    <paper id="25">
      <title>From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions</title>
      <author><first>Fabian</first><last>Retkowski</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>406-419</pages>
      <abstract>Text segmentation is a fundamental task in natural language processing, where documents are split into contiguous sections. However, prior research in this area has been constrained by limited datasets, which are either small in scale, synthesized, or only contain well-structured documents. In this paper, we address these limitations by introducing a novel benchmark YTSeg focusing on spoken content that is inherently more unstructured and both topically and structurally diverse. As part of this work, we introduce an efficient hierarchical segmentation model MiniSeg, that outperforms state-of-the-art baselines. Lastly, we expand the notion of text segmentation to a more practical “smart chaptering” task that involves the segmentation of unstructured content, the generation of meaningful segment titles, and a potential real-time application of the models.</abstract>
      <url hash="048cd823">2024.eacl-long.25</url>
      <bibkey>retkowski-waibel-2024-text</bibkey>
    </paper>
    <paper id="26">
      <title>Fréchet Distance for Offline Evaluation of Information Retrieval Systems with Sparse Labels</title>
      <author><first>Negar</first><last>Arabzadeh</last></author>
      <author><first>Charles</first><last>Clarke</last><affiliation>University of Waterloo</affiliation></author>
      <pages>420-431</pages>
      <abstract>The rapid advancement of natural language processing, information retrieval (IR), computer vision, and other technologies has presented significant challenges in evaluating the performance of these systems. One of the main challenges is the scarcity of human-labeled data, which hinders the fair and accurate assessment of these systems. In this work, we specifically focus on evaluating IR systems with sparse labels, borrowing from recent research on evaluating computer vision tasks.taking inspiration from the success of using Fréchet Inception Distance (FID) in assessing text-to-image generation systems. We propose leveraging the Fréchet Distance to measure the distance between the distributions of relevant judged items and retrieved results. Our experimental results on MS MARCO V1 dataset and TREC Deep Learning Tracks query sets demonstrate the effectiveness of the Fréchet Distance as a metric for evaluating IR systems, particularly in settings where a few labels are available.This approach contributes to the advancement of evaluation methodologies in real-world scenarios such as the assessment of generative IR systems.</abstract>
      <url hash="d9d00dfa">2024.eacl-long.26</url>
      <bibkey>arabzadeh-clarke-2024-frechet</bibkey>
    </paper>
    <paper id="27">
      <title>Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of <fixed-case>NLI</fixed-case> Models</title>
      <author><first>Erik</first><last>Arakelyan</last></author>
      <author><first>Zhaoqi</first><last>Liu</last></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>432-444</pages>
      <abstract>Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text generation, with the explicit condition that the NLI model predicts the relationship between the original and adversarial inputs as a symmetric equivalence entailment. We systematically study the effects of the phenomenon across NLI models for <tex-math>\text{\emph{in-}}</tex-math> and <tex-math>\text{\emph{out-of-}}</tex-math> domain settings. Our experiments show that semantic sensitivity causes performance degradations of 12.92% and 23.71% average over <tex-math>\text{\emph{in-}}</tex-math> and <tex-math>\text{\emph{out-of-}}</tex-math> domain settings, respectively. We further perform ablation studies, analysing this phenomenon across models, datasets, and variations in inference and show that semantic sensitivity can lead to major inconsistency within model predictions.</abstract>
      <url hash="98c94193">2024.eacl-long.27</url>
      <bibkey>arakelyan-etal-2024-semantic</bibkey>
    </paper>
    <paper id="28">
      <title>Exploring the Robustness of Task-oriented Dialogue Systems for Colloquial <fixed-case>G</fixed-case>erman Varieties</title>
      <author><first>Ekaterina</first><last>Artemova</last><affiliation>Toloka AI</affiliation></author>
      <author><first>Verena</first><last>Blaschke</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München and IT University of Copenhagen</affiliation></author>
      <pages>445-468</pages>
      <abstract>Mainstream cross-lingual task-oriented dialogue (ToD) systems leverage the transfer learning paradigm by training a joint model for intent recognition and slot-filling in English and applying it, zero-shot, to other languages.We address a gap in prior research, which often overlooked the transfer to lower-resource colloquial varieties due to limited test data.Inspired by prior work on English varieties, we craft and manually evaluate perturbation rules that transform German sentences into colloquial forms and use them to synthesize test sets in four ToD datasets.Our perturbation rules cover 18 distinct language phenomena, enabling us to explore the impact of each perturbation on slot and intent performance.Using these new datasets, we conduct an experimental evaluation across six different transformers.Here, we demonstrate that when applied to colloquial varieties, ToD systems maintain their intent recognition performance, losing 6% (4.62 percentage points) in accuracy on average. However, they exhibit a significant drop in slot detection, with a decrease of 31% (21 percentage points) in slot F<tex-math>_1</tex-math> score.Our findings are further supported by a transfer experiment from Standard American English to synthetic Urban African American Vernacular English.</abstract>
      <url hash="51ed9c7f">2024.eacl-long.28</url>
      <attachment type="software" hash="3a9f24a3">2024.eacl-long.28.software.zip</attachment>
      <bibkey>artemova-etal-2024-exploring</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>PEARL</fixed-case>: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</title>
      <author><first>Simeng</first><last>Sun</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Dan</first><last>Iter</last></author>
      <author><first>Chenguang</first><last>Zhu</last><affiliation>Zoom</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>469-486</pages>
      <abstract>Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents.</abstract>
      <url hash="3191421f">2024.eacl-long.29</url>
      <bibkey>sun-etal-2024-pearl</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>LA</fixed-case>ra<fixed-case>B</fixed-case>ench: Benchmarking <fixed-case>A</fixed-case>rabic <fixed-case>AI</fixed-case> with Large Language Models</title>
      <author><first>Ahmed</first><last>Abdelali</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Shammur</first><last>Chowdhury</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Maram</first><last>Hasanain</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Basel</first><last>Mousi</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Sabri</first><last>Boughorbel</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Samir</first><last>Abdaljalil</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Yassine</first><last>El Kheir</last></author>
      <author><first>Daniel</first><last>Izham</last><affiliation>Kanari AI</affiliation></author>
      <author><first>Fahim</first><last>Dalvi</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Majd</first><last>Hawasly</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Nizi</first><last>Nazar</last></author>
      <author><first>Youssef</first><last>Elshahawy</last><affiliation>Kanari AI</affiliation></author>
      <author><first>Ahmed</first><last>Ali</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Nadir</first><last>Durrani</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Natasa</first><last>Milic-Frayling</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>487-520</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions. Notably, larger computational models with few-shot learning techniques managed to reduce these performance gaps. Our findings provide valuable insights into the applicability of LLMs for Arabic NLP and speech processing tasks.</abstract>
      <url hash="c04171bb">2024.eacl-long.30</url>
      <bibkey>abdelali-etal-2024-larabench</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>S</fixed-case>entence<fixed-case>LDA</fixed-case>: Discriminative and Robust Document Representation with Sentence Level Topic Model</title>
      <author><first>Taehun</first><last>Cha</last><affiliation>Korea University</affiliation></author>
      <author><first>Donghun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <pages>521-538</pages>
      <abstract>A subtle difference in context results in totally different nuances even for lexically identical words. On the other hand, two words can convey similar meanings given a homogeneous context. As a result, considering only word spelling information is not sufficient to obtain quality text representation. We propose SentenceLDA, a sentence-level topic model. We combine modern SentenceBERT and classical LDA to extend the semantic unit from word to sentence. By extending the semantic unit, we verify that SentenceLDA returns more discriminative document representation than other topic models, while maintaining LDA’s elegant probabilistic interpretability. We also verify the robustness of SentenceLDA by comparing the inference results on original and paraphrased texts. Additionally, we implement one possible application of SentenceLDA on corpus-level key opinion mining by applying SentenceLDA on an argumentative corpus, DebateSum.</abstract>
      <url hash="44628ed9">2024.eacl-long.31</url>
      <attachment type="software" hash="265f6a16">2024.eacl-long.31.software.zip</attachment>
      <bibkey>cha-lee-2024-sentencelda</bibkey>
    </paper>
    <paper id="32">
      <title>Towards Hierarchical Spoken Language Disfluency Modeling</title>
      <author><first>Jiachen</first><last>Lian</last><affiliation>Electrical Engineering &amp; Computer Science Department, University of California Berkeley</affiliation></author>
      <author><first>Gopala</first><last>Anumanchipalli</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>539-551</pages>
      <abstract>Speech dysfluency modeling is the bottleneck for both speech therapy and language learning. However, there is no AI solution to systematically tackle this problem. We first propose to define the concept of dysfluent speech and dysfluent speech modeling. We then present Hierarchical Unconstrained Dysfluency Modeling (H-UDM) approach that addresses both dysfluency transcription and detection to eliminate the need for extensive manual annotation. Furthermore, we introduce a simulated dysfluent dataset called VCTK++ to enhance the capabilities of H-UDM in phonetic transcription. Our experimental results demonstrate the effectiveness and robustness of our proposed methods in both transcription and detection tasks.</abstract>
      <url hash="b36dc1d9">2024.eacl-long.32</url>
      <bibkey>lian-anumanchipalli-2024-towards</bibkey>
    </paper>
    <paper id="33">
      <title>Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing Approach For Uncovering Edge Cases with Minimal Distribution Distortion</title>
      <author><first>Aly</first><last>Kassem</last><affiliation>University of Windsor</affiliation></author>
      <author><first>Sherif</first><last>Saad</last><affiliation>University of Windsor</affiliation></author>
      <pages>552-572</pages>
      <abstract>Adversarial attacks against Language models (LMs) are a significant concern. In particular, adversarial samples exploit the model’s sensitivity to small input changes. While these changes appear insignificant on the semantics of the input sample, they result in significant decay in model performance. In this paper, we propose Targeted Paraphrasing via RL (TPRL), an approach to automatically learn a policy to generate challenging samples that improve the model’s performance. TPRL leverages FLAN-T5, a language model, as a generator and employs a self-learned policy using a proximal policy optimization to generate the adversarial examples automatically. TPRL’s reward is based on the confusion induced in the classifier, preserving the original text meaning through a Mutual Implication score. We demonstrate &amp; evaluate TPRL’s effectiveness in discovering natural adversarial attacks and improving model performance through extensive experiments on four diverse NLP classification tasks via Automatic &amp; Human evaluation. TPRL outperforms strong baselines, exhibits generalizability across classifiers and datasets, and combines the strengths of language modeling and reinforcement learning to generate diverse and influential adversarial examples.</abstract>
      <url hash="ab4fb34e">2024.eacl-long.33</url>
      <bibkey>kassem-saad-2024-finding</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>FAIR</fixed-case>: Filtering of Automatically Induced Rules</title>
      <author><first>Divya Jyoti</first><last>Bajpai</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Ayush</first><last>Maheshwari</last></author>
      <author><first>Manjesh</first><last>Hanawal</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology Bombay</affiliation></author>
      <pages>573-588</pages>
      <abstract>Availability of large annotated data can be a critical bottleneck in training machine learning algorithms successfully, especially when applied to diverse domains. Weak supervision offers a promising alternative by accelerating the creation of labeled training data using domain-specific rules. However, it requires users to write a diverse set of high-quality rules to assign labels to the unlabeled data (eg., Snorkel (CITATION)). Automatic Rule Induction (ARI) approaches such as Snuba (CITATION) circumvent this problem by automatically creating rules from features on a small labeled set and filtering a final set of rules from them. In the ARI approach, the crucial step is to filter out a set of a high-quality useful subset of rules from the large set of automatically created rules. In this paper, we propose an algorithm FAIR (Filtering of Automatically Induced Rules) to filter rules from a large number of automatically induced rules using submodular objective functions that account for the collective precision, coverage, and conflicts of the rule set. We experiment with three ARI approaches and five text classification datasets to validate the superior performance of our algorithm with respect to several semi-supervised label aggregation approaches. We show that our approach achieves statistically significant results in comparison to existing rule-filtering approaches. The anonymized source code is available at <url>https://anonymous.4open.science/r/FAIR-LF-Induction-9B60</url>.</abstract>
      <url hash="69aec150">2024.eacl-long.34</url>
      <bibkey>bajpai-etal-2024-fair</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>NNOSE</fixed-case>: Nearest Neighbor Occupational Skill Extraction</title>
      <author><first>Mike</first><last>Zhang</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Rob</first><last>Goot</last></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München and IT University of Copenhagen</affiliation></author>
      <pages>589-608</pages>
      <abstract>The labor market is changing rapidly, prompting increased interest in the automatic extraction of occupational skills from text. With the advent of English benchmark job description datasets, there is a need for systems that handle their diversity well. We tackle the complexity in occupational skill datasets tasks—combining and leveraging multiple datasets for skill extraction, to identify rarely observed skills within a dataset, and overcoming the scarcity of skills across datasets. In particular, we investigate the retrieval-augmentation of language models, employing an external datastore for retrieving similar skills in a dataset-unifying manner. Our proposed method, <b>N</b>earest <b>N</b>eighbor <b>O</b>ccupational <b>S</b>kill <b>E</b>xtraction (NNOSE) effectively leverages multiple datasets by retrieving neighboring skills from other datasets in the datastore. This improves skill extraction <i>without</i> additional fine-tuning. Crucially, we observe a performance gain in predicting infrequent patterns, with substantial gains of up to 30% span-F1 in cross-dataset settings.</abstract>
      <url hash="fa763f2b">2024.eacl-long.35</url>
      <bibkey>zhang-etal-2024-nnose</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>GAINER</fixed-case>: Graph Machine Learning with Node-specific Radius for Classification of Short Texts and Documents</title>
      <author><first>Naganand</first><last>Yadati</last></author>
      <pages>609-626</pages>
      <abstract>Graphs provide a natural, intuitive, and holistic means to capture relationships between different text elements in Natural Language Processing (NLP) such as words, sentences, and documents. Recent advancements in the field of Graph Machine Learning (GML) have led to the development of numerous models to process text for various natural language applications, including but not limited to short-text classification, document classification, and others.At the heart of GML models, specifically those based on Graph Neural Networks (GNNs), lies the message passing operation which has shown to be an essential component for strong empirical performance in NLP.However, the number of message passing steps (often known as the radius) is <tex-math>\textit{fixed for all the nodes}</tex-math> in existing GML models for NLP.Fixing the radius poses a fundamental restriction as nodes exhibit diverse properties and varying amounts of informative local structures in the input graph.This paper presents GAINER, a novel framework called Graph mAchine learnIng with Node-spEcific Radius, aimed at graph-based NLP. We propose non-neural and novel neural approaches built on the core ideas of GAINER.Through rigorous experimentation, we demonstrate the efficacy of GAINER in various popular NLP tasks.</abstract>
      <url hash="261a9e17">2024.eacl-long.36</url>
      <bibkey>yadati-2024-gainer</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>MAFIA</fixed-case>: Multi-Adapter Fused Inclusive Language Models</title>
      <author><first>Prachi</first><last>Jain</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ashutosh</first><last>Sathe</last></author>
      <author><first>Varun</first><last>Gumma</last><affiliation>Microsoft</affiliation></author>
      <author><first>Kabir</first><last>Ahuja</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft</affiliation></author>
      <pages>627-645</pages>
      <abstract>Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model in order to maintain the performance on the downstream task. In this work, we aim to modularly debias a pre-trained language model across multiple dimensions. Previous works extensively explored debiasing PLMs by using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of the approach.</abstract>
      <url hash="289af4f2">2024.eacl-long.37</url>
      <bibkey>jain-etal-2024-mafia</bibkey>
    </paper>
    <paper id="38">
      <title>Code-Switched Language Identification is Harder Than You Think</title>
      <author><first>Laurie</first><last>Burchell</last></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Robert</first><last>Thompson</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>646-658</pages>
      <abstract>Code switching (CS) is a very common phenomenon in written and spoken communication, but is handled poorly by many NLP applications. Looking to the application of building CS corpora, we explore CS language identification for corpus building. We make the task more realistic by scaling it to more languages and considering models with simpler architectures for faster inference. We also reformulate the task as a sentence-level multi-label tagging problem to make it more tractable. Having defined the task, we investigate three reasonable architectures for this task and define metrics which better reflect desired performance. We present empirical evidence that no current approach is adequate, and finally provide recommendations for future work in this area.</abstract>
      <url hash="2a2ec008">2024.eacl-long.38</url>
      <bibkey>burchell-etal-2024-code</bibkey>
    </paper>
    <paper id="39">
      <title>Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following <fixed-case>LLM</fixed-case></title>
      <author><first>Ruohong</first><last>Zhang</last></author>
      <author><first>Yau-Shian</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yiming</first><last>Yang</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>659-673</pages>
      <abstract>The remarkable performance of large language models (LLMs) in zero-shot language understanding has garnered significant attention.However, employing LLMs for large-scale inference or domain-specific fine-tuning requires immense computational resources due to their substantial model size. To overcome these limitations, we introduce a novel method, namely GenCo, which leverages the strong generative power of LLMs to assist in training a smaller and more adaptable language model. In our method, an LLM plays an important role in the self-training loop of a smaller model in two important ways. Firstly, we utilize an LLM to generate multiple augmented texts for each input instance to enhance its semantic meaning for better understanding. Secondly, we additionally generate high-quality training instances conditioned on predicted labels, ensuring the generated texts are relevant to the labels. In this way, GenCo not only corrects the errors of predicted labels during self-training but also eliminates the need for extensive unlabeled texts. In our experiments, GenCo outperforms previous state-of-the-art methods when only limited (<tex-math>&lt;5\%</tex-math> of original) in-domain text data is available. Notably, our approach surpasses Alpaca-7B with human instructions, highlighting the significance of self-training.</abstract>
      <url hash="b1c41a8f">2024.eacl-long.39</url>
      <attachment type="software" hash="6b1f6922">2024.eacl-long.39.software.zip</attachment>
      <bibkey>zhang-etal-2024-generation</bibkey>
    </paper>
    <paper id="40">
      <title>Quantifying the Hyperparameter Sensitivity of Neural Networks for Character-level Sequence-to-Sequence Tasks</title>
      <author><first>Adam</first><last>Wiemerslage</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Kyle</first><last>Gorman</last><affiliation>The Graduate Center, City University of New York and Google</affiliation></author>
      <author><first>Katharina</first><last>Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <pages>674-689</pages>
      <abstract>Hyperparameter tuning, the process of searching for suitable hyperparameters, becomes more difficult as the computing resources required to train neural networks continue to grow. This topic continues to receive little attention and discussion—much of it hearsay—despite its obvious importance. We attempt to formalize hyperparameter sensitivity using two metrics: similarity-based sensitivity and performance-based sensitivity. We then use these metrics to quantify two such claims: (1) transformers are more sensitive to hyperparameter choices than LSTMs and (2) transformers are particularly sensitive to batch size. We conduct experiments on two different character-level sequence-to-sequence tasks and find that, indeed, the transformer is slightly more sensitive to hyperparameters according to both of our metrics. However, we do not find that it is more sensitive to batch size in particular.</abstract>
      <url hash="1f7882b9">2024.eacl-long.40</url>
      <bibkey>wiemerslage-etal-2024-quantifying</bibkey>
    </paper>
    <paper id="41">
      <title>Examining Gender and Racial Bias in Large Vision–Language Models Using a Novel Dataset of Parallel Images</title>
      <author><first>Kathleen</first><last>Fraser</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Svetlana</first><last>Kiritchenko</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>690-713</pages>
      <abstract>Following on recent advances in large language models (LLMs) and subsequent chat models, a new wave of large vision–language models (LVLMs) has emerged. Such models can incorporate images as input in addition to text, and perform tasks such as visual question answering, image captioning, story generation, etc. Here, we examine potential gender and racial biases in such systems, based on the perceived characteristics of the people in the input images. To accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday Scenarios). The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and visual content, but differ along the dimensions of gender (man, woman) and race (Black, white). By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.</abstract>
      <url hash="81161232">2024.eacl-long.41</url>
      <attachment type="note" hash="24d2c338">2024.eacl-long.41.note.zip</attachment>
      <bibkey>fraser-kiritchenko-2024-examining</bibkey>
    </paper>
    <paper id="42">
      <title><fixed-case>C</fixed-case>onstraint<fixed-case>C</fixed-case>hecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases</title>
      <author><first>Quyet V.</first><last>Do</last></author>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Shizhe</first><last>Diao</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Zhaowei</first><last>Wang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>714-731</pages>
      <abstract>Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has been explored as a way to acquire new commonsense knowledge based on reference knowledge in the original CSKBs and external prior knowledge.Despite the advancement of Large Language Models (LLM) and prompt engineering techniques in various reasoning tasks, they still struggle to deal with CSKB reasoning.One of the problems is that it is hard for them to acquire explicit relational constraints in CSKBs from only in-context exemplars, due to a lack of symbolic reasoning capabilities (CITATION).To this end, we proposed **ConstraintChecker**, a plugin over prompting techniques to provide and check explicit constraints.When considering a new knowledge instance, ConstraintChecker employs a rule-based module to produce a list of constraints, then it uses a zero-shot learning module to check whether this knowledge instance satisfies all constraints.The acquired constraint-checking result is then aggregated with the output of the main prompting technique to produce the final output.Experimental results on CSKB Reasoning benchmarks demonstrate the effectiveness of our method by bringing consistent improvements over all prompting methods.</abstract>
      <url hash="d007f452">2024.eacl-long.42</url>
      <attachment type="software" hash="938f2c5d">2024.eacl-long.42.software.zip</attachment>
      <bibkey>do-etal-2024-constraintchecker</bibkey>
    </paper>
    <paper id="43">
      <title>A* shortest string decoding for non-idempotent semirings</title>
      <author><first>Kyle</first><last>Gorman</last><affiliation>The Graduate Center, City University of New York and Google</affiliation></author>
      <author><first>Cyril</first><last>Allauzen</last><affiliation>Google</affiliation></author>
      <pages>732-739</pages>
      <abstract>Abstract: The single shortest path algorithm is undefined for weighted finite-state automata over non-idempotent semirings because such semirings do not guarantee the existence of a shortest path. However, in non-idempotent semirings admitting an order satisfying a monotonicity condition (such as the plus-times or log semirings), the shortest string is well-defined. We describe an algorithm which finds the shortest string for a weighted non-deterministic automaton over such semirings using the backwards shortest distance of an equivalent deterministic automaton (DFA) as a heuristic for A* search performed over a companion idempotent semiring, which is proven to return the shortest string. There may be exponentially more states in the DFA, but the proposed algorithm needs to visit only a small fraction of them if determinization is performed “on the fly”.</abstract>
      <url hash="ac12cde0">2024.eacl-long.43</url>
      <attachment type="software" hash="e3595a77">2024.eacl-long.43.software.tgz</attachment>
      <bibkey>gorman-allauzen-2024-shortest</bibkey>
    </paper>
    <paper id="44">
      <title>Importance-Aware Data Augmentation for Document-Level Neural Machine Translation</title>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Yufei</first><last>Wang</last></author>
      <author><first>George</first><last>Foster</last><affiliation>Google</affiliation></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University</affiliation></author>
      <pages>740-752</pages>
      <abstract>Document-level neural machine translation (DocNMT) aims to generate translations that are both coherent and cohesive, in contrast to its sentence-level counterpart. However, due to its longer input length and limited availability of training data, DocNMT often faces the challenge of data sparsity. To overcome this issue, we propose a novel Importance-Aware Data Augmentation (IADA) algorithm for DocNMT that augments the training data based on token importance information estimated by the norm of hidden states and training gradients. We conduct comprehensive experiments on three widely-used DocNMT benchmarks. Our empirical results show that our proposed IADA outperforms strong DocNMT baselines as well as several data augmentation approaches, with statistical significance on both sentence-level and document-level BLEU.</abstract>
      <url hash="9ae09aa2">2024.eacl-long.44</url>
      <bibkey>wu-etal-2024-importance</bibkey>
    </paper>
    <paper id="45">
      <title>Lost in Translationese? Reducing Translation Effect Using <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation</title>
      <author><first>Shira</first><last>Wein</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Nathan</first><last>Schneider</last><affiliation>Georgetown University</affiliation></author>
      <pages>753-765</pages>
      <abstract>Translated texts bear several hallmarks distinct from texts originating in the language (“translationese”). Though individual translated texts are often fluent and preserve meaning, at a large scale, translated texts have statistical tendencies which distinguish them from text originally written in the language and can affect model performance. We frame the novel task of translationese reduction and hypothesize that Abstract Meaning Representation (AMR), a graph-based semantic representation which abstracts away from the surface form, can be used as an interlingua to reduce the amount of translationese in translated texts. By parsing English translations into an AMR and then generating text from that AMR, the result more closely resembles originally English text across three quantitative macro-level measures, without severely compromising fluency or adequacy. We compare our AMR-based approach against three other techniques based on machine translation or paraphrase generation. This work represents the first approach to reducing translationese in text and highlights the promise of AMR, given that our AMR-based approach outperforms more computationally intensive methods.</abstract>
      <url hash="0262de7e">2024.eacl-long.45</url>
      <bibkey>wein-schneider-2024-lost</bibkey>
    </paper>
    <paper id="46">
      <title>Comparing Template-based and Template-free Language Model Probing</title>
      <author><first>Sagi</first><last>Shaier</last></author>
      <author><first>Kevin</first><last>Bennett</last><affiliation>Memorial Healthcare System</affiliation></author>
      <author><first>Lawrence</first><last>Hunter</last><affiliation>University of Colorado at Denver</affiliation></author>
      <author><first>Katharina</first><last>Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <pages>766-776</pages>
      <abstract>The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets – 4 template-based and 6 template-free – in general and biomedical domains to answer the following research questions: (RQ1) Do model rankings differ between the two approaches? (RQ2) Do models’ absolute scores differ between the two approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and domain-specific models? Our findings are: 1) Template-free and template-based approaches often rank models differently, except for the top domain- specific models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts. 3) Perplexity is negatively correlated with accuracy in the template-free approach, but, counter-intuitively, they are positively correlated for template-based probing. 4) Models tend to predict the same answers frequently across prompts for template-based probing, which is less common when employing template-free techniques.</abstract>
      <url hash="1e17a564">2024.eacl-long.46</url>
      <bibkey>shaier-etal-2024-comparing</bibkey>
    </paper>
    <paper id="47">
      <title>Desiderata For The Context Use Of Question Answering Systems</title>
      <author><first>Sagi</first><last>Shaier</last></author>
      <author><first>Lawrence</first><last>Hunter</last><affiliation>University of Colorado at Denver</affiliation></author>
      <author><first>Katharina</first><last>Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <pages>777-792</pages>
      <abstract>Prior work has uncovered a set of common problems in state-of-the-art context-based question answering (QA) systems: a lack of attention to the context when the latter conflicts with a model’s parametric knowledge, little robustness to noise, and a lack of consistency with their answers. However, most prior work focus on one or two of those problems in isolation, which makes it difficult to see trends across them. We aim to close this gap, by first outlining a set of – previously discussed as well as novel – desiderata for QA models. We then survey relevant analysis and methods papers to provide an overview of the state of the field. The second part of our work presents experiments where we evaluate 15 QA systems on 5 datasets according to all desiderata at once. We find many novel trends, including (1) systems that are less susceptible to noise are not necessarily more consistent with their answers when given irrelevant context; (2) most systems that are more susceptible to noise are more likely to correctly answer according to a context that conflicts with their parametric knowledge; and (3) the combination of conflicting knowledge and noise can reduce system performance by up to 96%. As such, our desiderata help increase our understanding of how these models work and reveal potential avenues for improvements.</abstract>
      <url hash="cc1dac2e">2024.eacl-long.47</url>
      <bibkey>shaier-etal-2024-desiderata</bibkey>
    </paper>
    <paper id="48">
      <title>Scaling up Discovery of Latent Concepts in Deep <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Majd</first><last>Hawasly</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Fahim</first><last>Dalvi</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Nadir</first><last>Durrani</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>793-806</pages>
      <abstract>Despite the revolution caused by deep NLP models, they remain black boxes, necessitating research to understand their decision-making processes. A recent work by Dalvi et al. (2022) carried out representation analysis through the lens of clustering latent spaces within pre-trained models (PLMs), but that approach is limited to small scale due to the high cost of running Agglomerative hierarchical clustering. This paper studies clustering algorithms in order to scale the discovery of encoded concepts in PLM representations to larger datasets and models. We propose metrics for assessing the quality of discovered latent concepts and use them to compare the studied clustering algorithms. We found that K-Means-based concept discovery significantly enhances efficiency while maintaining the quality of the obtained concepts. Furthermore, we demonstrate the practicality of this newfound efficiency by scaling latent concept discovery to LLMs and phrasal concepts.</abstract>
      <url hash="1c518b33">2024.eacl-long.48</url>
      <attachment type="software" hash="13e2df5b">2024.eacl-long.48.software.zip</attachment>
      <bibkey>hawasly-etal-2024-scaling</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>A</fixed-case>nthro<fixed-case>S</fixed-case>core: A Computational Linguistic Measure of Anthropomorphism</title>
      <author><first>Myra</first><last>Cheng</last><affiliation>Stanford University</affiliation></author>
      <author><first>Kristina</first><last>Gligoric</last><affiliation>Stanford University</affiliation></author>
      <author><first>Tiziano</first><last>Piccardi</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <pages>807-825</pages>
      <abstract>Anthropomorphism, or the attribution of human-like characteristics to non-human entities, has shaped conversations about the impacts and possibilities of technology. We present AnthroScore, an automatic metric of implicit anthropomorphism in language. We use a masked language model to quantify how non-human entities are implicitly framed as human by the surrounding context. We show that AnthroScore corresponds with human judgments of anthropomorphism and dimensions of anthropomorphism described in social science literature. Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles. In research papers, we find that anthropomorphism has steadily increased over time, and that papers related to language models have the most anthropomorphism. Within ACL papers, temporal increases in anthropomorphism are correlated with key neural advancements. Building upon concerns of scientific misinformation in mass media, we identify higher levels of anthropomorphism in news headlines compared to the research papers they cite. Since AnthroScore is lexicon-free, it can be directly applied to a wide range of text sources.</abstract>
      <url hash="400f26de">2024.eacl-long.49</url>
      <attachment type="software" hash="976fa742">2024.eacl-long.49.software.zip</attachment>
      <attachment type="note" hash="e0c7f2b8">2024.eacl-long.49.note.zip</attachment>
      <bibkey>cheng-etal-2024-anthroscore</bibkey>
    </paper>
    <paper id="50">
      <title>Centering the Speech Community</title>
      <author><first>Steven</first><last>Bird</last><affiliation>Charles Darwin University</affiliation></author>
      <author><first>Dean</first><last>Yibarbuk</last><affiliation>Warddeken Land Management</affiliation></author>
      <pages>826-839</pages>
      <abstract>How can NLP/AI practitioners engage with oral societies and develop locally appropriate language technologies? We report on our experience of working together over five years in a remote community in the far north of Australia, and how we prototyped simple language technologies to support our collaboration. We navigated different understandings of language, the functional differentiation of oral vs institutional languages, and the distinct technology opportunities for each. Our collaboration unsettled the first author’s western framing of language as data for exploitation by machines, and we devised a design pattern that seems better aligned with local interests and aspirations. We call for new collaborations on the design of locally appropriate technologies for languages with primary orality.</abstract>
      <url hash="746b433a">2024.eacl-long.50</url>
      <bibkey>bird-yibarbuk-2024-centering</bibkey>
    </paper>
    <paper id="51">
      <title>Improving the <fixed-case>TENOR</fixed-case> of Labeling: Re-evaluating Topic Models for Content Analysis</title>
      <author><first>Zongxia</first><last>Li</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Andrew</first><last>Mao</last></author>
      <author><first>Daniel</first><last>Stephens</last></author>
      <author><first>Pranav</first><last>Goel</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Emily</first><last>Walpole</last><affiliation>NIST</affiliation></author>
      <author><first>Alden</first><last>Dima</last><affiliation>National Institute of Standards and Technology</affiliation></author>
      <author><first>Juan</first><last>Fung</last><affiliation>National Institute of Standards and Technology</affiliation></author>
      <author><first>Jordan</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>840-859</pages>
      <abstract>Topic models are a popular tool for understanding text collections, but their evaluation has been a point of contention. Automated evaluation metrics such as coherence are often used, however, their validity has been questioned for neural topic models (NTMs) and can overlook a model’s benefits in real-world applications. To this end, we conduct the first evaluation of neural, supervised and classical topic models in an interactive task-based setting. We combine topic models with a classifier and test their ability to help humans conduct content analysis and document annotation. From simulated, real user and expert pilot studies, the Contextual Neural Topic Model does the best on cluster evaluation metrics and human evaluations; however, LDA is competitive with two other NTMs under our simulated experiment and user study results, contrary to what coherence scores suggest. We show that current automated metrics do not provide a complete picture of topic modeling capabilities, but the right choice of NTMs can be better than classical models on practical tasks.</abstract>
      <url hash="9a220a85">2024.eacl-long.51</url>
      <bibkey>li-etal-2024-improving</bibkey>
    </paper>
    <paper id="52">
      <title>Quality Does Matter: A Detailed Look at the Quality and Utility of Web-Mined Parallel Corpora</title>
      <author><first>Surangika</first><last>Ranathunga</last><affiliation>University of Moratuwa</affiliation></author>
      <author><first>Nisansa</first><last>De Silva</last><affiliation>University of Moratuwa</affiliation></author>
      <author><first>Velayuthan</first><last>Menan</last><affiliation>University of Moratuwa</affiliation></author>
      <author><first>Aloka</first><last>Fernando</last><affiliation>University of Moratuwa</affiliation></author>
      <author><first>Charitha</first><last>Rathnayake</last><affiliation>University of Moratuwa</affiliation></author>
      <pages>860-880</pages>
      <abstract>We conducted a detailed analysis on the quality of web-mined corpora for two low-resource languages (making three language pairs, English-Sinhala, English-Tamil and Sinhala-Tamil). We ranked each corpus according to a similarity measure and carried out an intrinsic and extrinsic evaluation on different portions of this ranked corpus. We show that there are significant quality differences between different portions of web-mined corpora and that the quality varies across languages and datasets. We also show that, for some web-mined datasets, Neural Machine Translation (NMT) models trained with their highest-ranked 25k portion can be on par with human-curated datasets.</abstract>
      <url hash="fc413046">2024.eacl-long.52</url>
      <bibkey>ranathunga-etal-2024-quality</bibkey>
    </paper>
    <paper id="53">
      <title><fixed-case>VOLTAGE</fixed-case>: A Versatile Contrastive Learning based <fixed-case>OCR</fixed-case> Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction</title>
      <author><first>Prawaal</first><last>Sharma</last></author>
      <author><first>Poonam</first><last>Goyal</last><affiliation>BITS Pilani, Birla Institute of Technology and Science</affiliation></author>
      <author><first>Vidisha</first><last>Sharma</last></author>
      <author><first>Navneet</first><last>Goyal</last><affiliation>BITS Pilani, Birla Institute of Technology and Science</affiliation></author>
      <pages>881-899</pages>
      <abstract>UNESCO has classified 2500 out of 7000 languages spoken worldwide as endangered. Attrition of a language leads to loss of traditional wisdom, folk literature, and the essence of the community that uses it. It is therefore imperative to bring digital inclusion to these languages and avoid its extinction. Low resource languages are at a greater risk of extinction. Lack of unsupervised Optical Character Recognition(OCR) methodologies for low resource languages is one of the reasons impeding their digital inclusion. We propose VOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyph feature recommendation for cluster-based labelling. We augment the labelled data for diversity and volume using image transformations and Generative Adversarial Networks. Voltage has been designed using Takri - a family of scripts used in 16th to 20th century in the Himalayan regions of India. We present results for Takri along with other Indic scripts (both low and high resource) to substantiate the universal behavior of the methodology. An accuracy of 95% for machine printed and 87% for handwritten samples on Takri script has been achieved. We conduct baseline and ablation studies along with building downstream use cases for Takri, demonstrating the usefulness of our work.</abstract>
      <url hash="294438ac">2024.eacl-long.53</url>
      <attachment type="software" hash="5e15ff8b">2024.eacl-long.53.software.zip</attachment>
      <attachment type="note" hash="95d75bc5">2024.eacl-long.53.note.zip</attachment>
      <bibkey>sharma-etal-2024-voltage</bibkey>
    </paper>
    <paper id="54">
      <title>Unsupervised Contrast-Consistent Ranking with Language Models</title>
      <author><first>Niklas</first><last>Stoehr</last></author>
      <author><first>Pengxiang</first><last>Cheng</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Jing</first><last>Wang</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Daniel</first><last>Preotiuc-Pietro</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Rajarshi</first><last>Bhowmik</last><affiliation>Bloomberg L.P.</affiliation></author>
      <pages>900-914</pages>
      <abstract>Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank product reviews by sentiment. We compare pairwise, pointwise and listwise prompting techniques to elicit a language model’s ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probe guided by a logical constraint: a language model’s representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent, pairwise or listwise comparisons. To this end, we extend the binary CCS method to Contrast-Consistent Ranking (CCR) by adapting existing ranking methods such as the Max-Margin Loss, Triplet Loss and an Ordinal Regression objective. Across different models and datasets, our results confirm that CCR probing performs better or, at least, on a par with prompting.</abstract>
      <url hash="c21cf89a">2024.eacl-long.54</url>
      <attachment type="software" hash="4badab27">2024.eacl-long.54.software.zip</attachment>
      <attachment type="note" hash="7ef8b127">2024.eacl-long.54.note.zip</attachment>
      <bibkey>stoehr-etal-2024-unsupervised</bibkey>
    </paper>
    <paper id="55">
      <title>Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models</title>
      <author><first>Jongyoon</first><last>Song</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Nohil</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Bongkyu</first><last>Hwang</last><affiliation>SAMSUNG SDS RESEARCH KOREA</affiliation></author>
      <author><first>Jaewoong</first><last>Yun</last></author>
      <author><first>Seongho</first><last>Joe</last><affiliation>Samsung</affiliation></author>
      <author><first>Youngjune</first><last>Gwon</last><affiliation>Samsung SDS</affiliation></author>
      <author><first>Sungroh</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <pages>915-929</pages>
      <abstract>Abstractive summarization models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document. In this paper, we analyze the robustness of fine-tuning based summarization models to the knowledge conflict, which we call factual adaptiveness. We utilize pre-trained language models to construct evaluation sets and find that factual adaptiveness is not strongly correlated with factual consistency on original datasets. Furthermore, we introduce a controllable counterfactual data augmentation method where the degree of knowledge conflict within the augmented data can be adjustable. Our experimental results on two pre-trained language models (PEGASUS and BART) and two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method enhances factual adaptiveness while achieving factual consistency on original datasets on par with the contrastive learning baseline.</abstract>
      <url hash="06aa3cef">2024.eacl-long.55</url>
      <bibkey>song-etal-2024-entity</bibkey>
    </paper>
    <paper id="56">
      <title>Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes Through Multimodal Explanations</title>
      <author><first>Prince</first><last>Jha</last></author>
      <author><first>Krishanu</first><last>Maity</last></author>
      <author><first>Raghav</first><last>Jain</last><affiliation>Indian Institute of Technology, Patna.</affiliation></author>
      <author><first>Apoorv</first><last>Verma</last></author>
      <author><first>Sriparna</first><last>Saha</last><affiliation>Indian Institute of Technology Patna, India</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>930-943</pages>
      <abstract>Internet memes have gained significant influence in communicating political, psychological, and sociocultural ideas. While meme are often humorous, there has been a rise in the use of memes for trolling and cyberbullying. Although a wide variety of effective deep learning-based models have been developed for detecting offensive multimodal memes, only a few works have been done on explainability aspect. Recent laws like “right to explanations” of General Data Protection Regulation, have spurred research in developing interpretable models rather than only focusing on performance. Motivated by this, we introduce MultiBully-Ex, the first benchmark dataset for multimodal explanation from code-mixed cyberbullying memes. Here, both visual and textual modalities are highlighted to explain why a given meme is cyberbullying. A Contrastive Language-Image Pretraining (CLIP) projection based multimodal shared-private multitask approach has been proposed for visual and textual explanation of a meme. Experimental results demonstrate that training with multimodal explanations improves performance in generating textual justifications and more accurately identifying the visual evidence supporting a decision with reliable performance improvements.</abstract>
      <url hash="e4b37150">2024.eacl-long.56</url>
      <bibkey>jha-etal-2024-meme</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>L</fixed-case>a<fixed-case>M</fixed-case>ini-<fixed-case>LM</fixed-case>: A Diverse Herd of Distilled Models from Large-Scale Instructions</title>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Abdul</first><last>Waheed</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Chiyu</first><last>Zhang</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Alham</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Amazon</affiliation></author>
      <pages>944-964</pages>
      <abstract>Large language models (LLMs) with instruction fine-tuning demonstrate superior generative capabilities. However, these models are resource-intensive. To alleviate this issue, we explore distilling knowledge from instruction-tuned LLMs into much smaller ones. While other similar works have been done, they are often conducted on a limited set of (usually still large) models and are not accompanied by proper evaluations. To this end, we carefully develop a large set of 2.58M instructions based on both existing and newly-generated instructions. In addition to being sizable, we design our instructions to cover a broad set of topics to ensure diversity. Extensive analysis of our instruction dataset confirms its diversity, and we generate responses for these instructions using gpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of models, collectively referred to as LaMini-LM, which includes models from both the encoder-decoder and decoder-only families, with varying sizes. We evaluate the performance of our models using automatic metrics on 15 different natural language processing (NLP) benchmarks, as well as through human assessment. We also assess the model for hallucination and toxicity, and for the former, we introduce a new benchmark dataset for hallucination-inducing QA. The results demonstrate that our proposed LaMini-LM models are comparable to strong baselines while being much smaller in size.</abstract>
      <url hash="30928b55">2024.eacl-long.57</url>
      <bibkey>wu-etal-2024-lamini</bibkey>
    </paper>
    <paper id="58">
      <title>Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer</title>
      <author><first>V.S.D.S.Mahesh</first><last>Akavarapu</last><affiliation>IIT Kanpur, IIT Kanpur</affiliation></author>
      <author><first>Arnab</first><last>Bhattacharya</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>965-975</pages>
      <abstract>Identification of cognates across related languages is one of the primary problems in historical linguistics. Automated cognate identification is helpful for several downstream tasks including identifying sound correspondences, proto-language reconstruction, phylogenetic classification, etc. Previous state-of-the-art methods are mostly based on distributions of phonemes computed across multilingual wordlists and make little use of the cognacy labels that define links among cognate clusters. In this paper, we present a transformer-based architecture inspired by computational biology for the task of automated cognate detection. Beyond a certain amount of supervision, this method performs better than the existing methods, and shows steady improvement with further increase in supervision proving the efficacy of utilizing the labeled information. We also demonstrate that accepting multiple sequence alignments as input and having an end-to-end architecture with link prediction head saves much computation time while simultaneously yielding superior performance.</abstract>
      <url hash="9766a43a">2024.eacl-long.58</url>
      <attachment type="software" hash="5091b085">2024.eacl-long.58.software.zip</attachment>
      <bibkey>akavarapu-bhattacharya-2024-automated</bibkey>
    </paper>
    <paper id="59">
      <title>Leveraging Multi-lingual Positive Instances in Contrastive Learning to Improve Sentence Embedding</title>
      <author><first>Kaiyan</first><last>Zhao</last></author>
      <author><first>Qiyu</first><last>Wu</last><affiliation>The University of Tokyo, Tokyo Institute of Technology</affiliation></author>
      <author><first>Xin-Qiang</first><last>Cai</last></author>
      <author><first>Yoshimasa</first><last>Tsuruoka</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>976-991</pages>
      <abstract>Learning multilingual sentence embeddings is a fundamental task in natural language processing. Recent trends in learning both monolingual and multilingual sentence embeddings are mainly based on contrastive learning (CL) among an anchor, one positive, and multiple negative instances. In this work, we argue that leveraging multiple positives should be considered for multilingual sentence embeddings because (1) positives in a diverse set of languages can benefit cross-lingual learning, and (2) transitive similarity across multiple positives can provide reliable structural information for learning.In order to investigate the impact of multiple positives in CL, we propose a novel approach, named MPCL, to effectively utilize multiple positive instances to improve the learning of multilingual sentence embeddings. Experimental results on various backbone models and downstream tasks demonstrate that MPCL leads to better retrieval, semantic similarity, and classification performance compared to conventional CL. We also observe that in unseen languages, sentence embedding models trained on multiple positives show better cross-lingual transfer performance than models trained on a single positive instance.</abstract>
      <url hash="b048238d">2024.eacl-long.59</url>
      <bibkey>zhao-etal-2024-leveraging</bibkey>
    </paper>
    <paper id="60">
      <title>Moderation in the Wild: Investigating User-Driven Moderation in Online Discussions</title>
      <author><first>Neele</first><last>Falk</last><affiliation>University of Stuttgart, University of Stuttgart</affiliation></author>
      <author><first>Eva</first><last>Vecchi</last></author>
      <author><first>Iman</first><last>Jundi</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Gabriella</first><last>Lapesa</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>992-1013</pages>
      <abstract>Effective content moderation is imperative for fostering healthy and productive discussions in online domains. Despite the substantial efforts of moderators, the overwhelming nature of discussion flow can limit their effectiveness. However, it is not only trained moderators who intervene in online discussions to improve their quality. “Ordinary” users also act as moderators, actively intervening to correct information of other users’ posts, enhance arguments, and steer discussions back on course.This paper introduces the phenomenon of user moderation, documenting and releasing UMOD, the first dataset of comments in whichusers act as moderators. UMOD contains 1000 comment-reply pairs from the subreddit r/changemyview with crowdsourced annotations from a large annotator pool and with a fine-grained annotation schema targeting the functions of moderation, stylistic properties(aggressiveness, subjectivity, sentiment), constructiveness, as well as the individual perspectives of the annotators on the task. The releaseof UMOD is complemented by two analyses which focus on the constitutive features of constructiveness in user moderation and on thesources of annotator disagreements, given the high subjectivity of the task.</abstract>
      <url hash="7c29a4b5">2024.eacl-long.60</url>
      <bibkey>falk-etal-2024-moderation</bibkey>
    </paper>
    <paper id="61">
      <title>Cross-Lingual Transfer from Related Languages: Treating Low-Resource <fixed-case>M</fixed-case>altese as Multilingual Code-Switching</title>
      <author><first>Kurt</first><last>Micallef</last><affiliation>University of Malta</affiliation></author>
      <author><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author><first>Claudia</first><last>Borg</last><affiliation>University of Malta</affiliation></author>
      <author><first>Fadhl</first><last>Eryani</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Houda</first><last>Bouamor</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>1014-1025</pages>
      <abstract>Although multilingual language models exhibit impressive cross-lingual transfer capabilities on unseen languages, the performance on downstream tasks is impacted when there is a script disparity with the languages used in the multilingual model’s pre-training data. Using transliteration offers a straightforward yet effective means to align the script of a resource-rich language with a target language thereby enhancing cross-lingual transfer capabilities. However, for mixed languages, this approach is suboptimal, since only a subset of the language benefits from the cross-lingual transfer while the remainder is impeded. In this work, we focus on Maltese, a Semitic language, with substantial influences from Arabic, Italian, and English, and notably written in Latin script. We present a novel dataset annotated with word-level etymology. We use this dataset to train a classifier that enables us to make informed decisions regarding the appropriate processing of each token in the Maltese language. We contrast indiscriminate transliteration or translation to mixing processing pipelines that only transliterate words of Arabic origin, thereby resulting in text with a mixture of scripts. We fine-tune the processed data on four downstream tasks and show that conditional transliteration based on word etymology yields the best results, surpassing fine-tuning with raw Maltese or Maltese processed with non-selective pipelines.</abstract>
      <url hash="401ed994">2024.eacl-long.61</url>
      <attachment type="note" hash="12508e70">2024.eacl-long.61.note.zip</attachment>
      <bibkey>micallef-etal-2024-cross</bibkey>
    </paper>
    <paper id="62">
      <title>Where Do We Go From Here? Multi-scale Allocentric Relational Inferencefrom Natural Spatial Descriptions</title>
      <author><first>Tzuf</first><last>Paz-Argaman</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>John</first><last>Palowitch</last><affiliation>Google</affiliation></author>
      <author><first>Sayali</first><last>Kulkarni</last><affiliation>Research, Google and Google</affiliation></author>
      <author><first>Jason</first><last>Baldridge</last><affiliation>Google</affiliation></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Bar-Ilan University, Technion</affiliation></author>
      <pages>1026-1040</pages>
      <abstract>The concept of <i>acquired spatial knowledge</i> is crucial in spatial cognitive research, particularly when it comes to communicating routes. However, NLP navigation studies often overlook the impact of acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., ‘it will be on your right’) that require reasoning over the agent’s local perception. These instructions are typically given in a sequence of steps, with each action-step explicitly mentioned and followed by a landmark that the agent can use to verify that they are on the correct path (e.g., ‘turn right and then you will see...’). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its compositionality. These instructions typically contain allocentric relations, are non-sequential, with implicit actions and multiple spatial relations without any verification (e.g., ‘south of Central Park and a block north of a police station’). This paper introduces the Rendezvous (RVS) task and dataset, which includes 10,404 examples of English geospatial instructions for reaching a target location using map-knowledge. Our analysis reveals that RVS exhibits a richer use of spatial allocentric relations, and requires resolving more spatial relations simultaneously compared to previous text-based navigation benchmarks.</abstract>
      <url hash="591a1c74">2024.eacl-long.62</url>
      <attachment type="note" hash="b3c63758">2024.eacl-long.62.note.zip</attachment>
      <bibkey>paz-argaman-etal-2024-go</bibkey>
    </paper>
    <paper id="63">
      <title>Bias in Opinion Summarisation from Pre-training to Adaptation: A Case Study in Political Bias</title>
      <author><first>Nannan</first><last>Huang</last></author>
      <author><first>Haytham</first><last>Fayek</last><affiliation>Royal Melbourne Institute of Technology and Facebook</affiliation></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <pages>1041-1055</pages>
      <abstract>Opinion summarisation aims to summarise the salient information and opinions presented in documents such as product reviews, discussion forums, and social media texts into short summaries that enable users to effectively understand the opinions therein.Generating biased summaries has the risk of potentially swaying public opinion. Previous studies focused on studying bias in opinion summarisation using extractive models, but limited research has paid attention to abstractive summarisation models. In this study, using political bias as a case study, we first establish a methodology to quantify bias in abstractive models, then trace it from the pre-trained models to the task of summarising social media opinions using different models and adaptation methods. We find that most models exhibit intrinsic bias. Using a social media text summarisation dataset and contrasting various adaptation methods, we find that tuning a smaller number of parameters is less biased compared to standard fine-tuning; however, the diversity of topics in training data used for fine-tuning is critical.</abstract>
      <url hash="eca14f3f">2024.eacl-long.63</url>
      <bibkey>huang-etal-2024-bias</bibkey>
    </paper>
    <paper id="64">
      <title>Document Structure in Long Document Transformers</title>
      <author><first>Jan</first><last>Buchmann</last><affiliation>UKP (TU Darmstadt)</affiliation></author>
      <author><first>Max</first><last>Eichler</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Jan-Micha</first><last>Bodensohn</last><affiliation>CS Department, TU Darmstadt, Technische Universität Darmstadt and German Research Center for AI</affiliation></author>
      <author><first>Ilia</first><last>Kuznetsov</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>1056-1073</pages>
      <abstract>Long documents often exhibit structure with hierarchically organized elements of different functions, such as section headers and paragraphs. Despite the omnipresence of document structure, its role in natural language processing (NLP) remains opaque. Do long-document Transformer models acquire an internal representation of document structure during pre-training? How can structural information be communicated to a model after pre-training, and how does it influence downstream performance? To answer these questions, we develop a novel suite of probing tasks to assess structure-awareness of long-document Transformers, propose general-purpose structure infusion methods, and evaluate the effects of structure infusion on QASPER and Evidence Inference, two challenging long-document NLP tasks. Results on LED and LongT5 suggest that they acquire implicit understanding of document structure during pre-training, which can be further enhanced by structure infusion, leading to improved end-task performance. To foster research on the role of document structure in NLP modeling, we make our data and code publicly available.</abstract>
      <url hash="f54d7c69">2024.eacl-long.64</url>
      <attachment type="software" hash="397ee9e7">2024.eacl-long.64.software.zip</attachment>
      <attachment type="note" hash="57053c89">2024.eacl-long.64.note.zip</attachment>
      <bibkey>buchmann-etal-2024-document</bibkey>
    </paper>
    <paper id="65">
      <title>The Role of Data Curation in Image Captioning</title>
      <author><first>Wenyan</first><last>Li</last></author>
      <author><first>Jonas</first><last>Lotz</last></author>
      <author><first>Chen</first><last>Qiu</last><affiliation>Wuhan University Science and Technology</affiliation></author>
      <author><first>Desmond</first><last>Elliott</last><affiliation>and University of Copenhagen</affiliation></author>
      <pages>1074-1088</pages>
      <abstract>Image captioning models are typically trained by treating all samples equally, neglecting to account for mismatched or otherwise difficult data points. In contrast, recent work has shown the effectiveness of training models by scheduling the data using curriculum learning strategies. This paper contributes to this direction by actively curating difficult samples in datasets without increasing the total number of samples. We explore the effect of using three data curation methods within the training process: complete removal of an sample, caption replacement, or image replacement via a text-to-image generation model. Experiments on the Flickr30K and COCO datasets with the BLIP and BEiT-3 models demonstrate that these curation methods do indeed yield improved image captioning models, underscoring their efficacy.</abstract>
      <url hash="cfe82387">2024.eacl-long.65</url>
      <attachment type="software" hash="1b313c29">2024.eacl-long.65.software.zip</attachment>
      <bibkey>li-etal-2024-role</bibkey>
    </paper>
    <paper id="66">
      <title>Large-Scale Bitext Corpora Provide New Evidence for Cognitive Representations of Spatial Terms</title>
      <author><first>Peter</first><last>Viechnicki</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Kevin</first><last>Duh</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Anthony</first><last>Kostacos</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Barbara</first><last>Landau</last></author>
      <pages>1089-1099</pages>
      <abstract>Recent evidence from cognitive science suggests that there exist two classes of cognitive representations within the spatial terms of a language, one represented geometrically (e.g., above, below) and the other functionally (e.g., on, in). It has been hypothesized that geometric terms are more constrained and are mastered relatively early in language learning, whereas functional terms are less constrained and are mastered over longer time periods (Landau, 2016). One consequence of this hypothesis is that these two classes should exhibit different cross-linguistic variability, which is supported by human elicitation studies. In this work we present to our knowledge the first corpus-based empirical test of this hypothesis. We develop a pipeline for extracting, isolating, and aligning spatial terms in basic locative constructions from parallel text. Using Shannon entropy to measure the variability of spatial term use across eight languages, we find supporting evidence that variability in functional terms differs significantly from that of geometric terms. We also perform latent variable modeling and find support for the division of spatial terms into geometric and functional classes.</abstract>
      <url hash="be8947a4">2024.eacl-long.66</url>
      <bibkey>viechnicki-etal-2024-large</bibkey>
    </paper>
    <paper id="67">
      <title><fixed-case>REFINER</fixed-case>: Reasoning Feedback on Intermediate Representations</title>
      <author><first>Debjit</first><last>Paul</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Mete</first><last>Ismayilzada</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Maxime</first><last>Peyrard</last><affiliation>CNRS</affiliation></author>
      <author><first>Beatriz</first><last>Borges</last><affiliation>School of Computer and Communication Sciences, EPFL - EPF Lausanne</affiliation></author>
      <author><first>Antoine</first><last>Bosselut</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Robert</first><last>West</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Boi</first><last>Faltings</last></author>
      <pages>1100-1126</pages>
      <abstract>Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences,e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial contextand lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT-3.5 or ChatGPT as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be substituted with humans at inference time.</abstract>
      <url hash="3d68ab77">2024.eacl-long.67</url>
      <bibkey>paul-etal-2024-refiner</bibkey>
    </paper>
    <paper id="68">
      <title><fixed-case>H</fixed-case>um<fixed-case>BEL</fixed-case>: A Human-in-the-Loop Approach for Evaluating Demographic Factors of Language Models in Human-Machine Conversations</title>
      <author><first>Anthony</first><last>Sicilia</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jennifer</first><last>Gates</last></author>
      <author><first>Malihe</first><last>Alikhani</last><affiliation>Northeastern University</affiliation></author>
      <pages>1127-1143</pages>
      <abstract>While demographic factors like age and gender change the way people talk, and in particular, the way people talk to machines, there is little investigation into how large pre-trained language models (LMs) can adapt to these changes. To remedy this gap, we consider how demographic factors in LM language skills can be measured to determine compatibility with a target demographic. We suggest clinical techniques from Speech Language Pathology, which has norms for acquisition of language skills in humans. We conduct evaluation with a domain expert (i.e., a clinically licensed speech language pathologist), and also propose automated techniques to complement clinical evaluation at scale. Empirically, we focus on age, finding LM capability varies widely depending on task: GPT-3.5 mimics the ability of humans ranging from age 6-15 at tasks requiring inference, and simultaneously, outperforms a typical 21 year old at memorization. GPT-3.5 also has trouble with social language use, exhibiting less than 50% of the tested pragmatic skills. Findings affirm the importance of considering demographic alignment and conversational goals when using LMs as public-facing tools. Code, data, and a package will be available.</abstract>
      <url hash="a238df32">2024.eacl-long.68</url>
      <bibkey>sicilia-etal-2024-humbel</bibkey>
    </paper>
    <paper id="69">
      <title><fixed-case>LOCOST</fixed-case>: State-Space Models for Long Document Abstractive Summarization</title>
      <author><first>Florian</first><last>Le Bronnec</last></author>
      <author><first>Song</first><last>Duong</last></author>
      <author><first>Mathieu</first><last>Ravaut</last></author>
      <author><first>Alexandre</first><last>Allauzen</last><affiliation>Ecole supérieure de physique et chimie and Univeristé Paris-Dauphine</affiliation></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <author><first>Vincent</first><last>Guigue</last><affiliation>AgroParisTech</affiliation></author>
      <author><first>Alberto</first><last>Lumbreras</last><affiliation>Criteo</affiliation></author>
      <author><first>Laure</first><last>Soulier</last><affiliation>Université Pierre et Marie Curie - Paris 6, Sorbonne Université - Faculté des Sciences (Paris VI)</affiliation></author>
      <author><first>Patrick</first><last>Gallinari</last><affiliation>Criteo AI Lab and Sorbonne Universite</affiliation></author>
      <pages>1144-1159</pages>
      <abstract>State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of <tex-math>\mathcal{O}(L \log L)</tex-math>, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.</abstract>
      <url hash="e4b40864">2024.eacl-long.69</url>
      <bibkey>le-bronnec-etal-2024-locost</bibkey>
      <revision id="1" href="2024.eacl-long.69v1" hash="7d28d719"/>
      <revision id="2" href="2024.eacl-long.69v2" hash="e4b40864" date="2024-03-21">Add an extra acknowlegement.</revision>
    </paper>
    <paper id="70">
      <title>A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation</title>
      <author><first>Sahar</first><last>Sadrizadeh</last></author>
      <author><first>Ljiljana</first><last>Dolamic</last><affiliation>armasuisse</affiliation></author>
      <author><first>Pascal</first><last>Frossard</last><affiliation>EPFL</affiliation></author>
      <pages>1160-1177</pages>
      <abstract>Neural Machine Translation (NMT) models have been shown to be vulnerable to adversarial attacks, wherein carefully crafted perturbations of the input can mislead the target model. In this paper, we introduce ACT, a novel adversarial attack framework against NMT systems guided by a classifier. In our attack, the adversary aims to craft meaning-preserving adversarial examples whose translations in the target language by the NMT model belong to a different class than the original translations. Unlike previous attacks, our new approach has a more substantial effect on the translation by altering the overall meaning, which then leads to a different class determined by an oracle classifier. To evaluate the robustness of NMT models to our attack, we propose enhancements to existing black-box word-replacement-based attacks by incorporating output translations of the target NMT model and the output logits of a classifier within the attack process. Extensive experiments, including a comparison with existing untargeted attacks, show that our attack is considerably more successful in altering the class of the output translation and has more effect on the translation. This new paradigm can reveal the vulnerabilities of NMT systems by focusing on the class of translation rather than the mere translation quality as studied traditionally.</abstract>
      <url hash="3af276be">2024.eacl-long.70</url>
      <bibkey>sadrizadeh-etal-2024-classification</bibkey>
    </paper>
    <paper id="71">
      <title>Improving Generalization in Semantic Parsing by Increasing Natural Language Variation</title>
      <author><first>Irina</first><last>Saparina</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>1178-1193</pages>
      <abstract>Text-to-SQL semantic parsing has made significant progress in recent years, with various models demonstrating impressive performance on the challenging Spider benchmark. However, it has also been shown that these models often struggle to generalize even when faced with small perturbations of previously (accurately) parsed expressions. This is mainly due to the linguistic form of questions in Spider which are overly specific, unnatural, and display limited variation. In this work, we use data augmentation to enhance the robustness of text-to-SQL parsers against natural language variations. Existing approaches generate question reformulations either via models trained on Spider or only introduce local changes. In contrast, we leverage the capabilities of large language models to generate more realistic and diverse questions. Using only a few prompts, we achieve a two-fold increase in the number of questions in Spider. Training on this augmented dataset yields substantial improvements on a range of evaluation sets, including robustness benchmarks and out-of-domain data.</abstract>
      <url hash="5cba1ca2">2024.eacl-long.71</url>
      <bibkey>saparina-lapata-2024-improving</bibkey>
    </paper>
    <paper id="72">
      <title>Text-to-Code Generation with Modality-relative Pre-training</title>
      <author><first>Fenia</first><last>Christopoulou</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Guchun</first><last>Zhang</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Gerasimos</first><last>Lampouras</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>1194-1208</pages>
      <abstract>Large pre-trained language models have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model–where training sequences typically contain both natural and (linearised) programming language. Such approaches effectively map both modalities of the sequence into the same embedding space. However, programming language keywords (e.g. “while”) often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa. Assuming an already pre-trained language model, in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task. We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectives. We focus on text-to-code generation and observe consistent improvements across two backbone models and two test sets, measuring pass@<tex-math>k</tex-math> and a novel incremental variation.</abstract>
      <url hash="fa3b5357">2024.eacl-long.72</url>
      <bibkey>christopoulou-etal-2024-text</bibkey>
    </paper>
    <paper id="73">
      <title>No Error Left Behind: Multilingual Grammatical Error Correction with Pre-trained Translation Models</title>
      <author><first>Agnes</first><last>Luhtaru</last><affiliation>institute of computer science, University of Tartu</affiliation></author>
      <author><first>Elizaveta</first><last>Korotkova</last><affiliation>University of Tartu</affiliation></author>
      <author><first>Mark</first><last>Fishel</last><affiliation>University of Tartu</affiliation></author>
      <pages>1209-1222</pages>
      <abstract>Grammatical Error Correction (GEC) enhances language proficiency and promotes effective communication, but research has primarily centered around English. We propose a simple approach to multilingual and low-resource GEC by exploring the potential of multilingual machine translation (MT) models for error correction. We show that MT models are not only capable of error correction out-of-the-box, but that they can also be fine-tuned to even better correction quality. Results show the effectiveness of this approach, with our multilingual model outperforming similar-sized mT5-based models and even competing favourably with larger models.</abstract>
      <url hash="3e160efb">2024.eacl-long.73</url>
      <bibkey>luhtaru-etal-2024-error</bibkey>
    </paper>
    <paper id="74">
      <title>Quantifying Stereotypes in Language</title>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>1223-1240</pages>
      <abstract>A stereotype is a generalized perception of a specific group of humans. It is often potentially encoded in human language, which is more common in texts on social issues. Previous works simply define a sentence as stereotypical and anti-stereotypical. However, the stereotype of a sentence may require fine-grained quantification. In this paper, to fill this gap, we quantify stereotypes in language by annotating a dataset. We use the pre-trained language models (PLMs) to learn this dataset to predict stereotypes of sentences. Then, we discuss stereotypes about common social issues such as hate speech, sexism, sentiments, and disadvantaged and advantaged groups. We demonstrate the connections and differences between stereotypes and common social issues, and all four studies validate the general findings of the current studies. In addition, our work suggests that fine-grained stereotype scores are a highly relevant and competitive dimension for research on social issues. The models and datasets used in this paper are available at https://anonymous.4open.science/r/quantifying_stereotypes_in_language.</abstract>
      <url hash="ef21cc92">2024.eacl-long.74</url>
      <attachment type="software" hash="c4201e57">2024.eacl-long.74.software.zip</attachment>
      <bibkey>liu-2024-quantifying</bibkey>
    </paper>
    <paper id="75">
      <title>Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model</title>
      <author><first>Andrew</first><last>Brown</last></author>
      <author><first>Jiading</first><last>Zhu</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Mohamed</first><last>Abdelwahab</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Alec</first><last>Dong</last></author>
      <author><first>Cindy</first><last>Wang</last><affiliation>NA</affiliation></author>
      <author><first>Jonathan</first><last>Rose</last><affiliation>University of Toronto</affiliation></author>
      <pages>1241-1252</pages>
      <abstract>Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we fine-tune different sizes of the GPT-2 family. The GPT-2-small model achieves 83% success on a hold-out test set and the GPT-2 XL achieves 90% success. We also show that GPT-4 can help in the labor-intensive task of evaluating the quality of the distilled models, using it as a zero-shot classifier. Using triple-human review as a guide, the classifier achieves a Cohen-Kappa of 0.66, a substantial inter-rater reliability figure.</abstract>
      <url hash="5d0f148a">2024.eacl-long.75</url>
      <bibkey>brown-etal-2024-generation</bibkey>
    </paper>
    <paper id="76">
      <title>Multi-Reference Benchmarks for <fixed-case>R</fixed-case>ussian Grammatical Error Correction</title>
      <author><first>Frank</first><last>Palma Gomez</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <pages>1253-1270</pages>
      <abstract>This paper presents multi-reference benchmarks for the Grammatical Error Correction (GEC) of Russian, based on two existing single-reference datasets, for a total of 7,444 learner sentences from a variety of first language backgrounds. Each sentence is corrected independently by two new raters, and their corrections are reviewed by a senior annotator, resulting in a total of three references per sentence. Analysis of the annotations reveals that the new raters tend to make more changes, compared to the original raters, especially at the lexical level. We conduct experiments with two popular GEC approaches and show competitive performance on the original datasets and the new benchmarks. We also compare system scores as evaluated against individual annotators and discuss the effect of using multiple references overall and on specific error types. We find that using the union of the references increases system scores by more than 10 points and decreases the gap between system and human performance, thereby providing a more realistic evaluation of GEC system performance, although the effect is not the same across the error types. The annotations are available for research.</abstract>
      <url hash="04fe91fa">2024.eacl-long.76</url>
      <bibkey>palma-gomez-rozovskaya-2024-multi</bibkey>
    </paper>
    <paper id="77">
      <title>Plan-Grounded Large Language Models for Dual Goal Conversational Settings</title>
      <author><first>Diogo</first><last>Glória-Silva</last><affiliation>Universidade NOVA de Lisboa</affiliation></author>
      <author><first>Rafael</first><last>Ferreira</last><affiliation>Universidade NOVA de Lisboa</affiliation></author>
      <author><first>Diogo</first><last>Tavares</last><affiliation>Universidade NOVA de Lisboa</affiliation></author>
      <author><first>David</first><last>Semedo</last><affiliation>Universidade NOVA de Lisboa and Universidade NOVA de Lisboa</affiliation></author>
      <author><first>Joao</first><last>Magalhaes</last><affiliation>Universidade Nova de Lisboa</affiliation></author>
      <pages>1271-1292</pages>
      <abstract>Training Large Language Models (LLMs) to follow user instructions has shown to supply the LLM with ample capacity to converse fluently while being aligned with humans. Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings where instructions flow in both directions of the conversation, i.e. both the LLM and the user provide instructions to one another. In this paper, we tackle a dual goal mixed-initiative conversational setting where the LLM not only grounds the conversation on an arbitrary plan but also seeks to satisfy both a procedural plan and user instructions. The LLM is then responsible for guiding the user through the plan and, at the same time, adapting to new circumstances, answering questions, and activating safety guardrails when needed. We propose a novel LLM that grounds the dialogue on a procedural plan, can take the dialogue initiative, and enforces guardrails on the system’s behavior, while also improving the LLM’s responses to unexpected user behavior. Experiments in controlled settings and with real users show that the best-performing model, which we call PlanLLM, achieves a 2.1x improvement over a strong baseline. Moreover, experiments also show good generalization to unseen domains.</abstract>
      <url hash="81ed6b57">2024.eacl-long.77</url>
      <bibkey>gloria-silva-etal-2024-plan</bibkey>
    </paper>
    <paper id="78">
      <title>“Define Your Terms” : Enhancing Efficient Offensive Speech Classification with Definition</title>
      <author><first>Huy</first><last>Nghiem</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Umang</first><last>Gupta</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Fred</first><last>Morstatter</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>1293-1309</pages>
      <abstract>The propagation of offensive content through social media channels has garnered attention of the research community. Multiple works have proposed various semantically related yet subtle distinct categories of offensive speech. In this work, we explore meta-learning approaches to leverage the diversity of offensive speech corpora to enhance their reliable and efficient detection. We propose a joint embedding architecture that incorporates the input’s label and definition for classification via Prototypical Network. Our model achieves at least 75% of the maximal F1-score while using less than 10% of the available training data across 4 datasets. Our experimental findings also provide a case study of training strategies valuable to combat resource scarcity.</abstract>
      <url hash="da830553">2024.eacl-long.78</url>
      <bibkey>nghiem-etal-2024-define</bibkey>
    </paper>
    <paper id="79">
      <title><fixed-case>V</fixed-case>log<fixed-case>QA</fixed-case>: Task, Dataset, and Baseline Models for <fixed-case>V</fixed-case>ietnamese Spoken-Based Machine Reading Comprehension</title>
      <author><first>Thinh</first><last>Ngo</last></author>
      <author><first>Khoa</first><last>Dang</last></author>
      <author><first>Son</first><last>Luu</last><affiliation>University of Information Technology, Vietnam National University Hochiminh City</affiliation></author>
      <author><first>Kiet</first><last>Nguyen</last><affiliation>University of Information Technology, VNU-HCM</affiliation></author>
      <author><first>Ngan</first><last>Nguyen</last></author>
      <pages>1310-1324</pages>
      <abstract>This paper presents the development process of a Vietnamese spoken language corpus for machine reading comprehension (MRC) tasks and provides insights into the challenges and opportunities associated with using real-world data for machine reading comprehension tasks. The existing MRC corpora in Vietnamese mainly focus on formal written documents such as Wikipedia articles, online newspapers, or textbooks. In contrast, the VlogQA consists of 10,076 question-answer pairs based on 1,230 transcript documents sourced from YouTube – an extensive source of user-uploaded content, covering the topics of food and travel. By capturing the spoken language of native Vietnamese speakers in natural settings, an obscure corner overlooked in Vietnamese research, the corpus provides a valuable resource for future research in reading comprehension tasks for the Vietnamese language. Regarding performance evaluation, our deep-learning models achieved the highest F1 score of 75.34% on the test set, indicating significant progress in machine reading comprehension for Vietnamese spoken language data. In terms of EM, the highest score we accomplished is 53.97%, which reflects the challenge in processing spoken-based content and highlights the need for further improvement.</abstract>
      <url hash="ca4a50d7">2024.eacl-long.79</url>
      <attachment type="software" hash="2c8e4a99">2024.eacl-long.79.software.zip</attachment>
      <attachment type="note" hash="7fc74a90">2024.eacl-long.79.note.zip</attachment>
      <bibkey>ngo-etal-2024-vlogqa</bibkey>
      <revision id="1" href="2024.eacl-long.79v1" hash="8aa9495f"/>
      <revision id="2" href="2024.eacl-long.79v2" hash="ca4a50d7" date="2024-03-17">Minor updates.</revision>
    </paper>
    <paper id="80">
      <title><fixed-case>CEV</fixed-case>-<fixed-case>LM</fixed-case>: Controlled Edit Vector Language Model for Shaping Natural Language Generations</title>
      <author><first>Samraj</first><last>Moorjani</last></author>
      <author><first>Adit</first><last>Krishnan</last><affiliation>Amazon</affiliation></author>
      <author><first>Hari</first><last>Sundaram</last><affiliation>Department of Computer Science</affiliation></author>
      <pages>1325-1340</pages>
      <abstract>As large-scale language models become the standard for text generation, there is a greater need to tailor the generations to be more or less concise, targeted, and informative, depending on the audience/application. Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of text, but are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text. In this paper, we introduce CEV-LM - a lightweight, semi-autoregressive language model that utilizes constrained edit vectors to control three complementary metrics (speed, volume, and circuitousness) that quantify the shape of text (e.g., pacing of content). We study an extensive set of state-of-the-art CTG models and find that CEV-LM provides significantly more targeted and precise control of these three metrics while preserving semantic content, using less training data, and containing fewer parameters.</abstract>
      <url hash="92f39c93">2024.eacl-long.80</url>
      <attachment type="software" hash="020da64e">2024.eacl-long.80.software.zip</attachment>
      <bibkey>moorjani-etal-2024-cev</bibkey>
    </paper>
    <paper id="81">
      <title>It’s All Relative: Learning Interpretable Models for Scoring Subjective Bias in Documents from Pairwise Comparisons</title>
      <author><first>Aswin</first><last>Suresh</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Wu</first><last>Hsuan</last></author>
      <author><first>Matthias</first><last>Grossglauser</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <pages>1341-1353</pages>
      <abstract>We propose an interpretable model to score the subjective bias present in documents, based only on their textual content. Our model is trained on pairs of revisions of the same Wikipedia article, where one version is more biased than the other. Although prior approaches based on bias classification have struggled to obtain a high accuracy for the task, we are able to develop a useful model for scoring bias by learning to accurately perform pairwise comparisons. We show that we can interpret the parameters of the trained model to discover the words most indicative of bias. We also apply our model in three different settings by studying the temporal evolution of bias in Wikipedia articles, comparing news sources based on bias, and scoring bias in law amendments. In each case, we demonstrate that the outputs of the model can be explained and validated, even for the two domains that are outside the training-data domain. We also use the model to compare the general level of bias between domains, where we see that legal texts are the least biased and news media are the most biased, with Wikipedia articles in between.</abstract>
      <url hash="d2d5debb">2024.eacl-long.81</url>
      <bibkey>suresh-etal-2024-relative</bibkey>
    </paper>
    <paper id="82">
      <title><fixed-case>H</fixed-case>i<fixed-case>G</fixed-case>en: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification</title>
      <author><first>Vidit</first><last>Jain</last></author>
      <author><first>Mukund</first><last>Rungta</last></author>
      <author><first>Yuchen</first><last>Zhuang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yue</first><last>Yu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Zeyu</first><last>Wang</last><affiliation>Yale University</affiliation></author>
      <author><first>Mu</first><last>Gao</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Jeffrey</first><last>Skolnick</last></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>1354-1368</pages>
      <abstract>Hierarchical text classification (HTC) is a complex subtask under multi-label text classification, characterized by a hierarchical label taxonomy and data imbalance. The best-performing models aim to learn a static representation by combining document and hierarchical label information. However, the relevance of document sections can vary based on the hierarchy level, necessitating a dynamic document representation. To address this, we propose HiGen, a text-generation-based framework utilizing language models to encode dynamic text representations. We introduce a level-guided loss function to capture the relationship between text and label name semantics. Our approach incorporates a task-specific pretraining strategy, adapting the language model to in-domain knowledge and significantly enhancing performance for classes with limited examples. Furthermore, we present a new and valuable dataset called ENZYME, designed for HTC, which comprises articles from PubMed with the goal of predicting Enzyme Commission (EC) numbers. Through extensive experiments on the ENZYME dataset and the widely recognized WOS and NYT datasets, our methodology demonstrates superior performance, surpassing existing approaches while efficiently handling data and mitigating class imbalance. We release our code and dataset here: https://github.com/viditjain99/HiGen.</abstract>
      <url hash="f8e14848">2024.eacl-long.82</url>
      <attachment type="software" hash="0ff1d16e">2024.eacl-long.82.software.zip</attachment>
      <attachment type="note" hash="0ff1d16e">2024.eacl-long.82.note.zip</attachment>
      <bibkey>jain-etal-2024-higen</bibkey>
    </paper>
    <paper id="83">
      <title>M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Jonibek</first><last>Mansurov</last></author>
      <author><first>Petar</first><last>Ivanov</last></author>
      <author><first>Jinyan</first><last>Su</last><affiliation>Cornell University</affiliation></author>
      <author><first>Artem</first><last>Shelmanov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Akim</first><last>Tsvigun</last><affiliation>Semrush</affiliation></author>
      <author><first>Chenxi</first><last>Whitehouse</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Osama</first><last>Mohammed Afzal</last></author>
      <author><first>Tarek</first><last>Mahmoud</last></author>
      <author><first>Toru</first><last>Sasaki</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Thomas</first><last>Arnold</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Alham</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Amazon</affiliation></author>
      <author><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1369-1407</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries. However, this has also raised concerns about the potential misuse of such texts in journalism, education, and academia. In this study, we strive to create automated systems that can detect machine-generated texts and pinpoint potential misuse. We first introduce a large-scale benchmark M4, which is a multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written. These results show that the problem is far from solved and that there is a lot of room for improvement. We believe that our dataset will enable future research towards more robust approaches to this pressing societal problem. The dataset is available at https://github.com/mbzuai-nlp/M4</abstract>
      <url hash="3ba47fcc">2024.eacl-long.83</url>
      <bibkey>wang-etal-2024-m4</bibkey>
    </paper>
    <paper id="84">
      <title>A Truly Joint Neural Architecture for Segmentation and Parsing</title>
      <author><first>Danit</first><last>Yshaayahu Levi</last></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Bar-Ilan University, Technion</affiliation></author>
      <pages>1408-1420</pages>
      <abstract>Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages. The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance. Pre-neural dependency parsers for MRLs subscribed to the <i>joint morpho-syntactic hypothesis</i>, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing. However, neural state-of-the-art parsers to date use a strict pipeline. In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once. Our experiments on Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art performance on parsing, tagging and segmentation of the Hebrew section of UD, using a single model. This proposed architecture is LLM-based and language agnostic, providing a solid foundation for MRLs to obtain further performance improvements and bridge the gap with other languages.</abstract>
      <url hash="ecd3388e">2024.eacl-long.84</url>
      <bibkey>yshaayahu-levi-tsarfaty-2024-truly</bibkey>
    </paper>
    <paper id="85">
      <title><fixed-case>V</fixed-case>i<fixed-case>L</fixed-case>ex<fixed-case>N</fixed-case>orm: A Lexical Normalization Corpus for <fixed-case>V</fixed-case>ietnamese Social Media Text</title>
      <author><first>Thanh-Nhi</first><last>Nguyen</last><affiliation>University of Information Technology, VNU-HCM</affiliation></author>
      <author><first>Thanh-Phong</first><last>Le</last></author>
      <author><first>Kiet</first><last>Nguyen</last><affiliation>University of Information Technology, VNU-HCM</affiliation></author>
      <pages>1421-1437</pages>
      <abstract>Lexical normalization, a fundamental task in Natural Language Processing (NLP), involves the transformation of words into their canonical forms. This process has been proven to benefit various downstream NLP tasks greatly. In this work, we introduce Vietnamese Lexical Normalization (ViLexNorm), the first-ever corpus developed for the Vietnamese lexical normalization task. The corpus comprises over 10,000 pairs of sentences meticulously annotated by human annotators, sourced from public comments on Vietnam’s most popular social media platforms. Various methods were used to evaluate our corpus, and the best-performing system achieved a result of 57.74% using the Error Reduction Rate (ERR) metric (van der Goot, 2019a) with the Leave-As-Is (LAI) baseline. For extrinsic evaluation, employing the model trained on ViLexNorm demonstrates the positive impact of the Vietnamese lexical normalization task on other NLP tasks. Our corpus is publicly available exclusively for research purposes.</abstract>
      <url hash="2d2a03bf">2024.eacl-long.85</url>
      <attachment type="note" hash="38acac6f">2024.eacl-long.85.note.zip</attachment>
      <bibkey>nguyen-etal-2024-vilexnorm</bibkey>
    </paper>
    <paper id="86">
      <title>Diffusion-<fixed-case>NAT</fixed-case>: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation</title>
      <author><first>Kun</first><last>Zhou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yifan</first><last>Li</last></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>1438-1451</pages>
      <abstract>Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models (PLMs).To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance.By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task.In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich pre-learned knowledge of BART and the iterative refining paradigm of DDM.Besides, we also propose the iterative self-prompting strategy to further improve the generation quality.Experimental results on 7 datasets show that our approach can outperform competitive NAR methods, and even surpass autoregressive methods.Our code and data are released at <url>https://github.com/RUCAIBox/DiffusionNAT</url>.</abstract>
      <url hash="3e9bbf92">2024.eacl-long.86</url>
      <bibkey>zhou-etal-2024-diffusion</bibkey>
    </paper>
    <paper id="87">
      <title>Unleashing the Power of Discourse-Enhanced Transformers for Propaganda Detection</title>
      <author><first>Alexander</first><last>Chernyavskiy</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last><affiliation>Higher School of Economics</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1452-1462</pages>
      <abstract>The prevalence of information manipulation online has created a need for propaganda detection systems. Such systems have typically focused on the surface words, ignoring the linguistic structure. Here we aim to bridge this gap. In particular, we present the first attempt at using discourse analysis for the task. We consider both paragraph-level and token-level classification and we propose a discourse-aware Transformer architecture. Our experiments on English and Russian demonstrate sizeable performance gains compared to a number of baselines. Moreover, our ablation study emphasizes the importance of specific types of discourse features, and our in-depth analysis reveals a strong correlation between propaganda instances and discourse spans.</abstract>
      <url hash="33e6bec7">2024.eacl-long.87</url>
      <attachment type="software" hash="ade150f7">2024.eacl-long.87.software.zip</attachment>
      <bibkey>chernyavskiy-etal-2024-unleashing</bibkey>
    </paper>
    <paper id="88">
      <title>Predicting Client Emotions and Therapist Interventions in Psychotherapy Dialogues</title>
      <author><first>Tobias</first><last>Mayer</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Neha</first><last>Warikoo</last></author>
      <author><first>Amir</first><last>Eliassaf</last></author>
      <author><first>Dana</first><last>Atzil-Slonim</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>1463-1477</pages>
      <abstract>Natural Language Processing (NLP) can advance psychotherapy research by scaling up therapy dialogue analysis as well as by allowing researchers to examine client-therapist interactions in detail. Previous studies have mainly either explored the clients’ behavior or the therapists’ intervention in dialogues. Yet, modelling conversations from both dialogue participants is crucial to understanding the therapeutic interaction. This study explores speaker contribution-based dialogue acts at the utterance-level; i.e, the therapist - Intervention Prediction (IP) and the client - Emotion Recognition (ER) in psychotherapy using a pan-theoretical schema. We perform experiments with fine-tuned language models and light-weight adapter solutions on a Hebrew dataset. We deploy the results from our ER model predictions in investigating the coherence between client self-reports on emotion and the utterance-level emotions. Our best adapters achieved on-par performance with fully fine-tuned models, at 0.64 and 0.66 micro F1 for IP and ER, respectively. In addition, our analysis identifies ambiguities within categorical clinical coding, which can be used to fine-tune the coding schema. Finally, our results indicate a positive correlation between client self-reports and utterance-level emotions.</abstract>
      <url hash="28601b97">2024.eacl-long.88</url>
      <bibkey>mayer-etal-2024-predicting</bibkey>
    </paper>
    <paper id="89">
      <title>Who Needs Decoders? Efficient Estimation of Sequence-Level Attributes with Proxies</title>
      <author><first>Yassir</first><last>Fathullah</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Puria</first><last>Radmard</last></author>
      <author><first>Adian</first><last>Liusie</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Mark</first><last>Gales</last><affiliation>University of Cambridge</affiliation></author>
      <pages>1478-1496</pages>
      <abstract>Sequence-to-sequence models often require an expensive autoregressive decoding process. However, for some downstream tasks such as out-of-distribution (OOD) detection and resource allocation, the actual decoding output is not needed, just a scalar attribute of this sequence. In such scenarios, where knowing the quality of a system’s output to predict poor performance prevails over knowing the output itself, is it possible to bypass the autoregressive decoding? We propose Non-Autoregressive Proxy (NAP) models that can efficiently predict scalar-valued sequence-level attributes. Importantly, NAPs predict these metrics directly from the encodings, avoiding the expensive decoding stage. We consider two sequence tasks: Machine Translation (MT) and Automatic Speech Recognition (ASR). In OOD for MT, NAPs outperform ensembles while being significantly faster. NAPs are also proven capable of predicting metrics such as BERTScore (MT) or word error rate (ASR). For downstream tasks, such as data filtering and resource optimization, NAPs generate performance predictions that outperform predictive uncertainty while being highly inference efficient.</abstract>
      <url hash="dd85ade2">2024.eacl-long.89</url>
      <bibkey>fathullah-etal-2024-needs</bibkey>
    </paper>
    <paper id="90">
      <title>3<fixed-case>D</fixed-case> Rotation and Translation for Hyperbolic Knowledge Graph Embedding</title>
      <author><first>Yihua</first><last>Zhu</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Hidetoshi</first><last>Shimodaira</last><affiliation>Kyoto University and RIKEN</affiliation></author>
      <pages>1497-1515</pages>
      <abstract>The main objective of Knowledge Graph (KG) embeddings is to learn low-dimensional representations of entities and relations, enabling the prediction of missing facts. A significant challenge in achieving better KG embeddings lies in capturing relation patterns, including symmetry, antisymmetry, inversion, commutative composition, non-commutative composition, hierarchy, and multiplicity. This study introduces a novel model called 3H-TH (3D Rotation and Translation in Hyperbolic space) that captures these relation patterns simultaneously. In contrast, previous attempts have not achieved satisfactory performance across all the mentioned properties at the same time. The experimental results demonstrate that the new model outperforms existing state-of-the-art models in terms of accuracy, hierarchy property, and other relation patterns in low-dimensional space, meanwhile performing similarly in high-dimensional space.</abstract>
      <url hash="bb0a8eb9">2024.eacl-long.90</url>
      <attachment type="software" hash="ec58690a">2024.eacl-long.90.software.zip</attachment>
      <bibkey>zhu-shimodaira-2024-3d</bibkey>
    </paper>
    <paper id="91">
      <title>Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for <fixed-case>C</fixed-case>hinese Geographic Re-Ranking</title>
      <author><first>Yong</first><last>Cao</last></author>
      <author><first>Ruixue</first><last>Ding</last></author>
      <author><first>Boli</first><last>Chen</last><affiliation>Alibaba DAMO Academy</affiliation></author>
      <author><first>Xianzhi</first><last>Li</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Min</first><last>Chen</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Daniel</first><last>Hershcovich</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <pages>1516-1530</pages>
      <abstract>Chinese geographic re-ranking task aims to find the most relevant addresses among retrieved candidates, which is crucial for location-related services such as navigation maps. Unlike the general sentences, Chinese geographic contexts are closely intertwined with geographical concepts, from general spans (e.g., province) to specific spans (e.g., road). Given this feature, we propose an innovative framework, namely Geo-Encoder, to more effectively integrate Chinese geographical semantics into re-ranking pipelines. Our methodology begins by employing off-the-shelf tools to associate text with geographical spans, treating them as chunking units. Then, we present a multi-task learning module to simultaneously acquire an effective attention matrix that determines chunk contributions to geographic representations. Furthermore, we put forth an asynchronous update mechanism for the proposed task, aiming to guide the model to focus on specific chunks. Experiments on two Chinese benchmark datasets, show that the Geo-Encoder achieves significant improvements when compared to state-of-the-art baselines. Notably, it leads to a substantial improvement in the Hit@1 score of MGEO-BERT, increasing it by 6.22% from 62.76 to 68.98 on the GeoTES dataset.</abstract>
      <url hash="7c138386">2024.eacl-long.91</url>
      <bibkey>cao-etal-2024-geo</bibkey>
    </paper>
    <paper id="92">
      <title>Style-News: Incorporating Stylized News Generation and Adversarial Verification for Neural Fake News Detection</title>
      <author><first>Wei-Yao</first><last>Wang</last></author>
      <author><first>Yu-Chieh</first><last>Chang</last></author>
      <author><first>Wen-Chih</first><last>Peng</last><affiliation>National Ying Ming Chiao Tung University</affiliation></author>
      <pages>1531-1541</pages>
      <abstract>With the improvements in generative models, the issues of producing hallucinations in various domains (e.g., law, writing) have been brought to people’s attention due to concerns about misinformation. In this paper, we focus on neural fake news, which refers to content generated by neural networks aiming to mimic the style of real news to deceive people. To prevent harmful disinformation spreading fallaciously from malicious social media (e.g., content farms), we propose a novel verification framework, Style-News, using publisher metadata to imply a publisher’s template with the corresponding text types, political stance, and credibility. Based on threat modeling aspects, a style-aware neural news generator is introduced as an adversary for generating news content conditioning for a specific publisher, and style and source discriminators are trained to defend against this attack by identifying which publisher the style corresponds with, and discriminating whether the source of the given news is human-written or machine-generated. To evaluate the quality of the generated content, we integrate various dimensional metrics (language fluency, content preservation, and style adherence) and demonstrate that Style-News significantly outperforms the previous approaches by a margin of 0.35 for fluency, 15.24 for content, and 0.38 for style at most. Moreover, our discriminative model outperforms state-of-the-art baselines in terms of publisher prediction (up to 4.64%) and neural fake news detection (+6.94% 31.72%). We plan to release our Style-News publicly, with the aim of improving neural fake news detection.</abstract>
      <url hash="a246cb93">2024.eacl-long.92</url>
      <attachment type="software" hash="a8c317da">2024.eacl-long.92.software.zip</attachment>
      <bibkey>wang-etal-2024-style</bibkey>
    </paper>
    <paper id="93">
      <title>Graph-based Clustering for Detecting Semantic Change Across Time and Languages</title>
      <author><first>Xianghe</first><last>Ma</last></author>
      <author><first>Michael</first><last>Strube</last><affiliation>Heidelberg Institute for Theoretical Studies</affiliation></author>
      <author><first>Wei</first><last>Zhao</last><affiliation>Heidelberg Institute for Theoretical Studies</affiliation></author>
      <pages>1542-1561</pages>
      <abstract>Despite the predominance of contextualized embeddings in NLP, approaches to detect semantic change relying on these embeddings and clustering methods underperform simpler counterparts based on static word embeddings. This stems from the poor quality of the clustering methods to produce sense clusters—which struggle to capture word senses, especially those with low frequency. This issue hinders the next step in examining how changes in word senses in one language influence another. To address this issue, we propose a graph-based clustering approach to capture nuanced changes in both high- and low-frequency word senses across time and languages, including the acquisition and loss of these senses over time. Our experimental results show that our approach substantially surpasses previous approaches in the SemEval2020 binary classification task across four languages. Moreover, we showcase the ability of our approach as a versatile visualization tool to detect semantic changes in both intra-language and inter-language setups. We make our code and data publicly available.</abstract>
      <url hash="af7a6847">2024.eacl-long.93</url>
      <bibkey>ma-etal-2024-graph</bibkey>
    </paper>
    <paper id="94">
      <title>Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models</title>
      <author><first>Haoqiang</first><last>Kang</last></author>
      <author><first>Terra</first><last>Blevins</last><affiliation>University of Washington</affiliation></author>
      <author><first>Luke</first><last>Zettlemoyer</last><affiliation>University of Washington, Facebook and Meta</affiliation></author>
      <pages>1562-1575</pages>
      <abstract>Pretrained Language Models (PLMs) learn rich cross-lingual knowledge and perform well on diverse tasks such as translation and multilingual word sense disambiguation (WSD) when finetuned. However, they often struggle at disambiguating word sense in a zero-shot setting. To better understand this contrast, we present a new study investigating how well PLMs capture cross-lingual word sense with Contextual Word-Level Translation (C-WLT), an extension of word-level translation that prompts the model to translate a given word in context. We find that as the model size increases, PLMs encode more cross-lingual word sense knowledge and better use context to improve WLT performance. Building on C-WLT, we introduce a zero-shot prompting approach for WSD, tested on 18 languages from the XL-WSD dataset. Our method outperforms fully supervised baselines on recall for many evaluation languages without additional training or finetuning. This study presents a first step towards understanding how to best leverage the cross-lingual knowledge inside PLMs for robust zero-shot reasoning in any language.</abstract>
      <url hash="990aca57">2024.eacl-long.94</url>
      <bibkey>kang-etal-2024-translate</bibkey>
    </paper>
    <paper id="95">
      <title>Anchor Points: Benchmarking Models with Much Fewer Examples</title>
      <author><first>Rajan</first><last>Vivek</last></author>
      <author><first>Kawin</first><last>Ethayarajh</last><affiliation>Stanford University</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Douwe</first><last>Kiela</last><affiliation>Stanford University</affiliation></author>
      <pages>1576-1601</pages>
      <abstract>Modern language models often exhibit powerful but brittle behavior, leading to the development of larger and more diverse benchmarks to reliably assess their behavior. Here, we suggest that model performance can be benchmarked and elucidated with much smaller evaluation sets. We first show that in six popular language classification benchmarks, model confidence in the correct class on many pairs of points is strongly correlated across models. We build upon this phenomenon to propose Anchor Point Selection, a technique to select small subsets of datasets that capture model behavior across the entire dataset. Anchor points reliably rank models: across 87 diverse language model-prompt pairs, evaluating models using 1-30 anchor points outperforms uniform sampling and other baselines at accurately ranking models. Moreover, just a dozen anchor points can be used to estimate model per-class predictions on all other points in a dataset with low error, sufficient for gauging where the model is likely to fail. Lastly, we present Anchor Point Maps for visualizing these insights and facilitating comparisons of the performance of different models on various regions within the dataset distribution.</abstract>
      <url hash="7a4ad800">2024.eacl-long.95</url>
      <attachment type="software" hash="f5fc7bb4">2024.eacl-long.95.software.zip</attachment>
      <attachment type="note" hash="041c0990">2024.eacl-long.95.note.zip</attachment>
      <bibkey>vivek-etal-2024-anchor</bibkey>
    </paper>
    <paper id="96">
      <title><fixed-case>SCO</fixed-case>-<fixed-case>VIST</fixed-case>: Social Interaction Commonsense Knowledge-based Visual Storytelling</title>
      <author><first>Eileen</first><last>Wang</last></author>
      <author><first>Caren</first><last>Han</last><affiliation>University of Western Australia and University of Sydney</affiliation></author>
      <author><first>Josiah</first><last>Poon</last><affiliation>University of Sydney</affiliation></author>
      <pages>1602-1616</pages>
      <abstract>Visual storytelling aims to automatically generate a coherent story based on a given image sequence. Unlike tasks like image captioning, visual stories should contain factual descriptions, worldviews, and human social commonsense to put disjointed elements together to form a coherent and engaging human-writeable story. However, most models mainly focus on applying factual information and using taxonomic/lexical external knowledge when attempting to create stories. This paper introduces SCO-VIST, a framework representing the image sequence as a graph with objects and relations that includes human action motivation and its social interaction commonsense knowledge. SCO-VIST then takes this graph representing plot points and creates bridges between plot points with semantic and occurrence-based edge weights. This weighted story graph produces the storyline in a sequence of events using Floyd-Warshall’s algorithm. Our proposed framework produces stories superior across multiple metrics in terms of visual grounding, coherence, diversity, and humanness, per both automatic and human evaluations.</abstract>
      <url hash="7c775d81">2024.eacl-long.96</url>
      <bibkey>wang-etal-2024-sco</bibkey>
    </paper>
    <paper id="97">
      <title>Discovering and Articulating Frames of Communication from Social Media Using Chain-of-Thought Reasoning</title>
      <author><first>Maxwell</first><last>Weinzierl</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Sanda</first><last>Harabagiu</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>1617-1631</pages>
      <abstract>Frames of Communication (FoCs) are ubiquitous in social media discourse. They define what counts as a problem, diagnose what is causing the problem, elicit moral judgments and imply remedies for resolving the problem. Most research on automatic frame detection involved the recognition of the problems addressed by frames, but did not consider the articulation of frames. Articulating an FoC involves reasoning with salient problems, their cause and eventual solution. In this paper we present a method for Discovering and Articulating FoCs (DA-FoC) that relies on a combination of Chain-of-Thought prompting of large language models (LLMs) with In-Context Active Curriculum Learning. Very promising evaluation results indicate that 86.72% of the FoCs encoded by communication experts on the same reference dataset were also uncovered by DA-FoC. Moreover, DA-FoC uncovered many new FoCs, which escaped the experts. Interestingly, 55.1% of the known FoCs were judged as being better articulated than the human-written ones, while 93.8% of the new FoCs were judged as having sound rationale and being clearly articulated.</abstract>
      <url hash="ecd1ffc0">2024.eacl-long.97</url>
      <bibkey>weinzierl-harabagiu-2024-discovering</bibkey>
    </paper>
    <paper id="98">
      <title><fixed-case>VEIL</fixed-case>: Vetting Extracted Image Labels from In-the-Wild Captions for Weakly-Supervised Object Detection</title>
      <author><first>Arushi</first><last>Rai</last></author>
      <author><first>Adriana</first><last>Kovashka</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>1632-1649</pages>
      <abstract>The use of large-scale vision-language datasets is limited for object detection due to the negative impact of label noise on localization. Prior methods have shown how such large-scale datasets can be used for pretraining, which can provide initial signal for localization, but is insufficient without clean bounding-box data for at least some categories. We propose a technique to “vet” labels extracted from noisy captions, and use them for weakly-supervised object detection (WSOD), without any bounding boxes. We analyze and annotate the types of label noise in captions in our Caption Label Noise dataset, and train a classifier that predicts if an extracted label is actually present in the image or not. Our classifier generalizes across dataset boundaries and across categories. We compare the classifier to nine baselines on five datasets, and demonstrate that it can improve WSOD without label vetting by 30% (31.2 to 40.5 mAP when evaluated on PASCAL VOC). See dataset at: https://github.com/arushirai1/CLaNDataset.</abstract>
      <url hash="4432b0fc">2024.eacl-long.98</url>
      <attachment type="software" hash="51fccf86">2024.eacl-long.98.software.zip</attachment>
      <attachment type="note" hash="e92a37a3">2024.eacl-long.98.note.zip</attachment>
      <bibkey>rai-kovashka-2024-veil</bibkey>
    </paper>
    <paper id="99">
      <title><fixed-case>WSC</fixed-case>+: Enhancing The <fixed-case>W</fixed-case>inograd Schema Challenge Using Tree-of-Experts</title>
      <author><first>Pardis</first><last>Zahraei</last></author>
      <author><first>Ali</first><last>Emami</last><affiliation>Brock University</affiliation></author>
      <pages>1650-1671</pages>
      <abstract>The Winograd Schema Challenge (WSC) serves as a prominent benchmark for evaluating machine understanding. While Large Language Models (LLMs) excel at answering WSC questions, their ability to generate such questions remains less explored. In this work, we propose Tree-of-Experts (ToE), a novel prompting method which enhances the generation of WSC instances (50% valid cases vs. 10% in recent methods). Using this approach, we introduce WSC+, a novel dataset comprising 3,026 LLM-generated sentences. Notably, we extend the WSC framework by incorporating new ‘ambiguous’ and ‘offensive’ categories, providing a deeper insight into model overconfidence and bias. Our analysis reveals nuances in generation-evaluation consistency, suggesting that LLMs may not always outperform in evaluating their own generated questions when compared to those crafted by other models. On WSC+, GPT-4, the top-performing LLM, achieves an accuracy of 68.7%, significantly below the human benchmark of 95.1%.</abstract>
      <url hash="a39127b0">2024.eacl-long.99</url>
      <bibkey>zahraei-emami-2024-wsc</bibkey>
    </paper>
    <paper id="100">
      <title>Kardeş-<fixed-case>NLU</fixed-case>: Transfer to Low-Resource Languages with the Help of a High-Resource Cousin – A Benchmark and Evaluation for <fixed-case>T</fixed-case>urkic Languages</title>
      <author><first>Lütfi Kerem</first><last>Senel</last><affiliation>Ludwig Maximilian University of Munich and The Center for Information and Language Processing</affiliation></author>
      <author><first>Benedikt</first><last>Ebing</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Konul</first><last>Baghirova</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <pages>1672-1688</pages>
      <abstract>Cross-lingual transfer (XLT) driven by massively multilingual language models (mmLMs) has been shown largely ineffective for low-resource (LR) target languages with little (or no) representation in mmLM’s pretraining, especially if they are linguistically distant from the high-resource (HR) source language. Much of the recent focus in XLT research has been dedicated to <i>LR language families</i>, i.e., families without any HR languages (e.g., families of African languages or indigenous languages of the Americas). In this work, in contrast, we investigate a configuration that is arguably of practical relevance for more of the world’s languages: XLT to LR languages that do have a close HR relative. To explore the extent to which a HR language can facilitate transfer to its LR relatives, we (1) introduce Kardeş-NLU, an evaluation benchmark with language understanding datasets in five LR Turkic languages: Azerbaijani, Kazakh, Kyrgyz, Uzbek, and Uyghur; and (2) investigate (a) intermediate training and (b) fine-tuning strategies that leverage Turkish in XLT to these target languages. Our experimental results show that both - integrating Turkish in intermediate training and in downstream fine-tuning - yield substantial improvements in XLT to LR Turkic languages. Finally, we benchmark cutting-edge instruction-tuned large language models on Kardeş-NLU, showing that their performance is highly task- and language-dependent.</abstract>
      <url hash="9e3df1c2">2024.eacl-long.100</url>
      <attachment type="software" hash="34f32387">2024.eacl-long.100.software.zip</attachment>
      <bibkey>senel-etal-2024-kardes</bibkey>
      <revision id="1" href="2024.eacl-long.100v1" hash="e31fcfaf"/>
      <revision id="2" href="2024.eacl-long.100v2" hash="9e3df1c2" date="2024-03-25">The revision changes the title due to ethical considerations.</revision>
    </paper>
    <paper id="101">
      <title>Chaining Event Spans for Temporal Relation Grounding</title>
      <author><first>Jongho</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dohyeon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Minsoo</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>1689-1700</pages>
      <abstract>Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: “What finished right before the decision?” or “What finished right after the decision?”. To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC, and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline.</abstract>
      <url hash="c1779110">2024.eacl-long.101</url>
      <bibkey>kim-etal-2024-chaining</bibkey>
    </paper>
    <paper id="102">
      <title>Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks</title>
      <author><first>Huajian</first><last>Zhang</last></author>
      <author><first>Yumo</first><last>Xu</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Laura</first><last>Perez-Beltrachini</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>1701-1722</pages>
      <abstract>We study existing approaches to leverage off-the-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses. That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences. We propose a novel approach, namely INFUSE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses. Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks. We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation). In experiments, INFUSE obtains superior performance across the different summarisation tasks.</abstract>
      <url hash="0ae0f63d">2024.eacl-long.102</url>
      <bibkey>zhang-etal-2024-fine</bibkey>
    </paper>
    <paper id="103">
      <title><fixed-case>A</fixed-case>na<fixed-case>DE</fixed-case>1.0: A Novel Data Set for Benchmarking Analogy Detection and Extraction</title>
      <author><first>Bhavya</first><last>Bhavya</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Shradha</first><last>Sehgal</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Jinjun</first><last>Xiong</last><affiliation>State University of New York at Buffalo</affiliation></author>
      <author><first>ChengXiang</first><last>Zhai</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>1723-1737</pages>
      <abstract>Textual analogies that make comparisons between two concepts are often used for explaining complex ideas, creative writing, and scientific discovery. In this paper, we propose and study a new task, called Analogy Detection and Extraction (AnaDE), which includes three synergistic sub-tasks: 1) detecting documents containing analogies, 2) extracting text segments that make up the analogy, and 3) identifying the (source and target) concepts being compared. To facilitate the study of this new task, we create a benchmark dataset by scraping Metamia.com and investigate the performances of state-of-the-art models on all sub-tasks to establish the first-generation benchmark results for this new task. We find that the Longformer model achieves the best performance on all the three sub-tasks demonstrating its effectiveness for handling long texts. Moreover, smaller models fine-tuned on our dataset perform better than non-finetuned ChatGPT, suggesting high task difficulty. Overall, the models achieve a high performance on documents detection suggesting that it could be used to develop applications like analogy search engines. Further, there is a large room for improvement on the segment and concept extraction tasks.</abstract>
      <url hash="443d0109">2024.eacl-long.103</url>
      <bibkey>bhavya-etal-2024-anade1</bibkey>
    </paper>
    <paper id="104">
      <title>A Comprehensive Survey of Sentence Representations: From the <fixed-case>BERT</fixed-case> Epoch to the <fixed-case>CHATGPT</fixed-case> Era and Beyond</title>
      <author><first>Abhinav</first><last>Ramesh Kashyap</last></author>
      <author><first>Thanh-Tung</first><last>Nguyen</last></author>
      <author><first>Viktor</first><last>Schlegel</last><affiliation>ASUS Intelligent Cloud Solutions</affiliation></author>
      <author><first>Stefan</first><last>Winkler</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>1738-1751</pages>
      <abstract>Sentence representations are a critical component in NLP applications such as retrieval, question answering, and text classification. They capture the meaning of a sentence, enabling machines to understand and reason over human language. In recent years, significant progress has been made in developing methods for learning sentence representations, including unsupervised, supervised, and transfer learning approaches. However there is no literature review on sentence representations till now. In this paper, we provide an overview of the different methods for sentence representation learning, focusing mostly on deep learning models. We provide a systematic organization of the literature, highlighting the key contributions and challenges in this area. Overall, our review highlights the importance of this area in natural language processing, the progress made in sentence representation learning, and the challenges that remain. We conclude with directions for future research, suggesting potential avenues for improving the quality and efficiency of sentence representations.</abstract>
      <url hash="068fb55a">2024.eacl-long.104</url>
      <bibkey>ramesh-kashyap-etal-2024-comprehensive</bibkey>
    </paper>
    <paper id="105">
      <title>Learning to Retrieve In-Context Examples for Large Language Models</title>
      <author><first>Liang</first><last>Wang</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Nan</first><last>Yang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>1752-1767</pages>
      <abstract>Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.</abstract>
      <url hash="7a106ffb">2024.eacl-long.105</url>
      <attachment type="software" hash="67b4466e">2024.eacl-long.105.software.zip</attachment>
      <attachment type="note" hash="426bcdc8">2024.eacl-long.105.note.zip</attachment>
      <bibkey>wang-etal-2024-learning</bibkey>
    </paper>
    <paper id="106">
      <title><fixed-case>E</fixed-case>n<fixed-case>C</fixed-case>ore: Fine-Grained Entity Typing by Pre-Training Entity Encoders on Coreference Chains</title>
      <author><first>Frank</first><last>Mtumbuka</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Steven</first><last>Schockaert</last><affiliation>Cardiff University</affiliation></author>
      <pages>1768-1781</pages>
      <abstract>Entity typing is the task of assigning semantic types to the entities that are mentioned in a text. In the case of fine-grained entity typing (FET), a large set of candidate type labels is considered. Since obtaining sufficient amounts of manual annotations is then prohibitively expensive, FET models are typically trained using distant supervision. In this paper, we propose to improve on this process by pre-training an entity encoder such that embeddings of coreferring entities are more similar to each other than to the embeddings of other entities. The main problem with this strategy, which helps to explain why it has not previously been considered, is that predicted coreference links are often too noisy. We show that this problem can be addressed by using a simple trick: we only consider coreference links that are predicted by two different off-the-shelf systems. With this prudent use of coreference links, our pre-training strategy allows us to improve the state-of-the-art in benchmarks on fine-grained entity typing, as well as traditional entity extraction.</abstract>
      <url hash="fe158caa">2024.eacl-long.106</url>
      <bibkey>mtumbuka-schockaert-2024-encore</bibkey>
    </paper>
    <paper id="107">
      <title>Unsupervised stance detection for social media discussions: A generic baseline</title>
      <author><first>Maia</first><last>Sutter</last></author>
      <author><first>Antoine</first><last>Gourru</last><affiliation>Université Jean Monnet</affiliation></author>
      <author><first>Amine</first><last>Trabelsi</last><affiliation>Université de Sherbrooke</affiliation></author>
      <author><first>Christine</first><last>Largeron</last><affiliation>Université Jean Monnet</affiliation></author>
      <pages>1782-1792</pages>
      <abstract>With the ever-growing use of social media to express opinions on the national and international stage, unsupervised methods of stance detection are increasingly important to handle the task without costly annotation of data. The current unsupervised state-of-the-art models are designed for specific network types, either homophilic or heterophilic, and they fail to generalize to both. In this paper, we first analyze the generalization ability of recent baselines to these two very different network types. Then, we conduct extensive experiments with a baseline model based on text embeddings propagated with a graph neural network that generalizes well to heterophilic and homophilic networks. We show that it outperforms, on average, other state-of-the-art methods across the two network types. Additionally, we show that combining textual and network information outperforms using text only, and that the language model size has only a limited impact on the model performance.</abstract>
      <url hash="ad02d8f1">2024.eacl-long.107</url>
      <bibkey>sutter-etal-2024-unsupervised</bibkey>
    </paper>
    <paper id="108">
      <title>Putting Context in Context: the Impact of Discussion Structure on Text Classification</title>
      <author><first>Nicolò</first><last>Penzo</last></author>
      <author><first>Antonio</first><last>Longa</last></author>
      <author><first>Bruno</first><last>Lepri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <author><first>Marco</first><last>Guerini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>1793-1811</pages>
      <abstract>Current text classification approaches usually focus on the content to be classified. Contextual aspects (both linguistic and extra-linguistic) are usually neglected, even in tasks based on online discussions. Still in many cases the multi-party and multi-turn nature of the context from which these elements are selected can be fruitfully exploited. In this work, we propose a series of experiments on a large dataset for stance detection in English, in which we evaluate the contribution of different types of contextual information, i.e. linguistic, structural and temporal, by feeding them as natural language input into a transformer-based model. We also experiment with different amounts of training data and analyse the topology of local discussion networks in a privacy-compliant way. Results show that structural information can be highly beneficial to text classification but only under certain circumstances (e.g. depending on the amount of training data and on discussion chain complexity). Indeed, we show that contextual information on smaller datasets from other classification tasks does not yield significant improvements. Our framework, based on local discussion networks, allows the integration of structural information while minimising user profiling, thus preserving their privacy.</abstract>
      <url hash="eb4686ff">2024.eacl-long.108</url>
      <attachment type="software" hash="bd93ff36">2024.eacl-long.108.software.zip</attachment>
      <attachment type="note" hash="b013579d">2024.eacl-long.108.note.zip</attachment>
      <bibkey>penzo-etal-2024-putting</bibkey>
    </paper>
    <paper id="109">
      <title>Aligning Large and Small Language Models via Chain-of-Thought Reasoning</title>
      <author><first>Leonardo</first><last>Ranaldi</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>University of Manchester</affiliation></author>
      <pages>1812-1827</pages>
      <abstract>Chain-of-Thought (CoT) prompting empowersthe reasoning abilities of Large Language Models (LLMs), eliciting them to solve complexreasoning tasks in a step-wise manner. However, these capabilities appear only in models with billions of parameters, which represent an entry barrier for many users who are constrained to operate on a smaller model scale, i.e., Small Language Models (SLMs). Although many companies are releasing LLMs of the same family with fewer parameters, these models tend not to preserve all the reasoning capabilities of the original models, including CoT reasoning.In this paper, we propose a method for aligning and transferring reasoning abilities between larger to smaller Language Models. By using an Instruction-tuning-CoT method, that is, an Instruction-tuning designed around CoT-Demonstrations, we enable the SLMs to generate multi-step controlled reasoned answers when they are elicited with the CoT mechanism. Hence, we instruct a smaller Language Model using outputs generated by more robust models belonging to the same family or not, evaluating the impact across different types of models. Results obtained on question-answering and mathematical reasoning benchmarks show that LMs instructed via the Instruction-tuning CoT method produced by LLMs outperform baselines within both in-domain and out-domain scenarios.</abstract>
      <url hash="b286bc06">2024.eacl-long.109</url>
      <attachment type="software" hash="78e07a63">2024.eacl-long.109.software.zip</attachment>
      <bibkey>ranaldi-freitas-2024-aligning</bibkey>
    </paper>
    <paper id="110">
      <title>Disentangling the Roles of Target-side Transfer and Regularization in Multilingual Machine Translation</title>
      <author><first>Yan</first><last>Meng</last></author>
      <author><first>Christof</first><last>Monz</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <pages>1828-1840</pages>
      <abstract>Multilingual Machine Translation (MMT) benefits from knowledge transfer across different language pairs. However, improvements in one-to-many translation compared to many-to-one translation are only marginal and sometimes even negligible. This performance discrepancy raises the question of to what extent positive transfer plays a role on the target-side for one-to-many MT. In this paper, we conduct a large-scale study that varies the auxiliary target-side languages along two dimensions, i.e., linguistic similarity and corpus size, to show the dynamic impact of knowledge transfer on the main language pairs. We show that linguistically similar auxiliary target languages exhibit strong ability to transfer positive knowledge. With an increasing size of similar target languages, the positive transfer is further enhanced to benefit the main language pairs. Meanwhile, we find distant auxiliary target languages can also unexpectedly benefit main language pairs, even with minimal positive transfer ability. Apart from transfer, we show distant auxiliary target languages can act as a regularizer to benefit translation performance by enhancing the generalization and model inference calibration.</abstract>
      <url hash="386f1099">2024.eacl-long.110</url>
      <bibkey>meng-monz-2024-disentangling</bibkey>
    </paper>
    <paper id="111">
      <title>Uncovering Stereotypes in Large Language Models: A Task Complexity-based Approach</title>
      <author><first>Hari</first><last>Shrawgi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Prasanjit</first><last>Rath</last></author>
      <author><first>Tushar</first><last>Singhal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sandipan</first><last>Dandapat</last><affiliation>Microsoft</affiliation></author>
      <pages>1841-1857</pages>
      <abstract>Recent Large Language Models (LLMs) have unlocked unprecedented applications of AI. As these models continue to transform human life, there are growing socio-ethical concerns around their inherent stereotypes that can lead to bias in their applications. There is an urgent need for holistic bias evaluation of these LLMs. Few such benchmarks exist today and evaluation techniques that do exist are either non-holistic or may provide a false sense of security as LLMs become better at hiding their biases on simpler tasks. We address these issues with an extensible benchmark - LLM Stereotype Index (LSI). LSI is grounded on Social Progress Index, a holistic social benchmark. We also test the breadth and depth of bias protection provided by LLMs via a variety of tasks with varying complexities. Our findings show that both ChatGPT and GPT-4 have strong inherent prejudice with respect to nationality, gender, race, and religion. The exhibition of such issues becomes increasingly apparent as we increase task complexity. Furthermore, GPT-4 is better at hiding the biases, but when displayed it is more significant. Our findings highlight the harms and divide that these LLMs can bring to society if we do not take very diligent care in their use.</abstract>
      <url hash="c1a01154">2024.eacl-long.111</url>
      <attachment type="software" hash="02249be2">2024.eacl-long.111.software.zip</attachment>
      <attachment type="note" hash="ce47ec9a">2024.eacl-long.111.note.zip</attachment>
      <bibkey>shrawgi-etal-2024-uncovering</bibkey>
    </paper>
    <paper id="112">
      <title>Rainbow - A Benchmark for Systematic Testing of How Sensitive Visio-Linguistic Models are to Color Naming</title>
      <author><first>Marie</first><last>Bexte</last><affiliation>Fernuniversität Gesamthochschule Hagen</affiliation></author>
      <author><first>Andrea</first><last>Horbach</last><affiliation>Universität Hildesheim</affiliation></author>
      <author><first>Torsten</first><last>Zesch</last><affiliation>Fernuniversität in Hagen</affiliation></author>
      <pages>1858-1875</pages>
      <abstract>With the recent emergence of powerful visio-linguistic models comes the question of how fine-grained their multi-modal understanding is. This has lead to the release of several probing datasets. Results point towards models having trouble with prepositions and verbs, but being relatively robust when it comes to color.To gauge how deep this understanding goes, we compile a comprehensive probing dataset to systematically test multi-modal alignment around color. We demonstrate how human perception influences descriptions of color and pay special attention to the extent to which this is reflected within the predictions of a visio-linguistic model. Probing a set of models with diverse properties with our benchmark confirms the superiority of models that do not rely on pre-extracted image features, and demonstrates that augmentation with too much noisy pre-training data can produce an inferior model. While the benchmark remains challenging for all models we test, the overall result pattern suggests well-founded alignment of color terms with hues. Analyses do however reveal uncertainty regarding the boundaries between neighboring color terms.</abstract>
      <url hash="51ff0542">2024.eacl-long.112</url>
      <bibkey>bexte-etal-2024-rainbow</bibkey>
    </paper>
    <paper id="113">
      <title><fixed-case>CAT</fixed-case>f<fixed-case>OOD</fixed-case>: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration</title>
      <author><first>Rachneet</first><last>Sachdeva</last></author>
      <author><first>Martin</first><last>Tutek</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>1876-1898</pages>
      <abstract>In recent years, large language models (LLMs) have shown remarkable capabilities at scale, particularly at generating text conditioned on a prompt. In our work, we investigate the use of LLMs to augment training data of smaller language models (SLMs) with automatically generated counterfactual (CF) instances – i.e. minimally altered inputs – in order to improve out-of-domain (OOD) performance of SLMs in the extractive question answering (QA) setup. We show that, across various LLM generators, such data augmentation consistently enhances OOD performance and improves model calibration for both confidence-based and rationale-augmented calibrator models. Furthermore, these performance improvements correlate with higher diversity of CF instances in terms of their surface form and semantic content. Finally, we show that CF augmented models which are easier to calibrate also exhibit much lower entropy when assigning importance, indicating that rationale-augmented calibrators prefer concise explanations.</abstract>
      <url hash="46770007">2024.eacl-long.113</url>
      <attachment type="software" hash="afd2338a">2024.eacl-long.113.software.zip</attachment>
      <bibkey>sachdeva-etal-2024-catfood</bibkey>
    </paper>
    <paper id="114">
      <title><fixed-case>UP</fixed-case>5: Unbiased Foundation Model for Fairness-aware Recommendation</title>
      <author><first>Wenyue</first><last>Hua</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Yingqiang</first><last>Ge</last></author>
      <author><first>Shuyuan</first><last>Xu</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Jianchao</first><last>Ji</last></author>
      <author><first>Zelong</first><last>Li</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <pages>1899-1912</pages>
      <abstract>Recent advances in Foundation Models such as Large Language Models (LLMs) have propelled them to the forefront of Recommender Systems (RS). Despite their utility, there is a growing concern that LLMs might inadvertently perpetuate societal stereotypes, resulting in unfair recommendations. Since fairness is critical for RS as many users take it for decision-making and demand fulfillment, this paper focuses on user-side fairness for LLM-based recommendation where the users may require a recommender system to be fair on specific sensitive features such as gender or age. In this paper, we dive into the extent of unfairness exhibited by LLM-based recommender models based on both T5 and LLaMA backbones, and discuss appropriate methods for promoting equitable treatment of users in LLM-based recommendation models. We introduce a novel Counterfactually-Fair-Prompt (CFP) method towards Unbiased Foundation mOdels (UFO) for fairness-aware LLM-based recommendation. Experiments are conducted on two real-world datasets, MovieLens-1M and Insurance, and compared with both matching-based and sequential-based fairness-aware recommendation models. Results show that CFP achieves better recommendation performance with a high level of fairness.</abstract>
      <url hash="b09b0c02">2024.eacl-long.114</url>
      <bibkey>hua-etal-2024-up5</bibkey>
    </paper>
    <paper id="115">
      <title>Human Temporal Inferences Go Beyond Aspectual Class</title>
      <author><first>Katarzyna</first><last>Pruś</last></author>
      <author><first>Mark</first><last>Steedman</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Adam</first><last>Lopez</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>1913-1923</pages>
      <abstract>Past work in NLP has proposed the task of classifying English verb phrases into situation aspect categories, assuming that these categories play an important role in tasks requiring temporal reasoning. We investigate this assumption by gathering crowd-sourced judgements about aspectual entailments from non-expert, native English participants. The results suggest that aspectual class alone is not sufficient to explain the response patterns of the participants. We propose that looking at scenarios which can feasibly accompany an action description contributes towards a better explanation of the participants’ answers. A further experiment using GPT-3.5 shows that its outputs follow different patterns than human answers, suggesting that such conceivable scenarios cannot be fully accounted for in the language alone. We release our dataset to support further research.</abstract>
      <url hash="e1ae9fe4">2024.eacl-long.115</url>
      <bibkey>prus-etal-2024-human</bibkey>
    </paper>
    <paper id="116">
      <title>It is not True that Transformers are Inductive Learners: Probing <fixed-case>NLI</fixed-case> Models with External Negation</title>
      <author><first>Michael</first><last>Sullivan</last></author>
      <pages>1924-1945</pages>
      <abstract>NLI tasks necessitate a substantial degree of logical reasoning; as such, the remarkable performance of SoTA transformers on these tasks may lead us to believe that those models have learned to reason logically. The results presented in this paper demonstrate that (i) models fine-tuned on NLI datasets learn to treat external negation as a distractor, effectively ignoring its presence in hypothesis sentences; (ii) several near-SoTA encoder and encoder-decoder transformer models fail to inductively learn the law of the excluded middle for a single external negation prefix with respect to NLI tasks, despite extensive fine-tuning; (iii) those models which are are able to learn the law of the excluded middle for a single prefix are unable to generalize this pattern to similar prefixes. Given the critical role of negation in logical reasoning, we may conclude from these findings that transformers do not learn to reason logically when fine-tuned for NLI tasks. Furthermore, these results suggest that transformers may not be able to inductively learn the role of negation with respect to NLI tasks, calling into question their capacity to fully acquire logical reasoning abilities.</abstract>
      <url hash="4b876946">2024.eacl-long.116</url>
      <bibkey>sullivan-2024-true</bibkey>
    </paper>
    <paper id="117">
      <title>Polarized Opinion Detection Improves the Detection of Toxic Language</title>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <author><first>Aristidis</first><last>Likas</last><affiliation>University of Ioannina</affiliation></author>
      <pages>1946-1958</pages>
      <abstract>Distance from unimodality (DFU) has been found to correlate well with human judgment for the assessment of polarized opinions. However, its un-normalized nature makes it less intuitive and somewhat difficult to exploit in machine learning (e.g., as a supervised signal). In this work a normalized version of this measure, called nDFU, is proposed that leads to better assessment of the degree of polarization. Then, we propose a methodology for K-class text classification, based on nDFU, that exploits polarized texts in the dataset. Such polarized instances are assigned to a separate K+1 class, so that a K+1-class classifier is trained. An empirical analysis on three datasets for abusive language detection, shows that nDFU can be used to model polarized annotations and prevent them from harming the classification performance. Finally, we further exploit nDFU to specify conditions that could explain polarization given a dimension and present text examples that polarized the annotators when the dimension was gender and race. Our code is available at https://github.com/ipavlopoulos/ndfu.</abstract>
      <url hash="1eda6f0a">2024.eacl-long.117</url>
      <attachment type="software" hash="973b811f">2024.eacl-long.117.software.zip</attachment>
      <bibkey>pavlopoulos-likas-2024-polarized</bibkey>
    </paper>
    <paper id="118">
      <title>Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations</title>
      <author><first>Amit</first><last>Meghanani</last></author>
      <author><first>Thomas</first><last>Hain</last><affiliation>University of Sheffield</affiliation></author>
      <pages>1959-1967</pages>
      <abstract>Acoustic word embeddings (AWEs) are vector representations of spoken words. An effective method for obtaining AWEs is the Correspondence Auto-Encoder (CAE). In the past, the CAE method has been associated with traditional MFCC features. Representations obtained from self-supervised learning (SSL)-based speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many downstream tasks. However, they have not been well studied in the context of learning AWEs. This work explores the effectiveness of CAE with SSL-based speech representations to obtain improved AWEs. Additionally, the capabilities of SSL-based speech models are explored in cross-lingual scenarios for obtaining AWEs. Experiments are conducted on five languages: Polish, Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the best results for word discrimination in all languages, despite HuBERT being pre-trained on English only. Also, the HuBERT-based CAE model works well in cross-lingual settings. It outperforms MFCC-based CAE models trained on the target languages when trained on one source language and tested on target languages.</abstract>
      <url hash="9b57806b">2024.eacl-long.118</url>
      <attachment type="note" hash="608cd582">2024.eacl-long.118.note.zip</attachment>
      <bibkey>meghanani-hain-2024-improving</bibkey>
    </paper>
    <paper id="119">
      <title>Investigating Agency of <fixed-case>LLM</fixed-case>s in Human-<fixed-case>AI</fixed-case> Collaboration Tasks</title>
      <author><first>Ashish</first><last>Sharma</last></author>
      <author><first>Sudha</first><last>Rao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chris</first><last>Brockett</last><affiliation>Microsoft</affiliation></author>
      <author><first>Akanksha</first><last>Malhotra</last><affiliation>Microsoft</affiliation></author>
      <author><first>Nebojsa</first><last>Jojic</last><affiliation>Microsoft Research and Microsoft Research</affiliation></author>
      <author><first>Bill</first><last>Dolan</last></author>
      <pages>1968-1987</pages>
      <abstract>Agency, the capacity to proactively shape events, is central to how humans interact and collaborate. While LLMs are being developed to simulate human behavior and serve as human-like agents, little attention has been given to the Agency that these models should possess in order to proactively manage the direction of interaction and collaboration. In this paper, we investigate Agency as a desirable function of LLMs, and how it can be measured and managed. We build on social-cognitive theory to develop a framework of features through which Agency is expressed in dialogue – indicating what you intend to do (Intentionality), motivating your intentions (Motivation), having self-belief in intentions (Self-Efficacy), and being able to self-adjust (Self-Regulation). We collect a new dataset of 83 human-human collaborative interior design conversations containing 908 conversational snippets annotated for Agency features. Using this dataset, we develop methods for measuring Agency of LLMs. Automatic and human evaluations show that models that manifest features associated with high Intentionality, Motivation, Self-Efficacy, and Self-Regulation are more likely to be perceived as strongly agentive.</abstract>
      <url hash="1b326838">2024.eacl-long.119</url>
      <attachment type="note" hash="cb692aac">2024.eacl-long.119.note.zip</attachment>
      <bibkey>sharma-etal-2024-investigating</bibkey>
    </paper>
    <paper id="120">
      <title><fixed-case>S</fixed-case>ynth<fixed-case>DST</fixed-case>: Synthetic Data is All You Need for Few-Shot Dialog State Tracking</title>
      <author><first>Atharva</first><last>Kulkarni</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Bo-Hsiang</first><last>Tseng</last></author>
      <author><first>Joel</first><last>Moniz</last><affiliation>Apple</affiliation></author>
      <author><first>Dhivya</first><last>Piraviperumal</last></author>
      <author><first>Hong</first><last>Yu</last><affiliation>Apple</affiliation></author>
      <author><first>Shruti</first><last>Bhargava</last><affiliation>Apple</affiliation></author>
      <pages>1988-2001</pages>
      <abstract>In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, ‘<i>Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?</i>' Addressing this question, we propose , a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from results in <tex-math>4-5\%</tex-math> improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1 and 2.4. Remarkably, our few-shot learning approach recovers nearly 98% of the performance compared to the few-shot setup using human-annotated training data.</abstract>
      <url hash="d583a6cf">2024.eacl-long.120</url>
      <attachment type="note" hash="778c3b34">2024.eacl-long.120.note.zip</attachment>
      <bibkey>kulkarni-etal-2024-synthdst</bibkey>
    </paper>
    <paper id="121">
      <title>Argument Mining as a Text-to-Text Generation Task</title>
      <author><first>Masayuki</first><last>Kawarada</last></author>
      <author><first>Tsutomu</first><last>Hirao</last><affiliation>NTT Communication Science Laboratories</affiliation></author>
      <author><first>Wataru</first><last>Uchida</last></author>
      <author><first>Masaaki</first><last>Nagata</last><affiliation>NTT Corporation</affiliation></author>
      <pages>2002-2014</pages>
      <abstract>Argument Mining (AM) aims to uncover the argumentative structures within a text. Previous methods require several subtasks, such as span identification, component classification, and relation classification. Consequently, these methods need rule-based postprocessing to derive argumentative structures from the output of each subtask. This approach adds to the complexity of the model and expands the search space of the hyperparameters. To address this difficulty, we propose a simple yet strong method based on a text-to-text generation approach using a pretrained encoder-decoder language model. Our method simultaneously generates argumentatively annotated text for spans, components, and relations, eliminating the need for task-specific postprocessing and hyperparameter tuning. Furthermore, because it is a straightforward text-to-text generation method, we can easily adapt our approach to various types of argumentative structures.Experimental results demonstrate the effectiveness of our method, as it achieves state-of-the-art performance on three different types of benchmark datasets: the Argument-annotated Essays Corpus (AAEC), AbstRCT, and the Cornell eRulemaking Corpus (CDCP).</abstract>
      <url hash="11366197">2024.eacl-long.121</url>
      <bibkey>kawarada-etal-2024-argument</bibkey>
    </paper>
    <paper id="122">
      <title>Answering legal questions from laymen in <fixed-case>G</fixed-case>erman civil law system</title>
      <author><first>Marius</first><last>Büttner</last></author>
      <author><first>Ivan</first><last>Habernal</last><affiliation>Universität Paderborn</affiliation></author>
      <pages>2015-2027</pages>
      <abstract>What is preventing us from building a NLP system that could help real people in real situations, for instance when they need legal advice but don’t understand law? This question is trickier than one might think, because legal systems vary from country to country, so do the law books, availability of data, and incomprehensibility of legalese. In this paper we focus Germany (which employs the civil-law system where, roughly speaking, interpretation of law codes dominates over precedence) and lay a foundational work to address the laymen’s legal question answering empirically. We create GerLayQA, a new dataset comprising of 21k laymen’s legal questions paired with answers from lawyers and grounded to concrete law book paragraphs. We experiment with a variety of retrieval and answer generation models and provide an in-depth analysis of limitations, which helps us to provide first empirical answers to the question above.</abstract>
      <url hash="c825e62f">2024.eacl-long.122</url>
      <attachment type="software" hash="062d887d">2024.eacl-long.122.software.zip</attachment>
      <attachment type="note" hash="692534e5">2024.eacl-long.122.note.zip</attachment>
      <bibkey>buttner-habernal-2024-answering</bibkey>
    </paper>
    <paper id="123">
      <title>An Empirical Analysis of Diversity in Argument Summarization</title>
      <author><first>Michiel</first><last>Van Der Meer</last><affiliation>Leiden University</affiliation></author>
      <author><first>Piek</first><last>Vossen</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Catholijn</first><last>Jonker</last><affiliation>TU Delft</affiliation></author>
      <author><first>Pradeep</first><last>Murukannaiah</last><affiliation>Delft University of Technology</affiliation></author>
      <pages>2028-2045</pages>
      <abstract>Presenting high-level arguments is a crucial task for fostering participation in online societal discussions. Current argument summarization approaches miss an important facet of this task—capturing <i>diversity</i>—which is important for accommodating multiple perspectives. We introduce three aspects of diversity: those of opinions, annotators, and sources. We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations. We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths. Further, we observe that diversification of training data may ameliorate generalization in zero-shot cases. Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity.</abstract>
      <url hash="d367e18c">2024.eacl-long.123</url>
      <attachment type="software" hash="9f52a6dc">2024.eacl-long.123.software.zip</attachment>
      <bibkey>van-der-meer-etal-2024-empirical</bibkey>
    </paper>
    <paper id="124">
      <title>What Makes Medical Claims (Un)Verifiable? Analyzing Entity and Relation Properties for Fact Verification</title>
      <author><first>Amelie</first><last>Wuehrl</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Yarik</first><last>Menchaca Resendiz</last></author>
      <author><first>Lara</first><last>Grimminger</last></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <pages>2046-2058</pages>
      <abstract>Verifying biomedical claims fails if no evidence can be discovered. In these cases, the fact-checking verdict remains unknown and the claim is unverifiable. To improve this situation, we have to understand if there are any claim properties that impact its verifiability. In this work we assume that entities and relations define the core variables in a biomedical claim’s anatomy and analyze if their properties help us to differentiate verifiable from unverifiable claims. In a study with trained annotation experts we prompt them to find evidence for biomedical claims, and observe how they refine search queries for their evidence search. This leads to the first corpus for scientific fact verification annotated with subject–relation–object triplets, evidence documents, and fact-checking verdicts (the BEAR-FACT corpus). We find (1) that discovering evidence for negated claims (e.g., X–does-not-cause–Y) is particularly challenging. Further, we see that annotators process queries mostly by adding constraints to the search and by normalizing entities to canonical names. (2) We compare our in-house annotations with a small crowdsourcing setting where we employ both medical experts and laypeople. We find that domain expertise does not have a substantial effect on the reliability of annotations. Finally, (3), we demonstrate that it is possible to reliably estimate the success of evidence retrieval purely from the claim text (.82F<tex-math>_1</tex-math>), whereas identifying unverifiable claims proves more challenging (.27F<tex-math>_1</tex-math>)</abstract>
      <url hash="484e5662">2024.eacl-long.124</url>
      <attachment type="note" hash="e779fd35">2024.eacl-long.124.note.zip</attachment>
      <bibkey>wuehrl-etal-2024-makes</bibkey>
    </paper>
    <paper id="125">
      <title>Approximate Attributions for Off-the-Shelf <fixed-case>S</fixed-case>iamese Transformers</title>
      <author><first>Lucas</first><last>Moeller</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Dmitry</first><last>Nikolaev</last></author>
      <author><first>Sebastian</first><last>Padó</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <pages>2059-2071</pages>
      <abstract>Siamese encoders such as sentence transformers are among the least understood deep models.Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one. To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (Möller et al., 2023). However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models. In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model’s predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models.We extensively compare approximate and exact attributions and use them to analyze the models’ attendance to different linguistic aspects. We gain insights into which syntactic roles Siamese transformers attend to, confirm that they mostly ignore negation, explore how they judge semantically opposite adjectives, and find that they exhibit lexical bias.</abstract>
      <url hash="9534c6fa">2024.eacl-long.125</url>
      <attachment type="software" hash="dbdd906b">2024.eacl-long.125.software.zip</attachment>
      <bibkey>moeller-etal-2024-approximate</bibkey>
    </paper>
    <paper id="126">
      <title>Describing Images <tex-math>\textit{Fast and Slow}</tex-math>: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes</title>
      <author><first>Ece</first><last>Takmaz</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Sandro</first><last>Pezzelle</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Raquel</first><last>Fernández</last><affiliation>University of Amsterdam and University of Amsterdam</affiliation></author>
      <pages>2072-2087</pages>
      <abstract>There is an intricate relation between the properties of an image and how humans behave while describing the image. This behavior shows ample variation, as manifested in human signals such as eye movements and when humans start to describe the image. Despite the value of such signals of visuo-linguistic variation, they are virtually disregarded in the training of current pretrained models, which motivates further investigation. Using a corpus of Dutch image descriptions with concurrently collected eye-tracking data, we explore the nature of the variation in visuo-linguistic signals, and find that they correlate with each other. Given this result, we hypothesize that variation stems partly from the properties of the images, and explore whether image representations encoded by pretrained vision encoders can capture such variation. Our results indicate that pretrained models do so to a weak-to-moderate degree, suggesting that the models lack biases about what makes a stimulus complex for humans and what leads to variations in human outputs.</abstract>
      <url hash="1647aacb">2024.eacl-long.126</url>
      <bibkey>takmaz-etal-2024-describing</bibkey>
    </paper>
    <paper id="127">
      <title>Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge</title>
      <author><first>Xin</first><last>Zhao</last></author>
      <author><first>Naoki</first><last>Yoshinaga</last><affiliation>Institute of Industrial Science, the University of Tokyo</affiliation></author>
      <author><first>Daisuke</first><last>Oba</last><affiliation>Institute of Industrial Science, The University of Tokyo</affiliation></author>
      <pages>2088-2102</pages>
      <abstract>Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge, thus resorting to cross-lingual transfer in multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and represent factual knowledge. Using the multilingual factual knowledge probing dataset, mLAMA, we first conducted a neuron investigation of ML-LMs (specifically, multilingual BERT). We then traced the roots of facts back to the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire specific facts. We finally identified three patterns of acquiring and representing facts in ML-LMs: language-independent, cross-lingual shared and transferred, and devised methods for differentiating them. Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.</abstract>
      <url hash="1b80c08e">2024.eacl-long.127</url>
      <bibkey>zhao-etal-2024-tracing</bibkey>
    </paper>
    <paper id="128">
      <title>Comparing Knowledge Sources for Open-Domain Scientific Claim Verification</title>
      <author><first>Juraj</first><last>Vladika</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>2103-2114</pages>
      <abstract>The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient fact-checking systems for scientific claims. The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus. This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence. In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems. We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings. While keeping the pipeline’s evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different information retrieval techniques. We show that PubMed works better with specialized biomedical claims, while Wikipedia is more suited for everyday health concerns. Likewise, BM25 excels in retrieval precision, while semantic search in recall of relevant evidence. We discuss the results, outline frequent retrieval patterns and challenges, and provide promising future directions.</abstract>
      <url hash="18b877f4">2024.eacl-long.128</url>
      <attachment type="software" hash="4b04f269">2024.eacl-long.128.software.zip</attachment>
      <bibkey>vladika-matthes-2024-comparing</bibkey>
    </paper>
    <paper id="129">
      <title>Measuring Uncertainty in Neural Machine Translation with Similarity-Sensitive Entropy</title>
      <author><first>Julius</first><last>Cheng</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <pages>2115-2128</pages>
      <abstract>Uncertainty estimation is an important diagnostic tool for statistical models, and is often used to assess the confidence of model predictions. Previous work shows that neural machine translation (NMT) is an intrinsically uncertain task where there are often multiple correct and semantically equivalent translations, and that well-trained NMT models produce good translations despite spreading probability mass among many semantically similar translations. These findings suggest that popular measures of uncertainty based on token- and sequence-level entropies which measure surface form diversity may not be good proxies of the more useful quantity of interest, semantic diversity. We propose to adapt similarity-sensitive Shannon entropy (S3E), a concept borrowed from theoretical ecology, for NMT. By demonstrating significantly improved correlation between S3E and task performance on quality estimation and named entity recall, we show that S3E is a useful framework for measuring uncertainty in NMT.</abstract>
      <url hash="4d626cc3">2024.eacl-long.129</url>
      <bibkey>cheng-vlachos-2024-measuring</bibkey>
    </paper>
    <paper id="130">
      <title><fixed-case>L</fixed-case>egal<fixed-case>L</fixed-case>ens: Leveraging <fixed-case>LLM</fixed-case>s for Legal Violation Identification in Unstructured Text</title>
      <author><first>Dor</first><last>Bernsohn</last></author>
      <author><first>Gil</first><last>Semo</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Yaron</first><last>Vazana</last></author>
      <author><first>Gila</first><last>Hayat</last><affiliation>Darrow AI</affiliation></author>
      <author><first>Ben</first><last>Hagag</last></author>
      <author><first>Joel</first><last>Niklaus</last></author>
      <author><first>Rohit</first><last>Saha</last><affiliation>Georgian.io</affiliation></author>
      <author><first>Kyryl</first><last>Truskovskyi</last><affiliation>Georgian.io</affiliation></author>
      <pages>2129-2145</pages>
      <abstract>In this study, we focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals. We constructed two datasets using Large Language Models (LLMs) which were subsequently validated by domain expert annotators. Both tasks were designed specifically for the context of class-action cases. The experimental design incorporated fine-tuning models from the BERT family and open-source LLMs, and conducting few-shot experiments using closed-source LLMs. Our results, with an F1-score of 62.69% (violation identification) and 81.02% (associating victims), show that our datasets and setups can be used for both tasks. Finally, we publicly release the datasets and the code used for the experiments in order to advance further research in the area of legal natural language processing (NLP).</abstract>
      <url hash="d7140c68">2024.eacl-long.130</url>
      <bibkey>bernsohn-etal-2024-legallens</bibkey>
    </paper>
    <paper id="131">
      <title><tex-math>\mu</tex-math><fixed-case>PLAN</fixed-case>: Summarizing using a Content Plan as Cross-Lingual Bridge</title>
      <author><first>Fantine</first><last>Huot</last><affiliation>Google</affiliation></author>
      <author><first>Joshua</first><last>Maynez</last><affiliation>Google</affiliation></author>
      <author><first>Chris</first><last>Alberti</last><affiliation>Columbia University</affiliation></author>
      <author><first>Reinald Kim</first><last>Amplayo</last><affiliation>Google</affiliation></author>
      <author><first>Priyanka</first><last>Agrawal</last><affiliation>Google</affiliation></author>
      <author><first>Constanza</first><last>Fierro</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>Shashi</first><last>Narayan</last><affiliation>Google</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>2146-2163</pages>
      <abstract>Cross-lingual summarization aims to generate a summary in one languagegiven input in a different language, allowing for the dissemination ofrelevant content among different language speaking populations. Thetask is challenging mainly due to the paucity of cross-lingualdatasets and the compounded difficulty of summarizing andtranslating.This work presents <tex-math>\mu</tex-math>PLAN, an approach to cross-lingual summarization that uses an intermediate planning step as a cross-lingual bridge. We formulate the plan as a sequence of entities capturing thesummary’s content and the order in which it should becommunicated. Importantly, our plans abstract from surface form: usinga multilingual knowledge base, we align entities to their canonicaldesignation across languages and generate the summary conditioned onthis cross-lingual bridge and the input. Automatic and human evaluation on the XWikis dataset (across four language pairs) demonstrates that our planning objective achieves state-of-the-art performance interms of informativeness and faithfulness. Moreover, <tex-math>\mu</tex-math>PLAN modelsimprove the zero-shot transfer to new cross-lingual language pairscompared to baselines without a planning component.</abstract>
      <url hash="9963d914">2024.eacl-long.131</url>
      <bibkey>huot-etal-2024-mplan</bibkey>
    </paper>
    <paper id="132">
      <title>Exploring Data Augmentation in Neural <fixed-case>DRS</fixed-case>-to-Text Generation</title>
      <author><first>Muhammad Saad</first><last>Amin</last></author>
      <author><first>Luca</first><last>Anselma</last><affiliation>University of Turin</affiliation></author>
      <author><first>Alessandro</first><last>Mazzei</last><affiliation>University of Turin</affiliation></author>
      <pages>2164-2178</pages>
      <abstract>Neural networks are notoriously data-hungry. This represents an issue in cases where data are scarce such as in low-resource languages. Data augmentation is a technique commonly used in computer vision to provide neural networks with more data and increase their generalization power. When dealing with data augmentation for natural language, however, simple data augmentation techniques similar to the ones used in computer vision such as rotation and cropping cannot be employed because they would generate ungrammatical texts. Thus, data augmentation needs a specific design in the case of neural logic-to-text systems, especially for a structurally rich input format such as the ones used for meaning representation. This is the case of the neural natural language generation for Discourse Representation Structures (DRS-to-Text), where the logical nature of DRS needs a specific design of data augmentation. In this paper, we adopt a novel approach in DRS-to-Text to selectively augment a training set with new data by adding and varying two specific lexical categories, i.e. proper and common nouns. In particular, we propose using WordNet supersenses to produce new training sentences using both in-and-out-of-context nouns. We present a number of experiments for evaluating the role played by augmented lexical information. The experimental results prove the effectiveness of our approach for data augmentation in DRS-to-Text generation.</abstract>
      <url hash="fcedda01">2024.eacl-long.132</url>
      <bibkey>amin-etal-2024-exploring</bibkey>
    </paper>
    <paper id="133">
      <title>Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models</title>
      <author><first>Lukáš</first><last>Mikula</last></author>
      <author><first>Michal</first><last>Štefánik</last></author>
      <author><first>Marek</first><last>Petrovič</last></author>
      <author><first>Petr</first><last>Sojka</last><affiliation>Faculty of Informatics, Masaryk University</affiliation></author>
      <pages>2179-2193</pages>
      <abstract>While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets. Authors commonly assess model robustness by evaluating their models on out-of-distribution (OOD) datasets of the same task, but these datasets might share the bias of the training dataset. We propose a simple method for measuring a scale of models’ reliance on any identified spurious feature and assess the robustness towards a large set of known and newly found prediction biases for various pre-trained models and debiasing methods in Question Answering (QA). We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among different QA datasets. We further evidence this by measuring that performance of OOD models depends on bias features comparably to the ID model. Our findings motivate future work to refine the reports of LLMs’ robustness to a level of known spurious features.</abstract>
      <url hash="cf7135c3">2024.eacl-long.133</url>
      <attachment type="software" hash="83eda4a4">2024.eacl-long.133.software.zip</attachment>
      <bibkey>mikula-etal-2024-think</bibkey>
    </paper>
    <paper id="134">
      <title>Improving Contrastive Learning in Emotion Recognition in Conversation via Data Augmentation and Decoupled Neutral Emotion</title>
      <author><first>Yujin</first><last>Kang</last></author>
      <author><first>Yoon-Sik</first><last>Cho</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>2194-2208</pages>
      <abstract>Emotion recognition in conversation (ERC) has attracted much attention due to its wide applications. While consistent improvement is being made in this area, inevitable challenge comes from the dataset. The ERC dataset exhibits significantly imbalanced emotion distribution. While the utterances with neutral emotion predominate the data, this emotion label is always treated the same as other emotion labels in current approaches. To address the problem caused by the dataset, we propose a supervised contrastive learning specifically oriented for ERC task. We employ a novel data augmentation method emulating the emotion dynamics in a conversation and formulate supervised contrastive learning method tailored for ERC addressing the predominance and the ambiguity of neutral emotion. Experimental results on four benchmark datasets demonstrate the effectiveness of our approach.</abstract>
      <url hash="2b2331b0">2024.eacl-long.134</url>
      <bibkey>kang-cho-2024-improving</bibkey>
    </paper>
    <paper id="135">
      <title>CroCoAlign: A Cross-Lingual, Context-Aware and Fully-Neural Sentence Alignment System for Long Texts</title>
      <author><first>Francesco</first><last>Molfese</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Andrei</first><last>Bejgu</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Simone</first><last>Tedeschi</last></author>
      <author><first>Simone</first><last>Conia</last><affiliation>Sapienza University of Rome</affiliation></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>2209-2220</pages>
      <abstract>Sentence alignment – establishing links between corresponding sentences in two related documents – is an important NLP task with several downstream applications, such as machine translation (MT). Despite the fact that existing sentence alignment systems have achieved promising results, their effectiveness is based on auxiliary information such as document metadata or machine-generated translations, as well as hyperparameter-sensitive techniques. Moreover, these systems often overlook the crucial role that context plays in the alignment process. In this paper, we address the aforementioned issues and propose CroCoAlign: the first context-aware, end-to-end and fully neural architecture for sentence alignment. Our system maps source and target sentences in long documents by contextualizing their sentence embeddings with respect to the other sentences in the document. We extensively evaluate CroCoAlign on a multilingual dataset consisting of 20 language pairs derived from the Opus project, and demonstrate that our model achieves state-of-the-art performance. To ensure reproducibility, we release our code and model checkpoints at https://github.com/Babelscape/CroCoAlign.</abstract>
      <url hash="37cdec45">2024.eacl-long.135</url>
      <bibkey>molfese-etal-2024-neuralign</bibkey>
      <revision id="1" href="2024.eacl-long.135v1" hash="c49fbf28"/>
      <revision id="2" href="2024.eacl-long.135v2" hash="37cdec45" date="2024-03-19">Update CroCoAlign.</revision>
    </paper>
    <paper id="136">
      <title>Explaining Speech Classification Models via Word-Level Audio Segments and Paralinguistic Features</title>
      <author><first>Eliana</first><last>Pastor</last><affiliation>Polytechnic Institute of Turin</affiliation></author>
      <author><first>Alkis</first><last>Koudounas</last><affiliation>Polytechnic Institute of Turin</affiliation></author>
      <author><first>Giuseppe</first><last>Attanasio</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Elena</first><last>Baralis</last><affiliation>Politecnico di Torino</affiliation></author>
      <pages>2221-2238</pages>
      <abstract>Predictive models make mistakes and have biases. To combat both, we need to understand their predictions.Explainable AI (XAI) provides insights into models for vision, language, and tabular data. However, only a few approaches exist for speech classification models. Previous works focus on a selection of spoken language understanding (SLU) tasks, and most users find their explanations challenging to interpret.We propose a novel approach to explain speech classification models. It provides two types of insights. (i) Word-level. We measure the impact of each audio segment aligned with a word on the outcome. (ii) Paralinguistic. We evaluate how non-linguistic features (e.g., prosody and background noise) affect the outcome if perturbed.We validate our approach by explaining two state-of-the-art SLU models on two tasks in English and Italian. We test their plausibility with human subject ratings. Our results show that the explanations correctly represent the model’s inner workings and are plausible to humans.</abstract>
      <url hash="9921275b">2024.eacl-long.136</url>
      <bibkey>pastor-etal-2024-explaining</bibkey>
    </paper>
    <paper id="137">
      <title>Zero-Shot End-to-End Spoken Language Understanding via Cross-Modal Selective Self-Training</title>
      <author><first>Jianfeng</first><last>He</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Julian</first><last>Salazar</last><affiliation>Google</affiliation></author>
      <author><first>Kaisheng</first><last>Yao</last><affiliation>Google</affiliation></author>
      <author><first>Haoqi</first><last>Li</last><affiliation>Hume AI Inc.</affiliation></author>
      <author><first>Jason</first><last>Cai</last><affiliation>Amazon</affiliation></author>
      <pages>2239-2256</pages>
      <abstract>End-to-end (E2E) spoken language understanding (SLU) is constrained by the cost of collecting speech-semantics pairs, especially when label domains change. Hence, we explore <i>zero-shot</i> E2E SLU, which learns E2E SLU without speech-semantics pairs, instead using only speech-text and text-semantics pairs. Previous work achieved zero-shot by pseudolabeling all speech-text transcripts with a natural language understanding (NLU) model learned on text-semantics corpora. However, this method requires the domains of speech-text and text-semantics to match, which often mismatch due to separate collections. Furthermore, using the entire collected speech-text corpus from any domains leads to <i>imbalance</i> and <i>noise</i> issues. To address these, we propose <i>cross-modal selective self-training</i> (CMSST). CMSST tackles imbalance by clustering in a joint space of the three modalities (speech, text, and semantics) and handles label noise with a selection network. We also introduce two benchmarks for zero-shot E2E SLU, covering matched and found speech (mismatched) settings. Experiments show that CMSST improves performance in both two settings, with significantly reduced sample sizes and training time. Our code and data are released in https://github.com/amazon-science/zero-shot-E2E-slu.</abstract>
      <url hash="101e1a14">2024.eacl-long.137</url>
      <attachment type="software" hash="3818b9f4">2024.eacl-long.137.software.zip</attachment>
      <attachment type="note" hash="9c981a09">2024.eacl-long.137.note.zip</attachment>
      <bibkey>he-etal-2024-zero</bibkey>
    </paper>
    <paper id="138">
      <title>Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models</title>
      <author><first>Natalie</first><last>Shapira</last></author>
      <author><first>Mosh</first><last>Levy</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Seyed Hossein</first><last>Alavi</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Xuhui</first><last>Zhou</last></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Yoav</first><last>Goldberg</last><affiliation>Bar-Ilan University, Allen Institute for Artificial Intelligence and Bar Ilan University</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <pages>2257-2273</pages>
      <abstract>The escalating debate on AI’s capabilities warrants developing reliable metrics to assess machine “intelligence.” Recently, many anecdotal examples were used to suggest that newer Large Language Models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs’ N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.</abstract>
      <url hash="f7891313">2024.eacl-long.138</url>
      <bibkey>shapira-etal-2024-clever</bibkey>
    </paper>
    <paper id="139">
      <title><fixed-case>N</fixed-case>ev<fixed-case>IR</fixed-case>: Negation in Neural Information Retrieval</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Dawn</first><last>Lawrie</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <pages>2274-2287</pages>
      <abstract>Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine and human performance.</abstract>
      <url hash="6a5f023d">2024.eacl-long.139</url>
      <attachment type="note" hash="ed01c8e2">2024.eacl-long.139.note.zip</attachment>
      <bibkey>weller-etal-2024-nevir</bibkey>
    </paper>
    <paper id="140">
      <title>“According to . . . ”: Prompting Language Models Improves Quoting from Pre-Training Data</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Marc</first><last>Marone</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Nathaniel</first><last>Weir</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Dawn</first><last>Lawrie</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <pages>2288-2301</pages>
      <abstract>Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of “according to sources”, we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S. legal tax code) that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.</abstract>
      <url hash="d5d3d5c2">2024.eacl-long.140</url>
      <bibkey>weller-etal-2024-according</bibkey>
    </paper>
    <paper id="141">
      <title>Accurate and Well-Calibrated <fixed-case>ICD</fixed-case> Code Assignment Through Attention Over Diverse Label Embeddings</title>
      <author><first>Goncalo</first><last>Gomes</last></author>
      <author><first>Isabel</first><last>Coutinho</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Bruno</first><last>Martins</last><affiliation>InstituInstituto Superior Técnico</affiliation></author>
      <pages>2302-2315</pages>
      <abstract>Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches. This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work. We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently. The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms. Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance. Our approach also leads to properly calibrated classification results, which can effectively inform downstream tasks such as quantification.</abstract>
      <url hash="bdb511b7">2024.eacl-long.141</url>
      <bibkey>gomes-etal-2024-accurate</bibkey>
    </paper>
    <paper id="142">
      <title>Investigating Content Planning for Navigating Trade-offs in Knowledge-Grounded Dialogue</title>
      <author><first>Kushal</first><last>Chawla</last></author>
      <author><first>Hannah</first><last>Rashkin</last><affiliation>Google</affiliation></author>
      <author><first>Gaurav Singh</first><last>Tomar</last><affiliation>Google</affiliation></author>
      <author><first>David</first><last>Reitter</last><affiliation>Google, Brain, DeepMind</affiliation></author>
      <pages>2316-2335</pages>
      <abstract>Knowledge-grounded dialogue generation is a challenging task because it requires satisfying two fundamental, yet often competing constraints: being responsive in a manner that is specific to what the conversation partner has said while also being attributable to an underlying source document. In this work, we bring this trade-off between these two objectives (specificity and attribution) to light, and ask the question: Can explicit content planning before the response generation help the model to address this challenge? To answer this question, we design a framework called PLEDGE, which allows us to experiment with various plan variables explored in prior work supporting both metric-agnostic and metric-aware approaches. While content planning shows promise, our results on whether it can actually help to navigate this trade-off are mixed – planning mechanisms that are metric-aware (use automatic metrics during training) are better at automatic evaluations but underperform in human judgment compared to metric-agnostic mechanisms. We discuss how this may be caused by over-fitting to automatic metrics, and the need for future work to better calibrate these metrics towards human judgment. We hope the observations from our analysis will inform future work that aims to apply content planning in this context.</abstract>
      <url hash="7e8e7463">2024.eacl-long.142</url>
      <bibkey>chawla-etal-2024-investigating</bibkey>
    </paper>
    <paper id="143">
      <title><fixed-case>SPUQ</fixed-case>: Perturbation-Based Uncertainty Quantification for Large Language Models</title>
      <author><first>Xiang</first><last>Gao</last><affiliation>Intuit</affiliation></author>
      <author><first>Jiaxin</first><last>Zhang</last><affiliation>Intuit AI Research</affiliation></author>
      <author><first>Lalla</first><last>Mouatadid</last></author>
      <author><first>Kamalika</first><last>Das</last><affiliation>Intuit</affiliation></author>
      <pages>2336-2346</pages>
      <abstract>In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggregation techniques. Our findings show a substantial improvement in model uncertainty calibration, with a reduction in Expected Calibration Error (ECE) by 50% on average. Our findings suggest that our proposed UQ method offers promising steps toward enhancing the reliability and trustworthiness of LLMs.</abstract>
      <url hash="a22cae3a">2024.eacl-long.143</url>
      <bibkey>gao-etal-2024-spuq</bibkey>
    </paper>
    <paper id="144">
      <title><fixed-case>TESS</fixed-case>: Text-to-Text Self-Conditioned Simplex Diffusion</title>
      <author><first>Rabeeh</first><last>Karimi Mahabadi</last></author>
      <author><first>Hamish</first><last>Ivison</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jaesung</first><last>Tae</last><affiliation>Yale University</affiliation></author>
      <author><first>James</first><last>Henderson</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Iz</first><last>Beltagy</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Matthew</first><last>Peters</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>2347-2361</pages>
      <abstract>Diffusion models have emerged as a powerful paradigm for generation, obtaining strong performance in various continuous domains. However, applying continuous diffusion models to natural language remains challenging due to its discrete nature and the need for a large number of diffusion steps to generate text, making diffusion-based generation expensive.In this work, we propose Text-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion model that is fully non-autoregressive, employs a new form of self-conditioning, and applies the diffusion process on the logit simplex space rather than the learned embedding space.Through extensive experiments on natural language understanding and generation tasks including summarization, text simplification, paraphrase generation, and question generation, we demonstrate that TESS outperforms state-of-the-art non-autoregressive models, requires fewer diffusion steps with minimal drop in performance, and is competitive with pretrained autoregressive sequence-to-sequence models.</abstract>
      <url hash="0fcb490a">2024.eacl-long.144</url>
      <bibkey>karimi-mahabadi-etal-2024-tess</bibkey>
    </paper>
    <paper id="145">
      <title>Advancing Precise Outline-Conditioned Text Generation with Task Duality and Explicit Outline Control</title>
      <author><first>Yunzhe</first><last>Li</last></author>
      <author><first>Qian</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weixiang</first><last>Yan</last></author>
      <author><first>Wen</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Qinglin</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hari</first><last>Sundaram</last><affiliation>Department of Computer Science</affiliation></author>
      <pages>2362-2377</pages>
      <abstract>Existing works on outline-conditioned text generation typically aim to generate text using provided outlines as rough sketches, such as keywords and phrases. However, these approaches make it challenging to control the quality of text generation and assess consistency between outlines and generated texts due to lack of clarity and rationality of the rough outlines. In this paper, we introduce a novel text generation task called <b>
          <i>Precise Outline-conditioned Generation</i></b>, which requires generating stories based on <i>specific</i>, <i>sentence-level</i> outlines. To facilitate research on this task, we construct two new datasets, <b>WPOG</b> and <b>CDM</b>. We provide strong baselines based on fine-tuning models such as BART and GPT-2, and evaluating zero-shot performance of models such as ChatGPT and Vicuna. Furthermore, we identify an issue of <b>imbalanced utilization of the outline information</b> in the precise outline-conditioned generation, which is ubiquitously observed across fine-tuned models and zero-shot inference models. To address this issue, we propose an <b>explicit outline utilization control approach</b> and a novel framework that <b>leverages the task duality between summarization and generation</b>. Experimental results show that the proposed approaches effectively alleviate the issue of imbalanced outline utilization and enhance the quality of precise outline-conditioned text generation for both fine-tuning and zero-shot settings.</abstract>
      <url hash="781f3be8">2024.eacl-long.145</url>
      <attachment type="note" hash="bafbb14f">2024.eacl-long.145.note.zip</attachment>
      <bibkey>li-etal-2024-advancing</bibkey>
    </paper>
    <paper id="146">
      <title>Localization vs. Semantics: Visual Representations in Unimodal and Multimodal Models</title>
      <author><first>Zhuowan</first><last>Li</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Cihang</first><last>Xie</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Alan</first><last>Yuille</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>2378-2390</pages>
      <abstract>Despite the impressive advancements achieved through vision-and-language pretraining, it remains unclear whether multi-modal learning can help understand each individual modality. In this work, we conduct a comparative analysis of the visual representations in existing vision-and-language models and vision-only models by probing on a broad range of tasks. Five probing tasks are evaluated in order to assess the quality of the learned representations in a nuanced manner. Our results on five probing tasks suggest vision-and-language models are better at label prediction tasks like object and attribute prediction, while vision-only models are stronger at dense prediction tasks that require more localized information. We hope our study sheds light on the role of language in visual learning, and serves as an empirical guide for various pretrained models.</abstract>
      <url hash="4b808528">2024.eacl-long.146</url>
      <attachment type="software" hash="e80ee5cc">2024.eacl-long.146.software.zip</attachment>
      <bibkey>li-etal-2024-localization</bibkey>
    </paper>
    <paper id="147">
      <title>Creating Suspenseful Stories: Iterative Planning with Large Language Models</title>
      <author><first>Kaige</first><last>Xie</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Mark</first><last>Riedl</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>2391-2407</pages>
      <abstract>Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, *suspense* is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.</abstract>
      <url hash="6bfe995c">2024.eacl-long.147</url>
      <bibkey>xie-riedl-2024-creating</bibkey>
    </paper>
    <paper id="148">
      <title>Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer in Prompt Tuning</title>
      <author><first>Kaige</first><last>Xie</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Haoliang</first><last>Wang</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Junda</first><last>Wu</last></author>
      <author><first>Handong</first><last>Zhao</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ruiyi</first><last>Zhang</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Kanak</first><last>Mahadik</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ani</first><last>Nenkova</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Mark</first><last>Riedl</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>2408-2421</pages>
      <abstract>In real-world scenarios, labeled samples for dialogue summarization are usually limited (i.e., few-shot) due to high annotation costs for high-quality dialogue summaries. To efficiently learn from few-shot samples, previous works have utilized massive annotated data from other downstream tasks and then performed prompt transfer in prompt tuning so as to enable cross-task knowledge transfer. However, existing general-purpose prompt transfer techniques lack consideration for dialogue-specific information. In this paper, we focus on improving the prompt transfer from dialogue state tracking to dialogue summarization and propose Skeleton-Assisted Prompt Transfer (SAPT), which leverages skeleton generation as extra supervision that functions as a medium connecting the distinct source and target task and resulting in the model’s better consumption of dialogue state information. To automatically extract dialogue skeletons as supervised training data for skeleton generation, we design a novel approach with perturbation-based probes requiring neither annotation effort nor domain knowledge. Training the model on such skeletons can also help preserve model capability during prompt transfer. Our method significantly outperforms existing baselines. In-depth analyses demonstrate the effectiveness of our method in facilitating cross-task knowledge transfer in few-shot dialogue summarization.</abstract>
      <url hash="59fa61bb">2024.eacl-long.148</url>
      <bibkey>xie-etal-2024-shot</bibkey>
    </paper>
    <paper id="149">
      <title>Ask, Assess, and Refine: Rectifying Factual Consistency and Hallucination in <fixed-case>LLM</fixed-case>s with Metric-Guided Feedback Learning</title>
      <author><first>Dongyub</first><last>Lee</last><affiliation>NAVER and Korea University</affiliation></author>
      <author><first>Eunhwan</first><last>Park</last></author>
      <author><first>Hodong</first><last>Lee</last><affiliation>NAVER CLOUD</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>2422-2433</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have heralded unprecedented capabilities in information-seeking and text generation, as evidenced by applications like Bing Chat and perplexity.ai. Despite these strides, challenges on hallucination and factual inconsistency continue to impede their wider real-world adoption. Contemporary methods, including retrieval-augmented LLMs and feedback-based learning, serve as alternatives to mitigate these challenges. However, challenges remain, particularly regarding referencing erroneous evidence (citation errors) and generating information not present in the evidence (hallucination). In this paper, we introduce the <tex-math>\mathsf{A}^2\mathsf{R}</tex-math> framework: <b>A</b>sk, <b>A</b>ssess, and <b>R</b>efine. Our approach utilizes an <i>explicit</i> evaluation paradigm, incorporating metrics specifically tailored to assess citation errors and hallucination, aiming to address these prevalent challenges robustly. Capitalizing on these evaluations, we devise a strategy to formulate actionable natural language feedback, enabling iterative refinements that yield improved factual consistency and reduced hallucinations in responses. Our experiments on ASQA, ELI5, and QAMPARI datasets demonstrate our method’s superiority in enhancing correctness, fluency, and citation quality.</abstract>
      <url hash="396e7a17">2024.eacl-long.149</url>
      <attachment type="software" hash="a0a3be68">2024.eacl-long.149.software.zip</attachment>
      <attachment type="note" hash="cfcdc311">2024.eacl-long.149.note.zip</attachment>
      <bibkey>lee-etal-2024-ask</bibkey>
    </paper>
    <paper id="150">
      <title>Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters</title>
      <author><first>Shahed</first><last>Masoudian</last></author>
      <author><first>Cornelia</first><last>Volaucnik</last><affiliation>Johannes Kepler Universität Linz</affiliation></author>
      <author><first>Markus</first><last>Schedl</last><affiliation>Johannes Kepler Universität Linz</affiliation></author>
      <author><first>Navid</first><last>Rekabsaz</last></author>
      <pages>2434-2453</pages>
      <abstract>Bias mitigation of Language Models has been the topic of many studies with a recent focus on learning separate modules like adapters for on-demand debiasing. Besides optimizing for a modularized debiased model, it is often critical in practice to control the degree of bias reduction at inference time, e.g., in order to tune for a desired performance-fairness trade-off in search results or to control the strength of debiasing in classification tasks. In this paper, we introduce Controllable Gate Adapter (ConGater), a novel modular gating mechanism with adjustable sensitivity parameters, %In addition to better perseverance of task performance and enhanced information removal, which allows for a gradual transition from the biased state of the model to the fully debiased version at inference time. We demonstrate ConGater performance by (1) conducting adversarial debiasing experiments with three different models on three classification tasks with four protected attributes, and (2) reducing the bias of search results through fairness list-wise regularization to enable adjusting a trade-off between performance and fairness metrics. Our experiments on the classification tasks show that compared to baselines of the same caliber, ConGater can maintain higher task performance while containing less information regarding the attributes. Our results on the retrieval task show that the fully debiased ConGater can achieve the same fairness performance while maintaining more than twice as high task performance than recent strong baselines. Overall, besides strong performance ConGater enables the continuous transitioning between biased and debiased states of models, enhancing personalization of use and interpretability through controllability.</abstract>
      <url hash="b5cd27de">2024.eacl-long.150</url>
      <bibkey>masoudian-etal-2024-effective</bibkey>
    </paper>
    <paper id="151">
      <title><fixed-case>ST</fixed-case>able: Table Generation Framework for Encoder-Decoder Models</title>
      <author><first>Michał</first><last>Pietruszka</last><affiliation>Snowflake computing</affiliation></author>
      <author><first>Michał</first><last>Turski</last><affiliation>Snowflake and Adam Mickiewicz University in Poznań</affiliation></author>
      <author><first>Łukasz</first><last>Borchmann</last><affiliation>Snowflake</affiliation></author>
      <author><first>Tomasz</first><last>Dwojak</last><affiliation>Snowflake Inc.</affiliation></author>
      <author><first>Gabriela</first><last>Nowakowska</last><affiliation>Snowflake</affiliation></author>
      <author><first>Karolina</first><last>Szyndler</last><affiliation>Snowflake Inc.</affiliation></author>
      <author><first>Dawid</first><last>Jurkiewicz</last><affiliation>Snowflake Inc. and The Adam Mickiewicz University</affiliation></author>
      <author><first>Łukasz</first><last>Garncarek</last><affiliation>Snowflake</affiliation></author>
      <pages>2454-2472</pages>
      <abstract>The output structure of database-like tables, consisting of values structured in horizontal rows and vertical columns identifiable by name, can cover a wide range of NLP tasks. Following this constatation, we propose a framework for text-to-table neural models applicable to problems such as extraction of line items, joint entity and relation extraction, or knowledge base population. The permutation-based decoder of our proposal is a generalized sequential method that comprehends information from all cells in the table. The training maximizes the expected log-likelihood for a table’s content across all random permutations of the factorization order. During the content inference, we exploit the model’s ability to generate cells in any order by searching over possible orderings to maximize the model’s confidence and avoid substantial error accumulation, which other sequential models are prone to. Experiments demonstrate a high practical value of the framework, which establishes state-of-the-art results on several challenging datasets, outperforming previous solutions by up to <tex-math>15\\%</tex-math>.</abstract>
      <url hash="391e424b">2024.eacl-long.151</url>
      <attachment type="software" hash="08f6c1ea">2024.eacl-long.151.software.zip</attachment>
      <bibkey>pietruszka-etal-2024-stable</bibkey>
    </paper>
    <paper id="152">
      <title>A <fixed-case>R</fixed-case>el<fixed-case>E</fixed-case>nt<fixed-case>L</fixed-case>ess Benchmark for Modelling Graded Relations between Named Entities</title>
      <author><first>Asahi</first><last>Ushio</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Steven</first><last>Schockaert</last><affiliation>Cardiff University</affiliation></author>
      <pages>2473-2486</pages>
      <abstract>Relations such as “is influenced by”, “is known for” or “is a competitor of” are inherently graded: we can rank entity pairs based on how well they satisfy these relations, but it is hard to draw a line between those pairs that satisfy them and those that do not. Such graded relations play a central role in many applications, yet they are typically not covered by existing Knowledge Graphs. In this paper, we consider the possibility of using Large Language Models (LLMs) to fill this gap. To this end, we introduce a new benchmark, in which entity pairs have to be ranked according to how much they satisfy a given graded relation. The task is formulated as a few-shot ranking problem, where models only have access to a description of the relation and five prototypical instances. We use the proposed benchmark to evaluate state-of-the-art relation embedding strategies as well as several publicly available LLMs and closed conversational models such as GPT-4. We find that smaller language models struggle to outperform a naive baseline. Overall, the best results are obtained with the 11B parameter Flan-T5 model and the 13B parameter OPT model, where further increasing the model size does not seem to be beneficial. For all models, a clear gap with human performance remains.</abstract>
      <url hash="71132dc4">2024.eacl-long.152</url>
      <attachment type="note" hash="b10248b0">2024.eacl-long.152.note.zip</attachment>
      <bibkey>ushio-etal-2024-relentless</bibkey>
    </paper>
    <paper id="153">
      <title>A Multimodal Framework to Detect Target Aware Aggression in Memes</title>
      <author><first>Shawly</first><last>Ahsan</last></author>
      <author><first>Eftekhar</first><last>Hossain</last></author>
      <author><first>Omar</first><last>Sharif</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Avishek</first><last>Das</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>M.</first><last>Dewan</last><affiliation>Athabasca University</affiliation></author>
      <pages>2487-2500</pages>
      <abstract>Internet memes have gained immense traction as a medium for individuals to convey emotions, thoughts, and perspectives on social media. While memes often serve as sources of humor and entertainment, they can also propagate offensive, incendiary, or harmful content, deliberately targeting specific individuals or communities. Identifying such memes is challenging because of their satirical and cryptic characteristics. Most contemporary research on memes’ detrimental facets is skewed towards high-resource languages, often sidelining the unique challenges tied to low-resource languages, such as Bengali. To facilitate this research in low-resource languages, this paper presents a novel dataset MIMOSA (MultIMOdal aggreSsion dAtaset) in Bengali. MIMOSA encompasses 4,848 annotated memes across five aggression target categories: Political, Gender, Religious, Others, and non-aggressive. We also propose MAF (Multimodal Attentive Fusion), a simple yet effective approach that uses multimodal context to detect the aggression targets. MAF captures the selective modality-specific features of the input meme and jointly evaluates them with individual modality features. Experiments on MIMOSA exhibit that the proposed method outperforms several state-of-the-art rivaling approaches. Our code and data are available at https://github.com/shawlyahsan/Bengali-Aggression-Memes.</abstract>
      <url hash="63894988">2024.eacl-long.153</url>
      <bibkey>ahsan-etal-2024-multimodal</bibkey>
    </paper>
    <paper id="154">
      <title>Graph Guided Question Answer Generation for Procedural Question-Answering</title>
      <author><first>Hai</first><last>Pham</last><affiliation>Samsung AI Center</affiliation></author>
      <author><first>Isma</first><last>Hadji</last><affiliation>Samsung</affiliation></author>
      <author><first>Xinnuo</first><last>Xu</last></author>
      <author><first>Ziedune</first><last>Degutyte</last><affiliation>Samsung AI Center, Cambridge</affiliation></author>
      <author><first>Jay</first><last>Rainey</last><affiliation>Samsung AI Centre, Cambridge (SAIC-C)</affiliation></author>
      <author><first>Evangelos</first><last>Kazakos</last><affiliation>Czech Technical University of Prague</affiliation></author>
      <author><first>Afsaneh</first><last>Fazly</last><affiliation>Samsung</affiliation></author>
      <author><first>Georgios</first><last>Tzimiropoulos</last><affiliation>University of Nottingham and Queen Mary University London</affiliation></author>
      <author><first>Brais</first><last>Martinez</last><affiliation>Samsung</affiliation></author>
      <pages>2501-2525</pages>
      <abstract>In this paper, we focus on task-specific question answering (QA). To this end, we introduce a method for generating exhaustive and high-quality training data, which allows us to train compact (e.g., run on a mobile device), task-specific QA models that are competitive against GPT variants. The key technological enabler is a novel mechanism for automatic question-answer generation from procedural text which can ingest large amounts of textual instructions and produce exhaustive in-domain QA training data. While current QA data generation methods can produce well-formed and varied data, their non-exhaustive nature is sub-optimal for training a QA model. In contrast, we leverage the highly structured aspect of procedural text and represent each step and the overall flow of the procedure as graphs. We then condition on graph nodes to automatically generate QA pairs in an exhaustive and controllable manner. Comprehensive evaluations of our method show that: 1) small models trained with our data achieve excellent performance on the target QA task, even exceeding that of GPT3 and ChatGPT despite being several orders of magnitude smaller. 2) semantic coverage is the key indicator for downstream QA performance. Crucially, while large language models excel at syntactic diversity, this does not necessarily result in improvements on the end QA model. In contrast, the higher semantic coverage provided by our method is critical for QA performance.</abstract>
      <url hash="31aabf9f">2024.eacl-long.154</url>
      <attachment type="note" hash="77a407c4">2024.eacl-long.154.note.zip</attachment>
      <bibkey>pham-etal-2024-graph</bibkey>
    </paper>
    <paper id="155">
      <title>Contrastive Decoding Reduces Hallucinations in Large Multilingual Machine Translation Models</title>
      <author><first>Jonas</first><last>Waldendorf</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>2526-2539</pages>
      <abstract>In Neural Machine Translation (NMT), models will sometimes generate repetitive or fluent output that is not grounded in the source sentence. This phenomenon is known as hallucination and is a problem even in large-scale multilingual translation models. We propose to use Contrastive Decoding, an algorithm developed to improve generation from unconditional language models, to mitigate hallucinations in NMT. Specifically, we maximise the log-likelihood difference between a model and the same model with reduced contribution from the encoder outputs. Additionally, we propose an alternative implementation of Contrastive Decoding that dynamically weights the difference based on the maximum probability in the output distribution to reduce the effect of CD when the model is confident of its prediction. We evaluate our methods using the Small (418M) and Medium (1.2B) M2M models across 21 low and medium-resource language pairs. Our results show a 14.6 ± 0.5 and 11.0 ± 0.6 maximal increase in the mean COMET scores for the Small and Medium models on those sentences for which the M2M models initially generate a hallucination., respectively.</abstract>
      <url hash="e8507d57">2024.eacl-long.155</url>
      <bibkey>waldendorf-etal-2024-contrastive</bibkey>
    </paper>
    <paper id="156">
      <title>Leveraging fine-tuned Large Language Models with <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> for Effective Claim, Claimer, and Claim Object Detection</title>
      <author><first>Sotiris</first><last>Kotitsas</last><affiliation>National and Kapodistrian University of Athens</affiliation></author>
      <author><first>Panagiotis</first><last>Kounoudis</last><affiliation>Athena Research Center and National and Kapodistrian University of Athens</affiliation></author>
      <author><first>Eleni</first><last>Koutli</last></author>
      <author><first>Haris</first><last>Papageorgiou</last><affiliation>NATIONAL AND KAPODISTRIAN UNIVERISTY OF ATHENS, ATHENA RESEARCH AND INNOVATION CENTER and ATHENS UNIVERSITY OF ECONOMICS AND BUSINESS</affiliation></author>
      <pages>2540-2554</pages>
      <abstract>Misinformation and disinformation phenomena existed long before the advent of digital technologies. The exponential use of social media platforms, whose information feeds have created the conditions for many to many communication and instant amplification of the news has accelerated the diffusion of inaccurate and misleading information. As a result, the identification of claims have emerged as a pivotal technology for combating the influence of misinformation and disinformation within news media. Most existing work has concentrated on claim analysis at the sentence level, neglecting the crucial exploration of supplementary attributes such as the claimer and the claim object of the claim or confining it by limiting its scope to a predefined list of topics. Furthermore, previous research has been mostly centered around political debates, Wikipedia articles, and COVID-19 related content. By leveraging the advanced capabilities of Large Language Models (LLMs) in Natural Language Understanding (NLU) and text generation, we propose a novel architecture utilizing LLMs finetuned with LoRA to transform the claim, claimer and claim object detection task into a Question Answering (QA) setting. We evaluate our approach in a dataset of 867 scientific news articles of 3 domains (Health, Climate Change, Nutrition) (HCN), which are human annotated with the major claim, the claimer and the object of the major claim. We also evaluate our proposed model in the benchmark dataset of NEWSCLAIMS. Experimental and qualitative results showcase the effectiveness of the proposed approach. We make our dataset publicly available to encourage further research.</abstract>
      <url hash="3c7f19ca">2024.eacl-long.156</url>
      <bibkey>kotitsas-etal-2024-leveraging</bibkey>
    </paper>
    <paper id="157">
      <title>Should <fixed-case>I</fixed-case> try multiple optimizers when fine-tuning a pre-trained Transformer for <fixed-case>NLP</fixed-case> tasks? Should <fixed-case>I</fixed-case> tune their hyperparameters?</title>
      <author><first>Nefeli</first><last>Gkouti</last></author>
      <author><first>Prodromos</first><last>Malakasiotis</last><affiliation>Athens University of Economics and Business and Workable</affiliation></author>
      <author><first>Stavros</first><last>Toumpis</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <pages>2555-2574</pages>
      <abstract>NLP research has explored different neural model architectures and sizes, datasets, training objectives, and transfer learning techniques. However, the choice of optimizer during training has not been explored as extensively. Typically, some variant of Stochastic Gradient Descent (SGD) is employed, selected among numerous variants, using unclear criteria, often with minimal or no tuning of the optimizer’s hyperparameters. Experimenting with five GLUE datasets, two models (DistilBERT and DistilRoBERTa), and seven popular optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound), we find that when the hyperparameters of the optimizers are tuned, there is no substantial difference in test performance across the five more elaborate (adaptive) optimizers, despite differences in training loss. Furthermore, tuning just the learning rate is in most cases as good as tuning all the hyperparameters. Hence, we recommend picking any of the best-behaved adaptive optimizers (e.g., Adam) and tuning only its learning rate. When no hyperparameter can be tuned, SGD with Momentum is the best choice.</abstract>
      <url hash="128cc1d3">2024.eacl-long.157</url>
      <bibkey>gkouti-etal-2024-try</bibkey>
    </paper>
    <paper id="158">
      <title><fixed-case>GUM</fixed-case>sley: Evaluating Entity Salience in Summarization for 12 <fixed-case>E</fixed-case>nglish Genres</title>
      <author><first>Jessica</first><last>Lin</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Amir</first><last>Zeldes</last><affiliation>Georgetown University</affiliation></author>
      <pages>2575-2588</pages>
      <abstract>As NLP models become increasingly capable of understanding documents in terms of coherent entities rather than strings, obtaining the most salient entities for each document is not only an important end task in itself but also vital for Information Retrieval (IR) and other downstream applications such as controllable summarization. In this paper, we present and evaluate GUMsley, the first entity salience dataset covering all named and non-named salient entities for 12 genres of English text, aligned with entity types, Wikification links and full coreference resolution annotations. We promote a strict definition of salience using human summaries and demonstrate high inter-annotator agreement for salience based on whether a source entity is mentioned in the summary. Our evaluation shows poor performance by pre-trained SOTA summarization models and zero-shot LLM prompting in capturing salient entities in generated summaries. We also show that predicting or providing salient entities to several model architectures enhances performance and helps derive higher-quality summaries by alleviating the entity hallucination problem in existing abstractive summarization.</abstract>
      <url hash="1ee26f48">2024.eacl-long.158</url>
      <bibkey>lin-zeldes-2024-gumsley</bibkey>
    </paper>
    <paper id="159">
      <title>Sensitivity, Performance, Robustness: Deconstructing the Effect of Sociodemographic Prompting</title>
      <author><first>Tilman</first><last>Beck</last></author>
      <author><first>Hendrik</first><last>Schuff</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>2589-2615</pages>
      <abstract>Annotators’ sociodemographic backgrounds (i.e., the individual compositions of their gender, age, educational background, etc.) have a strong impact on their decisions when working on subjective NLP tasks, such as toxic language detection. Often, heterogeneous backgrounds result in high disagreements. To model this variation, recent work has explored sociodemographic prompting, a technique, which steers the output of prompt-based models towards answers that humans with specific sociodemographic profiles would give. However, the available NLP literature disagrees on the efficacy of this technique — it remains unclear for which tasks and scenarios it can help, and the role of the individual factors in sociodemographic prompting is still unexplored. We address this research gap by presenting the largest and most comprehensive study of sociodemographic prompting today. We use it to analyze its influence on model sensitivity, performance and robustness across seven datasets and six instruction-tuned model families. We show that sociodemographic information affects model predictions and can be beneficial for improving zero-shot learning in subjective NLP tasks.However, its outcomes largely vary for different model types, sizes, and datasets, and are subject to large variance with regards to prompt formulations. Most importantly, our results show that sociodemographic prompting should be used with care when used for data annotation or studying LLM alignment.</abstract>
      <url hash="b5ed6d7b">2024.eacl-long.159</url>
      <bibkey>beck-etal-2024-sensitivity</bibkey>
    </paper>
    <paper id="160">
      <title>Threat Behavior Textual Search by Attention Graph Isomorphism</title>
      <author><first>Chanwoo</first><last>Bae</last><affiliation>Purdue University</affiliation></author>
      <author><first>Guanhong</first><last>Tao</last><affiliation>Purdue University</affiliation></author>
      <author><first>Zhuo</first><last>Zhang</last></author>
      <author><first>Xiangyu</first><last>Zhang</last><affiliation>, Purdue University</affiliation></author>
      <pages>2616-2630</pages>
      <abstract>Cyber attacks cause over $1 trillion loss every year. An important task for cyber security analysts is attack forensics. It entails understanding malware behaviors and attack origins. However, existing automated or manual malware analysis can only disclose a subset of behaviors due to inherent difficulties (e.g., malware cloaking and obfuscation). As such, analysts often resort to text search techniques to identify existing malware reports based on the symptoms they observe, exploiting the fact that malware samples share a lot of similarity, especially those from the same origin. In this paper, we propose a novel malware behavior search technique that is based on graph isomorphism at the attention layers of Transformer models. We also compose a large dataset collected from various agencies to facilitate such research.Our technique outperforms state-of-the-art methods, such as those based on sentence embeddings and keywords by 6-14%. In the case study of 10 real-world malwares, our technique can correctly attribute 8 of them to their ground truth origins while using Google only works for 3 cases.</abstract>
      <url hash="25fc337d">2024.eacl-long.160</url>
      <bibkey>bae-etal-2024-threat</bibkey>
    </paper>
    <paper id="161">
      <title>Identifying Narrative Content in Podcast Transcripts</title>
      <author><first>Yosra</first><last>Abdessamed</last></author>
      <author><first>Shadi</first><last>Rezapour</last><affiliation>Drexel University</affiliation></author>
      <author><first>Steven</first><last>Wilson</last><affiliation>Oakland University (Michigan)</affiliation></author>
      <pages>2631-2643</pages>
      <abstract>As one of the oldest forms of human communication, narratives appear across a variety of genres and media. Computational methods have been applied to study narrativity in novels, social media, and patient records, leading to new approaches and insights. However, other types of media are growing in popularity, like podcasts. Podcasts contain a multitude of spoken narratives that can provide a meaningful glimpse into how people share stories with one another.In this paper, we outline and apply methods to process English-language podcast transcripts and extract narrative content from conversations within each episode. We provide an initial analysis of the types of narrative content that exists within a wide range of podcasts, and compare our results to other established narrative analysis tools.Our annotations for narrativity and pretrained models can help to enable future research into narrativity within a large corpus of approximately 100,000 podcast episodes.</abstract>
      <url hash="f3e8d18b">2024.eacl-long.161</url>
      <bibkey>abdessamed-etal-2024-identifying</bibkey>
    </paper>
    <paper id="162">
      <title>Frequency Explains the Inverse Correlation of Large Language Models’ Size, Training Data Amount, and Surprisal’s Fit to Reading Times</title>
      <author><first>Byung-Doh</first><last>Oh</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Shisen</first><last>Yue</last></author>
      <author><first>William</first><last>Schuler</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>2644-2663</pages>
      <abstract>Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able to accurately predict rare words based on both an effectively longer context window size as well as stronger local associations compared to smaller model variants. Taken together, these results indicate that Transformer-based language models’ surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.</abstract>
      <url hash="20fa4d2d">2024.eacl-long.162</url>
      <bibkey>oh-etal-2024-frequency</bibkey>
    </paper>
    <paper id="163">
      <title>Presentations by the Humans and For the Humans: Harnessing <fixed-case>LLM</fixed-case>s for Generating Persona-Aware Slides from Documents</title>
      <author><first>Ishani</first><last>Mondal</last></author>
      <author><first>Shwetha</first><last>S</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Anandhavelu</first><last>Natarajan</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Aparna</first><last>Garimella</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Sambaran</first><last>Bandyopadhyay</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Jordan</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>2664-2684</pages>
      <abstract>Scientific papers and slides are two different representations of the same underlying information, but both require substantial work to prepare. While there had been prior efforts on automating document-to-slides generation, there is still a pressing need of customizing the presentation of content aligning with the persona of target audience or duration of presentation. This paper first introduces the concept of end-user specification-aware document to slides conversion that incorporates end-user specifications into the conversion process. For this, we initially introduce a new dataset reuse the existing SciDuet dataset consisting of pairs of papers and corresponding slides decks from recent years’ *ACL conferences to create four persona-aware configurations. Secondly, we present Persona-Aware-D2S, a novel approach by finetuning LLMs using target audience feedback to create persona-aware slides from scientific documents. Our evaluation on both automated metrics and qualitative human evaluation suggests that by incorporating end-user specifications into the conversion process, our model can create presentations that are not only informative but also tailored to expectations and cognitive abilities of target audience.</abstract>
      <url hash="5c312eac">2024.eacl-long.163</url>
      <bibkey>mondal-etal-2024-presentations</bibkey>
    </paper>
    <paper id="164">
      <title><fixed-case>T</fixed-case>o<fixed-case>P</fixed-case>ro: Token-Level Prompt Decomposition for Cross-Lingual Sequence Labeling Tasks</title>
      <author><first>Bolei</first><last>Ma</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Ercong</first><last>Nie</last></author>
      <author><first>Shuzhou</first><last>Yuan</last></author>
      <author><first>Helmut</first><last>Schmid</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Michael</first><last>Färber</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Frauke</first><last>Kreuter</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>2685-2702</pages>
      <abstract>Prompt-based methods have been successfully applied to multilingual pretrained language models for zero-shot cross-lingual understanding. However, most previous studies primarily focused on sentence-level classification tasks, and only a few considered token-level labeling tasks such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose Token-Level Prompt Decomposition (ToPro), which facilitates the prompt-based method for token-level sequence labeling tasks. The ToPro method decomposes an input sentence into single tokens and applies one prompt template to each token. Our experiments on multilingual NER and POS tagging datasets demonstrate that ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning in zero-shot cross-lingual transfer, especially for languages that are typologically different from the source language English. Our method also attains state-of-the-art performance when employed with the mT5 model. Besides, our exploratory study in multilingual large language models shows that ToPro performs much better than the current in-context learning method. Overall, the performance improvements show that ToPro could potentially serve as a novel and simple benchmarking method for sequence labeling tasks.</abstract>
      <url hash="744e06d3">2024.eacl-long.164</url>
      <bibkey>ma-etal-2024-topro</bibkey>
      <revision id="1" href="2024.eacl-long.164v1" hash="fef9e762"/>
      <revision id="2" href="2024.eacl-long.164v2" hash="744e06d3" date="2024-03-25">Minor updates.</revision>
    </paper>
    <paper id="165">
      <title>Small Language Models Improve Giants by Rewriting Their Outputs</title>
      <author><first>Giorgos</first><last>Vernikos</last></author>
      <author><first>Arthur</first><last>Brazinskas</last></author>
      <author><first>Jakub</first><last>Adamek</last><affiliation>Google Research</affiliation></author>
      <author><first>Jonathan</first><last>Mallinson</last><affiliation>Research, Google</affiliation></author>
      <author><first>Aliaksei</first><last>Severyn</last></author>
      <author><first>Eric</first><last>Malmi</last><affiliation>Google</affiliation></author>
      <pages>2703-2718</pages>
      <abstract>Despite the impressive performance of large language models (LLMs), theyoften lag behind specialized models in various tasks. LLMs only use a fractionof the existing training data for in-context learning, while task-specificmodels harness the full dataset for fine-tuning. In this work, we tackle theproblem of leveraging training data to improve the performance of LLMs withoutfine-tuning. Our approach directly targets LLM predictions without requiringaccess to their weights. We create a pool of candidates from the LLM throughfew-shot prompting and we employ a compact model, the LM-corrector (LMCor),specifically trained to merge these candidates to produce an enhanced output.Our experiments on four natural language generation tasks demonstrate that evena small LMCor model (250M) substantially improves the few-shot performance ofLLMs (62B), matching and even outperforming standard fine-tuning. Furthermore,we illustrate the robustness of LMCor against different prompts, therebyminimizing the need for extensive prompt engineering. Finally, we show thatLMCor can be seamlessly integrated with different LLMs at inference, serving asa plug-and-play module to improve their performance.</abstract>
      <url hash="32cd8b0f">2024.eacl-long.165</url>
      <bibkey>vernikos-etal-2024-small</bibkey>
    </paper>
    <paper id="166">
      <title>Unintended Bias Detection and Mitigation in Misogynous Memes</title>
      <author><first>Gitanjali</first><last>Kumari</last></author>
      <author><first>Anubhav</first><last>Sinha</last></author>
      <author><first>Asif</first><last>Ekbal</last><affiliation>IIT Patna</affiliation></author>
      <pages>2719-2733</pages>
      <abstract>Online sexism has become a concerning issue in recent years, especially conveyed through memes. Although this alarming phenomenon has triggered many studies from computational linguistic and natural language processing points of view, less effort has been spent analyzing if those misogyny detection models are affected by an unintended bias. Such biases can lead models to incorrectly label non-misogynous memes misogynous due to specific identity terms, perpetuating harmful stereotypes and reinforcing negative attitudes. This paper presents the first and most comprehensive approach to measure and mitigate unintentional bias in the misogynous memes detection model, aiming to develop effective strategies to counter their harmful impact. Our proposed model, the <b>C</b>on<b>t</b>e<b>x</b>tualized <b>S</b>cene <b>G</b>raph-based <b>M</b>ultimodal <b>Net</b>work (CTXSGMNet), is an integrated architecture that combines VisualBERT, a CLIP-LSTM-based memory network, and an unbiased scene graph module with supervised contrastive loss, achieves state-of-the-art performance in mitigating unintentional bias in misogynous memes.Empirical evaluation, including both qualitative and quantitative analysis, demonstrates the effectiveness of our CTXSGMNet framework on the SemEval-2022 Task 5 (<b>MAMI</b> task) dataset, showcasing its promising performance in terms of Equity of Odds and F1 score. Additionally, we assess the generalizability of the proposed model by evaluating their performance on a few benchmark meme datasets, providing a comprehensive understanding of our approach’s efficacy across diverse datasets.</abstract>
      <url hash="c5645799">2024.eacl-long.166</url>
      <bibkey>kumari-etal-2024-unintended</bibkey>
    </paper>
    <paper id="167">
      <title>A Weak Supervision Approach for Few-Shot Aspect Based Sentiment Analysis</title>
      <author><first>Robert</first><last>Vacareanu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Siddharth</first><last>Varia</last></author>
      <author><first>Kishaloy</first><last>Halder</last><affiliation>Amazon</affiliation></author>
      <author><first>Shuai</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Giovanni</first><last>Paolini</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Neha</first><last>Anna John</last></author>
      <author><first>Miguel</first><last>Ballesteros</last><affiliation>Amazon</affiliation></author>
      <author><first>Smaranda</first><last>Muresan</last><affiliation>Amazon and Columbia University</affiliation></author>
      <pages>2734-2752</pages>
      <abstract>We explore how weak supervision on abundant unlabeled data can be leveraged to improve few-shot performance in aspect-based sentiment analysis (ABSA) tasks. We propose a pipeline approach to construct a noisy ABSA dataset, and we use it to adapt a pre-trained sequence-to-sequence model to the ABSA tasks. We test the resulting model on three widely used ABSA datasets, before and after fine-tuning. Our proposed method preserves the full fine-tuning performance while showing significant improvements (15.84 absolute F1) in the few-shot learning scenario for the harder tasks. In zero-shot (i.e., without fine-tuning), our method outperforms the previous state of the art on the aspect extraction sentiment classification (AESC) task and is, additionally, capable of performing the harder aspect sentiment triplet extraction (ASTE) task.</abstract>
      <url hash="08c07ded">2024.eacl-long.167</url>
      <bibkey>vacareanu-etal-2024-weak</bibkey>
    </paper>
    <paper id="168">
      <title>Counterfactual Reasoning with Knowledge Graph Embeddings</title>
      <author><first>Lena</first><last>Zellinger</last><affiliation>Universität Vienna</affiliation></author>
      <author><first>Andreas</first><last>Stephan</last></author>
      <author><first>Benjamin</first><last>Roth</last><affiliation>Universität Vienna</affiliation></author>
      <pages>2753-2772</pages>
      <abstract>Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories.In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR. We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules. We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained. We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark. Our results indicate that KGEs learn patterns in the graph without explicit training. We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow these patterns. An evaluation on human-annotated data reveals that KGEs adapted with COULDD are mostly unable to recognize changes to the graph that do not follow learned inference rules. In contrast, ChatGPT mostly outperforms KGEs in detecting plausible changes to the graph but has poor knowledge retention. In summary, CFKGR connects two previously distinct areas, namely KG completion and counterfactual reasoning.</abstract>
      <url hash="3ed63c53">2024.eacl-long.168</url>
      <attachment type="software" hash="f1098e07">2024.eacl-long.168.software.zip</attachment>
      <attachment type="note" hash="ab458e85">2024.eacl-long.168.note.zip</attachment>
      <bibkey>zellinger-etal-2024-counterfactual</bibkey>
    </paper>
    <paper id="169">
      <title>System-Level Natural Language Feedback</title>
      <author><first>Weizhe</first><last>Yuan</last></author>
      <author><first>Kyunghyun</first><last>Cho</last><affiliation>Genentech and New York University</affiliation></author>
      <author><first>Jason</first><last>Weston</last><affiliation>New York University and Facebook</affiliation></author>
      <pages>2773-2789</pages>
      <abstract>Natural language (NL) feedback offers rich insights into user experience. While existing studies focus on an instance-level approach, where feedback is used to refine specific examples, we introduce a framework for system-level use of NL feedback. We show how to use feedback to formalize system-level design decisions in a human-in-the-loop-process – in order to produce better models. In particular this is done through: (i) metric design for tasks; and (ii) language model prompt design for refining model responses. We conduct two case studies of this approach for improving search query and dialog response generation, demonstrating the effectiveness of system-level feedback. We show the combination of system-level and instance-level feedback brings further gains, and that human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying the importance of human feedback for building systems.</abstract>
      <url hash="8e1c4adb">2024.eacl-long.169</url>
      <bibkey>yuan-etal-2024-system</bibkey>
    </paper>
    <paper id="170">
      <title>Syntactic Preposing and Discourse Relations</title>
      <author><first>Yunfang</first><last>Dong</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Xixian</first><last>Liao</last></author>
      <author><first>Bonnie</first><last>Webber</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>2790-2802</pages>
      <abstract>Over 15 years ago, Ward &amp; Birner (2006) suggested that non-canonical constructions in English can serve both to mark information status and to structure the information flow of discourse. One such construction is preposing, where a phrasal constituent appears to the left of its canonical position, typically sentence-initially. But computational work on discourse has, to date, ignored non-canonical syntax. We take account of non-canonical syntax by providing quantitative evidence relating NP/PP preposing to discourse relations. The evidence comes from an LLM mask-filling task that compares the predictions when a mask is inserted between the arguments of an implicit inter-sentential discourse relation — first, when the right-hand argument (Arg2) starts with a preposed constituent, and again, when that constituent is in canonical (post-verbal) position. Results show that (1) the top-ranked mask-fillers in the preposed case agree more often with “gold” annotations in the Penn Discourse TreeBank than they do in the latter case, and (2) preposing in Arg2 can affect the distribution of discourse-relational senses.</abstract>
      <url hash="a8aa0d72">2024.eacl-long.170</url>
      <bibkey>dong-etal-2024-syntactic</bibkey>
    </paper>
    <paper id="171">
      <title>Can we obtain significant success in <fixed-case>RST</fixed-case> discourse parsing by using Large Language Models?</title>
      <author><first>Aru</first><last>Maekawa</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Tsutomu</first><last>Hirao</last><affiliation>NTT Communication Science Laboratories</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Division of Information Science, Nara Institute of Science and Technology</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>2803-2815</pages>
      <abstract>Recently, decoder-only pre-trained large language models (LLMs), with several tens of billion parameters, have significantly impacted a wide range of natural language processing (NLP) tasks. While encoder-only or encoder-decoder pre-trained language models have already proved to be effective in discourse parsing, the extent to which LLMs can perform this task remains an open research question. Therefore, this paper explores how beneficial such LLMs are for Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing process for both fundamental top-down and bottom-up strategies is converted into prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with QLoRA, which has fewer parameters that can be tuned. Experimental results on three benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate that Llama 2 with 70 billion parameters in the bottom-up strategy obtained state-of-the-art (SOTA) results with significant differences. Furthermore, our parsers demonstrated generalizability when evaluated on RST-DT, showing that, in spite of being trained with the GUM corpus, it obtained similar performances to those of existing parsers trained with RST-DT.</abstract>
      <url hash="58708507">2024.eacl-long.171</url>
      <bibkey>maekawa-etal-2024-obtain</bibkey>
    </paper>
    <paper id="172">
      <title>Ameli: Enhancing Multimodal Entity Linking with Fine-Grained Attributes</title>
      <author><first>Barry</first><last>Yao</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Sijia</first><last>Wang</last></author>
      <author><first>Yu</first><last>Chen</last><affiliation>Anytime.AI</affiliation></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Minqian</first><last>Liu</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Zhiyang</first><last>Xu</last></author>
      <author><first>Licheng</first><last>Yu</last><affiliation>Meta AI</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>Virginia Tech</affiliation></author>
      <pages>2816-2834</pages>
      <abstract>We propose attribute-aware multimodal entity linking, where the input consists of a mention described with a text paragraph and images, and the goal is to predict the corresponding target entity from a multimodal knowledge base (KB) where each entity is also accompanied by a text description, visual images, and a collection of attributes that present the meta-information of the entity in a structured format. To facilitate this research endeavor, we construct Ameli, encompassing a new multimodal entity linking benchmark dataset that contains 16,735 mentions described in text and associated with 30,472 images, and a multimodal knowledge base that covers 34,690 entities along with 177,873 entity images and 798,216 attributes. To establish baseline performance on Ameli, we experiment with several state-of-the-art architectures for multimodal entity linking and further propose a new approach that incorporates attributes of entities into disambiguation. Experimental results and extensive qualitative analysis demonstrate that extracting and understanding the attributes of mentions from their text descriptions and visual images play a vital role in multimodal entity linking. To the best of our knowledge, we are the first to integrate attributes in the multimodal entity linking task. The programs, model checkpoints, and the dataset are publicly available at https://github.com/VT-NLP/Ameli.</abstract>
      <url hash="2e8a4ccc">2024.eacl-long.172</url>
      <bibkey>yao-etal-2024-ameli</bibkey>
    </paper>
    <paper id="173">
      <title>Generative Dense Retrieval: Memory Can Be a Burden</title>
      <author><first>Peiwen</first><last>Yuan</last></author>
      <author><first>Xinglin</first><last>Wang</last></author>
      <author><first>Shaoxiong</first><last>Feng</last></author>
      <author><first>Boyuan</first><last>Pan</last></author>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Heda</first><last>Wang</last></author>
      <author><first>Xupeng</first><last>Miao</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>2835-2845</pages>
      <abstract>Generative Retrieval (GR), autoregressively decoding relevant document identifiers given a query, has been shown to perform well under the setting of small-scale corpora. By memorizing the document corpus with model parameters, GR implicitly achieves deep interaction between query and document. However, such a memorizing mechanism faces three drawbacks: (1) Poor memory accuracy for fine-grained features of documents; (2) Memory confusion gets worse as the corpus size increases; (3) Huge memory update costs for new documents. To alleviate these problems, we propose the Generative Dense Retrieval (GDR) paradigm. Specifically, GDR first uses the limited memory volume to achieve inter-cluster matching from query to relevant document clusters. Memorizing-free matching mechanism from Dense Retrieval (DR) is then introduced to conduct fine-grained intra-cluster matching from clusters to relevant documents. The coarse-to-fine process maximizes the advantages of GR’s deep interaction and DR’s scalability. Besides, we design a cluster identifier constructing strategy to facilitate corpus memory and a cluster-adaptive negative sampling strategy to enhance the intra-cluster mapping ability. Empirical results show that GDR obtains an average of 3.0 R@100 improvement on NQ dataset under multiple settings and has better scalability.</abstract>
      <url hash="77762d95">2024.eacl-long.173</url>
      <attachment type="note" hash="95b8af25">2024.eacl-long.173.note.zip</attachment>
      <bibkey>yuan-etal-2024-generative</bibkey>
    </paper>
    <paper id="174">
      <title>Backward Compatibility During Data Updates by Weight Interpolation</title>
      <author><first>Raphael</first><last>Schumann</last></author>
      <author><first>Elman</first><last>Mansimov</last><affiliation>Amazon</affiliation></author>
      <author><first>Yi-An</first><last>Lai</last></author>
      <author><first>Nikolaos</first><last>Pappas</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Xibin</first><last>Gao</last><affiliation>Amazon AWS AI Labs</affiliation></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <pages>2846-2861</pages>
      <abstract>Backward compatibility of model predictions is a desired property when updating a machine learning driven application. It allows to seamlessly improve the underlying model without introducing regression bugs. In classification tasks these bugs occur in the form of negative flips. This means an instance that was correctly classified by the old model is now classified incorrectly by the updated model. This has direct negative impact on the user experience of such systems e.g. a frequently used voice assistant query is suddenly misclassified.A common reason to update the model is when new training data becomes available and needs to be incorporated. Simply retraining the model with the updated data introduces the unwanted negative flips. We study the problem of regression during data updates and propose Backward Compatible Weight Interpolation (BCWI). This method interpolates between the weights of the old and new model and we show in extensive experiments that it reduces negative flips without sacrificing the improved accuracy of the new model. BCWI is straight forward to implement and does not increase inference cost. We also explore the use of importance weighting during interpolation and averaging the weights of multiple new models in order to further reduce negative flips.</abstract>
      <url hash="6eb9c76f">2024.eacl-long.174</url>
      <attachment type="note" hash="e7f03738">2024.eacl-long.174.note.zip</attachment>
      <bibkey>schumann-etal-2024-backward</bibkey>
    </paper>
    <paper id="175">
      <title>Gradient-Based Language Model Red Teaming</title>
      <author><first>Nevan</first><last>Wichers</last><affiliation>Google</affiliation></author>
      <author><first>Carson</first><last>Denison</last><affiliation>Anthropic</affiliation></author>
      <author><first>Ahmad</first><last>Beirami</last><affiliation>Google Research and Massachusetts Institute of Technology</affiliation></author>
      <pages>2862-2881</pages>
      <abstract>Red teaming is a common strategy for identifying weaknesses in generative language models (LMs) by producing adversarial prompts that trigger models to generate unsafe responses. Red teaming is instrumental for both model alignment and evaluation, but is labor-intensive and difficult to scale when done by humans. In this paper, we present Gradient-Based Red Teaming (GBRT), a novel red teaming method for automatically generating diverse prompts that are likely to cause an LM to output unsafe responses. GBRT is a form of prompt learning, trained by scoring an LM response with a safety classifier and then backpropagating through the frozen safety classifier and LM to update the prompt. To improve the coherence of input prompts, we introduce two variants that add a realism loss and fine-tune a pretrained model to generate the prompts instead of learning the prompts directly. Our experiments show that GBRT is more effective at finding prompts that trigger an LM to generate unsafe responses than a strong reinforcement learning-based red teaming approach and works even when the LM has been fine-tuned to produce safer outputs.</abstract>
      <url hash="5265c268">2024.eacl-long.175</url>
      <bibkey>wichers-etal-2024-gradient</bibkey>
    </paper>
    <paper id="176">
      <title>Do Moral Judgment and Reasoning Capability of <fixed-case>LLM</fixed-case>s Change with Language? A Study using the Multilingual Defining Issues Test</title>
      <author><first>Aditi</first><last>Khandelwal</last></author>
      <author><first>Utkarsh</first><last>Agarwal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Kumar</first><last>Tanmay</last><affiliation>Microsoft</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>2882-2894</pages>
      <abstract>This paper explores the moral judgment and moral reasoning abilities exhibited by Large Language Models (LLMs) across languages through the Defining Issues Test. It is a well known fact that moral judgment depends on the language in which the question is asked. We extend the work of beyond English, to 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe three LLMs – ChatGPT, GPT-4 and Llama2Chat-70B – that shows substantial multilingual text processing and generation abilities. Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages. The moral judgments too vary considerably by the language.</abstract>
      <url hash="8e597980">2024.eacl-long.176</url>
      <bibkey>khandelwal-etal-2024-moral</bibkey>
    </paper>
    <paper id="177">
      <title>Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models</title>
      <author><first>Sara</first><last>Rajaee</last></author>
      <author><first>Christof</first><last>Monz</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <pages>2895-2914</pages>
      <abstract>Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks of existing cross-lingual test data and evaluation setups, calling for a more nuanced understanding of the cross-lingual capabilities of multilingual models.</abstract>
      <url hash="d35f8b99">2024.eacl-long.177</url>
      <bibkey>rajaee-monz-2024-analyzing</bibkey>
    </paper>
    <paper id="178">
      <title>Large-Scale Label Interpretation Learning for Few-Shot Named Entity Recognition</title>
      <author><first>Jonas</first><last>Golde</last><affiliation>Department of Computer Science, Humboldt University Berlin, Humboldt Universität Berlin</affiliation></author>
      <author><first>Felix</first><last>Hamborg</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <pages>2915-2930</pages>
      <abstract>Few-shot named entity recognition (NER) detects named entities within text using only a few annotated examples. One promising line of research is to leverage natural language descriptions of each entity type: the common label PER might, for example, be verbalized as ”person entity.” In an initial label interpretation learning phase, the model learns to interpret such verbalized descriptions of entity types. In a subsequent few-shot tagset extension phase, this model is then given a description of a previously unseen entity type (such as ”music album”) and optionally a few training examples to perform few-shot NER for this type. In this paper, we systematically explore the impact of a strong semantic prior to interpret verbalizations of new entity types by massively scaling up the number and granularity of entity types used for label interpretation learning. To this end, we leverage an entity linking benchmark to create a dataset with orders of magnitude of more distinct entity types and descriptions as currently used datasets. We find that this increased signal yields strong results in zero- and few-shot NER in in-domain, cross-domain, and even cross-lingual settings. Our findings indicate significant potential for improving few-shot NER through heuristical data-based optimization.</abstract>
      <url hash="d0da8823">2024.eacl-long.178</url>
      <bibkey>golde-etal-2024-large</bibkey>
    </paper>
    <paper id="179">
      <title><fixed-case>MLC</fixed-case>opilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks</title>
      <author><first>Lei</first><last>Zhang</last></author>
      <author><first>Yuge</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Kan</first><last>Ren</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dongsheng</first><last>Li</last></author>
      <author><first>Yuqing</first><last>Yang</last><affiliation>Research, Microsoft</affiliation></author>
      <pages>2931-2959</pages>
      <abstract>The field of machine learning (ML) has gained widespread adoption, leading to significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time-consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art large language models to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the existing experiences of ML tasks and (ii) reason effectively to deliver promising results for new tasks. The solution generated can be used directly to achieve high levels of competitiveness.</abstract>
      <url hash="24fa996c">2024.eacl-long.179</url>
      <bibkey>zhang-etal-2024-mlcopilot</bibkey>
    </paper>
    <paper id="180">
      <title>Text-Guided Image Clustering</title>
      <author><first>Andreas</first><last>Stephan</last></author>
      <author><first>Lukas</first><last>Miklautz</last></author>
      <author><first>Kevin</first><last>Sidak</last><affiliation>Universität Vienna</affiliation></author>
      <author><first>Jan Philip</first><last>Wahle</last><affiliation>University of Göttingen, Germany</affiliation></author>
      <author><first>Bela</first><last>Gipp</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Claudia</first><last>Plant</last><affiliation>Universität Vienna</affiliation></author>
      <author><first>Benjamin</first><last>Roth</last><affiliation>Universität Vienna</affiliation></author>
      <pages>2960-2976</pages>
      <abstract>Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations. Those are usually in the form of text, begging the question of using text as an abstraction for image clustering. Current image clustering methods, however, neglect the use of generated textual descriptions. We, therefore, propose <i>Text-Guided Image Clustering</i>, i.e., generating text using image captioning and visual question-answering (VQA) models and subsequently clustering the generated text. Further, we introduce a novel approach to inject task- or domain knowledge for clustering by prompting VQA models. Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features. Additionally, we propose a counting-based cluster explainability method. Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests. Overall, this research challenges traditional approaches and paves the way for a paradigm shift in image clustering, using generated text.</abstract>
      <url hash="dab1dd8e">2024.eacl-long.180</url>
      <attachment type="software" hash="6fd49102">2024.eacl-long.180.software.zip</attachment>
      <bibkey>stephan-etal-2024-text</bibkey>
    </paper>
    <paper id="181">
      <title><fixed-case>CCP</fixed-case>refix: Counterfactual Contrastive Prefix-Tuning for Many-Class Classification</title>
      <author><first>Yang</first><last>Li</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Canran</first><last>Xu</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Guodong</first><last>Long</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Tao</first><last>Shen</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Chongyang</first><last>Tao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>University of Technology Sydney</affiliation></author>
      <pages>2977-2988</pages>
      <abstract>Recently, prefix-tuning was proposed to efficiently adapt pre-trained language models to a broad spectrum of natural language classification tasks. It leverages soft prefix as task-specific indicators and language verbalizers as categorical-label mentions to narrow the formulation gap from pre-training language models. However, when the label space increases considerably (i.e., many-class classification), such a tuning technique suffers from a verbalizer ambiguity problem since the many-class labels are represented by semantic-similar verbalizers in short language phrases. To overcome this, inspired by the human-decision process that the most ambiguous classes would be mulled over for an instance, we propose a brand-new prefix-tuning method, Counterfactual Contrastive Prefix-tuning (CCPrefix), for many-class classification. Basically, an instance-dependent soft prefix, derived from fact-counterfactual pairs in the label space, is leveraged to complement the language verbalizers in many-class classification. We conduct experiments on many-class benchmark datasets in both the fully supervised setting and the few-shot setting, which indicates that our model outperforms former baselines.</abstract>
      <url hash="67c2e72e">2024.eacl-long.181</url>
      <attachment type="software" hash="a0555e87">2024.eacl-long.181.software.zip</attachment>
      <attachment type="note" hash="bf7b8808">2024.eacl-long.181.note.zip</attachment>
      <bibkey>li-etal-2024-ccprefix</bibkey>
    </paper>
  </volume>
  <volume id="short" ingest-date="2024-03-03" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers)</booktitle>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Matthew</first><last>Purver</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St. Julian’s, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="75a70124">2024.eacl-short</url>
      <venue>eacl</venue>
    </meta>
    <frontmatter>
      <url hash="54034346">2024.eacl-short.0</url>
      <bibkey>eacl-2024-european-chapter-association-linguistics-2</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>F</fixed-case>rench <fixed-case>G</fixed-case>ossip<fixed-case>P</fixed-case>rompts: Dataset For Prevention of Generating <fixed-case>F</fixed-case>rench Gossip Stories By <fixed-case>LLM</fixed-case>s</title>
      <author><first>Msvpj</first><last>Sathvik</last><affiliation>Raickers AI</affiliation></author>
      <author><first>Abhilash</first><last>Dowpati</last></author>
      <author><first>Revanth</first><last>Narra</last></author>
      <pages>1-7</pages>
      <abstract>The realm of Large Language Models (LLMs) is undergoing a continuous and dynamic transformation. These state-of-the-art LLMs showcase an impressive ability to craft narratives based on contextual cues, highlighting their skill in comprehending and producing text resembling human writing. However, there exists a potential risk: the potential inclination of LLMs to create gossips when prompted with specific contexts. These LLMs possess the capacity to generate stories rooted in the context provided by the prompts. Yet, this very capability carries a risk of generating gossips given the context as input. To mitigate this, we introduce a dataset named “French GossipPrompts” designed for identifying prompts that lead to the creation of gossipy content in the French language. This dataset employs binary classification, categorizing whether a given prompt generates gossip or not. The dataset comprises a total of 7253 individual prompts. We have developed classification models and achieved an accuracy of 89.95%.</abstract>
      <url hash="9e6f7be2">2024.eacl-short.1</url>
      <bibkey>sathvik-etal-2024-french</bibkey>
    </paper>
    <paper id="2">
      <title>More Discriminative Sentence Embeddings via Semantic Graph Smoothing</title>
      <author><first>Chakib</first><last>Fettal</last></author>
      <author><first>Lazhar</first><last>Labiod</last><affiliation>parisdescartes.fr</affiliation></author>
      <author><first>Mohamed</first><last>Nadif</last><affiliation>Centre Borelli</affiliation></author>
      <pages>8-13</pages>
      <abstract>This paper explores an empirical approach to learn more discriminantive sentence representations in an unsupervised fashion. Leveraging semantic graph smoothing, we enhance sentence embeddings obtained from pretrained models to improve results for the text clustering and classification tasks. Our method, validated on eight benchmarks, demonstrates consistent improvements, showcasing the potential of semantic graph smoothing in improving sentence embeddings for the supervised and unsupervised document categorization tasks.</abstract>
      <url hash="4e7412d7">2024.eacl-short.2</url>
      <bibkey>fettal-etal-2024-discriminative</bibkey>
    </paper>
    <paper id="3">
      <title>Multi-Level Attention Aggregation for Language-Agnostic Speaker Replication</title>
      <author><first>Yejin</first><last>Jeon</last></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>14-20</pages>
      <abstract>This paper explores the task of language-agnostic speaker replication, a novel endeavor that seeks to replicate a speaker’s voice irrespective of the language they are speaking. Towards this end, we introduce a multi-level attention aggregation approach that systematically probes and amplifies various speaker-specific attributes in a hierarchical manner. Through rigorous evaluations across a wide range of scenarios including seen and unseen speakers conversing in seen and unseen lingua, we establish that our proposed model is able to achieve substantial speaker similarity, and is able to generalize to out-of-domain (OOD) cases.</abstract>
      <url hash="97db6d33">2024.eacl-short.3</url>
      <attachment type="software" hash="e7995468">2024.eacl-short.3.software.zip</attachment>
      <bibkey>jeon-lee-2024-multi</bibkey>
      <revision id="1" href="2024.eacl-short.3v1" hash="5a2d7aa2"/>
      <revision id="2" href="2024.eacl-short.3v2" hash="97db6d33" date="2024-03-30">This revision corrects a typo in Equation 1.</revision>
    </paper>
    <paper id="4">
      <title>Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding</title>
      <author><first>Rico</first><last>Sennrich</last><affiliation>University of Zurich and University of Edinburgh</affiliation></author>
      <author><first>Jannis</first><last>Vamvas</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Alireza</first><last>Mohammadshahi</last></author>
      <pages>21-33</pages>
      <abstract>Hallucinations and off-target translation remain unsolved problems in MT, especially for low-resource languages and massively multilingual models. In this paper, we introduce two related methods to mitigate these failure cases with a modified decoding objective, without either requiring retraining or external models. In source-contrastive decoding, we search for a translation that is probable given the correct input, but improbable given a random input segment. In language-contrastive decoding, we search for a translation that is probable, but improbable given the wrong language indicator token. Experiments on the massively multilingual models M2M-100 (418M) and SMaLL-100 show that these methods suppress hallucinations and off-target translations, reducing the number of translations with segment-level chrF2 below 10 by 67-83% on average across 57 tested translation directions. In a proof of concept on out-of-English translation, we also show that we can suppress off-target translations with large language models. We release code upon acceptance.</abstract>
      <url hash="fb365334">2024.eacl-short.4</url>
      <attachment type="software" hash="78e8b523">2024.eacl-short.4.software.zip</attachment>
      <bibkey>sennrich-etal-2024-mitigating</bibkey>
    </paper>
    <paper id="5">
      <title>Injecting <fixed-case>W</fixed-case>iktionary to improve token-level contextual representations using contrastive learning</title>
      <author><first>Anna</first><last>Mosolova</last></author>
      <author><first>Marie</first><last>Candito</last><affiliation>Université de Paris</affiliation></author>
      <author><first>Carlos</first><last>Ramisch</last><affiliation>LIS - Laboratoire d’Informatique et Systèmes and AMU - Aix Marseille University</affiliation></author>
      <pages>34-41</pages>
      <abstract>While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b). In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary. We also test how dimensionality reduction impacts the resulting contextual word embeddings. We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set). We achieve new SoTA result on the original WiC test set. We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements. We also observe improvements, although modest, for the semantic frame induction task. Although we experimented on English to allow comparison with related work, our method is adaptable to the many languages for which large Wiktionaries exist.</abstract>
      <url hash="bcff41b8">2024.eacl-short.5</url>
      <bibkey>mosolova-etal-2024-injecting</bibkey>
    </paper>
    <paper id="6">
      <title>Multilingual Gradient Word-Order Typology from <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Emi</first><last>Baylor</last><affiliation>McGill University</affiliation></author>
      <author><first>Esther</first><last>Ploeger</last></author>
      <author><first>Johannes</first><last>Bjerva</last><affiliation>Aalborg University</affiliation></author>
      <pages>42-49</pages>
      <abstract>While information from the field of linguistic typology has the potential to improve performance on NLP tasks, reliable typological data is a prerequisite. Existing typological databases, including WALS and Grambank, suffer from inconsistencies primarily caused by their categorical format. Furthermore, typological categorisations by definition differ significantly from the continuous nature of phenomena, as found in natural language corpora. In this paper, we introduce a new seed dataset made up of continuous-valued data, rather than categorical data, that can better reflect the variability of language. While this initial dataset focuses on word-order typology, we also present the methodology used to create the dataset, which can be easily adapted to generate data for a broader set of features and languages.</abstract>
      <url hash="128cb4ef">2024.eacl-short.6</url>
      <bibkey>baylor-etal-2024-multilingual</bibkey>
    </paper>
    <paper id="7">
      <title>Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains</title>
      <author><first>Sanjana</first><last>Ramprasad</last></author>
      <author><first>Kundan</first><last>Krishna</last></author>
      <author><first>Zachary</first><last>Lipton</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Byron</first><last>Wallace</last><affiliation>Northeastern University, Brown University and Northeastern University</affiliation></author>
      <pages>50-59</pages>
      <abstract>Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot—i.e., without explicit supervision—that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains?In this work we evaluate zero-shot generated summaries across specialized domains including: biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotations to facilitate additional research toward measuring and realizing factually accurate summarization, beyond news articles (The dataset can be downloaded from https://anonymous.4open.science/r/zero_shot_faceval_domains-9B83)</abstract>
      <url hash="19a8a9a2">2024.eacl-short.7</url>
      <bibkey>ramprasad-etal-2024-evaluating</bibkey>
    </paper>
    <paper id="8">
      <title>Leveraging Implicit Feedback from Deployment Data in Dialogue</title>
      <author><first>Richard Yuanzhe</first><last>Pang</last><affiliation>New York University</affiliation></author>
      <author><first>Stephen</first><last>Roller</last></author>
      <author><first>Kyunghyun</first><last>Cho</last><affiliation>Genentech and New York University</affiliation></author>
      <author><first>He</first><last>He</last><affiliation>New York University</affiliation></author>
      <author><first>Jason</first><last>Weston</last><affiliation>New York University and Facebook</affiliation></author>
      <pages>60-75</pages>
      <abstract>We study improving social conversational agents by learning from natural dialogue between users and a deployed model, without extra annotations. To implicitly measure the quality of a machine-generated utterance, we leverage signals like user response length, sentiment and reaction of the future human utterances in the collected dialogue episodes. Our experiments use the publicly released deployment data from BlenderBot (Xu et al., 2023). Human evaluation indicates improvements in our new models over baseline responses; however, we find that some proxy signals can lead to more generations with undesirable properties as well. For example, optimizing for conversation length can lead to more controversial or unfriendly generations compared to the baseline, whereas optimizing for positive sentiment or reaction can decrease these behaviors.</abstract>
      <url hash="8ce46674">2024.eacl-short.8</url>
      <bibkey>pang-etal-2024-leveraging</bibkey>
    </paper>
    <paper id="9">
      <title>Characterizing the Confidence of Large Language Model-Based Automatic Evaluation Metrics</title>
      <author><first>Rickard</first><last>Stureborg</last><affiliation>Duke University</affiliation></author>
      <author><first>Dimitris</first><last>Alikaniotis</last><affiliation>Grammarly</affiliation></author>
      <author><first>Yoshi</first><last>Suhara</last><affiliation>NVIDIA</affiliation></author>
      <pages>76-89</pages>
      <abstract>There has recently been a growing interest in using Large Language Models (LLMs) to evaluate NLP tasks automatically. Considerable research effort has been put into improving such systems towards achieving high correlations with human judgement. However, it is still unclear what level of correlation is good enough for practical applications of LLM-based automatic evaluation systems. This paper characterizes these LLM evaluators’ confidence in ranking candidate NLP models and develops a configurable Monte Carlo simulation method. We show that even automatic metrics with low correlation with human judgement can reach high-confidence rankings of candidate models with reasonable evaluation set sizes (100s of examples). Further, we describe tradeoff curves between the LLM evaluator performance (i.e., correlation with humans) and evaluation set size; loss in correlation can be compensated with modest increases in the evaluation set size. We validate our results on RoSE, a text summarization dataset, and find our estimates of confidence align with empirical observations.Code available at https://github.com/rickardstureborg/llm-eval-confidence</abstract>
      <url hash="861861e5">2024.eacl-short.9</url>
      <bibkey>stureborg-etal-2024-characterizing</bibkey>
    </paper>
    <paper id="10">
      <title>Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance</title>
      <author><first>Adrian</first><last>Theuma</last><affiliation>Monash University</affiliation></author>
      <author><first>Ehsan</first><last>Shareghi</last><affiliation>Monash University and University of Cambridge</affiliation></author>
      <pages>90-103</pages>
      <abstract>Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. We explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely depending on the LLM’s inherent abilities. More concretely, using financial domain question answering datasets, we apply supervised finetuning on a LLAMA-2 13B CHAT model to act both as a task router and task solver. The task router dynamically directs a question to either be answered internally by the LLM or externally via the right tool from the tool set. Our tool-equipped SFT model, RAVEN, demonstrates an improvement of 35.2% and 5.06% over the base model and SFT-only baselines, respectively, and is highly competitive with strong GPT-3.5 results. To the best of our knowledge, our work is the first that investigates tool augmentation of language models for the finance domain.</abstract>
      <url hash="d60bf33d">2024.eacl-short.10</url>
      <attachment type="note" hash="9e28b2b9">2024.eacl-short.10.note.zip</attachment>
      <bibkey>theuma-shareghi-2024-equipping</bibkey>
    </paper>
    <paper id="11">
      <title>Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement</title>
      <author><first>Hana</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Kai</first><last>Ong</last></author>
      <author><first>Seoyeon</first><last>Kim</last></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <pages>104-123</pages>
      <abstract>Memorizing and utilizing speakers’ personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation.While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.</abstract>
      <url hash="71818cd7">2024.eacl-short.11</url>
      <attachment type="software" hash="a8959d7d">2024.eacl-short.11.software.zip</attachment>
      <bibkey>kim-etal-2024-commonsense</bibkey>
    </paper>
    <paper id="12">
      <title>Investigating the Potential of Task Arithmetic for Cross-Lingual Transfer</title>
      <author><first>Marinela</first><last>Parović</last></author>
      <author><first>Ivan</first><last>Vulić</last><affiliation>University of Cambridge and PolyAI Limited</affiliation></author>
      <author><first>Anna</first><last>Korhonen</last><affiliation>University of Cambridge</affiliation></author>
      <pages>124-137</pages>
      <abstract>Cross-lingual transfer has recently been tackled through modular, parameter-efficient fine-tuning methods which allow arbitrary combinations of language and task modules for transfer of any task to any language. Concurrently, task arithmetic has emerged as a powerful and modular tool for editing pretrained models using multiple full fine-tunings. In this work, we connect the paradigms of task arithmetic and cross-lingual transfer, demonstrating that modularity for cross-lingual transfer can be achieved even with full model fine-tuning. Our approach displays strong performance on a range of multilingual benchmarks encompassing both high-resource and low-resource languages.</abstract>
      <url hash="9c22e854">2024.eacl-short.12</url>
      <bibkey>parovic-etal-2024-investigating</bibkey>
    </paper>
    <paper id="13">
      <title>On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization</title>
      <author><first>Lorenzo Jaime</first><last>Flores</last></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>138-150</pages>
      <abstract>Text summarization and simplification are among the most widely used applications of AI. However, such models are often prone to hallucination, which can result from training models on unaligned data. One efficient approach to address this issue is Loss Truncation (Kang and Hashimoto, 2020), an approach to modify the standard log loss to adaptively remove noisy examples during training. However, we find that LT alone yields a considerable number of hallucinated entities on various datasets. We study the behavior of the underlying losses between factual and non-factual examples, to understand and refine the performance of LT. We demonstrate that LT’s performance is limited when the underlying assumption that noisy targets have higher NLL loss is not satisfied, and find that word-level NLL among entities provides better signal for distinguishing factuality. We then leverage this to propose a fine-grained NLL loss and fine-grained data cleaning strategies, and observe improvements in hallucination reduction across some datasets. Our work is available at https://github.com/yale-nlp/Simplification-Projects.</abstract>
      <url hash="ddfdf9cc">2024.eacl-short.13</url>
      <attachment type="software" hash="8abd1078">2024.eacl-short.13.software.zip</attachment>
      <attachment type="note" hash="5eb8f975">2024.eacl-short.13.note.zip</attachment>
      <bibkey>flores-cohan-2024-benefits</bibkey>
    </paper>
    <paper id="14">
      <title>Evaluating Unsupervised Argument Aligners via Generation of Conclusions of Structured Scientific Abstracts</title>
      <author><first>Yingqiang</first><last>Gao</last></author>
      <author><first>Nianlong</first><last>Gu</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Jessica</first><last>Lam</last><affiliation>Insititute of Neuroinformatics, University of Zurich and ETH Zurich, ETHZ - ETH Zurich</affiliation></author>
      <author><first>James</first><last>Henderson</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Richard</first><last>Hahnloser</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <pages>151-160</pages>
      <abstract>Scientific abstracts provide a concise summary of research findings, making them a valuable resource for extracting scientific arguments. In this study, we assess various unsupervised approaches for extracting arguments as aligned premise-conclusion pairs: semantic similarity, text perplexity, and mutual information. We aggregate structured abstracts from PubMed Central Open Access papers published in 2022 and evaluate the argument aligners in terms of the performance of language models that we fine-tune to generate the conclusions from the extracted premise given as input prompts. We find that mutual information outperforms the other measures on this task, suggesting that the reasoning process in scientific abstracts hinges mostly on linguistic constructs beyond simple textual similarity.</abstract>
      <url hash="359f71fe">2024.eacl-short.14</url>
      <bibkey>gao-etal-2024-evaluating</bibkey>
    </paper>
    <paper id="15">
      <title>Over-Reasoning and Redundant Calculation of Large Language Models</title>
      <author><first>Cheng-Han</first><last>Chiang</last></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <pages>161-169</pages>
      <abstract>Large language models (LLMs) can solve problems step-by-step.While this chain-of-thought (CoT) reasoning boosts LLMs’ performance, it is unclear if LLMs know when to use CoT and whether those CoT are always necessary to answer the question. This paper shows that LLMs tend to generate redundant calculations and reasoning on a manually constructed math QA dataset, GSM8K-Zero.GSM8K-Zero is constructed such that the questions can be answered without any calculations, but LLMs, including Llama-2 models and Claude-2, tend to generate lengthy and unnecessary calculations to answer the questions.We also conduct experiments to explain why LLMs generate redundant calculations and reasonings.</abstract>
      <url hash="8a8e11cc">2024.eacl-short.15</url>
      <attachment type="note" hash="6d72ef54">2024.eacl-short.15.note.zip</attachment>
      <bibkey>chiang-lee-2024-reasoning</bibkey>
    </paper>
    <paper id="16">
      <title>Multimodal Fallacy Classification in Political Debates</title>
      <author><first>Eleonora</first><last>Mancini</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Federico</first><last>Ruggeri</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Paolo</first><last>Torroni</last><affiliation>University of Bologna</affiliation></author>
      <pages>170-178</pages>
      <abstract>Recent advances in NLP suggest that some tasks, such as argument detection and relation classification, are better framed in a multimodal perspective. We propose multimodal argument mining for argumentative fallacy classification in political debates. To this end, we release the first corpus for multimodal fallacy classification. Our experiments show that the integration of the audio modality leads to superior classification performance. Our findings confirm that framing fallacy classification as a multimodal task is essential to capture paralinguistic aspects of fallacious arguments.</abstract>
      <url hash="6a9f1b90">2024.eacl-short.16</url>
      <bibkey>mancini-etal-2024-multimodal</bibkey>
    </paper>
    <paper id="17">
      <title>The Parrot Dilemma: Human-Labeled vs. <fixed-case>LLM</fixed-case>-augmented Data in Classification Tasks</title>
      <author><first>Anders Giovanni</first><last>Møller</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Arianna</first><last>Pera</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Jacob</first><last>Dalsgaard</last></author>
      <author><first>Luca</first><last>Aiello</last><affiliation>IT University of Copenhagen</affiliation></author>
      <pages>179-192</pages>
      <abstract>In the realm of Computational Social Science (CSS), practitioners often navigate complex, low-resource domains and face the costly and time-intensive challenges of acquiring and annotating data. We aim to establish a set of guidelines to address such challenges, comparing the use of human-labeled data with synthetically generated data from GPT-4 and Llama-2 in ten distinct CSS classification tasks of varying complexity. Additionally, we examine the impact of training data sizes on performance. Our findings reveal that models trained on human-labeled data consistently exhibit superior or comparable performance compared to their synthetically augmented counterparts. Nevertheless, synthetic augmentation proves beneficial, particularly in improving performance on rare classes within multi-class tasks. Furthermore, we leverage GPT-4 and Llama-2 for zero-shot classification and find that, while they generally display strong performance, they often fall short when compared to specialized classifiers trained on moderately sized training sets.</abstract>
      <url hash="9268aea3">2024.eacl-short.17</url>
      <attachment type="software" hash="499519a7">2024.eacl-short.17.software.zip</attachment>
      <bibkey>moller-etal-2024-parrot</bibkey>
    </paper>
    <paper id="18">
      <title>Language Model Sentence Completion with a Parser-Driven Rhetorical Control Method</title>
      <author><first>Joshua</first><last>Zingale</last><affiliation>San Diego State University</affiliation></author>
      <author><first>Jugal</first><last>Kalita</last><affiliation>University of Colorado at Colorado Springs</affiliation></author>
      <pages>193-203</pages>
      <abstract>Controlled text generation (CTG) seeks to guide large language model (LLM) output, that statistical language generation would conform to desired criteria. The current study presents a novel CTG algorithm that enforces adherence toward specific rhetorical relations in an LLM sentence-completion context by a parser-driven decoding scheme that requires no model fine-tuning. The method is validated both with automatic and human evaluation.</abstract>
      <url hash="ee9d9743">2024.eacl-short.18</url>
      <bibkey>zingale-kalita-2024-language</bibkey>
    </paper>
    <paper id="19">
      <title>”It’s how you do things that matters”: Attending to Process to Better Serve Indigenous Communities with Language Technologies</title>
      <author><first>Ned</first><last>Cooper</last><affiliation>Australian National University</affiliation></author>
      <author><first>Courtney</first><last>Heldreth</last><affiliation>Research, Google</affiliation></author>
      <author><first>Ben</first><last>Hutchinson</last><affiliation>Google</affiliation></author>
      <pages>204-211</pages>
      <abstract>Indigenous languages are historically under-served by Natural Language Processing (NLP) technologies, but this is changing for some languages with the recent scaling of large multilingual models and an increased focus by the NLP community on endangered languages. This position paper explores ethical considerations in building NLP technologies for Indigenous languages, based on the premise that such projects should primarily serve Indigenous communities. We report on interviews with 17 researchers working in or with Aboriginal and/or Torres Strait Islander communities on language technology projects in Australia. Drawing on insights from the interviews, we recommend practices for NLP researchers to increase attention to the process of engagements with Indigenous communities, rather than focusing only on decontextualised artefacts.</abstract>
      <url hash="061935bb">2024.eacl-short.19</url>
      <bibkey>cooper-etal-2024-things</bibkey>
    </paper>
    <paper id="20">
      <title>Source Identification in Abstractive Summarization</title>
      <author><first>Yoshi</first><last>Suhara</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Dimitris</first><last>Alikaniotis</last><affiliation>Grammarly</affiliation></author>
      <pages>212-224</pages>
      <abstract>Neural abstractive summarization models make summaries in an end-to-end manner, and little is known about how the source information is actually converted into summaries. In this paper, we define input sentences that contain essential information in the generated summary as source sentences and study how abstractive summaries are made by analyzing the source sentences. To this end, we annotate source sentences for reference summaries and system summaries generated by PEGASUS on document-summary pairs sampled from the CNN/DailyMail and XSum datasets. We also formulate automatic source sentence detection and compare multiple methods to establish a strong baseline for the task. Experimental results show that the perplexity-based method performs well in highly abstractive settings, while similarity-based methods perform robustly in relatively extractive settings.</abstract>
      <url hash="9eaafcad">2024.eacl-short.20</url>
      <attachment type="software" hash="cfcdd5df">2024.eacl-short.20.software.zip</attachment>
      <attachment type="note" hash="a3ec5467">2024.eacl-short.20.note.zip</attachment>
      <bibkey>suhara-alikaniotis-2024-source</bibkey>
    </paper>
    <paper id="21">
      <title>From Partial to Strictly Incremental Constituent Parsing</title>
      <author><first>Ana</first><last>Ezquerro</last></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last><affiliation>Universidade da Coruña</affiliation></author>
      <author><first>David</first><last>Vilares</last><affiliation>Universidade da Coruña</affiliation></author>
      <pages>225-233</pages>
      <abstract>We study incremental constituent parsers to assess their capacity to output trees based on prefix representations alone. Guided by strictly left-to-right generative language models and tree-decoding modules, we build parsers that adhere to a strong definition of incrementality across languages. This builds upon work that asserted incrementality, but that mostly only enforced it on either the encoder or the decoder. Finally, we conduct an analysis against non-incremental and partially incremental models.</abstract>
      <url hash="add9b44e">2024.eacl-short.21</url>
      <bibkey>ezquerro-etal-2024-partial</bibkey>
    </paper>
    <paper id="22">
      <title>Predict the Next Word: &lt;Humans exhibit uncertainty in this task and language models _____&gt;</title>
      <author><first>Evgenia</first><last>Ilia</last></author>
      <author><first>Wilker</first><last>Aziz</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>234-255</pages>
      <abstract>Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgments (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM’s ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the ‘next word prediction’ task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty. We also verify the failure of expected calibration error (ECE) to reflect this, and as such, advise the community against relying on it in this setting.</abstract>
      <url hash="ca3d147e">2024.eacl-short.22</url>
      <bibkey>ilia-aziz-2024-predict</bibkey>
    </paper>
    <paper id="23">
      <title>A Prompt Response to the Demand for Automatic Gender-Neutral Translation</title>
      <author><first>Beatrice</first><last>Savoldi</last></author>
      <author><first>Andrea</first><last>Piergentili</last><affiliation>University of Trento and Fondazione Bruno Kessler</affiliation></author>
      <author><first>Dennis</first><last>Fucci</last></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Luisa</first><last>Bentivogli</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>256-267</pages>
      <abstract>Gender-neutral translation (GNT) that avoids biased and undue binary assumptions is a pivotal challenge for the creation of more inclusive translation technologies. Advancements for this task in Machine Translation (MT), however, are hindered by the lack of dedicated parallel data, which are necessary to adapt MT systems to satisfy neutral constraints. For such a scenario, large language models offer hitherto unforeseen possibilities, as they come with the distinct advantage of being versatile in various (sub)tasks when provided with explicit instructions. In this paper, we explore this potential to automate GNT by comparing MT with the popular GPT-4 model. Through extensive manual analyses, our study empirically reveals the inherent limitations of current MT systems in generating GNTs and provides valuable insights into the potential and challenges associated with prompting for neutrality.</abstract>
      <url hash="d875f994">2024.eacl-short.23</url>
      <bibkey>savoldi-etal-2024-prompt</bibkey>
    </paper>
    <paper id="24">
      <title>Interpreting Predictive Probabilities: Model Confidence or Human Label Variation?</title>
      <author><first>Joris</first><last>Baan</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Raquel</first><last>Fernández</last><affiliation>University of Amsterdam and University of Amsterdam</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München and IT University of Copenhagen</affiliation></author>
      <author><first>Wilker</first><last>Aziz</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>268-277</pages>
      <abstract>With the rise of increasingly powerful and user-facing NLP systems, there is growing interest in assessing whether they have a good _representation of uncertainty_ by evaluating the quality of their predictive distribution over outcomes. We identify two main perspectives that drive starkly different evaluation protocols. The first treats predictive probability as an indication of model confidence; the second as an indication of human label variation. We discuss their merits and limitations, and take the position that both are crucial for trustworthy and fair NLP systems, but that exploiting a single predictive distribution is limiting. We recommend tools and highlight exciting directions towards models with disentangled representations of uncertainty about predictions and uncertainty about human labels.</abstract>
      <url hash="55983a0b">2024.eacl-short.24</url>
      <bibkey>baan-etal-2024-interpreting</bibkey>
    </paper>
    <paper id="25">
      <title>Smaller Language Models are Better Zero-shot Machine-Generated Text Detectors</title>
      <author><first>Niloofar</first><last>Mireshghallah</last><affiliation>University of Washington</affiliation></author>
      <author><first>Justus</first><last>Mattern</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich and Rheinisch Westfälische Technische Hochschule Aachen</affiliation></author>
      <author><first>Sicun</first><last>Gao</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Reza</first><last>Shokri</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>278-293</pages>
      <abstract>As large language models are becoming more embedded in different user-facing services, it is important to be able to distinguish between human-written and machine-generated text to verify the authenticity of news articles, product reviews, etc. Thus, in this paper we set out to explore whether it is possible to use one language model to identify machine-generated text produced by another language model, in a zero-shot way, even if the two have different architectures and are trained on different data. We find that overall, smaller models are better universal machine-generated text detectors: they can more precisely detect text generated from both small and larger models, without the need for any additional training/data. Interestingly, we find that whether or not the detector and generator models were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.90 in detecting GPT4 generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.65.</abstract>
      <url hash="a5c3218e">2024.eacl-short.25</url>
      <bibkey>mireshghallah-etal-2024-smaller</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>C</fixed-case>har<fixed-case>S</fixed-case>pan: Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages</title>
      <author><first>Kaushal</first><last>Maurya</last><affiliation>Indian Institute of Technology Hyderabad</affiliation></author>
      <author><first>Rahul</first><last>Kejriwal</last></author>
      <author><first>Maunendra</first><last>Desarkar</last><affiliation>IIT Hyderabad and Indian Institute of Technology, Hyderabad,</affiliation></author>
      <author><first>Anoop</first><last>Kunchukuttan</last><affiliation>Microsoft and Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>294-310</pages>
      <abstract>We address the task of machine translation (MT) from extremely low-resource language (ELRL) to English by leveraging cross-lingual transfer from *closely-related* high-resource language (HRL). The development of an MT system for ELRL is challenging because these languages typically lack parallel corpora and monolingual corpora, and their representations are absent from large multilingual language models. Many ELRLs share lexical similarities with some HRLs, which presents a novel modeling opportunity. However, existing subword-based neural MT models do not explicitly harness this lexical similarity, as they only implicitly align HRL and ELRL latent embedding space. To overcome this limitation, we propose a novel, CharSpan, approach based on character-span noise augmentation into the training data of HRL. This serves as a regularization technique, making the model more robust to <i>lexical divergences</i> between the HRL and ELRL, thus facilitating effective cross-lingual transfer. Our method significantly outperformed strong baselines in zero-shot settings on closely related HRL and ELRL pairs from three diverse language families, emerging as the state-of-the-art model for ELRLs.</abstract>
      <url hash="f6c6284a">2024.eacl-short.26</url>
      <bibkey>maurya-etal-2024-charspan</bibkey>
    </paper>
    <paper id="27">
      <title>Robust Neural Machine Translation for Abugidas by Glyph Perturbation</title>
      <author><first>Hour</first><last>Kaing</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Chenchen</first><last>Ding</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Hideki</first><last>Tanaka</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Masao</first><last>Utiyama</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>311-318</pages>
      <abstract>Neural machine translation (NMT) systems are vulnerable when trained on limited data. This is a common scenario in low-resource tasks in the real world. To increase robustness, a solution is to intently add realistic noise in the training phase. Noise simulation using text perturbation has been proven to be efficient in writing systems that use Latin letters. In this study, we further explore perturbation techniques on more complex abugida writing systems, for which the visual similarity of complex glyphs is considered to capture the essential nature of these writing systems. Besides the generated noise, we propose a training strategy to improve robustness. We conducted experiments on six languages: Bengali, Hindi, Myanmar, Khmer, Lao, and Thai. By overcoming the introduced noise, we obtained non-degenerate NMT systems with improved robustness for low-resource tasks for abugida glyphs.</abstract>
      <url hash="d7605d79">2024.eacl-short.27</url>
      <bibkey>kaing-etal-2024-robust</bibkey>
    </paper>
    <paper id="28">
      <title>Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning</title>
      <author><first>Ashish</first><last>Agrawal</last></author>
      <author><first>Barah</first><last>Fazili</last></author>
      <author><first>Preethi</first><last>Jyothi</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <pages>319-329</pages>
      <abstract>Popular benchmarks (e.g., XNLI) used to evaluate cross-lingual language understanding consist of parallel versions of English evaluation sets in multiple target languages created with the help of professional translators. When creating such parallel data, it is critical to ensure high-quality translations for all target languages for an accurate characterization of cross-lingual transfer. In this work, we find that translation inconsistencies <i>do exist</i> and interestingly they <i>disproportionally impact low-resource languages</i> in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit.</abstract>
      <url hash="30ab5c84">2024.eacl-short.28</url>
      <bibkey>agrawal-etal-2024-translation</bibkey>
    </paper>
    <paper id="29">
      <title>Less is More for Long Document Summary Evaluation by <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yunshu</first><last>Wu</last><affiliation>University of California, Riverside</affiliation></author>
      <author><first>Hayate</first><last>Iso</last><affiliation>Megagon Labs, US</affiliation></author>
      <author><first>Pouya</first><last>Pezeshkpour</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Nikita</first><last>Bhutani</last></author>
      <author><first>Estevam</first><last>Hruschka</last><affiliation>Megagon Labs and Carnegie Mellon University</affiliation></author>
      <pages>330-343</pages>
      <abstract>Large Language Models (LLMs) have shown promising performance in summary evaluation tasks, yet they face challenges such as high computational costs and the <i>Lost-in-the-Middle</i> problem where important information in the middle of long documents is often overlooked. To address these issues, this paper introduces a novel approach, Extract-then-Evaluate, which involves extracting key sentences from a long source document and then evaluating the summary by prompting LLMs. The results reveal that the proposed method not only significantly reduces evaluation costs but also exhibits a higher correlation with human evaluations. Furthermore, we provide practical recommendations for optimal document length and sentence extraction methods, contributing to the development of cost-effective yet more accurate methods for LLM-based text generation evaluation.</abstract>
      <url hash="0fbcc882">2024.eacl-short.29</url>
      <bibkey>wu-etal-2024-less</bibkey>
    </paper>
    <paper id="30">
      <title>Leveraging <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> in Pharmacovigilance Event Extraction: An Empirical Study</title>
      <author><first>Zhaoyue</first><last>Sun</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Gabriele</first><last>Pergola</last></author>
      <author><first>Byron</first><last>Wallace</last><affiliation>Northeastern University, Brown University and Northeastern University</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>344-357</pages>
      <abstract>With the advent of large language models (LLMs), there has been growing interest in exploring their potential for medical applications. This research aims to investigate the ability of LLMs, specifically ChatGPT, in the context of pharmacovigilance event extraction, of which the main goal is to identify and extract adverse events or potential therapeutic events from textual medical sources. We conduct extensive experiments to assess the performance of ChatGPT in the pharmacovigilance event extraction task, employing various prompts and demonstration selection strategies. The findings demonstrate that while ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies, it still falls short compared to fully fine-tuned small models. Additionally, we explore the potential of leveraging ChatGPT for data augmentation. However, our investigation reveals that the inclusion of synthesized data into fine-tuning may lead to a decrease in performance, possibly attributed to noise in the ChatGPT-generated labels. To mitigate this, we explore different filtering strategies and find that, with the proper approach, more stable performance can be achieved, although constant improvement remains elusive.</abstract>
      <url hash="938df389">2024.eacl-short.30</url>
      <bibkey>sun-etal-2024-leveraging</bibkey>
    </paper>
    <paper id="31">
      <title>A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation</title>
      <author><first>Phillip</first><last>Schneider</last></author>
      <author><first>Manuel</first><last>Klettner</last><affiliation>Department of Informatics, Technische Universität München</affiliation></author>
      <author><first>Elena</first><last>Simperl</last><affiliation>King’s College London</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>358-367</pages>
      <abstract>Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models’ performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance.</abstract>
      <url hash="308f5740">2024.eacl-short.31</url>
      <attachment type="note" hash="588a7caa">2024.eacl-short.31.note.zip</attachment>
      <bibkey>schneider-etal-2024-comparative</bibkey>
    </paper>
    <paper id="32">
      <title>Extreme Fine-tuning: A Novel and Fast Fine-tuning Approach for Text Classification</title>
      <author><first>Boonnithi</first><last>Jiaramaneepinit</last></author>
      <author><first>Thodsaporn</first><last>Chay-intr</last></author>
      <author><first>Kotaro</first><last>Funakoshi</last><affiliation>Institute of Innovative Research, Tokyo Institute of Technology</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>368-379</pages>
      <abstract>Although fine-tuning a pre-trained model with a conventional approach has shown to be effective in various downstream tasks, previous work has used only backpropagation to fine-tune the model, which causes a massive amount of computational resources and time. We propose Extreme Fine-Tuning (EFT), a novel approach for fine-tuning a pre-trained model effectively and efficiently. EFT uses backpropagation for a brief fine-tuning and an iterative extreme learning machine for training a classifier. We applied EFT to four text classification datasets, MELD, IEMOCAP, IMDb, and AG News, and compared its performance with state-of-the-art (SOTA) approaches. The results indicate that EFT noticeably outperformed the other approaches in training-time measurement with comparable model performance. We will release our code at https://github.com/up-33/extreme-fine-tuning.</abstract>
      <url hash="d7ff3f30">2024.eacl-short.32</url>
      <bibkey>jiaramaneepinit-etal-2024-extreme</bibkey>
    </paper>
    <paper id="33">
      <title>Flow Matching for Conditional Text Generation in a Few Sampling Steps</title>
      <author><first>Vincent</first><last>Hu</last></author>
      <author><first>Di</first><last>Wu</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Yuki</first><last>Asano</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Pascal</first><last>Mettes</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Basura</first><last>Fernando</last><affiliation>A*STAR</affiliation></author>
      <author><first>Björn</first><last>Ommer</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Cees</first><last>Snoek</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>380-392</pages>
      <abstract>Diffusion models are a promising tool for high-quality text generation. However, current models face multiple drawbacks including slow sampling, noise schedule sensitivity, and misalignment between the training and sampling stages. In this paper, we introduce FlowSeq, which bypasses all current drawbacks by leveraging flow matching for conditional text generation. FlowSeq can generate text in a few steps by training with a novel anchor loss, alleviating the need for expensive hyperparameter optimization of the noise schedule prevalent in diffusion models. We extensively evaluate our proposed method and show competitive performance in tasks such as question generation, open-domain dialogue, and paraphrasing tasks.</abstract>
      <url hash="78d7e921">2024.eacl-short.33</url>
      <attachment type="software" hash="ab03dcde">2024.eacl-short.33.software.zip</attachment>
      <bibkey>hu-etal-2024-flow</bibkey>
    </paper>
    <paper id="34">
      <title>Corpus-Steered Query Expansion with Large Language Models</title>
      <author><first>Yibin</first><last>Lei</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Yu</first><last>Cao</last></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Tao</first><last>Shen</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Andrew</first><last>Yates</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>393-401</pages>
      <abstract>Recent studies demonstrate that query expansions generated by large language models (LLMs) can considerably enhance information retrieval systems by generating hypothetical documents that answer the queries as expansions. However, challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs. Inspired by Pseudo Relevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to promote the incorporation of knowledge embedded within the corpus. CSQE utilizes the relevance assessing capability of LLMs to systematically identify pivotal sentences in the initially-retrieved documents. These corpus-originated texts are subsequently used to expand the query together with LLM-knowledge empowered expansions, improving the relevance prediction between the query and the target documents. Extensive experiments reveal that CSQE exhibits strong performance without necessitating any training, especially with queries for which LLMs lack knowledge.</abstract>
      <url hash="5c0d35d7">2024.eacl-short.34</url>
      <attachment type="software" hash="532a4543">2024.eacl-short.34.software.zip</attachment>
      <bibkey>lei-etal-2024-corpus</bibkey>
    </paper>
    <paper id="35">
      <title>Defending Against Disinformation Attacks in Open-Domain Question Answering</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Aleem</first><last>Khan</last></author>
      <author><first>Nathaniel</first><last>Weir</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Dawn</first><last>Lawrie</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <pages>402-417</pages>
      <abstract>Recent work in open-domain question answering (ODQA) has shown that adversarial poisoning of the search collection can cause large drops in accuracy for production systems. However, little to no work has proposed methods to defend against these attacks. To do so, we rely on the intuition that redundant information often exists in large corpora. To find it, we introduce a method that uses query augmentation to search for a diverse set of passages that could answer the original question but are less likely to have been poisoned. We integrate these new passages into the model through the design of a novel confidence method, comparing the predicted answer to its appearance in the retrieved contexts (what we call Confidence from Answer Redundancy, i.e. CAR). Together these methods allow for a simple but effective way to defend against poisoning attacks that provides gains of nearly 20% exact match across varying levels of data poisoning/knowledge conflicts.</abstract>
      <url hash="59f6f95f">2024.eacl-short.35</url>
      <bibkey>weller-etal-2024-defending</bibkey>
    </paper>
    <paper id="36">
      <title>Sentence Representations via <fixed-case>G</fixed-case>aussian Embedding</title>
      <author><first>Shohei</first><last>Yoda</last></author>
      <author><first>Hayato</first><last>Tsukagoshi</last></author>
      <author><first>Ryohei</first><last>Sasano</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Koichi</first><last>Takeda</last><affiliation>Nagoya University</affiliation></author>
      <pages>418-425</pages>
      <abstract>Recent progress in sentence embedding, which represents a sentence’s meaning as a point in a vector space, has achieved high performance on several tasks such as the semantic textual similarity (STS) task.However, a sentence representation cannot adequately express the diverse information that sentences contain: for example, such representations cannot naturally handle asymmetric relationships between sentences.This paper proposes GaussCSE, a Gaussian-distribution-based contrastive learning framework for sentence embedding that can handle asymmetric inter-sentential relations, as well as a similarity measure for identifying entailment relations.Our experiments show that GaussCSE achieves performance comparable to that of previous methods on natural language inference (NLI) tasks, and that it can estimate the direction of entailment relations, which is difficult with point representations.</abstract>
      <url hash="fc1d33f4">2024.eacl-short.36</url>
      <bibkey>yoda-etal-2024-sentence</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>STOR</fixed-case>i<fixed-case>C</fixed-case>o: Storytelling <fixed-case>TTS</fixed-case> for <fixed-case>H</fixed-case>indi with Character Voice Modulation</title>
      <author><first>Pavan</first><last>Tankala</last></author>
      <author><first>Preethi</first><last>Jyothi</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Preeti</first><last>Rao</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>426-431</pages>
      <abstract>We present a new Hindi text-to-speech (TTS) dataset and demonstrate its utility for the expressive synthesis of children’s audio stories. The dataset comprises narration by a single female speaker who modifies her voice to produce different story characters. Annotation for dialogue identification, character labelling, and character attribution are provided, all of which are expected to facilitate the learning of character voice and speaking styles. Experiments are conducted using different versions of the annotated dataset that enable training a multi-speaker TTS model on the single-speaker data. Subjective tests show that the multi-speaker model improves expressiveness and character voice consistency compared to the baseline single-speaker TTS. With the multi-speaker model, objective evaluations show comparable word error rates, better speaker voice consistency, and higher correlations with ground-truth emotion attributes. We release a new 16.8 hours storytelling speech dataset in Hindi and propose effective solutions for expressive TTS with narrator voice modulation and character voice consistency.</abstract>
      <url hash="21284d0b">2024.eacl-short.37</url>
      <bibkey>tankala-etal-2024-storico</bibkey>
    </paper>
    <paper id="38">
      <title>Rethinking Loss Functions for Fact Verification</title>
      <author><first>Yuta</first><last>Mukobara</last></author>
      <author><first>Yutaro</first><last>Shigeto</last><affiliation>Chiba Institute of Technology</affiliation></author>
      <author><first>Masashi</first><last>Shimbo</last><affiliation>Chiba Institute of Technology</affiliation></author>
      <pages>432-442</pages>
      <abstract>We explore loss functions for fact verification in the FEVER shared task. While the cross-entropy loss is a standard objective for training verdict predictors, it fails to capture the heterogeneity among the FEVER verdict classes. In this paper, we develop two task-specific objectives tailored to FEVER. Experimental results confirm that the proposed objective functions outperform the standard cross-entropy. Performance is further improved when these objectives are combined with simple class weighting, which effectively overcomes the imbalance in the training data. The source code is available (https://github.com/yuta-mukobara/RLF-KGAT).</abstract>
      <url hash="3a81db24">2024.eacl-short.38</url>
      <bibkey>mukobara-etal-2024-rethinking</bibkey>
    </paper>
    <paper id="39">
      <title>A Dataset for Metaphor Detection in Early Medieval <fixed-case>H</fixed-case>ebrew Poetry</title>
      <author><first>Michael</first><last>Toker</last></author>
      <author><first>Oren</first><last>Mishali</last><affiliation>Computer Science Departmen, Technion-Israel Institute of Technology</affiliation></author>
      <author><first>Ophir</first><last>Münz-Manor</last><affiliation>Open University of Israel</affiliation></author>
      <author><first>Benny</first><last>Kimelfeld</last><affiliation>Computer Science Departmen, Technion-Israel Institute of Technology</affiliation></author>
      <author><first>Yonatan</first><last>Belinkov</last><affiliation>Technion, Technion</affiliation></author>
      <pages>443-453</pages>
      <abstract>There is a large volume of late antique and medieval Hebrew texts. They represent a crucial linguistic and cultural bridge between Biblical and modern Hebrew. Poetry is prominent in these texts and one of its main characteristics is the frequent use of metaphor. Distinguishing figurative and literal language use is a major task for scholars of the Humanities, especially in the fields of literature, linguistics, and hermeneutics. This paper presents a new, challenging dataset of late antique and medieval Hebrew poetry with expert annotations of metaphor, as well as some baseline results, which we hope will facilitate further research in this area.</abstract>
      <url hash="78f9e799">2024.eacl-short.39</url>
      <bibkey>toker-etal-2024-dataset</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>SOCIALITE</fixed-case>-<fixed-case>LLAMA</fixed-case>: An Instruction-Tuned Model for Social Scientific Tasks</title>
      <author><first>Gourab</first><last>Dey</last><affiliation>, State University of New York at Stony Brook</affiliation></author>
      <author><first>Adithya</first><last>V Ganesan</last><affiliation>, State University of New York, Stony Brook</affiliation></author>
      <author><first>Yash Kumar</first><last>Lal</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Manal</first><last>Shah</last></author>
      <author><first>Shreyashee</first><last>Sinha</last></author>
      <author><first>Matthew</first><last>Matero</last></author>
      <author><first>Salvatore</first><last>Giorgi</last></author>
      <author><first>Vivek</first><last>Kulkarni</last></author>
      <author><first>H.</first><last>Schwartz</last><affiliation>Stony Brook University (SUNY)</affiliation></author>
      <pages>454-468</pages>
      <abstract>Social science NLP tasks, such as emotion or humor detection, are required to capture the semantics along with the implicit pragmatics from text, often with limited amounts of training data. Instruction tuning has been shown to improve the many capabilities of large language models (LLMs) such as commonsense reasoning, reading comprehension, and computer programming. However, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured. We explore the use of instruction tuning for social science NLP tasks and introduce Socialite-Llama — an open-source, instruction-tuned Llama. On a suite of 20 social science tasks, Socialite-Llama improves upon the performance of Llama as well as matches or improves upon the performance of a state-of-the-art, multi-task finetuned model on a majority of them. Further, Socialite-Llama also leads to improvement on 5 out of 6 related social tasks as compared to Llama, suggesting instruction tuning can lead to generalized social understanding. All resources including our code, model and dataset can be found through [bit.ly/socialitellama](https://bit.ly/socialitellama/).</abstract>
      <url hash="91eb1d08">2024.eacl-short.40</url>
      <bibkey>dey-etal-2024-socialite</bibkey>
      <revision id="1" href="2024.eacl-short.40v1" hash="40eaf18d"/>
      <revision id="2" href="2024.eacl-short.40v2" hash="91eb1d08" date="2024-03-16">Added a sponsor.</revision>
    </paper>
    <paper id="41">
      <title>Pre-Training Methods for Question Reranking</title>
      <author><first>Stefano</first><last>Campese</last></author>
      <author><first>Ivano</first><last>Lauriola</last><affiliation>Amazon</affiliation></author>
      <author><first>Alessandro</first><last>Moschitti</last><affiliation>Amazon Alexa AI</affiliation></author>
      <pages>469-476</pages>
      <abstract>One interesting approach to Question Answering (QA) is to search for semantically similar questions, which have been answered before. This task is different from answer retrieval as it focuses on questions rather than only on the answers, therefore it requires different model training on different data.In this work, we introduce a novel unsupervised pre-training method specialized for retrieving and ranking questions. This leverages (i) knowledge distillation from a basic question retrieval model, and (ii) new pre-training task and objective for learning to rank questions in terms of their relevance with the query. Our experiments show that (i) the proposed technique achieves state-of-the-art performance on QRC and Quora-match datasets, and (ii) the benefit of combining re-ranking and retrieval models.</abstract>
      <url hash="b346af38">2024.eacl-short.41</url>
      <bibkey>campese-etal-2024-pre</bibkey>
    </paper>
    <paper id="42">
      <title>Dynamic Masking Rate Schedules for <fixed-case>MLM</fixed-case> Pretraining</title>
      <author><first>Zachary</first><last>Ankner</last></author>
      <author><first>Naomi</first><last>Saphra</last><affiliation>Harvard University</affiliation></author>
      <author><first>Davis</first><last>Blalock</last><affiliation>Mosaic ML</affiliation></author>
      <author><first>Jonathan</first><last>Frankle</last><affiliation>School of Engineering and Applied Sciences, Harvard University and MosaicML</affiliation></author>
      <author><first>Matthew</first><last>Leavitt</last><affiliation>Facebook</affiliation></author>
      <pages>477-487</pages>
      <abstract>Most works on transformers trained with the Masked Language Modeling (MLM) objective use the original BERT model’s fixed masking rate of 15%. We propose to instead dynamically schedule the masking rate throughout training. We find that linearly decreasing the masking rate over the course of pretraining improves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and BERT-large, respectively, compared to fixed rate baselines. These gains come from exposure to both high and low masking rate regimes, providing benefits from both settings. Our results demonstrate that masking rate scheduling is a simple way to improve the quality of masked language models, achieving up to a 1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for BERT-large.</abstract>
      <url hash="0ceeba81">2024.eacl-short.42</url>
      <bibkey>ankner-etal-2024-dynamic</bibkey>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2024-03-03" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</booktitle>
      <editor><first>Nikolaos</first><last>Aletras</last></editor>
      <editor><first>Orphee</first><last>De Clercq</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St. Julians, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="a56379aa">2024.eacl-demo</url>
      <venue>eacl</venue>
    </meta>
    <frontmatter>
      <url hash="96e3f288">2024.eacl-demo.0</url>
      <bibkey>eacl-2024-european</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>T</fixed-case>ext<fixed-case>BI</fixed-case>: An Interactive Dashboard for Visualizing Multidimensional <fixed-case>NLP</fixed-case> Annotations in Social Media Data</title>
      <author><first>Maxime</first><last>Masson</last><affiliation>LIUPPA, E2S, University of Pau and Pays Adour</affiliation></author>
      <author><first>Christian</first><last>Sallaberry</last><affiliation>LIUPPA, E2S, University of Pau and Pays Adour (UPPA)</affiliation></author>
      <author><first>Marie-Noelle</first><last>Bessagnet</last><affiliation>LIUPPA, E2S, University of Pau and Pays Adour (UPPA)</affiliation></author>
      <author><first>Annig</first><last>Le Parc Lacayrelle</last><affiliation>LIUPPA, E2S, University of Pau and Pays Adour (UPPA)</affiliation></author>
      <author><first>Philippe</first><last>Roose</last><affiliation>LIUPPA, E2S, University of Pau and Pays Adour (UPPA)</affiliation></author>
      <author><first>Rodrigo</first><last>Agerri</last><affiliation>HiTZ Center-Ixa, University of the Basque Country UPV/EHU</affiliation></author>
      <pages>1-9</pages>
      <abstract>In this paper we introduce TextBI, a multimodal generic dashboard designed to present multidimensional text annotations on large volumes of multilingual social media data. This tool focuses on four core dimensions: spatial, temporal, thematic, and personal, and also supports additional enrichment data such as sentiment and engagement. Multiple visualization modes are offered, including frequency, movement, and association. This dashboard addresses the challenge of facilitating the interpretation of NLP annotations by visualizing them in a user-friendly, interactive interface catering to two categories of users: (1) domain stakeholders and (2) NLP researchers. We conducted experiments within the domain of tourism leveraging data from X (formerly Twitter) and incorporating requirements from tourism offices. Our approach, TextBI, represents a significant advancement in the field of visualizing NLP annotations by integrating and blending features from a variety of Business Intelligence, Geographical Information Systems and NLP tools. A demonstration video is also provided https://youtu.be/x714RKvo9Cg</abstract>
      <url hash="df77c7ee">2024.eacl-demo.1</url>
      <bibkey>masson-etal-2024-textbi</bibkey>
    </paper>
    <paper id="2">
      <title>k<fixed-case>NN</fixed-case>-<fixed-case>BOX</fixed-case>: A Unified Framework for Nearest Neighbor Generation</title>
      <author><first>Wenhao</first><last>Zhu</last><affiliation>National Key Laboratory for Novel Software Technology, Nanjing University</affiliation></author>
      <author><first>Qianfeng</first><last>Zhao</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yunzhe</first><last>Lv</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>National Key Laboratory for Novel Software Technology, Nanjing University</affiliation></author>
      <author><first>Siheng</first><last>Zhao</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Sizhe</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <pages>10-17</pages>
      <abstract>Augmenting the base neural model with a token-level symbolic datastore is a novel generation paradigm and has achieved promising results in machine translation (MT). In this paper, we introduce a unified framework kNN-BOX, which enables quick development and visualization for this novel paradigm. kNN-BOX decomposes the datastore-augmentation approach into three modules: datastore, retriever and combiner, thus putting diverse kNN generation methods into a unified way. Currently, kNN-BOX has provided implementation of seven popular kNN-MT variants, covering research from performance enhancement to efficiency optimization. It is easy for users to reproduce these existing work or customize their own models. Besides, users can interact with their kNN generation systems with kNN-BOX to better understand the underlying inference process in a visualized way. In experiment section, we apply kNN-BOX for machine translation and three other seq2seq generation tasks (text simplification, paraphrase generation and question generation). Experiment results show that augmenting the base neural model with kNN-BOX can bring large performance improvement in all these tasks. The code and document of kNN-BOX is available at https://github.com/NJUNLP/knn-box. The demo can be accessed at http://nlp.nju.edu.cn/demo/knn-box/. The introduction video is available at https://www.youtube.com/watch?v=m0eJldHVR3w.</abstract>
      <url hash="b981b9a2">2024.eacl-demo.2</url>
      <bibkey>zhu-etal-2024-knn</bibkey>
    </paper>
    <paper id="3">
      <title>A Human-Centric Evaluation Platform for Explainable Knowledge Graph Completion</title>
      <author><first>Zhao</first><last>Xu</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Wiem</first><last>Ben Rim</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Kiril</first><last>Gashteovski</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Timo</first><last>Sztyler</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Carolin</first><last>Lawrence</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <pages>18-26</pages>
      <abstract>Explanations for AI are expected to help human users understand AI-driven predictions. Evaluating plausibility, the helpfulness of the explanations, is therefore essential for developing eXplainable AI (XAI) that can really aid human users. Here we propose a human-centric evaluation platform to measure plausibility of explanations in the context of eXplainable Knowledge Graph Completion (XKGC). The target audience of the platform are researchers and practitioners who want to 1) investigate real needs and interests of their target users in XKGC, 2) evaluate the plausibility of the XKGC methods. We showcase these two use cases in an experimental setting to illustrate what results can be achieved with our system.</abstract>
      <url hash="20749d1e">2024.eacl-demo.3</url>
      <bibkey>xu-etal-2024-human</bibkey>
    </paper>
    <paper id="4">
      <title>py<fixed-case>TLEX</fixed-case>: A Python Library for <fixed-case>T</fixed-case>ime<fixed-case>L</fixed-case>ine <fixed-case>EX</fixed-case>traction</title>
      <author><first>Akul</first><last>Singh</last><affiliation>Florida International University</affiliation></author>
      <author><first>Jared</first><last>Hummer</last><affiliation>Florida International University</affiliation></author>
      <author><first>Mustafa</first><last>Ocal</last><affiliation>Florida International University</affiliation></author>
      <author><first>Mark</first><last>Finlayson</last><affiliation>FIU</affiliation></author>
      <pages>27-34</pages>
      <abstract>pyTLEX is an implementation of the TimeLine EXtraction algorithm (TLEX; Finlayson et al.,2021) that enables users to work with TimeML annotations and perform advanced temporal analysis, offering a comprehensive suite of features. TimeML is a standardized markup language for temporal information in text. pyTLEX allows users to parse TimeML annotations, construct TimeML graphs, and execute the TLEX algorithm to effect complete timeline extraction. In contrast to previous implementations (i.e., jTLEX for Java), pyTLEX sets itself apart with a range of advanced features. It introduces a React-based visualization system, enhancing the exploration of temporal data and the comprehension of temporal connections within textual information. Furthermore, pyTLEX incorporates an algorithm for increasing connectivity in temporal graphs, which identifies graph disconnectivity and recommends links based on temporal reasoning, thus enhancing the coherence of the graph representation. Additionally, pyTLEX includes a built-in validation algorithm, ensuring compliance with TimeML annotation guidelines, which is essential for maintaining data quality and reliability. pyTLEX equips researchers and developers with an extensive toolkit for temporal analysis, and its testing across various datasets validates its accuracy and reliability.</abstract>
      <url hash="f73e10d4">2024.eacl-demo.4</url>
      <bibkey>singh-etal-2024-pytlex</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>D</fixed-case>epress<fixed-case>M</fixed-case>ind: A Depression Surveillance System for Social Media Analysis</title>
      <author><first>Roque</first><last>Fernández-Iglesias</last><affiliation>University of Santiago de Compostela</affiliation></author>
      <author><first>Marcos</first><last>Fernandez-Pichel</last><affiliation>University of Santiago de Compostela</affiliation></author>
      <author><first>Mario</first><last>Aragon</last><affiliation>Universidade de Santiago de Compostela</affiliation></author>
      <author><first>David E.</first><last>Losada</last><affiliation>University of Santiago de Compostela</affiliation></author>
      <pages>35-43</pages>
      <abstract>Depression is a pressing global issue that impacts millions of individuals worldwide. This prevailing psychologicaldisorder profoundly influences the thoughts and behavior of those who suffer from it. We have developed DepressMind, a versatile screening tool designed to facilitate the analysis of social network data. This automated tool explores multiple psychological dimensions associated with clinical depression and estimates the extent to which these symptoms manifest in language use. Our project comprises two distinct components: one for data extraction and another one for analysis.The data extraction phase is dedicated to harvesting texts and the associated meta-information from social networks and transforming them into a user-friendly format that serves various analytical purposes.For the analysis, the main objective is to conduct an in-depth inspection of the user publications and establish connections between the posted contents and dimensions or traits defined by well-established clinical instruments.Specifically, we aim to associate extracts authored by individuals with symptoms or dimensions of the Beck Depression Inventory (BDI).</abstract>
      <url hash="490e6099">2024.eacl-demo.5</url>
      <bibkey>fernandez-iglesias-etal-2024-depressmind</bibkey>
    </paper>
    <paper id="6">
      <title>Check News in One Click: <fixed-case>NLP</fixed-case>-Empowered Pro-Kremlin Propaganda Detection</title>
      <author><first>Veronika</first><last>Solopova</last><affiliation>Freie University of Berlin</affiliation></author>
      <author><first>Viktoriia</first><last>Herman</last><affiliation>Freie University of Berlin</affiliation></author>
      <author><first>Christoph</first><last>Benzmüller</last><affiliation>FU Berlin</affiliation></author>
      <author><first>Tim</first><last>Landgraf</last><affiliation>FU Berlin</affiliation></author>
      <pages>44-51</pages>
      <abstract>Many European citizens become targets of the Kremlin propaganda campaigns, aiming to minimise public support for Ukraine, foster a climate of mistrust and disunity, and shape elections (Meister, 2022). To address this challenge, we developed “Check News in 1 Click”, the first NLP-empowered pro-Kremlin propaganda detection application available in 7 languages, which provides the lay user with feedback on their news, and explains manipulative linguistic features and keywords. We conducted a user study, analysed user entries and models’ behaviour paired with questionnaire answers, and investigated the advantages and disadvantages of the proposed interpretative solution.</abstract>
      <url hash="9673cb80">2024.eacl-demo.6</url>
      <bibkey>solopova-etal-2024-check</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>NESTLE</fixed-case>: a No-Code Tool for Statistical Analysis of Legal Corpus</title>
      <author><first>Kyoungyeon</first><last>Cho</last><affiliation>LBox</affiliation></author>
      <author><first>Seungkum</first><last>Han</last><affiliation>LBox</affiliation></author>
      <author><first>Young Rok</first><last>Choi</last><affiliation>LBox</affiliation></author>
      <author><first>Wonseok</first><last>Hwang</last><affiliation>LBox</affiliation></author>
      <pages>52-61</pages>
      <abstract>The statistical analysis of large scale legal corpus can provide valuable legal insights. For such analysis one needs to (1) select a subset of the corpus using document retrieval tools, (2) structure text using information extraction (IE) systems, and (3) visualize the data for the statistical analysis. Each process demands either specialized tools or programming skills whereas no comprehensive unified “no-code” tools have been available. Here we provide NESTLE, a no-code tool for large-scale statistical analysis of legal corpus. Powered by a Large Language Model (LLM) and the internal custom end-to-end IE system, NESTLE can extract any type of information that has not been predefined in the IE system opening up the possibility of unlimited customizable statistical analysis of the corpus without writing a single line of code. We validate our system on 15 Korean precedent IE tasks and 3 legal text classification tasks from LexGLUE. The comprehensive experiments reveal NESTLE can achieve GPT-4 comparable performance by training the internal IE module with 4 human-labeled, and 192 LLM-labeled examples.</abstract>
      <url hash="03716bd4">2024.eacl-demo.7</url>
      <bibkey>cho-etal-2024-nestle</bibkey>
    </paper>
    <paper id="8">
      <title>Multi-party Multimodal Conversations Between Patients, Their Companions, and a Social Robot in a Hospital Memory Clinic</title>
      <author><first>Angus</first><last>Addlesee</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Neeraj</first><last>Cherakara</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Nivan</first><last>Nelson</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Daniel</first><last>Hernandez Garcia</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Nancie</first><last>Gunson</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Weronika</first><last>Sieińska</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Christian</first><last>Dondrup</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Oliver</first><last>Lemon</last><affiliation>Heriot-Watt University</affiliation></author>
      <pages>62-70</pages>
      <abstract>We have deployed an LLM-based spoken dialogue system in a real hospital. The ARI social robot embodies our system, which patients and their companions can have multi-party conversations with together. In order to enable this multi-party ability, multimodality is critical. Our system, therefore, receives speech and video as input, and generates both speech and gestures (arm, head, and eye movements). In this paper, we describe our complex setting and the architecture of our dialogue system. Each component is detailed, and a video of the full system is available with the appropriate components highlighted in real-time. Our system decides when it should take its turn, generates human-like clarification requests when the patient pauses mid-utterance, answers in-domain questions (grounding to the in-prompt knowledge), and responds appropriately to out-of-domain requests (like generating jokes or quizzes). This latter feature is particularly remarkable as real patients often utter unexpected sentences that could not be handled previously.</abstract>
      <url hash="9e81e1d3">2024.eacl-demo.8</url>
      <bibkey>addlesee-etal-2024-multi</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>S</fixed-case>cam<fixed-case>S</fixed-case>pot: Fighting Financial Fraud in <fixed-case>I</fixed-case>nstagram Comments</title>
      <author><first>Stefan</first><last>Erben</last><affiliation>Lucerne University of Applied Sciences and Arts</affiliation></author>
      <author><first>Andreas</first><last>Waldis</last><affiliation>Hochschule Luzern</affiliation></author>
      <pages>71-81</pages>
      <abstract>The long-standing problem of spam and fraudulent messages in the comment sections of Instagram pages in the financial sector claims new victims every day. Instagram’s current spam filter proves inadequate, and existing research approaches are primarily confined to theoretical concepts. Practical implementations with evaluated results are missing. To solve this problem, we propose ScamSpot, a comprehensive system that includes a browser extension, a fine-tuned BERT model and a REST API. This approach ensures public accessibility of our results for Instagram users using the Chrome browser. Furthermore, we conduct a data annotation study, shedding light on the reasons and causes of the problem and evaluate the system through user feedback and comparison with existing models. ScamSpot is an open-source project and is publicly available at https://scamspot.github.io/.</abstract>
      <url hash="d6613b8d">2024.eacl-demo.9</url>
      <bibkey>erben-waldis-2024-scamspot</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>N</fixed-case>arrative<fixed-case>P</fixed-case>lay: Interactive Narrative Understanding</title>
      <author><first>Runcong</first><last>Zhao</last><affiliation>King’s College London</affiliation></author>
      <author><first>Wenjia</first><last>Zhang</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Jiazheng</first><last>Li</last><affiliation>King’s College London</affiliation></author>
      <author><first>Lixing</first><last>Zhu</last><affiliation>Department of Computer Science, University of Warwick</affiliation></author>
      <author><first>Yanran</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London</affiliation></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London</affiliation></author>
      <pages>82-93</pages>
      <abstract>In this paper, we introduce NarrativePlay, a novel system that allows users to role-play a fictional character and interact with other characters in narratives in an immersive environment. We leverage Large Language Models (LLMs) to generate human-like responses, guided by personality traits extracted from narratives. The system incorporates auto-generated visual display of narrative settings, character portraits, and character speech, greatly enhancing the user experience. Our approach eschews predefined sandboxes, focusing instead on main storyline events from the perspective of a user-selected character. NarrativePlay has been evaluated on two types of narratives, detective and adventure stories, where users can either explore the world or increase affinity with other characters through conversations.</abstract>
      <url hash="a8809656">2024.eacl-demo.10</url>
      <bibkey>zhao-etal-2024-narrativeplay</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>DP</fixed-case>-<fixed-case>NMT</fixed-case>: Scalable Differentially Private Machine Translation</title>
      <author><first>Timour</first><last>Igamberdiev</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <author><first>Doan Nam Long</first><last>Vu</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <author><first>Felix</first><last>Kuennecke</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Zhuo</first><last>Yu</last><affiliation>Department of Computer Science, Technical University of Darmstadt</affiliation></author>
      <author><first>Jannik</first><last>Holmer</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Ivan</first><last>Habernal</last><affiliation>Paderborn University</affiliation></author>
      <pages>94-105</pages>
      <abstract>Neural machine translation (NMT) is a widely popular text generation task, yet there is a considerable research gap in the development of privacy-preserving NMT models, despite significant data privacy concerns for NMT systems. Differentially private stochastic gradient descent (DP-SGD) is a popular method for training machine learning models with concrete privacy guarantees; however, the implementation specifics of training a model with DP-SGD are not always clarified in existing models, with differing software libraries used and code bases not always being public, leading to reproducibility issues. To tackle this, we introduce DP-NMT, an open-source framework for carrying out research on privacy-preserving NMT with DP-SGD, bringing together numerous models, datasets, and evaluation metrics in one systematic software package. Our goal is to provide a platform for researchers to advance the development of privacy-preserving NMT systems, keeping the specific details of the DP-SGD algorithm transparent and intuitive to implement. We run a set of experiments on datasets from both general and privacy-related domains to demonstrate our framework in use. We make our framework publicly available and welcome feedback from the community.</abstract>
      <url hash="cfc57f01">2024.eacl-demo.11</url>
      <bibkey>igamberdiev-etal-2024-dp</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>A</fixed-case>nno<fixed-case>P</fixed-case>lot: Interactive Visualizations of Text Annotations</title>
      <author><first>Elisabeth</first><last>Fittschen</last><affiliation>Uni Hamburg</affiliation></author>
      <author><first>Tim</first><last>Fischer</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Daniel</first><last>Brühl</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Julia</first><last>Spahr</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Yuliia</first><last>Lysa</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Phuoc Thang</first><last>Le</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>106-114</pages>
      <abstract>This paper presents AnnoPlot, a web application designed to analyze, manage, and visualize annotated text data.Users can configure projects, upload datasets, and explore their data through interactive visualization of span annotations with scatter plots, clusters, and statistics. AnnoPlot supports various transformer models to compute high-dimensional embeddings of text annotations and utilizes dimensionality reduction algorithms to offer users a novel 2D view of their datasets.A dynamic approach to dimensionality reduction allows users to adjust visualizations in real-time, facilitating category reorganization and error identification. The proposed application is open-source, promoting transparency and user control.Especially suited for the Digital Humanities, AnnoPlot offers a novel solution to address challenges in dynamic annotation datasets, empowering users to enhance data integrity and adapt to evolving categorizations.</abstract>
      <url hash="088189e5">2024.eacl-demo.12</url>
      <bibkey>fittschen-etal-2024-annoplot</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>G</fixed-case>eospa<fixed-case>C</fixed-case>y: A tool for extraction and geographical referencing of spatial expressions in textual data</title>
      <author><first>Syed</first><last>Mehtab Alam</last><affiliation>CIRAD, TETIS</affiliation></author>
      <author><first>Elena</first><last>Arsevska</last><affiliation>Cirad, Inra</affiliation></author>
      <author><first>Mathieu</first><last>Roche</last><affiliation>CIRAD, TETIS</affiliation></author>
      <author><first>Maguelonne</first><last>Teisseire</last><affiliation>UMR TETIS (Earth Observation and Geoinformation for Environment and Land Management research Unit)</affiliation></author>
      <pages>115-126</pages>
      <abstract>Spatial information in text enables to understand the geographical context and relationships within text for better decision-making across various domains such as disease surveillance, disaster management and other location based services. Therefore, it is crucial to understand the precise geographical context for location-sensitive applications. In response to this necessity, we introduce the GeospaCy software tool, designed for the extraction and georeferencing of spatial information present in textual data. GeospaCy fulfils two primary objectives: 1) Geoparsing, which involves extracting spatial expressions, encompassing place names and associated spatial relations within the text data, and 2) Geocoding, which facilitates the assignment of geographical coordinates to the spatial expressions extracted during the Geoparsing task. Geoparsing is evaluated with a disease news article dataset consisting of event information, whereas a qualitative evaluation of geographical coordinates (polygons/geometries) of spatial expressions is performed by end-users for Geocoding task.</abstract>
      <url hash="ce33be76">2024.eacl-demo.13</url>
      <bibkey>mehtab-alam-etal-2024-geospacy</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>MAMMOTH</fixed-case>: Massively Multilingual Modular Open Translation @ <fixed-case>H</fixed-case>elsinki</title>
      <author><first>Timothee</first><last>Mickus</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Stig-Arne</first><last>Grönroos</last><affiliation>Silo.AI</affiliation></author>
      <author><first>Joseph</first><last>Attieh</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Michele</first><last>Boggia</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Ona</first><last>De Gibert</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Shaoxiong</first><last>Ji</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Niki Andreas</first><last>Loppi</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Alessandro</first><last>Raganato</last><affiliation>University of Milano-Bicocca</affiliation></author>
      <author><first>Raúl</first><last>Vázquez</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Jörg</first><last>Tiedemann</last><affiliation>University of Helsinki</affiliation></author>
      <pages>127-136</pages>
      <abstract>NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled. The trend goes to modularization, a necessary step into the direction of designing smaller sub-networks and components with specialized functionality. In this paper, we present the MAMMOTH toolkit: a framework designed for training massively multilingual modular machine translation systems at scale, initially derived from OpenNMT-py and then adapted to ensure efficient training across computation clusters.We showcase its efficiency across clusters of A100 and V100 NVIDIA GPUs, and discuss our design philosophy and plans for future information.The toolkit is publicly available online at https://github.com/Helsinki-NLP/mammoth.</abstract>
      <url hash="f7dfc576">2024.eacl-demo.14</url>
      <bibkey>mickus-etal-2024-mammoth</bibkey>
    </paper>
    <paper id="15">
      <title>The <fixed-case>DUR</fixed-case>el Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change</title>
      <author><first>Dominik</first><last>Schlechtweg</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Shafqat Mumtaz</first><last>Virk</last><affiliation>Språkbanken Text, Dept. of Swedish University of Gothenburg</affiliation></author>
      <author><first>Pauline</first><last>Sander</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Emma</first><last>Sköldberg</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Lukas</first><last>Theuer Linke</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Tuo</first><last>Zhang</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Nina</first><last>Tahmasebi</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Jonas</first><last>Kuhn</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Sabine</first><last>Schulte Im Walde</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>137-149</pages>
      <abstract>We present the DURel tool implementing the annotation of semantic proximity between word uses into an online, open source interface. The tool supports standardized human annotation as well as computational annotation, building on recent advances with Word-in-Context models. Annotator judgments are clustered with automatic graph clustering techniques and visualized for analysis. This allows to measure word senses with simple and intuitive micro-task judgments between use pairs, requiring minimal preparation efforts. The tool offers additional functionalities to compare the agreement between annotators to guarantee the inter-subjectivity of the obtained judgments and to calculate summary statistics over the annotated data giving insights into sense frequency distributions, semantic variation or changes of senses over time.</abstract>
      <url hash="a47730f3">2024.eacl-demo.15</url>
      <bibkey>schlechtweg-etal-2024-durel</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>RAGA</fixed-case>s: Automated Evaluation of Retrieval Augmented Generation</title>
      <author><first>Shahul</first><last>Es</last><affiliation>Exploding Gradients</affiliation></author>
      <author><first>Jithin</first><last>James</last><affiliation>Exploding Gradients</affiliation></author>
      <author><first>Luis</first><last>Espinosa Anke</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Steven</first><last>Schockaert</last><affiliation>Cardiff University</affiliation></author>
      <pages>150-158</pages>
      <abstract>We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs is available at [https://github.com/explodinggradients/ragas]. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.</abstract>
      <url hash="8b5c2d0c">2024.eacl-demo.16</url>
      <bibkey>es-etal-2024-ragas</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>N</fixed-case>euro<fixed-case>P</fixed-case>rompts: An Adaptive Framework to Optimize Prompts for Text-to-Image Generation</title>
      <author><first>Shachar</first><last>Rosenman</last><affiliation>Intel Labs</affiliation></author>
      <author><first>Vasudev</first><last>Lal</last><affiliation>Intel Labs</affiliation></author>
      <author><first>Phillip</first><last>Howard</last><affiliation>Intel Labs</affiliation></author>
      <pages>159-167</pages>
      <abstract>Despite impressive recent advances in text-to-image diffusion models, obtaining high-quality images often requires prompt engineering by humans who have developed expertise in using them. In this work, we present NeuroPrompts, an adaptive framework that automatically enhances a user’s prompt to improve the quality of generations produced by text-to-image models. Our framework utilizes constrained text decoding with a pre-trained language model that has been adapted to generate prompts similar to those produced by human prompt engineers. This approach enables higher-quality text-to-image generations and provides user control over stylistic features via constraint set specification. We demonstrate the utility of our framework by creating an interactive application for prompt enhancement and image generation using Stable Diffusion. Additionally, we conduct experiments utilizing a large dataset of human-engineered prompts for text-to-image generation and show that our approach automatically produces enhanced prompts that result in superior image quality. We make our code, a screencast video demo and a live demo instance of NeuroPrompts publicly available.</abstract>
      <url hash="f715b7cd">2024.eacl-demo.17</url>
      <bibkey>rosenman-etal-2024-neuroprompts</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>MEGA</fixed-case>nno+: A Human-<fixed-case>LLM</fixed-case> Collaborative Annotation System</title>
      <author><first>Hannah</first><last>Kim</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Kushan</first><last>Mitra</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Rafael</first><last>Li Chen</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Sajjadur</first><last>Rahman</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Dan</first><last>Zhang</last><affiliation>Megagon Labs</affiliation></author>
      <pages>168-176</pages>
      <abstract>Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.</abstract>
      <url hash="629984d8">2024.eacl-demo.18</url>
      <bibkey>kim-etal-2024-meganno</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>X</fixed-case>-<fixed-case>AMR</fixed-case> Annotation Tool</title>
      <author><first>Shafiuddin Rehan</first><last>Ahmed</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Jon</first><last>Cai</last><affiliation>The University of Colorado</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado</affiliation></author>
      <author><first>James H.</first><last>Martin</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>177-186</pages>
      <abstract>This paper presents a novel Cross-document Abstract Meaning Representation (X-AMR) annotation tool designed for annotating key corpus-level event semantics. Leveraging machine assistance through the Prodigy Annotation Tool, we enhance the user experience, ensuring ease and efficiency in the annotation process. Through empirical analyses, we demonstrate the effectiveness of our tool in augmenting an existing event corpus, highlighting its advantages when integrated with GPT-4. Code and annotations: href{https://anonymous.4open.science/r/xamr-9ED0}{anonymous.4open.science/r/xamr-9ED0} footnote Demo: {href{https://youtu.be/TuirftxciNE}{https://youtu.be/TuirftxciNE}} footnote Live Link: {href{https://tinyurl.com/mrxmafwh}{https://tinyurl.com/mrxmafwh}}</abstract>
      <url hash="0d9a8852">2024.eacl-demo.19</url>
      <bibkey>ahmed-etal-2024-x</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>D</fixed-case>oc<fixed-case>C</fixed-case>hecker: Bootstrapping Code Large Language Model for Detecting and Resolving Code-Comment Inconsistencies</title>
      <author><first>Anh</first><last>Dau</last><affiliation>FPT Software AI Center</affiliation></author>
      <author><first>Jin L.c.</first><last>Guo</last><affiliation>McGill University</affiliation></author>
      <author><first>Nghi</first><last>Bui</last><affiliation>Salesforce Research Asia</affiliation></author>
      <pages>187-194</pages>
      <abstract>Comments in source code are crucial for developers to understand the purpose of the code and to use it correctly. However, keeping comments aligned with the evolving codebase poses a significant challenge. With increasing interest in automated solutions to identify and rectify discrepancies between code and its associated comments, most existing methods rely heavily on heuristic rules. This paper introduces DocChecker, a language model-based framework adept at detecting inconsistencies between code and comments and capable of generating synthetic comments. This functionality allows DocChecker to identify and rectify cases where comments do not accurately represent the code they describe.The efficacy of DocChecker is demonstrated using the Just-In-Time and CodeXGlue datasets in various scenarios. Notably, DocChecker sets a new benchmark in the Inconsistency Code-Comment Detection (ICCD) task, achieving 72.3% accuracy, and scoring 33.64 in BLEU-4 on the code summarization task. These results surpass other Large Language Models (LLMs), including GPT 3.5 and CodeLlama.DocChecker is accessible for use and evaluation. It can be found on https://github.com/FSoft-AI4Code/DocChecker and at http://4.193.50.237:5000/. For a more comprehensive understanding of its functionality, a demonstration video is available on https://youtu.be/FqnPmd531xw.</abstract>
      <url hash="4ee92fab">2024.eacl-demo.20</url>
      <bibkey>dau-etal-2024-docchecker</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>TL</fixed-case>;<fixed-case>DR</fixed-case> Progress: Multi-faceted Literature Exploration in Text Summarization</title>
      <author><first>Shahbaz</first><last>Syed</last><affiliation>Leipzig University</affiliation></author>
      <author><first>Khalid</first><last>Al Khatib</last><affiliation>Groningen University</affiliation></author>
      <author><first>Martin</first><last>Potthast</last><affiliation>Leipzig University</affiliation></author>
      <pages>195-206</pages>
      <abstract>This paper presents TL;DR Progress, a new tool for exploring the literature on neural text summarization. It organizes 514~papers based on a comprehensive annotation scheme for text summarization approaches and enables fine-grained, faceted search. Each paper was manually annotated to capture aspects such as evaluation metrics, quality dimensions, learning paradigms, challenges addressed, datasets, and document domains. In addition, a succinct indicative summary is provided for each paper, describing contextual factors, issues, and proposed solutions. The tool is available at {url{https://www.tldr-progress.de}}, a demo video at {url{https://youtu.be/uCVRGFvXUj8}}</abstract>
      <url hash="54a7554b">2024.eacl-demo.21</url>
      <bibkey>syed-etal-2024-tl</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>FRAPPE</fixed-case>: <fixed-case>FRA</fixed-case>ming, Persuasion, and Propaganda Explorer</title>
      <author><first>Ahmed</first><last>Sajwani</last><affiliation>Khalifa University of Science and Technology</affiliation></author>
      <author><first>Alaa</first><last>El Setohy</last><affiliation>Egypt Japan University of Science and Technology</affiliation></author>
      <author><first>Ali</first><last>Mekky</last><affiliation>Alexandria University</affiliation></author>
      <author><first>Diana</first><last>Turmakhan</last><affiliation>Nazarbayev University</affiliation></author>
      <author><first>Lara</first><last>Hassan</last><affiliation>Alexandria University</affiliation></author>
      <author><first>Mohamed</first><last>El Zeftawy</last><affiliation>Alexandria University</affiliation></author>
      <author><first>Omar</first><last>El Herraoui</last><affiliation>NYU Abu Dhabi</affiliation></author>
      <author><first>Osama</first><last>Afzal</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Qisheng</first><last>Liao</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Tarek</first><last>Mahmoud</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Zain</first><last>Muhammad Mujahid</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Muhammad</first><last>Umar Salman</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Muhammad</first><last>Arslan Manzoor</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Massa</first><last>Baali</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Jakub</first><last>Piskorski</last><affiliation>Polish Academy of Sciences</affiliation></author>
      <author><first>Nicolas</first><last>Stefanovitch</last><affiliation>European Commission Joint Research Centre</affiliation></author>
      <author><first>Giovanni</first><last>Da San Martino</last><affiliation>University of Padova</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>207-213</pages>
      <abstract>The abundance of news sources and the urgent demand for reliable information have led to serious concerns about the threat of misleading information. In this paper, we present FRAPPE, a FRAming, Persuasion, and Propaganda Explorer system. FRAPPE goes beyond conventional news analysis of articles and unveils the intricate linguistic techniques used to shape readers’ opinions and emotions. Our system allows users not only to analyze individual articles for their genre, framings, and use of persuasion techniques, but also to draw comparisons between the strategies of persuasion and framing adopted by a diverse pool of news outlets and countries across multiple languages for different topics, thus providing a comprehensive understanding of how information is presented and manipulated. FRAPPE is publicly accessible at https://frappe.streamlit.app/ and a video explaining our system is available at https://www.youtube.com/watch?v=3RlTfSVnZmk</abstract>
      <url hash="e66c3473">2024.eacl-demo.22</url>
      <bibkey>sajwani-etal-2024-frappe</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>LLM</fixed-case>e<fixed-case>B</fixed-case>ench: A Flexible Framework for Accelerating <fixed-case>LLM</fixed-case>s Benchmarking</title>
      <author><first>Fahim</first><last>Dalvi</last><affiliation>Qatar Computing Research Institute, HBKU</affiliation></author>
      <author><first>Maram</first><last>Hasanain</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Sabri</first><last>Boughorbel</last><affiliation>Qatar Computing Research Institute, HBKU</affiliation></author>
      <author><first>Basel</first><last>Mousi</last><affiliation>QCRI</affiliation></author>
      <author><first>Samir</first><last>Abdaljalil</last><affiliation>Texas A&amp;M University</affiliation></author>
      <author><first>Nizi</first><last>Nazar</last><affiliation>Qatar Computing Research Institute, HBKU</affiliation></author>
      <author><first>Ahmed</first><last>Abdelali</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Shammur Absar</first><last>Chowdhury</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Hamdy</first><last>Mubarak</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Ahmed</first><last>Ali</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>214-222</pages>
      <abstract>The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework, which can be seamlessly customized to evaluate LLMs for any NLP task, regardless of language. The framework features generic dataset loaders, several model providers, and pre-implements most standard evaluation metrics. It supports in-context learning with zero- and few-shot settings. A specific dataset and task can be evaluated for a given LLM in less than 20 lines of code while allowing full flexibility to extend the framework for custom datasets, models, or tasks. The framework has been tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We open-sourced LLMeBench for the community (https://github.com/qcri/LLMeBench/) and a video demonstrating the framework is available online (https://youtu.be/9cC2m_abk3A).</abstract>
      <url hash="b9920324">2024.eacl-demo.23</url>
      <bibkey>dalvi-etal-2024-llmebench</bibkey>
    </paper>
    <paper id="24">
      <title>Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling</title>
      <author><first>Talia</first><last>Tseriotou</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Ryan</first><last>Chan</last><affiliation>The Alan Turing Institute</affiliation></author>
      <author><first>Adam</first><last>Tsakalidis</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Iman Munire</first><last>Bilal</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Elena</first><last>Kochkina</last><affiliation>Queen Mary University</affiliation></author>
      <author><first>Terry</first><last>Lyons</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Maria</first><last>Liakata</last><affiliation>Queen Mary University of London</affiliation></author>
      <pages>223-237</pages>
      <abstract>We present an open-source, pip installable toolkit, Sig-Networks, the first of its kind for longitudinal language modelling. A central focus is the incorporation of Signature-based Neural Network models, which have recently shown success in temporal tasks. We apply and extend published research providing a full suite of signature-based models. Their components can be used as PyTorch building blocks in future architectures. Sig-Networks enables task-agnostic dataset plug-in, seamless preprocessing for sequential data, parameter flexibility, automated tuning across a range of models. We examine signature networks under three different NLP tasks of varying temporal granularity: counselling conversations, rumour stance switch and mood changes in social media threads, showing SOTA performance in all three, and provide guidance for future tasks. We release the Toolkit as a PyTorch package with an introductory video, Git repositories for preprocessing and modelling including sample notebooks on the modeled NLP tasks.</abstract>
      <url hash="31e5544c">2024.eacl-demo.24</url>
      <bibkey>tseriotou-etal-2024-sig</bibkey>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2024-03-03" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop</booktitle>
      <editor><first>Neele</first><last>Falk</last></editor>
      <editor><first>Sara</first><last>Papi</last></editor>
      <editor><first>Mike</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St. Julian’s, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="84eb508b">2024.eacl-srw</url>
      <venue>eacl</venue>
    </meta>
    <frontmatter>
      <url hash="a15c8f49">2024.eacl-srw.0</url>
      <bibkey>eacl-2024-european-chapter</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>A</fixed-case>uto<fixed-case>A</fixed-case>ugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes</title>
      <author><first>Juhwan</first><last>Choi</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Kyohoon</first><last>Jin</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Junho</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Sangmin</first><last>Song</last></author>
      <author><first>YoungBin</first><last>Kim</last><affiliation>ChungAng University</affiliation></author>
      <pages>1-8</pages>
      <abstract>Text data augmentation is a complex problem due to the discrete nature of sentences. Although rule-based augmentation methods are widely adopted in real-world applications because of their simplicity, they suffer from potential semantic damage. Previous researchers have suggested easy data augmentation with soft labels (softEDA), employing label smoothing to mitigate this problem. However, finding the best factor for each model and dataset is challenging; therefore, using softEDA in real-world applications is still difficult. In this paper, we propose adapting AutoAugment to solve this problem. The experimental results suggest that the proposed method can boost existing augmentation methods and that rule-based methods can enhance cutting-edge pretrained language models. We offer the source code.</abstract>
      <url hash="71328d13">2024.eacl-srw.1</url>
      <bibkey>choi-etal-2024-autoaugment</bibkey>
    </paper>
    <paper id="2">
      <title>Generating Diverse Translation with Perturbed <tex-math>k</tex-math><fixed-case>NN</fixed-case>-<fixed-case>MT</fixed-case></title>
      <author><first>Yuto</first><last>Nishida</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Makoto</first><last>Morishita</last><affiliation>NTT</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Division of Information Science, Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>9-31</pages>
      <abstract>Generating multiple translation candidates would enable users to choose the one that satisfies their needs.Although there has been work on diversified generation, there exists room for improving the diversity mainly because the previous methods do not address the overcorrection problem—the model underestimates a prediction that is largely different from the training data, even if that prediction is likely.This paper proposes methods that generate more diverse translations by introducing perturbed <tex-math>k</tex-math>-nearest neighbor machine translation (<tex-math>k</tex-math>NN-MT).Our methods expand the search space of <tex-math>k</tex-math>NN-MT and help incorporate diverse words into candidates by addressing the overcorrection problem.Our experiments show that the proposed methods drastically improve candidate diversity and control the degree of diversity by tuning the perturbation’s magnitude.</abstract>
      <url hash="17ea0ef7">2024.eacl-srw.2</url>
      <bibkey>nishida-etal-2024-generating</bibkey>
    </paper>
    <paper id="3">
      <title>The <fixed-case>KIND</fixed-case> Dataset: A Social Collaboration Approach for Nuanced Dialect Data Collection</title>
      <author><first>Asma</first><last>Yamani</last></author>
      <author><first>Raghad</first><last>Alziyady</last><affiliation>NA</affiliation></author>
      <author><first>Reem</first><last>AlYami</last><affiliation>King Fahad University of Petroleum and Minerals</affiliation></author>
      <author><first>Salma</first><last>Albelali</last><affiliation>NA</affiliation></author>
      <author><first>Leina</first><last>Albelali</last><affiliation>NA</affiliation></author>
      <author><first>Jawharah</first><last>Almulhim</last><affiliation>NA</affiliation></author>
      <author><first>Amjad</first><last>Alsulami</last></author>
      <author><first>Motaz</first><last>Alfarraj</last><affiliation>King Fahad University of Petroleum and Minerals</affiliation></author>
      <author><first>Rabeah</first><last>Al-Zaidy</last></author>
      <pages>32-43</pages>
      <abstract>Nuanced dialects are a linguistic variant that pose several challenges for NLP models and techniques. One of the main challenges is the limited amount of datasets to enable extensive research and experimentation. We propose an approach for efficiently collecting nuanced dialectal datasets that are not only of high quality, but are versatile enough to be multipurpose as well. To test our approach we collect the KIND corpus, which is a collection of fine-grained Arabic dialect data. The data is short texts, and unlike many nuanced dialectal datasets, it is curated manually through social collaboration efforts as opposed to being crawled from social media. The collaborative approach is incentivized through educational gamification and competitions for which the community itself benefits from the open source dataset. Our approach aims to achieve: (1) coverage of dialects from under-represented groups and fine-grained dialectal varieties, (2) provide aligned parallel corpora for translation between Modern Standard Arabic (MSA) and multiple dialects to enable translation and comparison studies, (3) promote innovative approaches for nuanced dialect data collection. We explain the steps for the competition as well as the resulting datasets and the competing data collection systems. The KIND dataset is shared with the research community.</abstract>
      <url hash="4d7e6c03">2024.eacl-srw.3</url>
      <bibkey>yamani-etal-2024-kind</bibkey>
    </paper>
    <paper id="4">
      <title>Can Stanza be Used for Part-of-Speech Tagging Historical <fixed-case>P</fixed-case>olish?</title>
      <author><first>Maria Irena</first><last>Szawerna</last><affiliation>Göteborg University</affiliation></author>
      <pages>44-49</pages>
      <abstract>The goal of this paper is to evaluate the performance of Stanza, a part-of-speech (POS) tagger developed for modern Polish, on historical text to assess its possible use for automating the annotation of other historical texts. While the issue of the reliability of utilizing POS taggers on historical data has been previously discussed, most of the research focuses on languages whose grammar differs from Polish, meaning that their results need not be fully applicable in this case. The evaluation of Stanza is conducted on two sets of 10286 and 3270 manually annotated tokens from a piece of historical Polish writing (1899), and the errors are analyzed qualitatively and quantitatively. The results show a good performance of the tagger, especially when it comes to Universal Part-of-Speech (UPOS) tags, which is promising for utilizing the tagger for automatic annotation in larger projects, and pinpoint some common features of misclassified tokens.</abstract>
      <url hash="e05a57e9">2024.eacl-srw.4</url>
      <bibkey>szawerna-2024-stanza</bibkey>
    </paper>
    <paper id="5">
      <title>Toward Zero-Shot Instruction Following</title>
      <author><first>Renze</first><last>Lou</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Wenpeng</first><last>Yin</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>50-60</pages>
      <abstract>This work proposes a challenging yet more realistic setting for zero-shot cross-task generalization: zero-shot instruction following, presuming the existence of a paragraph-style task definition while no demonstrations exist. To better learn the task supervision from the definition, we propose two strategies: first, to automatically find out the critical sentences in the definition; second, a ranking objective to force the model to generate the gold outputs with higher probabilities when those critical parts are highlighted in the definition. The joint efforts of the two strategies yield state-of-the-art performance on the Super-NaturalInstructions. Our code is available on GitHub.</abstract>
      <url hash="0380949d">2024.eacl-srw.5</url>
      <bibkey>lou-yin-2024-toward</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>U</fixed-case>n<fixed-case>MASK</fixed-case>ed: Quantifying Gender Biases in Masked Language Models through Linguistically Informed Job Market Prompts</title>
      <author><first>Iñigo</first><last>Parra</last></author>
      <pages>61-70</pages>
      <abstract>Language models (LMs) have become pivotal in the realm of technological advancements. While their capabilities are vast and transformative, they often include societal biases encoded in the human-produced datasets used for their training. This research delves into the inherent biases present in masked language models (MLMs), with a specific focus on gender biases. This study evaluated six prominent models: BERT, RoBERTa, DistilBERT, BERT- multilingual, XLM-RoBERTa, and DistilBERT- multilingual. The methodology employed a novel dataset, bifurcated into two subsets: one containing prompts that encouraged models to generate subject pronouns in English and the other requiring models to return the probabilities of verbs, adverbs, and adjectives linked to the prompts’ gender pronouns. The analysis reveals stereotypical gender alignment of all models, with multilingual variants showing comparatively reduced biases.</abstract>
      <url hash="b149c26a">2024.eacl-srw.6</url>
      <bibkey>parra-2024-unmasked</bibkey>
    </paper>
    <paper id="7">
      <title>Distribution Shifts Are Bottlenecks: Extensive Evaluation for Grounding Language Models to Knowledge Bases</title>
      <author><first>Yiheng</first><last>Shu</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Zhiwei</first><last>Yu</last></author>
      <pages>71-88</pages>
      <abstract>Grounding language models (LMs) to knowledge bases (KBs) helps to obtain rich and accurate facts. However, it remains challenging because of the enormous size, complex structure, and partial observability of KBs. One reason is that current benchmarks fail to reflect robustness challenges and fairly evaluate models.This paper analyzes whether these robustness challenges arise from distribution shifts, including environmental, linguistic, and modal aspects.This affects the ability of LMs to cope with unseen schema, adapt to language variations, and perform few-shot learning. Thus, the paper proposes extensive evaluation protocols and conducts experiments to demonstrate that, despite utilizing our proposed data augmentation method, both advanced small and large language models exhibit poor robustness in these aspects. We conclude that current LMs are too fragile to navigate in complex environments due to distribution shifts. This underscores the need for future research focusing on data collection, evaluation protocols, and learning paradigms.</abstract>
      <url hash="2682b414">2024.eacl-srw.7</url>
      <bibkey>shu-yu-2024-distribution</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>A</fixed-case>ttri<fixed-case>S</fixed-case>age: Product Attribute Value Extraction Using Graph Neural Networks</title>
      <author><first>Rohan</first><last>Potta</last></author>
      <author><first>Mallika</first><last>Asthana</last></author>
      <author><first>Siddhant</first><last>Yadav</last></author>
      <author><first>Nidhi</first><last>Goyal</last></author>
      <author><first>Sai</first><last>Patnaik</last></author>
      <author><first>Parul</first><last>Jain</last><affiliation>NA</affiliation></author>
      <pages>89-94</pages>
      <abstract>Extracting the attribute value of a product from the given product description is essential for ecommerce functions like product recommendations, search, and information retrieval. Therefore, understanding products in E-commerce. Greater accuracy certainly gives any retailer the edge. The burdensome aspect of this problem lies in the diversity of the products and their attributes and values. Existing solutions typically employ large language models or sequence-tagging approaches to capture the context of a given product description and extract attribute values. However, they do so with limited accuracy, which serves as the underlying motivation to explore a more comprehensive solution. Through this paper, we present a novel approach for attribute value extraction from product description leveraging graphs and graph neural networks. Our proposed method demonstrates improvements in attribute value extraction accuracy compared to the baseline sequence tagging approaches.</abstract>
      <url hash="f2e3aaf6">2024.eacl-srw.8</url>
      <bibkey>potta-etal-2024-attrisage</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>H</fixed-case>ypo<fixed-case>T</fixed-case>erm<fixed-case>QA</fixed-case>: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Cem</first><last>Uluoglakci</last></author>
      <author><first>Tugba</first><last>Temizel</last><affiliation>Graduate School of Informatics</affiliation></author>
      <pages>95-136</pages>
      <abstract>Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs. The detection of hallucinations itself is also a formidable task, frequently requiring manual labeling or constrained evaluations. This paper introduces an automated scalable framework that combines benchmarking LLMs’ hallucination tendencies with efficient hallucination detection. We leverage LLMs to generate challenging tasks related to hypothetical phenomena, subsequently employing them as agents for efficient hallucination detection. The framework is domain-agnostic, allowing the use of any language model for benchmark creation or evaluation in any domain. We introduce the publicly available HypoTermQA Benchmarking Dataset, on which state-of-the-art models’ performance ranged between 3% and 11%, and evaluator agents demonstrated a 6% error rate in hallucination prediction. The proposed framework provides opportunities to test and improve LLMs. Additionally, it has the potential to generate benchmarking datasets tailored to specific domains, such as law, health, and finance.</abstract>
      <url hash="f53f8736">2024.eacl-srw.9</url>
      <bibkey>uluoglakci-temizel-2024-hypotermqa</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>A</fixed-case>rabic Synonym <fixed-case>BERT</fixed-case>-based Adversarial Examples for Text Classification</title>
      <author><first>Norah</first><last>Alshahrani</last></author>
      <author><first>Saied</first><last>Alshahrani</last></author>
      <author><first>Esma</first><last>Wali</last></author>
      <author><first>Jeanna</first><last>Matthews</last><affiliation>Clarkson University</affiliation></author>
      <pages>137-147</pages>
      <abstract>Text classification systems have been proven vulnerable to adversarial text examples, modified versions of the original text examples that are often unnoticed by human eyes, yet can force text classification models to alter their classification. Often, research works quantifying the impact of adversarial text attacks have been applied only to models trained in English. In this paper, we introduce the first word-level study of adversarial attacks in Arabic. Specifically, we use a synonym (word-level) attack using a Masked Language Modeling (MLM) task with a BERT model in a black-box setting to assess the robustness of the state-of-the-art text classification models to adversarial attacks in Arabic. To evaluate the grammatical and semantic similarities of the newly produced adversarial examples using our synonym BERT-based attack, we invite four human evaluators to assess and compare the produced adversarial examples with their original examples. We also study the transferability of these newly produced Arabic adversarial examples to various models and investigate the effectiveness of defense mechanisms against these adversarial examples on the BERT models. We find that fine-tuned BERT models were more susceptible to our synonym attacks than the other Deep Neural Networks (DNN) models like WordCNN and WordLSTM we trained. We also find that fine-tuned BERT models were more susceptible to transferred attacks. We, lastly, find that fine-tuned BERT models successfully regain at least 2% in accuracy after applying adversarial training as an initial defense mechanism.</abstract>
      <url hash="078ea6ea">2024.eacl-srw.10</url>
      <bibkey>alshahrani-etal-2024-arabic</bibkey>
    </paper>
    <paper id="11">
      <title>A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models</title>
      <author><first>Marc</first><last>Braun</last><affiliation>NA</affiliation></author>
      <author><first>Jenny</first><last>Kunz</last><affiliation>Linköping University</affiliation></author>
      <pages>148-161</pages>
      <abstract>The self-rationalising capabilities of LLMs are appealing because the generated explanations can give insights into the plausibility of the predictions. However, how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further.To this end, we propose a hypothesis-driven statistical framework. We use a Bayesian network to implement a hypothesis about how a task (in our example, natural language inference) is solved, and its internal states are translated into natural language with templates. Those explanations are then compared to LLM-generated free-text explanations using automatic and human evaluations. This allows us to judge how similar the LLM’s and the Bayesian network’s decision processes are. We demonstrate the usage of our framework with an example hypothesis and two realisations in Bayesian networks. The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework’s potential to approximate LLM decisions better in future work.</abstract>
      <url hash="b1becb9d">2024.eacl-srw.11</url>
      <bibkey>braun-kunz-2024-hypothesis</bibkey>
    </paper>
    <paper id="12">
      <title>Align before Attend: Aligning Visual and Textual Features for Multimodal Hateful Content Detection</title>
      <author><first>Eftekhar</first><last>Hossain</last></author>
      <author><first>Omar</first><last>Sharif</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Sarah Masud</first><last>Preum</last><affiliation>Dartmouth College</affiliation></author>
      <pages>162-174</pages>
      <abstract>Multimodal hateful content detection is a challenging task that requires complex reasoning across visual and textual modalities. Therefore, creating a meaningful multimodal representation that effectively captures the interplay between visual and textual features through intermediate fusion is critical. Conventional fusion techniques are unable to attend to the modality-specific features effectively. Moreover, most studies exclusively concentrated on English and overlooked other low-resource languages. This paper proposes a context-aware attention framework for multimodal hateful content detection and assesses it for both English and non-English languages. The proposed approach incorporates an attention layer to meaningfully align the visual and textual features. This alignment enables selective focus on modality-specific features before fusing them. We evaluate the proposed approach on two benchmark hateful meme datasets, viz. MUTE (Bengali code-mixed) and MultiOFF (English). Evaluation results demonstrate our proposed approach’s effectiveness with F1-scores of 69.7% and 70.3% for the MUTE and MultiOFF datasets. The scores show approximately 2.5% and 3.2% performance improvement over the state-of-the-art systems on these datasets. Our implementation is available at https://github.com/eftekhar-hossain/Bengali-Hateful-Memes.</abstract>
      <url hash="6461034d">2024.eacl-srw.12</url>
      <bibkey>hossain-etal-2024-align</bibkey>
    </paper>
    <paper id="13">
      <title>Topic-guided Example Selection for Domain Adaptation in <fixed-case>LLM</fixed-case>-based Machine Translation</title>
      <author><first>Seth</first><last>Aycock</last></author>
      <author><first>Rachel</first><last>Bawden</last><affiliation>Inria</affiliation></author>
      <pages>175-195</pages>
      <abstract>Current machine translation (MT) systems perform well in the domains on which they were trained, but adaptation to unseen domains remains a challenge. Rather than fine-tuning on domain data or modifying the architecture for training, an alternative approach exploits large language models (LLMs), which are performant across NLP tasks especially when presented with in-context examples. We focus on adapting a pre-trained LLM to a domain at inference through in-context example selection. For MT, examples are usually randomly selected from a development set. Some more recent methods though select using the more intuitive basis of test source similarity. We employ topic models to select examples based on abstract semantic relationships below the level of a domain. We test the relevance of these statistical models and use them to select informative examples even for out-of-domain inputs, experimenting on 7 diverse domains and 11 language pairs of differing resourcedness. Our method outperforms baselines on challenging multilingual out-of-domain tests, though it does not match performance with strong baselines for the in-language setting. We find that adding few-shot examples and related keywords consistently improves translation quality, that example diversity must be balanced with source similarity, and that our pipeline is overly restrictive for example selection when a targeted development set is available.</abstract>
      <url hash="9d871088">2024.eacl-srw.13</url>
      <bibkey>aycock-bawden-2024-topic</bibkey>
    </paper>
    <paper id="14">
      <title>Reforging : A Method for Constructing a Linguistically Valid <fixed-case>J</fixed-case>apanese <fixed-case>CCG</fixed-case> Treebank</title>
      <author><first>Asa</first><last>Tomita</last></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <author><first>Daisuke</first><last>Bekki</last><affiliation>Ochanomizu University</affiliation></author>
      <pages>196-207</pages>
      <abstract>The linguistic validity of Combinatory Categorial Grammar (CCG) parsing results relies heavily on treebanks for training and evaluation, so the treebank construction is crucial. Yet the current Japanese CCG treebank is known to have inaccuracies in its analyses of Japanese syntactic structures, including passive and causative constructions. While ABCTreebank, a treebank for ABC grammar, has been made to improve the analysis, particularly of argument structures, it lacks the detailed syntactic features required for Japanese CCG. In contrast, the Japanese CCG parser, lightblue, efficiently provides detailed syntactic features, but it does not accurately capture argument structures. We propose a method to generate a linguistically valid Japanese CCG treebank with detailed information by combining the strengths of ABCTreebank and lightblue. We develop an algorithm that filters lightblue’s lexical items using ABCTreebank, effectively converting lightblue output into a linguistically valid CCG treebank. To evaluate our treebank, we manually evaluate CCG syntactic structures and semantic representations and analyze conversion rates.</abstract>
      <url hash="20dd68a2">2024.eacl-srw.14</url>
      <bibkey>tomita-etal-2024-reforging</bibkey>
    </paper>
    <paper id="15">
      <title>Thesis Proposal: <fixed-case>D</fixed-case>etecting Agency Attribution</title>
      <author><first>Igor</first><last>Ryazanov</last><affiliation>Umea University</affiliation></author>
      <author><first>Johanna</first><last>Björklund</last></author>
      <pages>208-214</pages>
      <abstract>We explore computational methods for perceived agency attribution in natural language data. We consider ‘agency’ as the freedom and capacity to act, and the corresponding Natural Language Processing (NLP) task involves automatically detecting attributions of agency to entities in text. Our theoretical framework draws on semantic frame analysis, role labelling and related techniques. In initial experiments, we focus on the perceived agency of AI systems. To achieve this, we analyse a dataset of English-language news coverage of AI-related topics, published within one year surrounding the release of the Large Language Model-based service ChatGPT, a milestone in the general public’s awareness of AI. Building on this, we propose a schema to annotate a dataset for agency attribution and formulate additional research questions to answer by applying NLP models.</abstract>
      <url hash="f3118b97">2024.eacl-srw.15</url>
      <bibkey>ryazanov-bjorklund-2024-thesis</bibkey>
    </paper>
    <paper id="16">
      <title>A Thesis Proposal <fixed-case>C</fixed-case>laim<fixed-case>I</fixed-case>nspector Framework: A Hybrid Approach to Data Annotation using Fact-Checked Claims and <fixed-case>LLM</fixed-case>s</title>
      <author><first>Basak</first><last>Bozkurt</last><affiliation>University of Oxford</affiliation></author>
      <pages>215-224</pages>
      <abstract>This thesis explores the challenges and limitations encountered in automated fact-checking processes, with a specific emphasis on data annotation in the context of misinformation. Despite the widespread presence of misinformation in multiple formats and across various channels, current efforts concentrate narrowly on textual claims sourced mainly from Twitter, resulting in datasets with considerably limited scope. Furthermore, the absence of automated control measures, coupled with the reliance on human annotation, which is very limited, increases the risk of noisy data within these datasets. This thesis proposal examines the existing methods, elucidates their limitations and explores the potential integration of claim detection subtasks and Large Language Models to mitigate these issues. It introduces ClaimInspector, a novel framework designed for a systemic collection of multimodal data from the internet. By implementing this framework, this thesis will propose a dataset comprising fact-checks alongside the corresponding claims made by politicians. Overall, this thesis aims to enhance the accuracy and efficiency of annotation processes, thereby contributing to automated fact-checking efforts.</abstract>
      <url hash="4fbd553c">2024.eacl-srw.16</url>
      <bibkey>bozkurt-2024-thesis</bibkey>
    </paper>
    <paper id="17">
      <title>Large Language Models for Mathematical Reasoning: Progresses and Challenges</title>
      <author><first>Janice</first><last>Ahn</last></author>
      <author><first>Rishu</first><last>Verma</last></author>
      <author><first>Renze</first><last>Lou</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Di</first><last>Liu</last><affiliation>Temple University</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Wenpeng</first><last>Yin</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>225-237</pages>
      <abstract>Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges within this domain. To the best of our knowledge, this survey stands as one of the first extensive examinations of the landscape of LLMs in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.</abstract>
      <url hash="eb1dc4c9">2024.eacl-srw.17</url>
      <bibkey>ahn-etal-2024-large</bibkey>
    </paper>
    <paper id="18">
      <title>Representation and Generation of Machine Learning Test Functions</title>
      <author><first>Souha</first><last>Hassine</last></author>
      <author><first>Steven</first><last>Wilson</last><affiliation>Oakland University (Michigan)</affiliation></author>
      <pages>238-247</pages>
      <abstract>Writing tests for machine learning (ML) code is a crucial step towards ensuring the correctness and reliability of ML software. At the same time, Large Language Models (LLMs) have been adopted at a rapid pace for various code generation tasks, making it a natural choice for many developers who need to write ML tests. However, the implications of using these models, and how the LLM-generated tests differ from human-written ones, are relatively unexplored. In this work, we examine the use of LLMs to extract representations of ML source code and tests in order to understand the semantic relationships between human-written test functions and LLM-generated ones, and annotate a set of LLM-generated tests for several important qualities including usefulness, documentation, and correctness. We find that programmers prefer LLM-generated tests to those selected using retrieval-based methods, and in some cases, to those written by other humans.</abstract>
      <url hash="88e27c61">2024.eacl-srw.18</url>
      <bibkey>hassine-wilson-2024-representation</bibkey>
    </paper>
    <paper id="19">
      <title>The Generative <fixed-case>AI</fixed-case> Paradox in Evaluation: “What It Can Solve, It May Not Evaluate”</title>
      <author><first>Juhyun</first><last>Oh</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Eunsu</first><last>Kim</last></author>
      <author><first>Inha</first><last>Cha</last><affiliation>Upstage AI Research</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>248-257</pages>
      <abstract>This paper explores the assumption that Large Language Models (LLMs) skilled in generation tasks are equally adept as evaluators. We assess the performance of three LLMs and one open-source LM in Question-Answering (QA) and evaluation tasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks. Intriguingly, we discover instances of unfaithful evaluation where models accurately evaluate answers in areas where they lack competence, underscoring the need to examine the faithfulness and trustworthiness of LLMs as evaluators. This study contributes to the understanding of “the Generative AI Paradox” (West et al., 2023), highlighting a need to explore the correlation between generative excellence and evaluation proficiency, and the necessity to scrutinize the faithfulness aspect in model evaluations.</abstract>
      <url hash="2704af54">2024.eacl-srw.19</url>
      <bibkey>oh-etal-2024-generative</bibkey>
    </paper>
    <paper id="20">
      <title>Generative Data Augmentation using <fixed-case>LLM</fixed-case>s improves Distributional Robustness in Question Answering</title>
      <author><first>Arijit</first><last>Chowdhury</last></author>
      <author><first>Aman</first><last>Chadha</last><affiliation>Amazon</affiliation></author>
      <pages>258-265</pages>
      <abstract>Robustness in Natural Language Processing continues to be a pertinent issue, where state of the art models under-perform under naturally shifted distributions. In the context of Question Answering, work on domain adaptation methods continues to be a growing body of research. However, very little attention has been given to the notion of domain generalization under natural distribution shifts, where the target domain is unknown. With drastic improvements in the quality and access to generative models, we answer the question: How do generated datasets influence the performance of QA models under natural distribution shifts? We perform experiments on 4 different datasets under varying amounts of distribution shift, and analyze how “in-the-wild” generation can help achieve domain generalization. We take a two-step generation approach, generating both contexts and QA pairs to augment existing datasets. Through our experiments, we demonstrate how augmenting reading comprehension datasets with generated data leads to better robustness towards natural distribution shifts.</abstract>
      <url hash="12cd739f">2024.eacl-srw.20</url>
      <bibkey>chowdhury-chadha-2024-generative</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>J</fixed-case>apanese-<fixed-case>E</fixed-case>nglish Sentence Translation Exercises Dataset for Automatic Grading</title>
      <author><first>Naoki</first><last>Miura</last></author>
      <author><first>Hiroaki</first><last>Funayama</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Seiya</first><last>Kikuchi</last><affiliation>NA</affiliation></author>
      <author><first>Yuichiroh</first><last>Matsubayashi</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Yuya</first><last>Iwase</last></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence, RIKEN and Tohoku University</affiliation></author>
      <pages>266-278</pages>
      <abstract>This paper proposes the task of automatic assessment of Sentence Translation Exercises (STEs), that have been used in the early stage of L2 language learning.We formalize the task as grading student responses for each rubric criterion pre-specified by the educators.We then create a dataset for STE between Japanese and English including 21 questions, along with a total of 3,498 student responses (167 on average).The answer responses were collected from students and crowd workers.Using this dataset, we demonstrate the performance of baselines including a finetuned BERT model and GPT-3.5 with few-shot learning. Experimental results showed that the baseline model with fine-tuned BERT was able to classify correct responses with approximately 90% in <tex-math>F_1</tex-math>, but only less than 80% for incorrect responses. Furthermore, GPT-3.5 with few-shot learning shows a poorer result than the BERT model, indicating that our newly proposed task presents a challenging issue, even for the state-of-the-art large language model.</abstract>
      <url hash="a7e8f96b">2024.eacl-srw.21</url>
      <bibkey>miura-etal-2024-japanese</bibkey>
    </paper>
    <paper id="22">
      <title>The Impact of Integration Step on Integrated Gradients</title>
      <author><first>Masahiro</first><last>Makino</last></author>
      <author><first>Yuya</first><last>Asazuma</last></author>
      <author><first>Shota</first><last>Sasaki</last><affiliation>Cyberagent, Inc.</affiliation></author>
      <author><first>Jun</first><last>Suzuki</last><affiliation>Tohoku University</affiliation></author>
      <pages>279-289</pages>
      <abstract>Integrated Gradients (IG) serve as a potent tool for explaining the internal structure of a language model. The calculation of IG requires numerical integration, wherein the number of steps serves as a critical hyperparameter. The step count can drastically alter the results, inducing considerable errors in interpretability. To scrutinize the effect of step variation on IG, we measured the difference between theoretical and observed IG totals for each step amount.Our findings indicate that the ideal number of steps to maintain minimal error varies from instance to instance. Consequently, we advocate for customizing the step count for each instance. Our study is the first to quantitatively analyze the variation of IG values with the number of steps.</abstract>
      <url hash="9c8e15db">2024.eacl-srw.22</url>
      <bibkey>makino-etal-2024-impact</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>G</fixed-case>es<fixed-case>N</fixed-case>avi: Gesture-guided Outdoor Vision-and-Language Navigation</title>
      <author><first>Aman</first><last>Jain</last></author>
      <author><first>Teruhisa</first><last>Misu</last><affiliation>Honda Research Institute USA, Inc.</affiliation></author>
      <author><first>Kentaro</first><last>Yamada</last><affiliation>Honda Research Institute USA, Inc.</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <pages>290-295</pages>
      <abstract>Vision-and-Language Navigation (VLN) task involves navigating mobility using linguistic commands and has application in developing interfaces for autonomous mobility. In reality, natural human communication also encompasses non-verbal cues like hand gestures and gaze. These gesture-guided instructions have been explored in Human-Robot Interaction systems for effective interaction, particularly in object-referring expressions. However, a notable gap exists in tackling gesture-based demonstrative expressions in outdoor VLN task. To address this, we introduce a novel dataset for gesture-guided outdoor VLN instructions with demonstrative expressions, designed with a focus on complex instructions requiring multi-hop reasoning between the multiple input modalities. In addition, our work also includes a comprehensive analysis of the collected data and a comparative evaluation against the existing datasets.</abstract>
      <url hash="aeaa8cf9">2024.eacl-srw.23</url>
      <bibkey>jain-etal-2024-gesnavi</bibkey>
    </paper>
    <paper id="24">
      <title>Can docstring reformulation with an <fixed-case>LLM</fixed-case> improve code generation?</title>
      <author><first>Nicola</first><last>Dainese</last><affiliation>Aalto University</affiliation></author>
      <author><first>Alexander</first><last>Ilin</last><affiliation>Aalto University</affiliation></author>
      <author><first>Pekka</first><last>Marttinen</last><affiliation>Aalto University</affiliation></author>
      <pages>296-312</pages>
      <abstract>Generating code is an important application of Large Language Models (LLMs) and the task of function completion is one of the core open challenges in this context. Existing approaches focus on either training, fine-tuning or prompting LLMs to generate better outputs given the same input. We propose a novel and complementary approach: to optimize part of the input, the docstring (summary of a function’s purpose and usage), via reformulation with an LLM, in order to improve code generation. We develop two baseline methods for optimizing code generation via docstring reformulation and test them on the original HumanEval benchmark and multiple curated variants which are made more challenging by realistically worsening the docstrings. Our results show that, when operating on docstrings reformulated by an LLM instead of the original (or worsened) inputs, the performance of a number of open-source LLMs does not change significantlyThis finding demonstrates an unexpected robustness of current open-source LLMs to the details of the docstrings. We conclude by examining a series of questions, accompanied by in-depth analyses, pertaining to the sensitivity of current open-source LLMs to the details in the docstrings, the potential for improvement via docstring reformulation and the limitations of the methods employed in this work.</abstract>
      <url hash="bb36faa3">2024.eacl-srw.24</url>
      <bibkey>dainese-etal-2024-docstring</bibkey>
    </paper>
    <paper id="25">
      <title>Benchmarking Diffusion Models for Machine Translation</title>
      <author><first>Yunus</first><last>Demirag</last></author>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>313-324</pages>
      <abstract>Diffusion models have recently shown great potential on many generative tasks.In this work, we explore diffusion models for machine translation (MT).We adapt two prominent diffusion-based text generation models, Diffusion-LM and DiffuSeq, to perform machine translation.As the diffusion models generate non-autoregressively (NAR),we draw parallels to NAR machine translation models.With a comparison to conventional Transformer-based translation models, as well as to the Levenshtein Transformer,an established NAR MT model,we show that the multimodality problem that limits NAR machine translation performance is also a challenge to diffusion models.We demonstrate that knowledge distillation from an autoregressive model improves the performance of diffusion-based MT.A thorough analysis on the translation quality of inputs of different lengths shows that the diffusion models struggle more on long-range dependencies than other models.</abstract>
      <url hash="81e33602">2024.eacl-srw.25</url>
      <bibkey>demirag-etal-2024-benchmarking</bibkey>
    </paper>
    <paper id="26">
      <title>Forged-<fixed-case>GAN</fixed-case>-<fixed-case>BERT</fixed-case>: Authorship Attribution for <fixed-case>LLM</fixed-case>-Generated Forged Novels</title>
      <author><first>Kanishka</first><last>Silva</last><affiliation>University of Wolverhampton and University of Wolverhampton</affiliation></author>
      <author><first>Ingo</first><last>Frommholz</last><affiliation>NA</affiliation></author>
      <author><first>Burcu</first><last>Can</last><affiliation>University of Stirling</affiliation></author>
      <author><first>Fred</first><last>Blain</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Raheem</first><last>Sarwar</last><affiliation>NA</affiliation></author>
      <author><first>Laura</first><last>Ugolini</last><affiliation>NA</affiliation></author>
      <pages>325-337</pages>
      <abstract>The advancement of generative Large Language Models (LLMs), capable of producing human-like texts, introduces challenges related to the authenticity of the text documents. This requires exploring potential forgery scenarios within the context of authorship attribution, especially in the literary domain. Particularly,two aspects of doubted authorship may arise in novels, as a novel may be imposed by a renowned author or include a copied writing style of a well-known novel. To address these concerns, we introduce Forged-GAN-BERT, a modified GANBERT-based model to improve the classification of forged novels in two data-augmentation aspects: via the Forged Novels Generator (i.e., ChatGPT) and the generator in GAN. Compared to other transformer-based models, the proposed Forged-GAN-BERT model demonstrates an improved performance with F1 scores of 0.97 and 0.71 for identifying forged novels in single-author and multi-author classification settings. Additionally, we explore different prompt categories for generating the forged novels to analyse the quality of the generated texts using different similarity distance measures, including ROUGE-1, Jaccard Similarity, Overlap Confident, and Cosine Similarity.</abstract>
      <url hash="4026f703">2024.eacl-srw.26</url>
      <bibkey>silva-etal-2024-forged</bibkey>
    </paper>
    <paper id="27">
      <title>Thesis Proposal: Detecting Empathy Using Multimodal Language Model</title>
      <author><first>Md Rakibul</first><last>Hasan</last><affiliation>Curtin University of Technology and BRAC University, Bangladesh</affiliation></author>
      <author><first>Md Zakir</first><last>Hossain</last><affiliation>CSIRO and Australian National University</affiliation></author>
      <author><first>Aneesh</first><last>Krishna</last><affiliation>NA</affiliation></author>
      <author><first>Shafin</first><last>Rahman</last><affiliation>North South University</affiliation></author>
      <author><first>Tom</first><last>Gedeon</last></author>
      <pages>338-349</pages>
      <abstract>Empathy is crucial in numerous social interactions, including human-robot, patient-doctor, teacher-student, and customer-call centre conversations. Despite its importance, empathy detection in videos continues to be a challenging task because of the subjective nature of empathy and often remains under-explored. Existing studies have relied on scripted or semi-scripted interactions in text-, audio-, or video-only settings that fail to capture the complexities and nuances of real-life interactions. This PhD research aims to fill these gaps by developing a multimodal language model (MMLM) that detects empathy in audiovisual data. In addition to leveraging existing datasets, the proposed study involves collecting real-life interaction video and audio. This study will leverage optimisation techniques like neural architecture search to deliver an optimised small-scale MMLM. Successful implementation of this project has significant implications in enhancing the quality of social interactions as it enables real-time measurement of empathy and thus provides potential avenues for training for better empathy in interactions.</abstract>
      <url hash="c8a058db">2024.eacl-srw.27</url>
      <bibkey>hasan-etal-2024-thesis</bibkey>
    </paper>
    <paper id="28">
      <title>Toward Sentiment Aware Semantic Change Analysis</title>
      <author><first>Roksana</first><last>Goworek</last></author>
      <author><first>Haim</first><last>Dubossarsky</last><affiliation>Queen Mary University of London</affiliation></author>
      <pages>350-357</pages>
      <abstract>This student paper explores the potential of augmenting computational models of semantic change with sentiment information. It tests the efficacy of this approach on the English SemEval of Lexical Semantic Change and its associated historical corpora. We first establish the feasibility of our approach by demonstrating that existing models extract reliable sentiment information from historical corpora, and then validate that words that underwent semantic change also show greater sentiment change in comparison to historically stable words. We then integrate sentiment information into standard models of semantic change for individual words, and test if this can improve the overall performance of the latter, showing mixed results. This research contributes to our understanding of language change by providing the first attempt to enrich standard models of semantic change with additional information. It taps into the multifaceted nature of language change, that should not be reduced only to binary or scalar report of change, but adds additional dimensions to this change, sentiment being only one of these. As such, this student paper suggests novel directions for future work in integrating additional, more nuanced information of change and interpretation for finer-grained semantic change analysis.</abstract>
      <url hash="02b026c8">2024.eacl-srw.28</url>
      <bibkey>goworek-dubossarsky-2024-toward</bibkey>
    </paper>
    <paper id="29">
      <title>Dynamic Task-Oriented Dialogue: A Comparative Study of Llama-2 and Bert in Slot Value Generation</title>
      <author><first>Tiziano</first><last>Labruna</last></author>
      <author><first>Sofia</first><last>Brenna</last><affiliation>Free University of Bozen</affiliation></author>
      <author><first>Bernardo</first><last>Magnini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>358-368</pages>
      <abstract>Recent advancements in instruction-based language models have demonstrated exceptional performance across various natural language processing tasks. We present a comprehensive analysis of the performance of two open-source language models, BERT and Llama-2, in the context of dynamic task-oriented dialogues. Focusing on the Restaurant domain and utilizing the MultiWOZ 2.4 dataset, our investigation centers on the models’ ability to generate predictions for masked slot values within text. The dynamic aspect is introduced through simulated domain changes, mirroring real-world scenarios where new slot values are incrementally added to a domain over time.This study contributes to the understanding of instruction-based models’ effectiveness in dynamic natural language understanding tasks when compared to traditional language models and emphasizes the significance of open-source, reproducible models in advancing research within the academic community.</abstract>
      <url hash="e6a5f347">2024.eacl-srw.29</url>
      <bibkey>labruna-etal-2024-dynamic</bibkey>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2024-03-03" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts</booktitle>
      <editor><first>Mohsen</first><last>Mesgar</last></editor>
      <editor><first>Sharid</first><last>Loáiciga</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St. Julian’s, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="4b13db80">2024.eacl-tutorials</url>
      <venue>eacl</venue>
    </meta>
    <frontmatter>
      <url hash="e1e5283d">2024.eacl-tutorials.0</url>
      <bibkey>eacl-2024-european-chapter-association</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Computational modeling of semantic change</title>
      <author><first>Pierluigi</first><last>Cassotti</last></author>
      <author><first>Francesco</first><last>Periti</last></author>
      <author><first>Stefano</first><last>de Pascale</last></author>
      <author><first>Haim</first><last>Dubossarsky</last></author>
      <author><first>Nina</first><last>Tahmasebi</last></author>
      <pages>1-8</pages>
      <abstract>Languages change constantly over time, influenced by social, technological, cultural and political factors that affect how people express themselves. In particular, words can undergo the process of semantic change, which can be subtle and significantly impact the interpretation of texts. For example, the word terrific used to mean ‘causing terror’ and was as such synonymous to terrifying. Nowadays, speakers use the word in the sense of ‘excessive’ and even ‘amazing’. In Historical Linguistics, tools and methods have been developed to analyse this phenomenon, including systematic categorisations of the types of change, the causes and the mechanisms underlying the different types of change. However, traditional linguistic methods, while informative, are often based on small, carefully curated samples. Thanks to the availability of both large diachronic corpora, the computational means to model word meaning unsupervised, and evaluation benchmarks, we are seeing an increasing interest in the computational modelling of semantic change. This is evidenced by the increasing number of publications in this new domain as well as the organisation of initiatives and events related to this topic, such as four editions of the International Workshop on Computational Approaches to Historical Language Change LChange1, and several evaluation campaigns (Schlechtweg et al., 2020a; Basile et al., 2020b; Kutuzov et al.; Zamora-Reina et al., 2022).</abstract>
      <url hash="64cae14e">2024.eacl-tutorials.1</url>
      <bibkey>cassotti-etal-2024-computational</bibkey>
    </paper>
    <paper id="2">
      <title>Item Response Theory for Natural Language Processing</title>
      <author><first>John P.</first><last>Lalor</last></author>
      <author><first>Pedro</first><last>Rodriguez</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Jose</first><last>Hernandez-Orallo</last></author>
      <pages>9-13</pages>
      <abstract>This tutorial will introduce the NLP community to Item Response Theory (IRT; Baker 2001). IRT is a method from the field of psychometrics for model and dataset assessment. IRT has been used for decades to build test sets for human subjects and estimate latent characteristics of dataset examples. Recently, there has been an uptick in work applying IRT to tasks in NLP. It is our goal to introduce the wider NLP community to IRT and show its benefits for a number of NLP tasks. From this tutorial, we hope to encourage wider adoption of IRT among NLP researchers.</abstract>
      <url hash="b72deb42">2024.eacl-tutorials.2</url>
      <bibkey>lalor-etal-2024-item</bibkey>
    </paper>
    <paper id="3">
      <title>Language + Molecules</title>
      <author><first>Carl</first><last>Edwards</last></author>
      <author><first>Qingyun</first><last>Wang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>14-20</pages>
      <abstract>Climate change, access to food and water, pandemics–the world faces an enormous number of problems in the coming decades on scales of complexity never-before-seen. To address these issues, development of scientific solutions which are scalable, flexible, and inexpensive are critical. Over the last couple years, considerable interest has arisen for applying natural language-driven solutions to these problems. Particularly, the chemistry field is posed to be substantially accelerated by language+molecule models. This tutorial is designed to provide an introduction to this area of research. It requires no knowledge outside mainstream NLP, and it will enable participants to begin exploring relevant research. By discussing cutting-edge work, we will highlight the key roles language can fill for 1) abstract, compositional control of generative models, 2) bridging different biochemical modalities, 3) planning experimental procedures, and 4) broadening access to computational approaches. Beyond this, language models have also seen considerable success when applied to proteins or molecule structures, which can be considered as ‘exotic’ languages, and computational linguistics researchers’ expertise can be highly valuable for these impactful, possibly life-saving tasks.</abstract>
      <url hash="2fec547a">2024.eacl-tutorials.3</url>
      <bibkey>edwards-etal-2024-language</bibkey>
    </paper>
    <paper id="4">
      <title>Transformer-specific Interpretability</title>
      <author><first>Hosein</first><last>Mohebbi</last></author>
      <author><first>Jaap</first><last>Jumelet</last></author>
      <author><first>Michael</first><last>Hanna</last></author>
      <author><first>Afra</first><last>Alishahi</last></author>
      <author><first>Willem</first><last>Zuidema</last></author>
      <pages>21-26</pages>
      <abstract>Transformers have emerged as dominant play- ers in various scientific fields, especially NLP. However, their inner workings, like many other neural networks, remain opaque. In spite of the widespread use of model-agnostic interpretability techniques, including gradient-based and occlusion-based, their shortcomings are becoming increasingly apparent for Transformer interpretation, making the field of interpretability more demanding today. In this tutorial, we will present Transformer-specific interpretability methods, a new trending approach, that make use of specific features of the Transformer architecture and are deemed more promising for understanding Transformer-based models. We start by discussing the potential pitfalls and misleading results model-agnostic approaches may produce when interpreting Transformers. Next, we discuss Transformer-specific methods, including those designed to quantify context- mixing interactions among all input pairs (as the fundamental property of the Transformer architecture) and those that combine causal methods with low-level Transformer analysis to identify particular subnetworks within a model that are responsible for specific tasks. By the end of the tutorial, we hope participants will understand the advantages (as well as current limitations) of Transformer-specific interpretability methods, along with how these can be applied to their own research.</abstract>
      <url hash="8f958556">2024.eacl-tutorials.4</url>
      <bibkey>mohebbi-etal-2024-transformer</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>LLM</fixed-case>s for Low Resource Languages in Multilingual, Multimodal and Dialectal Settings</title>
      <author><first>Firoj</first><last>Alam</last></author>
      <author><first>Shammur Absar</first><last>Chowdhury</last></author>
      <author><first>Sabri</first><last>Boughorbel</last></author>
      <author><first>Maram</first><last>Hasanain</last></author>
      <pages>27-33</pages>
      <abstract>The recent breakthroughs in Artificial Intelligence (AI) can be attributed to the remarkable performance of Large Language Models (LLMs) across a spectrum of research areas (e.g., machine translation, question-answering, automatic speech recognition, text-to-speech generation) and application domains (e.g., business, law, healthcare, education, and psychology). The success of these LLMs largely de- pends on specific training techniques, most notably instruction tuning, RLHF, and subsequent prompting to achieve the desired output. As the development of such LLMs continues to increase in both closed and open settings, evaluation has become crucial for understanding their generalization capabilities across different tasks, modalities, languages, and dialects. This evaluation process is tightly coupled with prompting, which plays a key role in obtain- ing better outputs. There has been attempts to evaluate such models focusing on diverse tasks, languages, and dialects, which suggests that the capabilities of LLMs are still limited to medium-to-low-resource languages due to the lack of representative datasets. The tutorial offers an overview of this emerging research area. We explore the capabilities of LLMs in terms of their performance, zero- and few-shot settings, fine-tuning, instructions tuning, and close vs. open models with a special emphasis on low-resource settings. In addition to LLMs for standard NLP tasks, we will focus on speech and multimodality.</abstract>
      <url hash="b103fda2">2024.eacl-tutorials.5</url>
      <bibkey>alam-etal-2024-llms</bibkey>
    </paper>
  </volume>
  <event id="eacl-2024">
    <meta>
      <title>The 18th Conference of the European Chapter of the Association for Computational Linguistics</title>
      <location>St. Julian’s, Malta</location>
      <dates>March, 2024</dates>
    </meta>
    <links>
      <url type="website">https://2024.eacl.org</url>
    </links>
    <colocated>
      <volume-id>2024.findings-eacl</volume-id>
      <volume-id>2024.caldpseudo-1</volume-id>
      <volume-id>2024.case-1</volume-id>
      <volume-id>2024.clpsych-1</volume-id>
      <volume-id>2024.codi-1</volume-id>
      <volume-id>2024.computel-1</volume-id>
      <volume-id>2024.dravidianlangtech-1</volume-id>
      <volume-id>2024.latechclfl-1</volume-id>
      <volume-id>2024.law-1</volume-id>
      <volume-id>2024.ltedi-1</volume-id>
      <volume-id>2024.moomin-1</volume-id>
      <volume-id>2024.nlp4hr-1</volume-id>
      <volume-id>2024.personalize-1</volume-id>
      <volume-id>2024.scalellm-1</volume-id>
      <volume-id>2024.scichat-1</volume-id>
      <volume-id>2024.sigtyp-1</volume-id>
      <volume-id>2024.teicai-1</volume-id>
      <volume-id>2024.uncertainlp-1</volume-id>
      <volume-id>2024.unimplicit-1</volume-id>
      <volume-id>2024.wnut-1</volume-id>
    </colocated>
  </event>
</collection>
