<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.eacl">
  <volume id="demo" ingest-date="2024-03-03" type="proceedings">
    <meta>
      <booktitle>Proceedings of the The 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</booktitle>
      <editor><first>Nikolaos</first><last>Aletras</last></editor>
      <editor><first>Orphee</first><last>De Clercq</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St. Julians, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="a56379aa">2024.eacl-demo</url>
      <venue>eacl</venue>
    </meta>
    <frontmatter>
      <url hash="96e3f288">2024.eacl-demo.0</url>
      <bibkey>eacl-2024-european</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>T</fixed-case>ext<fixed-case>BI</fixed-case>: An Interactive Dashboard for Visualizing Multidimensional <fixed-case>NLP</fixed-case> Annotations in Social Media Data</title>
      <author><first>Maxime</first><last>Masson</last><affiliation>LIUPPA, E2S, University of Pau and Pays Adour</affiliation></author>
      <author><first>Christian</first><last>Sallaberry</last><affiliation>LIUPPA, E2S, University of Pau and Pays Adour (UPPA)</affiliation></author>
      <author><first>Marie-Noelle</first><last>Bessagnet</last><affiliation>LIUPPA, E2S, University of Pau and Pays Adour (UPPA)</affiliation></author>
      <author><first>Annig</first><last>Le Parc Lacayrelle</last><affiliation>LIUPPA, E2S, University of Pau and Pays Adour (UPPA)</affiliation></author>
      <author><first>Philippe</first><last>Roose</last><affiliation>LIUPPA, E2S, University of Pau and Pays Adour (UPPA)</affiliation></author>
      <author><first>Rodrigo</first><last>Agerri</last><affiliation>HiTZ Center-Ixa, University of the Basque Country UPV/EHU</affiliation></author>
      <pages>1-9</pages>
      <abstract>In this paper we introduce TextBI, a multimodal generic dashboard designed to present multidimensional text annotations on large volumes of multilingual social media data. This tool focuses on four core dimensions: spatial, temporal, thematic, and personal, and also supports additional enrichment data such as sentiment and engagement. Multiple visualization modes are offered, including frequency, movement, and association. This dashboard addresses the challenge of facilitating the interpretation of NLP annotations by visualizing them in a user-friendly, interactive interface catering to two categories of users: (1) domain stakeholders and (2) NLP researchers. We conducted experiments within the domain of tourism leveraging data from X (formerly Twitter) and incorporating requirements from tourism offices. Our approach, TextBI, represents a significant advancement in the field of visualizing NLP annotations by integrating and blending features from a variety of Business Intelligence, Geographical Information Systems and NLP tools. A demonstration video is also provided https://youtu.be/x714RKvo9Cg</abstract>
      <url hash="df77c7ee">2024.eacl-demo.1</url>
      <bibkey>masson-etal-2024-textbi</bibkey>
    </paper>
    <paper id="2">
      <title>k<fixed-case>NN</fixed-case>-<fixed-case>BOX</fixed-case>: A Unified Framework for Nearest Neighbor Generation</title>
      <author><first>Wenhao</first><last>Zhu</last><affiliation>National Key Laboratory for Novel Software Technology, Nanjing University</affiliation></author>
      <author><first>Qianfeng</first><last>Zhao</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yunzhe</first><last>Lv</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>National Key Laboratory for Novel Software Technology, Nanjing University</affiliation></author>
      <author><first>Siheng</first><last>Zhao</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Sizhe</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <pages>10-17</pages>
      <abstract>Augmenting the base neural model with a token-level symbolic datastore is a novel generation paradigm and has achieved promising results in machine translation (MT). In this paper, we introduce a unified framework kNN-BOX, which enables quick development and visualization for this novel paradigm. kNN-BOX decomposes the datastore-augmentation approach into three modules: datastore, retriever and combiner, thus putting diverse kNN generation methods into a unified way. Currently, kNN-BOX has provided implementation of seven popular kNN-MT variants, covering research from performance enhancement to efficiency optimization. It is easy for users to reproduce these existing work or customize their own models. Besides, users can interact with their kNN generation systems with kNN-BOX to better understand the underlying inference process in a visualized way. In experiment section, we apply kNN-BOX for machine translation and three other seq2seq generation tasks (text simplification, paraphrase generation and question generation). Experiment results show that augmenting the base neural model with kNN-BOX can bring large performance improvement in all these tasks. The code and document of kNN-BOX is available at https://github.com/NJUNLP/knn-box. The demo can be accessed at http://nlp.nju.edu.cn/demo/knn-box/. The introduction video is available at https://www.youtube.com/watch?v=m0eJldHVR3w.</abstract>
      <url hash="b981b9a2">2024.eacl-demo.2</url>
      <bibkey>zhu-etal-2024-knn</bibkey>
    </paper>
    <paper id="3">
      <title>A Human-Centric Evaluation Platform for Explainable Knowledge Graph Completion</title>
      <author><first>Zhao</first><last>Xu</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Wiem</first><last>Ben Rim</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Kiril</first><last>Gashteovski</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Timo</first><last>Sztyler</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Carolin</first><last>Lawrence</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <pages>18-26</pages>
      <abstract>Explanations for AI are expected to help human users understand AI-driven predictions. Evaluating plausibility, the helpfulness of the explanations, is therefore essential for developing eXplainable AI (XAI) that can really aid human users. Here we propose a human-centric evaluation platform to measure plausibility of explanations in the context of eXplainable Knowledge Graph Completion (XKGC). The target audience of the platform are researchers and practitioners who want to 1) investigate real needs and interests of their target users in XKGC, 2) evaluate the plausibility of the XKGC methods. We showcase these two use cases in an experimental setting to illustrate what results can be achieved with our system.</abstract>
      <url hash="20749d1e">2024.eacl-demo.3</url>
      <bibkey>xu-etal-2024-human</bibkey>
    </paper>
    <paper id="4">
      <title>py<fixed-case>TLEX</fixed-case>: A Python Library for <fixed-case>T</fixed-case>ime<fixed-case>L</fixed-case>ine <fixed-case>EX</fixed-case>traction</title>
      <author><first>Akul</first><last>Singh</last><affiliation>Florida International University</affiliation></author>
      <author><first>Jared</first><last>Hummer</last><affiliation>Florida International University</affiliation></author>
      <author><first>Mustafa</first><last>Ocal</last><affiliation>Florida International University</affiliation></author>
      <author><first>Mark</first><last>Finlayson</last><affiliation>FIU</affiliation></author>
      <pages>27-34</pages>
      <abstract>pyTLEX is an implementation of the TimeLine EXtraction algorithm (TLEX; Finlayson et al.,2021) that enables users to work with TimeML annotations and perform advanced temporal analysis, offering a comprehensive suite of features. TimeML is a standardized markup language for temporal information in text. pyTLEX allows users to parse TimeML annotations, construct TimeML graphs, and execute the TLEX algorithm to effect complete timeline extraction. In contrast to previous implementations (i.e., jTLEX for Java), pyTLEX sets itself apart with a range of advanced features. It introduces a React-based visualization system, enhancing the exploration of temporal data and the comprehension of temporal connections within textual information. Furthermore, pyTLEX incorporates an algorithm for increasing connectivity in temporal graphs, which identifies graph disconnectivity and recommends links based on temporal reasoning, thus enhancing the coherence of the graph representation. Additionally, pyTLEX includes a built-in validation algorithm, ensuring compliance with TimeML annotation guidelines, which is essential for maintaining data quality and reliability. pyTLEX equips researchers and developers with an extensive toolkit for temporal analysis, and its testing across various datasets validates its accuracy and reliability.</abstract>
      <url hash="f73e10d4">2024.eacl-demo.4</url>
      <bibkey>singh-etal-2024-pytlex</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>D</fixed-case>epress<fixed-case>M</fixed-case>ind: A Depression Surveillance System for Social Media Analysis</title>
      <author><first>Roque</first><last>Fernández-Iglesias</last><affiliation>University of Santiago de Compostela</affiliation></author>
      <author><first>Marcos</first><last>Fernandez-Pichel</last><affiliation>University of Santiago de Compostela</affiliation></author>
      <author><first>Mario</first><last>Aragon</last><affiliation>Universidade de Santiago de Compostela</affiliation></author>
      <author><first>David E.</first><last>Losada</last><affiliation>University of Santiago de Compostela</affiliation></author>
      <pages>35-43</pages>
      <abstract>Depression is a pressing global issue that impacts millions of individuals worldwide. This prevailing psychologicaldisorder profoundly influences the thoughts and behavior of those who suffer from it. We have developed DepressMind, a versatile screening tool designed to facilitate the analysis of social network data. This automated tool explores multiple psychological dimensions associated with clinical depression and estimates the extent to which these symptoms manifest in language use. Our project comprises two distinct components: one for data extraction and another one for analysis.The data extraction phase is dedicated to harvesting texts and the associated meta-information from social networks and transforming them into a user-friendly format that serves various analytical purposes.For the analysis, the main objective is to conduct an in-depth inspection of the user publications and establish connections between the posted contents and dimensions or traits defined by well-established clinical instruments.Specifically, we aim to associate extracts authored by individuals with symptoms or dimensions of the Beck Depression Inventory (BDI).</abstract>
      <url hash="490e6099">2024.eacl-demo.5</url>
      <bibkey>fernandez-iglesias-etal-2024-depressmind</bibkey>
    </paper>
    <paper id="6">
      <title>Check News in One Click: <fixed-case>NLP</fixed-case>-Empowered Pro-Kremlin Propaganda Detection</title>
      <author><first>Veronika</first><last>Solopova</last><affiliation>Freie University of Berlin</affiliation></author>
      <author><first>Viktoriia</first><last>Herman</last><affiliation>Freie University of Berlin</affiliation></author>
      <author><first>Christoph</first><last>Benzmüller</last><affiliation>FU Berlin</affiliation></author>
      <author><first>Tim</first><last>Landgraf</last><affiliation>FU Berlin</affiliation></author>
      <pages>44-51</pages>
      <abstract>Many European citizens become targets of the Kremlin propaganda campaigns, aiming to minimise public support for Ukraine, foster a climate of mistrust and disunity, and shape elections (Meister, 2022). To address this challenge, we developed “Check News in 1 Click”, the first NLP-empowered pro-Kremlin propaganda detection application available in 7 languages, which provides the lay user with feedback on their news, and explains manipulative linguistic features and keywords. We conducted a user study, analysed user entries and models’ behaviour paired with questionnaire answers, and investigated the advantages and disadvantages of the proposed interpretative solution.</abstract>
      <url hash="9673cb80">2024.eacl-demo.6</url>
      <bibkey>solopova-etal-2024-check</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>NESTLE</fixed-case>: a No-Code Tool for Statistical Analysis of Legal Corpus</title>
      <author><first>Kyoungyeon</first><last>Cho</last><affiliation>LBox</affiliation></author>
      <author><first>Seungkum</first><last>Han</last><affiliation>LBox</affiliation></author>
      <author><first>Young Rok</first><last>Choi</last><affiliation>LBox</affiliation></author>
      <author><first>Wonseok</first><last>Hwang</last><affiliation>LBox</affiliation></author>
      <pages>52-61</pages>
      <abstract>The statistical analysis of large scale legal corpus can provide valuable legal insights. For such analysis one needs to (1) select a subset of the corpus using document retrieval tools, (2) structure text using information extraction (IE) systems, and (3) visualize the data for the statistical analysis. Each process demands either specialized tools or programming skills whereas no comprehensive unified “no-code” tools have been available. Here we provide NESTLE, a no-code tool for large-scale statistical analysis of legal corpus. Powered by a Large Language Model (LLM) and the internal custom end-to-end IE system, NESTLE can extract any type of information that has not been predefined in the IE system opening up the possibility of unlimited customizable statistical analysis of the corpus without writing a single line of code. We validate our system on 15 Korean precedent IE tasks and 3 legal text classification tasks from LexGLUE. The comprehensive experiments reveal NESTLE can achieve GPT-4 comparable performance by training the internal IE module with 4 human-labeled, and 192 LLM-labeled examples.</abstract>
      <url hash="03716bd4">2024.eacl-demo.7</url>
      <bibkey>cho-etal-2024-nestle</bibkey>
    </paper>
    <paper id="8">
      <title>Multi-party Multimodal Conversations Between Patients, Their Companions, and a Social Robot in a Hospital Memory Clinic</title>
      <author><first>Angus</first><last>Addlesee</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Neeraj</first><last>Cherakara</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Nivan</first><last>Nelson</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Daniel</first><last>Hernandez Garcia</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Nancie</first><last>Gunson</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Weronika</first><last>Sieińska</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Christian</first><last>Dondrup</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Oliver</first><last>Lemon</last><affiliation>Heriot-Watt University</affiliation></author>
      <pages>62-70</pages>
      <abstract>We have deployed an LLM-based spoken dialogue system in a real hospital. The ARI social robot embodies our system, which patients and their companions can have multi-party conversations with together. In order to enable this multi-party ability, multimodality is critical. Our system, therefore, receives speech and video as input, and generates both speech and gestures (arm, head, and eye movements). In this paper, we describe our complex setting and the architecture of our dialogue system. Each component is detailed, and a video of the full system is available with the appropriate components highlighted in real-time. Our system decides when it should take its turn, generates human-like clarification requests when the patient pauses mid-utterance, answers in-domain questions (grounding to the in-prompt knowledge), and responds appropriately to out-of-domain requests (like generating jokes or quizzes). This latter feature is particularly remarkable as real patients often utter unexpected sentences that could not be handled previously.</abstract>
      <url hash="9e81e1d3">2024.eacl-demo.8</url>
      <bibkey>addlesee-etal-2024-multi</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>S</fixed-case>cam<fixed-case>S</fixed-case>pot: Fighting Financial Fraud in <fixed-case>I</fixed-case>nstagram Comments</title>
      <author><first>Stefan</first><last>Erben</last><affiliation>Lucerne University of Applied Sciences and Arts</affiliation></author>
      <author><first>Andreas</first><last>Waldis</last><affiliation>Hochschule Luzern</affiliation></author>
      <pages>71-81</pages>
      <abstract>The long-standing problem of spam and fraudulent messages in the comment sections of Instagram pages in the financial sector claims new victims every day. Instagram’s current spam filter proves inadequate, and existing research approaches are primarily confined to theoretical concepts. Practical implementations with evaluated results are missing. To solve this problem, we propose ScamSpot, a comprehensive system that includes a browser extension, a fine-tuned BERT model and a REST API. This approach ensures public accessibility of our results for Instagram users using the Chrome browser. Furthermore, we conduct a data annotation study, shedding light on the reasons and causes of the problem and evaluate the system through user feedback and comparison with existing models. ScamSpot is an open-source project and is publicly available at https://scamspot.github.io/.</abstract>
      <url hash="d6613b8d">2024.eacl-demo.9</url>
      <bibkey>erben-waldis-2024-scamspot</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>N</fixed-case>arrative<fixed-case>P</fixed-case>lay: Interactive Narrative Understanding</title>
      <author><first>Runcong</first><last>Zhao</last><affiliation>King’s College London</affiliation></author>
      <author><first>Wenjia</first><last>Zhang</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Jiazheng</first><last>Li</last><affiliation>King’s College London</affiliation></author>
      <author><first>Lixing</first><last>Zhu</last><affiliation>Department of Computer Science, University of Warwick</affiliation></author>
      <author><first>Yanran</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London</affiliation></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London</affiliation></author>
      <pages>82-93</pages>
      <abstract>In this paper, we introduce NarrativePlay, a novel system that allows users to role-play a fictional character and interact with other characters in narratives in an immersive environment. We leverage Large Language Models (LLMs) to generate human-like responses, guided by personality traits extracted from narratives. The system incorporates auto-generated visual display of narrative settings, character portraits, and character speech, greatly enhancing the user experience. Our approach eschews predefined sandboxes, focusing instead on main storyline events from the perspective of a user-selected character. NarrativePlay has been evaluated on two types of narratives, detective and adventure stories, where users can either explore the world or increase affinity with other characters through conversations.</abstract>
      <url hash="a8809656">2024.eacl-demo.10</url>
      <bibkey>zhao-etal-2024-narrativeplay</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>DP</fixed-case>-<fixed-case>NMT</fixed-case>: Scalable Differentially Private Machine Translation</title>
      <author><first>Timour</first><last>Igamberdiev</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <author><first>Doan Nam Long</first><last>Vu</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <author><first>Felix</first><last>Kuennecke</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Zhuo</first><last>Yu</last><affiliation>Department of Computer Science, Technical University of Darmstadt</affiliation></author>
      <author><first>Jannik</first><last>Holmer</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Ivan</first><last>Habernal</last><affiliation>Paderborn University</affiliation></author>
      <pages>94-105</pages>
      <abstract>Neural machine translation (NMT) is a widely popular text generation task, yet there is a considerable research gap in the development of privacy-preserving NMT models, despite significant data privacy concerns for NMT systems. Differentially private stochastic gradient descent (DP-SGD) is a popular method for training machine learning models with concrete privacy guarantees; however, the implementation specifics of training a model with DP-SGD are not always clarified in existing models, with differing software libraries used and code bases not always being public, leading to reproducibility issues. To tackle this, we introduce DP-NMT, an open-source framework for carrying out research on privacy-preserving NMT with DP-SGD, bringing together numerous models, datasets, and evaluation metrics in one systematic software package. Our goal is to provide a platform for researchers to advance the development of privacy-preserving NMT systems, keeping the specific details of the DP-SGD algorithm transparent and intuitive to implement. We run a set of experiments on datasets from both general and privacy-related domains to demonstrate our framework in use. We make our framework publicly available and welcome feedback from the community.</abstract>
      <url hash="cfc57f01">2024.eacl-demo.11</url>
      <bibkey>igamberdiev-etal-2024-dp</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>A</fixed-case>nno<fixed-case>P</fixed-case>lot: Interactive Visualizations of Text Annotations</title>
      <author><first>Elisabeth</first><last>Fittschen</last><affiliation>Uni Hamburg</affiliation></author>
      <author><first>Tim</first><last>Fischer</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Daniel</first><last>Brühl</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Julia</first><last>Spahr</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Yuliia</first><last>Lysa</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Phuoc Thang</first><last>Le</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>106-114</pages>
      <abstract>This paper presents AnnoPlot, a web application designed to analyze, manage, and visualize annotated text data.Users can configure projects, upload datasets, and explore their data through interactive visualization of span annotations with scatter plots, clusters, and statistics. AnnoPlot supports various transformer models to compute high-dimensional embeddings of text annotations and utilizes dimensionality reduction algorithms to offer users a novel 2D view of their datasets.A dynamic approach to dimensionality reduction allows users to adjust visualizations in real-time, facilitating category reorganization and error identification. The proposed application is open-source, promoting transparency and user control.Especially suited for the Digital Humanities, AnnoPlot offers a novel solution to address challenges in dynamic annotation datasets, empowering users to enhance data integrity and adapt to evolving categorizations.</abstract>
      <url hash="088189e5">2024.eacl-demo.12</url>
      <bibkey>fittschen-etal-2024-annoplot</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>G</fixed-case>eospa<fixed-case>C</fixed-case>y: A tool for extraction and geographical referencing of spatial expressions in textual data</title>
      <author><first>Syed</first><last>Mehtab Alam</last><affiliation>CIRAD, TETIS</affiliation></author>
      <author><first>Elena</first><last>Arsevska</last><affiliation>Cirad, Inra</affiliation></author>
      <author><first>Mathieu</first><last>Roche</last><affiliation>CIRAD, TETIS</affiliation></author>
      <author><first>Maguelonne</first><last>Teisseire</last><affiliation>UMR TETIS (Earth Observation and Geoinformation for Environment and Land Management research Unit)</affiliation></author>
      <pages>115-126</pages>
      <abstract>Spatial information in text enables to understand the geographical context and relationships within text for better decision-making across various domains such as disease surveillance, disaster management and other location based services. Therefore, it is crucial to understand the precise geographical context for location-sensitive applications. In response to this necessity, we introduce the GeospaCy software tool, designed for the extraction and georeferencing of spatial information present in textual data. GeospaCy fulfils two primary objectives: 1) Geoparsing, which involves extracting spatial expressions, encompassing place names and associated spatial relations within the text data, and 2) Geocoding, which facilitates the assignment of geographical coordinates to the spatial expressions extracted during the Geoparsing task. Geoparsing is evaluated with a disease news article dataset consisting of event information, whereas a qualitative evaluation of geographical coordinates (polygons/geometries) of spatial expressions is performed by end-users for Geocoding task.</abstract>
      <url hash="ce33be76">2024.eacl-demo.13</url>
      <bibkey>mehtab-alam-etal-2024-geospacy</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>MAMMOTH</fixed-case>: Massively Multilingual Modular Open Translation @ <fixed-case>H</fixed-case>elsinki</title>
      <author><first>Timothee</first><last>Mickus</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Stig-Arne</first><last>Grönroos</last><affiliation>Silo.AI</affiliation></author>
      <author><first>Joseph</first><last>Attieh</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Michele</first><last>Boggia</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Ona</first><last>De Gibert</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Shaoxiong</first><last>Ji</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Niki Andreas</first><last>Loppi</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Alessandro</first><last>Raganato</last><affiliation>University of Milano-Bicocca</affiliation></author>
      <author><first>Raúl</first><last>Vázquez</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Jörg</first><last>Tiedemann</last><affiliation>University of Helsinki</affiliation></author>
      <pages>127-136</pages>
      <abstract>NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled. The trend goes to modularization, a necessary step into the direction of designing smaller sub-networks and components with specialized functionality. In this paper, we present the MAMMOTH toolkit: a framework designed for training massively multilingual modular machine translation systems at scale, initially derived from OpenNMT-py and then adapted to ensure efficient training across computation clusters.We showcase its efficiency across clusters of A100 and V100 NVIDIA GPUs, and discuss our design philosophy and plans for future information.The toolkit is publicly available online at https://github.com/Helsinki-NLP/mammoth.</abstract>
      <url hash="f7dfc576">2024.eacl-demo.14</url>
      <bibkey>mickus-etal-2024-mammoth</bibkey>
    </paper>
    <paper id="15">
      <title>The <fixed-case>DUR</fixed-case>el Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change</title>
      <author><first>Dominik</first><last>Schlechtweg</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Shafqat Mumtaz</first><last>Virk</last><affiliation>Språkbanken Text, Dept. of Swedish University of Gothenburg</affiliation></author>
      <author><first>Pauline</first><last>Sander</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Emma</first><last>Sköldberg</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Lukas</first><last>Theuer Linke</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Tuo</first><last>Zhang</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Nina</first><last>Tahmasebi</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Jonas</first><last>Kuhn</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Sabine</first><last>Schulte Im Walde</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>137-149</pages>
      <abstract>We present the DURel tool implementing the annotation of semantic proximity between word uses into an online, open source interface. The tool supports standardized human annotation as well as computational annotation, building on recent advances with Word-in-Context models. Annotator judgments are clustered with automatic graph clustering techniques and visualized for analysis. This allows to measure word senses with simple and intuitive micro-task judgments between use pairs, requiring minimal preparation efforts. The tool offers additional functionalities to compare the agreement between annotators to guarantee the inter-subjectivity of the obtained judgments and to calculate summary statistics over the annotated data giving insights into sense frequency distributions, semantic variation or changes of senses over time.</abstract>
      <url hash="a47730f3">2024.eacl-demo.15</url>
      <bibkey>schlechtweg-etal-2024-durel</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>RAGA</fixed-case>s: Automated Evaluation of Retrieval Augmented Generation</title>
      <author><first>Shahul</first><last>Es</last><affiliation>Exploding Gradients</affiliation></author>
      <author><first>Jithin</first><last>James</last><affiliation>Exploding Gradients</affiliation></author>
      <author><first>Luis</first><last>Espinosa Anke</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Steven</first><last>Schockaert</last><affiliation>Cardiff University</affiliation></author>
      <pages>150-158</pages>
      <abstract>We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs is available at [https://github.com/explodinggradients/ragas]. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.</abstract>
      <url hash="8b5c2d0c">2024.eacl-demo.16</url>
      <bibkey>es-etal-2024-ragas</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>N</fixed-case>euro<fixed-case>P</fixed-case>rompts: An Adaptive Framework to Optimize Prompts for Text-to-Image Generation</title>
      <author><first>Shachar</first><last>Rosenman</last><affiliation>Intel Labs</affiliation></author>
      <author><first>Vasudev</first><last>Lal</last><affiliation>Intel Labs</affiliation></author>
      <author><first>Phillip</first><last>Howard</last><affiliation>Intel Labs</affiliation></author>
      <pages>159-167</pages>
      <abstract>Despite impressive recent advances in text-to-image diffusion models, obtaining high-quality images often requires prompt engineering by humans who have developed expertise in using them. In this work, we present NeuroPrompts, an adaptive framework that automatically enhances a user’s prompt to improve the quality of generations produced by text-to-image models. Our framework utilizes constrained text decoding with a pre-trained language model that has been adapted to generate prompts similar to those produced by human prompt engineers. This approach enables higher-quality text-to-image generations and provides user control over stylistic features via constraint set specification. We demonstrate the utility of our framework by creating an interactive application for prompt enhancement and image generation using Stable Diffusion. Additionally, we conduct experiments utilizing a large dataset of human-engineered prompts for text-to-image generation and show that our approach automatically produces enhanced prompts that result in superior image quality. We make our code, a screencast video demo and a live demo instance of NeuroPrompts publicly available.</abstract>
      <url hash="f715b7cd">2024.eacl-demo.17</url>
      <bibkey>rosenman-etal-2024-neuroprompts</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>MEGA</fixed-case>nno+: A Human-<fixed-case>LLM</fixed-case> Collaborative Annotation System</title>
      <author><first>Hannah</first><last>Kim</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Kushan</first><last>Mitra</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Rafael</first><last>Li Chen</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Sajjadur</first><last>Rahman</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Dan</first><last>Zhang</last><affiliation>Megagon Labs</affiliation></author>
      <pages>168-176</pages>
      <abstract>Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.</abstract>
      <url hash="629984d8">2024.eacl-demo.18</url>
      <bibkey>kim-etal-2024-meganno</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>X</fixed-case>-<fixed-case>AMR</fixed-case> Annotation Tool</title>
      <author><first>Shafiuddin Rehan</first><last>Ahmed</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Jon</first><last>Cai</last><affiliation>The University of Colorado</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado</affiliation></author>
      <author><first>James H.</first><last>Martin</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>177-186</pages>
      <abstract>This paper presents a novel {textbf{Cross}-document {textbf{A}bstract {textbf{M}eaning {textbf{R}epresentation (X-AMR) annotation tool designed for annotating key corpus-level event semantics. Leveraging machine assistance through the Prodigy Annotation Tool, we enhance the user experience, ensuring ease and efficiency in the annotation process. Through empirical analyses, we demonstrate the effectiveness of our tool in augmenting an existing event corpus, highlighting its advantages when integrated with GPT-4. Code and annotations: {href{https://anonymous.4open.science/r/xamr-9ED0}{anonymous.4open.science/r/xamr-9ED0}{footnote{Demo: {href{https://youtu.be/TuirftxciNE}{https://youtu.be/TuirftxciNE}} {footnote{Live Link: {href{https://tinyurl.com/mrxmafwh}{https://tinyurl.com/mrxmafwh}}</abstract>
      <url hash="0d9a8852">2024.eacl-demo.19</url>
      <bibkey>ahmed-etal-2024-x</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>D</fixed-case>oc<fixed-case>C</fixed-case>hecker: Bootstrapping Code Large Language Model for Detecting and Resolving Code-Comment Inconsistencies</title>
      <author><first>Anh</first><last>Dau</last><affiliation>FPT Software AI Center</affiliation></author>
      <author><first>Jin L.c.</first><last>Guo</last><affiliation>McGill University</affiliation></author>
      <author><first>Nghi</first><last>Bui</last><affiliation>Salesforce Research Asia</affiliation></author>
      <pages>187-194</pages>
      <abstract>Comments in source code are crucial for developers to understand the purpose of the code and to use it correctly. However, keeping comments aligned with the evolving codebase poses a significant challenge. With increasing interest in automated solutions to identify and rectify discrepancies between code and its associated comments, most existing methods rely heavily on heuristic rules. This paper introduces {textbf{DocChecker}, a language model-based framework adept at detecting inconsistencies between code and comments and capable of generating synthetic comments. This functionality allows {textbf{DocChecker} to identify and rectify cases where comments do not accurately represent the code they describe.The efficacy of DocChecker is demonstrated using the Just-In-Time and CodeXGlue datasets in various scenarios. Notably, DocChecker sets a new benchmark in the Inconsistency Code-Comment Detection (ICCD) task, achieving 72.3% accuracy, and scoring 33.64 in BLEU-4 on the code summarization task. These results surpass other Large Language Models (LLMs), including GPT 3.5 and CodeLlama.DocChecker is accessible for use and evaluation. It can be found on https://github.com/FSoft-AI4Code/DocChecker and at http://4.193.50.237:5000/. For a more comprehensive understanding of its functionality, a demonstration video is available on https://youtu.be/FqnPmd531xw.</abstract>
      <url hash="4ee92fab">2024.eacl-demo.20</url>
      <bibkey>dau-etal-2024-docchecker</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>TL</fixed-case>;<fixed-case>DR</fixed-case> Progress: Multi-faceted Literature Exploration in Text Summarization</title>
      <author><first>Shahbaz</first><last>Syed</last><affiliation>Leipzig University</affiliation></author>
      <author><first>Khalid</first><last>Al Khatib</last><affiliation>Groningen University</affiliation></author>
      <author><first>Martin</first><last>Potthast</last><affiliation>Leipzig University</affiliation></author>
      <pages>195-206</pages>
      <abstract>This paper presents TL;DR Progress, a new tool for exploring the literature on neural text summarization. It organizes 514~papers based on a comprehensive annotation scheme for text summarization approaches and enables fine-grained, faceted search. Each paper was manually annotated to capture aspects such as evaluation metrics, quality dimensions, learning paradigms, challenges addressed, datasets, and document domains. In addition, a succinct indicative summary is provided for each paper, describing contextual factors, issues, and proposed solutions. The tool is available at {{url{https://www.tldr-progress.de}}, a demo video at {{url{https://youtu.be/uCVRGFvXUj8}}</abstract>
      <url hash="54a7554b">2024.eacl-demo.21</url>
      <bibkey>syed-etal-2024-tl</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>FRAPPE</fixed-case>: <fixed-case>FRA</fixed-case>ming, Persuasion, and Propaganda Explorer</title>
      <author><first>Ahmed</first><last>Sajwani</last><affiliation>Khalifa University of Science and Technology</affiliation></author>
      <author><first>Alaa</first><last>El Setohy</last><affiliation>Egypt Japan University of Science and Technology</affiliation></author>
      <author><first>Ali</first><last>Mekky</last><affiliation>Alexandria University</affiliation></author>
      <author><first>Diana</first><last>Turmakhan</last><affiliation>Nazarbayev University</affiliation></author>
      <author><first>Lara</first><last>Hassan</last><affiliation>Alexandria University</affiliation></author>
      <author><first>Mohamed</first><last>El Zeftawy</last><affiliation>Alexandria University</affiliation></author>
      <author><first>Omar</first><last>El Herraoui</last><affiliation>NYU Abu Dhabi</affiliation></author>
      <author><first>Osama</first><last>Afzal</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Qisheng</first><last>Liao</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Tarek</first><last>Mahmoud</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>207-213</pages>
      <abstract>The abundance of news sources and the urgent demand for reliable information have led to serious concerns about the threat of misleading information. In this paper, we present FRAPPE, a FRAming, Persuasion, and Propaganda Explorer system. FRAPPE goes beyond conventional news analysis of articles and unveils the intricate linguistic techniques used to shape readers’ opinions and emotions. Our system allows users not only to analyze individual articles for their genre, framings, and use of persuasion techniques, but also to draw comparisons between the strategies of persuasion and framing adopted by a diverse pool of news outlets and countries across multiple languages for different topics, thus providing a comprehensive understanding of how information is presented and manipulated. FRAPPE is publicly accessible at https://frappe.streamlit.app/ and a video explaining our system is available at https://www.youtube.com/watch?v=3RlTfSVnZmk</abstract>
      <url hash="e66c3473">2024.eacl-demo.22</url>
      <bibkey>sajwani-etal-2024-frappe</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>LLM</fixed-case>e<fixed-case>B</fixed-case>ench: A Flexible Framework for Accelerating <fixed-case>LLM</fixed-case>s Benchmarking</title>
      <author><first>Fahim</first><last>Dalvi</last><affiliation>Qatar Computing Research Institute, HBKU</affiliation></author>
      <author><first>Maram</first><last>Hasanain</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Sabri</first><last>Boughorbel</last><affiliation>Qatar Computing Research Institute, HBKU</affiliation></author>
      <author><first>Basel</first><last>Mousi</last><affiliation>QCRI</affiliation></author>
      <author><first>Samir</first><last>Abdaljalil</last><affiliation>Texas A&amp;M University</affiliation></author>
      <author><first>Nizi</first><last>Nazar</last><affiliation>Qatar Computing Research Institute, HBKU</affiliation></author>
      <author><first>Ahmed</first><last>Abdelali</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Shammur Absar</first><last>Chowdhury</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Hamdy</first><last>Mubarak</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Ahmed</first><last>Ali</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>214-222</pages>
      <abstract>The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework, which can be seamlessly customized to evaluate LLMs for any NLP task, regardless of language. The framework features generic dataset loaders, several model providers, and pre-implements most standard evaluation metrics. It supports in-context learning with zero- and few-shot settings. A specific dataset and task can be evaluated for a given LLM in less than 20 lines of code while allowing full flexibility to extend the framework for custom datasets, models, or tasks. The framework has been tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We open-sourced LLMeBench for the community (https://github.com/qcri/LLMeBench/) and a video demonstrating the framework is available online (https://youtu.be/9cC2m_abk3A).</abstract>
      <url hash="b9920324">2024.eacl-demo.23</url>
      <bibkey>dalvi-etal-2024-llmebench</bibkey>
    </paper>
    <paper id="24">
      <title>Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling</title>
      <author><first>Talia</first><last>Tseriotou</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Ryan</first><last>Chan</last><affiliation>The Alan Turing Institute</affiliation></author>
      <author><first>Adam</first><last>Tsakalidis</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Iman Munire</first><last>Bilal</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Elena</first><last>Kochkina</last><affiliation>Queen Mary University</affiliation></author>
      <author><first>Terry</first><last>Lyons</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Maria</first><last>Liakata</last><affiliation>Queen Mary University of London</affiliation></author>
      <pages>223-237</pages>
      <abstract>We present an open-source, pip installable toolkit, Sig-Networks, the first of its kind for longitudinal language modelling. A central focus is the incorporation of Signature-based Neural Network models, which have recently shown success in temporal tasks. We apply and extend published research providing a full suite of signature-based models. Their components can be used as PyTorch building blocks in future architectures. Sig-Networks enables task-agnostic dataset plug-in, seamless preprocessing for sequential data, parameter flexibility, automated tuning across a range of models. We examine signature networks under three different NLP tasks of varying temporal granularity: counselling conversations, rumour stance switch and mood changes in social media threads, showing SOTA performance in all three, and provide guidance for future tasks. We release the Toolkit as a PyTorch package with an introductory video, Git repositories for preprocessing and modelling including sample notebooks on the modeled NLP tasks.</abstract>
      <url hash="31e5544c">2024.eacl-demo.24</url>
      <bibkey>tseriotou-etal-2024-sig</bibkey>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2024-03-03" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop</booktitle>
      <editor><first>Neele</first><last>Falk</last></editor>
      <editor><first>Sara</first><last>Papi</last></editor>
      <editor><first>Mike</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St. Julian’s, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="84eb508b">2024.eacl-srw</url>
      <venue>eacl</venue>
    </meta>
    <frontmatter>
      <url hash="a15c8f49">2024.eacl-srw.0</url>
      <bibkey>eacl-2024-european-chapter</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>A</fixed-case>uto<fixed-case>A</fixed-case>ugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes</title>
      <author><first>Juhwan</first><last>Choi</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Kyohoon</first><last>Jin</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Junho</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Sangmin</first><last>Song</last></author>
      <author><first>YoungBin</first><last>Kim</last><affiliation>ChungAng University</affiliation></author>
      <pages>1-8</pages>
      <abstract>Text data augmentation is a complex problem due to the discrete nature of sentences. Although rule-based augmentation methods are widely adopted in real-world applications because of their simplicity, they suffer from potential semantic damage. Previous researchers have suggested easy data augmentation with soft labels (softEDA), employing label smoothing to mitigate this problem. However, finding the best factor for each model and dataset is challenging; therefore, using softEDA in real-world applications is still difficult. In this paper, we propose adapting AutoAugment to solve this problem. The experimental results suggest that the proposed method can boost existing augmentation methods and that rule-based methods can enhance cutting-edge pretrained language models. We offer the source code.</abstract>
      <url hash="71328d13">2024.eacl-srw.1</url>
      <bibkey>choi-etal-2024-autoaugment</bibkey>
    </paper>
    <paper id="2">
      <title>Generating Diverse Translation with Perturbed <tex-math>k</tex-math><fixed-case>NN</fixed-case>-<fixed-case>MT</fixed-case></title>
      <author><first>Yuto</first><last>Nishida</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Makoto</first><last>Morishita</last><affiliation>NTT</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Division of Information Science, Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>9-31</pages>
      <abstract>Generating multiple translation candidates would enable users to choose the one that satisfies their needs.Although there has been work on diversified generation, there exists room for improving the diversity mainly because the previous methods do not address the overcorrection problem—the model underestimates a prediction that is largely different from the training data, even if that prediction is likely.This paper proposes methods that generate more diverse translations by introducing perturbed <tex-math>k</tex-math>-nearest neighbor machine translation (<tex-math>k</tex-math>NN-MT).Our methods expand the search space of <tex-math>k</tex-math>NN-MT and help incorporate diverse words into candidates by addressing the overcorrection problem.Our experiments show that the proposed methods drastically improve candidate diversity and control the degree of diversity by tuning the perturbation’s magnitude.</abstract>
      <url hash="17ea0ef7">2024.eacl-srw.2</url>
      <bibkey>nishida-etal-2024-generating</bibkey>
    </paper>
    <paper id="3">
      <title>The <fixed-case>KIND</fixed-case> Dataset: A Social Collaboration Approach for Nuanced Dialect Data Collection</title>
      <author><first>Asma</first><last>Yamani</last></author>
      <author><first>Raghad</first><last>Alziyady</last><affiliation>NA</affiliation></author>
      <author><first>Reem</first><last>AlYami</last><affiliation>King Fahad University of Petroleum and Minerals</affiliation></author>
      <author><first>Salma</first><last>Albelali</last><affiliation>NA</affiliation></author>
      <author><first>Leina</first><last>Albelali</last><affiliation>NA</affiliation></author>
      <author><first>Jawharah</first><last>Almulhim</last><affiliation>NA</affiliation></author>
      <author><first>Amjad</first><last>Alsulami</last></author>
      <author><first>Motaz</first><last>Alfarraj</last><affiliation>King Fahad University of Petroleum and Minerals</affiliation></author>
      <author><first>Rabeah</first><last>Al-Zaidy</last></author>
      <pages>32-43</pages>
      <abstract>Nuanced dialects are a linguistic variant that pose several challenges for NLP models and techniques. One of the main challenges is the limited amount of datasets to enable extensive research and experimentation. We propose an approach for efficiently collecting nuanced dialectal datasets that are not only of high quality, but are versatile enough to be multipurpose as well. To test our approach we collect the KIND corpus, which is a collection of fine-grained Arabic dialect data. The data is short texts, and unlike many nuanced dialectal datasets, it is curated manually through social collaboration efforts as opposed to being crawled from social media. The collaborative approach is incentivized through educational gamification and competitions for which the community itself benefits from the open source dataset. Our approach aims to achieve: (1) coverage of dialects from under-represented groups and fine-grained dialectal varieties, (2) provide aligned parallel corpora for translation between Modern Standard Arabic (MSA) and multiple dialects to enable translation and comparison studies, (3) promote innovative approaches for nuanced dialect data collection. We explain the steps for the competition as well as the resulting datasets and the competing data collection systems. The KIND dataset is shared with the research community.</abstract>
      <url hash="4d7e6c03">2024.eacl-srw.3</url>
      <bibkey>yamani-etal-2024-kind</bibkey>
    </paper>
    <paper id="4">
      <title>Can Stanza be Used for Part-of-Speech Tagging Historical <fixed-case>P</fixed-case>olish?</title>
      <author><first>Maria</first><last>Szawerna</last><affiliation>Göteborg University</affiliation></author>
      <pages>44-49</pages>
      <abstract>The goal of this paper is to evaluate the performance of Stanza, a part-of-speech (POS) tagger developed for modern Polish, on historical text to assess its possible use for automating the annotation of other historical texts. While the issue of the reliability of utilizing POS taggers on historical data has been previously discussed, most of the research focuses on languages whose grammar differs from Polish, meaning that their results need not be fully applicable in this case. The evaluation of Stanza is conducted on two sets of 10286 and 3270 manually annotated tokens from a piece of historical Polish writing (1899), and the errors are analyzed qualitatively and quantitatively. The results show a good performance of the tagger, especially when it comes to Universal Part-of-Speech (UPOS) tags, which is promising for utilizing the tagger for automatic annotation in larger projects, and pinpoint some common features of misclassified tokens.</abstract>
      <url hash="e05a57e9">2024.eacl-srw.4</url>
      <bibkey>szawerna-2024-stanza</bibkey>
    </paper>
    <paper id="5">
      <title>Toward Zero-Shot Instruction Following</title>
      <author><first>Renze</first><last>Lou</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Wenpeng</first><last>Yin</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>50-60</pages>
      <abstract>This work proposes a challenging yet more realistic setting for zero-shot cross-task generalization: zero-shot instruction following, presuming the existence of a paragraph-style task definition while no demonstrations exist. To better learn the task supervision from the definition, we propose two strategies: first, to automatically find out the critical sentences in the definition; second, a ranking objective to force the model to generate the gold outputs with higher probabilities when those critical parts are highlighted in the definition. The joint efforts of the two strategies yield state-of-the-art performance on the Super-NaturalInstructions. Our code is available on GitHub.</abstract>
      <url hash="0380949d">2024.eacl-srw.5</url>
      <bibkey>lou-yin-2024-toward</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>U</fixed-case>n<fixed-case>MASK</fixed-case>ed: Quantifying Gender Biases in Masked Language Models through Linguistically Informed Job Market Prompts</title>
      <author><first>Iñigo</first><last>Parra</last></author>
      <pages>61-70</pages>
      <abstract>Language models (LMs) have become pivotal in the realm of technological advancements. While their capabilities are vast and transformative, they often include societal biases encoded in the human-produced datasets used for their training. This research delves into the inherent biases present in masked language models (MLMs), with a specific focus on gender biases. This study evaluated six prominent models: BERT, RoBERTa, DistilBERT, BERT- multilingual, XLM-RoBERTa, and DistilBERT- multilingual. The methodology employed a novel dataset, bifurcated into two subsets: one containing prompts that encouraged models to generate subject pronouns in English and the other requiring models to return the probabilities of verbs, adverbs, and adjectives linked to the prompts’ gender pronouns. The analysis reveals stereotypical gender alignment of all models, with multilingual variants showing comparatively reduced biases.</abstract>
      <url hash="b149c26a">2024.eacl-srw.6</url>
      <bibkey>parra-2024-unmasked</bibkey>
    </paper>
    <paper id="7">
      <title>Distribution Shifts Are Bottlenecks: Extensive Evaluation for Grounding Language Models to Knowledge Bases</title>
      <author><first>Yiheng</first><last>Shu</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Zhiwei</first><last>Yu</last></author>
      <pages>71-88</pages>
      <abstract>Grounding language models (LMs) to knowledge bases (KBs) helps to obtain rich and accurate facts. However, it remains challenging because of the enormous size, complex structure, and partial observability of KBs. One reason is that current benchmarks fail to reflect robustness challenges and fairly evaluate models.This paper analyzes whether these robustness challenges arise from distribution shifts, including environmental, linguistic, and modal aspects.This affects the ability of LMs to cope with unseen schema, adapt to language variations, and perform few-shot learning. Thus, the paper proposes extensive evaluation protocols and conducts experiments to demonstrate that, despite utilizing our proposed data augmentation method, both advanced small and large language models exhibit poor robustness in these aspects. We conclude that current LMs are too fragile to navigate in complex environments due to distribution shifts. This underscores the need for future research focusing on data collection, evaluation protocols, and learning paradigms.</abstract>
      <url hash="2682b414">2024.eacl-srw.7</url>
      <bibkey>shu-yu-2024-distribution</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>A</fixed-case>ttri<fixed-case>S</fixed-case>age: Product Attribute Value Extraction Using Graph Neural Networks</title>
      <author><first>Rohan</first><last>Potta</last></author>
      <author><first>Mallika</first><last>Asthana</last></author>
      <author><first>Siddhant</first><last>Yadav</last></author>
      <author><first>Nidhi</first><last>Goyal</last></author>
      <author><first>Sai</first><last>Patnaik</last></author>
      <author><first>Parul</first><last>Jain</last><affiliation>NA</affiliation></author>
      <pages>89-94</pages>
      <abstract>Extracting the attribute value of a product from the given product description is essential for ecommerce functions like product recommendations, search, and information retrieval. Therefore, understanding products in E-commerce. Greater accuracy certainly gives any retailer the edge. The burdensome aspect of this problem lies in the diversity of the products and their attributes and values. Existing solutions typically employ large language models or sequence-tagging approaches to capture the context of a given product description and extract attribute values. However, they do so with limited accuracy, which serves as the underlying motivation to explore a more comprehensive solution. Through this paper, we present a novel approach for attribute value extraction from product description leveraging graphs and graph neural networks. Our proposed method demonstrates improvements in attribute value extraction accuracy compared to the baseline sequence tagging approaches.</abstract>
      <url hash="f2e3aaf6">2024.eacl-srw.8</url>
      <bibkey>potta-etal-2024-attrisage</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>H</fixed-case>ypo<fixed-case>T</fixed-case>erm<fixed-case>QA</fixed-case>: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Cem</first><last>Uluoglakci</last></author>
      <author><first>Tugba</first><last>Temizel</last><affiliation>Graduate School of Informatics</affiliation></author>
      <pages>95-136</pages>
      <abstract>Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs. The detection of hallucinations itself is also a formidable task, frequently requiring manual labeling or constrained evaluations. This paper introduces an automated scalable framework that combines benchmarking LLMs’ hallucination tendencies with efficient hallucination detection. We leverage LLMs to generate challenging tasks related to hypothetical phenomena, subsequently employing them as agents for efficient hallucination detection. The framework is domain-agnostic, allowing the use of any language model for benchmark creation or evaluation in any domain. We introduce the publicly available HypoTermQA Benchmarking Dataset, on which state-of-the-art models’ performance ranged between 3% and 11%, and evaluator agents demonstrated a 6% error rate in hallucination prediction. The proposed framework provides opportunities to test and improve LLMs. Additionally, it has the potential to generate benchmarking datasets tailored to specific domains, such as law, health, and finance.</abstract>
      <url hash="f53f8736">2024.eacl-srw.9</url>
      <bibkey>uluoglakci-temizel-2024-hypotermqa</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>A</fixed-case>rabic Synonym <fixed-case>BERT</fixed-case>-based Adversarial Examples for Text Classification</title>
      <author><first>Norah</first><last>Alshahrani</last></author>
      <author><first>Saied</first><last>Alshahrani</last></author>
      <author><first>Esma</first><last>Wali</last></author>
      <author><first>Jeanna</first><last>Matthews</last><affiliation>Clarkson University</affiliation></author>
      <pages>137-147</pages>
      <abstract>Text classification systems have been proven vulnerable to adversarial text examples, modified versions of the original text examples that are often unnoticed by human eyes, yet can force text classification models to alter their classification. Often, research works quantifying the impact of adversarial text attacks have been applied only to models trained in English. In this paper, we introduce the first word-level study of adversarial attacks in Arabic. Specifically, we use a synonym (word-level) attack using a Masked Language Modeling (MLM) task with a BERT model in a black-box setting to assess the robustness of the state-of-the-art text classification models to adversarial attacks in Arabic. To evaluate the grammatical and semantic similarities of the newly produced adversarial examples using our synonym BERT-based attack, we invite four human evaluators to assess and compare the produced adversarial examples with their original examples. We also study the transferability of these newly produced Arabic adversarial examples to various models and investigate the effectiveness of defense mechanisms against these adversarial examples on the BERT models. We find that fine-tuned BERT models were more susceptible to our synonym attacks than the other Deep Neural Networks (DNN) models like WordCNN and WordLSTM we trained. We also find that fine-tuned BERT models were more susceptible to transferred attacks. We, lastly, find that fine-tuned BERT models successfully regain at least 2% in accuracy after applying adversarial training as an initial defense mechanism.</abstract>
      <url hash="078ea6ea">2024.eacl-srw.10</url>
      <bibkey>alshahrani-etal-2024-arabic</bibkey>
    </paper>
    <paper id="11">
      <title>A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models</title>
      <author><first>Marc</first><last>Braun</last><affiliation>NA</affiliation></author>
      <author><first>Jenny</first><last>Kunz</last><affiliation>Linköping University</affiliation></author>
      <pages>148-161</pages>
      <abstract>The self-rationalising capabilities of LLMs are appealing because the generated explanations can give insights into the plausibility of the predictions. However, how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further.To this end, we propose a hypothesis-driven statistical framework. We use a Bayesian network to implement a hypothesis about how a task (in our example, natural language inference) is solved, and its internal states are translated into natural language with templates. Those explanations are then compared to LLM-generated free-text explanations using automatic and human evaluations. This allows us to judge how similar the LLM’s and the Bayesian network’s decision processes are. We demonstrate the usage of our framework with an example hypothesis and two realisations in Bayesian networks. The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework’s potential to approximate LLM decisions better in future work.</abstract>
      <url hash="b1becb9d">2024.eacl-srw.11</url>
      <bibkey>braun-kunz-2024-hypothesis</bibkey>
    </paper>
    <paper id="12">
      <title>Align before Attend: Aligning Visual and Textual Features for Multimodal Hateful Content Detection</title>
      <author><first>Eftekhar</first><last>Hossain</last></author>
      <author><first>Omar</first><last>Sharif</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Sarah Masud</first><last>Preum</last><affiliation>Dartmouth College</affiliation></author>
      <pages>162-174</pages>
      <abstract>Multimodal hateful content detection is a challenging task that requires complex reasoning across visual and textual modalities. Therefore, creating a meaningful multimodal representation that effectively captures the interplay between visual and textual features through intermediate fusion is critical. Conventional fusion techniques are unable to attend to the modality-specific features effectively. Moreover, most studies exclusively concentrated on English and overlooked other low-resource languages. This paper proposes a context-aware attention framework for multimodal hateful content detection and assesses it for both English and non-English languages. The proposed approach incorporates an attention layer to meaningfully align the visual and textual features. This alignment enables selective focus on modality-specific features before fusing them. We evaluate the proposed approach on two benchmark hateful meme datasets, viz. MUTE (Bengali code-mixed) and MultiOFF (English). Evaluation results demonstrate our proposed approach’s effectiveness with F1-scores of 69.7% and 70.3% for the MUTE and MultiOFF datasets. The scores show approximately 2.5% and 3.2% performance improvement over the state-of-the-art systems on these datasets. Our implementation is available at https://github.com/eftekhar-hossain/Bengali-Hateful-Memes.</abstract>
      <url hash="6461034d">2024.eacl-srw.12</url>
      <bibkey>hossain-etal-2024-align</bibkey>
    </paper>
    <paper id="13">
      <title>Topic-guided Example Selection for Domain Adaptation in <fixed-case>LLM</fixed-case>-based Machine Translation</title>
      <author><first>Seth</first><last>Aycock</last></author>
      <author><first>Rachel</first><last>Bawden</last><affiliation>Inria</affiliation></author>
      <pages>175-195</pages>
      <abstract>Current machine translation (MT) systems perform well in the domains on which they were trained, but adaptation to unseen domains remains a challenge. Rather than fine-tuning on domain data or modifying the architecture for training, an alternative approach exploits large language models (LLMs), which are performant across NLP tasks especially when presented with in-context examples. We focus on adapting a pre-trained LLM to a domain at inference through in-context example selection. For MT, examples are usually randomly selected from a development set. Some more recent methods though select using the more intuitive basis of test source similarity. We employ topic models to select examples based on abstract semantic relationships below the level of a domain. We test the relevance of these statistical models and use them to select informative examples even for out-of-domain inputs, experimenting on 7 diverse domains and 11 language pairs of differing resourcedness. Our method outperforms baselines on challenging multilingual out-of-domain tests, though it does not match performance with strong baselines for the in-language setting. We find that adding few-shot examples and related keywords consistently improves translation quality, that example diversity must be balanced with source similarity, and that our pipeline is overly restrictive for example selection when a targeted development set is available.</abstract>
      <url hash="9d871088">2024.eacl-srw.13</url>
      <bibkey>aycock-bawden-2024-topic</bibkey>
    </paper>
    <paper id="14">
      <title>Reforging : A Method for Constructing a Linguistically Valid <fixed-case>J</fixed-case>apanese <fixed-case>CCG</fixed-case> Treebank</title>
      <author><first>Asa</first><last>Tomita</last></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <author><first>Daisuke</first><last>Bekki</last><affiliation>Ochanomizu University</affiliation></author>
      <pages>196-207</pages>
      <abstract>The linguistic validity of Combinatory Categorial Grammar (CCG) parsing results relies heavily on treebanks for training and evaluation, so the treebank construction is crucial. Yet the current Japanese CCG treebank is known to have inaccuracies in its analyses of Japanese syntactic structures, including passive and causative constructions. While ABCTreebank, a treebank for ABC grammar, has been made to improve the analysis, particularly of argument structures, it lacks the detailed syntactic features required for Japanese CCG. In contrast, the Japanese CCG parser, lightblue, efficiently provides detailed syntactic features, but it does not accurately capture argument structures. We propose a method to generate a linguistically valid Japanese CCG treebank with detailed information by combining the strengths of ABCTreebank and lightblue. We develop an algorithm that filters lightblue’s lexical items using ABCTreebank, effectively converting lightblue output into a linguistically valid CCG treebank. To evaluate our treebank, we manually evaluate CCG syntactic structures and semantic representations and analyze conversion rates.</abstract>
      <url hash="20dd68a2">2024.eacl-srw.14</url>
      <bibkey>tomita-etal-2024-reforging</bibkey>
    </paper>
    <paper id="15">
      <title>Thesis Proposal: <fixed-case>D</fixed-case>etecting Agency Attribution</title>
      <author><first>Igor</first><last>Ryazanov</last><affiliation>Umea University</affiliation></author>
      <author><first>Johanna</first><last>Björklund</last></author>
      <pages>208-214</pages>
      <abstract>We explore computational methods for perceived agency attribution in natural language data. We consider ‘agency’ as the freedom and capacity to act, and the corresponding Natural Language Processing (NLP) task involves automatically detecting attributions of agency to entities in text. Our theoretical framework draws on semantic frame analysis, role labelling and related techniques. In initial experiments, we focus on the perceived agency of AI systems. To achieve this, we analyse a dataset of English-language news coverage of AI-related topics, published within one year surrounding the release of the Large Language Model-based service ChatGPT, a milestone in the general public’s awareness of AI. Building on this, we propose a schema to annotate a dataset for agency attribution and formulate additional research questions to answer by applying NLP models.</abstract>
      <url hash="f3118b97">2024.eacl-srw.15</url>
      <bibkey>ryazanov-bjorklund-2024-thesis</bibkey>
    </paper>
    <paper id="16">
      <title>A Thesis Proposal <fixed-case>C</fixed-case>laim<fixed-case>I</fixed-case>nspector Framework: A Hybrid Approach to Data Annotation using Fact-Checked Claims and <fixed-case>LLM</fixed-case>s</title>
      <author><first>Basak</first><last>Bozkurt</last><affiliation>University of Oxford</affiliation></author>
      <pages>215-224</pages>
      <abstract>This thesis explores the challenges and limitations encountered in automated fact-checking processes, with a specific emphasis on data annotation in the context of misinformation. Despite the widespread presence of misinformation in multiple formats and across various channels, current efforts concentrate narrowly on textual claims sourced mainly from Twitter, resulting in datasets with considerably limited scope. Furthermore, the absence of automated control measures, coupled with the reliance on human annotation, which is very limited, increases the risk of noisy data within these datasets. This thesis proposal examines the existing methods, elucidates their limitations and explores the potential integration of claim detection subtasks and Large Language Models to mitigate these issues. It introduces ClaimInspector, a novel framework designed for a systemic collection of multimodal data from the internet. By implementing this framework, this thesis will propose a dataset comprising fact-checks alongside the corresponding claims made by politicians. Overall, this thesis aims to enhance the accuracy and efficiency of annotation processes, thereby contributing to automated fact-checking efforts.</abstract>
      <url hash="4fbd553c">2024.eacl-srw.16</url>
      <bibkey>bozkurt-2024-thesis</bibkey>
    </paper>
    <paper id="17">
      <title>Large Language Models for Mathematical Reasoning: Progresses and Challenges</title>
      <author><first>Janice</first><last>Ahn</last></author>
      <author><first>Rishu</first><last>Verma</last></author>
      <author><first>Renze</first><last>Lou</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Di</first><last>Liu</last><affiliation>Temple University</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Wenpeng</first><last>Yin</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>225-237</pages>
      <abstract>Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges within this domain. To the best of our knowledge, this survey stands as one of the first extensive examinations of the landscape of LLMs in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.</abstract>
      <url hash="eb1dc4c9">2024.eacl-srw.17</url>
      <bibkey>ahn-etal-2024-large</bibkey>
    </paper>
    <paper id="18">
      <title>Representation and Generation of Machine Learning Test Functions</title>
      <author><first>Souha</first><last>Hassine</last></author>
      <author><first>Steven</first><last>Wilson</last><affiliation>Oakland University (Michigan)</affiliation></author>
      <pages>238-247</pages>
      <abstract>Writing tests for machine learning (ML) code is a crucial step towards ensuring the correctness and reliability of ML software. At the same time, Large Language Models (LLMs) have been adopted at a rapid pace for various code generation tasks, making it a natural choice for many developers who need to write ML tests. However, the implications of using these models, and how the LLM-generated tests differ from human-written ones, are relatively unexplored. In this work, we examine the use of LLMs to extract representations of ML source code and tests in order to understand the semantic relationships between human-written test functions and LLM-generated ones, and annotate a set of LLM-generated tests for several important qualities including usefulness, documentation, and correctness. We find that programmers prefer LLM-generated tests to those selected using retrieval-based methods, and in some cases, to those written by other humans.</abstract>
      <url hash="88e27c61">2024.eacl-srw.18</url>
      <bibkey>hassine-wilson-2024-representation</bibkey>
    </paper>
    <paper id="19">
      <title>The Generative <fixed-case>AI</fixed-case> Paradox in Evaluation: “What It Can Solve, It May Not Evaluate”</title>
      <author><first>Juhyun</first><last>Oh</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Eunsu</first><last>Kim</last></author>
      <author><first>Inha</first><last>Cha</last><affiliation>Upstage AI Research</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>248-257</pages>
      <abstract>This paper explores the assumption that Large Language Models (LLMs) skilled in generation tasks are equally adept as evaluators. We assess the performance of three LLMs and one open-source LM in Question-Answering (QA) and evaluation tasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks. Intriguingly, we discover instances of unfaithful evaluation where models accurately evaluate answers in areas where they lack competence, underscoring the need to examine the faithfulness and trustworthiness of LLMs as evaluators. This study contributes to the understanding of “the Generative AI Paradox” (West et al., 2023), highlighting a need to explore the correlation between generative excellence and evaluation proficiency, and the necessity to scrutinize the faithfulness aspect in model evaluations.</abstract>
      <url hash="2704af54">2024.eacl-srw.19</url>
      <bibkey>oh-etal-2024-generative</bibkey>
    </paper>
    <paper id="20">
      <title>Generative Data Augmentation using <fixed-case>LLM</fixed-case>s improves Distributional Robustness in Question Answering</title>
      <author><first>Arijit</first><last>Chowdhury</last></author>
      <author><first>Aman</first><last>Chadha</last><affiliation>Amazon</affiliation></author>
      <pages>258-265</pages>
      <abstract>Robustness in Natural Language Processing continues to be a pertinent issue, where state of the art models under-perform under naturally shifted distributions. In the context of Question Answering, work on domain adaptation methods continues to be a growing body of research. However, very little attention has been given to the notion of domain generalization under natural distribution shifts, where the target domain is unknown. With drastic improvements in the quality and access to generative models, we answer the question: How do generated datasets influence the performance of QA models under natural distribution shifts? We perform experiments on 4 different datasets under varying amounts of distribution shift, and analyze how “in-the-wild” generation can help achieve domain generalization. We take a two-step generation approach, generating both contexts and QA pairs to augment existing datasets. Through our experiments, we demonstrate how augmenting reading comprehension datasets with generated data leads to better robustness towards natural distribution shifts.</abstract>
      <url hash="12cd739f">2024.eacl-srw.20</url>
      <bibkey>chowdhury-chadha-2024-generative</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>J</fixed-case>apanese-<fixed-case>E</fixed-case>nglish Sentence Translation Exercises Dataset for Automatic Grading</title>
      <author><first>Naoki</first><last>Miura</last></author>
      <author><first>Hiroaki</first><last>Funayama</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Seiya</first><last>Kikuchi</last><affiliation>NA</affiliation></author>
      <author><first>Yuichiroh</first><last>Matsubayashi</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Yuya</first><last>Iwase</last></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence, RIKEN and Tohoku University</affiliation></author>
      <pages>266-278</pages>
      <abstract>This paper proposes the task of automatic assessment of Sentence Translation Exercises (STEs), that have been used in the early stage of L2 language learning.We formalize the task as grading student responses for each rubric criterion pre-specified by the educators.We then create a dataset for STE between Japanese and English including 21 questions, along with a total of 3,498 student responses (167 on average).The answer responses were collected from students and crowd workers.Using this dataset, we demonstrate the performance of baselines including a finetuned BERT model and GPT-3.5 with few-shot learning. Experimental results showed that the baseline model with fine-tuned BERT was able to classify correct responses with approximately 90% in <tex-math>F_1</tex-math>, but only less than 80% for incorrect responses. Furthermore, GPT-3.5 with few-shot learning shows a poorer result than the BERT model, indicating that our newly proposed task presents a challenging issue, even for the state-of-the-art large language model.</abstract>
      <url hash="a7e8f96b">2024.eacl-srw.21</url>
      <bibkey>miura-etal-2024-japanese</bibkey>
    </paper>
    <paper id="22">
      <title>The Impact of Integration Step on Integrated Gradients</title>
      <author><first>Masahiro</first><last>Makino</last></author>
      <author><first>Yuya</first><last>Asazuma</last></author>
      <author><first>Shota</first><last>Sasaki</last><affiliation>Cyberagent, Inc.</affiliation></author>
      <author><first>Jun</first><last>Suzuki</last><affiliation>Tohoku University</affiliation></author>
      <pages>279-289</pages>
      <abstract>Integrated Gradients (IG) serve as a potent tool for explaining the internal structure of a language model. The calculation of IG requires numerical integration, wherein the number of steps serves as a critical hyperparameter. The step count can drastically alter the results, inducing considerable errors in interpretability. To scrutinize the effect of step variation on IG, we measured the difference between theoretical and observed IG totals for each step amount.Our findings indicate that the ideal number of steps to maintain minimal error varies from instance to instance. Consequently, we advocate for customizing the step count for each instance. Our study is the first to quantitatively analyze the variation of IG values with the number of steps.</abstract>
      <url hash="9c8e15db">2024.eacl-srw.22</url>
      <bibkey>makino-etal-2024-impact</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>G</fixed-case>es<fixed-case>N</fixed-case>avi: Gesture-guided Outdoor Vision-and-Language Navigation</title>
      <author><first>Aman</first><last>Jain</last></author>
      <author><first>Teruhisa</first><last>Misu</last><affiliation>Honda Research Institute USA, Inc.</affiliation></author>
      <author><first>Kentaro</first><last>Yamada</last><affiliation>Honda Research Institute USA, Inc.</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <pages>290-295</pages>
      <abstract>Vision-and-Language Navigation (VLN) task involves navigating mobility using linguistic commands and has application in developing interfaces for autonomous mobility. In reality, natural human communication also encompasses non-verbal cues like hand gestures and gaze. These gesture-guided instructions have been explored in Human-Robot Interaction systems for effective interaction, particularly in object-referring expressions. However, a notable gap exists in tackling gesture-based demonstrative expressions in outdoor VLN task. To address this, we introduce a novel dataset for gesture-guided outdoor VLN instructions with demonstrative expressions, designed with a focus on complex instructions requiring multi-hop reasoning between the multiple input modalities. In addition, our work also includes a comprehensive analysis of the collected data and a comparative evaluation against the existing datasets.</abstract>
      <url hash="aeaa8cf9">2024.eacl-srw.23</url>
      <bibkey>jain-etal-2024-gesnavi</bibkey>
    </paper>
    <paper id="24">
      <title>Can docstring reformulation with an <fixed-case>LLM</fixed-case> improve code generation?</title>
      <author><first>Nicola</first><last>Dainese</last><affiliation>Aalto University</affiliation></author>
      <author><first>Alexander</first><last>Ilin</last><affiliation>Aalto University</affiliation></author>
      <author><first>Pekka</first><last>Marttinen</last><affiliation>Aalto University</affiliation></author>
      <pages>296-312</pages>
      <abstract>Generating code is an important application of Large Language Models (LLMs) and the task of function completion is one of the core open challenges in this context. Existing approaches focus on either training, fine-tuning or prompting LLMs to generate better outputs given the same input. We propose a novel and complementary approach: to optimize part of the input, the docstring (summary of a function’s purpose and usage), via reformulation with an LLM, in order to improve code generation. We develop two baseline methods for optimizing code generation via docstring reformulation and test them on the original HumanEval benchmark and multiple curated variants which are made more challenging by realistically worsening the docstrings. Our results show that, when operating on docstrings reformulated by an LLM instead of the original (or worsened) inputs, the performance of a number of open-source LLMs does not change significantlyThis finding demonstrates an unexpected robustness of current open-source LLMs to the details of the docstrings. We conclude by examining a series of questions, accompanied by in-depth analyses, pertaining to the sensitivity of current open-source LLMs to the details in the docstrings, the potential for improvement via docstring reformulation and the limitations of the methods employed in this work.</abstract>
      <url hash="bb36faa3">2024.eacl-srw.24</url>
      <bibkey>dainese-etal-2024-docstring</bibkey>
    </paper>
    <paper id="25">
      <title>Benchmarking Diffusion Models for Machine Translation</title>
      <author><first>Yunus</first><last>Demirag</last></author>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>313-324</pages>
      <abstract>Diffusion models have recently shown great potential on many generative tasks.In this work, we explore diffusion models for machine translation (MT).We adapt two prominent diffusion-based text generation models, Diffusion-LM and DiffuSeq, to perform machine translation.As the diffusion models generate non-autoregressively (NAR),we draw parallels to NAR machine translation models.With a comparison to conventional Transformer-based translation models, as well as to the Levenshtein Transformer,an established NAR MT model,we show that the multimodality problem that limits NAR machine translation performance is also a challenge to diffusion models.We demonstrate that knowledge distillation from an autoregressive model improves the performance of diffusion-based MT.A thorough analysis on the translation quality of inputs of different lengths shows that the diffusion models struggle more on long-range dependencies than other models.</abstract>
      <url hash="81e33602">2024.eacl-srw.25</url>
      <bibkey>demirag-etal-2024-benchmarking</bibkey>
    </paper>
    <paper id="26">
      <title>Forged-<fixed-case>GAN</fixed-case>-<fixed-case>BERT</fixed-case>: Authorship Attribution for <fixed-case>LLM</fixed-case>-Generated Forged Novels</title>
      <author><first>Kanishka</first><last>Silva</last><affiliation>University of Wolverhampton and University of Wolverhampton</affiliation></author>
      <author><first>Ingo</first><last>Frommholz</last><affiliation>NA</affiliation></author>
      <author><first>Burcu</first><last>Can</last><affiliation>University of Stirling</affiliation></author>
      <author><first>Fred</first><last>Blain</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Raheem</first><last>Sarwar</last><affiliation>NA</affiliation></author>
      <author><first>Laura</first><last>Ugolini</last><affiliation>NA</affiliation></author>
      <pages>325-337</pages>
      <abstract>The advancement of generative Large Language Models (LLMs), capable of producing human-like texts, introduces challenges related to the authenticity of the text documents. This requires exploring potential forgery scenarios within the context of authorship attribution, especially in the literary domain. Particularly,two aspects of doubted authorship may arise in novels, as a novel may be imposed by a renowned author or include a copied writing style of a well-known novel. To address these concerns, we introduce Forged-GAN-BERT, a modified GANBERT-based model to improve the classification of forged novels in two data-augmentation aspects: via the Forged Novels Generator (i.e., ChatGPT) and the generator in GAN. Compared to other transformer-based models, the proposed Forged-GAN-BERT model demonstrates an improved performance with F1 scores of 0.97 and 0.71 for identifying forged novels in single-author and multi-author classification settings. Additionally, we explore different prompt categories for generating the forged novels to analyse the quality of the generated texts using different similarity distance measures, including ROUGE-1, Jaccard Similarity, Overlap Confident, and Cosine Similarity.</abstract>
      <url hash="4026f703">2024.eacl-srw.26</url>
      <bibkey>silva-etal-2024-forged</bibkey>
    </paper>
    <paper id="27">
      <title>Thesis Proposal: Detecting Empathy Using Multimodal Language Model</title>
      <author><first>Md Rakibul</first><last>Hasan</last><affiliation>Curtin University of Technology and BRAC University, Bangladesh</affiliation></author>
      <author><first>Md Zakir</first><last>Hossain</last><affiliation>CSIRO and Australian National University</affiliation></author>
      <author><first>Aneesh</first><last>Krishna</last><affiliation>NA</affiliation></author>
      <author><first>Shafin</first><last>Rahman</last><affiliation>North South University</affiliation></author>
      <author><first>Tom</first><last>Gedeon</last></author>
      <pages>338-349</pages>
      <abstract>Empathy is crucial in numerous social interactions, including human-robot, patient-doctor, teacher-student, and customer-call centre conversations. Despite its importance, empathy detection in videos continues to be a challenging task because of the subjective nature of empathy and often remains under-explored. Existing studies have relied on scripted or semi-scripted interactions in text-, audio-, or video-only settings that fail to capture the complexities and nuances of real-life interactions. This PhD research aims to fill these gaps by developing a multimodal language model (MMLM) that detects empathy in audiovisual data. In addition to leveraging existing datasets, the proposed study involves collecting real-life interaction video and audio. This study will leverage optimisation techniques like neural architecture search to deliver an optimised small-scale MMLM. Successful implementation of this project has significant implications in enhancing the quality of social interactions as it enables real-time measurement of empathy and thus provides potential avenues for training for better empathy in interactions.</abstract>
      <url hash="c8a058db">2024.eacl-srw.27</url>
      <bibkey>hasan-etal-2024-thesis</bibkey>
    </paper>
    <paper id="28">
      <title>Toward Sentiment Aware Semantic Change Analysis</title>
      <author><first>Roksana</first><last>Goworek</last></author>
      <author><first>Haim</first><last>Dubossarsky</last><affiliation>Queen Mary University of London</affiliation></author>
      <pages>350-357</pages>
      <abstract>This student paper explores the potential of augmenting computational models of semantic change with sentiment information. It tests the efficacy of this approach on the English SemEval of Lexical Semantic Change and its associated historical corpora. We first establish the feasibility of our approach by demonstrating that existing models extract reliable sentiment information from historical corpora, and then validate that words that underwent semantic change also show greater sentiment change in comparison to historically stable words. We then integrate sentiment information into standard models of semantic change for individual words, and test if this can improve the overall performance of the latter, showing mixed results. This research contributes to our understanding of language change by providing the first attempt to enrich standard models of semantic change with additional information. It taps into the multifaceted nature of language change, that should not be reduced only to binary or scalar report of change, but adds additional dimensions to this change, sentiment being only one of these. As such, this student paper suggests novel directions for future work in integrating additional, more nuanced information of change and interpretation for finer-grained semantic change analysis.</abstract>
      <url hash="02b026c8">2024.eacl-srw.28</url>
      <bibkey>goworek-dubossarsky-2024-toward</bibkey>
    </paper>
    <paper id="29">
      <title>Dynamic Task-Oriented Dialogue: A Comparative Study of Llama-2 and Bert in Slot Value Generation</title>
      <author><first>Tiziano</first><last>Labruna</last></author>
      <author><first>Sofia</first><last>Brenna</last><affiliation>Free University of Bozen</affiliation></author>
      <author><first>Bernardo</first><last>Magnini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>358-368</pages>
      <abstract>Recent advancements in instruction-based language models have demonstrated exceptional performance across various natural language processing tasks. We present a comprehensive analysis of the performance of two open-source language models, BERT and Llama-2, in the context of dynamic task-oriented dialogues. Focusing on the Restaurant domain and utilizing the MultiWOZ 2.4 dataset, our investigation centers on the models’ ability to generate predictions for masked slot values within text. The dynamic aspect is introduced through simulated domain changes, mirroring real-world scenarios where new slot values are incrementally added to a domain over time.This study contributes to the understanding of instruction-based models’ effectiveness in dynamic natural language understanding tasks when compared to traditional language models and emphasizes the significance of open-source, reproducible models in advancing research within the academic community.</abstract>
      <url hash="e6a5f347">2024.eacl-srw.29</url>
      <bibkey>labruna-etal-2024-dynamic</bibkey>
    </paper>
  </volume>
</collection>
