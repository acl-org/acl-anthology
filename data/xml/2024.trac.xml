<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.trac">
  <volume id="1" ingest-date="2024-05-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Threat, Aggression &amp; Cyberbullying @ LREC-COLING-2024</booktitle>
      <editor><first>Ritesh</first><last>Kumar</last></editor>
      <editor><first>Atul Kr.</first><last>Ojha</last></editor>
      <editor><first>Shervin</first><last>Malmasi</last></editor>
      <editor><first>Bharathi Raja</first><last>Chakravarthi</last></editor>
      <editor><first>Bornini</first><last>Lahiri</last></editor>
      <editor><first>Siddharth</first><last>Singh</last></editor>
      <editor><first>Shyam</first><last>Ratan</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="1fa22f47">2024.trac-1</url>
      <venue>trac</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="cbf955e2">2024.trac-1.0</url>
      <bibkey>trac-2024-threat</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The Constant in <fixed-case>HATE</fixed-case>: Toxicity in <fixed-case>R</fixed-case>eddit across Topics and Languages</title>
      <author><first>Wondimagegnhue Tsegaye</first><last>Tufa</last></author>
      <author><first>Ilia</first><last>Markov</last></author>
      <author><first>Piek T.J.M.</first><last>Vossen</last></author>
      <pages>1–11</pages>
      <abstract>Toxic language remains an ongoing challenge on social media platforms, presenting significant issues for users and communities. This paper provides a cross-topic and cross-lingual analysis of toxicity in Reddit conversations. We collect 1.5 million comment threads from 481 communities in six languages. By aligning languages with topics, we thoroughly analyze how toxicity spikes within different communities. Our analysis targets six languages spanning different communities and topics such as Culture, Politics, and News. We observe consistent patterns across languages where toxicity increases within the same topics while also identifying significant differences where specific language communities exhibit notable variations in relation to certain topics.</abstract>
      <url hash="c3a5cb60">2024.trac-1.1</url>
      <bibkey>tufa-etal-2024-constant</bibkey>
    </paper>
    <paper id="2">
      <title>A Federated Learning Approach to Privacy Preserving Offensive Language Identification</title>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Damith</first><last>Premasiri</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <pages>12–20</pages>
      <abstract>The spread of various forms of offensive speech online is an important concern in social media. While platforms have been investing heavily in ways of coping with this problem, the question of privacy remains largely unaddressed. Models trained to detect offensive language on social media are trained and/or fine-tuned using large amounts of data often stored in centralized servers. Since most social media data originates from end users, we propose a privacy preserving decentralized architecture for identifying offensive language online by introducing Federated Learning (FL) in the context of offensive language identification. FL is a decentralized architecture that allows multiple models to be trained locally without the need for data sharing hence preserving users’ privacy. We propose a model fusion approach to perform FL. We trained multiple deep learning models on four publicly available English benchmark datasets (AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail. We also present initial cross-lingual experiments in English and Spanish. We show that the proposed model fusion approach outperforms baselines in all the datasets while preserving privacy.</abstract>
      <url hash="66b7ce6a">2024.trac-1.2</url>
      <bibkey>zampieri-etal-2024-federated</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>CLTL</fixed-case>@<fixed-case>H</fixed-case>arm<fixed-case>P</fixed-case>ot-<fixed-case>ID</fixed-case>: Leveraging Transformer Models for Detecting Offline Harm Potential and Its Targets in Low-Resource Languages</title>
      <author><first>Yeshan</first><last>Wang</last></author>
      <author><first>Ilia</first><last>Markov</last></author>
      <pages>21–26</pages>
      <abstract>We present the winning approach to the TRAC 2024 Shared Task on Offline Harm Potential Identification (HarmPot-ID). The task focused on low-resource Indian languages and consisted of two sub-tasks: 1a) predicting the offline harm potential and 1b) detecting the most likely target(s) of the offline harm. We explored low-source domain specific, cross-lingual, and monolingual transformer models and submitted the aggregate predictions from the MuRIL and BERT models. Our approach achieved 0.74 micro-averaged F1-score for sub-task 1a and 0.96 for sub-task 1b, securing the 1st rank for both sub-tasks in the competition.</abstract>
      <url hash="ad714d49">2024.trac-1.3</url>
      <bibkey>wang-markov-2024-cltl-harmpot</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>NJUST</fixed-case>-<fixed-case>KMG</fixed-case> at <fixed-case>TRAC</fixed-case>-2024 Tasks 1 and 2: Offline Harm Potential Identification</title>
      <author><first>Jingyuan</first><last>Wang</last></author>
      <author><first>Jack</first><last>Depp</last></author>
      <author><first>Yang</first><last>Yang</last></author>
      <pages>27–31</pages>
      <abstract>This report provide a detailed description of the method that we proposed in the TRAC-2024 Offline Harm Potential dentification which encloses two sub-tasks. The investigation utilized a rich dataset comprised of social media comments in several Indian languages, annotated with precision by expert judges to capture the nuanced implications for offline context harm. The objective assigned to the participants was to design algorithms capable of accurately assessing the likelihood of harm in given situations and identifying the most likely target(s) of offline harm. Our approach ranked second in two separate tracks, with F1 values of 0.73 and 0.96 respectively. Our method principally involved selecting pretrained models for finetuning, incorporating contrastive learning techniques, and culminating in an ensemble approach for the test set.</abstract>
      <url hash="62b82474">2024.trac-1.4</url>
      <bibkey>wang-etal-2024-njust</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>S</fixed-case>calar<fixed-case>L</fixed-case>ab@<fixed-case>TRAC</fixed-case>2024: Exploring Machine Learning Techniques for Identifying Potential Offline Harm in Multilingual Commentaries</title>
      <author><first>Anagha</first><last>H C</last></author>
      <author><first>Saatvik M.</first><last>Krishna</last></author>
      <author><first>Soumya Sangam</first><last>Jha</last></author>
      <author><first>Vartika T.</first><last>Rao</last></author>
      <author><first>Anand Kumar</first><last>M</last></author>
      <pages>32–36</pages>
      <abstract>The objective of the shared task, Offline Harm Potential Identification (HarmPot-ID), is to build models to predict the offline harm potential of social media texts. “Harm potential” is defined as the ability of an online post or comment to incite offline physical harm such as murder, arson, riot, rape, etc. The first subtask was to predict the level of harm potential, and the second was to identify the group to which this harm was directed towards. This paper details our submissions for the shared task that includes a cascaded SVM model, an XGBoost model, and a TF-IDF weighted Word2Vec embedding-supported SVM model. Several other models that were explored have also been detailed.</abstract>
      <url hash="ae8b4fdf">2024.trac-1.5</url>
      <bibkey>h-c-etal-2024-scalarlab</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>LLM</fixed-case>-Based Synthetic Datasets: Applications and Limitations in Toxicity Detection</title>
      <author><first>Udo</first><last>Kruschwitz</last></author>
      <author><first>Maximilian</first><last>Schmidhuber</last></author>
      <pages>37–51</pages>
      <abstract>Large Language Model (LLM)-based Synthetic Data is becoming an increasingly important field of research. One of its promising application is in training classifiers to detect online toxicity, which is of increasing concern in today’s digital landscape. In this work, we assess the feasibility of generative models to generate synthetic data for toxic speech detection. Our experiments are conducted on six different toxicity datasets, four of whom are hateful and two are toxic in the broader sense. We then employ a classifier trained on the original data for filtering. To explore the potential of this data, we conduct experiments using combinations of original and synthetic data, synthetic oversampling of the minority class, and a comparison of original vs. synthetic-only training. Results indicate that while our generative models offer benefits in certain scenarios, it does not improve hateful dataset classification. However, it does boost patronizing and condescending language detection. We find that synthetic data generated by LLMs is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods. Code is available on GitHub; the generated dataset will be available on Zenodo in the final submission.</abstract>
      <url hash="e546c31f">2024.trac-1.6</url>
      <bibkey>kruschwitz-schmidhuber-2024-llm</bibkey>
    </paper>
    <paper id="7">
      <title>Using Sarcasm to Improve Cyberbullying Detection</title>
      <author><first>Xiaoyu</first><last>Guo</last></author>
      <author><first>Susan</first><last>Gauch</last></author>
      <pages>52–59</pages>
      <abstract>Cyberbullying has become more prevalent over time, especially towards minority groups, and online human moderators cannot detect cyberbullying content efficiently. Prior work has addressed this problem by detecting cyberbullying with deep learning approaches. In this project, we compare several BERT-based benchmark methods for cyberbullying detection and do a failure analysis to see where the model fails to correctly identify cyberbullying. We find that many falsely classified texts are sarcastic, so we propose a method to mitigate the false classifications by incorporating neural network-based sarcasm detection. We define a simple multilayer perceptron (MLP) that incorpo- rates sarcasm detection in the final cyberbully classifications and demonstrate improvement over benchmark methods.</abstract>
      <url hash="d652062f">2024.trac-1.7</url>
      <bibkey>guo-gauch-2024-using</bibkey>
    </paper>
    <paper id="8">
      <title>Analyzing Offensive Language and Hate Speech in Political Discourse: A Case Study of <fixed-case>G</fixed-case>erman Politicians</title>
      <author><first>Maximilian</first><last>Weissenbacher</last></author>
      <author><first>Udo</first><last>Kruschwitz</last></author>
      <pages>60–72</pages>
      <abstract>Social media platforms have become key players in political discourse. Twitter (now ‘X’), for example, is used by many German politicians to communicate their views and interact with others. Due to its nature, however, social networks suffer from a number of issues such as offensive content, toxic language and hate speech. This has attracted a lot of research interest but in the context of political discourse there is a noticeable gap with no such study specifically looking at German politicians in a systematic way. We aim to help addressing this gap. We first create an annotated dataset of 1,197 Twitter posts mentioning German politicians. This is the basis to explore a number of approaches to detect hate speech and offensive language (HOF) and identify an ensemble of transformer models that achieves an F1-Macros score of 0.94. This model is then used to automatically classify two much larger, longitudinal datasets: one with 520,000 tweets posted by MPs, and the other with 2,200,000 tweets which comprise posts from the public mentioning politicians. We obtain interesting insights in regards to the distribution of hate and offensive content when looking at different independent variables.</abstract>
      <url hash="96a71387">2024.trac-1.8</url>
      <bibkey>weissenbacher-kruschwitz-2024-analyzing</bibkey>
    </paper>
    <paper id="9">
      <title>Ice and Fire: Dataset on Sentiment, Emotions, Toxicity, Sarcasm, Hate speech, Sympathy and More in <fixed-case>I</fixed-case>celandic Blog Comments</title>
      <author><first>Steinunn Rut</first><last>Friðriksdóttir</last></author>
      <author><first>Annika</first><last>Simonsen</last></author>
      <author><first>Atli Snær</first><last>Ásmundsson</last></author>
      <author><first>Guðrún Lilja</first><last>Friðjónsdóttir</last></author>
      <author><first>Anton Karl</first><last>Ingason</last></author>
      <author><first>Vésteinn</first><last>Snæbjarnarson</last></author>
      <author><first>Hafsteinn</first><last>Einarsson</last></author>
      <pages>73–84</pages>
      <abstract>This study introduces “Ice and Fire,” a Multi-Task Learning (MTL) dataset tailored for sentiment analysis in the Icelandic language, encompassing a wide range of linguistic tasks, including sentiment and emotion detection, as well as identification of toxicity, hate speech, encouragement, sympathy, sarcasm/irony, and trolling. With 261 fully annotated blog comments and 1045 comments annotated in at least one task, this contribution marks a significant step forward in the field of Icelandic natural language processing. It provides a comprehensive dataset for understanding the nuances of online communication in Icelandic and an interface to expand the annotation effort. Despite the challenges inherent in subjective interpretation of text, our findings highlight the positive potential of this dataset to improve text analysis techniques and encourage more inclusive online discourse in Icelandic communities. With promising baseline performances, “Ice and Fire” sets the stage for future research to enhance automated text analysis and develop sophisticated language technologies, contributing to healthier online environments and advancing Icelandic language resources.</abstract>
      <url hash="498e64dc">2024.trac-1.9</url>
      <bibkey>fridriksdottir-etal-2024-ice</bibkey>
    </paper>
    <paper id="10">
      <title>Detecting Hate Speech in <fixed-case>A</fixed-case>mharic Using Multimodal Analysis of Social Media Memes</title>
      <author><first>Melese Ayichlie</first><last>Jigar</last></author>
      <author><first>Abinew Ali</first><last>Ayele</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>85–95</pages>
      <abstract>In contemporary society, the proliferation of hate speech is increasingly prevalent across various social media platforms, with a notable trend of incorporating memes to amplify its visual impact and reach. The conventional text-based detection approaches frequently fail to address the complexities introduced by memes, thereby aggravating the challenges, particularly in low-resource languages such as Amharic. We develop Amharic meme hate speech detection models using 2,000 memes collected from Facebook, Twitter, and Telegram over four months. We employ native Amharic speakers to annotate each meme using a web-based tool, yielding a Fleiss’ kappa score of 0.50. We utilize different feature extraction techniques, namely VGG16 for images and word2Vec for textual content, and build unimodal and multimodal models such as LSTM, BiLSTM, and CNN. The BiLSTM model shows the best performance, achieving 63% accuracy for text and 75% for multimodal features. In image-only experiments, the CNN model achieves 69% in accuracy. Multimodal models demonstrate superior performance in detecting Amharic hate speech in memes, showcasing their potential to address the unique challenges posed by meme-based hate speech on social media.</abstract>
      <url hash="8fe87b8c">2024.trac-1.10</url>
      <bibkey>jigar-etal-2024-detecting</bibkey>
    </paper>
    <paper id="11">
      <title>Content Moderation in Online Platforms: A Study of Annotation Methods for Inappropriate Language</title>
      <author><first>Baran</first><last>Barbarestani</last></author>
      <author><first>Isa</first><last>Maks</last></author>
      <author><first>Piek T.J.M.</first><last>Vossen</last></author>
      <pages>96–104</pages>
      <abstract>Detecting inappropriate language in online platforms is vital for maintaining a safe and respectful digital environment, especially in the context of hate speech prevention. However, defining what constitutes inappropriate language can be highly subjective and context-dependent, varying from person to person. This study presents the outcomes of a comprehensive examination of the subjectivity involved in assessing inappropriateness within conversational contexts. Different annotation methods, including expert annotation, crowd annotation, ChatGPT-generated annotation, and lexicon-based annotation, were applied to English Reddit conversations. The analysis revealed a high level of agreement across these annotation methods, with most disagreements arising from subjective interpretations of inappropriate language. This emphasizes the importance of implementing content moderation systems that not only recognize inappropriate content but also understand and adapt to diverse user perspectives and contexts. The study contributes to the evolving field of hate speech annotation by providing a detailed analysis of annotation differences in relation to the subjective task of judging inappropriate words in conversations.</abstract>
      <url hash="5013cc8f">2024.trac-1.11</url>
      <bibkey>barbarestani-etal-2024-content</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>F</fixed-case>rench<fixed-case>T</fixed-case>oxicity<fixed-case>P</fixed-case>rompts: a Large Benchmark for Evaluating and Mitigating Toxicity in <fixed-case>F</fixed-case>rench Texts</title>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <pages>105–114</pages>
      <abstract>Large language models (LLMs) are increasingly popular but are also prone to generating bias, toxic or harmful language, which can have detrimental effects on individuals and communities. Although most efforts is put to assess and mitigate toxicity in generated content, it is primarily concentrated on English, while it’s essential to consider other languages as well. For addressing this issue, we create and release FrenchToxicityPrompts, a dataset of 50K naturally occurring French prompts and their continuations, annotated with toxicity scores from a widely used toxicity classifier. We evaluate 14 different models from four prevalent open-sourced families of LLMs against our dataset to assess their potential toxicity across various dimensions. We hope that our contribution will foster future research on toxicity detection and mitigation beyond English.</abstract>
      <url hash="c48bcf6f">2024.trac-1.12</url>
      <bibkey>brun-nikoulina-2024-frenchtoxicityprompts</bibkey>
    </paper>
    <paper id="13">
      <title>Studying Reactions to Stereotypes in Teenagers: an Annotated <fixed-case>I</fixed-case>talian Dataset</title>
      <author><first>Elisa</first><last>Chierchiello</last></author>
      <author><first>Tom</first><last>Bourgeade</last></author>
      <author><first>Giacomo</first><last>Ricci</last></author>
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Francesca</first><last>D’Errico</last></author>
      <pages>115–125</pages>
      <abstract>The paper introduces a novel corpus collected in a set of experiments in Italian schools, annotated for the presence of stereotypes, and related categories. It consists of comments written by teenage students in reaction to fabricated fake news, designed to elicit prejudiced responses, by featuring racial stereotypes. We make use of an annotation scheme which takes into account the implicit or explicit nature of different instances of stereotypes, alongside their forms of discredit. We also annotate the stance of the commenter towards the news article, using a schema inspired by rumor and fake news stance detection tasks. Through this rarely studied setting, we provide a preliminary exploration of the production of stereotypes in a more controlled context. Alongside this novel dataset, we provide both quantitative and qualitative analyses of these reactions, to validate the categories used in their annotation. Through this work, we hope to increase the diversity of available data in the study of the propagation and the dynamics of negative stereotypes.</abstract>
      <url hash="b2401052">2024.trac-1.13</url>
      <bibkey>chierchiello-etal-2024-studying</bibkey>
    </paper>
    <paper id="14">
      <title>Offensiveness, Hate, Emotion and <fixed-case>GPT</fixed-case>: Benchmarking <fixed-case>GPT</fixed-case>3.5 and <fixed-case>GPT</fixed-case>4 as Classifiers on <fixed-case>T</fixed-case>witter-specific Datasets</title>
      <author><first>Nikolaj</first><last>Bauer</last></author>
      <author><first>Moritz</first><last>Preisig</last></author>
      <author><first>Martin</first><last>Volk</last></author>
      <pages>126–133</pages>
      <abstract>In this paper, we extend the work of benchmarking GPT by turning GPT models into classifiers and applying them on three different Twitter datasets on Hate-Speech Detection, Offensive Language Detection, and Emotion Classification. We use a Zero-Shot and Few-Shot approach to evaluate the classification capabilities of the GPT models. Our results show that GPT models do not always beat fine-tuned models on the tested benchmarks. However, in Hate-Speech and Emotion Detection, using a Few-Shot approach, state-of-the-art performance can be achieved. The results also reveal that GPT-4 is more sensitive to the examples given in a Few-Shot prompt, highlighting the importance of choosing fitting examples for inference and prompt formulation.</abstract>
      <url hash="aea78193">2024.trac-1.14</url>
      <bibkey>bauer-etal-2024-offensiveness</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>D</fixed-case>o<fixed-case>D</fixed-case>o Learning: Domain-Demographic Transfer in Language Models for Detecting Abuse Targeted at Public Figures</title>
      <author><first>Angus Redlarski</first><last>Williams</last></author>
      <author><first>Hannah Rose</first><last>Kirk</last></author>
      <author><first>Liam</first><last>Burke-Moore</last></author>
      <author><first>Yi-Ling</first><last>Chung</last></author>
      <author><first>Ivan</first><last>Debono</last></author>
      <author><first>Pica</first><last>Johansson</last></author>
      <author><first>Francesca</first><last>Stevens</last></author>
      <author><first>Jonathan</first><last>Bright</last></author>
      <author><first>Scott</first><last>Hale</last></author>
      <pages>134–154</pages>
      <abstract>Public figures receive disproportionate levels of abuse on social media, impacting their active participation in public life. Automated systems can identify abuse at scale but labelling training data is expensive and potentially harmful. So, it is desirable that systems are efficient and generalisable, handling shared and specific aspects of abuse. We explore the dynamics of cross-group text classification in order to understand how well models trained on one domain or demographic can transfer to others, with a view to building more generalisable abuse classifiers. We fine-tune language models to classify tweets targeted at public figures using our novel DoDo dataset, containing 28,000 entries with fine-grained labels, split equally across four Domain-Demographic pairs (male and female footballers and politicians). We find that (i) small amounts of diverse data are hugely beneficial to generalisation and adaptation; (ii) models transfer more easily across demographics but cross-domain models are more generalisable; (iii) some groups contribute more to generalisability than others; and (iv) dataset similarity is a signal of transferability.</abstract>
      <url hash="e5d93650">2024.trac-1.15</url>
      <attachment type="OptionalSupplementaryMaterial" hash="fb8a952b">2024.trac-1.15.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>williams-etal-2024-dodo</bibkey>
    </paper>
    <paper id="16">
      <title>Empowering Users and Mitigating Harm: Leveraging Nudging Principles to Enhance Social Media Safety</title>
      <author><first>Gregor</first><last>Donabauer</last></author>
      <author><first>Emily</first><last>Theophilou</last></author>
      <author><first>Francesco</first><last>Lomonaco</last></author>
      <author><first>Sathya</first><last>Bursic</last></author>
      <author><first>Davide</first><last>Taibi</last></author>
      <author><first>Davinia</first><last>Hernández-Leo</last></author>
      <author><first>Udo</first><last>Kruschwitz</last></author>
      <author><first>Dimitri</first><last>Ognibene</last></author>
      <pages>155–166</pages>
      <abstract>Social media have become an integral part of our daily lives, yet they have also resulted in various negative effects on users, ranging from offensive or hateful content to the spread of misinformation. In recent years, numerous automated approaches have been proposed to identify and combat such harmful content. However, it is crucial to recognize the human aspect of users who engage with this content in designing efforts to mitigate these threats. We propose to incorporate principles of behavioral science, specifically the concept of nudging into social media platforms. Our approach involves augmenting social media feeds with informative diagrams, which provide insights into the content that users are presented. The goal of our work is to empower social media users to make well-informed decisions for themselves and for others within these platforms. Nudges serve as a means to gently draw users’ attention to content in an unintrusive manner, a crucial consideration in the context of social media. To evaluate the effectiveness of our approach, we conducted a user study involving 120 Italian-speaking participants who interacted with a social media interface augmented with these nudging diagrams. Participants who had used the augmented interface were able to outperform those using the plain interface in a successive harmful content detection test where nudging diagrams were not visible anymore. Our findings demonstrate that our approach significantly improves users’ awareness of potentially harmful content with effects lasting beyond the duration of the interaction. In this work, we provide a comprehensive overview of our experimental materials and setup, present our findings, and refer to the limitations identified during our study.</abstract>
      <url hash="89575323">2024.trac-1.16</url>
      <bibkey>donabauer-etal-2024-empowering</bibkey>
    </paper>
    <paper id="17">
      <title>Exploring Boundaries and Intensities in Offensive and Hate Speech: Unveiling the Complex Spectrum of Social Media Discourse</title>
      <author><first>Abinew Ali</first><last>Ayele</last></author>
      <author><first>Esubalew Alemneh</first><last>Jalew</last></author>
      <author><first>Adem Chanie</first><last>Ali</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>167–178</pages>
      <abstract>The prevalence of digital media and evolving sociopolitical dynamics have significantly amplified the dissemination of hateful content. Existing studies mainly focus on classifying texts into binary categories, often overlooking the continuous spectrum of offensiveness and hatefulness inherent in the text. In this research, we present an extensive benchmark dataset for Amharic, comprising 8,258 tweets annotated for three distinct tasks: category classification, identification of hate targets, and rating offensiveness and hatefulness intensities. Our study highlights that a considerable majority of tweets belong to the less offensive and less hate intensity levels, underscoring the need for early interventions by stakeholders. The prevalence of ethnic and political hatred targets, with significant overlaps in our dataset, emphasizes the complex relationships within Ethiopia’s sociopolitical landscape. We build classification and regression models and investigate the efficacy of models in handling these tasks. Our results reveal that hate and offensive speech can not be addressed by a simplistic binary classification, instead manifesting as variables across a continuous range of values. The afro-XLMR-large model exhibits the best performances achieving F1-scores of 75.30%, 70.59%, and 29.42% for the category, target, and regression tasks, respectively. The 80.22% correlation coefficient of the Afro-XLMR-large model indicates strong alignments.</abstract>
      <url hash="266a970d">2024.trac-1.17</url>
      <bibkey>ayele-etal-2024-exploring</bibkey>
    </paper>
  </volume>
</collection>
