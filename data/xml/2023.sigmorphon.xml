<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.sigmorphon">
  <volume id="1" ingest-date="2023-07-12" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
      <editor><first>Garrett</first><last>Nicolai</last></editor>
      <editor><first>Eleanor</first><last>Chodroff</last></editor>
      <editor><first>Frederic</first><last>Mailhot</last></editor>
      <editor><first>Çağrı</first><last>Çöltekin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="8cc77566">2023.sigmorphon-1</url>
      <venue>sigmorphon</venue>
    </meta>
    <frontmatter>
      <url hash="5950be3a">2023.sigmorphon-1.0</url>
      <bibkey>sigmorphon-2023-sigmorphon</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Translating a low-resource language using <fixed-case>GPT</fixed-case>-3 and a human-readable dictionary</title>
      <author><first>Micha</first><last>Elsner</last><affiliation>The Ohio State University</affiliation></author>
      <author><first>Jordan</first><last>Needle</last><affiliation>The Ohio State University</affiliation></author>
      <pages>1-13</pages>
      <abstract>We investigate how well words in the polysynthetic language Inuktitut can be translated by combining dictionary definitions, without use of a neural machine translation model trained on parallel text. Such a translation system would allow natural language technology to benefit from resources designed for community use in a language revitalization or education program, rather than requiring a separate parallel corpus. We show that the text-to-text generation capabilities of GPT-3 allow it to perform this task with BLEU scores of up to 18.5. We investigate prompting GPT-3 to provide multiple translations, which can help slightly, and providing it with grammar information, which is mostly ineffective. Finally, we test GPT-3’s ability to derive morpheme definitions from whole-word translations, but find this process is prone to errors including hallucinations.</abstract>
      <url hash="abf49caf">2023.sigmorphon-1.2</url>
      <bibkey>elsner-needle-2023-translating</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.2</doi>
      <video href="2023.sigmorphon-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Evaluating Cross Lingual Transfer for Morphological Analysis: a Case Study of <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Siddhesh</first><last>Pawar</last><affiliation>Google</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology Bombay and Patna</affiliation></author>
      <author><first>Partha</first><last>Talukdar</last><affiliation>Google Research and IISc</affiliation></author>
      <pages>14-26</pages>
      <abstract>Recent advances in pretrained multilingual models such as Multilingual T5 (mT5) have facilitated cross-lingual transfer by learning shared representations across languages. Leveraging pretrained multilingual models for scaling morphology analyzers to low-resource languages is a unique opportunity that has been under-explored so far. We investigate this line of research in the context of Indian languages, focusing on two important morphological sub-tasks: root word extraction and tagging morphosyntactic descriptions (MSD), viz., gender, number, and person (GNP). We experiment with six Indian languages from two language families (Dravidian and Indo-Aryan) to train a multilingual morphology analyzers for the first time for Indian languages. We demonstrate the usability of multilingual models for few-shot cross-lingual transfer through an average 7% increase in GNP tagging in a cross-lingual setting as compared to a monolingual setting through controlled experiments. We provide an overview of the state of the datasets available related to our tasks and point-out a few modeling limitations due to datasets. Lastly, we analyze the cross-lingual transfer of morphological tags for verbs and nouns, which provides a proxy for the quality of representations of word markings learned by the model.</abstract>
      <url hash="fdd77252">2023.sigmorphon-1.3</url>
      <bibkey>pawar-etal-2023-evaluating</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.3</doi>
    </paper>
    <paper id="4">
      <title>Joint Learning Model for Low-Resource Agglutinative Language Morphological Tagging</title>
      <author><first>Gulinigeer</first><last>Abudouwaili</last><affiliation>School of Information Science and Engineering Xinjiang University</affiliation></author>
      <author><first>Kahaerjiang</first><last>Abiderexiti</last><affiliation>School of Information Science and Engineering, Xinjiang University</affiliation></author>
      <author><first>Nian</first><last>Yi</last><affiliation>School of Information Science and Engineering Xinjiang University</affiliation></author>
      <author><first>Aishan</first><last>Wumaier</last><affiliation>School of Science and Engineering, Xinjiang University; Xinjiang Provincial Key Laboratory of Multi-lingual Information Technology</affiliation></author>
      <pages>27-37</pages>
      <abstract>Due to the lack of data resources, rule-based or transfer learning is mainly used in the morphological tagging of low-resource languages. However, these methods require expert knowledge, ignore contextual features, and have error propagation. Therefore, we propose a joint morphological tagger for low-resource agglutinative languages to alleviate the above challenges. First, we represent the contextual input with multi-dimensional features of agglutinative words. Second, joint training reduces the direct impact of part-of-speech errors on morphological features and increases the indirect influence between the two types of labels through a fusion mechanism. Finally, our model separately predicts part-of-speech and morphological features. Part-of-speech tagging is regarded as sequence tagging. When predicting morphological features, two-label adjacency graphs are dynamically reconstructed by integrating multilingual global features and monolingual local features. Then, a graph convolution network is used to learn the higher-order intersection of labels. A series of experiments show that the proposed model in this paper is superior to other comparative models.</abstract>
      <url hash="578f8729">2023.sigmorphon-1.4</url>
      <bibkey>abudouwaili-etal-2023-joint</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.4</doi>
    </paper>
    <paper id="5">
      <title>Revisiting and Amending <fixed-case>C</fixed-case>entral <fixed-case>K</fixed-case>urdish Data on <fixed-case>U</fixed-case>ni<fixed-case>M</fixed-case>orph 4.0</title>
      <author><first>Sina</first><last>Ahmadi</last><affiliation>George Mason University</affiliation></author>
      <author><first>Aso</first><last>Mahmudi</last><affiliation>Independent</affiliation></author>
      <pages>38-48</pages>
      <abstract>UniMorph–the Universal Morphology project is a collaborative initiative to create and maintain morphological data and organize numerous related tasks for various language processing communities. The morphological data is provided by linguists for over 160 languages in the latest version of UniMorph 4.0. This paper sheds light on the Central Kurdish data on UniMorph 4.0 by analyzing the existing data, its fallacies, and systematic morphological errors. It also presents an approach to creating more reliable morphological data by considering various specific phenomena in Central Kurdish that have not been addressed previously, such as Izafe and several enclitics.</abstract>
      <url hash="c14c46d8">2023.sigmorphon-1.5</url>
      <bibkey>ahmadi-mahmudi-2023-revisiting</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.5</doi>
      <video href="2023.sigmorphon-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Investigating Phoneme Similarity with Artificially Accented Speech</title>
      <author><first>Margot</first><last>Masson</last><affiliation>University College Dublin</affiliation></author>
      <author><first>Julie</first><last>Carson-berndsen</last><affiliation>University College Dublin</affiliation></author>
      <pages>49-57</pages>
      <abstract>While the deep learning revolution has led to significant performance improvements in speech recognition, accented speech remains a challenge. Current approaches to this challenge typically do not seek to understand and provide explanations for the variations of accented speech, whether they stem from native regional variation or non-native error patterns. This paper seeks to address non-native speaker variations from both a knowledge-based and a data-driven perspective. We propose to approximate non-native accented-speech pronunciation patterns by the means of two approaches: based on phonetic and phonological knowledge on the one hand and inferred from a text-to-speech system on the other. Artificial speech is then generated with a range of variants which have been captured in confusion matrices representing phoneme similarities. We then show that non-native accent confusions actually propagate to the transcription from the ASR, thus suggesting that the inference of accent specific phoneme confusions is achievable from artificial speech.</abstract>
      <url hash="c1e760ed">2023.sigmorphon-1.6</url>
      <bibkey>masson-carson-berndsen-2023-investigating</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.6</doi>
    </paper>
    <paper id="7">
      <title>Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation</title>
      <author><first>David R.</first><last>Mortensen</last><affiliation>Language Technologies Institute, Carnegie Mellon University</affiliation></author>
      <author><first>Ela</first><last>Gulsen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Taiqi</first><last>He</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Nathaniel</first><last>Robinson</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jonathan</first><last>Amith</last><affiliation>Gettysburg College</affiliation></author>
      <author><first>Lindia</first><last>Tjuatja</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Lori</first><last>Levin</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>58-67</pages>
      <abstract>Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation conventionâ€”Generalized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.</abstract>
      <url hash="9f02f658">2023.sigmorphon-1.7</url>
      <bibkey>mortensen-etal-2023-generalized</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.7</doi>
    </paper>
    <paper id="8">
      <title>Jambu: A historical linguistic database for <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>sian languages</title>
      <author><first>Aryaman</first><last>Arora</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Adam</first><last>Farris</last><affiliation>Stanford University</affiliation></author>
      <author><first>Samopriya</first><last>Basu</last><affiliation>Simon Fraser University</affiliation></author>
      <author><first>Suresh</first><last>Kolichala</last><affiliation>Microsoft</affiliation></author>
      <pages>68-77</pages>
      <abstract>We introduce JAMBU, a cognate database of South Asian languages which unifies dozens of previous sources in a structured and accessible format. The database includes nearly 287k lemmata from 602 lects, grouped together in 23k sets of cognates. We outline the data wrangling necessary to compile the dataset and train neural models for reflex prediction on the Indo- Aryan subset of the data. We hope that JAMBU is an invaluable resource for all historical linguists and Indologists, and look towards further improvement and expansion of the database.</abstract>
      <url hash="7e343d73">2023.sigmorphon-1.8</url>
      <bibkey>arora-etal-2023-jambu</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.8</doi>
    </paper>
    <paper id="9">
      <title>Lightweight morpheme labeling in context: Using structured linguistic representations to support linguistic analysis for the language documentation context</title>
      <author><first>Bhargav</first><last>Shandilya</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Alexis</first><last>Palmer</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>78-92</pages>
      <abstract>Linguistic analysis is a core task in the process of documenting, analyzing, and describing endangered and less-studied languages. In addition to providing insight into the properties of the language being studied, having tools to automatically label words in a language for grammatical category and morphological features can support a range of applications useful for language pedagogy and revitalization. At the same time, most modern NLP methods for these tasks require both large amounts of data in the language and compute costs well beyond the capacity of most research groups and language communities. In this paper, we present a gloss-to-gloss (g2g) model for linguistic analysis (specifically, morphological analysis and part-of-speech tagging) that is lightweight in terms of both data requirements and computational expense. The model is designed for the interlinear glossed text (IGT) format, in which we expect the source text of a sentence in a low-resource language, a translation of that sentence into a language of wider communication, and a detailed glossing of the morphological properties of each word in the sentence. We first produce silver standard parallel glossed data by automatically labeling the high-resource translation. The model then learns to transform source language morphological labels into output labels for the target language, mediated by a structured linguistic representation layer. We test the model on both low-resource and high-resource languages, and find that our simple CNN-based model achieves comparable performance to a state-of-the-art transformer-based model, at a fraction of the computational cost.</abstract>
      <url hash="3b8c658a">2023.sigmorphon-1.9</url>
      <bibkey>shandilya-palmer-2023-lightweight</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.9</doi>
      <video href="2023.sigmorphon-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Improving Automated Prediction of <fixed-case>E</fixed-case>nglish Lexical Blends Through the Use of Observable Linguistic Features</title>
      <author><first>Jarem</first><last>Saunders</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>93-97</pages>
      <abstract>The process of lexical blending is difficult to reliably predict. This difficulty has been shown by machine learning approaches in blend modeling, including attempts using then state-of-the-art LSTM deep neural networks trained on character embeddings, which were able to predict lexical blends given the ordered constituent words in less than half of cases, at maximum. This project introduces a novel model architecture which dramatically increases the correct prediction rates for lexical blends, using only Polynomial regression and Random Forest models. This is achieved by generating multiple possible blend candidates for each input word pairing and evaluating them based on observable linguistic features. The success of this model architecture illustrates the potential usefulness of observable linguistic features for problems that elude more advanced models which utilize only features discovered in the latent space.</abstract>
      <url hash="a92c8c4d">2023.sigmorphon-1.10</url>
      <bibkey>saunders-2023-improving</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.10</doi>
      <video href="2023.sigmorphon-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Colexifications for Bootstrapping Cross-lingual Datasets: The Case of Phonology, Concreteness, and Affectiveness</title>
      <author><first>Yiyi</first><last>Chen</last><affiliation>Aalborg University</affiliation></author>
      <author><first>Johannes</first><last>Bjerva</last><affiliation>Department of Computer Science, Aalborg University</affiliation></author>
      <pages>98-109</pages>
      <abstract>Colexification refers to the linguistic phenomenon where a single lexical form is used to convey multiple meanings. By studying cross-lingual colexifications, researchers have gained valuable insights into fields such as psycholinguistics and cognitive sciences (Jack- son et al., 2019; Xu et al., 2020; Karjus et al., 2021; Schapper and Koptjevskaja-Tamm, 2022; FranÃ§ois, 2022). While several multilingual colexification datasets exist, there is untapped potential in using this information to bootstrap datasets across such semantic features. In this paper, we aim to demonstrate how colexifications can be leveraged to create such cross-lingual datasets. We showcase curation procedures which result in a dataset covering 142 languages across 21 language families across the world. The dataset includes ratings of concreteness and affectiveness, mapped with phonemes and phonological features. We further analyze the dataset along different dimensions to demonstrate potential of the proposed procedures in facilitating further interdisciplinary research in psychology, cognitive science, and multilingual natural language processing (NLP). Based on initial investigations, we observe that i) colexifications that are closer in concreteness/affectiveness are more likely to colexify ; ii) certain initial/last phonemes are significantly correlated with concreteness/affectiveness intra language families, such as /k/ as the initial phoneme in both Turkic and Tai-Kadai correlated with concreteness, and /p/ in Dravidian and Sino-Tibetan correlated with Valence; iii) the type-to-token ratio (TTR) of phonemes are positively correlated with concreteness across several language families, while the length of phoneme segments are negatively correlated with concreteness; iv) certain phonological features are negatively correlated with concreteness across languages. The dataset is made public online for further research.</abstract>
      <url hash="ec9a5fa3">2023.sigmorphon-1.11</url>
      <bibkey>chen-bjerva-2023-colexifications</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.11</doi>
      <video href="2023.sigmorphon-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Character alignment methods for dialect-to-standard normalization</title>
      <author><first>Yves</first><last>Scherrer</last><affiliation>University of Helsinki</affiliation></author>
      <pages>110-116</pages>
      <abstract>This paper evaluates various character alignment methods on the task of sentence-level standardization of dialect transcriptions. We compare alignment methods from different scientific traditions (dialectometry, speech processing, machine translation) and apply them to Finnish, Norwegian and Swiss German dialect datasets. In the absence of gold alignments, we evaluate the methods on a set of characteristics that are deemed undesirable for the task. We find that trained alignment methods only show marginal benefits to simple Levenshtein distance. On this particular task, eflomal outperforms related methods such as GIZA++ or fast_align by a large margin.</abstract>
      <url hash="3b48144c">2023.sigmorphon-1.12</url>
      <bibkey>scherrer-2023-character</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.12</doi>
      <video href="2023.sigmorphon-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title><fixed-case>SIGMORPHON</fixed-case>–<fixed-case>U</fixed-case>ni<fixed-case>M</fixed-case>orph 2023 Shared Task 0: Typologically Diverse Morphological Inflection</title>
      <author><first>Omer</first><last>Goldman</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Khuyagbaatar</first><last>Batsuren</last><affiliation>National University of Mongolia</affiliation></author>
      <author><first>Salam</first><last>Khalifa</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Aryaman</first><last>Arora</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Garrett</first><last>Nicolai</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Ekaterina</first><last>Vylomova</last><affiliation>University of Melbourne</affiliation></author>
      <pages>117-125</pages>
      <abstract>The 2023 SIGMORPHON–UniMorph shared task on typologically diverse morphological inflection included a wide range of languages: 26 languages from 9 primary language families. The data this year was all lemma-split, to allow testing models’ generalization ability, and structured along the new hierarchical schema presented in (Batsuren et al., 2022). The systems submitted this year, 9 in number, showed ingenuity and innovativeness, including hard attention for explainability and bidirectional decoding. Special treatment was also given by many participants to the newly-introduced data in Japanese, due to the high abundance of unseen Kanji characters in its test set.</abstract>
      <url hash="ac67d292">2023.sigmorphon-1.13</url>
      <bibkey>goldman-etal-2023-sigmorphon</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>SIGMORPHON</fixed-case>–<fixed-case>U</fixed-case>ni<fixed-case>M</fixed-case>orph 2023 Shared Task 0, Part 2: Cognitively Plausible Morphophonological Generalization in <fixed-case>K</fixed-case>orean</title>
      <author><first>Canaan</first><last>Breiss</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jinyoung</first><last>Jo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>126-131</pages>
      <abstract>This paper summarises data collection and curation for Part 2 of the 2023 SIGMORPHON-UniMorph Shared Task 0, which focused on modeling speaker knowledge and generalization of a pair of interacting phonological processes in Korean. We briefly describe how modeling the generalization task could be of interest to researchers in both Natural Language Processing and linguistics, and then summarise the traditional description of the phonological processes that are at the center of the modeling challenge. We then describe the criteria we used to select and code cases of process application in two Korean speech corpora, which served as the primary learning data. We also report the technical details of the experiment we carried out that served as the primary test data.</abstract>
      <url hash="e0b8733d">2023.sigmorphon-1.14</url>
      <bibkey>breiss-jo-2023-sigmorphon</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.14</doi>
    </paper>
    <paper id="15">
      <title>Morphological reinflection with weighted finite-state transducers</title>
      <author><first>Alice</first><last>Kwak</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Michael</first><last>Hammond</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Cheyenne</first><last>Wing</last><affiliation>University of Arizona</affiliation></author>
      <pages>132-137</pages>
      <abstract>This paper describes the submission by the University of Arizona to the SIGMORPHON 2023 Shared Task on typologically diverse morphological (re-)infection. In our submission, we investigate the role of frequency, length, and weighted transducers in addressing the challenge of morphological reinflection. We start with the non-neural baseline provided for the task and show how some improvement can be gained by integrating length and frequency in prefix selection. We also investigate using weighted finite-state transducers, jump-started from edit distance and directly augmented with frequency. Our specific technique is promising and quite simple, but we see only modest improvements for some languages here.</abstract>
      <url hash="b9cc6960">2023.sigmorphon-1.15</url>
      <bibkey>kwak-etal-2023-morphological</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.15</doi>
    </paper>
    <paper id="16">
      <title>Linear Discriminative Learning: a competitive non-neural baseline for morphological inflection</title>
      <author><first>Cheonkam</first><last>Jeong</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Dominic</first><last>Schmitz</last><affiliation>Heinrich Heine University Düsseldorf, Germany</affiliation></author>
      <author><first>Akhilesh</first><last>Kakolu Ramarao</last><affiliation>Heinrich-Heine-Universität Düsseldorf</affiliation></author>
      <author><first>Anna</first><last>Stein</last><affiliation>Heinrich Heine Universität</affiliation></author>
      <author><first>Kevin</first><last>Tang</last><affiliation>Heinrich-Heine-Universität Düsseldorf</affiliation></author>
      <pages>138-150</pages>
      <abstract>This paper presents our submission to the SIGMORPHON 2023 task 2 of Cognitively Plausible Morphophonological Generalization in Korean. We implemented both Linear Discriminative Learning and Transformer models and found that the Linear Discriminative Learning model trained on a combination of corpus and experimental data showed the best performance with the overall accuracy of around 83%. We found that the best model must be trained on both corpus data and the experimental data of one particular participant. Our examination of speaker-variability and speaker-specific information did not explain why a particular participant combined well with the corpus data. We recommend Linear Discriminative Learning models as a future non-neural baseline system, owning to its training speed, accuracy, model interpretability and cognitive plausibility. In order to improve the model performance, we suggest using bigger data and/or performing data augmentation and incorporating speaker- and item-specifics considerably.</abstract>
      <url hash="28240b3a">2023.sigmorphon-1.16</url>
      <bibkey>jeong-etal-2023-linear</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.16</doi>
    </paper>
    <paper id="17">
      <title>Tü-<fixed-case>CL</fixed-case> at <fixed-case>SIGMORPHON</fixed-case> 2023: Straight-Through Gradient Estimation for Hard Attention</title>
      <author><first>Leander</first><last>Girrbach</last><affiliation>University of Tübingen</affiliation></author>
      <pages>151-165</pages>
      <abstract>This paper describes our systems participating in the 2023 SIGMORPHON Shared Task on Morphological Inflection and in the 2023 SIGMORPHON Shared Task on Interlinear Glossing. We propose methods to enrich predictions from neural models with discrete, i.e. interpretable, information. For morphological inflection, our models learn deterministic mappings from subsets of source lemma characters and morphological tags to individual target characters, which introduces interpretability. For interlinear glossing, our models learn a shallow morpheme segmentation in an unsupervised way jointly with predicting glossing lines. Estimated segmentation may be useful when no ground-truth segmentation is available. As both methods introduce discreteness into neural models, our technical contribution is to show that straight-through gradient estimators are effective to train hard attention models.</abstract>
      <url hash="ba37b95d">2023.sigmorphon-1.17</url>
      <bibkey>girrbach-2023-tu</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.17</doi>
    </paper>
    <paper id="18">
      <title>The <fixed-case>BGU</fixed-case>-<fixed-case>M</fixed-case>e<fixed-case>L</fixed-case>e<fixed-case>L</fixed-case> System for the <fixed-case>SIGMORPHON</fixed-case> 2023 Shared Task on Morphological Inflection</title>
      <author><first>Gal</first><last>Astrach</last><affiliation>Ben Gurion University</affiliation></author>
      <author><first>Yuval</first><last>Pinter</last><affiliation>Ben Gurion University</affiliation></author>
      <pages>166-170</pages>
      <abstract>This paper presents the submission by the MeLeL team to the SIGMORPHON–UniMorph Shared Task on Typologically Diverse and Acquisition-Inspired Morphological Inflection Generation Part 3: Models of Acquisition of Inflectional Noun Morphology in Polish, Estonian, and Finnish. This task requires us to produce the word form given a lemma and a grammatical case, while trying to produce the same error-rate as in children. We approach this task with a reduced-size character-based transformer model, multilingual training and an upsampling method to introduce bias.</abstract>
      <url hash="aa2c4493">2023.sigmorphon-1.18</url>
      <bibkey>astrach-pinter-2023-bgu</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.18</doi>
    </paper>
    <paper id="19">
      <title>Tü-<fixed-case>CL</fixed-case> at <fixed-case>SIGMORPHON</fixed-case> 2023: Straight-Through Gradient Estimation for Hard Attention</title>
      <author><first>Leander</first><last>Girrbach</last><affiliation>University of Tübingen</affiliation></author>
      <pages>171-185</pages>
      <abstract>This paper describes our systems participating in the 2023 SIGMORPHON Shared Task on Morphological Inflection and in the 2023 SIGMORPHON Shared Task on Interlinear Glossing. We propose methods to enrich predictions from neural models with discrete, i.e. interpretable, information. For morphological inflection, our models learn deterministic mappings from subsets of source lemma characters and morphological tags to individual target characters, which introduces interpretability. For interlinear glossing, our models learn a shallow morpheme segmentation in an unsupervised way jointly with predicting glossing lines. Estimated segmentation may be useful when no ground-truth segmentation is available. As both methods introduce discreteness into neural models, our technical contribution is to show that straight-through gradient estimators are effective to train hard attention models.</abstract>
      <url hash="ba37b95d">2023.sigmorphon-1.19</url>
      <bibkey>girrbach-2023-tu-cl</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.19</doi>
    </paper>
    <paper id="20">
      <title>Findings of the <fixed-case>SIGMORPHON</fixed-case> 2023 Shared Task on Interlinear Glossing</title>
      <author><first>Michael</first><last>Ginn</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Sarah</first><last>Moeller</last><affiliation>University of Florida</affiliation></author>
      <author><first>Alexis</first><last>Palmer</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Anna</first><last>Stacey</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Garrett</first><last>Nicolai</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Mans</first><last>Hulden</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Miikka</first><last>Silfverberg</last><affiliation>University of British Columbia</affiliation></author>
      <pages>186-201</pages>
      <abstract>This paper presents the findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing. This first iteration of the shared task explores glossing of a set of six typologically diverse languages: Arapaho, Gitksan, Lezgi, Natügu, Tsez and Uspanteko. The shared task encompasses two tracks: a resource-scarce closed track and an open track, where participants are allowed to utilize external data resources. Five teams participated in the shared task. The winning team Tü-CL achieved a 23.99%-point improvement over a baseline RoBERTa system in the closed track and a 17.42%-point improvement in the open track.</abstract>
      <url hash="aa719d85">2023.sigmorphon-1.20</url>
      <bibkey>ginn-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>LISN</fixed-case> @ <fixed-case>SIGMORPHON</fixed-case> 2023 Shared Task on Interlinear Glossing</title>
      <author><first>Shu</first><last>Okabe</last><affiliation>LISN/CNRS, UniversitÃ© Paris-Saclay</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>ISIR CNRS &amp; Sorbonne UniversitÃ©</affiliation></author>
      <pages>202-208</pages>
      <abstract>This paper describes LISN”’“s submission to the second track (open track) of the shared task on Interlinear Glossing for SIGMORPHON 2023. Our systems are based on Lost, a variation of linear Conditional Random Fields initially developed as a probabilistic translation model and then adapted to the glossing task. This model allows us to handle one of the main challenges posed by glossing, i.e. the fact that the list of potential labels for lexical morphemes is not fixed in advance and needs to be extended dynamically when labelling units are not seen in training. In such situations, we show how to make use of candidate lexical glosses found in the translation and discuss how such extension affects the training and inference procedures. The resulting automatic glossing systems prove to yield very competitive results, especially in low-resource settings.</abstract>
      <url hash="3c714c60">2023.sigmorphon-1.21</url>
      <bibkey>okabe-yvon-2023-lisn</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.21</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>S</fixed-case>ig<fixed-case>M</fixed-case>ore<fixed-case>F</fixed-case>un Submission to the <fixed-case>SIGMORPHON</fixed-case> Shared Task on Interlinear Glossing</title>
      <author><first>Taiqi</first><last>He</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Lindia</first><last>Tjuatja</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Nathaniel</first><last>Robinson</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>David R.</first><last>Mortensen</last><affiliation>Language Technologies Institute, Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Lori</first><last>Levin</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>209-216</pages>
      <abstract>In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.</abstract>
      <url hash="056e7b6b">2023.sigmorphon-1.22</url>
      <bibkey>he-etal-2023-sigmorefun</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.22</doi>
    </paper>
    <paper id="23">
      <title>An Ensembled Encoder-Decoder System for Interlinear Glossed Text</title>
      <author><first>Edith</first><last>Coates</last><affiliation>UBC Mathematics</affiliation></author>
      <pages>217-221</pages>
      <abstract>This paper presents my submission to Track 1 of the 2023 SIGMORPHON shared task on interlinear glossed text (IGT). There are a wide amount of techniques for building and training IGT models (see Moeller and Hulden, 2018; McMillan-Major, 2020; Zhao et al., 2020). I describe my ensembled sequence-to-sequence approach, perform experiments, and share my submission’s test-set accuracy. I also discuss future areas of research in low-resource token classification methods for IGT.</abstract>
      <url hash="e6ca8534">2023.sigmorphon-1.23</url>
      <bibkey>coates-2023-ensembled</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.23</doi>
    </paper>
    <paper id="24">
      <title>Glossy Bytes: Neural Glossing using Subword Encoding</title>
      <author><first>Ziggy</first><last>Cross</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Michelle</first><last>Yun</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Ananya</first><last>Apparaju</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Jata</first><last>MacCabe</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Garrett</first><last>Nicolai</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Miikka</first><last>Silfverberg</last><affiliation>University of British Columbia</affiliation></author>
      <pages>222-229</pages>
      <abstract>This paper presents several different neural subword modelling based approaches to interlinear glossing for seven under-resourced languages as a part of the 2023 SIGMORPHON shared task on interlinear glossing. We experiment with various augmentation and tokenization strategies for both the open and closed tracks of data. We found that while byte-level models may perform well for greater amounts of data, character based approaches remain competitive in their performance in lower resource settings.</abstract>
      <url hash="f3f3de35">2023.sigmorphon-1.24</url>
      <bibkey>cross-etal-2023-glossy</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.24</doi>
    </paper>
    <paper id="27">
      <title>The <fixed-case>SIGMORPHON</fixed-case> 2022 Shared Task on Cross-lingual and Low-Resource Grapheme-to-Phoneme Conversion</title>
      <author><first>Arya D.</first><last>McCarthy</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Jackson L.</first><last>Lee</last><affiliation/></author>
      <author><first>Alexandra</first><last>DeLucia</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Travis</first><last>Bartley</last><affiliation>City University of New York</affiliation></author>
      <author><first>Milind</first><last>Agarwal</last><affiliation>George Mason University</affiliation></author>
      <author><first>Lucas F.E.</first><last>Ashby</last><affiliation>City University of New York</affiliation></author>
      <author><first>Luca</first><last>Del Signore</last><affiliation>City University of New York</affiliation></author>
      <author><first>Cameron</first><last>Gibson</last><affiliation>City University of New York</affiliation></author>
      <author><first>Reuben</first><last>Raff</last><affiliation>City University of New York</affiliation></author>
      <author><first>Winston</first><last>Wu</last><affiliation>University of Michigan</affiliation></author>
      <pages>230-238</pages>
      <abstract>Grapheme-to-phoneme conversion is an important component in many speech technologies, but until recently there were no multilingual benchmarks for this task. The third iteration of the SIGMORPHON shared task on multilingual grapheme-to-phoneme conversion features many improvements from the previous year’s task (Ashby et al., 2021), including additional languages, three subtasks varying the amount of available resources, extensive quality assurance procedures, and automated error analyses. Three teams submitted a total of fifteen systems, at best achieving relative reductions of word error rate of 14% in the crosslingual subtask and 14% in the very-low resource subtask. The generally consistent result is that cross-lingual transfer substantially helps grapheme-to-phoneme modeling, but not to the same degree as in-language examples.</abstract>
      <url hash="3a123f9d">2023.sigmorphon-1.27</url>
      <bibkey>mccarthy-etal-2023-sigmorphon</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.27</doi>
    </paper>
    <paper id="28">
      <title><fixed-case>SIGMORPHON</fixed-case> 2022 Shared Task on Grapheme-to-Phoneme Conversion Submission Description: Sequence Labelling for <fixed-case>G</fixed-case>2<fixed-case>P</fixed-case></title>
      <author><first>Leander</first><last>Girrbach</last><affiliation>The University of Tübingen</affiliation></author>
      <pages>239-244</pages>
      <abstract>This paper describes our participation in the Third SIGMORPHON Shared Task on Grapheme-to-Phoneme Conversion (Low-Resource and Cross-Lingual) (McCarthy et al.,2022). Our models rely on different sequence labelling methods. The main model predicts multiple phonemes from each grapheme and is trained using CTC loss (Graves et al., 2006). We find that sequence labelling methods yield worse performance than the baseline when enough data is available, but can still be used when very little data is available. Furthermore, we demonstrate that alignments learned by the sequence labelling models can be easily inspected.</abstract>
      <url hash="48c553ec">2023.sigmorphon-1.28</url>
      <bibkey>girrbach-2023-sigmorphon</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.28</doi>
    </paper>
    <paper id="29">
      <title>Low-resource grapheme-to-phoneme mapping with phonetically-conditioned transfer</title>
      <author><first>Michael</first><last>Hammond</last><affiliation>The University of Arizona</affiliation></author>
      <pages>245-248</pages>
      <abstract>In this paper we explore a very simple nonneural approach to mapping orthography to phonetic transcription in a low-resource context with transfer data from a related language. We start from a baseline system and focus our efforts on data augmentation. We make three principal moves. First, we start with an HMMbased system (Novak et al., 2012). Second, we augment our basic system by recombining legal substrings in restricted fashion (Ryan and Hulden, 2020). Finally, we limit our transfer data by only using training pairs where the phonetic form shares all bigrams with the target language.</abstract>
      <url hash="46552681">2023.sigmorphon-1.29</url>
      <bibkey>hammond-2023-low</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.29</doi>
    </paper>
    <paper id="30">
      <title>A future for universal grapheme-phoneme transduction modeling with neuralized finite-state transducers</title>
      <author><first>Chu-Cheng Lin</first><last>Lin</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>249-249</pages>
      <abstract>We propose a universal grapheme-phoneme transduction model using neuralized finite-state transducers. Many computational models of grapheme-phoneme transduction nowadays are based on the (autoregressive) sequence-to-sequence string transduction paradigm. While such models have achieved state-of-the-art performance, they suffer from theoretical limitations of autoregressive models. On the other hand, neuralized finite-state transducers (NFSTs) have shown promising results on various string transduction tasks. NFSTs can be seen as a generalization of weighted finite-state transducers (WFSTs), and can be seen as pairs of a featurized finite-state machine (‘marked finite-state transducer’ or MFST in NFST terminology), and a string scoring function. Instead of taking a product of local contextual feature weights on FST arcs, NFSTs can employ arbitrary scoring functions to weight global contextual features of a string transduction, and therefore break the Markov property. Furthermore, NFSTs can be formally shown to be more expressive than (autoregressive) seq2seq models. Empirically, joint grapheme-phoneme transduction NFSTs have consistently outperformed vanilla seq2seq models on grapheme-tophoneme and phoneme-to-grapheme transduction tasks for English. Furthermore, they provide interpretable aligned string transductions, thanks to their finite-state machine component. In this talk, we propose a multilingual extension of the joint grapheme-phoneme NFST. We achieve this goal by modeling typological and phylogenetic features of languages and scripts as optional latent variables using a finite-state machine. The result is a versatile graphemephoneme transduction model: in addition to standard monolingual and multilingual transduction, the proposed multilingual NFST can also be used in various controlled generation scenarios, such as phoneme-to-grapheme transduction of an unseen language-script pair. We also plan to release an NFST software package.</abstract>
      <url hash="2d3ed4ba">2023.sigmorphon-1.30</url>
      <bibkey>lin-2023-future</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.30</doi>
    </paper>
    <paper id="31">
      <title>Fine-tuning m<fixed-case>SLAM</fixed-case> for the <fixed-case>SIGMORPHON</fixed-case> 2022 Shared Task on Grapheme-to-Phoneme Conversion</title>
      <author><first>Dan</first><last>Garrette</last><affiliation>Google Research</affiliation></author>
      <pages>250-250</pages>
      <abstract>Grapheme-to-phoneme (G2P) conversion is a task that is inherently related to both written and spoken language. Therefore, our submission to the G2P shared task builds off of mSLAM (Bapna et al., 2022), a 600M parameter encoder model pretrained simultaneously on text from 101 languages and speech from 51 languages. For fine-tuning a G2P model, we combined mSLAM’s text encoder, which uses characters as its input tokens, with an uninitialized single-layer RNN-T decoder (Graves, 2012) whose vocabulary is the set of all 381 phonemes appearing in the shared task data. We took an explicitly multilingual approach to modeling the G2P tasks, fine-tuning and evaluating a single model that covered all the languages in each task, and adding language codes as prefixes to the input strings as a means of specifying the language of each example. Our models perform well in the shared task’s “high” setting (in which they were trained on 1,000 words from each language), though they do poorly in the “low” task setting (training on only 100 words from each language). Our models also perform reasonably in the “mixed” setting (training on 100 words in the target language and 1000 words in a related language), hinting that mSLAM’s multilingual pretraining may be enabling useful cross-lingual sharing.</abstract>
      <url hash="958fdfeb">2023.sigmorphon-1.31</url>
      <bibkey>garrette-2023-fine</bibkey>
      <doi>10.18653/v1/2023.sigmorphon-1.31</doi>
    </paper>
  </volume>
</collection>
