<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.acl">
  <volume id="srw" ingest-date="2023-06-29">
    <meta>
      <booktitle>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics - Student Research Workshop</booktitle>
      <editor><first>Vishakh</first><last>Padmakumar</last></editor>
      <editor><first>Gisela</first><last>Vallejo</last></editor>
      <editor><first>Yao</first><last>Fu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="d8f06fd0">2023.acl-srw</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="44ac9b55">2023.acl-srw.0</url>
      <bibkey>acl-2023-association</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</title>
      <author><first>Dongqi</first><last>Pu</last><affiliation>Saarland University</affiliation></author>
      <author><first>Vera</first><last>Demberg</last><affiliation>Saarland University</affiliation></author>
      <pages>1-18</pages>
      <abstract>ChatGPT Analysis</abstract>
      <url hash="14931f61">2023.acl-srw.1</url>
      <bibkey>pu-demberg-2023-chatgpt</bibkey>
    </paper>
    <paper id="2">
      <title>Multi-Dialectal Representation Learning of Sinitic Phonology</title>
      <author><first>Zhibai</first><last>Jia</last><affiliation>No.2 High School of East China Normal University</affiliation></author>
      <pages>19-29</pages>
      <abstract>Representation learning of Sinitic Phonology using a knowledge graph based method</abstract>
      <url hash="86e7ec92">2023.acl-srw.2</url>
      <attachment type="SupplementaryMaterial" hash="7bc909c5">2023.acl-srw.2.SupplementaryMaterial.zip</attachment>
      <bibkey>jia-2023-multi</bibkey>
    </paper>
    <paper id="4">
      <title>Prompt-based Zero-shot Text Classification with Conceptual Knowledge</title>
      <author><first>Yuqi</first><last>Wang</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Qi</first><last>Chen</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Kaizhu</first><last>Huang</last><affiliation>Duke Kunshan University</affiliation></author>
      <author><first>Anh</first><last>Nguyen</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Suparna</first><last>De</last><affiliation>University of Surrey</affiliation></author>
      <pages>30-38</pages>
      <abstract>The proposed framework incorporates conceptual knowledge for prompt-based text classification in the extreme zero-shot setting, which outperforms existing approaches in sentiment analysis and topic detection on four widely-used datasets.</abstract>
      <url hash="32d39d3c">2023.acl-srw.4</url>
      <bibkey>wang-etal-2023-prompt</bibkey>
    </paper>
    <paper id="5">
      <title>How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in <fixed-case>J</fixed-case>apanese</title>
      <author><first>Takuro</first><last>Fujii</last><affiliation>Yokohama National University</affiliation></author>
      <author><first>Koki</first><last>Shibata</last><affiliation>University of Tsukuba</affiliation></author>
      <author><first>Atsuki</first><last>Yamaguchi</last><affiliation>Hitachi, Ltd.</affiliation></author>
      <author><first>Terufumi</first><last>Morishita</last><affiliation>Hitachi.ltd</affiliation></author>
      <author><first>Yasuhiro</first><last>Sogawa</last><affiliation>Hitachi, Ltd.</affiliation></author>
      <pages>39-49</pages>
      <abstract>We investigate the impact of different tokenizers on downstream performance in Japanese NLP, with the case of BERT architecture.</abstract>
      <url hash="7805d815">2023.acl-srw.5</url>
      <bibkey>fujii-etal-2023-different</bibkey>
    </paper>
    <paper id="7">
      <title>Semantic-Aware Dynamic Retrospective-Prospective Reasoning for Event-Level Video Question Answering</title>
      <author><first>Chenyang</first><last>Lyu</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Tianbo</first><last>Ji</last><affiliation>Nantong University</affiliation></author>
      <author><first>Yvette</first><last>Graham</last><affiliation>ADAPT, Trinity College Dublin</affiliation></author>
      <author><first>Jennifer</first><last>Foster</last><affiliation>Dublin City University</affiliation></author>
      <pages>50-56</pages>
      <abstract>The paper is about video QA. It utilizes SRL partitioning to improve multi-step attention and reasoning of the models to attend to different frames for different parts of the question.</abstract>
      <url hash="92b00d3d">2023.acl-srw.7</url>
      <bibkey>lyu-etal-2023-semantic</bibkey>
    </paper>
    <paper id="8">
      <title>Jamp: Controlled <fixed-case>J</fixed-case>apanese Temporal Inference Dataset for Evaluating Generalization Capacity of Language Models</title>
      <author><first>Tomoki</first><last>Sugimoto</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yasumasa</first><last>Onoe</last><affiliation>The University of Texas at Austin</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>57-68</pages>
      <abstract>We construct Jamp, which is a Japanese NLI dataset for temporal inference, and evaluate the generalization capacity of several LMs on our dataset.</abstract>
      <url hash="7c374394">2023.acl-srw.8</url>
      <bibkey>sugimoto-etal-2023-jamp</bibkey>
    </paper>
    <paper id="10">
      <title>Constructing Multilingual Code Search Dataset Using Neural Machine Translation</title>
      <author><first>Ryo</first><last>Sekizawa</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Shuai</first><last>Lu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>69-75</pages>
      <abstract>We create a multilingual code search dataset by translating the existing English dataset with a machine translation model and conduct a baseline experiment on a code search task.</abstract>
      <url hash="dd8df7e4">2023.acl-srw.10</url>
      <bibkey>sekizawa-etal-2023-constructing</bibkey>
    </paper>
    <paper id="12">
      <title>Multimodal Neural Machine Translation Using Synthetic Images Transformed by Latent Diffusion Model</title>
      <author><first>Ryoya</first><last>Yuasa</last><affiliation>Doshisha University</affiliation></author>
      <author><first>Akihiro</first><last>Tamura</last><affiliation>Doshisha University</affiliation></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last><affiliation>Ehime University</affiliation></author>
      <author><first>Takashi</first><last>Ninomiya</last><affiliation>Ehime University</affiliation></author>
      <author><first>Tsuneo</first><last>Kato</last><affiliation>Doshisha university</affiliation></author>
      <pages>76-82</pages>
      <abstract>This study proposes a new multimodal neural machine translation model using synthetic images transformed by a latent diffusion model.</abstract>
      <url hash="d793880d">2023.acl-srw.12</url>
      <bibkey>yuasa-etal-2023-multimodal</bibkey>
    </paper>
    <paper id="15">
      <title>Enhancing <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Understanding with Derived Noisy Syntax Trees</title>
      <author><first>Ping</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Shitou</first><last>Zhang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Zuchao</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jingrui</first><last>Hou</last><affiliation>Department of Computer Science,Loughborough University</affiliation></author>
      <pages>83-92</pages>
      <abstract>This paper introduces a confidence-based syntax encoding network (cSEN) to incorporate syntax in ancient Chinese understanding tasks, effectively improving performance by mitigating noise and incompatibility issues.</abstract>
      <url hash="39081656">2023.acl-srw.15</url>
      <bibkey>wang-etal-2023-enhancing</bibkey>
    </paper>
    <paper id="17">
      <title>The <fixed-case>T</fixed-case>uring Quest: Can Transformers Make Good <fixed-case>NPC</fixed-case>s?</title>
      <author><first>Qi Chen</first><last>Gao</last><affiliation>Brock University</affiliation></author>
      <author><first>Ali</first><last>Emami</last><affiliation>Brock University</affiliation></author>
      <pages>93-103</pages>
      <abstract>We explored the generation of NPC dialogue using a zero-shot prompting method as well as the ability of LMs to self-evaluate and score dialogue with few-shot learning.</abstract>
      <url hash="1b12fd49">2023.acl-srw.17</url>
      <bibkey>gao-emami-2023-turing</bibkey>
    </paper>
    <paper id="18">
      <title>Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section</title>
      <author><first>Hongyi</first><last>Zheng</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Yixin</first><last>Zhu</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Lavender</first><last>Jiang</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Kyunghyun</first><last>Cho</last><affiliation>Center for Data Science and Courant Institute of Mathematical Sciences, New York University; Prescient Design; CIFAR</affiliation></author>
      <author><first>Eric</first><last>Oermann</last><affiliation>Department of Neurosurgery, NYU Langone Health; Department of Neurosurgery and Radiology, NYU Grossman School of Medicine</affiliation></author>
      <pages>104-108</pages>
      <abstract>We propose a data-driven framework to select clinical note sections with high predictive power.</abstract>
      <url hash="bc3e066d">2023.acl-srw.18</url>
      <bibkey>zheng-etal-2023-making</bibkey>
    </paper>
    <paper id="19">
      <title>Intriguing Effect of the Correlation Prior on <fixed-case>ICD</fixed-case>-9 Code Assignment</title>
      <author><first>Zihao</first><last>Yang</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Chenkang</first><last>Zhang</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Muru</first><last>Wu</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Xujin</first><last>Liu</last><affiliation>Department of Electrical and Computer Engineering, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Lavender</first><last>Jiang</last><affiliation>Center of Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Kyunghyun</first><last>Cho</last><affiliation>Center of Data Science &amp; Courant Institute of Mathematical Sciences, New York University; Prescient Design; CIFAR</affiliation></author>
      <author><first>Eric</first><last>Oermann</last><affiliation>Department of Neurosurgery, NYU Langone Health; Department of Neurosurgery and Radiology, NYU Grossman School of Medicine</affiliation></author>
      <pages>109-118</pages>
      <abstract>This paper investigates the usefulness of correlation bias in improving language models’ performance on predicting imbalanced clinical codes from discharge summaries.</abstract>
      <url hash="d23d745e">2023.acl-srw.19</url>
      <bibkey>yang-etal-2023-intriguing</bibkey>
    </paper>
    <paper id="20">
      <title>Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks</title>
      <author><first>Mateusz</first><last>Baran</last><affiliation>Wrocław University of Science and Technology</affiliation></author>
      <author><first>Joanna</first><last>Baran</last><affiliation>Wroclaw University of Science and Technology</affiliation></author>
      <author><first>Mateusz</first><last>Wójcik</last><affiliation>Wroclaw University of Science and Technology. Alphamoon Ltd.</affiliation></author>
      <author><first>Maciej</first><last>Zięba</last><affiliation>Wroclaw University of Science and Technology</affiliation></author>
      <author><first>Adam</first><last>Gonczarek</last><affiliation>Alphamoon Ltd.</affiliation></author>
      <pages>119-129</pages>
      <abstract>The paper evaluates existing OOD detection methods for NLP systems and emphasizes the need for further research to develop more effective approaches to ensure safety and trustworthiness of NLP systems.</abstract>
      <url hash="c6515660">2023.acl-srw.20</url>
      <attachment type="SupplementaryMaterial" hash="6402114e">2023.acl-srw.20.SupplementaryMaterial.zip</attachment>
      <bibkey>baran-etal-2023-classical</bibkey>
    </paper>
    <paper id="22">
      <title>Can <fixed-case>LM</fixed-case>s Store and Retrieve 1-to-N Relational Knowledge?</title>
      <author><first>Haruki</first><last>Nagasawa</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Benjamin</first><last>Heinzerling</last><affiliation>RIKEN AIP &amp; Tohoku University</affiliation></author>
      <author><first>Kazuma</first><last>Kokuta</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>Tohoku University / Riken</affiliation></author>
      <pages>130-138</pages>
      <abstract>Our study aimed to explore the feasibility of using LMs as KBs, and we focused specifically on 1-to-N relational knowledge, an area that has not been extensively researched, and proposed a comprehensive approach that involved identifying the unique characteristics of this type of knowledge, designing appropriate training methods, and developing evaluation perspectives.</abstract>
      <url hash="6971f41d">2023.acl-srw.22</url>
      <bibkey>nagasawa-etal-2023-lms</bibkey>
    </paper>
    <paper id="24">
      <title>Theoretical Linguistics Rivals Embeddings in Language Clustering for Multilingual Named Entity Recognition</title>
      <author><first>Sakura</first><last>Imai</last><affiliation>Waseda University</affiliation></author>
      <author><first>Daisuke</first><last>Kawahara</last><affiliation>Waseda University</affiliation></author>
      <author><first>Naho</first><last>Orita</last><affiliation>Waseda University</affiliation></author>
      <author><first>Hiromune</first><last>Oda</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>139-151</pages>
      <abstract>This study investigates whether and how theoretical linguistics improves language clustering for multilingual named entity recognition (NER), with the two types of language groupings proposed: one based on morpho-syntactic features in a nominal domain and one based on a head parameter.</abstract>
      <url hash="318c1d81">2023.acl-srw.24</url>
      <bibkey>imai-etal-2023-theoretical</bibkey>
    </paper>
    <paper id="26">
      <title>Native Language Prediction from Gaze: a Reproducibility Study</title>
      <author><first>Lina</first><last>Skerath</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Paulina</first><last>Toborek</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Anita</first><last>Zielińska</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Maria</first><last>Barrett</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Rob</first><last>Van Der Goot</last><affiliation>IT University of Copenhagen</affiliation></author>
      <pages>152-159</pages>
      <abstract>A reproduction study of native language prediction from English as second language reading eye-tracking data.</abstract>
      <url hash="ba76735e">2023.acl-srw.26</url>
      <bibkey>skerath-etal-2023-native</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>M</fixed-case>ed<fixed-case>T</fixed-case>em2.0: Prompt-based Temporal Classification of Treatment Events from Discharge Summaries</title>
      <author><first>Yang</first><last>Cui</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Lifeng</first><last>Han</last><affiliation>The University of Manchester</affiliation></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <pages>160-183</pages>
      <abstract>We use Prompt-based learning on LLMs for Temporal Classification of Treatment Events from Discharge Summaries of clinical data.</abstract>
      <url hash="517efa23">2023.acl-srw.27</url>
      <bibkey>cui-etal-2023-medtem2</bibkey>
    </paper>
    <paper id="28">
      <title>Sudden Semantic Shifts in <fixed-case>S</fixed-case>wedish <fixed-case>NATO</fixed-case> discourse</title>
      <author><first>Brian</first><last>Bonafilia</last><affiliation>Chalmers University of Technology</affiliation></author>
      <author><first>Bastiaan</first><last>Bruinsma</last><affiliation>Chalmers University of Technology</affiliation></author>
      <author><first>Denitsa</first><last>Saynova</last><affiliation>Chalmers University of Technology</affiliation></author>
      <author><first>Moa</first><last>Johansson</last><affiliation>Chalmers University of Technology</affiliation></author>
      <pages>184-193</pages>
      <abstract>We look at short-term semantic shifts in the Swedish discussion about NATO membership.</abstract>
      <url hash="9e42105e">2023.acl-srw.28</url>
      <bibkey>bonafilia-etal-2023-sudden</bibkey>
    </paper>
    <paper id="29">
      <title>Building a Buzzer-quiz Answering System</title>
      <author><first>Naoya</first><last>Sugiura</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Kosuke</first><last>Yamada</last><affiliation>Nagoya university</affiliation></author>
      <author><first>Ryohei</first><last>Sasano</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Koichi</first><last>Takeda</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Katsuhiko</first><last>Toyama</last><affiliation>Nagoya University</affiliation></author>
      <pages>194-199</pages>
      <abstract>This paper presents two types of buzzer-quiz answering systems that can predict the answer from only part of a question and then proposes a method to estimate the accuracy of the answers for each system by using the internal scores of each model.</abstract>
      <url hash="21b4e3c7">2023.acl-srw.29</url>
      <bibkey>sugiura-etal-2023-building</bibkey>
    </paper>
    <paper id="30">
      <title>Probing for Hyperbole in Pre-Trained Language Models</title>
      <author><first>Nina</first><last>Schneidermann</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Daniel</first><last>Hershcovich</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Bolette</first><last>Pedersen</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>200-211</pages>
      <abstract>This paper contributes to hyperbole identification research in NLP with two probing tasks (edge and MDL probing) for 3 pre-trained language models, as well as an attempt to shed light on problems annotating hyperbole.</abstract>
      <url hash="5bd81614">2023.acl-srw.30</url>
      <bibkey>schneidermann-etal-2023-probing</bibkey>
    </paper>
    <paper id="31">
      <title>Towards Efficient Dialogue Processing in the Emergency Response Domain</title>
      <author><first>Tatiana</first><last>Anikina</last><affiliation>DFKI / Saarland Informatics Campus</affiliation></author>
      <pages>212-225</pages>
      <abstract>This paper is about dialogue act classification and slot tagging in the emergency response domain.</abstract>
      <url hash="bb4799cb">2023.acl-srw.31</url>
      <bibkey>anikina-2023-towards</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>I</fixed-case> already said that! Degenerating redundant questions in open-domain dialogue systems.</title>
      <author><first>Long</first><last>Mai</last><affiliation>University College Dublin</affiliation></author>
      <author><first>Julie</first><last>Carson-berndsen</last><affiliation>University College Dublin</affiliation></author>
      <pages>226-236</pages>
      <abstract>This paper propose methods to reduce the number of redundant questions generated by open-domain dialogue systems.</abstract>
      <url hash="6dc7fccc">2023.acl-srw.33</url>
      <bibkey>mai-carson-berndsen-2023-already</bibkey>
    </paper>
    <paper id="34">
      <title>Is a Knowledge-based Response Engaging?: An Analysis on Knowledge-Grounded Dialogue with Information Source Annotation</title>
      <author><first>Takashi</first><last>Kodama</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Hirokazu</first><last>Kiyomaru</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Yin Jou</first><last>Huang</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Taro</first><last>Okahisa</last><affiliation>Shizuoka University</affiliation></author>
      <author><first>Sadao</first><last>Kurohashi</last><affiliation>Kyoto University</affiliation></author>
      <pages>237-243</pages>
      <abstract>This paper investigates how humans incorporate speaker-derived information by annotating the utterances in a knowledge-grounded dialogue corpus.</abstract>
      <url hash="7d7191ff">2023.acl-srw.34</url>
      <bibkey>kodama-etal-2023-knowledge</bibkey>
    </paper>
    <paper id="35">
      <title>Choosing What to Mask: More Informed Masking for Multimodal Machine Translation</title>
      <author><first>Julia</first><last>Sato</last><affiliation>Federal University of Sao Carlos</affiliation></author>
      <author><first>Helena</first><last>Caseli</last><affiliation>Federal University of São Carlos</affiliation></author>
      <author><first>Lucia</first><last>Specia</last><affiliation>Imperial College London</affiliation></author>
      <pages>244-253</pages>
      <abstract>More informed masking in cross-lingual visual pre-training for multimodal machine translation.</abstract>
      <url hash="ed7f9453">2023.acl-srw.35</url>
      <bibkey>sato-etal-2023-choosing</bibkey>
    </paper>
    <paper id="36">
      <title>Combining Tradition with Modernness: Exploring Event Representations in Vision-and-Language Models for Visual Goal-Step Inference</title>
      <author><first>Chong</first><last>Shen</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Carina</first><last>Silberer</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>254-265</pages>
      <abstract>This paper studies various methods and their effects on multimodal procedural knowledge understanding of injecting the early shallow017 event representations to nowadays multimodal deep learning-based models.</abstract>
      <url hash="890fb69a">2023.acl-srw.36</url>
      <bibkey>shen-silberer-2023-combining</bibkey>
    </paper>
    <paper id="37">
      <title>Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values</title>
      <author><first>Stephanie</first><last>Schoch</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Yangfeng</first><last>Ji</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Ritwick</first><last>Mishra</last><affiliation>University of Virginia</affiliation></author>
      <pages>266-275</pages>
      <abstract>This paper proposes a sampling chain based method to make Shapley values computationally feasible for data valuation and selection for large language models.</abstract>
      <url hash="0e209134">2023.acl-srw.37</url>
      <bibkey>schoch-etal-2023-data</bibkey>
    </paper>
    <paper id="38">
      <title>Distractor Generation for Fill-in-the-Blank Exercises by Question Type</title>
      <author><first>Nana</first><last>Yoshimi</last><affiliation>Ehime University</affiliation></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last><affiliation>Ehime University</affiliation></author>
      <author><first>Satoru</first><last>Uchida</last><affiliation>Kyushu University</affiliation></author>
      <author><first>Yuki</first><last>Arase</last><affiliation>Osaka University</affiliation></author>
      <author><first>Takashi</first><last>Ninomiya</last><affiliation>Ehime University</affiliation></author>
      <pages>276-281</pages>
      <abstract>We define three types of questions (grammar, function word, and context) for fill-in-the-blank exercises and propose a method to generate distractors according to the characteristics of each question type.</abstract>
      <url hash="34f8aa38">2023.acl-srw.38</url>
      <bibkey>yoshimi-etal-2023-distractor</bibkey>
    </paper>
    <paper id="40">
      <title>Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity</title>
      <author><first>Gabriel</first><last>Simmons</last><affiliation>University of California, Davis</affiliation></author>
      <pages>282-297</pages>
      <abstract>This paper studies whether LLMs moral preferences based on prompted political ideology replicate known results obtained in social science studies, using tools from Moral Foundations Theory</abstract>
      <url hash="9b4c8c27">2023.acl-srw.40</url>
      <bibkey>simmons-2023-moral</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>LECO</fixed-case>: Improving Early Exiting via Learned Exits and Comparison-based Exiting Mechanism</title>
      <author><first>Jingfan</first><last>Zhang</last><affiliation>Univ of Ottawa</affiliation></author>
      <author><first>Ming</first><last>Tan</last><affiliation>Tencent</affiliation></author>
      <author><first>Pengyu</first><last>Dai</last><affiliation>Sjtu</affiliation></author>
      <author><first>Wei</first><last>Zhu</last><affiliation>East China Normal University</affiliation></author>
      <pages>298-309</pages>
      <abstract>Speeding up the inference of pretrained models by designing better intermediate early exits and a comparison-based early exiting mechanism</abstract>
      <url hash="98549855">2023.acl-srw.43</url>
      <bibkey>zhang-etal-2023-leco</bibkey>
    </paper>
    <paper id="44">
      <title>Authorship Attribution of Late 19th Century Novels using <fixed-case>GAN</fixed-case>-<fixed-case>BERT</fixed-case></title>
      <author><first>Kanishka</first><last>Silva</last><affiliation>University of Wolverhampton</affiliation></author>
      <author><first>Burcu</first><last>Can</last><affiliation>University of Stirling</affiliation></author>
      <author><first>Frédéric</first><last>Blain</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Raheem</first><last>Sarwar</last><affiliation>OTEHM, Manchester Metropolitan University</affiliation></author>
      <author><first>Laura</first><last>Ugolini</last><affiliation>University of Wolverhampton</affiliation></author>
      <author><first>Ruslan</first><last>Mitkov</last><affiliation>University of Wolverhampton</affiliation></author>
      <pages>310-320</pages>
      <abstract>This paper is about performing authorship attribution of long 19th-century novels using the GAN-BERT model, comparing author counts, author combinations and sample text sizes.</abstract>
      <url hash="b31cd570">2023.acl-srw.44</url>
      <bibkey>silva-etal-2023-authorship</bibkey>
    </paper>
    <paper id="46">
      <title>How-to Guides for Specific Audiences: A Corpus and Initial Findings</title>
      <author><first>Nicola</first><last>Fanton</last><affiliation>Universität Stuttgart</affiliation></author>
      <author><first>Agnieszka</first><last>Falenska</last><affiliation>IMS, University of Stuttgart</affiliation></author>
      <author><first>Michael</first><last>Roth</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>321-333</pages>
      <abstract>We collect how-to guides for different target audiences and investigate qualitative and quantitative differences.</abstract>
      <url hash="fd5a1489">2023.acl-srw.46</url>
      <bibkey>fanton-etal-2023-guides</bibkey>
    </paper>
    <paper id="47">
      <title>“When Words Fail, Emojis Prevail”: A Novel Architecture for Generating Sarcastic Sentences With Emoji Using Valence Reversal and Semantic Incongruity</title>
      <author><first>Faria Binte</first><last>Kader</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Nafisa</first><last>Hossain Nujat</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Tasmia Binte</first><last>Sogir</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Mohsinul</first><last>Kabir</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Hasan</first><last>Mahmud</last><affiliation>Associate Professor</affiliation></author>
      <author><first>Md Kamrul</first><last>Hasan</last><affiliation>Islamic University of Technology</affiliation></author>
      <pages>334-351</pages>
      <abstract>A new framework in which when given a non-sarcastic text as input, the text is converted into a sarcastic one with emoji where the emoji will specifically help to identify the sarcastic intent of the text.</abstract>
      <url hash="c23cc187">2023.acl-srw.47</url>
      <bibkey>kader-etal-2023-words</bibkey>
    </paper>
    <paper id="48">
      <title>Semantic Accuracy in Natural Language Generation: A Thesis Proposal</title>
      <author><first>Patricia</first><last>Schmidtova</last><affiliation>Charles University</affiliation></author>
      <pages>352-361</pages>
      <abstract>We propose a thesis in which we explore how evaluation and interpretability techniques could lead to better natural language generation systems.</abstract>
      <url hash="a4d44d8d">2023.acl-srw.48</url>
      <bibkey>schmidtova-2023-semantic</bibkey>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2023-07-06">
    <meta>
      <booktitle>Proceedings of the System Demonstrations</booktitle>
      <editor><first>Danushka</first><last>Bollegala</last></editor>
      <editor><first>Ruihong</first><last>Huang</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada.</address>
      <month>July</month>
      <year>2023</year>
      <url hash="71fe706c">2023.acl-demo</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="8dbfcbe9">2023.acl-demo.0</url>
      <bibkey>acl-2023-system</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Human-in-the-loop Schema Induction</title>
      <author><first>Tianyi</first><last>Zhang</last><affiliation>Department of Computer and Information Science, University of Pennsylvania</affiliation></author>
      <author><first>Isaac</first><last>Tham</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Zhaoyi</first><last>Hou</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Jiaxuan</first><last>Ren</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Leon</first><last>Zhou</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Hainiu</first><last>Xu</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Li</first><last>Zhang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Lara</first><last>Martin</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author><first>Rotem</first><last>Dror</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Sha</first><last>Li</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois at Urbana-Champaign and Amazon (Amazon Scholar)</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado</affiliation></author>
      <author><first>Susan Windisch</first><last>Brown</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Reece</first><last>Suchocki</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>1-10</pages>
      <abstract>Schema induction builds a graph representation explaining how events unfold in a scenario. Existing approaches have been based on information retrieval (IR) and information extraction (IE), often with limited human curation. We demonstrate a human-in-the-loop schema induction system powered by GPT-3. We first describe the different modules of our system, including prompting to generate schematic elements, manual edit of those elements, and conversion of those into a schema graph. By qualitatively comparing our system to previous ones, we show that our system not only transfers to new domains more easily than previous approaches, but also reduces efforts of human curation thanks to our interactive interface.</abstract>
      <url hash="4a2342e8">2023.acl-demo.1</url>
      <bibkey>zhang-etal-2023-human</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>P</fixed-case>ers<fixed-case>LEARN</fixed-case>: Research Training through the Lens of Perspective Cultivation</title>
      <author><first>Yu-Zhe</first><last>Shi</last><affiliation>Peking University</affiliation></author>
      <author><first>Shiqian</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Xinyi</first><last>Niu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Qiao</first><last>Xu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Jiawen</first><last>Liu</last><affiliation>Tongji University</affiliation></author>
      <author><first>Yifan</first><last>Xu</last><affiliation>Kyushu University</affiliation></author>
      <author><first>Shiyu</first><last>Gu</last><affiliation>Peking University</affiliation></author>
      <author><first>Bingru</first><last>He</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xinyang</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xinyu</first><last>Zhao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zijian</first><last>Zhao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yidong</first><last>Lyu</last><affiliation>Chinese University of Hong Kong</affiliation></author>
      <author><first>Zhen</first><last>Li</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Sijia</first><last>Liu</last><affiliation>University of the Arts London</affiliation></author>
      <author><first>Lin</first><last>Qiu</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Jinhao</first><last>Ji</last><affiliation>Boston College</affiliation></author>
      <author><first>Lecheng</first><last>Ruan</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Yuxi</first><last>Ma</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Wenjuan</first><last>Han</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yixin</first><last>Zhu</last><affiliation>Peking University</affiliation></author>
      <pages>11-30</pages>
      <abstract>Scientific research is inherently shaped by its authors’ perspectives, influenced by various factorssuch as their personality, community, or society. Junior researchers often face challenges in identifying the perspectives reflected in the existing literature and struggle to develop their own viewpoints. In response to this issue, we introduce PersLEARN , a tool designed to facilitate the cultivation of scientific perspectives, starting from a basic seed idea and progressing to a well-articulated framework. By interacting with a prompt-based model, researchers can develop their perspectives explicitly. Our humanstudy reveals that scientific perspectives developed by students using PersLEARN exhibit a superior level of logical coherence and depth compared to those that did not. Furthermore, our pipeline outperforms baseline approaches across multiple domains of literature from various perspectives. These results suggest that PersLEARN could help foster a greater appreciation of diversity in scientific perspectives as an essential component of research training.</abstract>
      <url hash="5a04b1c9">2023.acl-demo.2</url>
      <bibkey>shi-etal-2023-perslearn</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>LAVIS</fixed-case>: A One-stop Library for Language-Vision Intelligence</title>
      <author><first>Dongxu</first><last>Li</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Junnan</first><last>Li</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Hung</first><last>Le</last><affiliation>Salesforce</affiliation></author>
      <author><first>Guangsen</first><last>Wang</last><affiliation>Salesforce Research Asia</affiliation></author>
      <author><first>Silvio</first><last>Savarese</last><affiliation>Salesforce</affiliation></author>
      <author><first>Steven C.H.</first><last>Hoi</last><affiliation>Salesforce</affiliation></author>
      <pages>31-41</pages>
      <abstract>We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks.</abstract>
      <url hash="53c67a31">2023.acl-demo.3</url>
      <bibkey>li-etal-2023-lavis</bibkey>
    </paper>
    <paper id="4">
      <title>Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases among Foundation Models</title>
      <author><first>Bum Chul</first><last>Kwon</last><affiliation>IBM Research</affiliation></author>
      <author><first>Nandana</first><last>Mihindukulasooriya</last><affiliation>IBM Research AI</affiliation></author>
      <pages>42-50</pages>
      <abstract>Pre-trained transformer-based language models are becoming increasingly popular due to their exceptional performance on various benchmarks. However, concerns persist regarding the presence of hidden biases within these models, which can lead to discriminatory outcomes and reinforce harmful stereotypes. To address this issue, we propose Finspector, a human-centered visual inspection tool designed to detect biases in different categories through log-likelihood scores generated by language models. The goal of the tool is to enable researchers to easily identify potential biases using visual analytics, ultimately contributing to a fairer and more just deployment of these models in both academic and industrial settings. Finspector is available at https://github.com/IBM/finspector.</abstract>
      <url hash="e8322338">2023.acl-demo.4</url>
      <bibkey>kwon-mihindukulasooriya-2023-finspector</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>P</fixed-case>rime<fixed-case>QA</fixed-case>: The Prime Repository for State-of-the-Art Multilingual Question Answering Research and Development</title>
      <author><first>Avi</first><last>Sil</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Jaydeep</first><last>Sen</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Bhavani</first><last>Iyer</last><affiliation>Ibm</affiliation></author>
      <author><first>Martin</first><last>Franz</last><affiliation>IBM T.J. Watson Research Center</affiliation></author>
      <author><first>Kshitij</first><last>Fadnis</last><affiliation>IBM Research</affiliation></author>
      <author><first>Mihaela</first><last>Bornea</last><affiliation>IBM Research</affiliation></author>
      <author><first>Sara</first><last>Rosenthal</last><affiliation>IBM Research</affiliation></author>
      <author><first>Scott</first><last>McCarley</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Rong</first><last>Zhang</last><affiliation>IBM.com</affiliation></author>
      <author><first>Vishwajeet</first><last>Kumar</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Yulong</first><last>Li</last><affiliation>IBM research</affiliation></author>
      <author><first>Md Arafat</first><last>Sultan</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Riyaz</first><last>Bhat</last><affiliation>Ibm Irl</affiliation></author>
      <author><first>Juergen</first><last>Bross</last><affiliation>IBM Research</affiliation></author>
      <author><first>Radu</first><last>Florian</last><affiliation>IBM Research</affiliation></author>
      <author><first>Salim</first><last>Roukos</last><affiliation>IBM Research AI</affiliation></author>
      <pages>51-62</pages>
      <abstract>The field of Question Answering (QA) has made remarkable progress in recent years, thanks to the advent of large pre-trained language models, newer realistic benchmark datasets with leaderboards, and novel algorithms for key components such as retrievers and readers. In this paper, we introduce PrimeQA: a one-stop and open-source QA repository with an aim to democratize QA research and facilitate easy replication of state-of-the-art (SOTA) QA methods. PrimeQA supports core QA functionalities like retrieval and reading comprehension as well as auxiliary capabilities such as question generation. It has been designed as an end-to-end toolkit for various use cases: building front-end applications, replicating SOTA methods on public benchmarks, and expanding pre-existing methods. PrimeQA is available at: https://github.com/primeqa.</abstract>
      <url hash="3eca73cb">2023.acl-demo.5</url>
      <bibkey>sil-etal-2023-primeqa</bibkey>
    </paper>
    <paper id="6">
      <title>Lingxi: A Diversity-aware <fixed-case>C</fixed-case>hinese Modern Poetry Generation System</title>
      <author><first>Xinran</first><last>Zhang</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jiafeng</first><last>Liu</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Xiaobing</first><last>Li</last><affiliation>Central Conservatory of Music</affiliation></author>
      <pages>63-75</pages>
      <abstract>Chinese modern poetry generation has been a challenging task. One issue is the Chinese word segmentation (CWS) which is critical to comprehend the Chinese language but was not always considered in common tokenization methods. Another is the decoding (sampling) method which may induce repetition and boredom and severely lower the diversity of the generated poetry. To address these issues, we present Lingxi, a diversity-aware Chinese modern poetry generation system. For the CWS issue, we propose a novel framework that incorporates CWS in the tokenization process. The proposed method can achieve a high vocabulary coverage rate with a reasonable vocabulary size. For the decoding method and the diversity issue, we propose a novel sampling algorithm that flattens the high likelihood part of the predicted distribution of the language model to emphasize the comparatively low-likelihood words and increase the diversity of generated poetry. Empirical results show that even when the top 60% of cumulative probability mass of the predicted distribution is flattened, our method achieves comparable or even better performance than baseline sampling methods. Our system is available at http://lingxi.website.</abstract>
      <url hash="f8e6b332">2023.acl-demo.6</url>
      <bibkey>zhang-etal-2023-lingxi</bibkey>
    </paper>
    <paper id="7">
      <title>Autodive: An Integrated Onsite Scientific Literature Annotation Tool</title>
      <author><first>Yi</first><last>Du</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ludi</first><last>Wang</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <author><first>Mengyi</first><last>Huang</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <author><first>Dongze</first><last>Song</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wenjuan</first><last>Cui</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuanchun</first><last>Zhou</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <pages>76-85</pages>
      <abstract>Scientific literature is always available in Adobe’s Portable Document Format (PDF), which is friendly for scientists to read. Compared with raw text, annotating directly on PDF documents can greatly improve the labeling efficiency of scientists whose annotation costs are very high. In this paper, we present Autodive, an integrated onsite scientific literature annotation tool for natural scientists and Natural Language Processing (NLP) researchers. This tool provides six core functions of annotation that support the whole lifecycle of corpus generation including i)annotation project management, ii)resource management, iii)ontology management, iv)manual annotation, v)onsite auto annotation, and vi)annotation task statistic. Two experiments are carried out to verify efficiency of the presented tool. A live demo of Autodive is available at http://autodive.sciwiki.cn. The source code is available at https://github.com/Autodive.</abstract>
      <url hash="8ab7c2d9">2023.acl-demo.7</url>
      <bibkey>du-etal-2023-autodive</bibkey>
    </paper>
    <paper id="8">
      <title>A Practical Toolkit for Multilingual Question and Answer Generation</title>
      <author><first>Asahi</first><last>Ushio</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Fernando</first><last>Alva-Manchego</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Jose</first><last>Camacho-Collados</last><affiliation>Cardiff University</affiliation></author>
      <pages>86-94</pages>
      <abstract>Generating questions along with associated answers from a text has applications in several domains, such as creating reading comprehension tests for students, or improving document search by providing auxiliary questions and answers based on the query. Training models for question and answer generation (QAG) is not straightforward due to the expected structured output (i.e. a list of question and answer pairs), as it requires more than generating a single sentence. This results in a small number of publicly accessible QAG models. In this paper, we introduce AutoQG, an online service for multilingual QAG along with lmqg, an all-in-one python package for model fine-tuning, generation, and evaluation. We also release QAG models in eight languages fine-tuned on a few variants of pre-trained encoder-decoder language models, which can be used online via AutoQG or locally via lmqg. With these resources, practitioners of any level can benefit from a toolkit that includes a web interface for end users, and easy-to-use code for developers who require custom models or fine-grained controls for generation.</abstract>
      <url hash="d8a24244">2023.acl-demo.8</url>
      <bibkey>ushio-etal-2023-practical</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>O</fixed-case>pen<fixed-case>SLU</fixed-case>: A Unified, Modularized, and Extensible Toolkit for Spoken Language Understanding</title>
      <author><first>Libo</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Qiguang</first><last>Chen</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xiao</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yunlong</first><last>Feng</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>95-102</pages>
      <abstract>Spoken Language Understanding (SLU) is one of the core components of a task-oriented dialogue system, which aims to extract the semantic meaning of user queries (e.g., intents and slots). In this work, we introduce OpenSLU, an open-source toolkit to provide a unified, modularized, and extensible toolkit for spoken language understanding. Specifically, OpenSLU unifies 10 SLU models for both single-intent and multi-intent scenarios, which support both non-pretrained and pretrained models simultaneously. Additionally, OpenSLU is highly modularized and extensible by decomposing the model architecture, inference, and learning process into reusable modules, which allows researchers to quickly set up SLU experiments with highly flexible configurations. OpenSLU is implemented based on PyTorch, and released at https://github.com/LightChen233/OpenSLU.</abstract>
      <url hash="3149735d">2023.acl-demo.9</url>
      <bibkey>qin-etal-2023-openslu</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>S</fixed-case>anskrit<fixed-case>S</fixed-case>hala: A Neural <fixed-case>S</fixed-case>anskrit <fixed-case>NLP</fixed-case> Toolkit with Web-Based Interface for Pedagogical and Annotation Purposes</title>
      <author><first>Jivnesh</first><last>Sandhan</last><affiliation>IIT Kanpur</affiliation></author>
      <author><first>Anshul</first><last>Agarwal</last><affiliation>IIT Kanpur</affiliation></author>
      <author><first>Laxmidhar</first><last>Behera</last><affiliation>IIT Kanpur / Mandi</affiliation></author>
      <author><first>Tushar</first><last>Sandhan</last><affiliation>IIT Kanpur</affiliation></author>
      <author><first>Pawan</first><last>Goyal</last><affiliation>IIT Kharagpur</affiliation></author>
      <pages>103-112</pages>
      <abstract>We present a neural Sanskrit Natural Language Processing (NLP) toolkit named SanskritShala (a school of Sanskrit) to facilitate computational linguistic analyses for several tasks such as word segmentation, morphological tagging, dependency parsing, and compound type identification. Our systems currently report state-of-the-art performance on available benchmark datasets for all tasks. SanskritShala is deployed as a web-based application, which allows a user to get real-time analysis for the given input. It is built with easy-to-use interactive data annotation features that allow annotators to correct the system predictions when it makes mistakes. We publicly release the source codes of the 4 modules included in the toolkit, 7 word embedding models that have been trained on publicly available Sanskrit corpora and multiple annotated datasets such as word similarity, relatedness, categorization, analogy prediction to assess intrinsic properties of word embeddings. So far as we know, this is the first neural-based Sanskrit NLP toolkit that has a web-based interface and a number of NLP modules. We are sure that the people who are willing to work with Sanskrit will find it useful for pedagogical and annotative purposes. SanskritShala is available at: https://cnerg.iitkgp.ac.in/sanskritshala. The demo video of our platform can be accessed at: https://youtu.be/x0X31Y9k0mw4.</abstract>
      <url hash="383583db">2023.acl-demo.10</url>
      <bibkey>sandhan-etal-2023-sanskritshala</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>LIDA</fixed-case>: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models</title>
      <author><first>Victor</first><last>Dibia</last><affiliation>Microsoft Research</affiliation></author>
      <pages>113-126</pages>
      <abstract>Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) and image generation models (IGMs) are suitable to addressing these tasks. We present LIDA, a novel tool for generating grammar-agnostic visualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER that converts data into a rich but compact natural language summary, a GOAL EXPLORER that enumerates visualization goals given the data, a VISGENERATOR that generates, refines, executes and filters visualization code and an INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA provides a python api, and a hybrid user interface (direct manipulation and multilingual natural language) for interactive chart, infographics and data story generation. Code and demo are available at this url - https://microsoft.github.io/lida/</abstract>
      <url hash="779dfdb3">2023.acl-demo.11</url>
      <bibkey>dibia-2023-lida</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>M</fixed-case>eta<fixed-case>P</fixed-case>ro Online: A Computational Metaphor Processing Online System</title>
      <author><first>Rui</first><last>Mao</last><affiliation>Ruimao Tech</affiliation></author>
      <author><first>Xiao</first><last>Li</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Kai</first><last>He</last><affiliation>School of Electronic and Information Engineering, Xi’an Jiaotong University</affiliation></author>
      <author><first>Mengshi</first><last>Ge</last><affiliation>Nanyang Technology University</affiliation></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>127-135</pages>
      <abstract>Metaphoric expressions are a special linguistic phenomenon, frequently appearing in everyday language. Metaphors do not take their literal meanings in contexts, which may cause obstacles for language learners to understand them. Metaphoric expressions also reflect the cognition of humans via concept mappings, attracting great attention from cognitive science and psychology communities. Thus, we aim to develop a computational metaphor processing online system, termed MetaPro Online, that allows users without a coding background, e.g., language learners and linguists, to easily query metaphoricity labels, metaphor paraphrases, and concept mappings for non-domain-specific text. The outputs of MetaPro can be directly used by language learners and natural language processing downstream tasks because MetaPro is an end-to-end system.</abstract>
      <url hash="dbe22762">2023.acl-demo.12</url>
      <bibkey>mao-etal-2023-metapro</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>DIAGRAPH</fixed-case>: An Open-Source Graphic Interface for Dialog Flow Design</title>
      <author><first>Dirk</first><last>Väth</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Lindsey</first><last>Vanderlyn</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Ngoc Thang</first><last>Vu</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>136-143</pages>
      <abstract>In this work, we present DIAGRAPH, an open-source graphical dialog flow editor built on the ADVISER toolkit. Our goal for this tool is threefold: 1) To support subject-experts to intuitively create complex and flexible dialog systems,2) To support rapid prototyping of dialog system behavior, e.g., for research, and 3) To provide a hands-on test bed for students learning about dialog systems.To facilitate this, DIAGRAPH aims to provide a clean and intuitive graphical interface for creating dialog systems without requiring any coding knowledge.Once a dialog graph has been created, it is automatically turned into a dialog system using state of the art language models. This allows for rapid prototyping and testing.Dialog designers can then distribute a link to their finished dialog system or embed it into a website.Additionally, to support scientific experiments and data collection, dialog designers can access chat logs. Finally, to verify the usability of DIAGRAPH, we performed evaluation with subject-experts who extensively worked with the tool and users testing it for the first time, receiving above average System Usability Scale (SUS) scores from both (82 out 100 and 75 out of 100, respectively).In this way, we hope DIAGRAPH helps reduce the barrier to entry for creating dialog interactions.</abstract>
      <url hash="2a38585d">2023.acl-demo.13</url>
      <bibkey>vath-etal-2023-diagraph</bibkey>
    </paper>
    <paper id="14">
      <title>disco: a toolkit for Distributional Control of Generative Models</title>
      <author><first>Germán</first><last>Kruszewski</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>Jos</first><last>Rozen</last><affiliation>NAVER LABS Europe</affiliation></author>
      <author><first>Marc</first><last>Dymetman</last><affiliation>Independent researcher</affiliation></author>
      <pages>144-160</pages>
      <abstract>Pre-trained language models and other generative models have revolutionized NLP and beyond. However, these models tend to reproduce undesirable biases present in their training data. Also, they may overlook patterns that are important but challenging to capture. To address these limitations, researchers have introduced distributional control techniques. These techniques, not limited to language, allow controlling the prevalence (i.e. expectations) of any features of interest in the model’s outputs. Despite their potential, the widespread adoption of these techniques has been hindered by the difficulty in adapting the complex, disconnected code. Here, we present disco, an open-source Python library that brings these techniques to the broader public</abstract>
      <url hash="9b3f1943">2023.acl-demo.14</url>
      <bibkey>kruszewski-etal-2023-disco</bibkey>
    </paper>
    <paper id="15">
      <title>A Hyperparameter Optimization Toolkit for Neural Machine Translation Research</title>
      <author><first>Xuan</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Kevin</first><last>Duh</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Paul</first><last>McNamee</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>161-168</pages>
      <abstract>Hyperparameter optimization is an important but often overlooked process in the research of deep learning technologies. To obtain a good model, one must carefully tune hyperparameters that determine the architecture and training algorithm. Insufficient tuning may result in poor results, while inequitable tuning may lead to exaggerated differences between models. We present a hyperparameter optimization toolkit for neural machine translation (NMT) to help researchers focus their time on the creative rather than the mundane. The toolkit is implemented as a wrapper on top of the open-source Sockeye NMT software. Using the Asynchronous Successive Halving Algorithm (ASHA), we demonstrate that it is possible to discover near-optimal models under a computational budget with little effort.Code: https://github.com/kevinduh/sockeye-recipes3Video demo: https://cs.jhu.edu/ kevinduh/j/demo.mp4</abstract>
      <url hash="55512c6d">2023.acl-demo.15</url>
      <bibkey>zhang-etal-2023-hyperparameter</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>J</fixed-case>apanese-to-<fixed-case>E</fixed-case>nglish Simultaneous Dubbing Prototype</title>
      <author><first>Xiaolin</first><last>Wang</last><affiliation>Nict</affiliation></author>
      <author><first>Masao</first><last>Utiyama</last><affiliation>Nict</affiliation></author>
      <author><first>Eiichiro</first><last>Sumita</last><affiliation>Nict</affiliation></author>
      <pages>169-178</pages>
      <abstract>Live video streaming has become an important form of communication such as virtual conferences. However, for cross-language communication in live video streaming, reading subtitles degrades the viewing experience. To address this problem, our simultaneous dubbing prototype translates and replaces the original speech of a live video stream in a simultaneous manner. Tests on a collection of 90 public videos show that our system achieves a low average latency of 11.90 seconds for smooth playback. Our method is general and can be extended to other language pairs.</abstract>
      <url hash="786216f1">2023.acl-demo.16</url>
      <bibkey>wang-etal-2023-japanese</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>V</fixed-case>is<fixed-case>K</fixed-case>o<fixed-case>P</fixed-case>: Visual Knowledge oriented Programming for Interactive Knowledge Base Question Answering</title>
      <author><first>Zijun</first><last>Yao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yuanyong</first><last>Chen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xin</first><last>Lv</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shulin</first><last>Cao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Amy</first><last>Xin</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jifan</first><last>Yu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Hailong</first><last>Jin</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jianjun</first><last>Xu</last><affiliation>beijing caizhi technology co.,LTD.</affiliation></author>
      <author><first>Peng</first><last>Zhang</last><affiliation>Zhipu.AI</affiliation></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <pages>179-189</pages>
      <abstract>We present Visual Knowledge oriented Programming platform (&lt;b&gt;VisKoP&lt;/b&gt;), a knowledge base question answering (KBQA) system that integrates human into the loop to edit and debug the knowledge base (KB) queries. VisKoP not only provides a neural program induction module, which converts natural language questions into knowledge oriented program language (KoPL), but also maps KoPL programs into graphical elements. KoPL programs can be edited with simple graphical operators, such as &lt;i&gt;”dragging”&lt;/i&gt; to add knowledge operators and &lt;i&gt;”slot filling”&lt;/i&gt; to designate operator arguments. Moreover, VisKoP provides auto-completion for its knowledge base schema and users can easily debug the KoPL program by checking its intermediate results. To facilitate the practical KBQA on a million-entity-level KB, we design a highly efficient KoPL execution engine for the back-end. Experiment results show that VisKoP is highly efficient and user interaction can fix a large portion of wrong KoPL programs to acquire the correct answer. The VisKoP online demo, highly efficient KoPL engine, and screencast video are now publicly available.</abstract>
      <url hash="06bf026d">2023.acl-demo.17</url>
      <bibkey>yao-etal-2023-viskop</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>PEEP</fixed-case>-Talk: A Situational Dialogue-based Chatbot for <fixed-case>E</fixed-case>nglish Education</title>
      <author><first>Seugnjun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Yoonna</first><last>Jang</last><affiliation>Department of Computer Science and Engineering, Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Upstage</affiliation></author>
      <author><first>Jungseob</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaehyung</first><last>Seo</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Sugyeong</first><last>Eo</last><affiliation>Korea University</affiliation></author>
      <author><first>Seounghoon</first><last>Lee</last><affiliation>Institute for Infocomm Research, A*STAR</affiliation></author>
      <author><first>Bernardo</first><last>Yahya</last><affiliation>Hankuk University of Foreign Studies</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>190-207</pages>
      <abstract>English is acknowledged worldwide as a mode of communication. However, due to the absence of realistic practicing scenarios, students learning English as a foreign language (EFL) typically have limited chances to converse and share feedback with others. In this paper, we propose PEEP-Talk, a real-world situational dialogue-based chatbot designed for English education. It also naturally switches to a new topic or situation in response to out-of-topic utterances, which are common among English beginners. Furthermore, PEEP-Talk provides feedback score on conversation and grammar error correction. We performed automatic and user evaluations to validate performance and education efficiency of our system. The results show that PEEP-Talk generates appropriate responses in various real-life situations while providing accurate feedback to learners. Moreover, we demonstrate a positive impact on English-speaking, grammar, and English learning anxiety, implying that PEEP-Talk can lower the barrier to learning natural conversation in effective ways.</abstract>
      <url hash="5d80598e">2023.acl-demo.18</url>
      <bibkey>lee-etal-2023-peep</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>O</fixed-case>pen<fixed-case>TIPE</fixed-case>: An Open-source Translation Framework for Interactive Post-Editing Research</title>
      <author><first>Fabian</first><last>Landwehr</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Thomas</first><last>Steinmann</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Laura</first><last>Mascarell</last><affiliation>ETH Zurich</affiliation></author>
      <pages>208-216</pages>
      <abstract>Despite the latest improvements on machine translation, professional translators still must review and post-edit the automatic output to ensure high-quality translations. The research on automating this process lacks an interactive post-editing environment implemented for this purpose; therefore, current approaches do not consider the human interactions that occur in real post-editing scenarios. To address this issue, we present OpenTIPE, a flexible and extensible framework that aims at supporting research on interactive post-editing. Specifically, the interactive environment of OpenTIPE allows researchers to explore human-centered approaches for the post-editing task. We release the OpenTIPE source code and showcase its main functionalities with a demonstration video and an online live demo.</abstract>
      <url hash="80c04c5a">2023.acl-demo.19</url>
      <bibkey>landwehr-etal-2023-opentipe</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>T</fixed-case>encent<fixed-case>P</fixed-case>retrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities</title>
      <author><first>Zhe</first><last>Zhao</last><affiliation>Tencent</affiliation></author>
      <author><first>Yudong</first><last>Li</last><affiliation>China University of Geosciences (Beijing)</affiliation></author>
      <author><first>Cheng</first><last>Hou</last><affiliation>Tencent</affiliation></author>
      <author><first>Jing</first><last>Zhao</last><affiliation>Tencent</affiliation></author>
      <author><first>Rong</first><last>Tian</last><affiliation>Tencent</affiliation></author>
      <author><first>Weijie</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Yiren</first><last>Chen</last><affiliation>Tencent</affiliation></author>
      <author><first>Ningyuan</first><last>Sun</last><affiliation>Tencent</affiliation></author>
      <author><first>Haoyan</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Weiquan</first><last>Mao</last><affiliation>Tencent</affiliation></author>
      <author><first>Han</first><last>Guo</last><affiliation>Tencent</affiliation></author>
      <author><first>Weigang</first><last>Gou</last><affiliation>Tencent</affiliation></author>
      <author><first>Taiqiang</first><last>Wu</last><affiliation>Tencent</affiliation></author>
      <author><first>Tao</first><last>Zhu</last><affiliation>Tencent</affiliation></author>
      <author><first>Wenhang</first><last>Shi</last><affiliation>RENMIN UNIVERSITY of CHINA</affiliation></author>
      <author><first>Chen</first><last>Chen</last><affiliation>Tencent</affiliation></author>
      <author><first>Shan</first><last>Huang</last><affiliation>Tencent</affiliation></author>
      <author><first>Sihong</first><last>Chen</last><affiliation>Tencent</affiliation></author>
      <author><first>Liqun</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Feifei</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Xiaoshuai</first><last>Chen</last><affiliation>Tencent</affiliation></author>
      <author><first>Xingwu</first><last>Sun</last><affiliation>Tencent</affiliation></author>
      <author><first>Zhanhui</first><last>Kang</last><affiliation>Tencent</affiliation></author>
      <author><first>Xiaoyong</first><last>Du</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Linlin</first><last>Shen</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Kimmo</first><last>Yan</last><affiliation>Tencent</affiliation></author>
      <pages>217-225</pages>
      <abstract>Recently, the success of pre-training in text domain has been fully extended to vision, audio, and cross-modal scenarios. The proposed pre-training models of different modalities are showing a rising trend of homogeneity in their model structures, which brings the opportunity to implement different pre-training models within a uniform framework. In this paper, we present TencentPretrain, a toolkit supporting pre-training models of different modalities. The core feature of TencentPretrain is the modular design. The toolkit uniformly divides pre-training models into 5 components: embedding, encoder, target embedding, decoder, and target. As almost all of common modules are provided in each component, users can choose the desired modules from different components to build a complete pre-training model. The modular design enables users to efficiently reproduce existing pre-training models or build brand-new one. We test the toolkit on text, vision, and audio benchmarks and show that it can match the performance of the original implementations.</abstract>
      <url hash="61ba5b4c">2023.acl-demo.20</url>
      <bibkey>zhao-etal-2023-tencentpretrain</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>N</fixed-case>euro<fixed-case>X</fixed-case> Library for Neuron Analysis of Deep <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Fahim</first><last>Dalvi</last><affiliation>Qatar Computing Research Institute, HBKU</affiliation></author>
      <author><first>Hassan</first><last>Sajjad</last><affiliation>Dalhousie University</affiliation></author>
      <author><first>Nadir</first><last>Durrani</last><affiliation>Qcri</affiliation></author>
      <pages>226-234</pages>
      <abstract>Neuron analysis provides insights into how knowledge is structured in representations and discovers the role of neurons in the network. In addition to developing an understanding of our models, neuron analysis enables various applications such as debiasing, domain adaptation and architectural search. We present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis of natural language processing models. It implements various interpretation methods under a unified API, and provides a framework for data processing and evaluation, thus making it easier for researchers and practitioners to perform neuron analysis. The Python toolkit is available at https://www.github.com/fdalvi/NeuroX.Demo Video available at: https://youtu.be/mLhs2YMx4u8</abstract>
      <url hash="18071e2e">2023.acl-demo.21</url>
      <bibkey>dalvi-etal-2023-neurox</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>S</fixed-case>ci<fixed-case>L</fixed-case>it: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation</title>
      <author><first>Nianlong</first><last>Gu</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Richard H.R.</first><last>Hahnloser</last><affiliation>ETH Zurich</affiliation></author>
      <pages>235-246</pages>
      <abstract>Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes. Although in many workflows these processes are serially linked, there are opportunities for natural language processing (NLP) to provide end-to-end assistive tools. We propose SciLit, a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and keywords. SciLit efficiently recommends papers from large databases of hundreds of millions of papers using a two-stage pre-fetching and re-ranking literature search system that flexibly deals with addition and removal of a paper database. We provide a convenient user interface that displays the recommended papers as extractive summaries and that offers abstractively-generated citing sentences which are aligned with the provided context and which mention the chosen keyword(s). Our assistive tool for literature discovery and scientific writing is available at https://scilit.vercel.app</abstract>
      <url hash="a3232d6e">2023.acl-demo.22</url>
      <bibkey>gu-hahnloser-2023-scilit</bibkey>
    </paper>
    <paper id="23">
      <title>Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search</title>
      <author><first>Chris</first><last>Jenkins</last><affiliation>University of Southern California Information Sciences Institute</affiliation></author>
      <author><first>Shantanu</first><last>Agarwal</last><affiliation>Information Sciences Institute, University of Southern California</affiliation></author>
      <author><first>Joel</first><last>Barry</last><affiliation>University of Southern California Information Sciences Institute</affiliation></author>
      <author><first>Steven</first><last>Fincke</last><affiliation>University of Southern California Information Sciences Institute</affiliation></author>
      <author><first>Elizabeth</first><last>Boschee</last><affiliation>Information Sciences Institute</affiliation></author>
      <pages>247-256</pages>
      <abstract>In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual, zero-shot event extraction system and accompanying user interface for event visualization &amp; search. Using only English training data, ISI-Clear makes global events available on-demand, processing user-supplied text in 100 languages ranging from Afrikaans to Yiddish. We provide multiple event-centric views of extracted events, including both a graphical representation and a document-level summary. We also integrate existing cross-lingual search algorithms with event extraction capabilities to provide cross-lingual event-centric search, allowing English-speaking users to search over events automatically extracted from a corpus of non-English documents, using either English natural language queries (e.g. “cholera outbreaks in Iran”) or structured queries (e.g. find all events of type Disease-Outbreak with agent “cholera” and location “Iran”).</abstract>
      <url hash="96533dd8">2023.acl-demo.23</url>
      <bibkey>jenkins-etal-2023-massively</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>YANMTT</fixed-case>: Yet Another Neural Machine Translation Toolkit</title>
      <author><first>Raj</first><last>Dabre</last><affiliation>Nict</affiliation></author>
      <author><first>Diptesh</first><last>Kanojia</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Chinmay</first><last>Sawant</last><affiliation>Surrey University</affiliation></author>
      <author><first>Eiichiro</first><last>Sumita</last><affiliation>Nict</affiliation></author>
      <pages>257-263</pages>
      <abstract>In this paper, we present our open-source neural machine translation (NMT) toolkit called “Yet Another Neural Machine Translation Toolkit” abbreviated as YANMTT - https://github.com/prajdabre/yanmtt, which is built on top of the HuggingFace Transformers library. YANMTT focuses on transfer learning and enables easy pre-training and fine-tuning of sequence-to-sequence models at scale. It can be used for training parameter-heavy models with minimal parameter sharing and efficient, lightweight models via heavy parameter sharing. Additionally, it supports parameter-efficient fine-tuning (PEFT) through adapters and prompts. Our toolkit also comes with a user interface that can be used to demonstrate these models and visualize various parts of the model. Apart from these core features, our toolkit also provides other advanced functionalities such as but not limited to document/multi-source NMT, simultaneous NMT, mixtures-of-experts, model compression and continual learning.</abstract>
      <url hash="dd54c284">2023.acl-demo.24</url>
      <bibkey>dabre-etal-2023-yanmtt</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>XMD</fixed-case>: An End-to-End Framework for Interactive Explanation-Based Debugging of <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Dong-Ho</first><last>Lee</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Akshen</first><last>Kadakia</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Brihi</first><last>Joshi</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Aaron</first><last>Chan</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Ziyi</first><last>Liu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Kiran</first><last>Narahari</last><affiliation>Information Sciences Institute</affiliation></author>
      <author><first>Takashi</first><last>Shibuya</last><affiliation>Sony Corporation</affiliation></author>
      <author><first>Ryosuke</first><last>Mitani</last><affiliation>Sony Group</affiliation></author>
      <author><first>Toshiyuki</first><last>Sekiya</last><affiliation>Sony Group Corporation</affiliation></author>
      <author><first>Jay</first><last>Pujara</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xiang</first><last>Ren</last><affiliation>University of Southern California</affiliation></author>
      <pages>264-273</pages>
      <abstract>NLP models are susceptible to learning spurious biases (i.e., bugs) that work on some datasets but do not properly reflect the underlying task. Explanation-based model debugging aims to resolve spurious biases by showing human users explanations of model behavior, asking users to give feedback on the behavior, thenusing the feedback to update the model. While existing model debugging methods have shown promise, their prototype-level implementations provide limited practical utility. Thus, we propose XMD: the first open-source, end-to-end framework for explanation-based model debugging. Given task- or instance-level explanations,users can flexibly provide various forms of feedback via an intuitive, web-based UI. After receiving user feedback, XMD automatically updates the model in real time, by regularizing the model so that its explanationsalign with the user feedback. The new model can then be easily deployed into real-world applications via Hugging Face. Using XMD, we can improve the model’s OOD performance on text classification tasks by up to 18%.</abstract>
      <url hash="cc4a4e56">2023.acl-demo.25</url>
      <bibkey>lee-etal-2023-xmd</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>O</fixed-case>pen<fixed-case>D</fixed-case>elta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models</title>
      <author><first>Shengding</first><last>Hu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ning</first><last>Ding</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Weilin</first><last>Zhao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xingtai</first><last>Lv</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhen</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>274-281</pages>
      <abstract>The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as “delta tuning” in Ding et al. (2022), which updates only a small subset of parameters, known as “delta modules”, while keeping the backbone model’s parameters fixed. However, the practicality and flexibility of delta tuning have been limited due to existing implementations that directly modify the code of the backbone PTMs and hard-code specific delta tuning methods for each PTM. In this paper, we present OpenDelta, an open-source library that overcomes these limitations by providing a plug-and-play implementation of various delta tuning methods. Our novel techniques eliminate the need to modify the backbone PTMs’ code, making OpenDelta compatible with different, even novel PTMs. OpenDelta is designed to be simple, modular, and extensible, providing a comprehensive platform for researchers and practitioners to adapt large PTMs efficiently.</abstract>
      <url hash="38099019">2023.acl-demo.26</url>
      <bibkey>hu-etal-2023-opendelta</bibkey>
    </paper>
    <paper id="27">
      <title>Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation</title>
      <author><first>Itay</first><last>Yair</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Hillel</first><last>Taub-Tabib</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yoav</first><last>Goldberg</last><affiliation>Bar Ilan University</affiliation></author>
      <pages>282-290</pages>
      <abstract>Information extraction systems often producehundreds to thousands of strings on a specifictopic. We present a method that facilitatesbetter consumption of these strings, in an ex-ploratory setting in which a user wants to bothget a broad overview of what’s available, and achance to dive deeper on some aspects. The sys-tem works by grouping similar items together,and arranging the remaining items into a hierar-chical navigable DAG structure. We apply themethod to medical information extraction.</abstract>
      <url hash="19921ae2">2023.acl-demo.27</url>
      <bibkey>yair-etal-2023-hierarchy</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>CARE</fixed-case>: Collaborative <fixed-case>AI</fixed-case>-Assisted Reading Environment</title>
      <author><first>Dennis</first><last>Zyska</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <author><first>Nils</first><last>Dycke</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <author><first>Jan</first><last>Buchmann</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <author><first>Ilia</first><last>Kuznetsov</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <pages>291-303</pages>
      <abstract>Recent years have seen impressive progress in AI-assisted writing, yet the developments in AI-assisted reading are lacking. We propose inline commentary as a natural vehicle for AI-based reading assistance, and present CARE: the first open integrated platform for the study of inline commentary and reading. CARE facilitates data collection for inline commentaries in a commonplace collaborative reading environment, and provides a framework for enhancing reading with NLP-based assistance, such as text classification, generation or question answering. The extensible behavioral logging allows unique insights into the reading and commenting behavior, and flexible configuration makes the platform easy to deploy in new scenarios. To evaluate CARE in action, we apply the platform in a user study dedicated to scholarly peer review. CARE facilitates the data collection and study of inline commentary in NLP, extrinsic evaluation of NLP assistance, and application prototyping. We invite the community to explore and build upon the open source implementation of CARE.Github Repository: https://github.com/UKPLab/CAREPublic Live Demo: https://care.ukp.informatik.tu-darmstadt.de</abstract>
      <url hash="2632e07d">2023.acl-demo.28</url>
      <bibkey>zyska-etal-2023-care</bibkey>
    </paper>
    <paper id="29">
      <title>The <fixed-case>ROOTS</fixed-case> Search Tool: Data Transparency for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Aleksandra</first><last>Piktus</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Christopher</first><last>Akiki</last><affiliation>Leipzig University</affiliation></author>
      <author><first>Paulo</first><last>Villegas</last><affiliation>Telefonica I+D</affiliation></author>
      <author><first>Hugo</first><last>Laurençon</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Gérard</first><last>Dupont</last><affiliation>Mavenoid</affiliation></author>
      <author><first>Sasha</first><last>Luccioni</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Yacine</first><last>Jernite</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Anna</first><last>Rogers</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>304-314</pages>
      <abstract>ROOTS is a 1.6TB multilingual text corpus developed for the training of BLOOM, currently the largest language model explicitly accompanied by commensurate data governance efforts. In continuation of these efforts, we present the ROOTS Search Tool: a search engine over the entire ROOTS corpus offering both fuzzy and exact search capabilities. ROOTS is the largest corpus to date that can be investigated this way. The ROOTS Search Tool is open-sourced and available on Hugging Face Spaces: https://huggingface.co/spaces/bigscience-data/roots-search. We describe our implementation and the possible use cases of our tool.</abstract>
      <url hash="e3477782">2023.acl-demo.29</url>
      <bibkey>piktus-etal-2023-roots</bibkey>
    </paper>
    <paper id="30">
      <title>The <fixed-case>OPUS</fixed-case>-<fixed-case>MT</fixed-case> Dashboard – A Toolkit for a Systematic Evaluation of Open Machine Translation Models</title>
      <author><first>Jörg</first><last>Tiedemann</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Ona</first><last>de Gibert</last><affiliation>University of Helsinki</affiliation></author>
      <pages>315-327</pages>
      <abstract>The OPUS-MT dashboard is a web-based platform that provides a comprehensive overview of open translation models. We focus on a systematic collection of benchmark results with verifiable translation performance and large coverage in terms of languages and domains. We provide results for in-house OPUS-MT and Tatoeba models as well as external models from the Huggingface repository and user-contributed translations. The functionalities of the evaluation tool include summaries of benchmarks for over 2,300 models covering 4,560 language directions and 294 languages, as well as the inspection of predicted translations against their human reference. We focus on centralization, reproducibility and coverage of MT evaluation combined with scalability. The dashboard can be accessed live at https://opus.nlpl.eu/dashboard/.</abstract>
      <url hash="caebd787">2023.acl-demo.30</url>
      <bibkey>tiedemann-de-gibert-2023-opus</bibkey>
    </paper>
    <paper id="31">
      <title>The <fixed-case>D</fixed-case>-<fixed-case>WISE</fixed-case> Tool Suite: Multi-Modal Machine-Learning-Powered Tools Supporting and Enhancing Digital Discourse Analysis</title>
      <author><first>Florian</first><last>Schneider</last><affiliation>University of Hamburg</affiliation></author>
      <author><first>Tim</first><last>Fischer</last><affiliation>University of Hamburg</affiliation></author>
      <author><first>Fynn</first><last>Petersen-Frey</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Isabel</first><last>Eiser</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Gertraud</first><last>Koch</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>328-335</pages>
      <abstract>This work introduces the D-WISE Tool Suite (DWTS), a novel working environment for digital qualitative discourse analysis in the Digital Humanities (DH). The DWTS addresses limitations of current DH tools induced by the ever-increasing amount of heterogeneous, unstructured, and multi-modal data in which the discourses of contemporary societies are encoded. To provide meaningful insights from such data, our system leverages and combines state-of-the-art machine learning technologies from Natural Language Processing and Com-puter Vision. Further, the DWTS is conceived and developed by an interdisciplinary team ofcultural anthropologists and computer scientists to ensure the tool’s usability for modernDH research. Central features of the DWTS are: a) import of multi-modal data like text, image, audio, and video b) preprocessing pipelines for automatic annotations c) lexical and semantic search of documents d) manual span, bounding box, time-span, and frame annotations e) documentation of the research process.</abstract>
      <url hash="ac4609f8">2023.acl-demo.31</url>
      <bibkey>schneider-etal-2023-wise</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>O</fixed-case>pen<fixed-case>RT</fixed-case>: An Open-source Framework for Reasoning Over Tabular Data</title>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Boyu</first><last>Mi</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhenting</first><last>Qi</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Linyong</first><last>Nan</last><affiliation>Yale University</affiliation></author>
      <author><first>Minghao</first><last>Guo</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Dragomir</first><last>Radev</last><affiliation>Yale University</affiliation></author>
      <pages>336-347</pages>
      <abstract>There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing table pre-training models for performance comparison and develop new models quickly. We implemented and compared six table pre-training models on four question answering, one fact checking, and one faithful text generation datasets. Moreover, to enable the community to easily construct new table reasoning datasets, we developed TaRAT, an annotation tool which supports multi-person collaborative annotations for various kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed dataset to OpenRT and compare the performances of different baseline systems.</abstract>
      <url hash="d025d3dd">2023.acl-demo.32</url>
      <bibkey>zhao-etal-2023-openrt</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>UINAUIL</fixed-case>: A Unified Benchmark for <fixed-case>I</fixed-case>talian Natural Language Understanding</title>
      <author><first>Valerio</first><last>Basile</last><affiliation>University of Turin</affiliation></author>
      <author><first>Livio</first><last>Bioglio</last><affiliation>University of Turin</affiliation></author>
      <author><first>Alessio</first><last>Bosca</last><affiliation>CELI s.r.l.</affiliation></author>
      <author><first>Cristina</first><last>Bosco</last><affiliation>Dipartimento di Informatica - Università di Torino</affiliation></author>
      <author><first>Viviana</first><last>Patti</last><affiliation>University of Turin, Dipartimento di Informatica</affiliation></author>
      <pages>348-356</pages>
      <abstract>This paper introduces the Unified Interactive Natural Understanding of the Italian Language (UINAUIL), a benchmark of six tasks for Italian Natural Language Understanding. We present a description of the tasks and software library that collects the data from the European Language Grid, harmonizes the data format, and exposes functionalities to facilitates data manipulation and the evaluation of custom models. We also present the results of tests conducted with available Italian and multilingual language models on UINAUIL, providing an updated picture of the current state of the art in Italian NLU.</abstract>
      <url hash="ab30cb9d">2023.acl-demo.33</url>
      <bibkey>basile-etal-2023-uinauil</bibkey>
    </paper>
    <paper id="34">
      <title>Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction</title>
      <author><first>Gabriele</first><last>Picco</last><affiliation>IBM Research Europe</affiliation></author>
      <author><first>Marcos</first><last>Martinez Galindo</last><affiliation>IBM Research Europe</affiliation></author>
      <author><first>Alberto</first><last>Purpura</last><affiliation>IBM Research Europe</affiliation></author>
      <author><first>Leopold</first><last>Fuchs</last><affiliation>IBM Research Europe</affiliation></author>
      <author><first>Vanessa</first><last>Lopez</last><affiliation>IBM Research Europe</affiliation></author>
      <author><first>Thanh Lam</first><last>Hoang</last><affiliation>IBM Research Europe</affiliation></author>
      <pages>357-368</pages>
      <abstract>The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readily available APIs for production under the standard SpaCy NLP pipeline. Our API is extendible and evaluable, moreover, we include numerous enhancements such as boosting the accuracy with pipeline ensembling and visualization utilities available as a SpaCy extension.</abstract>
      <url hash="fe4b1f2f">2023.acl-demo.34</url>
      <bibkey>picco-etal-2023-zshot</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>B</fixed-case>i<fixed-case>S</fixed-case>ync: A Bilingual Editor for Synchronized Monolingual Texts</title>
      <author><first>Josep</first><last>Crego</last><affiliation>Systran</affiliation></author>
      <author><first>Jitao</first><last>Xu</last><affiliation>NetEase YouDao</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>ISIR CNRS &amp; Sorbonne Université</affiliation></author>
      <pages>369-376</pages>
      <abstract>In our globalized world, a growing number of situations arise where people are required to communicate in one or several foreign languages. In the case of written communication, users with a good command of a foreign language may find assistance from computer-aided translation (CAT) technologies. These technologies often allow users to access external resources, such as dictionaries, terminologies or bilingual concordancers, thereby interrupting and considerably hindering the writing process. In addition, CAT systems assume that the source sentence is fixed and also restrict the possible changes on the target side. In order to make the writing process smoother, we present BiSync, a bilingual writing assistant that allows users to freely compose text in two languages, while maintaining the two monolingual texts synchronized. We also include additional functionalities, such as the display of alternative prefix translations and paraphrases, which are intended to facilitate the authoring of texts. We detail the model architecture used for synchronization and evaluate the resulting tool, showing that high accuracy can be attained with limited computational resources. The interface and models are publicly available at https://github.com/jmcrego/BiSync and a demonstration video can be watched on YouTube https://youtu.be/_l-ugDHfNgU.</abstract>
      <url hash="97c8aa27">2023.acl-demo.35</url>
      <bibkey>crego-etal-2023-bisync</bibkey>
    </paper>
    <paper id="36">
      <title>Riveter: Measuring Power and Social Dynamics Between Entities</title>
      <author><first>Maria</first><last>Antoniak</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Anjalie</first><last>Field</last><affiliation>Stanford University</affiliation></author>
      <author><first>Jimin</first><last>Mun</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Melanie</first><last>Walsh</last><affiliation>University of Washington</affiliation></author>
      <author><first>Lauren</first><last>Klein</last><affiliation>Emory University</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>377-388</pages>
      <abstract>Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research.</abstract>
      <url hash="44e99965">2023.acl-demo.36</url>
      <bibkey>antoniak-etal-2023-riveter</bibkey>
    </paper>
    <paper id="37">
      <title>Fast Whitespace Correction with Encoder-Only Transformers</title>
      <author><first>Hannah</first><last>Bast</last><affiliation>University of Freiburg</affiliation></author>
      <author><first>Matthias</first><last>Hertel</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Sebastian</first><last>Walter</last><affiliation>University of Freiburg</affiliation></author>
      <pages>389-399</pages>
      <abstract>The goal of whitespace correction is to fix space errors in arbitrary given text. For example, given the text “whi te space correctio nwithTransf or mers”, produce “whitespace correction with Transformers”. We compare two Transformer-based models, a character-level encoder-decoder model and a byte-level encoder-only model. We find that the encoder-only model is both faster and achieves higher quality. We provide an easy-to-use tool that is over 900 times faster than the previous best tool, with the same high quality. Our tool repairs text at a rate of over 200 kB/s on GPU, with a sequence-averaged F1-score ranging from 87.5% for hard-to-correct text up to 99% for text without any spaces.</abstract>
      <url hash="670acc21">2023.acl-demo.37</url>
      <bibkey>bast-etal-2023-fast</bibkey>
    </paper>
    <paper id="38">
      <title><fixed-case>ESP</fixed-case>net-<fixed-case>ST</fixed-case>-v2: Multipurpose Spoken Language Translation Toolkit</title>
      <author><first>Brian</first><last>Yan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jiatong</first><last>Shi</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yun</first><last>Tang</last><affiliation>Facebook</affiliation></author>
      <author><first>Hirofumi</first><last>Inaguma</last><affiliation>Meta AI</affiliation></author>
      <author><first>Yifan</first><last>Peng</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Siddharth</first><last>Dalmia</last><affiliation>Google</affiliation></author>
      <author><first>Peter</first><last>Polak</last><affiliation>Charles University, MFF UFAL</affiliation></author>
      <author><first>Patrick</first><last>Fernandes</last><affiliation>Carnegie Mellon University, Instituto de Telecomunicações</affiliation></author>
      <author><first>Dan</first><last>Berrebbi</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Tomoki</first><last>Hayashi</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Xiaohui</first><last>Zhang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Zhaoheng</first><last>Ni</last><affiliation>Meta AI</affiliation></author>
      <author><first>Moto</first><last>Hira</last><affiliation>Meta AI</affiliation></author>
      <author><first>Soumi</first><last>Maiti</last><affiliation>ML researcher</affiliation></author>
      <author><first>Juan</first><last>Pino</last><affiliation>Facebook</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>400-411</pages>
      <abstract>ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.</abstract>
      <url hash="0780987f">2023.acl-demo.38</url>
      <bibkey>yan-etal-2023-espnet</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>CB</fixed-case>2: Collaborative Natural Language Interaction Research Platform</title>
      <author><first>Jacob</first><last>Sharf</last><affiliation>Cornell Tech</affiliation></author>
      <author><first>Mustafa Omer</first><last>Gul</last><affiliation>Cornell University</affiliation></author>
      <author><first>Yoav</first><last>Artzi</last><affiliation>Cornell University</affiliation></author>
      <pages>412-420</pages>
      <abstract>CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.</abstract>
      <url hash="00eb7169">2023.acl-demo.39</url>
      <bibkey>sharf-etal-2023-cb2</bibkey>
    </paper>
    <paper id="40">
      <title>Inseq: An Interpretability Toolkit for Sequence Generation Models</title>
      <author><first>Gabriele</first><last>Sarti</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Nils</first><last>Feldhus</last><affiliation>German Research Center for Artificial Intelligence (DFKI)</affiliation></author>
      <author><first>Ludwig</first><last>Sickert</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Oskar</first><last>van der Wal</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>421-435</pages>
      <abstract>Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models’ internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.</abstract>
      <url hash="e25605b3">2023.acl-demo.40</url>
      <bibkey>sarti-etal-2023-inseq</bibkey>
    </paper>
    <paper id="41">
      <title>Pipeline for modeling causal beliefs from natural language</title>
      <author><first>John</first><last>Priniski</last><affiliation>Ucla</affiliation></author>
      <author><first>Ishaan</first><last>Verma</last><affiliation>Usc</affiliation></author>
      <author><first>Fred</first><last>Morstatter</last><affiliation>Usc</affiliation></author>
      <pages>436-443</pages>
      <abstract>We present a causal language analysis pipeline that leverages a Large Language Model to identify causal claims made in natural language documents, and aggregates claims across a corpus to produce a causal claim network. The pipeline then applies a clustering algorithm that groups causal claims based on their semantic topics. We demonstrate the pipeline by modeling causal belief systems surrounding the Covid-19 vaccine from tweets.</abstract>
      <url hash="13a93472">2023.acl-demo.41</url>
      <bibkey>priniski-etal-2023-pipeline</bibkey>
    </paper>
    <paper id="42">
      <title><fixed-case>T</fixed-case>ab<fixed-case>G</fixed-case>enie: A Toolkit for Table-to-Text Generation</title>
      <author><first>Zdeněk</first><last>Kasner</last><affiliation>Charles University</affiliation></author>
      <author><first>Ekaterina</first><last>Garanina</last><affiliation>University of Groningen / Charles University</affiliation></author>
      <author><first>Ondrej</first><last>Platek</last><affiliation>Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics</affiliation></author>
      <author><first>Ondrej</first><last>Dusek</last><affiliation>Charles University</affiliation></author>
      <pages>444-455</pages>
      <abstract>Heterogenity of data-to-text generation datasets limits the research on data-to-text generation systems. We present TabGenie – a toolkit which enables researchers to explore, preprocess, and analyze a variety of data-to-text generation datasets through the unified framework of table-to-text generation. In TabGenie, all inputs are represented as tables with associated metadata. The tables can be explored through a web interface, which also provides an interactive mode for debugging table-to-text generation, facilitates side-by-side comparison of generated system outputs, and allows easy exports for manual analysis. Furthermore, TabGenie is equipped with command line processing tools and Python bindings for unified dataset loading and processing. We release TabGenie as a PyPI package and provide its open-source code and a live demo at https://github.com/kasnerz/tabgenie.</abstract>
      <url hash="c89f234e">2023.acl-demo.42</url>
      <bibkey>kasner-etal-2023-tabgenie</bibkey>
    </paper>
    <paper id="43">
      <title>An Efficient Conversational Smart Compose System</title>
      <author><first>Yun</first><last>Zhu</last><affiliation>Google</affiliation></author>
      <author><first>Xiayu</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Lei</first><last>Shu</last><affiliation>Google</affiliation></author>
      <author><first>Bowen</first><last>Tan</last><affiliation>Google</affiliation></author>
      <author><first>Xinying</first><last>Song</last><affiliation>Google</affiliation></author>
      <author><first>Lijuan</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>Maria</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Jindong</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Ning</first><last>Ruan</last><affiliation>Google</affiliation></author>
      <pages>456-462</pages>
      <abstract>Online conversation is a ubiquitous way to share information and connect everyone but repetitive idiomatic text typing takes users a lot of time.This paper demonstrates a simple yet effective cloud based smart compose system to improve human-to-human conversation efficiency. Heuristics from different perspectives are designed to achieve the best trade-off between quality and latency.From the modeling side, the decoder-only model exploited the previous turns of conversational history in a computation lightweight manner. Besides, a novel phrase tokenizer is proposed to reduce latency without losing the composing quality further. Additionally, the caching mechanism is applied to the serving framework. The demo video of the system is available at https://youtu.be/U1KXkaqr60g.We open-sourced our phrase tokenizer in https://github.com/tensorflow/text.</abstract>
      <url hash="975cfbd7">2023.acl-demo.43</url>
      <bibkey>zhu-etal-2023-efficient</bibkey>
    </paper>
    <paper id="44">
      <title>Which Spurious Correlations Impact Reasoning in <fixed-case>NLI</fixed-case> Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals</title>
      <author><first>Robin</first><last>Chan</last><affiliation>ETH Zürich</affiliation></author>
      <author><first>Afra</first><last>Amini</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Mennatallah</first><last>El-Assady</last><affiliation>University of Konstanz</affiliation></author>
      <pages>463-470</pages>
      <abstract>We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback.Through our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories: Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying training data and assessing NLI models’ robustness by creating adversarial test suites.</abstract>
      <url hash="f45d48e5">2023.acl-demo.44</url>
      <bibkey>chan-etal-2023-spurious</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>L</fixed-case>a<fixed-case>T</fixed-case>e<fixed-case>X</fixed-case>2<fixed-case>S</fixed-case>olver: a Hierarchical Semantic Parsing of <fixed-case>L</fixed-case>a<fixed-case>T</fixed-case>e<fixed-case>X</fixed-case> Document into Code for an Assistive Optimization Modeling Application</title>
      <author><first>Rindra</first><last>Ramamonjison</last><affiliation>Huawei Technologies Canada</affiliation></author>
      <author><first>Timothy</first><last>Yu</last><affiliation>Huawei Technologies Canada</affiliation></author>
      <author><first>Linzi</first><last>Xing</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Mahdi</first><last>Mostajabdaveh</last><affiliation>Huawei Technologies Canada</affiliation></author>
      <author><first>Xiaorui</first><last>Li</last><affiliation>Huawei Technologies Canada Co., Ltd</affiliation></author>
      <author><first>Xiaojin</first><last>Fu</last><affiliation>Huawei Noah’s Ark Lab - Hong Kong</affiliation></author>
      <author><first>Xiongwei</first><last>Han</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Yuanzhe</first><last>Chen</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Ren</first><last>Li</last><affiliation>Huawei UCD Center</affiliation></author>
      <author><first>Kun</first><last>Mao</last><affiliation>Huawei Cloud Computing Technologies</affiliation></author>
      <author><first>Yong</first><last>Zhang</last><affiliation>Huawei Technologies Canada Co., Ltd</affiliation></author>
      <pages>471-478</pages>
      <abstract>We demonstrate an interactive system to help operations research (OR) practitioners convert the mathematical formulation of optimization problems from TeX document format into the solver modeling language. In practice, a manual translation is cumbersome and time-consuming. Moreover, it requires an in-depth understanding of the problem description and a technical expertise to produce the modeling code. Thus, our proposed system TeX2Solver helps partially automate this conversion and help the users build optimization models more efficiently. In this paper, we describe its interface and the components of the hierarchical parsing system. A video demo walk-through is available online at http://bit.ly/3kuOm3x</abstract>
      <url hash="b0779e54">2023.acl-demo.45</url>
      <bibkey>ramamonjison-etal-2023-latex2solver</bibkey>
    </paper>
    <paper id="46">
      <title>Alfred: A System for Prompted Weak Supervision</title>
      <author><first>Peilin</first><last>Yu</last><affiliation>Brown University</affiliation></author>
      <author><first>Stephen</first><last>Bach</last><affiliation>Brown University</affiliation></author>
      <pages>479-488</pages>
      <abstract>Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural language prompts for language and vision-language models. Alfred provides a simple Python interface for the key steps of this emerging paradigm, with a high-throughput backend for large-scale data labeling. Users can quickly create, evaluate, and refine their prompt-based weak supervision sources; map the results to weak labels; and resolve their disagreements with a label model. Alfred enables a seamless local development experience backed by models served from self-managed computing clusters. It automatically optimizes the execution of prompts with optimized batching mechanisms. We find that this optimization improves query throughput by 2.9x versus a naive approach. We present two example use cases demonstrating Alfred on YouTube comment spam detection and pet breeds classification. Alfred is open source, available at https://github.com/BatsResearch/alfred.</abstract>
      <url hash="f4174f6c">2023.acl-demo.46</url>
      <bibkey>yu-bach-2023-alfred</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>O</fixed-case>pen<fixed-case>ICL</fixed-case>: An Open-Source Framework for In-context Learning</title>
      <author><first>Zhenyu</first><last>Wu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Yaoxiang</first><last>Wang</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Jiacheng</first><last>Ye</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhiyong</first><last>Wu</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Jiangtao</first><last>Feng</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Jingjing</first><last>Xu</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>Shanghai AI Lab</affiliation></author>
      <pages>489-498</pages>
      <abstract>In recent years, In-context Learning (ICL) has gained increasing attentionand emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates.However, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components.To facilitate ICL research, we introduce OpenICL, an open-source toolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly flexible architecture that users can easily combine different components to suit their needs.It also provides various state-of-the-art retrieval and inference methods to streamline the process of adapting ICL to cutting-edge research.The effectiveness of OpenICL has been validated on a wide range of NLP tasks, including classification, QA, machine translation, and semantic parsing. As a side-product, we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released at https://github.com/Shark-NLP/OpenICL.</abstract>
      <url hash="7e881c46">2023.acl-demo.47</url>
      <bibkey>wu-etal-2023-openicl</bibkey>
    </paper>
    <paper id="48">
      <title>Self-Supervised Sentence Polishing by Adding Engaging Modifiers</title>
      <author><first>Zhexin</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jian</first><last>Guan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xin</first><last>Cui</last><affiliation>Platform and Content Group, Tencent Technology Co., Ltd.</affiliation></author>
      <author><first>Yu</first><last>Ran</last><affiliation>Platform and Content Group, Tencent Technology Co., Ltd.</affiliation></author>
      <author><first>Bo</first><last>Liu</last><affiliation>Platform and Content Group, Tencent Technology Co., Ltd.</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>499-507</pages>
      <abstract>Teachers often guide students to improve their essays by adding engaging modifiers to polish the sentences. In this work, we present the first study on automatic sentence polishing by adding modifiers. Since there is no available dataset for the new task, we first automatically construct a large number of parallel data by removing modifiers in the engaging sentences collected from public resources. Then we fine-tune LongLM to reconstruct the original sentences from the corrupted ones. Considering that much overlap between inputs and outputs may bias the model to completely copy the inputs, we split each source sentence into sub-sentences and only require the model to generate the modified sub-sentences. Furthermore, we design a retrieval augmentation algorithm to prompt the model to add suitable modifiers. Automatic and manual evaluation on the auto-constructed test set and real human texts show that our model can generate more engaging sentences with suitable modifiers than strong baselines while keeping fluency. We deploy the model at <url>http://coai.cs.tsinghua.edu.cn/static/polishSent/</url>. A demo video is available at <url>https://youtu.be/Y6gFHOgSv8Y</url>.</abstract>
      <url hash="d104a115">2023.acl-demo.48</url>
      <bibkey>zhang-etal-2023-self</bibkey>
    </paper>
    <paper id="49">
      <title>Effidit: An Assistant for Improving Writing Efficiency</title>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Enbo</first><last>Zhao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wei</first><last>Bi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Deng</first><last>Cai</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Leyang</first><last>Cui</last><affiliation>Tencent AI LAB</affiliation></author>
      <author><first>Xinting</first><last>Huang</last><affiliation>Tencent</affiliation></author>
      <author><first>Haiyun</first><last>Jiang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Duyu</first><last>Tang</last><affiliation>Tencent</affiliation></author>
      <author><first>Kaiqiang</first><last>Song</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Longyue</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Chenyan</first><last>Huang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Guoping</first><last>Huang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yan</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Piji</first><last>Li</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <pages>508-515</pages>
      <abstract>Writing assistants are valuable tools that can help writers improve their writing skills. We introduce Effidit (<b>Eff</b>icient and <b>I</b>ntelligent E<b>dit</b>ing), a digital writing assistant that facilitates users to write higher-quality text more efficiently through the use of Artificial Intelligence (AI) and Natural Language Processing (NLP) technologies. We significantly expand the capacities of a writing assistantby providing functions in three modules: text completion, hint recommendation, and writing refinement. Based on the above efforts, Effidit can efficiently assist users in creating their own text. Effidit has been deployed to several Tencent products and publicly released at <url>https://effidit.qq.com/</url>.</abstract>
      <url hash="36f3b865">2023.acl-demo.49</url>
      <bibkey>shi-etal-2023-effidit</bibkey>
    </paper>
    <paper id="50">
      <title><fixed-case>W</fixed-case>iz<fixed-case>M</fixed-case>ap: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings</title>
      <author><first>Zijie J.</first><last>Wang</last><affiliation>Georgia Tech</affiliation></author>
      <author><first>Fred</first><last>Hohman</last><affiliation>Apple</affiliation></author>
      <author><first>Duen Horng</first><last>Chau</last><affiliation>Georgia Tech</affiliation></author>
      <pages>516-523</pages>
      <abstract>Machine learning models often learn latent embedding representations that capture the domain semantics of their training data. These embedding representations are valuable for interpreting trained models, building new models, and analyzing new datasets. However, interpreting and using embeddings can be challenging due to their opaqueness, high dimensionality, and the large size of modern datasets. To tackle these challenges, we present WizMap, an interactive visualization tool to help researchers and practitioners easily explore large embeddings. With a novel multi-resolution embedding summarization method and a familiar map-like interaction design, WizMap enables users to navigate and interpret embedding spaces with ease. Leveraging modern web technologies such as WebGL and Web Workers, WizMap scales to millions of embedding points directly in users’ web browsers and computational notebooks without the need for dedicated backend servers. WizMap is open-source and available at the following public demo link: https://poloclub.github.io/wizmap.</abstract>
      <url hash="1b09ad4b">2023.acl-demo.50</url>
      <bibkey>wang-etal-2023-wizmap</bibkey>
    </paper>
    <paper id="51">
      <title>A System for Answering Simple Questions in Multiple Languages</title>
      <author><first>Anton</first><last>Razzhigaev</last><affiliation>Skoltech</affiliation></author>
      <author><first>Mikhail</first><last>Salnikov</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Valentin</first><last>Malykh</last><affiliation>Huawei Noah’s Ark Lab / Kazan Federal University</affiliation></author>
      <author><first>Pavel</first><last>Braslavski</last><affiliation>Ural Federal University and HSE University</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skolkovo Institue of Science and Technology</affiliation></author>
      <pages>524-537</pages>
      <abstract>Our research focuses on the most prevalent type of queries— simple questions —exemplified by questions like “What is the capital of France?”. These questions reference an entity such as “France”, which is directly connected (one hop) to the answer entity “Paris” in the underlying knowledge graph (KG). We propose a multilingual Knowledge Graph Question Answering (KGQA) technique that orders potential responses based on the distance between the question’s text embeddings and the answer’s graph embeddings. A system incorporating this novel method is also described in our work.Through comprehensive experimentation using various English and multilingual datasets and two KGs — Freebase and Wikidata — we illustrate the comparative advantage of the proposed method across diverse KG embeddings and languages. This edge is apparent even against robust baseline systems, including seq2seq QA models, search-based solutions and intricate rule-based pipelines. Interestingly, our research underscores that even advanced AI systems like ChatGPT encounter difficulties when tasked with answering simple questions. This finding emphasizes the relevance and effectiveness of our approach, which consistently outperforms such systems. We are making the source code and trained models from our study publicly accessible to promote further advancements in multilingual KGQA.</abstract>
      <url hash="0b7340c5">2023.acl-demo.51</url>
      <bibkey>razzhigaev-etal-2023-system</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>KWJA</fixed-case>: A Unified <fixed-case>J</fixed-case>apanese Analyzer Based on Foundation Models</title>
      <author><first>Nobuhiro</first><last>Ueda</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Kazumasa</first><last>Omura</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Takashi</first><last>Kodama</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Hirokazu</first><last>Kiyomaru</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Yugo</first><last>Murawaki</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Daisuke</first><last>Kawahara</last><affiliation>Waseda University</affiliation></author>
      <author><first>Sadao</first><last>Kurohashi</last><affiliation>Kyoto University</affiliation></author>
      <pages>538-548</pages>
      <abstract>We present KWJA, a high-performance unified Japanese text analyzer based on foundation models.KWJA supports a wide range of tasks, including typo correction, word segmentation, word normalization, morphological analysis, named entity recognition, linguistic feature tagging, dependency parsing, PAS analysis, bridging reference resolution, coreference resolution, and discourse relation analysis, making it the most versatile among existing Japanese text analyzers.KWJA solves these tasks in a multi-task manner but still achieves competitive or better performance compared to existing analyzers specialized for each task.KWJA is publicly available under the MIT license at https://github.com/ku-nlp/kwja.</abstract>
      <url hash="2e7f285c">2023.acl-demo.52</url>
      <bibkey>ueda-etal-2023-kwja</bibkey>
    </paper>
    <paper id="53">
      <title>Disease Network Constructor: a Pathway Extraction and Visualization</title>
      <author><first>Mohammad Golam</first><last>Sohrab</last><affiliation>Artificial Intelligence Research Centre at AIST</affiliation></author>
      <author><first>Khoa</first><last>Duong</last><affiliation>Aist</affiliation></author>
      <author><first>Goran</first><last>Topić</last><affiliation>Aist</affiliation></author>
      <author><first>Masami</first><last>Ikeda</last><affiliation>National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Nozomi</first><last>Nagano</last><affiliation>Artificial Intelligence Research Centre at AIST</affiliation></author>
      <author><first>Yayoi</first><last>Natsume-Kitatani</last><affiliation>National Institutes of Biomedical Innovation, Health and Nutrition</affiliation></author>
      <author><first>Masakata</first><last>Kuroda</last><affiliation>National Institutes of Biomedical Innovation, Health and Nutrition</affiliation></author>
      <author><first>Mari</first><last>Itoh</last><affiliation>National Institutes of Biomedical Innovation, Health and Nutrition</affiliation></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>The National Institute of Advanced Industrial Science and Technology (AIST)</affiliation></author>
      <pages>549-557</pages>
      <abstract>We present Disease Network Constructor (DNC), a system that extracts and visualizes a disease network, in which nodes are entities such as diseases, proteins, and genes, and edges represent regulation relation. We focused on the disease network derived through regulation events found in scientific articles on idiopathic pulmonary fibrosis (IPF). The front-end web-base user interface of DNC includes two-dimensional (2D) and 3D visualizations of the constructed disease network. The back-end system of DNC includes several natural language processing (NLP) techniques to process biomedical text including BERT-based tokenization on the basis of Bidirectional Encoder Representations from Transformers (BERT), flat and nested named entity recognition (NER), candidate generation and candidate ranking for entity linking (EL) or, relation extraction (RE), and event extraction (EE) tasks. We evaluated the end-to-end EL and end-to-end nested EE systems to determine the DNC’s back-endimplementation performance. To the best of our knowledge, this is the first attempt that addresses neural NER, EL, RE, and EE tasks in an end-to-end manner that constructs a path-way visualization from events, which we name Disease Network Constructor. The demonstration video can be accessed from https://youtu.be/rFhWwAgcXE8. We release an online system for end users and the source code is available at https://github.com/aistairc/PRISM-APIs/.</abstract>
      <url hash="cbfc9b29">2023.acl-demo.53</url>
      <bibkey>sohrab-etal-2023-disease</bibkey>
    </paper>
    <paper id="54">
      <title>Petals: Collaborative Inference and Fine-tuning of Large Models</title>
      <author><first>Alexander</first><last>Borzunov</last><affiliation>HSE University, Yandex</affiliation></author>
      <author><first>Dmitry</first><last>Baranchuk</last><affiliation>Yandex</affiliation></author>
      <author><first>Tim</first><last>Dettmers</last><affiliation>University of Washington, Facebook AI Research</affiliation></author>
      <author><first>Maksim</first><last>Riabinin</last><affiliation>Yandex/HSE University</affiliation></author>
      <author><first>Younes</first><last>Belkada</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Artem</first><last>Chumachenko</last><affiliation>Yandex</affiliation></author>
      <author><first>Pavel</first><last>Samygin</last><affiliation>Yandex School of Data Analysis</affiliation></author>
      <author><first>Colin</first><last>Raffel</last><affiliation>University of North Carolina/Hugging Face</affiliation></author>
      <pages>558-568</pages>
      <abstract>Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers. In some cases, LLMs can be used more affordably via RAM offloading or hosted APIs. However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are not flexible enough for research that requires access to weights, attention or logits. In this work, we propose Petals - a system for inference and fine-tuning of large models collaboratively by joining the resources of multiple parties. We demonstrate that this strategy outperforms offloading for very large models, running inference of BLOOM-176B on consumer GPUs with ≈1 step per second, which is enough for many interactive LLM applications. Unlike most inference APIs, Petals also natively exposes hidden states of served models, allowing to train and share custom model extensions based on efficient fine-tuning methods. The system, its source code, and documentation are available at https://petals.mlVideo (2 min): https://youtu.be/F4muLI-0hTE</abstract>
      <url hash="0d227535">2023.acl-demo.54</url>
      <bibkey>borzunov-etal-2023-petals</bibkey>
    </paper>
    <paper id="55">
      <title><fixed-case>UKP</fixed-case>-<fixed-case>SQ</fixed-case>u<fixed-case>ARE</fixed-case> v3: A Platform for Multi-Agent <fixed-case>QA</fixed-case> Research</title>
      <author><first>Haritz</first><last>Puerto</last><affiliation>UKP Lab, TU Darmstadt</affiliation></author>
      <author><first>Tim</first><last>Baumgärtner</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Rachneet</first><last>Sachdeva</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Haishuo</first><last>Fang</last><affiliation>UKP Lab, TU Darmstadt</affiliation></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Sewin</first><last>Tariverdian</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Kexin</first><last>Wang</last><affiliation>Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science, Technical University of Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <pages>569-580</pages>
      <abstract>The continuous development of Question Answering (QA) datasets has drawn the research community’s attention toward multi-domain models. A popular approach is to use multi-dataset models, which are models trained on multiple datasets to learn their regularities and prevent overfitting to a single dataset. However, with the proliferation of QA models in online repositories such as GitHub or Hugging Face, an alternative is becoming viable. Recent works have demonstrated that combining expert agents can yield large performance gains over multi-dataset models. To ease research in multi-agent models, we extend UKP-SQuARE, an online platform for QA research, to support three families of multi-agent systems: i) agent selection, ii) early-fusion of agents, and iii) late-fusion of agents. We conduct experiments to evaluate their inference speed and discuss the performance vs. speed trade-off compared to multi-dataset models. UKP-SQuARE is open-source and publicly available.</abstract>
      <url hash="ff896f8f">2023.acl-demo.55</url>
      <bibkey>puerto-etal-2023-ukp</bibkey>
    </paper>
    <paper id="56">
      <title>Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation</title>
      <author><first>Mete</first><last>Sertkan</last><affiliation>TU Wien</affiliation></author>
      <author><first>Sophia</first><last>Althammer</last><affiliation>Technical University of Vienna</affiliation></author>
      <author><first>Sebastian</first><last>Hofstätter</last><affiliation>TU Wien</affiliation></author>
      <pages>581-587</pages>
      <abstract>In this paper, we introduce Ranger - a toolkit to facilitate the easy use of effect-size-based meta-analysis for multi-task evaluation in NLP and IR. We observed that our communities often face the challenge of aggregating results over incomparable metrics and scenarios, which makes conclusions and take-away messages less reliable. With Ranger, we aim to address this issue by providing a task-agnostic toolkit that combines the effect of a treatment on multiple tasks into one statistical evaluation, allowing for comparison of metrics and computation of an overall summary effect. Our toolkit produces publication-ready forest plots that enable clear communication of evaluation results over multiple tasks. Our goal with the ready-to-use Ranger toolkit is to promote robust, effect-size-based evaluation and improve evaluation standards in the community. We provide two case studies for common IR and NLP settings to highlight Ranger’s benefits.</abstract>
      <url hash="1e550fa5">2023.acl-demo.56</url>
      <bibkey>sertkan-etal-2023-ranger</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>GAIA</fixed-case> Search: Hugging Face and Pyserini Interoperability for <fixed-case>NLP</fixed-case> Training Data Exploration</title>
      <author><first>Aleksandra</first><last>Piktus</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Odunayo</first><last>Ogundepo</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Christopher</first><last>Akiki</last><affiliation>Leipzig University</affiliation></author>
      <author><first>Akintunde</first><last>Oladipo</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Xinyu</first><last>Zhang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Hailey</first><last>Schoelkopf</last><affiliation>EleutherAI</affiliation></author>
      <author><first>Stella</first><last>Biderman</last><affiliation>EleutherAI, Booz Allen Hamilton</affiliation></author>
      <author><first>Martin</first><last>Potthast</last><affiliation>Leipzig University</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>588-598</pages>
      <abstract>Noticing the urgent need to provide tools for fast and user-friendly qualitative analysis of large-scale textual corpora of the modern NLP, we propose to turn to the mature and well-tested methods from the domain of Information Retrieval (IR) - a research field with a long history of tackling TB-scale document collections. We discuss how Pyserini - a widely used toolkit for reproducible IR research can be integrated with the Hugging Face ecosystem of open-source AI libraries and artifacts. We leverage the existing functionalities of both platforms while proposing novel features further facilitating their integration. Our goal is to give NLP researchers tools that will allow them to develop retrieval-based instrumentation for their data analytics needs with ease and agility.We include a Jupyter Notebook-based walk through the core interoperability features, available on GitHub: https://github.com/huggingface/gaia.We then demonstrate how the ideas we present can be operationalized to create a powerful tool for qualitative data analysis in NLP. We present GAIA Search - a search engine built following previously laid out principles, giving access to four popular large-scale text collections. GAIA serves a dual purpose of illustrating the potential of methodologies we discuss but also as a standalone qualitative analysis tool that can be leveraged by NLP researchers aiming to understand datasets prior to using them in training. GAIA is hosted live on Hugging Face Spaces: https://huggingface.co/spaces/spacerini/gaia.</abstract>
      <url hash="0baea1b2">2023.acl-demo.57</url>
      <bibkey>piktus-etal-2023-gaia</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>D</fixed-case>eep<fixed-case>P</fixed-case>avlov Dream: Platform for Building Generative <fixed-case>AI</fixed-case> Assistants</title>
      <author><first>Diliara</first><last>Zharikova</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Daniel</first><last>Kornev</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Fedor</first><last>Ignatov</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Maxim</first><last>Talimanchuk</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Dmitry</first><last>Evseev</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Ksenya</first><last>Petukhova</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Veronika</first><last>Smilga</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Dmitry</first><last>Karpov</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Yana</first><last>Shishkina</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Dmitry</first><last>Kosenko</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Mikhail</first><last>Burtsev</last><affiliation>DeepPavlov.ai</affiliation></author>
      <pages>599-607</pages>
      <abstract>An open-source DeepPavlov Dream Platform is specifically tailored for development of complex dialog systems like Generative AI Assistants. The stack prioritizes efficiency, modularity, scalability, and extensibility with the goal to make it easier to develop complex dialog systems from scratch. It supports modular approach to implementation of conversational agents enabling their development through the choice of NLP components and conversational skills from a rich library organized into the distributions of ready-for-use multi-skill AI assistant systems. In DeepPavlov Dream, multi-skill Generative AI Assistant consists of NLP components that extract features from user utterances, conversational skills that generate or retrieve a response, skill and response selectors that facilitate choice of relevant skills and the best response, as well as a conversational orchestrator that enables creation of multi-skill Generative AI Assistants scalable up to industrial grade AI assistants. The platform allows to integrate large language models into dialog pipeline, customize with prompt engineering, handle multiple prompts during the same dialog session and create simple multimodal assistants.</abstract>
      <url hash="19d8774b">2023.acl-demo.58</url>
      <bibkey>zharikova-etal-2023-deeppavlov</bibkey>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2023-07-06">
    <meta>
      <booktitle>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</booktitle>
      <editor><first>Yun-Nung (Vivian)</first><last>Chen</last></editor>
      <editor><first>Margot</first><last>Margot</last></editor>
      <editor><first>Siva</first><last>Reddy</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="85bb242f">2023.acl-tutorials</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="67b8aa9b">2023.acl-tutorials.0</url>
      <bibkey>acl-2023-association-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Goal Awareness for Conversational <fixed-case>AI</fixed-case>: Proactivity, Non-collaborativity, and Beyond</title>
      <author><first>Yang</first><last>Deng</last></author>
      <author><first>Wenqiang</first><last>Lei</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <pages>1-10</pages>
      <abstract>Conversational systems are envisioned to provide social support or functional service to human users via natural language interactions. Conventional conversation researches mainly focus on the responseability of the system, such as dialogue context understanding and response generation, but overlooks the design of an essential property in intelligent conversations, i.e., goal awareness. The awareness of goals means the state of not only being responsive to the users but also aware of the target conversational goal and capable of leading the conversation towards the goal, which is a significant step towards higher-level intelligence and artificial consciousness. It can not only largely improve user engagement and service efficiency in the conversation, but also empower the system to handle more complicated conversation tasks that involve strategical and motivational interactions. In this tutorial, we will introduce the recent advances on the design of agent’s awareness of goals in a wide range of conversational systems.</abstract>
      <url hash="f9e8dfc5">2023.acl-tutorials.1</url>
      <bibkey>deng-etal-2023-goal</bibkey>
    </paper>
    <paper id="2">
      <title>Complex Reasoning in Natural Languag</title>
      <author><first>Wenting</first><last>Zhao</last></author>
      <author><first>Mor</first><last>Geva</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Michihiro</first><last>Yasunaga</last></author>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <pages>11-20</pages>
      <abstract>Teaching machines to reason over texts has been a long-standing goal of natural language processing (NLP). To this end, researchers have designed a diverse set of complex reasoning tasks that involve compositional reasoning, knowledge retrieval, grounding, commonsense reasoning, etc. A standard choice for building systems that perform a desired type of reasoning is to fine-tune a pretrained language model (LM) on specific downstream tasks. However, recent research has demonstrated that such a straightforward approach is often brittle. For example, Elazar et al. (2021) and Branco et al. (2021) show that, on question-answering (QA) tasks, similar performance can be achieved with questions removed from the inputs. Min et al. (2019), Chen and Durrett (2019), and Tang et al. (2021) show that models trained on multi-hop QA do not generalize to answer single-hop questions. The reasoning capabilities of these models thus remain at a surface level, i.e., exploiting data patterns. Consequently, augmenting LMs with techniques that make them robust and effective becomes an active research area. We will start the tutorial by providing an overview of complex reasoning tasks where the standard application of pretrained language models fails. This tutorial then reviews recent promising directions for tackling these tasks. Specifically, we focus on the following groups of approaches that explicitly consider problem structures: (1) knowledge-augmented methods, where the knowledge is either incorporated during fine-tuning or pretraining; (2) few-shot prompting methods, which effectively guide the models to follow instructions; (3) neuro-symbolic methods, which produce explicit intermediate representations; and, (4) rationale-based methods, one of the most popular forms of the neuro-symbolic methods, which highlight subsets of input as explanations for individual model predictions.</abstract>
      <url hash="79ac0f68">2023.acl-tutorials.2</url>
      <bibkey>zhao-etal-2023-complex</bibkey>
    </paper>
    <paper id="3">
      <title>Everything you need to know about Multilingual <fixed-case>LLM</fixed-case>s: Towards fair, performant and reliable models for languages of the world</title>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Barun</first><last>Patra</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Kabir</first><last>Ahuja</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <pages>21-26</pages>
      <abstract>This tutorial will describe various aspects of scaling up language technologies to many of the world’s languages by describing the latest research in Massively Multilingual Language Models (MMLMs). We will cover topics such as data collection, training and fine-tuning of models, Responsible AI issues such as fairness, bias and toxicity, linguistic diversity and evaluation in the context of MMLMs, specifically focusing on issues in non-English and low-resource languages. Further, we will also talk about some of the real-world challenges in deploying these models in language communities in the field. With the performance of MMLMs improving in the zero-shot setting for many languages, it is now becoming feasible to use them for building language technologies in many languages of the world, and this tutorial will provide the computational linguistics community with unique insights from the latest research in multilingual models.</abstract>
      <url hash="355687d4">2023.acl-tutorials.3</url>
      <bibkey>sitaram-etal-2023-everything</bibkey>
    </paper>
    <paper id="4">
      <title>Generating Text from Language Models</title>
      <author><first>Afra</first><last>Amini</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>John</first><last>Hewitt</last></author>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <pages>27-31</pages>
      <abstract>An increasingly large percentage of natural language processing (NLP) tasks center around the generation of text from probabilistic language models. Despite this trend, techniques for improving or specifying preferences in these generated texts rely mostly on intuition-based heuristics. Further, there lacks a unified presentation of their motivations, practical implementation, successes and pitfalls. Practitioners must, therefore, choose somewhat blindly between generation algorithms—like top-p sampling or beam search—which can lead to wildly different results. At the same time, language generation research continues to criticize and improve the standard toolboxes, further adding entropy to the state of the field. In this tutorial, we will provide a centralized and cohesive discussion of critical considerations when choosing how to generate from a language model. We will cover a wide range of empirically-observed problems (like degradation, hallucination, repetition) and their corresponding proposed algorithmic solutions from recent research (like top-p sampling and its successors). We will then discuss a subset of these algorithms under a unified light; most stochastic generation strategies can be framed as locally adapting the probabilities of a model to avoid failure cases. Finally, we will then cover methods in controlled generation, that go beyond just ensuring coherence to ensure text exhibits specific desired properties. We aim for NLP practitioners and researchers to leave our tutorial with a unified framework which they can use to evaluate and contribute to the latest research in language generation.</abstract>
      <url hash="7814afed">2023.acl-tutorials.4</url>
      <bibkey>amini-etal-2023-generating</bibkey>
    </paper>
    <paper id="5">
      <title>Indirectly Supervised Natural Language Processing</title>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>32-40</pages>
      <abstract>This tutorial targets researchers and practitioners who are interested in ML technologies for NLP from indirect supervision. In particular, we will present a diverse thread of indirect supervision studies that try to answer the following questions: (i) when and how can we provide supervision for a target task T, if all we have is data that corresponds to a “related” task T′? (ii) humans do not use exhaustive supervision; they rely on occasional feedback, and learn from incidental signals from various sources; how can we effectively incorporate such supervision in machine learning? (iii) how can we leverage multi-modal supervision to help NLP? To the end, we will discuss several lines of research that address those challenges, including (i) indirect supervision from T ′ that handles T with outputs spanning from a moderate size to an open space, (ii) the use of sparsely occurring and incidental signals, such as partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations—all having statistical associations with the task, (iii) principled ways to measure and understand why these incidental signals can contribute to our target tasks, and (iv) indirect supervision from vision-language signals. We will conclude the tutorial by outlining directions for further investigation.</abstract>
      <url hash="fc645450">2023.acl-tutorials.5</url>
      <bibkey>yin-etal-2023-indirectly</bibkey>
    </paper>
    <paper id="6">
      <title>Retrieval-based Language Models and Applications</title>
      <author><first>Akari</first><last>Asai</last></author>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Zexuan</first><last>Zhong</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>41-46</pages>
      <abstract>Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.</abstract>
      <url hash="fe3433be">2023.acl-tutorials.6</url>
      <bibkey>asai-etal-2023-retrieval</bibkey>
    </paper>
  </volume>
  <volume id="industry" ingest-date="2023-07-06">
    <meta>
      <booktitle>Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics - Industry Track</booktitle>
      <editor><first>Sunayana</first><last>Sitaram</last></editor>
      <editor><first>Beata</first><last>Beigman Klebanov</last></editor>
      <editor><first>Jason D</first><last>Williams</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="8bd46a0d">2023.acl-industry</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="b5bf638c">2023.acl-industry.0</url>
      <bibkey>acl-2023-association-linguistics-industry</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>CWS</fixed-case>eg: An Efficient and General Approach to <fixed-case>C</fixed-case>hinese Word Segmentation</title>
      <author><first>Dedong</first><last>Li</last><affiliation>SenseTime Research</affiliation></author>
      <author><first>Rui</first><last>Zhao</last><affiliation>SenseTime Group Limited</affiliation></author>
      <author><first>Fei</first><last>Tan</last><affiliation>Sensetime Research</affiliation></author>
      <pages>1-10</pages>
      <abstract>In this work, we report our efforts in advancing Chinese Word Segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in the deployment. It includes the balance between performance and cost, segmentation ambiguity due to domain diversity and vague words boundary, and multi-grained segmentation. In this context, we propose a simple yet effective approach, namely CWSeg, to augment PLM-based schemes by developing cohort training and versatile decoding strategies. Extensive experiments on benchmark datasets demonstrate the efficiency and generalization of our approach. The corresponding segmentation system is also implemented for practical usage and the demo is recorded.</abstract>
      <url hash="7b7b62bc">2023.acl-industry.1</url>
      <bibkey>li-etal-2023-cwseg</bibkey>
    </paper>
    <paper id="2">
      <title>“Knowledge is Power”: Constructing Knowledge Graph of Abdominal Organs and Using Them for Automatic Radiology Report Generation</title>
      <author><first>Kaveri</first><last>Kale</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology Bombay and Patna</affiliation></author>
      <author><first>Aditya</first><last>Shetty</last><affiliation>Consultant Radiologist, Breach Candy Hospital, Mumbai</affiliation></author>
      <author><first>Milind</first><last>Gune</last><affiliation>Consultant Radiologist, Mumbai, Thane</affiliation></author>
      <author><first>Kush</first><last>Shrivastava</last><affiliation>Augnito India Pvt Ltd</affiliation></author>
      <author><first>Rustom</first><last>Lawyer</last><affiliation>Augnito India Pvt Ltd</affiliation></author>
      <author><first>Spriha</first><last>Biswas</last><affiliation>Augnito India Pvt Ltd</affiliation></author>
      <pages>11-24</pages>
      <abstract>In conventional radiology practice, the radiologist dictates the diagnosis to the transcriptionist, who then prepares a preliminary formatted report referring to the notes, after which the radiologist reviews the report, corrects the errors, and signs off. This workflow is prone to delay and error. In this paper, we report our work on automatic radiology report generation from radiologists’ dictation, which is in collaboration with a startup about to become Unicorn. A major contribution of our work is the set of knowledge graphs (KGs) of ten abdominal organs- Liver, Kidney, Gallbladder, Uterus, Urinary bladder, Ovary, Pancreas, Prostate, Biliary Tree, and Bowel. Our method for constructing these KGs relies on extracting entity1-relation-entity2 triplets from a large collection (about 10,000) of free-text radiology reports. The quality and coverage of the KGs are verified by two experienced radiologists (practicing for the last 30 years and 8 years, respectively). The dictation of the radiologist is automatically converted to what is called a pathological description which is the clinical description of the findings of the radiologist during ultrasonography (USG). Our knowledge-enhanced deep learning model improves the reported BLEU-3, ROUGE-L, METEOR, and CIDEr scores of the pathological description generation by 2%, 4%, 2% and 2% respectively. To the best of our knowledge, this is the first attempt at representing the abdominal organs in the form of knowledge graphs and utilising these graphs for the automatic generation of USG reports. A Minimum Viable Product (MVP) has been made available to the beta users, i.e., radiologists of reputed hospitals, for testing and evaluation. Our solution guarantees report generation within 30 seconds of running a scan.</abstract>
      <url hash="9e124ec6">2023.acl-industry.2</url>
      <bibkey>kale-etal-2023-knowledge</bibkey>
    </paper>
    <paper id="3">
      <title>Hunt for Buried Treasures: Extracting Unclaimed Embodiments from Patent Specifications</title>
      <author><first>Chikara</first><last>Hashimoto</last><affiliation>Rakuten</affiliation></author>
      <author><first>Gautam</first><last>Kumar</last><affiliation>Rakuten, Inc. Tokyo</affiliation></author>
      <author><first>Shuichiro</first><last>Hashimoto</last><affiliation>Rakuten Group, Inc.</affiliation></author>
      <author><first>Jun</first><last>Suzuki</last><affiliation>Tohoku University / RIKEN Center for AIP</affiliation></author>
      <pages>25-36</pages>
      <abstract>Patent applicants write patent specificationsthat describe embodiments of inventions.Some embodiments are claimed for a patent,while others may be unclaimeddue to strategic considerations.Unclaimed embodiments may be extracted byapplicants later and claimed incontinuing applications togain advantages over competitors.Despite being essential for corporate intellectual property (IP) strategies,unclaimed embodiment extraction is conducted manually,and little research has been conducted on its automation.This paper presents a novel task ofunclaimed embodiment extraction (UEE)and a novel dataset for the task.Our experiments with Transformer-based modelsdemonstratedthat the task was challenging as it requiredconducting natural language inference onpatent specifications, which consisted oftechnical, long, syntactically and semanticallyinvolved sentences.We release the dataset and code to foster this new area of research.</abstract>
      <url hash="99e3c4c1">2023.acl-industry.3</url>
      <bibkey>hashimoto-etal-2023-hunt</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>M</fixed-case>ath<fixed-case>P</fixed-case>rompter: Mathematical Reasoning using Large Language Models</title>
      <author><first>Shima</first><last>Imani</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Liang</first><last>Du</last><affiliation>Microsoft Research, Redmond</affiliation></author>
      <author><first>Harsh</first><last>Shrivastava</last><affiliation>Microsoft Research, Redmond</affiliation></author>
      <pages>37-42</pages>
      <abstract>Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose ‘MathPrompter’, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the ‘MultiArith’ dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.</abstract>
      <url hash="d5b4eb87">2023.acl-industry.4</url>
      <bibkey>imani-etal-2023-mathprompter</bibkey>
    </paper>
    <paper id="5">
      <title>Constrained Policy Optimization for Controlled Self-Learning in Conversational <fixed-case>AI</fixed-case> Systems</title>
      <author><first>Mohammad</first><last>Kachuee</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Sungjin</first><last>Lee</last><affiliation>Amazon Alexa AI</affiliation></author>
      <pages>43-52</pages>
      <abstract>Recently, self-learning methods based on user satisfaction metrics and contextual bandits have shown promising results to enable consistent improvements in conversational AI systems. However, directly targeting such metrics by off-policy bandit learning objectives often increases the risk of making abrupt policy changes that break the current user experience. In this study, we introduce a scalable framework for supporting fine-grained exploration targets for individual domains via user-defined constraints. For example, we may want to ensure fewer policy deviations in business-critical domains such as shopping, while allocating more exploration budget to domains such as music. We present a novel meta-gradient learning approach that is scalable and practical to address this problem. The proposed method adjusts constraint violation penalty terms adaptively through a meta objective that encourages balanced constraint satisfaction across domains. We conducted extensive experiments on a real-world conversational AI and using a set of realistic constraint benchmarks. The proposed approach has been deployed in production for a large-scale commercial assistant, enabling the best balance between the policy value and constraint satisfaction rate.</abstract>
      <url hash="fbe4736e">2023.acl-industry.5</url>
      <bibkey>kachuee-lee-2023-constrained</bibkey>
    </paper>
    <paper id="6">
      <title>p<fixed-case>NLP</fixed-case>-Mixer: an Efficient all-<fixed-case>MLP</fixed-case> Architecture for Language</title>
      <author><first>Francesco</first><last>Fusco</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Damian</first><last>Pascual</last><affiliation>Telepathy Labs</affiliation></author>
      <author><first>Peter</first><last>Staar</last><affiliation>IBM Research</affiliation></author>
      <author><first>Diego</first><last>Antognini</last><affiliation>IBM Research</affiliation></author>
      <pages>53-60</pages>
      <abstract>Large pre-trained language models based on transformer architectureƒhave drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot filling and intent classification, with model sizes in the order of the megabyte. This work introduces the pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP that achieves high weight-efficiency thanks to a novel projection layer. We evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves 99.4% and 97.8% the performance of mBERT on MTOP and multiATIS, while using 170x less parameters. Our model consistently beats the state-of-the-art of tiny models (pQRNN), which is twice as large, by a margin up to 7.8% on MTOP.</abstract>
      <url hash="73eebf7e">2023.acl-industry.6</url>
      <bibkey>fusco-etal-2023-pnlp</bibkey>
    </paper>
    <paper id="7">
      <title>Extracting Text Representations for Terms and Phrases in Technical Domains</title>
      <author><first>Francesco</first><last>Fusco</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Diego</first><last>Antognini</last><affiliation>IBM Research</affiliation></author>
      <pages>61-70</pages>
      <abstract>Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to summarization. Common approaches to create dense representations include training domain-specific embeddings with self-supervised setups or using sentence encoder models trained over similarity tasks. In contrast to static embeddings, sentence encoders do not suffer from the out-of-vocabulary (OOV) problem, but impose significant computational costs. In this paper, we propose a fully unsupervised approach to text encoding that consists of training small character-based models with the objective of reconstructing large pre-trained embedding matrices. Models trained with this approach can not only match the quality of sentence encoders in technical domains, but are 5 times smaller and up to 10 times faster, even on high-end GPUs.</abstract>
      <url hash="492f7bb4">2023.acl-industry.7</url>
      <bibkey>fusco-antognini-2023-extracting</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>C</fixed-case>oca<fixed-case>CLIP</fixed-case>: Exploring Distillation of Fully-Connected Knowledge Interaction Graph for Lightweight Text-Image Retrieval</title>
      <author><first>Jiapeng</first><last>Wang</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Chengyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaodan</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jun</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Lianwen</first><last>Jin</last><affiliation>South China University of Technology</affiliation></author>
      <pages>71-80</pages>
      <abstract>Large-scale pre-trained text-image models with dual-encoder architectures (such as CLIP) are typically adopted for various vision-language applications, including text-image retrieval. However, these models are still less practical on edge devices or for real-time situations, due to the substantial indexing and inference time and the large consumption of computational resources. Although knowledge distillation techniques have been widely utilized for uni-modal model compression, how to expand them to the situation when the numbers of modalities and teachers/students are doubled has been rarely studied. In this paper, we conduct comprehensive experiments on this topic and propose the fully-Connected knowledge interaction graph (Coca) technique for cross-modal pre-training distillation. Based on our findings, the resulting CocaCLIP achieves SOTA performances on the widely-used Flickr30K and MSCOCO benchmarks under the lightweight setting. An industry application of our method on an e-commercial platform further demonstrates the significant effectiveness of CocaCLIP.</abstract>
      <url hash="44d79736">2023.acl-industry.8</url>
      <bibkey>wang-etal-2023-cocaclip</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>KG</fixed-case>-<fixed-case>FLIP</fixed-case>: Knowledge-guided Fashion-domain Language-Image Pre-training for <fixed-case>E</fixed-case>-commerce</title>
      <author><first>Qinjin</first><last>Jia</last><affiliation>North Carolina State University</affiliation></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Daoping</first><last>Wu</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Shaoyuan</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Huidong</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Jinmiao</first><last>Fu</last><affiliation>Amazon</affiliation></author>
      <author><first>Roland</first><last>Vollgraf</last><affiliation>Amazon</affiliation></author>
      <author><first>Bryan</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <pages>81-88</pages>
      <abstract>Various Vision-Language Pre-training (VLP) models (e.g., CLIP, BLIP) have sprung up and dramatically advanced the benchmarks for public general-domain datasets (e.g., COCO, Flickr30k). Such models usually learn the cross-modal alignment from large-scale well-aligned image-text datasets without leveraging external knowledge. Adapting these models to downstream applications in specific domains like fashion requires fine-grained in-domain image-text corpus, which are usually less semantically aligned and in small scale that requires efficient pre-training strategies. In this paper, we propose a knowledge-guided fashion-domain language-image pre-training (FLIP) framework that focuses on learning fine-grained representations in e-commerce domain and utilizes external knowledge (i.e., product attribute schema), to improve the pre-training efficiency. Experiments demonstrate that FLIP outperforms previous state-of-the-art VLP models on Amazon data and on the Fashion-Gen dataset by large margins. FLIP has been successfully deployed in the Amazon catalog system to backfill missing attributes and improve the customer shopping experience.</abstract>
      <url hash="e62b0356">2023.acl-industry.9</url>
      <bibkey>jia-etal-2023-kg</bibkey>
    </paper>
    <paper id="10">
      <title>Domain-specific transformer models for query translation</title>
      <author><first>Mandar</first><last>Kulkarni</last><affiliation>Sr. Data Scientist, Flipkart</affiliation></author>
      <author><first>Nikesh</first><last>Garera</last><affiliation>Flipkart</affiliation></author>
      <author><first>Anusua</first><last>Trivedi</last><affiliation>Flipkart</affiliation></author>
      <pages>89-95</pages>
      <abstract>Due to the democratization of e-commerce, many product companies are listing their goods for online shopping. For periodic buying within a domain such as Grocery, consumers are generally inclined to buy certain brands of products.Due to a large non-English speaking population in India, we observe a significant percentage of code-mix Hinglish search queries e.g., sasta atta. An intuitive approach to dealing with code-mix queries is to train an encoder-decoder model to translate the query to English to perform the search. However, the problem becomes non-trivial when the brand names themselves have Hinglish names and possibly have a literal English translation. In such queries, only the context (non-brand name) Hinglish words needs to be translated. In this paper, we propose a simple yet effective modification to the transformer training to preserve/correct Grocery brand names in the output while selectively translating the context words. To achieve this, we use an additional dataset of popular Grocery brand names. Brand names are added as tokens to the model vocabulary, and the token embeddings are randomly initialized. Further, we introduce a Brand loss in training the translation model. Brand loss is a cross entropy loss computed using a denoising auto-encoder objective with brand name data. We warm-start the training from a public pre-trained checkpoint (such as BART/T5) and further adapt it for query translation using the domain data. The proposed model is generic and can be used with English as well as code-mix Hinglish queries alleviating the need for language detection. To reduce the latency of the model for the production deployment, we use knowledge distillation and quantization. Experimental evaluation indicates that the proposed approach improves translation results by preserving/correcting English/Hinglish brand names. After positive results with A/B testing, the model is currently deployed in production.</abstract>
      <url hash="f086b266">2023.acl-industry.10</url>
      <bibkey>kulkarni-etal-2023-domain</bibkey>
    </paper>
    <paper id="11">
      <title>Label efficient semi-supervised conversational intent classification</title>
      <author><first>Mandar</first><last>Kulkarni</last><affiliation>Sr. Data Scientist, Flipkart</affiliation></author>
      <author><first>Kyung</first><last>Kim</last><affiliation>Flipkart</affiliation></author>
      <author><first>Nikesh</first><last>Garera</last><affiliation>Flipkart</affiliation></author>
      <author><first>Anusua</first><last>Trivedi</last><affiliation>Flipkart</affiliation></author>
      <pages>96-102</pages>
      <abstract>To provide a convenient shopping experience and to answer user queries at scale, conversational platforms are essential for e-commerce. The user queries can be pre-purchase questions, such as product specifications and delivery time related, or post-purchase queries, such as exchange and return. A chatbot should be able to understand and answer a variety of such queries to help users with relevant information. One of the important modules in the chatbot is automated intent identification, i.e., understanding the user’s intention from the query text. Due to non-English speaking users interacting with the chatbot, we often get a significant percentage of code mix queries and queries with grammatical errors, which makes the problem more challenging. This paper proposes a simple yet competent Semi-Supervised Learning (SSL) approach for label-efficient intent classification. We use a small labeled corpus and relatively larger unlabeled query data to train a transformer model. For training the model with labeled data, we explore supervised MixUp data augmentation. To train with unlabeled data, we explore label consistency with dropout noise. We experiment with different pre-trained transformer architectures, such as BERT and sentence-BERT. Experimental results demonstrate that the proposed approach significantly improves over the supervised baseline, even with a limited labeled set. A variant of the model is currently deployed in production.</abstract>
      <url hash="960d0866">2023.acl-industry.11</url>
      <bibkey>kulkarni-etal-2023-label</bibkey>
    </paper>
    <paper id="12">
      <title>x<fixed-case>PQA</fixed-case>: Cross-Lingual Product Question Answering in 12 Languages</title>
      <author><first>Xiaoyu</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <author><first>Akari</first><last>Asai</last><affiliation>University of Washington</affiliation></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Adria</first><last>De Gispert</last><affiliation>Amazon</affiliation></author>
      <pages>103-115</pages>
      <abstract>Product Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers’ questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product information available in English. To study this practical industrial task, we present xPQA, a large-scale annotated cross-lingual PQA dataset in 12 languages, and report results in (1) candidate ranking, to select the best English candidate containing the information to answer a non-English question; and (2) answer generation, to generate a natural-sounding non-English answer based on the selected English candidate.We evaluate various approaches involving machine translation at runtime or offline, leveraging multilingual pre-trained LMs, and including or excluding xPQA training data. We find that in-domain data is essential as cross-lingual rankers trained on other domains perform poorly on the PQA task, and that translation-based approaches are most effective for candidate ranking while multilingual finetuning works best for answer generation. Still, there remains a significant performance gap between the English and the cross-lingual test sets.</abstract>
      <url hash="82e2b3a8">2023.acl-industry.12</url>
      <bibkey>shen-etal-2023-xpqa</bibkey>
    </paper>
    <paper id="13">
      <title>Learn over Past, Evolve for Future: Forecasting Temporal Trends for Fake News Detection</title>
      <author><first>Beizhe</first><last>Hu</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Qiang</first><last>Sheng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Juan</first><last>Cao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Yongchun</first><last>Zhu</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Danding</first><last>Wang</last><affiliation>Institute of Computing, CAS</affiliation></author>
      <author><first>Zhengjia</first><last>Wang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhiwei</first><last>Jin</last><affiliation>Ruijian Technology Corproation Limited</affiliation></author>
      <pages>116-125</pages>
      <abstract>Fake news detection has been a critical task for maintaining the health of the online news ecosystem. However, very few existing works consider the temporal shift issue caused by the rapidly-evolving nature of news data in practice, resulting in significant performance degradation when training on past data and testing on future data. In this paper, we observe that the appearances of news events on the same topic may display discernible patterns over time, and posit that such patterns can assist in selecting training instances that could make the model adapt better to future data. Specifically, we design an effective framework FTT (Forecasting Temporal Trends), which could forecast the temporal distribution patterns of news data and then guide the detector to fast adapt to future distribution. Experiments on the real-world temporally split dataset demonstrate the superiority of our proposed framework.</abstract>
      <url hash="71822bf9">2023.acl-industry.13</url>
      <bibkey>hu-etal-2023-learn</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>AVEN</fixed-case>-<fixed-case>GR</fixed-case>: Attribute Value Extraction and Normalization using product <fixed-case>GR</fixed-case>aphs</title>
      <author><first>Thomas</first><last>Ricatte</last><affiliation>Amazon</affiliation></author>
      <author><first>Donato</first><last>Crisostomi</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>126-133</pages>
      <abstract>Getting a good understanding of the user intent is vital for e-commerce applications to surface the right product to a given customer query. Query Understanding (QU) systems are essential for this purpose, and many e-commerce providers are working on complex solutions that need to be data efficient and able to capture early emerging market trends. Query Attribute Understanding (QAU) is a sub-component of QU that involves extracting named attributes from user queries and linking them to existing e-commerce entities such as brand, material, color, etc. While extracting named entities from text has been extensively explored in the literature, QAU requires specific attention due to the nature of the queries, which are often short, noisy, ambiguous, and constantly evolving. This paper makes three contributions to QAU. First, we propose a novel end-to-end approach that jointly solves Named Entity Recognition (NER) and Entity Linking (NEL) and enables open-world reasoning for QAU. Second, we introduce a novel method for utilizing product graphs to enhance the representation of query entities. Finally, we present a new dataset constructed from public sources that can be used to evaluate the performance of future QAU systems.</abstract>
      <url hash="a1dd8f0e">2023.acl-industry.14</url>
      <bibkey>ricatte-crisostomi-2023-aven</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>GKD</fixed-case>: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model</title>
      <author><first>Shicheng</first><last>Tan</last><affiliation>Anhui University</affiliation></author>
      <author><first>Weng Lam</first><last>Tam</last><affiliation>Zhipu.AI</affiliation></author>
      <author><first>Yuanchun</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Wenwen</first><last>Gong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shu</first><last>Zhao</last><affiliation>Anhui University</affiliation></author>
      <author><first>Peng</first><last>Zhang</last><affiliation>Zhipu.AI</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>134-148</pages>
      <abstract>Currently, the reduction in the parameter scale of large-scale pre-trained language models (PLMs) through knowledge distillation has greatly facilitated their widespread deployment on various devices. However, the deployment of knowledge distillation systems faces great challenges in real-world industrial-strength applications, which require the use of complex distillation methods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the switching of methods. To overcome these challenges, we propose GKD, a general knowledge distillation framework that supports distillation on larger-scale PLMs using various distillation methods. With GKD, developers can build larger distillation models on memory-limited GPUs and easily switch and combine different distillation methods within a single framework. Experimental results show that GKD can support the distillation of at least 100B-scale PLMs and 25 mainstream methods on 8 NVIDIA A100 (40GB) GPUs.</abstract>
      <url hash="e0e876f4">2023.acl-industry.15</url>
      <bibkey>tan-etal-2023-gkd</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>F</fixed-case>ashion<fixed-case>KLIP</fixed-case>: Enhancing <fixed-case>E</fixed-case>-Commerce Image-Text Retrieval with Fashion Multi-Modal Conceptual Knowledge Graph</title>
      <author><first>Xiaodan</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Chengyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Zhixu</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Ben</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Linbo</first><last>Jin</last><affiliation>Alibaba</affiliation></author>
      <author><first>Jun</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Ming</first><last>Gao</last><affiliation>East China Normal University</affiliation></author>
      <pages>149-158</pages>
      <abstract>Image-text retrieval is a core task in the multi-modal domain, which arises a lot of attention from both research and industry communities. Recently, the booming of visual-language pre-trained (VLP) models has greatly enhanced the performance of cross-modal retrieval. However, the fine-grained interactions between objects from different modalities are far from well-established. This issue becomes more severe in the e-commerce domain, which lacks sufficient training data and fine-grained cross-modal knowledge. To alleviate the problem, this paper proposes a novel e-commerce knowledge-enhanced VLP model FashionKLIP. We first automatically establish a multi-modal conceptual knowledge graph from large-scale e-commerce image-text data, and then inject the prior knowledge into the VLP model to align across modalities at the conceptual level. The experiments conducted on a public benchmark dataset demonstrate that FashionKLIP effectively enhances the performance of e-commerce image-text retrieval upon state-of-the-art VLP models by a large margin. The application of the method in real industrial scenarios also proves the feasibility and efficiency of FashionKLIP.</abstract>
      <url hash="f38bcd66">2023.acl-industry.16</url>
      <bibkey>wang-etal-2023-fashionklip</bibkey>
    </paper>
    <paper id="17">
      <title>Entity Contrastive Learning in a Large-Scale Virtual Assistant System</title>
      <author><first>Jonathan</first><last>Rubin</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Jason</first><last>Crowley</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>George</first><last>Leung</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Morteza</first><last>Ziyadi</last><affiliation>Applied Scientist at Amazon</affiliation></author>
      <author><first>Maria</first><last>Minakova</last><affiliation>Amazon Alexa AI</affiliation></author>
      <pages>159-171</pages>
      <abstract>Conversational agents are typically made up of domain (DC) and intent classifiers (IC) that identify the general subject an utterance belongs to and the specific action a user wishes to achieve. In addition, named entity recognition (NER) performs per token labeling to identify specific entities of interest in a spoken utterance. We investigate improving joint IC and NER models using entity contrastive learning that attempts to cluster similar entities together in a learned representation space. We compare a full virtual assistant system trained using entity contrastive learning to a production baseline system that does not use contrastive learning. We present both offline results, using retrospective test sets, as well as live online results from an A/B test that compared the two systems. In both the offline and online settings, entity contrastive training improved overall performance against production baselines. Furthermore, we provide a detailed analysis of learned entity embeddings, including both qualitative analysis via dimensionality-reduced visualizations and quantitative analysis by computing alignment and uniformity metrics. We show that entity contrastive learning improves alignment metrics and produces well-formed embedding clusters in representation space.</abstract>
      <url hash="3204fcd7">2023.acl-industry.17</url>
      <bibkey>rubin-etal-2023-entity</bibkey>
    </paper>
    <paper id="18">
      <title>Tab-Cleaner: Weakly Supervised Tabular Data Cleaning via Pre-training for <fixed-case>E</fixed-case>-commerce Catalog</title>
      <author><first>Kewei</first><last>Cheng</last><affiliation>Ucla</affiliation></author>
      <author><first>Xian</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhengyang</first><last>Wang</last><affiliation>Amazon.com</affiliation></author>
      <author><first>Chenwei</first><last>Zhang</last><affiliation>Amazon.com</affiliation></author>
      <author><first>Binxuan</first><last>Huang</last><affiliation>Amazon.com</affiliation></author>
      <author><first>Yifan Ethan</first><last>Xu</last><affiliation>Meta</affiliation></author>
      <author><first>Xin Luna</first><last>Dong</last><affiliation>Meta</affiliation></author>
      <author><first>Yizhou</first><last>Sun</last><affiliation>Ucla</affiliation></author>
      <pages>172-185</pages>
      <abstract>Product catalogs, conceptually in the form of text-rich tables, are self-reported by individual retailers and thus inevitably contain noisy facts. Verifying such textual attributes in product catalogs is essential to improve their reliability. However, popular methods for processing free-text content, such as pre-trained language models, are not particularly effective on structured tabular data since they are typically trained on free-form natural language texts. In this paper, we present Tab-Cleaner, a model designed to handle error detection over text-rich tabular data following a pre-training / fine-tuning paradigm. We train Tab-Cleaner on a real-world Amazon Product Catalog table w.r.t millions of products and show improvements over state-of-the-art methods by 16{% on PR AUC over attribute applicability classification task and by 11{% on PR AUC over attribute value validation task.</abstract>
      <url hash="f59a0402">2023.acl-industry.18</url>
      <bibkey>cheng-etal-2023-tab</bibkey>
    </paper>
    <paper id="19">
      <title>Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs</title>
      <author><first>Abishek</first><last>Komma</last><affiliation>Amazon</affiliation></author>
      <author><first>Nagesh</first><last>Panyam Chandrasekarasastry</last><affiliation>Amazon</affiliation></author>
      <author><first>Timothy</first><last>Leffel</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Anuj</first><last>Goyal</last><affiliation>Amazon Alexa</affiliation></author>
      <author><first>Angeliki</first><last>Metallinou</last><affiliation>Amazon</affiliation></author>
      <author><first>Spyros</first><last>Matsoukas</last><affiliation>Amazon.com</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>USC Information Sciences Institute</affiliation></author>
      <pages>186-195</pages>
      <abstract>Measurement of interaction quality is a critical task for the improvement of large-scale spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the proposed evaluation model shows better domain generalization ability compared to the baselines. On the basis of these results, we argue that having high-quality human-annotated data is an important component of evaluating interaction quality for large industrial-scale voice assistant platforms.</abstract>
      <url hash="07a589c6">2023.acl-industry.19</url>
      <bibkey>komma-etal-2023-toward</bibkey>
    </paper>
    <paper id="20">
      <title>Tab-<fixed-case>CQA</fixed-case>: A Tabular Conversational Question Answering Dataset on Financial Reports</title>
      <author><first>Chuang</first><last>Liu</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Junzhuo</first><last>Li</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>196-207</pages>
      <abstract>Existing conversational question answering (CQA) datasets have been usually constructed from unstructured texts in English. In this paper, we propose Tab-CQA, a tabular CQA dataset created from Chinese financial reports that are extracted from listed companies in a wide range of different sectors in the past 30 years. From these reports, we select 2,463 tables, and manually generate 2,463 conversations with 35,494 QA pairs. Additionally, we select 4,578 tables, from which 4,578 conversations with 73,595 QA pairs are automatically created via a template-based method. With the manually- and automatically-generated conversations, Tab-CQA contains answerable and unanswerable questions. For the answerable questions, we further diversify them to cover a wide range of skills, e.g., table retrieval, fact checking, numerical reasoning, so as to accommodate real-world scenarios. We further propose two different tabular CQA models, a text-based model and an operation-based model, and evaluate them on Tab-CQA. Experiment results show that Tab-CQA is a very challenging dataset, where a huge performance gap exists between human and neural models. We will publicly release Tab-CQA as a benchmark testbed to promote further research on Chinese tabular CQA.</abstract>
      <url hash="b8569995">2023.acl-industry.20</url>
      <bibkey>liu-etal-2023-tab</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>K</fixed-case>o<fixed-case>SBI</fixed-case>: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications</title>
      <author><first>Hwaran</first><last>Lee</last><affiliation>NAVER AI Lab</affiliation></author>
      <author><first>Seokhee</first><last>Hong</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Joonsuk</first><last>Park</last><affiliation>University of Richmond</affiliation></author>
      <author><first>Takyoung</first><last>Kim</last><affiliation>NAVER AI Lab</affiliation></author>
      <author><first>Gunhee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jung-woo</first><last>Ha</last><affiliation>NAVER Cloud AI Lab</affiliation></author>
      <pages>208-224</pages>
      <abstract>Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the safe and effective deployment of LLMs. To this end, we present KosBi, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories. We find that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for HyperClova (30B and 82B), and GPT-3.</abstract>
      <url hash="7e0680a6">2023.acl-industry.21</url>
      <bibkey>lee-etal-2023-kosbi</bibkey>
    </paper>
    <paper id="22">
      <title>Improving Knowledge Production Efficiency With Question Answering on Conversation</title>
      <author><first>Changlin</first><last>Yang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Siye</first><last>Liu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Sen</first><last>Hu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Wangshu</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Teng</first><last>Xu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Jing</first><last>Zheng</last><affiliation>Ant Group</affiliation></author>
      <pages>225-234</pages>
      <abstract>Through an online customer service application, we have collected many conversations between customer service agents and customers. Building a knowledge production system can help reduce the labor cost of maintaining the FAQ database for the customer service chatbot, whose core module is question answering (QA) on these conversations. However, most existing researches focus on document-based QA tasks, and there is a lack of researches on conversation-based QA and related datasets, especially in Chinese language. The challenges of conversation-based QA include: 1) answers may be scattered among multiple dialogue turns; 2) understanding complex dialogue contexts is more complicated than documents. To address these challenges, we propose a multi-span extraction model on this task and introduce continual pre-training and multi-task learning schemes to further improve model performance. To validate our approach, we construct two Chinese datasets using dialogues as the knowledge source, namely cs-qaconv and kd-qaconv, respectively. Experimental results demonstrate that the proposed model outperforms the baseline on both datasets. The online application also verifies the effectiveness of our method. The dataset kd-qaconv will be released publicly for research purposes.</abstract>
      <url hash="6577d6fd">2023.acl-industry.22</url>
      <bibkey>yang-etal-2023-improving</bibkey>
    </paper>
    <paper id="23">
      <title>Mitigating the Burden of Redundant Datasets via Batch-Wise Unique Samples and Frequency-Aware Losses</title>
      <author><first>Donato</first><last>Crisostomi</last><affiliation>Sapienza University of Rome</affiliation></author>
      <author><first>Andrea</first><last>Caciolai</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Alessandro</first><last>Pedrani</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Kay</first><last>Rottmann</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Alessandro</first><last>Manzotti</last><affiliation>Amazon</affiliation></author>
      <author><first>Enrico</first><last>Palumbo</last><affiliation>Amazon</affiliation></author>
      <author><first>Davide</first><last>Bernardi</last><affiliation>Amazon</affiliation></author>
      <pages>235-247</pages>
      <abstract>Datasets used to train deep learning models in industrial settings often exhibit skewed distributions with some samples repeated a large number of times.This paper presents a simple yet effective solution to reduce the increased burden of repeated computation on redundant datasets.Our approach eliminates duplicates at the batch level, without altering the data distribution observed by the model, making it model-agnostic and easy to implement as a plug-and-play module. We also provide a mathematical expression to estimate the reduction in training time that our approach provides. Through empirical evidence, we show that our approach significantly reduces training times on various models across datasets with varying redundancy factors, without impacting their performance on the Named Entity Recognition task, both on publicly available datasets and in real industrial settings.In the latter, the approach speeds training by up to 87%, and by 46% on average, with a drop in model performance of 0.2% relative at worst.We finally release a modular and reusable codebase to further advance research in this area.</abstract>
      <url hash="363d031b">2023.acl-industry.23</url>
      <bibkey>crisostomi-etal-2023-mitigating</bibkey>
    </paper>
    <paper id="24">
      <title>Distilled Language Models are economically efficient for the enterprise. ...mostly.</title>
      <author><first>Kristen</first><last>Howell</last><affiliation>LivePerson Inc.</affiliation></author>
      <author><first>Gwen</first><last>Christian</last><affiliation>LivePerson</affiliation></author>
      <author><first>Pavel</first><last>Fomitchov</last><affiliation>LivePerson, Inc</affiliation></author>
      <author><first>Gitit</first><last>Kehat</last><affiliation>LivePerson</affiliation></author>
      <author><first>Julianne</first><last>Marzulla</last><affiliation>LivePerson</affiliation></author>
      <author><first>Leanne</first><last>Rolston</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jadin</first><last>Tredup</last><affiliation>LivePerson</affiliation></author>
      <author><first>Ilana</first><last>Zimmerman</last><affiliation>Liveperson</affiliation></author>
      <author><first>Ethan</first><last>Selfridge</last><affiliation>LivePerson</affiliation></author>
      <author><first>Joseph</first><last>Bradley</last><affiliation>LivePerson</affiliation></author>
      <pages>248-267</pages>
      <abstract>Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. With their ability to handle large context windows, Large Language Models (LLMs) are a natural fit for this use case. However, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model’s utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM — prompt engineering, fine-tuning, and knowledge distillation — using feedback from the brand’s customer service agents. We find that the usability of a model’s responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our findings to the broader enterprise space.</abstract>
      <url hash="9506861b">2023.acl-industry.24</url>
      <bibkey>howell-etal-2023-distilled</bibkey>
    </paper>
    <paper id="25">
      <title>Application-Agnostic Language Modeling for On-Device <fixed-case>ASR</fixed-case></title>
      <author><first>Markus</first><last>Nussbaum-thom</last><affiliation>Apple</affiliation></author>
      <author><first>Lyan</first><last>Verwimp</last><affiliation>Apple</affiliation></author>
      <author><first>Youssef</first><last>Oualil</last><affiliation>Apple</affiliation></author>
      <pages>268-275</pages>
      <abstract>On-device automatic speech recognition systems face several challenges compared to server-based systems. They have to meet stricter constraints in terms of speed, disk size and memory while maintaining the same accuracy. Often they have to serve several ap- plications with different distributions at once, such as communicating with a virtual assistant and speech-to-text. The simplest solution to serve multiple applications is to build application-specific (language) models, but this leads to an increase in memory. Therefore, we explore different data- and architecture-driven language modeling approaches to build a single application-agnostic model. We propose two novel feed-forward architectures that find an optimal trade off between different on-device constraints. In comparison to the application-specific solution, one of our novel approaches reduces the disk size by half, while maintaining speed and accuracy of the original model.</abstract>
      <url hash="389e8625">2023.acl-industry.25</url>
      <bibkey>nussbaum-thom-etal-2023-application</bibkey>
    </paper>
    <paper id="26">
      <title>Building Accurate Low Latency <fixed-case>ASR</fixed-case> for Streaming Voice Search in <fixed-case>E</fixed-case>-commerce</title>
      <author><first>Abhinav</first><last>Goyal</last><affiliation>Flipkart</affiliation></author>
      <author><first>Nikesh</first><last>Garera</last><affiliation>Flipkart</affiliation></author>
      <pages>276-283</pages>
      <abstract>Automatic Speech Recognition (ASR) is essential for any voice-based application. The streaming capability of ASR becomes necessary to provide immediate feedback to the user in applications like Voice Search. LSTM/RNN and CTC based ASR systems are very simple to train and deploy for low latency streaming applications but have lower accuracy when compared to the state-of-the-art models. In this work, we build accurate LSTM, attention and CTC based streaming ASR models for large-scale Hinglish (blend of Hindi and English) Voice Search. We evaluate how various modifications in vanilla LSTM training improve the system’s accuracy while preserving the streaming capabilities. We also discuss a simple integration of end-of-speech (EOS) detection with CTC models, which helps reduce the overall search latency. Our model achieves a word error rate (WER) of 3.69% without EOS and 4.78% with EOS, with ~1300 ms (~46.64%) reduction in latency.</abstract>
      <url hash="ab22d7b6">2023.acl-industry.26</url>
      <bibkey>goyal-garera-2023-building</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>PLA</fixed-case>t<fixed-case>E</fixed-case>: A Large-scale Dataset for List Page Web Extraction</title>
      <author><first>Aidan</first><last>San</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Yuan</first><last>Zhuang</last><affiliation>University of Utah</affiliation></author>
      <author><first>Jan</first><last>Bakus</last><affiliation>Amazon</affiliation></author>
      <author><first>Colin</first><last>Lockard</last><affiliation>Amazon</affiliation></author>
      <author><first>David</first><last>Ciemiewicz</last><affiliation>Amazon</affiliation></author>
      <author><first>Sandeep</first><last>Atluri</last><affiliation>Amazon</affiliation></author>
      <author><first>Kevin</first><last>Small</last><affiliation>Amazon</affiliation></author>
      <author><first>Yangfeng</first><last>Ji</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Heba</first><last>Elfardy</last><affiliation>Amazon</affiliation></author>
      <pages>284-294</pages>
      <abstract>Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items encompassing the tasks of: (1) finding product list segmentation boundaries and (2) extracting attributes for each product. PLAtE is composed of 52,898 items collected from 6,694 pages and 156,014 attributes, making it the first large-scale list page web extraction dataset. We use a multi-stage approach to collect and annotate the dataset and adapt three state-of-the-art web extraction models to the two tasks comparing their strengths and weaknesses both quantitatively and qualitatively.</abstract>
      <url hash="471e4c95">2023.acl-industry.27</url>
      <bibkey>san-etal-2023-plate</bibkey>
    </paper>
    <paper id="28">
      <title>Rapid Diffusion: Building Domain-Specific Text-to-Image Synthesizers with Fast Inference Speed</title>
      <author><first>Bingyan</first><last>Liu</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Weifeng</first><last>Lin</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Zhongjie</first><last>Duan</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Chengyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wu</first><last>Ziheng</last><affiliation>Alibaba</affiliation></author>
      <author><first>Zhang</first><last>Zipeng</last><affiliation>Alibaba Inc.</affiliation></author>
      <author><first>Kui</first><last>Jia</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Lianwen</first><last>Jin</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Cen</first><last>Chen</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Jun</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>295-304</pages>
      <abstract>Text-to-Image Synthesis (TIS) aims to generate images based on textual inputs. Recently, several large pre-trained diffusion models have been released to create high-quality images with pre-trained text encoders and diffusion-based image synthesizers. However, popular diffusion-based models from the open-source community cannot support industrial domain-specific applications due to the lack of entity knowledge and low inference speed.In this paper, we propose Rapid Diffusion, a novel framework for training and deploying super-resolution, text-to-image latent diffusion models with rich entity knowledge injected and optimized networks.Furthermore, we employ BladeDISC, an end-to-end Artificial Intelligence (AI) compiler, and FlashAttention techniques to optimize computational graphs of the generated models for online deployment. Experiments verify the effectiveness of our approach in terms of image quality and inference speed. In addition, we present industrial use cases and integrate Rapid Diffusion to an AI platform to show its practical values.</abstract>
      <url hash="98ce2b58">2023.acl-industry.28</url>
      <bibkey>liu-etal-2023-rapid</bibkey>
    </paper>
    <paper id="29">
      <title>Large Scale Generative Multimodal Attribute Extraction for <fixed-case>E</fixed-case>-commerce Attributes</title>
      <author><first>Anant</first><last>Khandelwal</last><affiliation>Amazon</affiliation></author>
      <author><first>Happy</first><last>Mittal</last><affiliation>Amazon</affiliation></author>
      <author><first>Shreyas</first><last>Kulkarni</last><affiliation>Amazon</affiliation></author>
      <author><first>Deepak</first><last>Gupta</last><affiliation>Amazon</affiliation></author>
      <pages>305-312</pages>
      <abstract>E-commerce websites (e.g. Amazon, Alibaba) have a plethora of structured and unstructured information (text and images) present on the product pages. Sellers often don’t label or mislabel values of the attributes (e.g. color, size etc.) for their products. Automatically identifying these attribute values from an eCommerce product page that contains both text and images is a challenging task, especially when the attribute value is not explicitly mentioned in the catalog. In this paper, we present a scalable solution for this problem where we pose attribute extraction problem as a question-answering task, which we solve using MXT, that consists of three key components: (i) MAG (Multimodal Adaptation Gate), (ii) Xception network, and (iii) T5 encoder-decoder. Our system consists of a generative model that generates attribute-values for a given product by using both textual and visual characteristics (e.g. images) of the product. We show that our system is capable of handling zero-shot attribute prediction (when attribute value is not seen in training data) and value-absent prediction (when attribute value is not mentioned in the text) which are missing in traditional classification-based and NER-based models respectively. We have trained our models using distant supervision, removing dependency on human labeling, thus making them practical for real-world applications. With this framework, we are able to train a single model for 1000s of (product-type, attribute) pairs, thus reducing the overhead of training and maintaining separate models. Extensive experiments on two real world datasets (total 57 attributes) show that our framework improves the absolute recall@90P by 10.16% and 6.9{ from the existing state of the art models. In a popular e-commerce store, we have productionized our models that cater to 12K (product-type, attribute) pairs, and have extracted 150MM attribute values.</abstract>
      <url hash="c75af4ac">2023.acl-industry.29</url>
      <bibkey>khandelwal-etal-2023-large</bibkey>
    </paper>
    <paper id="30">
      <title>Consistent Text Categorization using Data Augmentation in e-Commerce</title>
      <author><first>Noa</first><last>Avigdor</last><affiliation>Yahoo Research</affiliation></author>
      <author><first>Guy</first><last>Horowitz</last><affiliation>Technion</affiliation></author>
      <author><first>Ariel</first><last>Raviv</last><affiliation>Yahoo Research</affiliation></author>
      <author><first>Stav</first><last>Yanovsky Daye</last><affiliation>Yahoo Research</affiliation></author>
      <pages>313-321</pages>
      <abstract>The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications.At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model’s output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model’s consistency while maintaining its production-level performance. We use a semi-supervised approach for data augmentation and presents two different methods for utilizing unlabeled samples. One method relies directly on existing catalogs, while the other uses a generative model. We compare the pros and cons of each approach and present our experimental results.</abstract>
      <url hash="86c41c1a">2023.acl-industry.30</url>
      <bibkey>avigdor-etal-2023-consistent</bibkey>
    </paper>
    <paper id="31">
      <title>An efficient method for Natural Language Querying on Structured Data</title>
      <author><first>Hanoz</first><last>Bhathena</last><affiliation>JPMorgan Chase</affiliation></author>
      <author><first>Aviral</first><last>Joshi</last><affiliation>JPMorgan Chase</affiliation></author>
      <author><first>Prateek</first><last>Singh</last><affiliation>JPMorgan Chase</affiliation></author>
      <pages>322-331</pages>
      <abstract>We present an efficient and reliable approach to Natural Language Querying (NLQ) on databases (DB) which is not based on text-to-SQL type semantic parsing. Our approach simplifies the NLQ on structured data problem to the following “bread and butter” NLP tasks: (a) Domain classification, for choosing which DB table to query, whether the question is out-of-scope (b) Multi-head slot/entity extraction (SE) to extract the field criteria and other attributes such as its role (filter, sort etc) from the raw text and (c) Slot value disambiguation (SVD) to resolve/normalize raw spans from SE to format suitable to query a DB. This is a general purpose, DB language agnostic approach and the output can be used to query any DB and return results to the user. Also each of these tasks is extremely well studied, mature, easier to collect data for and enables better error analysis by tracing problems to specific components when something goes wrong.</abstract>
      <url hash="bc971531">2023.acl-industry.31</url>
      <bibkey>bhathena-etal-2023-efficient</bibkey>
    </paper>
    <paper id="32">
      <title>Boosting Transformers and Language Models for Clinical Prediction in Immunotherapy</title>
      <author><first>Zekai</first><last>Chen</last><affiliation>Bristol-Myers Squibb</affiliation></author>
      <author><first>Mariann</first><last>Micsinai Balan</last><affiliation>Bristol-Myers Squibb</affiliation></author>
      <author><first>Kevin</first><last>Brown</last><affiliation>Bristol-Myers Squibb</affiliation></author>
      <pages>332-340</pages>
      <abstract>Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients’ clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases.</abstract>
      <url hash="c3d00b6d">2023.acl-industry.32</url>
      <bibkey>chen-etal-2023-boosting</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>E</fixed-case>volve<fixed-case>MT</fixed-case>: an Ensemble <fixed-case>MT</fixed-case> Engine Improving Itself with Usage Only</title>
      <author><first>Kamer</first><last>Yüksel</last><affiliation>aiXplain, inc.</affiliation></author>
      <author><first>Ahmet</first><last>Gunduz</last><affiliation>aiXplain</affiliation></author>
      <author><first>Mohamed</first><last>Al-badrashiny</last><affiliation>aiXplain inc.</affiliation></author>
      <author><first>Hassan</first><last>Sawaf</last><affiliation>Aixplain, Inc.</affiliation></author>
      <pages>341-346</pages>
      <abstract>This work proposes a method named EvolveMT for the efficient combination of multiple machine translation (MT) engines. The method selects the output from one engine for each segment, using online learning techniques to predict the most appropriate system for each translation request. A neural quality estimation metric supervises the method without requiring reference translations. The method’s online learning capability enables it to adapt to changes in the domain or MT engines dynamically, eliminating the requirement for retraining. The method selects a subset of translation engines to be called based on the source sentence features. The degree of exploration is configurable according to the desired quality-cost trade-off. Results from custom datasets demonstrate that EvolveMT achieves similar translation accuracy at a lower cost than selecting the best translation of each segment from all translations using an MT quality estimator. To the best of our knowledge, EvolveMT is the first MT system that adapts itself after deployment to incoming translation requests from the production environment without needing costly retraining on human feedback.</abstract>
      <url hash="d33255f6">2023.acl-industry.33</url>
      <bibkey>yuksel-etal-2023-evolvemt</bibkey>
    </paper>
    <paper id="34">
      <title>A Static Evaluation of Code Completion by Large Language Models</title>
      <author><first>Hantian</first><last>Ding</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Varun</first><last>Kumar</last><affiliation>Amazon</affiliation></author>
      <author><first>Yuchen</first><last>Tian</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Zijian</first><last>Wang</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Rob</first><last>Kwiatkowski</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Xiaopeng</first><last>Li</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Murali Krishna</first><last>Ramanathan</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Baishakhi</first><last>Ray</last><affiliation>Columbia University</affiliation></author>
      <author><first>Parminder</first><last>Bhatia</last><affiliation>Amazon</affiliation></author>
      <author><first>Sudipta</first><last>Sengupta</last><affiliation>Amazon</affiliation></author>
      <pages>347-360</pages>
      <abstract>Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the other hand, static analysis tools such as linters, which can detect errors without running the program, haven’t been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models.Through extensive studies, we also show the impact of sampling temperature, model size, and context on static errors in code completions.</abstract>
      <url hash="37f63dc3">2023.acl-industry.34</url>
      <bibkey>ding-etal-2023-static</bibkey>
    </paper>
    <paper id="35">
      <title>Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems</title>
      <author><first>Sarthak</first><last>Ahuja</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Mohammad</first><last>Kachuee</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Fatemeh</first><last>Sheikholeslami</last><affiliation>Amazon</affiliation></author>
      <author><first>Weiqing</first><last>Liu</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Jaeyoung</first><last>Do</last><affiliation>Amazon Alexa AI</affiliation></author>
      <pages>361-367</pages>
      <abstract>Off-Policy reinforcement learning has been the driving force for the state-of-the-art conversational AIs leading to more natural human-agent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable long-term policy improvements.</abstract>
      <url hash="831bb1e6">2023.acl-industry.35</url>
      <bibkey>ahuja-etal-2023-scalable</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>M</fixed-case>obile<fixed-case>NMT</fixed-case>: Enabling Translation in 15<fixed-case>MB</fixed-case> and 30ms</title>
      <author><first>Ye</first><last>Lin</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xiaohui</first><last>Wang</last><affiliation>Bytedance AI Lab</affiliation></author>
      <author><first>Zhexi</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Mingxuan</first><last>Wang</last><affiliation>Bytedance AI Lab</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jingbo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <pages>368-378</pages>
      <abstract>Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuses on a single metric such as FLOPs or general engine which is not good at auto-regressive decoding. In this paper, we present MobileNMT, a system that can translate in 15MB and 30ms on devices. We propose a series of principles for model compression when combined with quantization. Further, we implement an engine that is friendly to INT8 and decoding. With the co-design of model and engine, compared with the existing system, we speed up 47.0x and save 99.5% of memory with only 11.6% loss of BLEU. Our code will be publicly available after the anonymity period.</abstract>
      <url hash="accd596c">2023.acl-industry.36</url>
      <bibkey>lin-etal-2023-mobilenmt</bibkey>
    </paper>
    <paper id="37">
      <title>Multi-doc Hybrid Summarization via Salient Representation Learning</title>
      <author><first>Min</first><last>Xiao</last><affiliation>Microsoft</affiliation></author>
      <pages>379-389</pages>
      <abstract>Multi-document summarization is gaining more and more attention recently and serves as an invaluable tool to obtain key facts among a large information pool. In this paper, we proposed a multi-document hybrid summarization approach, which simultaneously generates a human-readable summary and extracts corresponding key evidences based on multi-doc inputs. To fulfill that purpose, we crafted a salient representation learning method to induce latent salient features, which are effective for joint evidence extraction and summary generation. In order to train this model, we conducted multi-task learning to optimize a composited loss, constructed over extractive and abstractive sub-components in a hierarchical way. We implemented the system based on a ubiquiotously adopted transformer architecture and conducted experimental studies on multiple datasets across two domains, achieving superior performance over the baselines.</abstract>
      <url hash="698f862a">2023.acl-industry.37</url>
      <bibkey>xiao-2023-multi</bibkey>
    </paper>
    <paper id="38">
      <title><fixed-case>S</fixed-case>a<fixed-case>FER</fixed-case>: A Robust and Efficient Framework for Fine-tuning <fixed-case>BERT</fixed-case>-based Classifier with Noisy Labels</title>
      <author><first>Zhenting</first><last>Qi</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xiaoyu</first><last>Tan</last><affiliation>INF Technology (Shanghai) Co., Ltd.</affiliation></author>
      <author><first>Chao</first><last>Qu</last><affiliation>INF Technology (Shanghai) Co., Ltd.</affiliation></author>
      <author><first>Yinghui</first><last>Xu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yuan</first><last>Qi</last><affiliation>Fudan University</affiliation></author>
      <pages>390-403</pages>
      <abstract>Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100% accurate labels is difficult, thus many datasets are accompanied by label noise at different levels. Previous work has shown that existing noise-handling methods could not improve the peak performance of BERT on noisy datasets, and might even deteriorate it. In this paper, we propose SaFER, a robust and efficient fine-tuning framework for BERT-based text classifiers, combating label noises without access to any clean data for training or validation. Utilizing a label-agnostic early-stopping strategy and self-supervised learning, our proposed framework achieves superior performance in terms of both accuracy and speed on multiple text classification benchmarks. The trained model is finally fully deployed in several industrial biomedical literature mining tasks and demonstrates high effectiveness and efficiency.</abstract>
      <url hash="749c4dcd">2023.acl-industry.38</url>
      <bibkey>qi-etal-2023-safer</bibkey>
    </paper>
    <paper id="39">
      <title>Chemical Language Understanding Benchmark</title>
      <author><first>Yunsoo</first><last>Kim</last><affiliation>LG Chem</affiliation></author>
      <author><first>Hyuk</first><last>Ko</last><affiliation>LG Chem</affiliation></author>
      <author><first>Jane</first><last>Lee</last><affiliation>LG Chem</affiliation></author>
      <author><first>Hyun Young</first><last>Heo</last><affiliation>LG Chem</affiliation></author>
      <author><first>Jinyoung</first><last>Yang</last><affiliation>LG Chem</affiliation></author>
      <author><first>Sungsoo</first><last>Lee</last><affiliation>LG Chem</affiliation></author>
      <author><first>Kyu-hwang</first><last>Lee</last><affiliation>LG Chem</affiliation></author>
      <pages>404-411</pages>
      <abstract>In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of chemical language understanding benchmark datasets consisted of tasks for both patent and literature articles provided by industrial organization. All the datasets are internally made by chemists from scratch. Finally, we evaluate the datasets on the various language models based on BERT and RoBERTa, and demonstrate the model performs better when the domain of the pretrained models are closer to chemistry domain. We provide baselines for our benchmark as 0.8054 in average, and we hope this benchmark is used by many researchers in both industry and academia.</abstract>
      <url hash="bebf1bf6">2023.acl-industry.39</url>
      <bibkey>kim-etal-2023-chemical</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>H</fixed-case>yper<fixed-case>T</fixed-case>5: Towards Compute-Efficient <fixed-case>K</fixed-case>orean Language Modeling</title>
      <author><first>Dongju</first><last>Park</last><affiliation>NAVER Clova</affiliation></author>
      <author><first>Soonwon</first><last>Ka</last><affiliation>Naver</affiliation></author>
      <author><first>Kang Min</first><last>Yoo</last><affiliation>NAVER AI Lab</affiliation></author>
      <author><first>Gichang</first><last>Lee</last><affiliation>NAVER Cloud</affiliation></author>
      <author><first>Jaewook</first><last>Kang</last><affiliation>NAVER Search</affiliation></author>
      <pages>412-424</pages>
      <abstract>Pretraining and fine-tuning language models have become the standard practice in industrial natural language processing (NLP), but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or communities whose main focus is languages with less accessible resources (e.g., non-English). This paper explores the sequence-to-sequence (seq2seq) language model architecture as a more practical and compute-efficient alternative to the decoder-oriented approach (e.g., GPT-3), accompanied by novel findings in compute-optimality analyses. We successfully trained billion-scale Korean-language seq2seq language models that strongly outperform other competitive models in Korean benchmarks. Moreover, we demonstrate that such language models can be more efficiently utilized by employing a heavy pre-finetuning strategy, by showcasing a case study on dialog-task adaptation. Our case study shows that adopting language models with more readily available domain-specific unlabeled data greatly improves fine-tuning data efficiency in low-resource settings.</abstract>
      <url hash="02f6996b">2023.acl-industry.40</url>
      <bibkey>park-etal-2023-hypert5</bibkey>
    </paper>
    <paper id="41">
      <title>Semantic Ambiguity Detection in Sentence Classification using Task-Specific Embeddings</title>
      <author><first>Jong Myoung</first><last>Kim</last><affiliation>SK Telecom</affiliation></author>
      <author><first>Young-jun</first><last>Lee</last><affiliation>Kaist</affiliation></author>
      <author><first>Sangkeun</first><last>Jung</last><affiliation>Chungnam National University</affiliation></author>
      <author><first>Ho-jin</first><last>Choi</last><affiliation>Kaist</affiliation></author>
      <pages>425-437</pages>
      <abstract>Ambiguity is a major obstacle to providing services based on sentence classification. However, because of the structural limitations of the service, there may not be sufficient contextual information to resolve the ambiguity. In this situation, we focus on ambiguity detection so that service design considering ambiguity is possible. We utilize similarity in a semantic space to detect ambiguity in service scenarios and training data. In addition, we apply task-specific embedding to improve performance. Our results demonstrate that ambiguities and resulting labeling errors in training data or scenarios can be detected. Additionally, we confirm that it can be used to debug services</abstract>
      <url hash="90eddd1f">2023.acl-industry.41</url>
      <bibkey>kim-etal-2023-semantic</bibkey>
    </paper>
    <paper id="42">
      <title>Reliable and Interpretable Drift Detection in Streams of Short Texts</title>
      <author><first>Ella</first><last>Rabinovich</last><affiliation>IBM Research</affiliation></author>
      <author><first>Matan</first><last>Vetzler</last><affiliation>IBM Research</affiliation></author>
      <author><first>Samuel</first><last>Ackerman</last><affiliation>IBM Research</affiliation></author>
      <author><first>Ateret</first><last>Anaby Tavor</last><affiliation>IBM Research AI</affiliation></author>
      <pages>438-446</pages>
      <abstract>Data drift is the change in model input data that is one of the key factors leading to machine learning models performance degradation over time. Monitoring drift helps detecting these issues and preventing their harmful consequences. Meaningful drift interpretation is a fundamental step towards effective re-training of the model. In this study we propose an end-to-end framework for reliable model-agnostic change-point detection and interpretation in large task-oriented dialog systems, proven effective in multiple customer deployments. We evaluate our approach and demonstrate its benefits with a novel variant of intent classification training dataset, simulating customer requests to a dialog system. We make the data publicly available.</abstract>
      <url hash="45b0beb2">2023.acl-industry.42</url>
      <bibkey>rabinovich-etal-2023-reliable</bibkey>
    </paper>
    <paper id="43">
      <title>Sharing Encoder Representations across Languages, Domains and Tasks in Large-Scale Spoken Language Understanding</title>
      <author><first>Jonathan</first><last>Hueser</last><affiliation>Amazon</affiliation></author>
      <author><first>Judith</first><last>Gaspers</last><affiliation>Amazon</affiliation></author>
      <author><first>Thomas</first><last>Gueudre</last><affiliation>Amazon</affiliation></author>
      <author><first>Chandana</first><last>Prakash</last><affiliation>Amazon</affiliation></author>
      <author><first>Jin</first><last>Cao</last><affiliation>Apple</affiliation></author>
      <author><first>Daniil</first><last>Sorokin</last><affiliation>Amazon</affiliation></author>
      <author><first>Quynh</first><last>Do</last><affiliation>Amazon</affiliation></author>
      <author><first>Nicolas</first><last>Anastassacos</last><affiliation>Amazon</affiliation></author>
      <author><first>Tobias</first><last>Falke</last><affiliation>Amazon</affiliation></author>
      <author><first>Turan</first><last>Gojayev</last><affiliation>Amazon</affiliation></author>
      <pages>447-456</pages>
      <abstract>Leveraging representations from pre-trained transformer-based encoders achieves state-of-the-art performance on numerous NLP tasks.Larger encoders can improve accuracy for spoken language understanding (SLU) but are challenging to use given the inference latency constraints of online systems (especially on CPU machines).We evaluate using a larger 170M parameter BERT encoder that shares representations across languages, domains and tasks for SLU compared to using smaller 17M parameter BERT encoders with language-, domain- and task-decoupled finetuning.Running inference with a larger shared encoder on GPU is latency neutral and reduces infrastructure cost compared to running inference for decoupled smaller encoders on CPU machines.The larger shared encoder reduces semantic error rates by 4.62% for test sets representing user requests to voice-controlled devices and 5.79% on the tail of the test sets on average across four languages.</abstract>
      <url hash="8698ab3a">2023.acl-industry.43</url>
      <bibkey>hueser-etal-2023-sharing</bibkey>
    </paper>
    <paper id="44">
      <title>Annotating Research Infrastructure in Scientific Papers: An <fixed-case>NLP</fixed-case>-driven Approach</title>
      <author><first>Seyed Amin</first><last>Tabatabaei</last><affiliation>Elsevier</affiliation></author>
      <author><first>Georgios</first><last>Cheirmpos</last><affiliation>Elsevier</affiliation></author>
      <author><first>Marius</first><last>Doornenbal</last><affiliation>Elsevier</affiliation></author>
      <author><first>Alberto</first><last>Zigoni</last><affiliation>Elsevier</affiliation></author>
      <author><first>Veronique</first><last>Moore</last><affiliation>Elsevier</affiliation></author>
      <author><first>Georgios</first><last>Tsatsaronis</last><affiliation>Elsevier</affiliation></author>
      <pages>457-463</pages>
      <abstract>In this work, we present a natural language processing (NLP) pipeline for the identification, extraction and linking of Research Infrastructure (RI) used in scientific publications. Links between scientific equipment and publications where the equipment was used can support multiple use cases, such as evaluating the impact of RI investment, and supporting Open Science and research reproducibility. These links can also be used to establish a profile of the RI portfolio of each institution and associate each equipment with scientific output. The system we are describing here is already in production, and has been used to address real business use cases, some of which we discuss in this paper. The computational pipeline at the heart of the system comprises both supervised and unsupervised modules to detect the usage of research equipment by processing the full text of the articles. Additionally, we have created a knowledge graph of RI, which is utilized to annotate the articles with metadata. Finally, examples of the business value of the insights made possible by this NLP pipeline are illustrated.</abstract>
      <url hash="23df447e">2023.acl-industry.44</url>
      <bibkey>tabatabaei-etal-2023-annotating</bibkey>
    </paper>
    <paper id="45">
      <title>Event-Centric Query Expansion in Web Search</title>
      <author><first>Yanan</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Weijie</first><last>Cui</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yangfan</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Xiaoling</first><last>Bai</last><affiliation>Tencent</affiliation></author>
      <author><first>Zhe</first><last>Zhang</last><affiliation>Ustc.edu</affiliation></author>
      <author><first>Jin</first><last>Ma</last><affiliation>Ustc</affiliation></author>
      <author><first>Xiang</first><last>Chen</last><affiliation>Tencent</affiliation></author>
      <author><first>Tianhua</first><last>Zhou</last><affiliation>Tencent</affiliation></author>
      <pages>464-475</pages>
      <abstract>In search engines, query expansion (QE) is a crucial technique to improve search experience. Previous studies often rely on long-term search log mining, which leads to slow updates and is sub-optimal for time-sensitive news searches. In this work, we present Event-Centric Query Expansion (EQE), the QE system used in a famous Chinese search engine. EQE utilizes a novel event retrieval framework that consists of four stages, i.e., event collection, event reformulation, semantic retrieval and online ranking, which can select the best expansion from a significant amount of potential events rapidly and accurately. Specifically, we first collect and filter news headlines from websites. Then we propose a generation model that incorporates contrastive learning and prompt-tuning techniques to reformulate these headlines to concise candidates. Additionally, we fine-tune a dual-tower semantic model to serve as an encoder for event retrieval and explore a two-stage contrastive training approach to enhance the accuracy of event retrieval. Finally, we rank the retrieved events and select the optimal one as QE, which is then used to improve the retrieval of event-related documents. Through offline analysis and online A/B testing, we observed that the EQE system has significantly improved many indicators compared to the baseline. The system has been deployed in a real production environment and serves hundreds of millions of users.</abstract>
      <url hash="2b4bc720">2023.acl-industry.45</url>
      <bibkey>zhang-etal-2023-event</bibkey>
    </paper>
    <paper id="46">
      <title>Transferable and Efficient: Unifying Dynamic Multi-Domain Product Categorization</title>
      <author><first>Shansan</first><last>Gong</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Zelin</first><last>Zhou</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Shuo</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Fengjiao</first><last>Chen</last><affiliation>Meituan</affiliation></author>
      <author><first>Xiujie</first><last>Song</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Xuezhi</first><last>Cao</last><affiliation>Meituan</affiliation></author>
      <author><first>Yunsen</first><last>Xian</last><affiliation>Meituan</affiliation></author>
      <author><first>Kenny</first><last>Zhu</last><affiliation>University of Texas at Arlington</affiliation></author>
      <pages>476-486</pages>
      <abstract>As e-commerce platforms develop different business lines, a special but challenging product categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time. In order to unify the categorization process and ensure efficiency, we propose a two-stage taxonomy-agnostic framework that relies solely on calculating the semantic relatedness between product titles and category names in the vector space. To further enhance domain transferability and better exploit cross-domain data, we design two plug-in modules: a heuristic mapping scorer and a pretrained contrastive ranking module with the help of meta concepts, which represent keyword knowledge shared across domains.Comprehensive offline experiments show that our method outperforms strong baselineson three dynamic multi-domain product categorization (DMPC) tasks,and online experiments reconfirm its efficacy with a5% increase on seasonal purchase revenue. Related datasets will be released.</abstract>
      <url hash="ce944cd3">2023.acl-industry.46</url>
      <bibkey>gong-etal-2023-transferable</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>DISCOSQA</fixed-case>: A Knowledge Base Question Answering System for Space Debris based on Program Induction</title>
      <author><first>Paul</first><last>Darm</last><affiliation>University of Strathclyde</affiliation></author>
      <author><first>Antonio Valerio</first><last>Miceli Barone</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Shay B.</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Annalisa</first><last>Riccardi</last><affiliation>University of Strathclyde</affiliation></author>
      <pages>487-499</pages>
      <abstract>Space program agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems.Knowledge Base (KB) databases are an effective way of storing and accessing such information to scale.In this work we present a system, developed for the European Space Agency, that can answer complex natural language queries, to support engineers in accessing the information contained in a KB that models the orbital space debris environment. Our system is based on a pipeline which first generates a program sketch from a natural language question, then specializes the sketch into a concrete query program with mentions of entities, attributes and relations, and finally executes the program against the database.This pipeline decomposition approach enables us to train the system by leveraging out-of-domain data and semi-synthetic data generated by GPT-3, thus reducing overfitting and shortcut learning even with limited amount of in-domain training data.</abstract>
      <url hash="8068407c">2023.acl-industry.47</url>
      <bibkey>darm-etal-2023-discosqa</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>BADGE</fixed-case>: Speeding Up <fixed-case>BERT</fixed-case> Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting</title>
      <author><first>Wei</first><last>Zhu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Peng</first><last>Wang</last><affiliation>Paht</affiliation></author>
      <author><first>Yuan</first><last>Ni</last><affiliation>PingAn</affiliation></author>
      <author><first>Guotong</first><last>Xie</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Xiaoling</first><last>Wang</last><affiliation>East China Normal University</affiliation></author>
      <pages>500-509</pages>
      <abstract>Early exiting can reduce the average latency of pre-trained language models (PLMs) via its adaptive inference mechanism and work with other inference speed-up methods like model pruning, thus drawing much attention from the industry. In this work, we propose a novel framework, BADGE, which consists of two off-the-shelf methods for improving PLMs’ early exiting. We first address the issues of training a multi-exit PLM, the backbone model for early exiting. We propose the novel architecture of block-wise bypasses, which can alleviate the conflicts in jointly training multiple intermediate classifiers and thus improve the overall performances of multi-exit PLM while introducing negligible additional flops to the model. Second, we propose a novel divergence-based early exiting (DGE) mechanism, which obtains early exiting signals by comparing the predicted distributions of two adjacent layers’ exits. Extensive experiments on three proprietary datasets and three GLUE benchmark tasks demonstrate that our method can obtain a better speedup-performance trade-off than the existing baseline methods.{footnote{Code will be made publicly available to the research community upon acceptance.}</abstract>
      <url hash="c98f0f33">2023.acl-industry.48</url>
      <bibkey>zhu-etal-2023-badge</bibkey>
    </paper>
    <paper id="49">
      <title>K-pop and fake facts: from texts to smart alerting for maritime security</title>
      <author><first>Maxime</first><last>Prieur</last><affiliation>Airbus</affiliation></author>
      <author><first>Souhir</first><last>Gahbiche</last><affiliation>Airbus</affiliation></author>
      <author><first>Guillaume</first><last>Gadek</last><affiliation>Airbus</affiliation></author>
      <author><first>Sylvain</first><last>Gatepaille</last><affiliation>AIRBUS Defence and Space</affiliation></author>
      <author><first>Kilian</first><last>Vasnier</last><affiliation>Airbus</affiliation></author>
      <author><first>Valerian</first><last>Justine</last><affiliation>Airbus</affiliation></author>
      <pages>510-517</pages>
      <abstract>Maritime security requires full-time monitoring of the situation, mainly based on technical data (radar, AIS) but also from OSINT-like inputs (e.g., newspapers). Some threats to the operational reliability of this maritime surveillance, such as malicious actors, introduce discrepancies between hard and soft data (sensors and texts), either by tweaking their AIS emitters or by emitting false information on pseudo-newspapers.Many techniques exist to identify these pieces of false information, including using knowledge base population techniques to build a structured view of the information. This paper presents a use case for suspect data identification in a maritime setting. The proposed system UMBAR ingests data from sensors and texts, processing them through an information extraction step, in order to feed a Knowledge Base and finally perform coherence checks between the extracted facts.</abstract>
      <url hash="53e9ff69">2023.acl-industry.49</url>
      <bibkey>prieur-etal-2023-k</bibkey>
    </paper>
    <paper id="50">
      <title>Evaluating Embedding <fixed-case>API</fixed-case>s for Information Retrieval</title>
      <author><first>Ehsan</first><last>Kamalloo</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Xinyu</first><last>Zhang</last><affiliation>David R. Cheriton School of Computer Science, University of Waterloo</affiliation></author>
      <author><first>Odunayo</first><last>Ogundepo</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Nandan</first><last>Thakur</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>David</first><last>Alfonso-hermelo</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last><affiliation>Noah’s Ark Lab Huawei</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>518-526</pages>
      <abstract>The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vector representations of a given text. With a growing number of APIs at our disposal, in this paper, our goal is to analyze semantic embedding APIs in realistic retrieval scenarios in order to assist practitioners and researchers in finding suitable services according to their needs. Specifically, we wish to investigate the capabilities of existing APIs on domain generalization and multilingual retrieval. For this purpose, we evaluate the embedding APIs on two standard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 results using the APIs is a budget-friendly approach and is most effective on English, in contrast to the standard practice, i.e., employing them as first-stage retrievers. For non-English retrieval, re-ranking still improves the results, but a hybrid model with BM25 works best albeit at a higher cost. We hope our work lays the groundwork for thoroughly evaluating APIs that are critical in search and more broadly, in information retrieval.</abstract>
      <url hash="3bf39f9c">2023.acl-industry.50</url>
      <bibkey>kamalloo-etal-2023-evaluating</bibkey>
    </paper>
    <paper id="51">
      <title>Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform</title>
      <author><first>Mateusz</first><last>Wójcik</last><affiliation>Wroclaw University of Science and Technology. Alphamoon Ltd.</affiliation></author>
      <author><first>Witold</first><last>Kościukiewicz</last><affiliation>Wrocław University of Science and Technology</affiliation></author>
      <author><first>Mateusz</first><last>Baran</last><affiliation>Wrocław University of Science and Technology</affiliation></author>
      <author><first>Tomasz</first><last>Kajdanowicz</last><affiliation>Wroclaw University of Science and Technology</affiliation></author>
      <author><first>Adam</first><last>Gonczarek</last><affiliation>Alphamoon Ltd.</affiliation></author>
      <pages>527-537</pages>
      <abstract>Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. Particularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic gradient learning have been shown to struggle in such setups or have limitations like memory buffers, and being restricted to specific domains that disable its usage in real-world scenarios. For this reason, we present a fully differentiable architecture based on the Mixture of Experts model, that enables the training of high-performance classifiers when examples from each class are presented separately. We conducted exhaustive experiments that proved its applicability in various domains and ability to learn online in production environments. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods.</abstract>
      <url hash="e08c8703">2023.acl-industry.51</url>
      <bibkey>wojcik-etal-2023-domain</bibkey>
    </paper>
    <paper id="52">
      <title>Regression-Free Model Updates for Spoken Language Understanding</title>
      <author><first>Andrea</first><last>Caciolai</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Verena</first><last>Weber</last><affiliation>Amazon</affiliation></author>
      <author><first>Tobias</first><last>Falke</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Alessandro</first><last>Pedrani</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Davide</first><last>Bernardi</last><affiliation>Amazon</affiliation></author>
      <pages>538-551</pages>
      <abstract>In real-world systems, an important requirement for model updates is to avoid regressions in user experience caused by flips of previously correct classifications to incorrect ones. Multiple techniques for that have been proposed in the recent literature. In this paper, we apply one such technique, focal distillation, to model updates in a goal-oriented dialog system and assess its usefulness in practice. In particular, we evaluate its effectiveness for key language understanding tasks, including sentence classification and sequence labeling tasks, we further assess its effect when applied to repeated model updates over time, and test its compatibility with mislabeled data. Our experiments on a public benchmark and data from a deployed dialog system demonstrate that focal distillation can substantially reduce regressions, at only minor drops in accuracy, and that it further outperforms naive supervised training in challenging mislabeled data and label expansion settings.</abstract>
      <url hash="bdfb772c">2023.acl-industry.52</url>
      <bibkey>caciolai-etal-2023-regression</bibkey>
    </paper>
    <paper id="53">
      <title>Reducing cohort bias in natural language understanding systems with targeted self-training scheme</title>
      <author><first>Dieu-thu</first><last>Le</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Gabriela</first><last>Hernandez</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Bei</first><last>Chen</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Melanie</first><last>Bradford</last><affiliation>Amazon Alexa AI</affiliation></author>
      <pages>552-560</pages>
      <abstract>Bias in machine learning models can be an issue when the models are trained on particular types of data that do not generalize well, causing under performance in certain groups of users. In this work, we focus on reducing the bias related to new customers in a digital voice assistant system. It is observed that natural language understanding models often have lower performance when dealing with requests coming from new users rather than experienced users. To mitigate this problem, we propose a framework that consists of two phases (1) a fixing phase with four active learning strategies used to identify important samples coming from new users, and (2) a self training phase where a teacher model trained from the first phase is used to annotate semi-supervised samples to expand the training data with relevant cohort utterances. We explain practical strategies that involve an identification of representative cohort-based samples through density clustering as well as employing implicit customer feedbacks to improve new customers’ experience. We demonstrate the effectiveness of our approach in a real world large scale voice assistant system for two languages, German and French through both offline experiments as well as A/B testings.</abstract>
      <url hash="abedd3f6">2023.acl-industry.53</url>
      <bibkey>le-etal-2023-reducing</bibkey>
    </paper>
    <paper id="54">
      <title>Content Moderation for Evolving Policies using Binary Question Answering</title>
      <author><first>Sankha Subhra</first><last>Mullick</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Mohan</first><last>Bhambhani</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Suhit</first><last>Sinha</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Akshat</first><last>Mathur</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Somya</first><last>Gupta</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Jidnya</first><last>Shah</last><affiliation>LinkedIn</affiliation></author>
      <pages>561-573</pages>
      <abstract>Content moderation on social media is governed by policies that are intricate and frequently updated with evolving world events. However, automated content moderation systems often restrict easy adaptation to policy changes and are expected to learn policy intricacies from limited amounts of labeled data, which make effective policy compliance challenging. We propose to model content moderation as a binary question answering problem where the questions validate the loosely coupled themes constituting a policy. A decision logic is applied on top to aggregate the theme-specific validations. This way the questions pass theme information to a transformer network as explicit policy prompts, that in turn enables explainability. This setting further allows for faster adaptation to policy updates by leveraging zero-shot capabilities of pre-trained transformers. We showcase improved recall for our proposed method at 95{% precision on two proprietary datasets of social media posts and comments respectively annotated under curated Hate Speech and Commercial Spam policies.</abstract>
      <url hash="6eedde4e">2023.acl-industry.54</url>
      <bibkey>mullick-etal-2023-content</bibkey>
    </paper>
    <paper id="55">
      <title>Weighted Contrastive Learning With False Negative Control to Help Long-tailed Product Classification</title>
      <author><first>Tianqi</first><last>Wang</last><affiliation>University at Buffalo</affiliation></author>
      <author><first>Lei</first><last>Chen</last><affiliation>Rakuten</affiliation></author>
      <author><first>Xiaodan</first><last>Zhu</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Younghun</first><last>Lee</last><affiliation>Purdue University</affiliation></author>
      <author><first>Jing</first><last>Gao</last><affiliation>Purdue University</affiliation></author>
      <pages>574-580</pages>
      <abstract>Item categorization (IC) aims to classify product descriptions into leaf nodes in a categorical taxonomy, which is a key technology used in a wide range of applications. Along with the fact that most datasets often has a long-tailed distribution, classification performances on tail labels tend to be poor due to scarce supervision, causing many issues in real-life applications. To address IC task’s long-tail issue, K-positive contrastive loss (KCL) is proposed on image classification task and can be applied on the IC task when using text-based contrastive learning, e.g., SimCSE. However, one shortcoming of using KCL has been neglected in previous research: false negative (FN) instances may harm the KCL’s representation learning. To address the FN issue in the KCL, we proposed to re-weight the positive pairs in the KCL loss with a regularization that the sum of weights should be constrained to K+1 as close as possible. After controlling FN instances with the proposed method, IC performance has been further improved and is superior to other LT-addressing methods.</abstract>
      <url hash="621d6ccd">2023.acl-industry.55</url>
      <bibkey>wang-etal-2023-weighted</bibkey>
    </paper>
    <paper id="56">
      <title>Towards Building a Robust Toxicity Predictor</title>
      <author><first>Dmitriy</first><last>Bespalov</last><affiliation>Amazon</affiliation></author>
      <author><first>Sourav</first><last>Bhabesh</last><affiliation>Amazon</affiliation></author>
      <author><first>Yi</first><last>Xiang</last><affiliation>Amazon</affiliation></author>
      <author><first>Liutong</first><last>Zhou</last><affiliation>Amazon</affiliation></author>
      <author><first>Yanjun</first><last>Qi</last><affiliation>Faculty, CS, UVA</affiliation></author>
      <pages>581-598</pages>
      <abstract>Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, {texttt{ToxicTrap}, introducing small word-level perturbations to fool SOTA text classifiers to predict toxic text samples as benign. {texttt{ToxicTrap} exploits greedy based search strategies to enable fast and effective generation of toxic adversarial examples. Two novel goal function designs allow {texttt{ToxicTrap} to identify weaknesses in both multiclass and multilabel toxic language detectors. Our empirical results show that SOTA toxicity text classifiers are indeed vulnerable to the proposed attacks, attaining over 98{% attack success rates in multilabel cases. We also show how a vanilla adversarial training and its improved version can help increase robustness of a toxicity detector even against unseen attacks.</abstract>
      <url hash="e3d9085b">2023.acl-industry.56</url>
      <bibkey>bespalov-etal-2023-towards</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>AI</fixed-case> Coach Assist: An Automated Approach for Call Recommendation in Contact Centers for Agent Coaching</title>
      <author><first>Md Tahmid Rahman</first><last>Laskar</last><affiliation>Dialpad Inc</affiliation></author>
      <author><first>Cheng</first><last>Chen</last><affiliation>Dialpad Inc</affiliation></author>
      <author><first>Xue-yong</first><last>Fu</last><affiliation>Dialpad Inc</affiliation></author>
      <author><first>Mahsa</first><last>Azizi</last><affiliation>Dialpad Inc</affiliation></author>
      <author><first>Shashi</first><last>Bhushan</last><affiliation>Dialpad Inc</affiliation></author>
      <author><first>Simon</first><last>Corston-oliver</last><affiliation>Dialpad Inc</affiliation></author>
      <pages>599-607</pages>
      <abstract>In recent years, the utilization of Artificial Intelligence (AI) in the contact center industry is on the rise. One area where AI can have a significant impact is in the coaching of contact center agents. By analyzing call transcripts, AI can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor. In this paper, we present “AI Coach Assis”, which leverages the pre-trained transformer-based language models to determine whether a given call is coachable or not based on the quality assurance (QA) queries/questions asked by the contact center managers or supervisors. The system was trained and evaluated on a large dataset collected from real-world contact centers and provides an efficient and effective way to determine which calls are most relevant for coaching purposes. Extensive experimental evaluation demonstrates the potential of AI Coach Assist to improve the coaching process, resulting in enhancing the performance of contact center agents.</abstract>
      <url hash="cd7e1cc3">2023.acl-industry.57</url>
      <bibkey>laskar-etal-2023-ai</bibkey>
    </paper>
    <paper id="58">
      <title>Unified Contextual Query Rewriting</title>
      <author><first>Yingxue</first><last>Zhou</last><affiliation>Amazon</affiliation></author>
      <author><first>Jie</first><last>Hao</last><affiliation>Amazon</affiliation></author>
      <author><first>Mukund</first><last>Rungta</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Eunah</first><last>Cho</last><affiliation>Amazon, Alexa AI</affiliation></author>
      <author><first>Xing</first><last>Fan</last><affiliation>Amazon Corporation</affiliation></author>
      <author><first>Yanbin</first><last>Lu</last><affiliation>Amazon</affiliation></author>
      <author><first>Vishal</first><last>Vasudevan</last><affiliation>Amazon</affiliation></author>
      <author><first>Kellen</first><last>Gillespie</last><affiliation>Amazon</affiliation></author>
      <author><first>Zeynab</first><last>Raeesy</last><affiliation>Amazon</affiliation></author>
      <pages>608-615</pages>
      <abstract>Query rewriting (QR) is an important technique for user friction (i.e. recovering ASR error or system error) reduction and contextual carryover (i.e. ellipsis and co-reference) in conversational AI systems. Recently, generation-based QR models have achieved promising results on these two tasks separately. Although these two tasks have many similarities such as they both use the previous dialogue along with the current request as model input, there is no unified model to solve them jointly. To this end, we propose a unified contextual query rewriting model that unifies QR for both reducing friction and contextual carryover purpose. Moreover, we involve multiple auxiliary tasks such as trigger prediction and NLU interpretation tasks to boost the performance of the rewrite. We leverage the text-to-text unified framework which uses independent tasks with weighted loss to account for task importance. Then we propose new unified multitask learning strategies including a sequential model which outputs one sentence for multi-tasks, and a hybrid model where some tasks are independent and some tasks are sequentially generated. Our experimental results demonstrate the effectiveness of the proposed unified learning methods.</abstract>
      <url hash="e141bf9c">2023.acl-industry.58</url>
      <bibkey>zhou-etal-2023-unified</bibkey>
    </paper>
    <paper id="59">
      <title>Context-Aware Query Rewriting for Improving Users’ Search Experience on <fixed-case>E</fixed-case>-commerce Websites</title>
      <author><first>Simiao</first><last>Zuo</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Qingyu</first><last>Yin</last><affiliation>Amazon</affiliation></author>
      <author><first>Haoming</first><last>Jiang</last><affiliation>Amazon Search</affiliation></author>
      <author><first>Shaohui</first><last>Xi</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Bing</first><last>Yin</last><affiliation>Amazon.com</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Tech</affiliation></author>
      <author><first>Tuo</first><last>Zhao</last><affiliation>Georgia Tech</affiliation></author>
      <pages>616-628</pages>
      <abstract>E-commerce queries are often short and ambiguous. Consequently, query understanding often uses query rewriting to disambiguate user-input queries. While using e-commerce search tools, users tend to enter multiple searches, which we call context, before purchasing. These history searches contain contextual insights about users’ true shopping intents. Therefore, modeling such contextual information is critical to a better query rewriting model. However, existing query rewriting models ignore users’ history behaviors and consider only the instant search query, which is often a short string offering limited information about the true shopping intent.We propose an end-to-end context-aware query rewriting model to bridge this gap, which takes the search context into account. Specifically, our model builds a session graph using the history search queries and their contained words. We then employ a graph attention mechanism that models cross-query relations and computes contextual information of the session. The model subsequently calculates session representations by combining the contextual information with the instant search query using an aggregation network. The session representations are then decoded to generate rewritten queries. Empirically, we demonstrate the superiority of our method to state-of-the-art approaches under various metrics.</abstract>
      <url hash="c60a5e74">2023.acl-industry.59</url>
      <bibkey>zuo-etal-2023-context</bibkey>
    </paper>
    <paper id="60">
      <title>Federated Learning of Gboard Language Models with Differential Privacy</title>
      <author><first>Zheng</first><last>Xu</last><affiliation>Google Research</affiliation></author>
      <author><first>Yanxiang</first><last>Zhang</last><affiliation>Google</affiliation></author>
      <author><first>Galen</first><last>Andrew</last><affiliation>Google Research</affiliation></author>
      <author><first>Christopher</first><last>Choquette</last><affiliation>Google Research</affiliation></author>
      <author><first>Peter</first><last>Kairouz</last><affiliation>Google Research</affiliation></author>
      <author><first>Brendan</first><last>Mcmahan</last><affiliation>Google Research</affiliation></author>
      <author><first>Jesse</first><last>Rosenstock</last><affiliation>Google</affiliation></author>
      <author><first>Yuanbo</first><last>Zhang</last><affiliation>Google</affiliation></author>
      <pages>629-639</pages>
      <abstract>We train and deploy language models (LMs) with federated learning (FL) and differential privacy (DP) in Google Keyboard (Gboard). The recent DP-Follow the Regularized Leader (DP-FTRL) algorithm is applied to achieve meaningfully formal DP guarantees without requiring uniform sampling of clients. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation of training. With the help of pretraining on public data, we trained and deployed more than fifteen Gboard LMs that achieve high utility and ${rho-$zCDP privacy guarantees with ${rho {in (0.3, 2)$, with one model additionally trained with secure aggregation.We summarize our experience and provide concrete suggestions on DP training for practitioners.</abstract>
      <url hash="1d472e43">2023.acl-industry.60</url>
      <bibkey>xu-etal-2023-federated</bibkey>
    </paper>
    <paper id="61">
      <title><fixed-case>R</fixed-case>ad<fixed-case>L</fixed-case>ing: Towards Efficient Radiology Report Understanding</title>
      <author><first>Rikhiya</first><last>Ghosh</last><affiliation>Siemens Healthineers</affiliation></author>
      <author><first>Oladimeji</first><last>Farri</last><affiliation>Siemens Healthineers</affiliation></author>
      <author><first>Sanjeev Kumar</first><last>Karn</last><affiliation>Siemens</affiliation></author>
      <author><first>Manuela</first><last>Danu</last><affiliation>Siemens</affiliation></author>
      <author><first>Ramya</first><last>Vunikili</last><affiliation>Siemens Healthineers</affiliation></author>
      <author><first>Larisa</first><last>Micu</last><affiliation>Siemens Advanta Development Romania</affiliation></author>
      <pages>640-651</pages>
      <abstract>Most natural language tasks in the radiology domain use language models pre-trained on biomedical corpus. There are few pretrained language models trained specifically for radiology, and fewer still that have been trained in a low data setting and gone on to produce comparable results in fine-tuning tasks. We present RadLing, a continuously pretrained language model using ELECTRA-small architecture, trained using over 500K radiology reports that can compete with state-of-the-art results for fine tuning tasks in radiology domain. Our main contribution in this paper is knowledge-aware masking which is an taxonomic knowledge-assisted pre-training task that dynamically masks tokens to inject knowledge during pretraining. In addition, we also introduce an knowledge base-aided vocabulary extension to adapt the general tokenization vocabulary to radiology domain.</abstract>
      <url hash="362cada7">2023.acl-industry.61</url>
      <bibkey>ghosh-etal-2023-radling</bibkey>
    </paper>
    <paper id="62">
      <title>Predicting Customer Satisfaction with Soft Labels for Ordinal Classification</title>
      <author><first>Etienne</first><last>Manderscheid</last><affiliation>Dialpad, Inc</affiliation></author>
      <author><first>Matthias</first><last>Lee</last><affiliation>Dialpad, Inc</affiliation></author>
      <pages>652-659</pages>
      <abstract>In a typical call center, only up to 8% of callersleave a Customer Satisfaction (CSAT) surveyresponse at the end of the call, and these tend tobe customers with strongly positive or negativeexperiences. To manage this data sparsity andresponse bias, we outline a predictive CSATdeep learning algorithm that infers CSAT onthe 1-5 scale on inbound calls to the call centerwith minimal latency. The key metric to maximize is the precision for CSAT = 1 (lowestCSAT). We maximize this metric in two ways.First, reframing the problemas a binary class, rather than five-class problem during model fine-tuning, and then mapping binary outcomes back to five classes usingtemperature-scaled model probabilities. Second, using soft labels to represent the classes. Theresult is a production model able to support keycustomer workflows with high accuracy overmillions of calls a month.</abstract>
      <url hash="1f3097d3">2023.acl-industry.62</url>
      <bibkey>manderscheid-lee-2023-predicting</bibkey>
    </paper>
    <paper id="63">
      <title>Accurate Training of Web-based Question Answering Systems with Feedback from Ranked Users</title>
      <author><first>Liang</first><last>Wang</last><affiliation>Boston University</affiliation></author>
      <author><first>Ivano</first><last>Lauriola</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Alessandro</first><last>Moschitti</last><affiliation>Amazon</affiliation></author>
      <pages>660-667</pages>
      <abstract>Recent work has shown that large-scale annotated datasets are essential for training state-of-the-art Question Answering (QA) models.Unfortunately, creating this data is expensive and requires a huge amount of annotation work. An alternative and cheaper source of supervision is given by feedback data collected from deployed QA systems.This data can be collected from tens of millions of user with no additional cost, for real-world QA services, e.g., Alexa, Google Home, and etc. The main drawback is the noise affecting feedback on individual examples. Recent literature on QA systems has shown the benefit of training models even with noisy feedback. However, these studies have multiple limitations: (i) they used uniform random noise to simulate feedback responses, which is typically an unrealistic approximation as noise follows specific patterns, depending on target examples and users; and (ii) they do not show how to aggregate feedback for improving training signals.In this paper, we first collect a large scale (16M) QA dataset with real feedback sampled from the QA traffic of a popular Virtual Assistant.Second, we use this data to develop two strategies for filtering unreliable users and thus de-noise feedback: (i) ranking users with an automatic classifier, and (ii) aggregating feedback over similar instances and comparing users between each other. Finally, we train QA models on our filtered feedback data, showing a significant improvement over the state of the art.</abstract>
      <url hash="c95c9af4">2023.acl-industry.63</url>
      <bibkey>wang-etal-2023-accurate</bibkey>
    </paper>
    <paper id="64">
      <title><fixed-case>SPM</fixed-case>: A Split-Parsing Method for Joint Multi-Intent Detection and Slot Filling</title>
      <author><first>Sheng</first><last>Jiang</last><affiliation>AI Speech</affiliation></author>
      <author><first>Su</first><last>Zhu</last><affiliation>AISpeech Co., Ltd</affiliation></author>
      <author><first>Ruisheng</first><last>Cao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Qingliang</first><last>Miao</last><affiliation>AiSpeech</affiliation></author>
      <author><first>Kai</first><last>Yu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>668-675</pages>
      <abstract>In a task-oriented dialogue system, joint intent detection and slot filling for multi-intent utterances become meaningful since users tend to query more. The current state-of-the-art studies choose to process multi-intent utterances through a single joint model of sequence labelling and multi-label classification, which cannot generalize to utterances with more intents than training samples. Meanwhile, it lacks the ability to assign slots to each corresponding intent. To overcome these problems, we propose a Split-Parsing Method (SPM) for joint multiple intent detection and slot filling, which is a two-stage method. It first splits an input sentence into multiple sub-sentences which contain a single-intent, and then a joint single intent detection and slot filling model is applied to parse each sub-sentence recurrently. Finally, we integrate the parsed results. The sub-sentence split task is also treated as a sequence labelling problem with only one entity-label, which can effectively generalize to a sentence with more intents unseen in the training set. Experimental results on three multi-intent datasets show that our method obtains substantial improvements over different baselines.</abstract>
      <url hash="2d485b48">2023.acl-industry.64</url>
      <bibkey>jiang-etal-2023-spm</bibkey>
    </paper>
    <paper id="65">
      <title><fixed-case>NAG</fixed-case>-<fixed-case>NER</fixed-case>: a Unified Non-Autoregressive Generation Framework for Various <fixed-case>NER</fixed-case> Tasks</title>
      <author><first>Xinpeng</first><last>Zhang</last><affiliation>Netease</affiliation></author>
      <author><first>Ming</first><last>Tan</last><affiliation>Tencent</affiliation></author>
      <author><first>Jingfan</first><last>Zhang</last><affiliation>Univ of Ottawa</affiliation></author>
      <author><first>Wei</first><last>Zhu</last><affiliation>East China Normal University</affiliation></author>
      <pages>676-686</pages>
      <abstract>Recently, the recognition of flat, nested, and discontinuous entities by a unified generative model framework has received increasing attention both in the research field and industry. However, the current generative NER methods force the entities to be generated in a predefined order, suffering from error propagation and inefficient decoding. In this work, we propose a unified non-autoregressive generation (NAG) framework for general NER tasks, referred to as NAG-NER. First, we propose to generate entities as a set instead of a sequence, avoiding error propagation. Second, we propose incorporating NAG in NER tasks for efficient decoding by treating each entity as a target sequence. Third, to enhance the generation performances of the NAG decoder, we employ the NAG encoder to detect potential entity mentions. Extensive experiments show that our NAG-NER model outperforms the state-of-the-art generative NER models on three benchmark NER datasets of different types and two of our proprietary NER tasks.{footnote{Code will be publicly available to the research community upon acceptance.}</abstract>
      <url hash="bb943775">2023.acl-industry.65</url>
      <bibkey>zhang-etal-2023-nag</bibkey>
    </paper>
    <paper id="66">
      <title>Search Query Spell Correction with Weak Supervision in <fixed-case>E</fixed-case>-commerce</title>
      <author><first>Vishal</first><last>Kakkar</last><affiliation>Microsoft India</affiliation></author>
      <author><first>Chinmay</first><last>Sharma</last><affiliation>Flipkart Internet Private Limited</affiliation></author>
      <author><first>Madhura</first><last>Pande</last><affiliation>Flipkart Internet Private Limited</affiliation></author>
      <author><first>Surender</first><last>Kumar</last><affiliation>Flipkart Internet Private Limited</affiliation></author>
      <pages>687-694</pages>
      <abstract>Misspelled search queries in e-commerce can lead to empty or irrelevant products. Besides inadvertent typing mistakes, most spell mistakes occur because the user does not know the correct spelling, hence typing it as it is pronounced colloquially. This colloquial typing creates countless misspelling patterns for a single correct query. In this paper, we first systematically analyze and group different spell errors into error classes and then leverage the state-of-the-art Transformer model for contextual spell correction. We overcome the constraint of limited human labelled data by proposing novel synthetic data generation techniques for voluminous generation of training pairs needed by data hungry Transformers, without any human intervention. We further utilize weakly supervised data coupled with curriculum learning strategies to improve on tough spell mistakes without regressing on the easier ones. We show significant improvements from our model on human labeled data and online A/B experiments against multiple state-of-art models.</abstract>
      <url hash="a44d4a80">2023.acl-industry.66</url>
      <bibkey>kakkar-etal-2023-search</bibkey>
    </paper>
    <paper id="67">
      <title>“Let’s not Quote out of Context”: Unified Vision-Language Pretraining for Context Assisted Image Captioning</title>
      <author><first>Abisek</first><last>Rajakumar Kalarani</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology Bombay and Patna</affiliation></author>
      <author><first>Niyati</first><last>Chhaya</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Sumit</first><last>Shekhar</last><affiliation>Adobe Systems</affiliation></author>
      <pages>695-706</pages>
      <abstract>Well-formed context aware image captions and tags in enterprise content such as marketing material are critical to ensure their brand presence and content recall. Manual creation and updates to ensure the same is non trivial given the scale and the tedium towards this task. We propose a new unified Vision-Language (VL) model based on the One For All (OFA) model, with a focus on context-assisted image captioning where the caption is generated based on both the image and its context. Our approach aims to overcome the context-independent (image and text are treated independently) nature of the existing approaches. We exploit context by pretraining our model with datasets of three tasks- news image captioning where the news article is the context, contextual visual entailment, and keyword extraction from the context. The second pretraining task is a new VL task, and we construct and release two datasets for the task with 1.1M and 2.2K data instances. Our system achieves state-of-the-art results with an improvement of up to 8.34 CIDEr score on the benchmark news image captioning datasets. To the best of our knowledge, ours is the first effort at incorporating contextual information in pretraining the models for the VL tasks.</abstract>
      <url hash="75f1e70e">2023.acl-industry.67</url>
      <bibkey>rajakumar-kalarani-etal-2023-lets</bibkey>
    </paper>
    <paper id="68">
      <title>What, When, and How to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue</title>
      <author><first>Deuksin</first><last>Kwon</last><affiliation>SK Telecom</affiliation></author>
      <author><first>Sunwoo</first><last>Lee</last><affiliation>SK Telecom</affiliation></author>
      <author><first>Ki Hyun</first><last>Kim</last><affiliation>SK Telecom</affiliation></author>
      <author><first>Seojin</first><last>Lee</last><affiliation>SK Telecom</affiliation></author>
      <author><first>Taeyoon</first><last>Kim</last><affiliation>SK Telecom</affiliation></author>
      <author><first>Eric</first><last>Davis</last><affiliation>SK Telecom</affiliation></author>
      <pages>707-719</pages>
      <abstract>This paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of WWH in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.</abstract>
      <url hash="82f5249e">2023.acl-industry.68</url>
      <bibkey>kwon-etal-2023-ground</bibkey>
    </paper>
    <paper id="69">
      <title><fixed-case>CUPID</fixed-case>: Curriculum Learning Based Real-Time Prediction using Distillation</title>
      <author><first>Arindam</first><last>Bhattacharya</last><affiliation>Amazon</affiliation></author>
      <author><first>Ankith</first><last>Ms</last><affiliation>Amazon</affiliation></author>
      <author><first>Ankit</first><last>Gandhi</last><affiliation>Amazon</affiliation></author>
      <author><first>Vijay</first><last>Huddar</last><affiliation>Amazon</affiliation></author>
      <author><first>Atul</first><last>Saroop</last><affiliation>Amazon</affiliation></author>
      <author><first>Rahul</first><last>Bhagat</last><affiliation>Amazon</affiliation></author>
      <pages>720-728</pages>
      <abstract>Relevance in E-commerce Product Search is crucial for providing customers with accurate results that match their query intent. With recent advancements in NLP and Deep Learning, Transformers have become the default choice for relevance classification tasks. In such a setting, the relevance model uses query text and product title as input features, and estimates if the product is relevant for the customer query. While cross-attention in Transformers enables a more accurate relevance prediction in such a setting, its high evaluation latency makes it unsuitable for real-time predictions in which thousands of products must be evaluated against a user query within few milliseconds. To address this issue, we propose CUPID: a Curriculum learning based real-time Prediction using Distillation that utilizes knowledge distillation within a curriculum learning setting to learn a simpler architecture that can be evaluated within low latency budgets. In a bi-lingual relevance prediction task, our approach shows an 302 bps improvement on English and 676 bps improvement for low-resource Arabic, while maintaining the low evaluation latency on CPUs.</abstract>
      <url hash="c42e25de">2023.acl-industry.69</url>
      <bibkey>bhattacharya-etal-2023-cupid</bibkey>
    </paper>
    <paper id="70">
      <title>Answering Unanswered Questions through Semantic Reformulations in Spoken <fixed-case>QA</fixed-case></title>
      <author><first>Pedro</first><last>Faustini</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Zhiyu</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Besnik</first><last>Fetahu</last><affiliation>Amazon</affiliation></author>
      <author><first>Oleg</first><last>Rokhlenko</last><affiliation>Amazon Research</affiliation></author>
      <author><first>Shervin</first><last>Malmasi</last><affiliation>Amazon</affiliation></author>
      <pages>729-743</pages>
      <abstract>Spoken Question Answering (QA) is a key feature of voice assistants, usually backed by multiple QA systems. Users ask questions via spontaneous speech that can contain disfluencies, errors, and informal syntax or phrasing. This is a major challenge in QA, causing unanswered questions or irrelevant answers, leading to bad user experiences. We analyze failed QA requests to identify core challenges: lexical gaps, proposition types, complex syntactic structure, and high specificity. We propose a Semantic Question Reformulation (SURF) model offering three linguistically-grounded operations (repair, syntactic reshaping, generalization) to rewrite questions to facilitate answering. Offline evaluation on 1M unanswered questions from a leading voice assistant shows that SURF significantly improves answer rates: up to 24% of previously unanswered questions obtain relevant answers (75%). Live deployment shows positive impact for millions of customers with unanswered questions; explicit relevance feedback shows high user satisfaction.</abstract>
      <url hash="83a6a420">2023.acl-industry.70</url>
      <bibkey>faustini-etal-2023-answering</bibkey>
    </paper>
    <paper id="71">
      <title>Exploring Zero and Few-shot Techniques for Intent Classification</title>
      <author><first>Soham</first><last>Parikh</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Mitul</first><last>Tiwari</last><affiliation>ServiceNow</affiliation></author>
      <author><first>Prashil</first><last>Tumbade</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Quaizar</first><last>Vohra</last><affiliation>ServiceNow Inc</affiliation></author>
      <pages>744-751</pages>
      <abstract>Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe on Flan-T5 yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions is also very competitive.</abstract>
      <url hash="fa22e133">2023.acl-industry.71</url>
      <bibkey>parikh-etal-2023-exploring</bibkey>
    </paper>
    <paper id="72">
      <title>Referring to Screen Texts with Voice Assistants</title>
      <author><first>Shruti</first><last>Bhargava</last><affiliation>Apple</affiliation></author>
      <author><first>Anand</first><last>Dhoot</last><affiliation>Apple Inc.</affiliation></author>
      <author><first>Ing-marie</first><last>Jonsson</last><affiliation>Apple Inc.</affiliation></author>
      <author><first>Hoang Long</first><last>Nguyen</last><affiliation>Apple</affiliation></author>
      <author><first>Alkesh</first><last>Patel</last><affiliation>Apple Inc.</affiliation></author>
      <author><first>Hong</first><last>Yu</last><affiliation>Apple Inc.</affiliation></author>
      <author><first>Vincent</first><last>Renkens</last><affiliation>Apple Inc.</affiliation></author>
      <pages>752-762</pages>
      <abstract>Voice assistants help users make phone calls, send messages, create events, navigate and do a lot more. However assistants have limited capacity to understand their users’ context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to phone numbers, addresses, email addresses, urls, and dates on their phone screens. We focus on reference understanding, which is particularly interesting when, similar to visual grounding, there are multiple similar texts on screen. We collect a dataset and propose a lightweight general purpose model for this novel experience. Since consuming pixels directly is expensive, our system is designed to rely only on text extracted from the UI. Our model is modular, offering flexibility, better interpretability and efficient run time memory.</abstract>
      <url hash="7c496a48">2023.acl-industry.72</url>
      <bibkey>bhargava-etal-2023-referring</bibkey>
    </paper>
    <paper id="73">
      <title>Generate-then-Retrieve: Intent-Aware <fixed-case>FAQ</fixed-case> Retrieval in Product Search</title>
      <author><first>Zhiyu</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Jason</first><last>Choi</last><affiliation>Amazon</affiliation></author>
      <author><first>Besnik</first><last>Fetahu</last><affiliation>Amazon</affiliation></author>
      <author><first>Oleg</first><last>Rokhlenko</last><affiliation>Amazon Research</affiliation></author>
      <author><first>Shervin</first><last>Malmasi</last><affiliation>Amazon</affiliation></author>
      <pages>763-771</pages>
      <abstract>Frequently Asked Question (FAQ) retrieval aims at retrieving question-answer pairs for a given a user query. Integrating FAQ retrieval with product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Providing FAQ content without disrupting user’s shopping experience poses challenges on deciding when and how to show FAQ results. Our proposed intent-aware FAQ retrieval consists of (1) an intent classifier that predicts whether the query is looking for an FAQ; (2) a reformulation model that rewrites query into a natural question. Offline evaluation demonstrates that our approach improves 12% in Hit@1 on retrieving ground-truth FAQs, while reducing latency by 95% compared to baseline systems. These improvements are further validated by real user feedback, where more than 99% of users consider FAQs displayed on top of product search results is helpful. Overall, our findings show promising directions for integrating FAQ retrieval into product search at scale.</abstract>
      <url hash="4fdafe86">2023.acl-industry.73</url>
      <bibkey>chen-etal-2023-generate</bibkey>
    </paper>
    <paper id="74">
      <title><fixed-case>KAFA</fixed-case>: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models</title>
      <author><first>Zhiwei</first><last>Jia</last><affiliation>UC San Diego</affiliation></author>
      <author><first>Pradyumna</first><last>Narayana</last><affiliation>Google</affiliation></author>
      <author><first>Arjun</first><last>Akula</last><affiliation>Google AI</affiliation></author>
      <author><first>Garima</first><last>Pruthi</last><affiliation>Google</affiliation></author>
      <author><first>Hao</first><last>Su</last><affiliation>UC San Diego</affiliation></author>
      <author><first>Sugato</first><last>Basu</last><affiliation>Google</affiliation></author>
      <author><first>Varun</first><last>Jampani</last><affiliation>Google</affiliation></author>
      <pages>772-785</pages>
      <abstract>Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational vision-language models (VLMs) featuring impressive generalizability and adaptability. In this paper, we perform the first empirical study of image ad understanding through the lens of pre-trained VLMs. We benchmark and reveal practical challenges in adapting these VLMs to image ad understanding. We propose a simple feature adaptation strategy to effectively fuse multimodal information for image ads and further empower it with knowledge of real-world entities. We hope our study draws more attention to image ad understanding which is broadly relevant to the advertising industry.</abstract>
      <url hash="2993d8ef">2023.acl-industry.74</url>
      <bibkey>jia-etal-2023-kafa</bibkey>
    </paper>
    <paper id="75">
      <title>Weakly supervised hierarchical multi-task classification of customer questions</title>
      <author><first>Jitenkumar</first><last>Rana</last><affiliation>Amazon</affiliation></author>
      <author><first>Promod</first><last>Yenigalla</last><affiliation>Amazon</affiliation></author>
      <author><first>Chetan</first><last>Aggarwal</last><affiliation>Amazon</affiliation></author>
      <author><first>Sandeep Sricharan</first><last>Mukku</last><affiliation>IIIT Hyderabad</affiliation></author>
      <author><first>Manan</first><last>Soni</last><affiliation>Amazon</affiliation></author>
      <author><first>Rashmi</first><last>Patange</last><affiliation>Amazon</affiliation></author>
      <pages>786-793</pages>
      <abstract>Identifying granular and actionable topics from customer questions (CQ) posted on e-commerce websites helps surface the missing information expected by customers on the product detail page (DP), provide insights to brands and sellers on what critical product information that the customers are looking before making a purchase decision and helps enrich the catalog quality to improve the overall customer experience (CX). We propose a weakly supervised Hierarchical Multi-task Classification Framework (HMCF) to identify topics from customer questions at various granularities. Complexity lies in creating a list of granular topics (taxonomy) for 1000s of product categories and building a scalable classification system. To this end, we introduce a clustering based Taxonomy Creation and Data Labeling (TCDL) module for creating taxonomy and labelled data with minimal supervision. Using TCDL module, taxonomy and labelled data creation task reduces to 2 hours as compared to 2 weeks of manual efforts by a subject matter expert. For classification, we propose a two level HMCF that performs multi-class classification to identify coarse level-1 topic and leverages NLI based label-aware approach to identify granular level-2 topic. We showcase that HMCF (based on BERT and NLI) a) achieves absolute improvement of 13% in Top-1 accuracy over single-task non-hierarchical baselines b) learns a generic domain invariant function that can adapt to constantly evolving taxonomy (open label set) without need of re-training. c) reduces model deployment efforts significantly since it needs only one model that caters to 1000s of product categories.</abstract>
      <url hash="83721cda">2023.acl-industry.75</url>
      <bibkey>rana-etal-2023-weakly</bibkey>
    </paper>
    <paper id="76">
      <title>Automated Digitization of Unstructured Medical Prescriptions</title>
      <author><first>Megha</first><last>Sharma</last><affiliation>Amazon</affiliation></author>
      <author><first>Tushar</first><last>Vatsal</last><affiliation>Amazon</affiliation></author>
      <author><first>Srujana</first><last>Merugu</last><affiliation>Amazon</affiliation></author>
      <author><first>Aruna</first><last>Rajan</last><affiliation>Google</affiliation></author>
      <pages>794-805</pages>
      <abstract>Automated digitization of prescription images is a critical prerequisite to scale digital healthcare services such as online pharmacies. This is challenging in emerging markets since prescriptions are not digitized at source and patients lack the medical expertise to interpret prescriptions to place orders. In this paper, we present prescription digitization system for online medicine ordering built with minimal supervision. Our system uses a modular pipeline comprising a mix of ML and rule-based components for (a) image to text extraction, (b) segmentation into blocks and medication items, (c) medication attribute extraction, (d) matching against medicine catalog, and (e) shopping cart building. Our approach efficiently utilizes multiple signals like layout, medical ontologies, and semantic embeddings via LayoutLMv2 model to yield substantial improvement relative to strong baselines on medication attribute extraction. Our pipeline achieves +5.9% gain in precision@3 and +5.6% in recall@3 over catalog-based fuzzy matching baseline for shopping cart building for printed prescriptions.</abstract>
      <url hash="f9b592fe">2023.acl-industry.76</url>
      <bibkey>sharma-etal-2023-automated</bibkey>
    </paper>
  </volume>
</collection>
