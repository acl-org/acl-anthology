<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.acl">
  <volume id="srw" ingest-date="2023-06-29">
    <meta>
      <booktitle>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics - Student Research Workshop</booktitle>
      <editor><first>Vishakh</first><last>Padmakumar</last></editor>
      <editor><first>Gisela</first><last>Vallejo</last></editor>
      <editor><first>Yao</first><last>Fu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="d8f06fd0">2023.acl-srw</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="44ac9b55">2023.acl-srw.0</url>
      <bibkey>acl-2023-association</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</title>
      <author><first>Dongqi</first><last>Pu</last><affiliation>Saarland University</affiliation></author>
      <author><first>Vera</first><last>Demberg</last><affiliation>Saarland University</affiliation></author>
      <pages>1-18</pages>
      <abstract>ChatGPT Analysis</abstract>
      <url hash="14931f61">2023.acl-srw.1</url>
      <bibkey>pu-demberg-2023-chatgpt</bibkey>
    </paper>
    <paper id="2">
      <title>Multi-Dialectal Representation Learning of Sinitic Phonology</title>
      <author><first>Zhibai</first><last>Jia</last><affiliation>No.2 High School of East China Normal University</affiliation></author>
      <pages>19-29</pages>
      <abstract>Representation learning of Sinitic Phonology using a knowledge graph based method</abstract>
      <url hash="86e7ec92">2023.acl-srw.2</url>
      <attachment type="SupplementaryMaterial" hash="7bc909c5">2023.acl-srw.2.SupplementaryMaterial.zip</attachment>
      <bibkey>jia-2023-multi</bibkey>
    </paper>
    <paper id="4">
      <title>Prompt-based Zero-shot Text Classification with Conceptual Knowledge</title>
      <author><first>Yuqi</first><last>Wang</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Qi</first><last>Chen</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Kaizhu</first><last>Huang</last><affiliation>Duke Kunshan University</affiliation></author>
      <author><first>Anh</first><last>Nguyen</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Suparna</first><last>De</last><affiliation>University of Surrey</affiliation></author>
      <pages>30-38</pages>
      <abstract>The proposed framework incorporates conceptual knowledge for prompt-based text classification in the extreme zero-shot setting, which outperforms existing approaches in sentiment analysis and topic detection on four widely-used datasets.</abstract>
      <url hash="32d39d3c">2023.acl-srw.4</url>
      <bibkey>wang-etal-2023-prompt</bibkey>
    </paper>
    <paper id="5">
      <title>How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in <fixed-case>J</fixed-case>apanese</title>
      <author><first>Takuro</first><last>Fujii</last><affiliation>Yokohama National University</affiliation></author>
      <author><first>Koki</first><last>Shibata</last><affiliation>University of Tsukuba</affiliation></author>
      <author><first>Atsuki</first><last>Yamaguchi</last><affiliation>Hitachi, Ltd.</affiliation></author>
      <author><first>Terufumi</first><last>Morishita</last><affiliation>Hitachi.ltd</affiliation></author>
      <author><first>Yasuhiro</first><last>Sogawa</last><affiliation>Hitachi, Ltd.</affiliation></author>
      <pages>39-49</pages>
      <abstract>We investigate the impact of different tokenizers on downstream performance in Japanese NLP, with the case of BERT architecture.</abstract>
      <url hash="7805d815">2023.acl-srw.5</url>
      <bibkey>fujii-etal-2023-different</bibkey>
    </paper>
    <paper id="7">
      <title>Semantic-Aware Dynamic Retrospective-Prospective Reasoning for Event-Level Video Question Answering</title>
      <author><first>Chenyang</first><last>Lyu</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Tianbo</first><last>Ji</last><affiliation>Nantong University</affiliation></author>
      <author><first>Yvette</first><last>Graham</last><affiliation>ADAPT, Trinity College Dublin</affiliation></author>
      <author><first>Jennifer</first><last>Foster</last><affiliation>Dublin City University</affiliation></author>
      <pages>50-56</pages>
      <abstract>The paper is about video QA. It utilizes SRL partitioning to improve multi-step attention and reasoning of the models to attend to different frames for different parts of the question.</abstract>
      <url hash="92b00d3d">2023.acl-srw.7</url>
      <bibkey>lyu-etal-2023-semantic</bibkey>
    </paper>
    <paper id="8">
      <title>Jamp: Controlled <fixed-case>J</fixed-case>apanese Temporal Inference Dataset for Evaluating Generalization Capacity of Language Models</title>
      <author><first>Tomoki</first><last>Sugimoto</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yasumasa</first><last>Onoe</last><affiliation>The University of Texas at Austin</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>57-68</pages>
      <abstract>We construct Jamp, which is a Japanese NLI dataset for temporal inference, and evaluate the generalization capacity of several LMs on our dataset.</abstract>
      <url hash="7c374394">2023.acl-srw.8</url>
      <bibkey>sugimoto-etal-2023-jamp</bibkey>
    </paper>
    <paper id="10">
      <title>Constructing Multilingual Code Search Dataset Using Neural Machine Translation</title>
      <author><first>Ryo</first><last>Sekizawa</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Shuai</first><last>Lu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>69-75</pages>
      <abstract>We create a multilingual code search dataset by translating the existing English dataset with a machine translation model and conduct a baseline experiment on a code search task.</abstract>
      <url hash="dd8df7e4">2023.acl-srw.10</url>
      <bibkey>sekizawa-etal-2023-constructing</bibkey>
    </paper>
    <paper id="12">
      <title>Multimodal Neural Machine Translation Using Synthetic Images Transformed by Latent Diffusion Model</title>
      <author><first>Ryoya</first><last>Yuasa</last><affiliation>Doshisha University</affiliation></author>
      <author><first>Akihiro</first><last>Tamura</last><affiliation>Doshisha University</affiliation></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last><affiliation>Ehime University</affiliation></author>
      <author><first>Takashi</first><last>Ninomiya</last><affiliation>Ehime University</affiliation></author>
      <author><first>Tsuneo</first><last>Kato</last><affiliation>Doshisha university</affiliation></author>
      <pages>76-82</pages>
      <abstract>This study proposes a new multimodal neural machine translation model using synthetic images transformed by a latent diffusion model.</abstract>
      <url hash="d793880d">2023.acl-srw.12</url>
      <bibkey>yuasa-etal-2023-multimodal</bibkey>
    </paper>
    <paper id="15">
      <title>Enhancing <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Understanding with Derived Noisy Syntax Trees</title>
      <author><first>Ping</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Shitou</first><last>Zhang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Zuchao</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jingrui</first><last>Hou</last><affiliation>Department of Computer Science,Loughborough University</affiliation></author>
      <pages>83-92</pages>
      <abstract>This paper introduces a confidence-based syntax encoding network (cSEN) to incorporate syntax in ancient Chinese understanding tasks, effectively improving performance by mitigating noise and incompatibility issues.</abstract>
      <url hash="39081656">2023.acl-srw.15</url>
      <bibkey>wang-etal-2023-enhancing</bibkey>
    </paper>
    <paper id="17">
      <title>The <fixed-case>T</fixed-case>uring Quest: Can Transformers Make Good <fixed-case>NPC</fixed-case>s?</title>
      <author><first>Qi Chen</first><last>Gao</last><affiliation>Brock University</affiliation></author>
      <author><first>Ali</first><last>Emami</last><affiliation>Brock University</affiliation></author>
      <pages>93-103</pages>
      <abstract>We explored the generation of NPC dialogue using a zero-shot prompting method as well as the ability of LMs to self-evaluate and score dialogue with few-shot learning.</abstract>
      <url hash="1b12fd49">2023.acl-srw.17</url>
      <bibkey>gao-emami-2023-turing</bibkey>
    </paper>
    <paper id="18">
      <title>Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section</title>
      <author><first>Hongyi</first><last>Zheng</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Yixin</first><last>Zhu</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Lavender</first><last>Jiang</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Kyunghyun</first><last>Cho</last><affiliation>Center for Data Science and Courant Institute of Mathematical Sciences, New York University; Prescient Design; CIFAR</affiliation></author>
      <author><first>Eric</first><last>Oermann</last><affiliation>Department of Neurosurgery, NYU Langone Health; Department of Neurosurgery and Radiology, NYU Grossman School of Medicine</affiliation></author>
      <pages>104-108</pages>
      <abstract>We propose a data-driven framework to select clinical note sections with high predictive power.</abstract>
      <url hash="bc3e066d">2023.acl-srw.18</url>
      <bibkey>zheng-etal-2023-making</bibkey>
    </paper>
    <paper id="19">
      <title>Intriguing Effect of the Correlation Prior on <fixed-case>ICD</fixed-case>-9 Code Assignment</title>
      <author><first>Zihao</first><last>Yang</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Chenkang</first><last>Zhang</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Muru</first><last>Wu</last><affiliation>Center for Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Xujin</first><last>Liu</last><affiliation>Department of Electrical and Computer Engineering, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Lavender</first><last>Jiang</last><affiliation>Center of Data Science, New York University; Department of Neurosurgery, NYU Langone Health</affiliation></author>
      <author><first>Kyunghyun</first><last>Cho</last><affiliation>Center of Data Science &amp; Courant Institute of Mathematical Sciences, New York University; Prescient Design; CIFAR</affiliation></author>
      <author><first>Eric</first><last>Oermann</last><affiliation>Department of Neurosurgery, NYU Langone Health; Department of Neurosurgery and Radiology, NYU Grossman School of Medicine</affiliation></author>
      <pages>109-118</pages>
      <abstract>This paper investigates the usefulness of correlation bias in improving language models’ performance on predicting imbalanced clinical codes from discharge summaries.</abstract>
      <url hash="d23d745e">2023.acl-srw.19</url>
      <bibkey>yang-etal-2023-intriguing</bibkey>
    </paper>
    <paper id="20">
      <title>Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks</title>
      <author><first>Mateusz</first><last>Baran</last><affiliation>Wrocław University of Science and Technology</affiliation></author>
      <author><first>Joanna</first><last>Baran</last><affiliation>Wroclaw University of Science and Technology</affiliation></author>
      <author><first>Mateusz</first><last>Wójcik</last><affiliation>Wroclaw University of Science and Technology. Alphamoon Ltd.</affiliation></author>
      <author><first>Maciej</first><last>Zięba</last><affiliation>Wroclaw University of Science and Technology</affiliation></author>
      <author><first>Adam</first><last>Gonczarek</last><affiliation>Alphamoon Ltd.</affiliation></author>
      <pages>119-129</pages>
      <abstract>The paper evaluates existing OOD detection methods for NLP systems and emphasizes the need for further research to develop more effective approaches to ensure safety and trustworthiness of NLP systems.</abstract>
      <url hash="c6515660">2023.acl-srw.20</url>
      <attachment type="SupplementaryMaterial" hash="6402114e">2023.acl-srw.20.SupplementaryMaterial.zip</attachment>
      <bibkey>baran-etal-2023-classical</bibkey>
    </paper>
    <paper id="22">
      <title>Can <fixed-case>LM</fixed-case>s Store and Retrieve 1-to-N Relational Knowledge?</title>
      <author><first>Haruki</first><last>Nagasawa</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Benjamin</first><last>Heinzerling</last><affiliation>RIKEN AIP &amp; Tohoku University</affiliation></author>
      <author><first>Kazuma</first><last>Kokuta</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>Tohoku University / Riken</affiliation></author>
      <pages>130-138</pages>
      <abstract>Our study aimed to explore the feasibility of using LMs as KBs, and we focused specifically on 1-to-N relational knowledge, an area that has not been extensively researched, and proposed a comprehensive approach that involved identifying the unique characteristics of this type of knowledge, designing appropriate training methods, and developing evaluation perspectives.</abstract>
      <url hash="6971f41d">2023.acl-srw.22</url>
      <bibkey>nagasawa-etal-2023-lms</bibkey>
    </paper>
    <paper id="24">
      <title>Theoretical Linguistics Rivals Embeddings in Language Clustering for Multilingual Named Entity Recognition</title>
      <author><first>Sakura</first><last>Imai</last><affiliation>Waseda University</affiliation></author>
      <author><first>Daisuke</first><last>Kawahara</last><affiliation>Waseda University</affiliation></author>
      <author><first>Naho</first><last>Orita</last><affiliation>Waseda University</affiliation></author>
      <author><first>Hiromune</first><last>Oda</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>139-151</pages>
      <abstract>This study investigates whether and how theoretical linguistics improves language clustering for multilingual named entity recognition (NER), with the two types of language groupings proposed: one based on morpho-syntactic features in a nominal domain and one based on a head parameter.</abstract>
      <url hash="318c1d81">2023.acl-srw.24</url>
      <bibkey>imai-etal-2023-theoretical</bibkey>
    </paper>
    <paper id="26">
      <title>Native Language Prediction from Gaze: a Reproducibility Study</title>
      <author><first>Lina</first><last>Skerath</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Paulina</first><last>Toborek</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Anita</first><last>Zielińska</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Maria</first><last>Barrett</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Rob</first><last>Van Der Goot</last><affiliation>IT University of Copenhagen</affiliation></author>
      <pages>152-159</pages>
      <abstract>A reproduction study of native language prediction from English as second language reading eye-tracking data.</abstract>
      <url hash="ba76735e">2023.acl-srw.26</url>
      <bibkey>skerath-etal-2023-native</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>M</fixed-case>ed<fixed-case>T</fixed-case>em2.0: Prompt-based Temporal Classification of Treatment Events from Discharge Summaries</title>
      <author><first>Yang</first><last>Cui</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Lifeng</first><last>Han</last><affiliation>The University of Manchester</affiliation></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <pages>160-183</pages>
      <abstract>We use Prompt-based learning on LLMs for Temporal Classification of Treatment Events from Discharge Summaries of clinical data.</abstract>
      <url hash="517efa23">2023.acl-srw.27</url>
      <bibkey>cui-etal-2023-medtem2</bibkey>
    </paper>
    <paper id="28">
      <title>Sudden Semantic Shifts in <fixed-case>S</fixed-case>wedish <fixed-case>NATO</fixed-case> discourse</title>
      <author><first>Brian</first><last>Bonafilia</last><affiliation>Chalmers University of Technology</affiliation></author>
      <author><first>Bastiaan</first><last>Bruinsma</last><affiliation>Chalmers University of Technology</affiliation></author>
      <author><first>Denitsa</first><last>Saynova</last><affiliation>Chalmers University of Technology</affiliation></author>
      <author><first>Moa</first><last>Johansson</last><affiliation>Chalmers University of Technology</affiliation></author>
      <pages>184-193</pages>
      <abstract>We look at short-term semantic shifts in the Swedish discussion about NATO membership.</abstract>
      <url hash="9e42105e">2023.acl-srw.28</url>
      <bibkey>bonafilia-etal-2023-sudden</bibkey>
    </paper>
    <paper id="29">
      <title>Building a Buzzer-quiz Answering System</title>
      <author><first>Naoya</first><last>Sugiura</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Kosuke</first><last>Yamada</last><affiliation>Nagoya university</affiliation></author>
      <author><first>Ryohei</first><last>Sasano</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Koichi</first><last>Takeda</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Katsuhiko</first><last>Toyama</last><affiliation>Nagoya University</affiliation></author>
      <pages>194-199</pages>
      <abstract>This paper presents two types of buzzer-quiz answering systems that can predict the answer from only part of a question and then proposes a method to estimate the accuracy of the answers for each system by using the internal scores of each model.</abstract>
      <url hash="21b4e3c7">2023.acl-srw.29</url>
      <bibkey>sugiura-etal-2023-building</bibkey>
    </paper>
    <paper id="30">
      <title>Probing for Hyperbole in Pre-Trained Language Models</title>
      <author><first>Nina</first><last>Schneidermann</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Daniel</first><last>Hershcovich</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Bolette</first><last>Pedersen</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>200-211</pages>
      <abstract>This paper contributes to hyperbole identification research in NLP with two probing tasks (edge and MDL probing) for 3 pre-trained language models, as well as an attempt to shed light on problems annotating hyperbole.</abstract>
      <url hash="5bd81614">2023.acl-srw.30</url>
      <bibkey>schneidermann-etal-2023-probing</bibkey>
    </paper>
    <paper id="31">
      <title>Towards Efficient Dialogue Processing in the Emergency Response Domain</title>
      <author><first>Tatiana</first><last>Anikina</last><affiliation>DFKI / Saarland Informatics Campus</affiliation></author>
      <pages>212-225</pages>
      <abstract>This paper is about dialogue act classification and slot tagging in the emergency response domain.</abstract>
      <url hash="bb4799cb">2023.acl-srw.31</url>
      <bibkey>anikina-2023-towards</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>I</fixed-case> already said that! Degenerating redundant questions in open-domain dialogue systems.</title>
      <author><first>Long</first><last>Mai</last><affiliation>University College Dublin</affiliation></author>
      <author><first>Julie</first><last>Carson-berndsen</last><affiliation>University College Dublin</affiliation></author>
      <pages>226-236</pages>
      <abstract>This paper propose methods to reduce the number of redundant questions generated by open-domain dialogue systems.</abstract>
      <url hash="6dc7fccc">2023.acl-srw.33</url>
      <bibkey>mai-carson-berndsen-2023-already</bibkey>
    </paper>
    <paper id="34">
      <title>Is a Knowledge-based Response Engaging?: An Analysis on Knowledge-Grounded Dialogue with Information Source Annotation</title>
      <author><first>Takashi</first><last>Kodama</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Hirokazu</first><last>Kiyomaru</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Yin Jou</first><last>Huang</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Taro</first><last>Okahisa</last><affiliation>Shizuoka University</affiliation></author>
      <author><first>Sadao</first><last>Kurohashi</last><affiliation>Kyoto University</affiliation></author>
      <pages>237-243</pages>
      <abstract>This paper investigates how humans incorporate speaker-derived information by annotating the utterances in a knowledge-grounded dialogue corpus.</abstract>
      <url hash="7d7191ff">2023.acl-srw.34</url>
      <bibkey>kodama-etal-2023-knowledge</bibkey>
    </paper>
    <paper id="35">
      <title>Choosing What to Mask: More Informed Masking for Multimodal Machine Translation</title>
      <author><first>Julia</first><last>Sato</last><affiliation>Federal University of Sao Carlos</affiliation></author>
      <author><first>Helena</first><last>Caseli</last><affiliation>Federal University of São Carlos</affiliation></author>
      <author><first>Lucia</first><last>Specia</last><affiliation>Imperial College London</affiliation></author>
      <pages>244-253</pages>
      <abstract>More informed masking in cross-lingual visual pre-training for multimodal machine translation.</abstract>
      <url hash="ed7f9453">2023.acl-srw.35</url>
      <bibkey>sato-etal-2023-choosing</bibkey>
    </paper>
    <paper id="36">
      <title>Combining Tradition with Modernness: Exploring Event Representations in Vision-and-Language Models for Visual Goal-Step Inference</title>
      <author><first>Chong</first><last>Shen</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Carina</first><last>Silberer</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>254-265</pages>
      <abstract>This paper studies various methods and their effects on multimodal procedural knowledge understanding of injecting the early shallow017 event representations to nowadays multimodal deep learning-based models.</abstract>
      <url hash="890fb69a">2023.acl-srw.36</url>
      <bibkey>shen-silberer-2023-combining</bibkey>
    </paper>
    <paper id="37">
      <title>Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values</title>
      <author><first>Stephanie</first><last>Schoch</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Yangfeng</first><last>Ji</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Ritwick</first><last>Mishra</last><affiliation>University of Virginia</affiliation></author>
      <pages>266-275</pages>
      <abstract>This paper proposes a sampling chain based method to make Shapley values computationally feasible for data valuation and selection for large language models.</abstract>
      <url hash="0e209134">2023.acl-srw.37</url>
      <bibkey>schoch-etal-2023-data</bibkey>
    </paper>
    <paper id="38">
      <title>Distractor Generation for Fill-in-the-Blank Exercises by Question Type</title>
      <author><first>Nana</first><last>Yoshimi</last><affiliation>Ehime University</affiliation></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last><affiliation>Ehime University</affiliation></author>
      <author><first>Satoru</first><last>Uchida</last><affiliation>Kyushu University</affiliation></author>
      <author><first>Yuki</first><last>Arase</last><affiliation>Osaka University</affiliation></author>
      <author><first>Takashi</first><last>Ninomiya</last><affiliation>Ehime University</affiliation></author>
      <pages>276-281</pages>
      <abstract>We define three types of questions (grammar, function word, and context) for fill-in-the-blank exercises and propose a method to generate distractors according to the characteristics of each question type.</abstract>
      <url hash="34f8aa38">2023.acl-srw.38</url>
      <bibkey>yoshimi-etal-2023-distractor</bibkey>
    </paper>
    <paper id="40">
      <title>Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity</title>
      <author><first>Gabriel</first><last>Simmons</last><affiliation>University of California, Davis</affiliation></author>
      <pages>282-297</pages>
      <abstract>This paper studies whether LLMs moral preferences based on prompted political ideology replicate known results obtained in social science studies, using tools from Moral Foundations Theory</abstract>
      <url hash="9b4c8c27">2023.acl-srw.40</url>
      <bibkey>simmons-2023-moral</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>LECO</fixed-case>: Improving Early Exiting via Learned Exits and Comparison-based Exiting Mechanism</title>
      <author><first>Jingfan</first><last>Zhang</last><affiliation>Univ of Ottawa</affiliation></author>
      <author><first>Ming</first><last>Tan</last><affiliation>Tencent</affiliation></author>
      <author><first>Pengyu</first><last>Dai</last><affiliation>Sjtu</affiliation></author>
      <author><first>Wei</first><last>Zhu</last><affiliation>East China Normal University</affiliation></author>
      <pages>298-309</pages>
      <abstract>Speeding up the inference of pretrained models by designing better intermediate early exits and a comparison-based early exiting mechanism</abstract>
      <url hash="98549855">2023.acl-srw.43</url>
      <bibkey>zhang-etal-2023-leco</bibkey>
    </paper>
    <paper id="44">
      <title>Authorship Attribution of Late 19th Century Novels using <fixed-case>GAN</fixed-case>-<fixed-case>BERT</fixed-case></title>
      <author><first>Kanishka</first><last>Silva</last><affiliation>University of Wolverhampton</affiliation></author>
      <author><first>Burcu</first><last>Can</last><affiliation>University of Stirling</affiliation></author>
      <author><first>Frédéric</first><last>Blain</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Raheem</first><last>Sarwar</last><affiliation>OTEHM, Manchester Metropolitan University</affiliation></author>
      <author><first>Laura</first><last>Ugolini</last><affiliation>University of Wolverhampton</affiliation></author>
      <author><first>Ruslan</first><last>Mitkov</last><affiliation>University of Wolverhampton</affiliation></author>
      <pages>310-320</pages>
      <abstract>This paper is about performing authorship attribution of long 19th-century novels using the GAN-BERT model, comparing author counts, author combinations and sample text sizes.</abstract>
      <url hash="b31cd570">2023.acl-srw.44</url>
      <bibkey>silva-etal-2023-authorship</bibkey>
    </paper>
    <paper id="46">
      <title>How-to Guides for Specific Audiences: A Corpus and Initial Findings</title>
      <author><first>Nicola</first><last>Fanton</last><affiliation>Universität Stuttgart</affiliation></author>
      <author><first>Agnieszka</first><last>Falenska</last><affiliation>IMS, University of Stuttgart</affiliation></author>
      <author><first>Michael</first><last>Roth</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>321-333</pages>
      <abstract>We collect how-to guides for different target audiences and investigate qualitative and quantitative differences.</abstract>
      <url hash="fd5a1489">2023.acl-srw.46</url>
      <bibkey>fanton-etal-2023-guides</bibkey>
    </paper>
    <paper id="47">
      <title>“When Words Fail, Emojis Prevail”: A Novel Architecture for Generating Sarcastic Sentences With Emoji Using Valence Reversal and Semantic Incongruity</title>
      <author><first>Faria Binte</first><last>Kader</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Nafisa</first><last>Hossain Nujat</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Tasmia Binte</first><last>Sogir</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Mohsinul</first><last>Kabir</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Hasan</first><last>Mahmud</last><affiliation>Associate Professor</affiliation></author>
      <author><first>Md Kamrul</first><last>Hasan</last><affiliation>Islamic University of Technology</affiliation></author>
      <pages>334-351</pages>
      <abstract>A new framework in which when given a non-sarcastic text as input, the text is converted into a sarcastic one with emoji where the emoji will specifically help to identify the sarcastic intent of the text.</abstract>
      <url hash="c23cc187">2023.acl-srw.47</url>
      <bibkey>kader-etal-2023-words</bibkey>
    </paper>
    <paper id="48">
      <title>Semantic Accuracy in Natural Language Generation: A Thesis Proposal</title>
      <author><first>Patricia</first><last>Schmidtova</last><affiliation>Charles University</affiliation></author>
      <pages>352-361</pages>
      <abstract>We propose a thesis in which we explore how evaluation and interpretability techniques could lead to better natural language generation systems.</abstract>
      <url hash="a4d44d8d">2023.acl-srw.48</url>
      <bibkey>schmidtova-2023-semantic</bibkey>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2023-07-06">
    <meta>
      <booktitle>Proceedings of the System Demonstrations</booktitle>
      <editor><first>Danushka</first><last>Bollegala</last></editor>
      <editor><first>Ruihong</first><last>Huang</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada.</address>
      <month>July</month>
      <year>2023</year>
      <url hash="71fe706c">2023.acl-demo</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="8dbfcbe9">2023.acl-demo.0</url>
      <bibkey>acl-2023-system</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Human-in-the-loop Schema Induction</title>
      <author><first>Tianyi</first><last>Zhang</last><affiliation>Department of Computer and Information Science, University of Pennsylvania</affiliation></author>
      <author><first>Isaac</first><last>Tham</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Zhaoyi</first><last>Hou</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Jiaxuan</first><last>Ren</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Leon</first><last>Zhou</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Hainiu</first><last>Xu</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Li</first><last>Zhang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Lara</first><last>Martin</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author><first>Rotem</first><last>Dror</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Sha</first><last>Li</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois at Urbana-Champaign and Amazon (Amazon Scholar)</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado</affiliation></author>
      <author><first>Susan Windisch</first><last>Brown</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Reece</first><last>Suchocki</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>1-10</pages>
      <abstract>Schema induction builds a graph representation explaining how events unfold in a scenario. Existing approaches have been based on information retrieval (IR) and information extraction (IE), often with limited human curation. We demonstrate a human-in-the-loop schema induction system powered by GPT-3. We first describe the different modules of our system, including prompting to generate schematic elements, manual edit of those elements, and conversion of those into a schema graph. By qualitatively comparing our system to previous ones, we show that our system not only transfers to new domains more easily than previous approaches, but also reduces efforts of human curation thanks to our interactive interface.</abstract>
      <url hash="4a2342e8">2023.acl-demo.1</url>
      <bibkey>zhang-etal-2023-human</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>P</fixed-case>ers<fixed-case>LEARN</fixed-case>: Research Training through the Lens of Perspective Cultivation</title>
      <author><first>Yu-Zhe</first><last>Shi</last><affiliation>Peking University</affiliation></author>
      <author><first>Shiqian</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Xinyi</first><last>Niu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Qiao</first><last>Xu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Jiawen</first><last>Liu</last><affiliation>Tongji University</affiliation></author>
      <author><first>Yifan</first><last>Xu</last><affiliation>Kyushu University</affiliation></author>
      <author><first>Shiyu</first><last>Gu</last><affiliation>Peking University</affiliation></author>
      <author><first>Bingru</first><last>He</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xinyang</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xinyu</first><last>Zhao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zijian</first><last>Zhao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yidong</first><last>Lyu</last><affiliation>Chinese University of Hong Kong</affiliation></author>
      <author><first>Zhen</first><last>Li</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Sijia</first><last>Liu</last><affiliation>University of the Arts London</affiliation></author>
      <author><first>Lin</first><last>Qiu</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Jinhao</first><last>Ji</last><affiliation>Boston College</affiliation></author>
      <author><first>Lecheng</first><last>Ruan</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Yuxi</first><last>Ma</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Wenjuan</first><last>Han</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yixin</first><last>Zhu</last><affiliation>Peking University</affiliation></author>
      <pages>11-30</pages>
      <abstract>Scientific research is inherently shaped by its authors’ perspectives, influenced by various factorssuch as their personality, community, or society. Junior researchers often face challenges in identifying the perspectives reflected in the existing literature and struggle to develop their own viewpoints. In response to this issue, we introduce PersLEARN , a tool designed to facilitate the cultivation of scientific perspectives, starting from a basic seed idea and progressing to a well-articulated framework. By interacting with a prompt-based model, researchers can develop their perspectives explicitly. Our humanstudy reveals that scientific perspectives developed by students using PersLEARN exhibit a superior level of logical coherence and depth compared to those that did not. Furthermore, our pipeline outperforms baseline approaches across multiple domains of literature from various perspectives. These results suggest that PersLEARN could help foster a greater appreciation of diversity in scientific perspectives as an essential component of research training.</abstract>
      <url hash="5a04b1c9">2023.acl-demo.2</url>
      <bibkey>shi-etal-2023-perslearn</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>LAVIS</fixed-case>: A One-stop Library for Language-Vision Intelligence</title>
      <author><first>Dongxu</first><last>Li</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Junnan</first><last>Li</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Hung</first><last>Le</last><affiliation>Salesforce</affiliation></author>
      <author><first>Guangsen</first><last>Wang</last><affiliation>Salesforce Research Asia</affiliation></author>
      <author><first>Silvio</first><last>Savarese</last><affiliation>Salesforce</affiliation></author>
      <author><first>Steven C.H.</first><last>Hoi</last><affiliation>Salesforce</affiliation></author>
      <pages>31-41</pages>
      <abstract>We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks.</abstract>
      <url hash="53c67a31">2023.acl-demo.3</url>
      <bibkey>li-etal-2023-lavis</bibkey>
    </paper>
    <paper id="4">
      <title>Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases among Foundation Models</title>
      <author><first>Bum Chul</first><last>Kwon</last><affiliation>IBM Research</affiliation></author>
      <author><first>Nandana</first><last>Mihindukulasooriya</last><affiliation>IBM Research AI</affiliation></author>
      <pages>42-50</pages>
      <abstract>Pre-trained transformer-based language models are becoming increasingly popular due to their exceptional performance on various benchmarks. However, concerns persist regarding the presence of hidden biases within these models, which can lead to discriminatory outcomes and reinforce harmful stereotypes. To address this issue, we propose Finspector, a human-centered visual inspection tool designed to detect biases in different categories through log-likelihood scores generated by language models. The goal of the tool is to enable researchers to easily identify potential biases using visual analytics, ultimately contributing to a fairer and more just deployment of these models in both academic and industrial settings. Finspector is available at https://github.com/IBM/finspector.</abstract>
      <url hash="e8322338">2023.acl-demo.4</url>
      <bibkey>kwon-mihindukulasooriya-2023-finspector</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>P</fixed-case>rime<fixed-case>QA</fixed-case>: The Prime Repository for State-of-the-Art Multilingual Question Answering Research and Development</title>
      <author><first>Avi</first><last>Sil</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Jaydeep</first><last>Sen</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Bhavani</first><last>Iyer</last><affiliation>Ibm</affiliation></author>
      <author><first>Martin</first><last>Franz</last><affiliation>IBM T.J. Watson Research Center</affiliation></author>
      <author><first>Kshitij</first><last>Fadnis</last><affiliation>IBM Research</affiliation></author>
      <author><first>Mihaela</first><last>Bornea</last><affiliation>IBM Research</affiliation></author>
      <author><first>Sara</first><last>Rosenthal</last><affiliation>IBM Research</affiliation></author>
      <author><first>Scott</first><last>McCarley</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Rong</first><last>Zhang</last><affiliation>IBM.com</affiliation></author>
      <author><first>Vishwajeet</first><last>Kumar</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Yulong</first><last>Li</last><affiliation>IBM research</affiliation></author>
      <author><first>Md Arafat</first><last>Sultan</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Riyaz</first><last>Bhat</last><affiliation>Ibm Irl</affiliation></author>
      <author><first>Juergen</first><last>Bross</last><affiliation>IBM Research</affiliation></author>
      <author><first>Radu</first><last>Florian</last><affiliation>IBM Research</affiliation></author>
      <author><first>Salim</first><last>Roukos</last><affiliation>IBM Research AI</affiliation></author>
      <pages>51-62</pages>
      <abstract>The field of Question Answering (QA) has made remarkable progress in recent years, thanks to the advent of large pre-trained language models, newer realistic benchmark datasets with leaderboards, and novel algorithms for key components such as retrievers and readers. In this paper, we introduce PrimeQA: a one-stop and open-source QA repository with an aim to democratize QA research and facilitate easy replication of state-of-the-art (SOTA) QA methods. PrimeQA supports core QA functionalities like retrieval and reading comprehension as well as auxiliary capabilities such as question generation. It has been designed as an end-to-end toolkit for various use cases: building front-end applications, replicating SOTA methods on public benchmarks, and expanding pre-existing methods. PrimeQA is available at: https://github.com/primeqa.</abstract>
      <url hash="3eca73cb">2023.acl-demo.5</url>
      <bibkey>sil-etal-2023-primeqa</bibkey>
    </paper>
    <paper id="6">
      <title>Lingxi: A Diversity-aware <fixed-case>C</fixed-case>hinese Modern Poetry Generation System</title>
      <author><first>Xinran</first><last>Zhang</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jiafeng</first><last>Liu</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Xiaobing</first><last>Li</last><affiliation>Central Conservatory of Music</affiliation></author>
      <pages>63-75</pages>
      <abstract>Chinese modern poetry generation has been a challenging task. One issue is the Chinese word segmentation (CWS) which is critical to comprehend the Chinese language but was not always considered in common tokenization methods. Another is the decoding (sampling) method which may induce repetition and boredom and severely lower the diversity of the generated poetry. To address these issues, we present Lingxi, a diversity-aware Chinese modern poetry generation system. For the CWS issue, we propose a novel framework that incorporates CWS in the tokenization process. The proposed method can achieve a high vocabulary coverage rate with a reasonable vocabulary size. For the decoding method and the diversity issue, we propose a novel sampling algorithm that flattens the high likelihood part of the predicted distribution of the language model to emphasize the comparatively low-likelihood words and increase the diversity of generated poetry. Empirical results show that even when the top 60% of cumulative probability mass of the predicted distribution is flattened, our method achieves comparable or even better performance than baseline sampling methods. Our system is available at http://lingxi.website.</abstract>
      <url hash="f8e6b332">2023.acl-demo.6</url>
      <bibkey>zhang-etal-2023-lingxi</bibkey>
    </paper>
    <paper id="7">
      <title>Autodive: An Integrated Onsite Scientific Literature Annotation Tool</title>
      <author><first>Yi</first><last>Du</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ludi</first><last>Wang</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <author><first>Mengyi</first><last>Huang</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <author><first>Dongze</first><last>Song</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wenjuan</first><last>Cui</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuanchun</first><last>Zhou</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <pages>76-85</pages>
      <abstract>Scientific literature is always available in Adobe’s Portable Document Format (PDF), which is friendly for scientists to read. Compared with raw text, annotating directly on PDF documents can greatly improve the labeling efficiency of scientists whose annotation costs are very high. In this paper, we present Autodive, an integrated onsite scientific literature annotation tool for natural scientists and Natural Language Processing (NLP) researchers. This tool provides six core functions of annotation that support the whole lifecycle of corpus generation including i)annotation project management, ii)resource management, iii)ontology management, iv)manual annotation, v)onsite auto annotation, and vi)annotation task statistic. Two experiments are carried out to verify efficiency of the presented tool. A live demo of Autodive is available at http://autodive.sciwiki.cn. The source code is available at https://github.com/Autodive.</abstract>
      <url hash="8ab7c2d9">2023.acl-demo.7</url>
      <bibkey>du-etal-2023-autodive</bibkey>
    </paper>
    <paper id="8">
      <title>A Practical Toolkit for Multilingual Question and Answer Generation</title>
      <author><first>Asahi</first><last>Ushio</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Fernando</first><last>Alva-Manchego</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Jose</first><last>Camacho-Collados</last><affiliation>Cardiff University</affiliation></author>
      <pages>86-94</pages>
      <abstract>Generating questions along with associated answers from a text has applications in several domains, such as creating reading comprehension tests for students, or improving document search by providing auxiliary questions and answers based on the query. Training models for question and answer generation (QAG) is not straightforward due to the expected structured output (i.e. a list of question and answer pairs), as it requires more than generating a single sentence. This results in a small number of publicly accessible QAG models. In this paper, we introduce AutoQG, an online service for multilingual QAG along with lmqg, an all-in-one python package for model fine-tuning, generation, and evaluation. We also release QAG models in eight languages fine-tuned on a few variants of pre-trained encoder-decoder language models, which can be used online via AutoQG or locally via lmqg. With these resources, practitioners of any level can benefit from a toolkit that includes a web interface for end users, and easy-to-use code for developers who require custom models or fine-grained controls for generation.</abstract>
      <url hash="d8a24244">2023.acl-demo.8</url>
      <bibkey>ushio-etal-2023-practical</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>O</fixed-case>pen<fixed-case>SLU</fixed-case>: A Unified, Modularized, and Extensible Toolkit for Spoken Language Understanding</title>
      <author><first>Libo</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Qiguang</first><last>Chen</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xiao</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yunlong</first><last>Feng</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>95-102</pages>
      <abstract>Spoken Language Understanding (SLU) is one of the core components of a task-oriented dialogue system, which aims to extract the semantic meaning of user queries (e.g., intents and slots). In this work, we introduce OpenSLU, an open-source toolkit to provide a unified, modularized, and extensible toolkit for spoken language understanding. Specifically, OpenSLU unifies 10 SLU models for both single-intent and multi-intent scenarios, which support both non-pretrained and pretrained models simultaneously. Additionally, OpenSLU is highly modularized and extensible by decomposing the model architecture, inference, and learning process into reusable modules, which allows researchers to quickly set up SLU experiments with highly flexible configurations. OpenSLU is implemented based on PyTorch, and released at https://github.com/LightChen233/OpenSLU.</abstract>
      <url hash="3149735d">2023.acl-demo.9</url>
      <bibkey>qin-etal-2023-openslu</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>S</fixed-case>anskrit<fixed-case>S</fixed-case>hala: A Neural <fixed-case>S</fixed-case>anskrit <fixed-case>NLP</fixed-case> Toolkit with Web-Based Interface for Pedagogical and Annotation Purposes</title>
      <author><first>Jivnesh</first><last>Sandhan</last><affiliation>IIT Kanpur</affiliation></author>
      <author><first>Anshul</first><last>Agarwal</last><affiliation>IIT Kanpur</affiliation></author>
      <author><first>Laxmidhar</first><last>Behera</last><affiliation>IIT Kanpur / Mandi</affiliation></author>
      <author><first>Tushar</first><last>Sandhan</last><affiliation>IIT Kanpur</affiliation></author>
      <author><first>Pawan</first><last>Goyal</last><affiliation>IIT Kharagpur</affiliation></author>
      <pages>103-112</pages>
      <abstract>We present a neural Sanskrit Natural Language Processing (NLP) toolkit named SanskritShala (a school of Sanskrit) to facilitate computational linguistic analyses for several tasks such as word segmentation, morphological tagging, dependency parsing, and compound type identification. Our systems currently report state-of-the-art performance on available benchmark datasets for all tasks. SanskritShala is deployed as a web-based application, which allows a user to get real-time analysis for the given input. It is built with easy-to-use interactive data annotation features that allow annotators to correct the system predictions when it makes mistakes. We publicly release the source codes of the 4 modules included in the toolkit, 7 word embedding models that have been trained on publicly available Sanskrit corpora and multiple annotated datasets such as word similarity, relatedness, categorization, analogy prediction to assess intrinsic properties of word embeddings. So far as we know, this is the first neural-based Sanskrit NLP toolkit that has a web-based interface and a number of NLP modules. We are sure that the people who are willing to work with Sanskrit will find it useful for pedagogical and annotative purposes. SanskritShala is available at: https://cnerg.iitkgp.ac.in/sanskritshala. The demo video of our platform can be accessed at: https://youtu.be/x0X31Y9k0mw4.</abstract>
      <url hash="383583db">2023.acl-demo.10</url>
      <bibkey>sandhan-etal-2023-sanskritshala</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>LIDA</fixed-case>: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models</title>
      <author><first>Victor</first><last>Dibia</last><affiliation>Microsoft Research</affiliation></author>
      <pages>113-126</pages>
      <abstract>Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) and image generation models (IGMs) are suitable to addressing these tasks. We present LIDA, a novel tool for generating grammar-agnostic visualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER that converts data into a rich but compact natural language summary, a GOAL EXPLORER that enumerates visualization goals given the data, a VISGENERATOR that generates, refines, executes and filters visualization code and an INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA provides a python api, and a hybrid user interface (direct manipulation and multilingual natural language) for interactive chart, infographics and data story generation. Code and demo are available at this url - https://microsoft.github.io/lida/</abstract>
      <url hash="779dfdb3">2023.acl-demo.11</url>
      <bibkey>dibia-2023-lida</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>M</fixed-case>eta<fixed-case>P</fixed-case>ro Online: A Computational Metaphor Processing Online System</title>
      <author><first>Rui</first><last>Mao</last><affiliation>Ruimao Tech</affiliation></author>
      <author><first>Xiao</first><last>Li</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Kai</first><last>He</last><affiliation>School of Electronic and Information Engineering, Xi’an Jiaotong University</affiliation></author>
      <author><first>Mengshi</first><last>Ge</last><affiliation>Nanyang Technology University</affiliation></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>127-135</pages>
      <abstract>Metaphoric expressions are a special linguistic phenomenon, frequently appearing in everyday language. Metaphors do not take their literal meanings in contexts, which may cause obstacles for language learners to understand them. Metaphoric expressions also reflect the cognition of humans via concept mappings, attracting great attention from cognitive science and psychology communities. Thus, we aim to develop a computational metaphor processing online system, termed MetaPro Online, that allows users without a coding background, e.g., language learners and linguists, to easily query metaphoricity labels, metaphor paraphrases, and concept mappings for non-domain-specific text. The outputs of MetaPro can be directly used by language learners and natural language processing downstream tasks because MetaPro is an end-to-end system.</abstract>
      <url hash="dbe22762">2023.acl-demo.12</url>
      <bibkey>mao-etal-2023-metapro</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>DIAGRAPH</fixed-case>: An Open-Source Graphic Interface for Dialog Flow Design</title>
      <author><first>Dirk</first><last>Väth</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Lindsey</first><last>Vanderlyn</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Ngoc Thang</first><last>Vu</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>136-143</pages>
      <abstract>In this work, we present DIAGRAPH, an open-source graphical dialog flow editor built on the ADVISER toolkit. Our goal for this tool is threefold: 1) To support subject-experts to intuitively create complex and flexible dialog systems,2) To support rapid prototyping of dialog system behavior, e.g., for research, and 3) To provide a hands-on test bed for students learning about dialog systems.To facilitate this, DIAGRAPH aims to provide a clean and intuitive graphical interface for creating dialog systems without requiring any coding knowledge.Once a dialog graph has been created, it is automatically turned into a dialog system using state of the art language models. This allows for rapid prototyping and testing.Dialog designers can then distribute a link to their finished dialog system or embed it into a website.Additionally, to support scientific experiments and data collection, dialog designers can access chat logs. Finally, to verify the usability of DIAGRAPH, we performed evaluation with subject-experts who extensively worked with the tool and users testing it for the first time, receiving above average System Usability Scale (SUS) scores from both (82 out 100 and 75 out of 100, respectively).In this way, we hope DIAGRAPH helps reduce the barrier to entry for creating dialog interactions.</abstract>
      <url hash="2a38585d">2023.acl-demo.13</url>
      <bibkey>vath-etal-2023-diagraph</bibkey>
    </paper>
    <paper id="14">
      <title>disco: a toolkit for Distributional Control of Generative Models</title>
      <author><first>Germán</first><last>Kruszewski</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>Jos</first><last>Rozen</last><affiliation>NAVER LABS Europe</affiliation></author>
      <author><first>Marc</first><last>Dymetman</last><affiliation>Independent researcher</affiliation></author>
      <pages>144-160</pages>
      <abstract>Pre-trained language models and other generative models have revolutionized NLP and beyond. However, these models tend to reproduce undesirable biases present in their training data. Also, they may overlook patterns that are important but challenging to capture. To address these limitations, researchers have introduced distributional control techniques. These techniques, not limited to language, allow controlling the prevalence (i.e. expectations) of any features of interest in the model’s outputs. Despite their potential, the widespread adoption of these techniques has been hindered by the difficulty in adapting the complex, disconnected code. Here, we present disco, an open-source Python library that brings these techniques to the broader public</abstract>
      <url hash="9b3f1943">2023.acl-demo.14</url>
      <bibkey>kruszewski-etal-2023-disco</bibkey>
    </paper>
    <paper id="15">
      <title>A Hyperparameter Optimization Toolkit for Neural Machine Translation Research</title>
      <author><first>Xuan</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Kevin</first><last>Duh</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Paul</first><last>McNamee</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>161-168</pages>
      <abstract>Hyperparameter optimization is an important but often overlooked process in the research of deep learning technologies. To obtain a good model, one must carefully tune hyperparameters that determine the architecture and training algorithm. Insufficient tuning may result in poor results, while inequitable tuning may lead to exaggerated differences between models. We present a hyperparameter optimization toolkit for neural machine translation (NMT) to help researchers focus their time on the creative rather than the mundane. The toolkit is implemented as a wrapper on top of the open-source Sockeye NMT software. Using the Asynchronous Successive Halving Algorithm (ASHA), we demonstrate that it is possible to discover near-optimal models under a computational budget with little effort.Code: https://github.com/kevinduh/sockeye-recipes3Video demo: https://cs.jhu.edu/ kevinduh/j/demo.mp4</abstract>
      <url hash="55512c6d">2023.acl-demo.15</url>
      <bibkey>zhang-etal-2023-hyperparameter</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>J</fixed-case>apanese-to-<fixed-case>E</fixed-case>nglish Simultaneous Dubbing Prototype</title>
      <author><first>Xiaolin</first><last>Wang</last><affiliation>Nict</affiliation></author>
      <author><first>Masao</first><last>Utiyama</last><affiliation>Nict</affiliation></author>
      <author><first>Eiichiro</first><last>Sumita</last><affiliation>Nict</affiliation></author>
      <pages>169-178</pages>
      <abstract>Live video streaming has become an important form of communication such as virtual conferences. However, for cross-language communication in live video streaming, reading subtitles degrades the viewing experience. To address this problem, our simultaneous dubbing prototype translates and replaces the original speech of a live video stream in a simultaneous manner. Tests on a collection of 90 public videos show that our system achieves a low average latency of 11.90 seconds for smooth playback. Our method is general and can be extended to other language pairs.</abstract>
      <url hash="786216f1">2023.acl-demo.16</url>
      <bibkey>wang-etal-2023-japanese</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>V</fixed-case>is<fixed-case>K</fixed-case>o<fixed-case>P</fixed-case>: Visual Knowledge oriented Programming for Interactive Knowledge Base Question Answering</title>
      <author><first>Zijun</first><last>Yao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yuanyong</first><last>Chen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xin</first><last>Lv</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shulin</first><last>Cao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Amy</first><last>Xin</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jifan</first><last>Yu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Hailong</first><last>Jin</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jianjun</first><last>Xu</last><affiliation>beijing caizhi technology co.,LTD.</affiliation></author>
      <author><first>Peng</first><last>Zhang</last><affiliation>Zhipu.AI</affiliation></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <pages>179-189</pages>
      <abstract>We present Visual Knowledge oriented Programming platform (&lt;b&gt;VisKoP&lt;/b&gt;), a knowledge base question answering (KBQA) system that integrates human into the loop to edit and debug the knowledge base (KB) queries. VisKoP not only provides a neural program induction module, which converts natural language questions into knowledge oriented program language (KoPL), but also maps KoPL programs into graphical elements. KoPL programs can be edited with simple graphical operators, such as &lt;i&gt;”dragging”&lt;/i&gt; to add knowledge operators and &lt;i&gt;”slot filling”&lt;/i&gt; to designate operator arguments. Moreover, VisKoP provides auto-completion for its knowledge base schema and users can easily debug the KoPL program by checking its intermediate results. To facilitate the practical KBQA on a million-entity-level KB, we design a highly efficient KoPL execution engine for the back-end. Experiment results show that VisKoP is highly efficient and user interaction can fix a large portion of wrong KoPL programs to acquire the correct answer. The VisKoP online demo, highly efficient KoPL engine, and screencast video are now publicly available.</abstract>
      <url hash="06bf026d">2023.acl-demo.17</url>
      <bibkey>yao-etal-2023-viskop</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>PEEP</fixed-case>-Talk: A Situational Dialogue-based Chatbot for <fixed-case>E</fixed-case>nglish Education</title>
      <author><first>Seugnjun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Yoonna</first><last>Jang</last><affiliation>Department of Computer Science and Engineering, Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Upstage</affiliation></author>
      <author><first>Jungseob</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaehyung</first><last>Seo</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Sugyeong</first><last>Eo</last><affiliation>Korea University</affiliation></author>
      <author><first>Seounghoon</first><last>Lee</last><affiliation>Institute for Infocomm Research, A*STAR</affiliation></author>
      <author><first>Bernardo</first><last>Yahya</last><affiliation>Hankuk University of Foreign Studies</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>190-207</pages>
      <abstract>English is acknowledged worldwide as a mode of communication. However, due to the absence of realistic practicing scenarios, students learning English as a foreign language (EFL) typically have limited chances to converse and share feedback with others. In this paper, we propose PEEP-Talk, a real-world situational dialogue-based chatbot designed for English education. It also naturally switches to a new topic or situation in response to out-of-topic utterances, which are common among English beginners. Furthermore, PEEP-Talk provides feedback score on conversation and grammar error correction. We performed automatic and user evaluations to validate performance and education efficiency of our system. The results show that PEEP-Talk generates appropriate responses in various real-life situations while providing accurate feedback to learners. Moreover, we demonstrate a positive impact on English-speaking, grammar, and English learning anxiety, implying that PEEP-Talk can lower the barrier to learning natural conversation in effective ways.</abstract>
      <url hash="5d80598e">2023.acl-demo.18</url>
      <bibkey>lee-etal-2023-peep</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>O</fixed-case>pen<fixed-case>TIPE</fixed-case>: An Open-source Translation Framework for Interactive Post-Editing Research</title>
      <author><first>Fabian</first><last>Landwehr</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Thomas</first><last>Steinmann</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Laura</first><last>Mascarell</last><affiliation>ETH Zurich</affiliation></author>
      <pages>208-216</pages>
      <abstract>Despite the latest improvements on machine translation, professional translators still must review and post-edit the automatic output to ensure high-quality translations. The research on automating this process lacks an interactive post-editing environment implemented for this purpose; therefore, current approaches do not consider the human interactions that occur in real post-editing scenarios. To address this issue, we present OpenTIPE, a flexible and extensible framework that aims at supporting research on interactive post-editing. Specifically, the interactive environment of OpenTIPE allows researchers to explore human-centered approaches for the post-editing task. We release the OpenTIPE source code and showcase its main functionalities with a demonstration video and an online live demo.</abstract>
      <url hash="80c04c5a">2023.acl-demo.19</url>
      <bibkey>landwehr-etal-2023-opentipe</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>T</fixed-case>encent<fixed-case>P</fixed-case>retrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities</title>
      <author><first>Zhe</first><last>Zhao</last><affiliation>Tencent</affiliation></author>
      <author><first>Yudong</first><last>Li</last><affiliation>China University of Geosciences (Beijing)</affiliation></author>
      <author><first>Cheng</first><last>Hou</last><affiliation>Tencent</affiliation></author>
      <author><first>Jing</first><last>Zhao</last><affiliation>Tencent</affiliation></author>
      <author><first>Rong</first><last>Tian</last><affiliation>Tencent</affiliation></author>
      <author><first>Weijie</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Yiren</first><last>Chen</last><affiliation>Tencent</affiliation></author>
      <author><first>Ningyuan</first><last>Sun</last><affiliation>Tencent</affiliation></author>
      <author><first>Haoyan</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Weiquan</first><last>Mao</last><affiliation>Tencent</affiliation></author>
      <author><first>Han</first><last>Guo</last><affiliation>Tencent</affiliation></author>
      <author><first>Weigang</first><last>Gou</last><affiliation>Tencent</affiliation></author>
      <author><first>Taiqiang</first><last>Wu</last><affiliation>Tencent</affiliation></author>
      <author><first>Tao</first><last>Zhu</last><affiliation>Tencent</affiliation></author>
      <author><first>Wenhang</first><last>Shi</last><affiliation>RENMIN UNIVERSITY of CHINA</affiliation></author>
      <author><first>Chen</first><last>Chen</last><affiliation>Tencent</affiliation></author>
      <author><first>Shan</first><last>Huang</last><affiliation>Tencent</affiliation></author>
      <author><first>Sihong</first><last>Chen</last><affiliation>Tencent</affiliation></author>
      <author><first>Liqun</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Feifei</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Xiaoshuai</first><last>Chen</last><affiliation>Tencent</affiliation></author>
      <author><first>Xingwu</first><last>Sun</last><affiliation>Tencent</affiliation></author>
      <author><first>Zhanhui</first><last>Kang</last><affiliation>Tencent</affiliation></author>
      <author><first>Xiaoyong</first><last>Du</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Linlin</first><last>Shen</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Kimmo</first><last>Yan</last><affiliation>Tencent</affiliation></author>
      <pages>217-225</pages>
      <abstract>Recently, the success of pre-training in text domain has been fully extended to vision, audio, and cross-modal scenarios. The proposed pre-training models of different modalities are showing a rising trend of homogeneity in their model structures, which brings the opportunity to implement different pre-training models within a uniform framework. In this paper, we present TencentPretrain, a toolkit supporting pre-training models of different modalities. The core feature of TencentPretrain is the modular design. The toolkit uniformly divides pre-training models into 5 components: embedding, encoder, target embedding, decoder, and target. As almost all of common modules are provided in each component, users can choose the desired modules from different components to build a complete pre-training model. The modular design enables users to efficiently reproduce existing pre-training models or build brand-new one. We test the toolkit on text, vision, and audio benchmarks and show that it can match the performance of the original implementations.</abstract>
      <url hash="61ba5b4c">2023.acl-demo.20</url>
      <bibkey>zhao-etal-2023-tencentpretrain</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>N</fixed-case>euro<fixed-case>X</fixed-case> Library for Neuron Analysis of Deep <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Fahim</first><last>Dalvi</last><affiliation>Qatar Computing Research Institute, HBKU</affiliation></author>
      <author><first>Hassan</first><last>Sajjad</last><affiliation>Dalhousie University</affiliation></author>
      <author><first>Nadir</first><last>Durrani</last><affiliation>Qcri</affiliation></author>
      <pages>226-234</pages>
      <abstract>Neuron analysis provides insights into how knowledge is structured in representations and discovers the role of neurons in the network. In addition to developing an understanding of our models, neuron analysis enables various applications such as debiasing, domain adaptation and architectural search. We present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis of natural language processing models. It implements various interpretation methods under a unified API, and provides a framework for data processing and evaluation, thus making it easier for researchers and practitioners to perform neuron analysis. The Python toolkit is available at https://www.github.com/fdalvi/NeuroX.Demo Video available at: https://youtu.be/mLhs2YMx4u8</abstract>
      <url hash="18071e2e">2023.acl-demo.21</url>
      <bibkey>dalvi-etal-2023-neurox</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>S</fixed-case>ci<fixed-case>L</fixed-case>it: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation</title>
      <author><first>Nianlong</first><last>Gu</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Richard H.R.</first><last>Hahnloser</last><affiliation>ETH Zurich</affiliation></author>
      <pages>235-246</pages>
      <abstract>Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes. Although in many workflows these processes are serially linked, there are opportunities for natural language processing (NLP) to provide end-to-end assistive tools. We propose SciLit, a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and keywords. SciLit efficiently recommends papers from large databases of hundreds of millions of papers using a two-stage pre-fetching and re-ranking literature search system that flexibly deals with addition and removal of a paper database. We provide a convenient user interface that displays the recommended papers as extractive summaries and that offers abstractively-generated citing sentences which are aligned with the provided context and which mention the chosen keyword(s). Our assistive tool for literature discovery and scientific writing is available at https://scilit.vercel.app</abstract>
      <url hash="a3232d6e">2023.acl-demo.22</url>
      <bibkey>gu-hahnloser-2023-scilit</bibkey>
    </paper>
    <paper id="23">
      <title>Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search</title>
      <author><first>Chris</first><last>Jenkins</last><affiliation>University of Southern California Information Sciences Institute</affiliation></author>
      <author><first>Shantanu</first><last>Agarwal</last><affiliation>Information Sciences Institute, University of Southern California</affiliation></author>
      <author><first>Joel</first><last>Barry</last><affiliation>University of Southern California Information Sciences Institute</affiliation></author>
      <author><first>Steven</first><last>Fincke</last><affiliation>University of Southern California Information Sciences Institute</affiliation></author>
      <author><first>Elizabeth</first><last>Boschee</last><affiliation>Information Sciences Institute</affiliation></author>
      <pages>247-256</pages>
      <abstract>In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual, zero-shot event extraction system and accompanying user interface for event visualization &amp; search. Using only English training data, ISI-Clear makes global events available on-demand, processing user-supplied text in 100 languages ranging from Afrikaans to Yiddish. We provide multiple event-centric views of extracted events, including both a graphical representation and a document-level summary. We also integrate existing cross-lingual search algorithms with event extraction capabilities to provide cross-lingual event-centric search, allowing English-speaking users to search over events automatically extracted from a corpus of non-English documents, using either English natural language queries (e.g. “cholera outbreaks in Iran”) or structured queries (e.g. find all events of type Disease-Outbreak with agent “cholera” and location “Iran”).</abstract>
      <url hash="96533dd8">2023.acl-demo.23</url>
      <bibkey>jenkins-etal-2023-massively</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>YANMTT</fixed-case>: Yet Another Neural Machine Translation Toolkit</title>
      <author><first>Raj</first><last>Dabre</last><affiliation>Nict</affiliation></author>
      <author><first>Diptesh</first><last>Kanojia</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Chinmay</first><last>Sawant</last><affiliation>Surrey University</affiliation></author>
      <author><first>Eiichiro</first><last>Sumita</last><affiliation>Nict</affiliation></author>
      <pages>257-263</pages>
      <abstract>In this paper, we present our open-source neural machine translation (NMT) toolkit called “Yet Another Neural Machine Translation Toolkit” abbreviated as YANMTT - https://github.com/prajdabre/yanmtt, which is built on top of the HuggingFace Transformers library. YANMTT focuses on transfer learning and enables easy pre-training and fine-tuning of sequence-to-sequence models at scale. It can be used for training parameter-heavy models with minimal parameter sharing and efficient, lightweight models via heavy parameter sharing. Additionally, it supports parameter-efficient fine-tuning (PEFT) through adapters and prompts. Our toolkit also comes with a user interface that can be used to demonstrate these models and visualize various parts of the model. Apart from these core features, our toolkit also provides other advanced functionalities such as but not limited to document/multi-source NMT, simultaneous NMT, mixtures-of-experts, model compression and continual learning.</abstract>
      <url hash="dd54c284">2023.acl-demo.24</url>
      <bibkey>dabre-etal-2023-yanmtt</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>XMD</fixed-case>: An End-to-End Framework for Interactive Explanation-Based Debugging of <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Dong-Ho</first><last>Lee</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Akshen</first><last>Kadakia</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Brihi</first><last>Joshi</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Aaron</first><last>Chan</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Ziyi</first><last>Liu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Kiran</first><last>Narahari</last><affiliation>Information Sciences Institute</affiliation></author>
      <author><first>Takashi</first><last>Shibuya</last><affiliation>Sony Corporation</affiliation></author>
      <author><first>Ryosuke</first><last>Mitani</last><affiliation>Sony Group</affiliation></author>
      <author><first>Toshiyuki</first><last>Sekiya</last><affiliation>Sony Group Corporation</affiliation></author>
      <author><first>Jay</first><last>Pujara</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xiang</first><last>Ren</last><affiliation>University of Southern California</affiliation></author>
      <pages>264-273</pages>
      <abstract>NLP models are susceptible to learning spurious biases (i.e., bugs) that work on some datasets but do not properly reflect the underlying task. Explanation-based model debugging aims to resolve spurious biases by showing human users explanations of model behavior, asking users to give feedback on the behavior, thenusing the feedback to update the model. While existing model debugging methods have shown promise, their prototype-level implementations provide limited practical utility. Thus, we propose XMD: the first open-source, end-to-end framework for explanation-based model debugging. Given task- or instance-level explanations,users can flexibly provide various forms of feedback via an intuitive, web-based UI. After receiving user feedback, XMD automatically updates the model in real time, by regularizing the model so that its explanationsalign with the user feedback. The new model can then be easily deployed into real-world applications via Hugging Face. Using XMD, we can improve the model’s OOD performance on text classification tasks by up to 18%.</abstract>
      <url hash="cc4a4e56">2023.acl-demo.25</url>
      <bibkey>lee-etal-2023-xmd</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>O</fixed-case>pen<fixed-case>D</fixed-case>elta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models</title>
      <author><first>Shengding</first><last>Hu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ning</first><last>Ding</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Weilin</first><last>Zhao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xingtai</first><last>Lv</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhen</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>274-281</pages>
      <abstract>The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as “delta tuning” in Ding et al. (2022), which updates only a small subset of parameters, known as “delta modules”, while keeping the backbone model’s parameters fixed. However, the practicality and flexibility of delta tuning have been limited due to existing implementations that directly modify the code of the backbone PTMs and hard-code specific delta tuning methods for each PTM. In this paper, we present OpenDelta, an open-source library that overcomes these limitations by providing a plug-and-play implementation of various delta tuning methods. Our novel techniques eliminate the need to modify the backbone PTMs’ code, making OpenDelta compatible with different, even novel PTMs. OpenDelta is designed to be simple, modular, and extensible, providing a comprehensive platform for researchers and practitioners to adapt large PTMs efficiently.</abstract>
      <url hash="38099019">2023.acl-demo.26</url>
      <bibkey>hu-etal-2023-opendelta</bibkey>
    </paper>
    <paper id="27">
      <title>Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation</title>
      <author><first>Itay</first><last>Yair</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Hillel</first><last>Taub-Tabib</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yoav</first><last>Goldberg</last><affiliation>Bar Ilan University</affiliation></author>
      <pages>282-290</pages>
      <abstract>Information extraction systems often producehundreds to thousands of strings on a specifictopic. We present a method that facilitatesbetter consumption of these strings, in an ex-ploratory setting in which a user wants to bothget a broad overview of what’s available, and achance to dive deeper on some aspects. The sys-tem works by grouping similar items together,and arranging the remaining items into a hierar-chical navigable DAG structure. We apply themethod to medical information extraction.</abstract>
      <url hash="19921ae2">2023.acl-demo.27</url>
      <bibkey>yair-etal-2023-hierarchy</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>CARE</fixed-case>: Collaborative <fixed-case>AI</fixed-case>-Assisted Reading Environment</title>
      <author><first>Dennis</first><last>Zyska</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <author><first>Nils</first><last>Dycke</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <author><first>Jan</first><last>Buchmann</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <author><first>Ilia</first><last>Kuznetsov</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <pages>291-303</pages>
      <abstract>Recent years have seen impressive progress in AI-assisted writing, yet the developments in AI-assisted reading are lacking. We propose inline commentary as a natural vehicle for AI-based reading assistance, and present CARE: the first open integrated platform for the study of inline commentary and reading. CARE facilitates data collection for inline commentaries in a commonplace collaborative reading environment, and provides a framework for enhancing reading with NLP-based assistance, such as text classification, generation or question answering. The extensible behavioral logging allows unique insights into the reading and commenting behavior, and flexible configuration makes the platform easy to deploy in new scenarios. To evaluate CARE in action, we apply the platform in a user study dedicated to scholarly peer review. CARE facilitates the data collection and study of inline commentary in NLP, extrinsic evaluation of NLP assistance, and application prototyping. We invite the community to explore and build upon the open source implementation of CARE.Github Repository: https://github.com/UKPLab/CAREPublic Live Demo: https://care.ukp.informatik.tu-darmstadt.de</abstract>
      <url hash="2632e07d">2023.acl-demo.28</url>
      <bibkey>zyska-etal-2023-care</bibkey>
    </paper>
    <paper id="29">
      <title>The <fixed-case>ROOTS</fixed-case> Search Tool: Data Transparency for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Aleksandra</first><last>Piktus</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Christopher</first><last>Akiki</last><affiliation>Leipzig University</affiliation></author>
      <author><first>Paulo</first><last>Villegas</last><affiliation>Telefonica I+D</affiliation></author>
      <author><first>Hugo</first><last>Laurençon</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Gérard</first><last>Dupont</last><affiliation>Mavenoid</affiliation></author>
      <author><first>Sasha</first><last>Luccioni</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Yacine</first><last>Jernite</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Anna</first><last>Rogers</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>304-314</pages>
      <abstract>ROOTS is a 1.6TB multilingual text corpus developed for the training of BLOOM, currently the largest language model explicitly accompanied by commensurate data governance efforts. In continuation of these efforts, we present the ROOTS Search Tool: a search engine over the entire ROOTS corpus offering both fuzzy and exact search capabilities. ROOTS is the largest corpus to date that can be investigated this way. The ROOTS Search Tool is open-sourced and available on Hugging Face Spaces: https://huggingface.co/spaces/bigscience-data/roots-search. We describe our implementation and the possible use cases of our tool.</abstract>
      <url hash="e3477782">2023.acl-demo.29</url>
      <bibkey>piktus-etal-2023-roots</bibkey>
    </paper>
    <paper id="30">
      <title>The <fixed-case>OPUS</fixed-case>-<fixed-case>MT</fixed-case> Dashboard – A Toolkit for a Systematic Evaluation of Open Machine Translation Models</title>
      <author><first>Jörg</first><last>Tiedemann</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Ona</first><last>de Gibert</last><affiliation>University of Helsinki</affiliation></author>
      <pages>315-327</pages>
      <abstract>The OPUS-MT dashboard is a web-based platform that provides a comprehensive overview of open translation models. We focus on a systematic collection of benchmark results with verifiable translation performance and large coverage in terms of languages and domains. We provide results for in-house OPUS-MT and Tatoeba models as well as external models from the Huggingface repository and user-contributed translations. The functionalities of the evaluation tool include summaries of benchmarks for over 2,300 models covering 4,560 language directions and 294 languages, as well as the inspection of predicted translations against their human reference. We focus on centralization, reproducibility and coverage of MT evaluation combined with scalability. The dashboard can be accessed live at https://opus.nlpl.eu/dashboard/.</abstract>
      <url hash="caebd787">2023.acl-demo.30</url>
      <bibkey>tiedemann-de-gibert-2023-opus</bibkey>
    </paper>
    <paper id="31">
      <title>The <fixed-case>D</fixed-case>-<fixed-case>WISE</fixed-case> Tool Suite: Multi-Modal Machine-Learning-Powered Tools Supporting and Enhancing Digital Discourse Analysis</title>
      <author><first>Florian</first><last>Schneider</last><affiliation>University of Hamburg</affiliation></author>
      <author><first>Tim</first><last>Fischer</last><affiliation>University of Hamburg</affiliation></author>
      <author><first>Fynn</first><last>Petersen-Frey</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Isabel</first><last>Eiser</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Gertraud</first><last>Koch</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>328-335</pages>
      <abstract>This work introduces the D-WISE Tool Suite (DWTS), a novel working environment for digital qualitative discourse analysis in the Digital Humanities (DH). The DWTS addresses limitations of current DH tools induced by the ever-increasing amount of heterogeneous, unstructured, and multi-modal data in which the discourses of contemporary societies are encoded. To provide meaningful insights from such data, our system leverages and combines state-of-the-art machine learning technologies from Natural Language Processing and Com-puter Vision. Further, the DWTS is conceived and developed by an interdisciplinary team ofcultural anthropologists and computer scientists to ensure the tool’s usability for modernDH research. Central features of the DWTS are: a) import of multi-modal data like text, image, audio, and video b) preprocessing pipelines for automatic annotations c) lexical and semantic search of documents d) manual span, bounding box, time-span, and frame annotations e) documentation of the research process.</abstract>
      <url hash="ac4609f8">2023.acl-demo.31</url>
      <bibkey>schneider-etal-2023-wise</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>O</fixed-case>pen<fixed-case>RT</fixed-case>: An Open-source Framework for Reasoning Over Tabular Data</title>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Boyu</first><last>Mi</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhenting</first><last>Qi</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Linyong</first><last>Nan</last><affiliation>Yale University</affiliation></author>
      <author><first>Minghao</first><last>Guo</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Dragomir</first><last>Radev</last><affiliation>Yale University</affiliation></author>
      <pages>336-347</pages>
      <abstract>There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing table pre-training models for performance comparison and develop new models quickly. We implemented and compared six table pre-training models on four question answering, one fact checking, and one faithful text generation datasets. Moreover, to enable the community to easily construct new table reasoning datasets, we developed TaRAT, an annotation tool which supports multi-person collaborative annotations for various kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed dataset to OpenRT and compare the performances of different baseline systems.</abstract>
      <url hash="d025d3dd">2023.acl-demo.32</url>
      <bibkey>zhao-etal-2023-openrt</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>UINAUIL</fixed-case>: A Unified Benchmark for <fixed-case>I</fixed-case>talian Natural Language Understanding</title>
      <author><first>Valerio</first><last>Basile</last><affiliation>University of Turin</affiliation></author>
      <author><first>Livio</first><last>Bioglio</last><affiliation>University of Turin</affiliation></author>
      <author><first>Alessio</first><last>Bosca</last><affiliation>CELI s.r.l.</affiliation></author>
      <author><first>Cristina</first><last>Bosco</last><affiliation>Dipartimento di Informatica - Università di Torino</affiliation></author>
      <author><first>Viviana</first><last>Patti</last><affiliation>University of Turin, Dipartimento di Informatica</affiliation></author>
      <pages>348-356</pages>
      <abstract>This paper introduces the Unified Interactive Natural Understanding of the Italian Language (UINAUIL), a benchmark of six tasks for Italian Natural Language Understanding. We present a description of the tasks and software library that collects the data from the European Language Grid, harmonizes the data format, and exposes functionalities to facilitates data manipulation and the evaluation of custom models. We also present the results of tests conducted with available Italian and multilingual language models on UINAUIL, providing an updated picture of the current state of the art in Italian NLU.</abstract>
      <url hash="ab30cb9d">2023.acl-demo.33</url>
      <bibkey>basile-etal-2023-uinauil</bibkey>
    </paper>
    <paper id="34">
      <title>Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction</title>
      <author><first>Gabriele</first><last>Picco</last><affiliation>IBM Research Europe</affiliation></author>
      <author><first>Marcos</first><last>Martinez Galindo</last><affiliation>IBM Research Europe</affiliation></author>
      <author><first>Alberto</first><last>Purpura</last><affiliation>IBM Research Europe</affiliation></author>
      <author><first>Leopold</first><last>Fuchs</last><affiliation>IBM Research Europe</affiliation></author>
      <author><first>Vanessa</first><last>Lopez</last><affiliation>IBM Research Europe</affiliation></author>
      <author><first>Thanh Lam</first><last>Hoang</last><affiliation>IBM Research Europe</affiliation></author>
      <pages>357-368</pages>
      <abstract>The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readily available APIs for production under the standard SpaCy NLP pipeline. Our API is extendible and evaluable, moreover, we include numerous enhancements such as boosting the accuracy with pipeline ensembling and visualization utilities available as a SpaCy extension.</abstract>
      <url hash="fe4b1f2f">2023.acl-demo.34</url>
      <bibkey>picco-etal-2023-zshot</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>B</fixed-case>i<fixed-case>S</fixed-case>ync: A Bilingual Editor for Synchronized Monolingual Texts</title>
      <author><first>Josep</first><last>Crego</last><affiliation>Systran</affiliation></author>
      <author><first>Jitao</first><last>Xu</last><affiliation>NetEase YouDao</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>ISIR CNRS &amp; Sorbonne Université</affiliation></author>
      <pages>369-376</pages>
      <abstract>In our globalized world, a growing number of situations arise where people are required to communicate in one or several foreign languages. In the case of written communication, users with a good command of a foreign language may find assistance from computer-aided translation (CAT) technologies. These technologies often allow users to access external resources, such as dictionaries, terminologies or bilingual concordancers, thereby interrupting and considerably hindering the writing process. In addition, CAT systems assume that the source sentence is fixed and also restrict the possible changes on the target side. In order to make the writing process smoother, we present BiSync, a bilingual writing assistant that allows users to freely compose text in two languages, while maintaining the two monolingual texts synchronized. We also include additional functionalities, such as the display of alternative prefix translations and paraphrases, which are intended to facilitate the authoring of texts. We detail the model architecture used for synchronization and evaluate the resulting tool, showing that high accuracy can be attained with limited computational resources. The interface and models are publicly available at https://github.com/jmcrego/BiSync and a demonstration video can be watched on YouTube https://youtu.be/_l-ugDHfNgU.</abstract>
      <url hash="97c8aa27">2023.acl-demo.35</url>
      <bibkey>crego-etal-2023-bisync</bibkey>
    </paper>
    <paper id="36">
      <title>Riveter: Measuring Power and Social Dynamics Between Entities</title>
      <author><first>Maria</first><last>Antoniak</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Anjalie</first><last>Field</last><affiliation>Stanford University</affiliation></author>
      <author><first>Jimin</first><last>Mun</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Melanie</first><last>Walsh</last><affiliation>University of Washington</affiliation></author>
      <author><first>Lauren</first><last>Klein</last><affiliation>Emory University</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>377-388</pages>
      <abstract>Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research.</abstract>
      <url hash="44e99965">2023.acl-demo.36</url>
      <bibkey>antoniak-etal-2023-riveter</bibkey>
    </paper>
    <paper id="37">
      <title>Fast Whitespace Correction with Encoder-Only Transformers</title>
      <author><first>Hannah</first><last>Bast</last><affiliation>University of Freiburg</affiliation></author>
      <author><first>Matthias</first><last>Hertel</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Sebastian</first><last>Walter</last><affiliation>University of Freiburg</affiliation></author>
      <pages>389-399</pages>
      <abstract>The goal of whitespace correction is to fix space errors in arbitrary given text. For example, given the text “whi te space correctio nwithTransf or mers”, produce “whitespace correction with Transformers”. We compare two Transformer-based models, a character-level encoder-decoder model and a byte-level encoder-only model. We find that the encoder-only model is both faster and achieves higher quality. We provide an easy-to-use tool that is over 900 times faster than the previous best tool, with the same high quality. Our tool repairs text at a rate of over 200 kB/s on GPU, with a sequence-averaged F1-score ranging from 87.5% for hard-to-correct text up to 99% for text without any spaces.</abstract>
      <url hash="670acc21">2023.acl-demo.37</url>
      <bibkey>bast-etal-2023-fast</bibkey>
    </paper>
    <paper id="38">
      <title><fixed-case>ESP</fixed-case>net-<fixed-case>ST</fixed-case>-v2: Multipurpose Spoken Language Translation Toolkit</title>
      <author><first>Brian</first><last>Yan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jiatong</first><last>Shi</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yun</first><last>Tang</last><affiliation>Facebook</affiliation></author>
      <author><first>Hirofumi</first><last>Inaguma</last><affiliation>Meta AI</affiliation></author>
      <author><first>Yifan</first><last>Peng</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Siddharth</first><last>Dalmia</last><affiliation>Google</affiliation></author>
      <author><first>Peter</first><last>Polak</last><affiliation>Charles University, MFF UFAL</affiliation></author>
      <author><first>Patrick</first><last>Fernandes</last><affiliation>Carnegie Mellon University, Instituto de Telecomunicações</affiliation></author>
      <author><first>Dan</first><last>Berrebbi</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Tomoki</first><last>Hayashi</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Xiaohui</first><last>Zhang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Zhaoheng</first><last>Ni</last><affiliation>Meta AI</affiliation></author>
      <author><first>Moto</first><last>Hira</last><affiliation>Meta AI</affiliation></author>
      <author><first>Soumi</first><last>Maiti</last><affiliation>ML researcher</affiliation></author>
      <author><first>Juan</first><last>Pino</last><affiliation>Facebook</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>400-411</pages>
      <abstract>ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.</abstract>
      <url hash="0780987f">2023.acl-demo.38</url>
      <bibkey>yan-etal-2023-espnet</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>CB</fixed-case>2: Collaborative Natural Language Interaction Research Platform</title>
      <author><first>Jacob</first><last>Sharf</last><affiliation>Cornell Tech</affiliation></author>
      <author><first>Mustafa Omer</first><last>Gul</last><affiliation>Cornell University</affiliation></author>
      <author><first>Yoav</first><last>Artzi</last><affiliation>Cornell University</affiliation></author>
      <pages>412-420</pages>
      <abstract>CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.</abstract>
      <url hash="00eb7169">2023.acl-demo.39</url>
      <bibkey>sharf-etal-2023-cb2</bibkey>
    </paper>
    <paper id="40">
      <title>Inseq: An Interpretability Toolkit for Sequence Generation Models</title>
      <author><first>Gabriele</first><last>Sarti</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Nils</first><last>Feldhus</last><affiliation>German Research Center for Artificial Intelligence (DFKI)</affiliation></author>
      <author><first>Ludwig</first><last>Sickert</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Oskar</first><last>van der Wal</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>421-435</pages>
      <abstract>Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models’ internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.</abstract>
      <url hash="e25605b3">2023.acl-demo.40</url>
      <bibkey>sarti-etal-2023-inseq</bibkey>
    </paper>
    <paper id="41">
      <title>Pipeline for modeling causal beliefs from natural language</title>
      <author><first>John</first><last>Priniski</last><affiliation>Ucla</affiliation></author>
      <author><first>Ishaan</first><last>Verma</last><affiliation>Usc</affiliation></author>
      <author><first>Fred</first><last>Morstatter</last><affiliation>Usc</affiliation></author>
      <pages>436-443</pages>
      <abstract>We present a causal language analysis pipeline that leverages a Large Language Model to identify causal claims made in natural language documents, and aggregates claims across a corpus to produce a causal claim network. The pipeline then applies a clustering algorithm that groups causal claims based on their semantic topics. We demonstrate the pipeline by modeling causal belief systems surrounding the Covid-19 vaccine from tweets.</abstract>
      <url hash="13a93472">2023.acl-demo.41</url>
      <bibkey>priniski-etal-2023-pipeline</bibkey>
    </paper>
    <paper id="42">
      <title><fixed-case>T</fixed-case>ab<fixed-case>G</fixed-case>enie: A Toolkit for Table-to-Text Generation</title>
      <author><first>Zdeněk</first><last>Kasner</last><affiliation>Charles University</affiliation></author>
      <author><first>Ekaterina</first><last>Garanina</last><affiliation>University of Groningen / Charles University</affiliation></author>
      <author><first>Ondrej</first><last>Platek</last><affiliation>Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics</affiliation></author>
      <author><first>Ondrej</first><last>Dusek</last><affiliation>Charles University</affiliation></author>
      <pages>444-455</pages>
      <abstract>Heterogenity of data-to-text generation datasets limits the research on data-to-text generation systems. We present TabGenie – a toolkit which enables researchers to explore, preprocess, and analyze a variety of data-to-text generation datasets through the unified framework of table-to-text generation. In TabGenie, all inputs are represented as tables with associated metadata. The tables can be explored through a web interface, which also provides an interactive mode for debugging table-to-text generation, facilitates side-by-side comparison of generated system outputs, and allows easy exports for manual analysis. Furthermore, TabGenie is equipped with command line processing tools and Python bindings for unified dataset loading and processing. We release TabGenie as a PyPI package and provide its open-source code and a live demo at https://github.com/kasnerz/tabgenie.</abstract>
      <url hash="c89f234e">2023.acl-demo.42</url>
      <bibkey>kasner-etal-2023-tabgenie</bibkey>
    </paper>
    <paper id="43">
      <title>An Efficient Conversational Smart Compose System</title>
      <author><first>Yun</first><last>Zhu</last><affiliation>Google</affiliation></author>
      <author><first>Xiayu</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Lei</first><last>Shu</last><affiliation>Google</affiliation></author>
      <author><first>Bowen</first><last>Tan</last><affiliation>Google</affiliation></author>
      <author><first>Xinying</first><last>Song</last><affiliation>Google</affiliation></author>
      <author><first>Lijuan</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>Maria</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Jindong</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Ning</first><last>Ruan</last><affiliation>Google</affiliation></author>
      <pages>456-462</pages>
      <abstract>Online conversation is a ubiquitous way to share information and connect everyone but repetitive idiomatic text typing takes users a lot of time.This paper demonstrates a simple yet effective cloud based smart compose system to improve human-to-human conversation efficiency. Heuristics from different perspectives are designed to achieve the best trade-off between quality and latency.From the modeling side, the decoder-only model exploited the previous turns of conversational history in a computation lightweight manner. Besides, a novel phrase tokenizer is proposed to reduce latency without losing the composing quality further. Additionally, the caching mechanism is applied to the serving framework. The demo video of the system is available at https://youtu.be/U1KXkaqr60g.We open-sourced our phrase tokenizer in https://github.com/tensorflow/text.</abstract>
      <url hash="975cfbd7">2023.acl-demo.43</url>
      <bibkey>zhu-etal-2023-efficient</bibkey>
    </paper>
    <paper id="44">
      <title>Which Spurious Correlations Impact Reasoning in <fixed-case>NLI</fixed-case> Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals</title>
      <author><first>Robin</first><last>Chan</last><affiliation>ETH Zürich</affiliation></author>
      <author><first>Afra</first><last>Amini</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Mennatallah</first><last>El-Assady</last><affiliation>University of Konstanz</affiliation></author>
      <pages>463-470</pages>
      <abstract>We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback.Through our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories: Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying training data and assessing NLI models’ robustness by creating adversarial test suites.</abstract>
      <url hash="f45d48e5">2023.acl-demo.44</url>
      <bibkey>chan-etal-2023-spurious</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>L</fixed-case>a<fixed-case>T</fixed-case>e<fixed-case>X</fixed-case>2<fixed-case>S</fixed-case>olver: a Hierarchical Semantic Parsing of <fixed-case>L</fixed-case>a<fixed-case>T</fixed-case>e<fixed-case>X</fixed-case> Document into Code for an Assistive Optimization Modeling Application</title>
      <author><first>Rindra</first><last>Ramamonjison</last><affiliation>Huawei Technologies Canada</affiliation></author>
      <author><first>Timothy</first><last>Yu</last><affiliation>Huawei Technologies Canada</affiliation></author>
      <author><first>Linzi</first><last>Xing</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Mahdi</first><last>Mostajabdaveh</last><affiliation>Huawei Technologies Canada</affiliation></author>
      <author><first>Xiaorui</first><last>Li</last><affiliation>Huawei Technologies Canada Co., Ltd</affiliation></author>
      <author><first>Xiaojin</first><last>Fu</last><affiliation>Huawei Noah’s Ark Lab - Hong Kong</affiliation></author>
      <author><first>Xiongwei</first><last>Han</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Yuanzhe</first><last>Chen</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Ren</first><last>Li</last><affiliation>Huawei UCD Center</affiliation></author>
      <author><first>Kun</first><last>Mao</last><affiliation>Huawei Cloud Computing Technologies</affiliation></author>
      <author><first>Yong</first><last>Zhang</last><affiliation>Huawei Technologies Canada Co., Ltd</affiliation></author>
      <pages>471-478</pages>
      <abstract>We demonstrate an interactive system to help operations research (OR) practitioners convert the mathematical formulation of optimization problems from TeX document format into the solver modeling language. In practice, a manual translation is cumbersome and time-consuming. Moreover, it requires an in-depth understanding of the problem description and a technical expertise to produce the modeling code. Thus, our proposed system TeX2Solver helps partially automate this conversion and help the users build optimization models more efficiently. In this paper, we describe its interface and the components of the hierarchical parsing system. A video demo walk-through is available online at http://bit.ly/3kuOm3x</abstract>
      <url hash="b0779e54">2023.acl-demo.45</url>
      <bibkey>ramamonjison-etal-2023-latex2solver</bibkey>
    </paper>
    <paper id="46">
      <title>Alfred: A System for Prompted Weak Supervision</title>
      <author><first>Peilin</first><last>Yu</last><affiliation>Brown University</affiliation></author>
      <author><first>Stephen</first><last>Bach</last><affiliation>Brown University</affiliation></author>
      <pages>479-488</pages>
      <abstract>Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural language prompts for language and vision-language models. Alfred provides a simple Python interface for the key steps of this emerging paradigm, with a high-throughput backend for large-scale data labeling. Users can quickly create, evaluate, and refine their prompt-based weak supervision sources; map the results to weak labels; and resolve their disagreements with a label model. Alfred enables a seamless local development experience backed by models served from self-managed computing clusters. It automatically optimizes the execution of prompts with optimized batching mechanisms. We find that this optimization improves query throughput by 2.9x versus a naive approach. We present two example use cases demonstrating Alfred on YouTube comment spam detection and pet breeds classification. Alfred is open source, available at https://github.com/BatsResearch/alfred.</abstract>
      <url hash="f4174f6c">2023.acl-demo.46</url>
      <bibkey>yu-bach-2023-alfred</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>O</fixed-case>pen<fixed-case>ICL</fixed-case>: An Open-Source Framework for In-context Learning</title>
      <author><first>Zhenyu</first><last>Wu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Yaoxiang</first><last>Wang</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Jiacheng</first><last>Ye</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhiyong</first><last>Wu</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Jiangtao</first><last>Feng</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Jingjing</first><last>Xu</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>Shanghai AI Lab</affiliation></author>
      <pages>489-498</pages>
      <abstract>In recent years, In-context Learning (ICL) has gained increasing attentionand emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates.However, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components.To facilitate ICL research, we introduce OpenICL, an open-source toolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly flexible architecture that users can easily combine different components to suit their needs.It also provides various state-of-the-art retrieval and inference methods to streamline the process of adapting ICL to cutting-edge research.The effectiveness of OpenICL has been validated on a wide range of NLP tasks, including classification, QA, machine translation, and semantic parsing. As a side-product, we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released at https://github.com/Shark-NLP/OpenICL.</abstract>
      <url hash="7e881c46">2023.acl-demo.47</url>
      <bibkey>wu-etal-2023-openicl</bibkey>
    </paper>
    <paper id="48">
      <title>Self-Supervised Sentence Polishing by Adding Engaging Modifiers</title>
      <author><first>Zhexin</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jian</first><last>Guan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xin</first><last>Cui</last><affiliation>Platform and Content Group, Tencent Technology Co., Ltd.</affiliation></author>
      <author><first>Yu</first><last>Ran</last><affiliation>Platform and Content Group, Tencent Technology Co., Ltd.</affiliation></author>
      <author><first>Bo</first><last>Liu</last><affiliation>Platform and Content Group, Tencent Technology Co., Ltd.</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>499-507</pages>
      <abstract>Teachers often guide students to improve their essays by adding engaging modifiers to polish the sentences. In this work, we present the first study on automatic sentence polishing by adding modifiers. Since there is no available dataset for the new task, we first automatically construct a large number of parallel data by removing modifiers in the engaging sentences collected from public resources. Then we fine-tune LongLM to reconstruct the original sentences from the corrupted ones. Considering that much overlap between inputs and outputs may bias the model to completely copy the inputs, we split each source sentence into sub-sentences and only require the model to generate the modified sub-sentences. Furthermore, we design a retrieval augmentation algorithm to prompt the model to add suitable modifiers. Automatic and manual evaluation on the auto-constructed test set and real human texts show that our model can generate more engaging sentences with suitable modifiers than strong baselines while keeping fluency. We deploy the model at <url>http://coai.cs.tsinghua.edu.cn/static/polishSent/</url>. A demo video is available at <url>https://youtu.be/Y6gFHOgSv8Y</url>.</abstract>
      <url hash="d104a115">2023.acl-demo.48</url>
      <bibkey>zhang-etal-2023-self</bibkey>
    </paper>
    <paper id="49">
      <title>Effidit: An Assistant for Improving Writing Efficiency</title>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Enbo</first><last>Zhao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wei</first><last>Bi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Deng</first><last>Cai</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Leyang</first><last>Cui</last><affiliation>Tencent AI LAB</affiliation></author>
      <author><first>Xinting</first><last>Huang</last><affiliation>Tencent</affiliation></author>
      <author><first>Haiyun</first><last>Jiang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Duyu</first><last>Tang</last><affiliation>Tencent</affiliation></author>
      <author><first>Kaiqiang</first><last>Song</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Longyue</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Chenyan</first><last>Huang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Guoping</first><last>Huang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yan</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Piji</first><last>Li</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <pages>508-515</pages>
      <abstract>Writing assistants are valuable tools that can help writers improve their writing skills. We introduce Effidit (<b>Eff</b>icient and <b>I</b>ntelligent E<b>dit</b>ing), a digital writing assistant that facilitates users to write higher-quality text more efficiently through the use of Artificial Intelligence (AI) and Natural Language Processing (NLP) technologies. We significantly expand the capacities of a writing assistantby providing functions in three modules: text completion, hint recommendation, and writing refinement. Based on the above efforts, Effidit can efficiently assist users in creating their own text. Effidit has been deployed to several Tencent products and publicly released at <url>https://effidit.qq.com/</url>.</abstract>
      <url hash="36f3b865">2023.acl-demo.49</url>
      <bibkey>shi-etal-2023-effidit</bibkey>
    </paper>
    <paper id="50">
      <title><fixed-case>W</fixed-case>iz<fixed-case>M</fixed-case>ap: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings</title>
      <author><first>Zijie J.</first><last>Wang</last><affiliation>Georgia Tech</affiliation></author>
      <author><first>Fred</first><last>Hohman</last><affiliation>Apple</affiliation></author>
      <author><first>Duen Horng</first><last>Chau</last><affiliation>Georgia Tech</affiliation></author>
      <pages>516-523</pages>
      <abstract>Machine learning models often learn latent embedding representations that capture the domain semantics of their training data. These embedding representations are valuable for interpreting trained models, building new models, and analyzing new datasets. However, interpreting and using embeddings can be challenging due to their opaqueness, high dimensionality, and the large size of modern datasets. To tackle these challenges, we present WizMap, an interactive visualization tool to help researchers and practitioners easily explore large embeddings. With a novel multi-resolution embedding summarization method and a familiar map-like interaction design, WizMap enables users to navigate and interpret embedding spaces with ease. Leveraging modern web technologies such as WebGL and Web Workers, WizMap scales to millions of embedding points directly in users’ web browsers and computational notebooks without the need for dedicated backend servers. WizMap is open-source and available at the following public demo link: https://poloclub.github.io/wizmap.</abstract>
      <url hash="1b09ad4b">2023.acl-demo.50</url>
      <bibkey>wang-etal-2023-wizmap</bibkey>
    </paper>
    <paper id="51">
      <title>A System for Answering Simple Questions in Multiple Languages</title>
      <author><first>Anton</first><last>Razzhigaev</last><affiliation>Skoltech</affiliation></author>
      <author><first>Mikhail</first><last>Salnikov</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Valentin</first><last>Malykh</last><affiliation>Huawei Noah’s Ark Lab / Kazan Federal University</affiliation></author>
      <author><first>Pavel</first><last>Braslavski</last><affiliation>Ural Federal University and HSE University</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skolkovo Institue of Science and Technology</affiliation></author>
      <pages>524-537</pages>
      <abstract>Our research focuses on the most prevalent type of queries— simple questions —exemplified by questions like “What is the capital of France?”. These questions reference an entity such as “France”, which is directly connected (one hop) to the answer entity “Paris” in the underlying knowledge graph (KG). We propose a multilingual Knowledge Graph Question Answering (KGQA) technique that orders potential responses based on the distance between the question’s text embeddings and the answer’s graph embeddings. A system incorporating this novel method is also described in our work.Through comprehensive experimentation using various English and multilingual datasets and two KGs — Freebase and Wikidata — we illustrate the comparative advantage of the proposed method across diverse KG embeddings and languages. This edge is apparent even against robust baseline systems, including seq2seq QA models, search-based solutions and intricate rule-based pipelines. Interestingly, our research underscores that even advanced AI systems like ChatGPT encounter difficulties when tasked with answering simple questions. This finding emphasizes the relevance and effectiveness of our approach, which consistently outperforms such systems. We are making the source code and trained models from our study publicly accessible to promote further advancements in multilingual KGQA.</abstract>
      <url hash="0b7340c5">2023.acl-demo.51</url>
      <bibkey>razzhigaev-etal-2023-system</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>KWJA</fixed-case>: A Unified <fixed-case>J</fixed-case>apanese Analyzer Based on Foundation Models</title>
      <author><first>Nobuhiro</first><last>Ueda</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Kazumasa</first><last>Omura</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Takashi</first><last>Kodama</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Hirokazu</first><last>Kiyomaru</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Yugo</first><last>Murawaki</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Daisuke</first><last>Kawahara</last><affiliation>Waseda University</affiliation></author>
      <author><first>Sadao</first><last>Kurohashi</last><affiliation>Kyoto University</affiliation></author>
      <pages>538-548</pages>
      <abstract>We present KWJA, a high-performance unified Japanese text analyzer based on foundation models.KWJA supports a wide range of tasks, including typo correction, word segmentation, word normalization, morphological analysis, named entity recognition, linguistic feature tagging, dependency parsing, PAS analysis, bridging reference resolution, coreference resolution, and discourse relation analysis, making it the most versatile among existing Japanese text analyzers.KWJA solves these tasks in a multi-task manner but still achieves competitive or better performance compared to existing analyzers specialized for each task.KWJA is publicly available under the MIT license at https://github.com/ku-nlp/kwja.</abstract>
      <url hash="2e7f285c">2023.acl-demo.52</url>
      <bibkey>ueda-etal-2023-kwja</bibkey>
    </paper>
    <paper id="53">
      <title>Disease Network Constructor: a Pathway Extraction and Visualization</title>
      <author><first>Mohammad Golam</first><last>Sohrab</last><affiliation>Artificial Intelligence Research Centre at AIST</affiliation></author>
      <author><first>Khoa</first><last>Duong</last><affiliation>Aist</affiliation></author>
      <author><first>Goran</first><last>Topić</last><affiliation>Aist</affiliation></author>
      <author><first>Masami</first><last>Ikeda</last><affiliation>National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Nozomi</first><last>Nagano</last><affiliation>Artificial Intelligence Research Centre at AIST</affiliation></author>
      <author><first>Yayoi</first><last>Natsume-Kitatani</last><affiliation>National Institutes of Biomedical Innovation, Health and Nutrition</affiliation></author>
      <author><first>Masakata</first><last>Kuroda</last><affiliation>National Institutes of Biomedical Innovation, Health and Nutrition</affiliation></author>
      <author><first>Mari</first><last>Itoh</last><affiliation>National Institutes of Biomedical Innovation, Health and Nutrition</affiliation></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>The National Institute of Advanced Industrial Science and Technology (AIST)</affiliation></author>
      <pages>549-557</pages>
      <abstract>We present Disease Network Constructor (DNC), a system that extracts and visualizes a disease network, in which nodes are entities such as diseases, proteins, and genes, and edges represent regulation relation. We focused on the disease network derived through regulation events found in scientific articles on idiopathic pulmonary fibrosis (IPF). The front-end web-base user interface of DNC includes two-dimensional (2D) and 3D visualizations of the constructed disease network. The back-end system of DNC includes several natural language processing (NLP) techniques to process biomedical text including BERT-based tokenization on the basis of Bidirectional Encoder Representations from Transformers (BERT), flat and nested named entity recognition (NER), candidate generation and candidate ranking for entity linking (EL) or, relation extraction (RE), and event extraction (EE) tasks. We evaluated the end-to-end EL and end-to-end nested EE systems to determine the DNC’s back-endimplementation performance. To the best of our knowledge, this is the first attempt that addresses neural NER, EL, RE, and EE tasks in an end-to-end manner that constructs a path-way visualization from events, which we name Disease Network Constructor. The demonstration video can be accessed from https://youtu.be/rFhWwAgcXE8. We release an online system for end users and the source code is available at https://github.com/aistairc/PRISM-APIs/.</abstract>
      <url hash="cbfc9b29">2023.acl-demo.53</url>
      <bibkey>sohrab-etal-2023-disease</bibkey>
    </paper>
    <paper id="54">
      <title>Petals: Collaborative Inference and Fine-tuning of Large Models</title>
      <author><first>Alexander</first><last>Borzunov</last><affiliation>HSE University, Yandex</affiliation></author>
      <author><first>Dmitry</first><last>Baranchuk</last><affiliation>Yandex</affiliation></author>
      <author><first>Tim</first><last>Dettmers</last><affiliation>University of Washington, Facebook AI Research</affiliation></author>
      <author><first>Maksim</first><last>Riabinin</last><affiliation>Yandex/HSE University</affiliation></author>
      <author><first>Younes</first><last>Belkada</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Artem</first><last>Chumachenko</last><affiliation>Yandex</affiliation></author>
      <author><first>Pavel</first><last>Samygin</last><affiliation>Yandex School of Data Analysis</affiliation></author>
      <author><first>Colin</first><last>Raffel</last><affiliation>University of North Carolina/Hugging Face</affiliation></author>
      <pages>558-568</pages>
      <abstract>Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers. In some cases, LLMs can be used more affordably via RAM offloading or hosted APIs. However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are not flexible enough for research that requires access to weights, attention or logits. In this work, we propose Petals - a system for inference and fine-tuning of large models collaboratively by joining the resources of multiple parties. We demonstrate that this strategy outperforms offloading for very large models, running inference of BLOOM-176B on consumer GPUs with ≈1 step per second, which is enough for many interactive LLM applications. Unlike most inference APIs, Petals also natively exposes hidden states of served models, allowing to train and share custom model extensions based on efficient fine-tuning methods. The system, its source code, and documentation are available at https://petals.mlVideo (2 min): https://youtu.be/F4muLI-0hTE</abstract>
      <url hash="0d227535">2023.acl-demo.54</url>
      <bibkey>borzunov-etal-2023-petals</bibkey>
    </paper>
    <paper id="55">
      <title><fixed-case>UKP</fixed-case>-<fixed-case>SQ</fixed-case>u<fixed-case>ARE</fixed-case> v3: A Platform for Multi-Agent <fixed-case>QA</fixed-case> Research</title>
      <author><first>Haritz</first><last>Puerto</last><affiliation>UKP Lab, TU Darmstadt</affiliation></author>
      <author><first>Tim</first><last>Baumgärtner</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Rachneet</first><last>Sachdeva</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Haishuo</first><last>Fang</last><affiliation>UKP Lab, TU Darmstadt</affiliation></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Sewin</first><last>Tariverdian</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Kexin</first><last>Wang</last><affiliation>Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science, Technical University of Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <pages>569-580</pages>
      <abstract>The continuous development of Question Answering (QA) datasets has drawn the research community’s attention toward multi-domain models. A popular approach is to use multi-dataset models, which are models trained on multiple datasets to learn their regularities and prevent overfitting to a single dataset. However, with the proliferation of QA models in online repositories such as GitHub or Hugging Face, an alternative is becoming viable. Recent works have demonstrated that combining expert agents can yield large performance gains over multi-dataset models. To ease research in multi-agent models, we extend UKP-SQuARE, an online platform for QA research, to support three families of multi-agent systems: i) agent selection, ii) early-fusion of agents, and iii) late-fusion of agents. We conduct experiments to evaluate their inference speed and discuss the performance vs. speed trade-off compared to multi-dataset models. UKP-SQuARE is open-source and publicly available.</abstract>
      <url hash="ff896f8f">2023.acl-demo.55</url>
      <bibkey>puerto-etal-2023-ukp</bibkey>
    </paper>
    <paper id="56">
      <title>Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation</title>
      <author><first>Mete</first><last>Sertkan</last><affiliation>TU Wien</affiliation></author>
      <author><first>Sophia</first><last>Althammer</last><affiliation>Technical University of Vienna</affiliation></author>
      <author><first>Sebastian</first><last>Hofstätter</last><affiliation>TU Wien</affiliation></author>
      <pages>581-587</pages>
      <abstract>In this paper, we introduce Ranger - a toolkit to facilitate the easy use of effect-size-based meta-analysis for multi-task evaluation in NLP and IR. We observed that our communities often face the challenge of aggregating results over incomparable metrics and scenarios, which makes conclusions and take-away messages less reliable. With Ranger, we aim to address this issue by providing a task-agnostic toolkit that combines the effect of a treatment on multiple tasks into one statistical evaluation, allowing for comparison of metrics and computation of an overall summary effect. Our toolkit produces publication-ready forest plots that enable clear communication of evaluation results over multiple tasks. Our goal with the ready-to-use Ranger toolkit is to promote robust, effect-size-based evaluation and improve evaluation standards in the community. We provide two case studies for common IR and NLP settings to highlight Ranger’s benefits.</abstract>
      <url hash="1e550fa5">2023.acl-demo.56</url>
      <bibkey>sertkan-etal-2023-ranger</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>GAIA</fixed-case> Search: Hugging Face and Pyserini Interoperability for <fixed-case>NLP</fixed-case> Training Data Exploration</title>
      <author><first>Aleksandra</first><last>Piktus</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Odunayo</first><last>Ogundepo</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Christopher</first><last>Akiki</last><affiliation>Leipzig University</affiliation></author>
      <author><first>Akintunde</first><last>Oladipo</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Xinyu</first><last>Zhang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Hailey</first><last>Schoelkopf</last><affiliation>EleutherAI</affiliation></author>
      <author><first>Stella</first><last>Biderman</last><affiliation>EleutherAI, Booz Allen Hamilton</affiliation></author>
      <author><first>Martin</first><last>Potthast</last><affiliation>Leipzig University</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>588-598</pages>
      <abstract>Noticing the urgent need to provide tools for fast and user-friendly qualitative analysis of large-scale textual corpora of the modern NLP, we propose to turn to the mature and well-tested methods from the domain of Information Retrieval (IR) - a research field with a long history of tackling TB-scale document collections. We discuss how Pyserini - a widely used toolkit for reproducible IR research can be integrated with the Hugging Face ecosystem of open-source AI libraries and artifacts. We leverage the existing functionalities of both platforms while proposing novel features further facilitating their integration. Our goal is to give NLP researchers tools that will allow them to develop retrieval-based instrumentation for their data analytics needs with ease and agility.We include a Jupyter Notebook-based walk through the core interoperability features, available on GitHub: https://github.com/huggingface/gaia.We then demonstrate how the ideas we present can be operationalized to create a powerful tool for qualitative data analysis in NLP. We present GAIA Search - a search engine built following previously laid out principles, giving access to four popular large-scale text collections. GAIA serves a dual purpose of illustrating the potential of methodologies we discuss but also as a standalone qualitative analysis tool that can be leveraged by NLP researchers aiming to understand datasets prior to using them in training. GAIA is hosted live on Hugging Face Spaces: https://huggingface.co/spaces/spacerini/gaia.</abstract>
      <url hash="0baea1b2">2023.acl-demo.57</url>
      <bibkey>piktus-etal-2023-gaia</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>D</fixed-case>eep<fixed-case>P</fixed-case>avlov Dream: Platform for Building Generative <fixed-case>AI</fixed-case> Assistants</title>
      <author><first>Diliara</first><last>Zharikova</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Daniel</first><last>Kornev</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Fedor</first><last>Ignatov</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Maxim</first><last>Talimanchuk</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Dmitry</first><last>Evseev</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Ksenya</first><last>Petukhova</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Veronika</first><last>Smilga</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Dmitry</first><last>Karpov</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Yana</first><last>Shishkina</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Dmitry</first><last>Kosenko</last><affiliation>DeepPavlov.ai</affiliation></author>
      <author><first>Mikhail</first><last>Burtsev</last><affiliation>DeepPavlov.ai</affiliation></author>
      <pages>599-607</pages>
      <abstract>An open-source DeepPavlov Dream Platform is specifically tailored for development of complex dialog systems like Generative AI Assistants. The stack prioritizes efficiency, modularity, scalability, and extensibility with the goal to make it easier to develop complex dialog systems from scratch. It supports modular approach to implementation of conversational agents enabling their development through the choice of NLP components and conversational skills from a rich library organized into the distributions of ready-for-use multi-skill AI assistant systems. In DeepPavlov Dream, multi-skill Generative AI Assistant consists of NLP components that extract features from user utterances, conversational skills that generate or retrieve a response, skill and response selectors that facilitate choice of relevant skills and the best response, as well as a conversational orchestrator that enables creation of multi-skill Generative AI Assistants scalable up to industrial grade AI assistants. The platform allows to integrate large language models into dialog pipeline, customize with prompt engineering, handle multiple prompts during the same dialog session and create simple multimodal assistants.</abstract>
      <url hash="19d8774b">2023.acl-demo.58</url>
      <bibkey>zharikova-etal-2023-deeppavlov</bibkey>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2023-07-06">
    <meta>
      <booktitle>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</booktitle>
      <editor><first>Yun-Nung (Vivian)</first><last>Chen</last></editor>
      <editor><first>Margot</first><last>Margot</last></editor>
      <editor><first>Siva</first><last>Reddy</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="85bb242f">2023.acl-tutorials</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="67b8aa9b">2023.acl-tutorials.0</url>
      <bibkey>acl-2023-association-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Goal Awareness for Conversational <fixed-case>AI</fixed-case>: Proactivity, Non-collaborativity, and Beyond</title>
      <author><first>Yang</first><last>Deng</last></author>
      <author><first>Wenqiang</first><last>Lei</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <pages>1-10</pages>
      <abstract>Conversational systems are envisioned to provide social support or functional service to human users via natural language interactions. Conventional conversation researches mainly focus on the responseability of the system, such as dialogue context understanding and response generation, but overlooks the design of an essential property in intelligent conversations, i.e., goal awareness. The awareness of goals means the state of not only being responsive to the users but also aware of the target conversational goal and capable of leading the conversation towards the goal, which is a significant step towards higher-level intelligence and artificial consciousness. It can not only largely improve user engagement and service efficiency in the conversation, but also empower the system to handle more complicated conversation tasks that involve strategical and motivational interactions. In this tutorial, we will introduce the recent advances on the design of agent’s awareness of goals in a wide range of conversational systems.</abstract>
      <url hash="f9e8dfc5">2023.acl-tutorials.1</url>
      <bibkey>deng-etal-2023-goal</bibkey>
    </paper>
    <paper id="2">
      <title>Complex Reasoning in Natural Languag</title>
      <author><first>Wenting</first><last>Zhao</last></author>
      <author><first>Mor</first><last>Geva</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Michihiro</first><last>Yasunaga</last></author>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <pages>11-20</pages>
      <abstract>Teaching machines to reason over texts has been a long-standing goal of natural language processing (NLP). To this end, researchers have designed a diverse set of complex reasoning tasks that involve compositional reasoning, knowledge retrieval, grounding, commonsense reasoning, etc. A standard choice for building systems that perform a desired type of reasoning is to fine-tune a pretrained language model (LM) on specific downstream tasks. However, recent research has demonstrated that such a straightforward approach is often brittle. For example, Elazar et al. (2021) and Branco et al. (2021) show that, on question-answering (QA) tasks, similar performance can be achieved with questions removed from the inputs. Min et al. (2019), Chen and Durrett (2019), and Tang et al. (2021) show that models trained on multi-hop QA do not generalize to answer single-hop questions. The reasoning capabilities of these models thus remain at a surface level, i.e., exploiting data patterns. Consequently, augmenting LMs with techniques that make them robust and effective becomes an active research area. We will start the tutorial by providing an overview of complex reasoning tasks where the standard application of pretrained language models fails. This tutorial then reviews recent promising directions for tackling these tasks. Specifically, we focus on the following groups of approaches that explicitly consider problem structures: (1) knowledge-augmented methods, where the knowledge is either incorporated during fine-tuning or pretraining; (2) few-shot prompting methods, which effectively guide the models to follow instructions; (3) neuro-symbolic methods, which produce explicit intermediate representations; and, (4) rationale-based methods, one of the most popular forms of the neuro-symbolic methods, which highlight subsets of input as explanations for individual model predictions.</abstract>
      <url hash="79ac0f68">2023.acl-tutorials.2</url>
      <bibkey>zhao-etal-2023-complex</bibkey>
    </paper>
    <paper id="3">
      <title>Everything you need to know about Multilingual <fixed-case>LLM</fixed-case>s: Towards fair, performant and reliable models for languages of the world</title>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Barun</first><last>Patra</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Kabir</first><last>Ahuja</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <pages>21-26</pages>
      <abstract>This tutorial will describe various aspects of scaling up language technologies to many of the world’s languages by describing the latest research in Massively Multilingual Language Models (MMLMs). We will cover topics such as data collection, training and fine-tuning of models, Responsible AI issues such as fairness, bias and toxicity, linguistic diversity and evaluation in the context of MMLMs, specifically focusing on issues in non-English and low-resource languages. Further, we will also talk about some of the real-world challenges in deploying these models in language communities in the field. With the performance of MMLMs improving in the zero-shot setting for many languages, it is now becoming feasible to use them for building language technologies in many languages of the world, and this tutorial will provide the computational linguistics community with unique insights from the latest research in multilingual models.</abstract>
      <url hash="355687d4">2023.acl-tutorials.3</url>
      <bibkey>sitaram-etal-2023-everything</bibkey>
    </paper>
    <paper id="4">
      <title>Generating Text from Language Models</title>
      <author><first>Afra</first><last>Amini</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>John</first><last>Hewitt</last></author>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <pages>27-31</pages>
      <abstract>An increasingly large percentage of natural language processing (NLP) tasks center around the generation of text from probabilistic language models. Despite this trend, techniques for improving or specifying preferences in these generated texts rely mostly on intuition-based heuristics. Further, there lacks a unified presentation of their motivations, practical implementation, successes and pitfalls. Practitioners must, therefore, choose somewhat blindly between generation algorithms—like top-p sampling or beam search—which can lead to wildly different results. At the same time, language generation research continues to criticize and improve the standard toolboxes, further adding entropy to the state of the field. In this tutorial, we will provide a centralized and cohesive discussion of critical considerations when choosing how to generate from a language model. We will cover a wide range of empirically-observed problems (like degradation, hallucination, repetition) and their corresponding proposed algorithmic solutions from recent research (like top-p sampling and its successors). We will then discuss a subset of these algorithms under a unified light; most stochastic generation strategies can be framed as locally adapting the probabilities of a model to avoid failure cases. Finally, we will then cover methods in controlled generation, that go beyond just ensuring coherence to ensure text exhibits specific desired properties. We aim for NLP practitioners and researchers to leave our tutorial with a unified framework which they can use to evaluate and contribute to the latest research in language generation.</abstract>
      <url hash="7814afed">2023.acl-tutorials.4</url>
      <bibkey>amini-etal-2023-generating</bibkey>
    </paper>
    <paper id="5">
      <title>Indirectly Supervised Natural Language Processing</title>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>32-40</pages>
      <abstract>This tutorial targets researchers and practitioners who are interested in ML technologies for NLP from indirect supervision. In particular, we will present a diverse thread of indirect supervision studies that try to answer the following questions: (i) when and how can we provide supervision for a target task T, if all we have is data that corresponds to a “related” task T′? (ii) humans do not use exhaustive supervision; they rely on occasional feedback, and learn from incidental signals from various sources; how can we effectively incorporate such supervision in machine learning? (iii) how can we leverage multi-modal supervision to help NLP? To the end, we will discuss several lines of research that address those challenges, including (i) indirect supervision from T ′ that handles T with outputs spanning from a moderate size to an open space, (ii) the use of sparsely occurring and incidental signals, such as partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations—all having statistical associations with the task, (iii) principled ways to measure and understand why these incidental signals can contribute to our target tasks, and (iv) indirect supervision from vision-language signals. We will conclude the tutorial by outlining directions for further investigation.</abstract>
      <url hash="fc645450">2023.acl-tutorials.5</url>
      <bibkey>yin-etal-2023-indirectly</bibkey>
    </paper>
    <paper id="6">
      <title>Retrieval-based Language Models and Applications</title>
      <author><first>Akari</first><last>Asai</last></author>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Zexuan</first><last>Zhong</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>41-46</pages>
      <abstract>Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.</abstract>
      <url hash="fe3433be">2023.acl-tutorials.6</url>
      <bibkey>asai-etal-2023-retrieval</bibkey>
    </paper>
  </volume>
</collection>
