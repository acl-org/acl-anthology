<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.calcs">
  <volume id="1" ingest-date="2023-12-07" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 6th Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
      <editor><first>Genta</first><last>Winata</last></editor>
      <editor><first>Sudipta</first><last>Kar</last></editor>
      <editor><first>Marina</first><last>Zhukova</last></editor>
      <editor><first>Thamar</first><last>Solorio</last></editor>
      <editor><first>Mona</first><last>Diab</last></editor>
      <editor><first>Sunayana</first><last>Sitaram</last></editor>
      <editor><first>Monojit</first><last>Choudhury</last></editor>
      <editor><first>Kalika</first><last>Bali</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore</address>
      <month>December</month>
      <year>2023</year>
      <url hash="c39afaad">2023.calcs-1</url>
      <venue>calcs</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="14bfa1b0">2023.calcs-1.0</url>
      <bibkey>calcs-2023-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>T</fixed-case>ongue<fixed-case>S</fixed-case>witcher: Fine-Grained Identification of <fixed-case>G</fixed-case>erman-<fixed-case>E</fixed-case>nglish Code-Switching</title>
      <author><first>Igor</first><last>Sterner</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <pages>1-13</pages>
      <abstract>This paper contributes to German-English code-switching research. We provide the largest corpus of naturally occurring German-English code-switching, where English is included in German text, and two methods for code-switching identification. The first method is rule-based, using wordlists and morphological processing. We use this method to compile a corpus of 25.6M tweets employing German-English code-switching. In our second method, we continue pretraining of a neural language model on this corpus and classify tokens based on embeddings from this language model. Our systems establish SoTA on our new corpus and an existing German-English code-switching benchmark. In particular, we systematically study code-switching for language-ambiguous words which can only be resolved in context, and morphologically mixed words consisting of both English and German morphemes. We distribute both corpora and systems to the research community.</abstract>
      <url hash="b065e7fb">2023.calcs-1.1</url>
      <bibkey>sterner-teufel-2023-tongueswitcher</bibkey>
      <doi>10.18653/v1/2023.calcs-1.1</doi>
    </paper>
    <paper id="2">
      <title>Towards Real-World Streaming Speech Translation for Code-Switched Speech</title>
      <author><first>Belen</first><last>Alastruey</last><affiliation>TALP Research Center, Universitat Politècnica de Catalunya</affiliation></author>
      <author><first>Matthias</first><last>Sperber</last><affiliation>Apple</affiliation></author>
      <author><first>Christian</first><last>Gollan</last><affiliation>Apple</affiliation></author>
      <author><first>Dominic</first><last>Telaar</last><affiliation>Apple</affiliation></author>
      <author><first>Tim</first><last>Ng</last><affiliation>Apple</affiliation></author>
      <author><first>Aashish</first><last>Agarwal</last><affiliation>Universität Duisburg-Essen</affiliation></author>
      <pages>14-22</pages>
      <abstract>Code-switching (CS), i.e. mixing different languages in a single sentence, is a common phenomenon in communication and can be challenging in many Natural Language Processing (NLP) settings. Previous studies on CS speech have shown promising results for end-to-end speech translation (ST), but have been limited to offline scenarios and to translation to one of the languages present in the source monolingual transcription). In this paper, we focus on two essential yet unexplored areas for real-world CS speech translation: streaming settings, and translation to a third language (i.e., a language not included in the source). To this end, we extend the Fisher and Miami test and validation datasets to include new targets in Spanish and German. Using this data, we train a model for both offline and streaming ST and we establish baseline results for the two settings mentioned earlier.</abstract>
      <url hash="7240bb13">2023.calcs-1.2</url>
      <bibkey>alastruey-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.calcs-1.2</doi>
    </paper>
    <paper id="3">
      <title>Language Preference for Expression of Sentiment for <fixed-case>N</fixed-case>epali-<fixed-case>E</fixed-case>nglish Bilingual Speakers on Social Media</title>
      <author><first>Niraj</first><last>Pahari</last></author>
      <author><first>Kazutaka</first><last>Shimada</last></author>
      <pages>23-32</pages>
      <abstract>Nepali-English code-switching (CS) has been a growing phenomenon in Nepalese society, especially in social media. The code-switching text can be leveraged to understand the socio-linguistic behaviours of the multilingual speakers. Existing studies have attempted to identify the language preference of the multilingual speakers for expressing different emotions using text in different language pairs. In this work, we aim to study the language preference of multilingual Nepali-English CS speakers while expressing sentiment in social media. We create a novel dataset for sentiment analysis using the public Nepali-English code-switched comments in YouTube. After performing the statistical study on the dataset, we find that the proportion of use of Nepali language is higher in negative comments when compared with positive comments, hence concluding the preference for using native language while expressing negative sentiment. Machine learning and transformer-based models are used as the baseline models for the dataset for sentiment classification. The dataset is released publicly.</abstract>
      <url hash="bb8693fd">2023.calcs-1.3</url>
      <bibkey>pahari-shimada-2023-language</bibkey>
      <doi>10.18653/v1/2023.calcs-1.3</doi>
    </paper>
    <paper id="4">
      <title>Text-Derived Language Identity Incorporation for End-to-End Code-Switching Speech Recognition</title>
      <author><first>Qinyi</first><last>Wang</last></author>
      <author><first>Haizhou</first><last>Li</last></author>
      <pages>33-42</pages>
      <abstract>Recognizing code-switching (CS) speech often presents challenges for an automatic speech recognition system (ASR) due to limited linguistic context in short monolingual segments, resulting in language confusion. To mitigate this issue, language identity (LID) is often integrated into the speech recognition system to provide additional linguistic context. However, previous works predominately focus on extracting language identity from speech signals. We introduce a novel approach to learn language identity from pure text data via a dedicated language identity-language model. Besides, we explore two strategies: LID state fusion and language posterior biasing, to integrate the text-derived language identities into the end-to-end ASR system. By incorporating hypothesized language identities, our ASR system gains crucial contextual cues, effectively capturing language transitions and patterns within code-switched utterances. We conduct speech recognition experiments on the SEAME corpus and demonstrate the effectiveness of our proposed methods. Our results reveal significantly improved transcriptions in code-switching scenarios, underscoring the potential of text-derived LID in enhancing code-switching speech recognition.</abstract>
      <url hash="172947d8">2023.calcs-1.4</url>
      <bibkey>wang-li-2023-text</bibkey>
      <doi>10.18653/v1/2023.calcs-1.4</doi>
    </paper>
    <paper id="5">
      <title>Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South <fixed-case>E</fixed-case>ast <fixed-case>A</fixed-case>sian Languages</title>
      <author><first>Zheng Xin</first><last>Yong</last></author>
      <author><first>Ruochen</first><last>Zhang</last></author>
      <author><first>Jessica</first><last>Forde</last></author>
      <author><first>Skyler</first><last>Wang</last></author>
      <author><first>Arjun</first><last>Subramonian</last></author>
      <author><first>Holy</first><last>Lovenia</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Genta</first><last>Winata</last></author>
      <author><first>Lintang</first><last>Sutawika</last></author>
      <author><first>Jan Christian Blaise</first><last>Cruz</last></author>
      <author><first>Yin Lin</first><last>Tan</last></author>
      <author><first>Long</first><last>Phan</last></author>
      <author><first>Long</first><last>Phan</last></author>
      <author><first>Rowena</first><last>Garcia</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <author><first>Alham</first><last>Aji</last></author>
      <pages>43-63</pages>
      <abstract>While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The recent proliferation of Large Language Models (LLMs) compels one to ask: how capable are these systems in generating code-mixed data? In this paper, we explore prompting multilingual LLMs in a zero-shot manner to generate code-mixed data for seven languages in South East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese, Tamil, and Singlish. We find that publicly available multilingual instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of producing texts with phrases or clauses from different languages. ChatGPT exhibits inconsistent capabilities in generating code-mixed texts, wherein its performance varies depending on the prompt template and language pairing. For instance, ChatGPT generates fluent and natural Singlish texts (an English-based creole spoken in Singapore), but for English-Tamil language pair, the system mostly produces grammatically incorrect or semantically meaningless utterances. Furthermore, it may erroneously introduce languages not specified in the prompt. Based on our investigation, existing multilingual LLMs exhibit a wide range of proficiency in code-mixed data generation for SEA languages. As such, we advise against using LLMs in this context without extensive human checks.</abstract>
      <url hash="a4b0e996">2023.calcs-1.5</url>
      <bibkey>yong-etal-2023-prompting</bibkey>
      <doi>10.18653/v1/2023.calcs-1.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>CONFLATOR</fixed-case>: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling</title>
      <author><first>Mohsin</first><last>Mohammed</last></author>
      <author><first>Sai</first><last>Kandukuri</last></author>
      <author><first>Neeharika</first><last>Gupta</last></author>
      <author><first>Parth</first><last>Patwa</last></author>
      <author><first>Anubhab</first><last>Chatterjee</last></author>
      <author><first>Vinija</first><last>Jain</last></author>
      <author><first>Aman</first><last>Chadha</last></author>
      <author><first>Amitava</first><last>Das</last></author>
      <pages>64-73</pages>
      <abstract>The mixing of two or more languages is called Code-Mixing (CM). CM is a social norm in multilingual societies. Neural Language Models (NLMs) like transformers have been effective on many NLP tasks. However, NLM for CM is an under-explored area. Though transformers are capable and powerful, they cannot always encode positional information since they are non-recurrent. Therefore, to enrich word information and incorporate positional information, positional encoding is defined. We hypothesize that Switching Points (SPs), i.e., junctions in the text where the language switches (L1 -&gt; L2 or L2 -&gt; L1), pose a challenge for CM Language Models (LMs), and hence give special emphasis to SPs in the modeling process. We experiment with several positional encoding mechanisms and show that rotatory positional encodings along with switching point information yield the best results.

We introduce CONFLATOR: a neural language modeling approach for code-mixed languages. CONFLATOR tries to learn to emphasize switching points using smarter positional encoding, both at unigram and bigram levels. CONFLATOR outperforms the state-of-the-art on two tasks based on code-mixed Hindi and English (Hinglish): (i) sentiment analysis and (ii) machine translation.</abstract>
      <url hash="e616bc03">2023.calcs-1.6</url>
      <bibkey>mohammed-etal-2023-conflator</bibkey>
      <doi>10.18653/v1/2023.calcs-1.6</doi>
    </paper>
    <paper id="7">
      <title>Unified Model for Code-Switching Speech Recognition and Language Identification Based on Concatenated Tokenizer</title>
      <author><first>Kunal</first><last>Dhawan</last></author>
      <author><first>KDimating</first><last>Rekesh</last></author>
      <author><first>Boris</first><last>Ginsburg</last></author>
      <pages>74-82</pages>
      <abstract>Code-Switching (CS) multilingual Automatic Speech Recognition (ASR) models can transcribe speech containing two or more alternating languages during a conversation. This paper proposes (1) a new method for creating code-switching ASR datasets from purely monolingual data sources, and (2) a novel Concatenated Tokenizer that enables ASR models to generate language ID for each emitted text token while reusing existing monolingual tokenizers. The efficacy of these approaches for building CS ASR models is demonstrated for two language pairs, English-Hindi and English-Spanish, where we achieve new state-of-the-art results on the Miami Bangor CS evaluation corpus. In addition to competitive ASR performance, the proposed Concatenated Tokenizer models are highly effective for spoken language identification, achieving 98%+ accuracy on the out-of-distribution FLEURS dataset.</abstract>
      <url hash="f1ff2fca">2023.calcs-1.7</url>
      <bibkey>dhawan-etal-2023-unified</bibkey>
      <doi>10.18653/v1/2023.calcs-1.7</doi>
    </paper>
    <paper id="8">
      <title>Multilingual self-supervised speech representations improve the speech recognition of low-resource <fixed-case>A</fixed-case>frican languages with codeswitching</title>
      <author><first>Tolulope</first><last>Ogunremi</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>83-88</pages>
      <abstract>While many speakers of low-resource languages regularly code-switch between their languages and other regional languages or English, datasets of codeswitched speech are too small to train bespoke acoustic models from scratch or do language model rescoring. Here we propose finetuning self-supervised speech representations such as wav2vec 2.0 XLSR to recognize code-switched data. We find that finetuning self-supervised multilingual representations and augmenting them with n-gram language models trained from transcripts reduces absolute word error rates by up to 20% compared to baselines of hybrid models trained from scratch on code-switched data. Our findings suggest that in circumstances with limited training data finetuning self-supervised representations is a better performing and viable solution.</abstract>
      <url hash="8f240fd9">2023.calcs-1.8</url>
      <bibkey>ogunremi-etal-2023-multilingual</bibkey>
      <doi>10.18653/v1/2023.calcs-1.8</doi>
    </paper>
  </volume>
</collection>
