<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.lantern">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</booktitle>
      <editor><first>Aditya</first><last>Mogadala</last></editor>
      <editor><first>Sandro</first><last>Pezzelle</last></editor>
      <editor><first>Dietrich</first><last>Klakow</last></editor>
      <editor><first>Marie-Francine</first><last>Moens</last></editor>
      <editor><first>Zeynep</first><last>Akata</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain</address>
      <month>December</month>
      <year>2020</year>
      <venue>lantern</venue>
    </meta>
    <frontmatter>
      <url hash="a1f49e6f">2020.lantern-1.0</url>
      <bibkey>lantern-2020-beyond</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Eyes on the Parse: Using Gaze Features in Syntactic Parsing</title>
      <author><first>Abhishek</first><last>Agrawal</last></author>
      <author><first>Rudolf</first><last>Rosa</last></author>
      <pages>1–16</pages>
      <abstract>In this paper, we explore the potential benefits of leveraging eye-tracking information for dependency parsing on the English part of the Dundee corpus. To achieve this, we cast dependency parsing as a sequence labelling task and then augment the neural model for sequence labelling with eye-tracking features. We also augment a graph-based parser with eye-tracking features and parse the Dundee Corpus to corroborate our findings from the sequence labelling parser. We then experiment with a variety of parser setups ranging from parsing with all features to a delexicalized parser. Our experiments show that for a parser with all features, although the improvements are positive for the LAS score they are not significant whereas our delexicalized parser significantly outperforms the baseline we established. We also analyze the contribution of various eye-tracking features towards the different parser setups and find that eye-tracking features contain information which is complementary in nature, thus implying that augmenting the parser with various gaze features grouped together provides better performance than any individual gaze feature.</abstract>
      <url hash="45548fae">2020.lantern-1.1</url>
      <bibkey>agrawal-rosa-2020-eyes</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="2">
      <title>Leveraging Visual Question Answering to Improve Text-to-Image Synthesis</title>
      <author><first>Stanislav</first><last>Frolov</last></author>
      <author><first>Shailza</first><last>Jolly</last></author>
      <author><first>Jörn</first><last>Hees</last></author>
      <author><first>Andreas</first><last>Dengel</last></author>
      <pages>17–22</pages>
      <abstract>Generating images from textual descriptions has recently attracted a lot of interest. While current models can generate photo-realistic images of individual objects such as birds and human faces, synthesising images with multiple objects is still very difficult. In this paper, we propose an effective way to combine Text-to-Image (T2I) synthesis with Visual Question Answering (VQA) to improve the image quality and image-text alignment of generated images by leveraging the VQA 2.0 dataset. We create additional training samples by concatenating question and answer (QA) pairs and employ a standard VQA model to provide the T2I model with an auxiliary learning signal. We encourage images generated from QA pairs to look realistic and additionally minimize an external VQA loss. Our method lowers the FID from 27.84 to 25.38 and increases the R-prec. from 83.82% to 84.79% when compared to the baseline, which indicates that T2I synthesis can successfully be improved using a standard VQA model.</abstract>
      <url hash="fc21f93a">2020.lantern-1.2</url>
      <bibkey>frolov-etal-2020-leveraging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="3">
      <title>Seeing the World through Text: Evaluating Image Descriptions for Commonsense Reasoning in Machine Reading Comprehension</title>
      <author><first>Diana</first><last>Galvan-Sosa</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kyosuke</first><last>Nishida</last></author>
      <author><first>Koji</first><last>Matsuda</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>23–29</pages>
      <abstract>Despite recent achievements in natural language understanding, reasoning over commonsense knowledge still represents a big challenge to AI systems. As the name suggests, common sense is related to perception and as such, humans derive it from experience rather than from literary education. Recent works in the NLP and the computer vision field have made the effort of making such knowledge explicit using written language and visual inputs, respectively. Our premise is that the latter source fits better with the characteristics of commonsense acquisition. In this work, we explore to what extent the descriptions of real-world scenes are sufficient to learn common sense about different daily situations, drawing upon visual information to answer script knowledge questions.</abstract>
      <url hash="3ed02888">2020.lantern-1.3</url>
      <bibkey>galvan-sosa-etal-2020-seeing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mcscript">MCScript</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="4">
      <title>How Do Image Description Systems Describe People? A Targeted Assessment of System Competence in the <fixed-case>PEOPLE</fixed-case>-domain</title>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <pages>30–36</pages>
      <abstract>Evaluations of image description systems are typically domain-general: generated descriptions for the held-out test images are either compared to a set of reference descriptions (using automated metrics), or rated by human judges on one or more Likert scales (for fluency, overall quality, and other quality criteria). While useful, these evaluations do not tell us anything about the kinds of image descriptions that systems are able to produce. Or, phrased differently, these evaluations do not tell us anything about the cognitive capabilities of image description systems. This paper proposes a different kind of assessment, that is able to quantify the extent to which these systems are able to describe humans. This assessment is based on a manual characterisation (a context-free grammar) of English entity labels in the PEOPLE domain, to determine the range of possible outputs. We examined 9 systems to see what kinds of labels they actually use. We found that these systems only use a small subset of at most 13 different kinds of modifiers (e.g. tall and short modify HEIGHT, sad and happy modify MOOD), but 27 kinds of modifiers are never used. Future research could study these semantic dimensions in more detail.</abstract>
      <url hash="7661817b">2020.lantern-1.4</url>
      <bibkey>van-miltenburg-2020-image</bibkey>
      <pwccode url="https://github.com/evanmiltenburg/analysepeopledescriptions" additional="false">evanmiltenburg/analysepeopledescriptions</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
  </volume>
</collection>
