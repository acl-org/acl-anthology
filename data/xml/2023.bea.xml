<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.bea">
  <volume id="1" ingest-date="2023-07-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)</booktitle>
      <editor><first>Ekaterina</first><last>Kochmar</last></editor>
      <editor><first>Jill</first><last>Burstein</last></editor>
      <editor><first>Andrea</first><last>Horbach</last></editor>
      <editor><first>Ronja</first><last>Laarmann-Quante</last></editor>
      <editor><first>Nitin</first><last>Madnani</last></editor>
      <editor><first>Anaïs</first><last>Tack</last></editor>
      <editor><first>Victoria</first><last>Yaneva</last></editor>
      <editor><first>Zheng</first><last>Yuan</last></editor>
      <editor><first>Torsten</first><last>Zesch</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="5e7cec8f">2023.bea-1</url>
      <venue>bea</venue>
    </meta>
    <frontmatter>
      <url hash="457d8978">2023.bea-1.0</url>
      <bibkey>bea-2023-innovative</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>LFTK</fixed-case>: Handcrafted Features in Computational Linguistics</title>
      <author><first>Bruce W.</first><last>Lee</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Jason</first><last>Lee</last><affiliation>Lxper Ai</affiliation></author>
      <pages>1-19</pages>
      <abstract>Past research has identified a rich set of handcrafted linguistic features that can potentially assist various tasks. However, their extensive number makes it difficult to effectively select and utilize existing handcrafted features. Coupled with the problem of inconsistent implementation across research works, there has been no categorization scheme or generally-accepted feature names. This creates unwanted confusion. Also, no actively-maintained open-source library extracts a wide variety of handcrafted features. The current handcrafted feature extraction practices have several inefficiencies, and a researcher often has to build such an extraction system from the ground up. We collect and categorize more than 220 popular handcrafted features grounded on past literature. Then, we conduct a correlation analysis study on several task-specific datasets and report the potential use cases of each feature. Lastly, we devise a multilingual handcrafted linguistic feature extraction system in a systematically expandable manner. We open-source our system to give the community a rich set of pre-implemented handcrafted features.</abstract>
      <url hash="a5db7006">2023.bea-1.1</url>
      <bibkey>lee-lee-2023-lftk</bibkey>
      <doi>10.18653/v1/2023.bea-1.1</doi>
    </paper>
    <paper id="2">
      <title>Improving Mathematics Tutoring With A Code Scratchpad</title>
      <author><first>Shriyash</first><last>Upadhyay</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Etan</first><last>Ginsberg</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>20-28</pages>
      <abstract>Large language models can solve reasoning tasks (like math problems) more effectively when they are allowed to generate rationales. However, a good tutoring system should not just generate solutions, but should also generate explanations and should be able to correct and guide students. We show that providing a code scratchpad improves performance on each tutoring step with a gradeschool mathematics dataset. On these tutoring tasks, GPT-3 models provided with a code scratchpad significantly outperform those given only a language scratchpad (77.7% vs 48.7% cumulative accuracy).</abstract>
      <url hash="c620ffb9">2023.bea-1.2</url>
      <bibkey>upadhyay-etal-2023-improving</bibkey>
      <doi>10.18653/v1/2023.bea-1.2</doi>
    </paper>
    <paper id="3">
      <title>A Transfer Learning Pipeline for Educational Resource Discovery with Application in Survey Generation</title>
      <author><first>Irene</first><last>Li</last><affiliation>University of Tokyo</affiliation></author>
      <author><first>Thomas</first><last>George</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Alex</first><last>Fabbri</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Tammy</first><last>Liao</last><affiliation>Yale University</affiliation></author>
      <author><first>Benjamin</first><last>Chen</last><affiliation>Yale</affiliation></author>
      <author><first>Rina</first><last>Kawamura</last><affiliation>Yale University</affiliation></author>
      <author><first>Richard</first><last>Zhou</last><affiliation>Yale</affiliation></author>
      <author><first>Vanessa</first><last>Yan</last><affiliation>Yale</affiliation></author>
      <author><first>Swapnil</first><last>Hingmire</last><affiliation>Na</affiliation></author>
      <author><first>Dragomir</first><last>Radev</last><affiliation>Yale University</affiliation></author>
      <pages>29-43</pages>
      <abstract>Effective human learning depends on a wide selection of educational materials that align with the learner’s current understanding of the topic. While the Internet has revolutionized human learning or education, a substantial resource accessibility barrier still exists. Namely, the excess of online information can make it challenging to navigate and discover high-quality learning materials in a given subject area. In this paper, we propose an automatic pipeline for building an educational resource discovery system for new domains. The pipeline consists of three main steps: resource searching, feature extraction, and resource classification. We first collect frequent queries from a set of seed documents, and search the web with these queries to obtain candidate resources such as lecture slides and introductory blog posts. Then, we process these resources for BERT-based features and meta-features. Next, we train a tree-based classifier to decide whether they are suitable learning materials. The pipeline achieves F1 scores of 0.94 and 0.82 when evaluated on two similar but novel domains. Finally, we demonstrate how this pipeline can benefit two applications: prerequisite chain learning and leading paragraph generation for surveys. We also release a corpus of 39,728 manually labeled web resources and 659 queries from NLP, Computer Vision (CV), and Statistics (STATS).</abstract>
      <url hash="b44e48f2">2023.bea-1.3</url>
      <bibkey>li-etal-2023-transfer</bibkey>
      <doi>10.18653/v1/2023.bea-1.3</doi>
      <video href="2023.bea-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Using Learning Analytics for Adaptive Exercise Generation</title>
      <author><first>Tanja</first><last>Heck</last><affiliation>University of Tbingen</affiliation></author>
      <author><first>Detmar</first><last>Meurers</last><affiliation>Universitt Tbingen</affiliation></author>
      <pages>44-56</pages>
      <abstract>Single Choice exercises constitute a central exercise type for language learning in a learner’s progression from mere implicit exposure through input enhancement to productive language use in open exercises. Distractors that support learning in the individual zone of proximal development should not be derived from static analyses of learner corpora, but rely on dynamic learning analytics based on half-open exercises. We demonstrate how a system’s error diagnosis module can be re-used for automatic and dynamic generation and adaptation of distractors, as well as to inform exercise generation in terms of relevant learning goals and reasonable chunking in Jumbled Sentences exercises.</abstract>
      <url hash="207dd266">2023.bea-1.4</url>
      <bibkey>heck-meurers-2023-using</bibkey>
      <doi>10.18653/v1/2023.bea-1.4</doi>
    </paper>
    <paper id="5">
      <title>Reviewriter: <fixed-case>AI</fixed-case>-Generated Instructions For Peer Review Writing</title>
      <author><first>Xiaotian</first><last>Su</last><affiliation>Epfl</affiliation></author>
      <author><first>Thiemo</first><last>Wambsganss</last><affiliation>Swiss Federal Institute of Technology in Lausanne (EPFL)</affiliation></author>
      <author><first>Roman</first><last>Rietsche</last><affiliation>University of St.Gallen</affiliation></author>
      <author><first>Seyed Parsa</first><last>Neshaei</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Tanja</first><last>Käser</last><affiliation>Epfl</affiliation></author>
      <pages>57-71</pages>
      <url hash="9b731a08">2023.bea-1.5</url>
      <attachment type="attachment" hash="eda17148">2023.bea-1.5.attachment.mp4</attachment>
      <bibkey>su-etal-2023-reviewriter</bibkey>
      <abstract>Large Language Models (LLMs) offer novel opportunities for educational applications that have the potential to transform traditional learning for students. Despite AI-enhanced applications having the potential to provide personalized learning experiences, more studies are needed on the design of generative AI systems and evidence for using them in real educational settings. In this paper, we design, implement and evaluate \texttt{Reviewriter}, a novel tool to provide students with AI-generated instructions for writing peer reviews in German. Our study identifies three key aspects: a) we provide insights into student needs when writing peer reviews with generative models which we then use to develop a novel system to provide adaptive instructions b) we fine-tune three German language models on a selected corpus of 11,925 student-written peer review texts in German and choose German-GPT2 based on quantitative measures and human evaluation, and c) we evaluate our tool with fourteen students, revealing positive technology acceptance based on quantitative measures. Additionally, the qualitative feedback presents the benefits and limitations of generative AI in peer review writing.</abstract>
      <doi>10.18653/v1/2023.bea-1.5</doi>
      <video href="2023.bea-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Towards <fixed-case>L</fixed-case>2-friendly pipelines for learner corpora: A case of written production by <fixed-case>L</fixed-case>2-<fixed-case>K</fixed-case>orean learners</title>
      <author><first>Hakyung</first><last>Sung</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Gyu-Ho</first><last>Shin</last><affiliation>University of Illinois Chicago</affiliation></author>
      <pages>72-82</pages>
      <abstract>We introduce the Korean-Learner-Morpheme (KLM) corpus, a manually annotated dataset consisting of 129,784 morphemes from second language (L2) learners of Korean, featuring morpheme tokenization and part-of-speech (POS) tagging. We evaluate the performance of four Korean morphological analyzers in tokenization and POS tagging on the L2- Korean corpus. Results highlight the analyzers’ reduced performance on L2 data, indicating the limitation of advanced deep-learning models when dealing with L2-Korean corpora. We further show that fine-tuning one of the models with the KLM corpus improves its accuracy of tokenization and POS tagging on L2-Korean dataset.</abstract>
      <url hash="6a076d3b">2023.bea-1.6</url>
      <bibkey>sung-shin-2023-towards</bibkey>
      <doi>10.18653/v1/2023.bea-1.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>C</fixed-case>hat<fixed-case>B</fixed-case>ack: Investigating Methods of Providing Grammatical Error Feedback in a <fixed-case>GUI</fixed-case>-based Language Learning Chatbot</title>
      <author><first>Kai-Hui</first><last>Liang</last><affiliation>Columbia University</affiliation></author>
      <author><first>Sam</first><last>Davidson</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Xun</first><last>Yuan</last><affiliation>Columbia University</affiliation></author>
      <author><first>Shehan</first><last>Panditharatne</last><affiliation>Columbia University</affiliation></author>
      <author><first>Chun-Yen</first><last>Chen</last><affiliation>Articulate.AI</affiliation></author>
      <author><first>Ryan</first><last>Shea</last><affiliation>Columbia University</affiliation></author>
      <author><first>Derek</first><last>Pham</last><affiliation>Columbia University</affiliation></author>
      <author><first>Yinghua</first><last>Tan</last><affiliation>Passioncy</affiliation></author>
      <author><first>Erik</first><last>Voss</last><affiliation>Columbia University</affiliation></author>
      <author><first>Luke</first><last>Fryer</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>83-99</pages>
      <abstract>The increasing use of AI chatbots as conversation partners for second-language learners highlights the importance of providing effective feedback. To ensure a successful learning experience, it is essential for researchers and practitioners to understand the optimal timing, methods of delivery, and types of feedback that are most beneficial to learners. Synchronous grammar corrective feedback (CF) has been shown to be more effective than asynchronous methods in online writing tasks. Additionally, self-correction by language learners has proven more beneficial than teacher-provided correction, particularly for spoken language skills and non-novice learners. However, existing language-learning AI chatbots often lack synchronous CF and self-correction capabilities. To address this, we propose a synchronous conversational corrective feedback (CCF) method, which allows self-correction and provides metalinguistic explanations (ME). Our study suggests that in chatbot-driven language-learning tools, corrective feedback is more effectively delivered through means other than the social chatbot, such as a GUI interface. Furthermore, we found that guided self-correction offers a superior learning experience compared to providing explicit corrections, particularly for learners with high learning motivation or lower linguistic ability.</abstract>
      <url hash="c816abab">2023.bea-1.7</url>
      <bibkey>liang-etal-2023-chatback</bibkey>
      <doi>10.18653/v1/2023.bea-1.7</doi>
    </paper>
    <paper id="8">
      <title>Enhancing Video-based Learning Using Knowledge Tracing: Personalizing Students’ Learning Experience with <fixed-case>ORBITS</fixed-case></title>
      <author><first>Shady</first><last>Shehata</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</affiliation></author>
      <author><first>David</first><last>Santandreu Calonge</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Philip</first><last>Purnell</last><affiliation>Mohamed Bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Mark</first><last>Thompson</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>100-107</pages>
      <abstract>As the world regains its footing following the COVID-19 pandemic, academia is striving to consolidate the gains made in students’ education experience. New technologies such as video-based learning have shown some early improvement in student learning and engagement. In this paper, we present ORBITS predictive engine at YOURIKA company, a video-based student support platform powered by knowledge tracing. In an exploratory case study of one master’s level Speech Processing course at the Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI) in Abu Dhabi, half the students used the system while the other half did not. Student qualitative feedback was universally positive and compared the system favorably against current available methods. These findings support the use of artificial intelligence techniques to improve the student learning experience.</abstract>
      <url hash="d52bf878">2023.bea-1.8</url>
      <bibkey>shehata-etal-2023-enhancing</bibkey>
      <doi>10.18653/v1/2023.bea-1.8</doi>
    </paper>
    <paper id="9">
      <title>Enhancing Human Summaries for Question-Answer Generation in Education</title>
      <author><first>Hannah</first><last>Gonzalez</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Liam</first><last>Dugan</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Eleni</first><last>Miltsakaki</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Zhiqi</first><last>Cui</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Jiaxuan</first><last>Ren</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Bryan</first><last>Li</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Shriyash</first><last>Upadhyay</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Etan</first><last>Ginsberg</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>108-118</pages>
      <abstract>We address the problem of generating high-quality question-answer pairs for educational materials. Previous work on this problem showed that using summaries as input improves the quality of question generation (QG) over original textbook text and that human-written summaries result in higher quality QG than automatic summaries. In this paper, a) we show that advances in Large Language Models (LLMs) are not yet sufficient to generate quality summaries for QG and b) we introduce a new methodology for enhancing bullet point student notes into fully fledged summaries and find that our methodology yields higher quality QG. We conducted a large-scale human annotation study of generated question-answer pairs for the evaluation of our methodology. In order to aid in future research, we release a new dataset of 9.2K human annotations of generated questions.</abstract>
      <url hash="fae50c7e">2023.bea-1.9</url>
      <attachment type="attachment" hash="f9384768">2023.bea-1.9.attachment.zip</attachment>
      <bibkey>gonzalez-etal-2023-enhancing</bibkey>
      <doi>10.18653/v1/2023.bea-1.9</doi>
      <video href="2023.bea-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Difficulty-Controllable Neural Question Generation for Reading Comprehension using Item Response Theory</title>
      <author><first>Masaki</first><last>Uto</last><affiliation>The University of Electro-Communications</affiliation></author>
      <author><first>Yuto</first><last>Tomikawa</last><affiliation>The University of Electro-Communications</affiliation></author>
      <author><first>Ayaka</first><last>Suzuki</last><affiliation>The University of Electro-Communications</affiliation></author>
      <pages>119-129</pages>
      <abstract>Question generation (QG) for reading comprehension, a technology for automatically generating questions related to given reading passages, has been used in various applications, including in education. Recently, QG methods based on deep neural networks have succeeded in generating fluent questions that are pertinent to given reading passages. One example of how QG can be applied in education is a reading tutor that automatically offers reading comprehension questions related to various reading materials. In such an application, QG methods should provide questions with difficulty levels appropriate for each learner’s reading ability in order to improve learning efficiency. Several difficulty-controllable QG methods have been proposed for doing so. However, conventional methods focus only on generating questions and cannot generate answers to them. Furthermore, they ignore the relation between question difficulty and learner ability, making it hard to determine an appropriate difficulty for each learner. To resolve these problems, we propose a new method for generating question–answer pairs that considers their difficulty, estimated using item response theory. The proposed difficulty-controllable generation is realized by extending two pre-trained transformer models: BERT and GPT-2.</abstract>
      <url hash="2341c00c">2023.bea-1.10</url>
      <bibkey>uto-etal-2023-difficulty</bibkey>
      <doi>10.18653/v1/2023.bea-1.10</doi>
    </paper>
    <paper id="11">
      <title>Evaluating Classroom Potential for Card-it: Digital <fixed-case>F</fixed-case>lashcards for Studying and Learning <fixed-case>I</fixed-case>talian Morphology</title>
      <author><first>Mariana</first><last>Shimabukuro</last><affiliation>Ontario Tech University</affiliation></author>
      <author><first>Jessica</first><last>Zipf</last><affiliation>University of Konstanz</affiliation></author>
      <author><first>Shawn</first><last>Yama</last><affiliation>Ontario Tech University</affiliation></author>
      <author><first>Christopher</first><last>Collins</last><affiliation>Ontario Tech University</affiliation></author>
      <pages>130-136</pages>
      <abstract>This paper presents Card-it, a web-based application for learning Italian verb conjugation. Card-it integrates a large-scale finite-state morphological~(FSM) analyzer and a flashcard application as a user-friendly way for learners to utilize the analyzer. While Card-it can be used by individual learners, to support classroom adoption, we implemented simple classroom management functionalities such as sharing flashcards to a class and tracking students’ progression. We evaluated Card-it with teachers of Italian. Card-it was reported as engaging and supportive, especially by featuring two different quiz types combined with a verb form look-up feature. Teachers were optimistic about the potential of Card-it as a classroom supplementary tool for learners of Italian as L2. Future work includes sample sentences and a complete learners evaluation.</abstract>
      <url hash="568e2eb7">2023.bea-1.11</url>
      <attachment type="attachment" hash="152d9b35">2023.bea-1.11.attachment.mp4</attachment>
      <bibkey>shimabukuro-etal-2023-evaluating</bibkey>
      <doi>10.18653/v1/2023.bea-1.11</doi>
      <video href="2023.bea-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Scalable and Explainable Automated Scoring for Open-Ended Constructed Response Math Word Problems</title>
      <author><first>Scott</first><last>Hellman</last><affiliation>Pearson</affiliation></author>
      <author><first>Alejandro</first><last>Andrade</last><affiliation>Pearson</affiliation></author>
      <author><first>Kyle</first><last>Habermehl</last><affiliation>Pearson</affiliation></author>
      <pages>137-147</pages>
      <abstract>Open-ended constructed response math word problems (“math plus text”, or MPT) are a powerful tool in the assessment of students’ abilities to engage in mathematical reasoning and creative thinking. Such problems ask the student to compute a value or construct an expression and then explain, potentially in prose, what steps they took and why they took them. MPT items can be scored against highly structured rubrics, and we develop a novel technique for the automated scoring of MPT items that leverages these rubrics to provide explainable scoring. We show that our approach can be trained automatically and performs well on a large dataset of 34,417 responses across 14 MPT items.</abstract>
      <url hash="8d22deee">2023.bea-1.12</url>
      <bibkey>hellman-etal-2023-scalable</bibkey>
      <doi>10.18653/v1/2023.bea-1.12</doi>
      <video href="2023.bea-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Gender-Inclusive Grammatical Error Correction through Augmentation</title>
      <author><first>Gunnar</first><last>Lund</last><affiliation>Grammarly</affiliation></author>
      <author><first>Kostiantyn</first><last>Omelianchuk</last><affiliation>Grammarly</affiliation></author>
      <author><first>Igor</first><last>Samokhin</last><affiliation>Grammarly</affiliation></author>
      <pages>148-162</pages>
      <abstract>In this paper we show that GEC systems display gender bias related to the use of masculine and feminine terms and the gender-neutral singular “they”. We develop parallel datasets of texts with masculine and feminine terms, and singular “they”, and use them to quantify gender bias in three competitive GEC systems. We contribute a novel data augmentation technique for singular “they” leveraging linguistic insights about its distribution relative to plural “they”. We demonstrate that both this data augmentation technique and a refinement of a similar augmentation technique for masculine and feminine terms can generate training data that reduces bias in GEC systems, especially with respect to singular “they” while maintaining the same level of quality.</abstract>
      <url hash="4804b0ed">2023.bea-1.13</url>
      <bibkey>lund-etal-2023-gender</bibkey>
      <doi>10.18653/v1/2023.bea-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>R</fixed-case>ead<fixed-case>A</fixed-case>long Studio Web Interface for Digital Interactive Storytelling</title>
      <author><first>Aidan</first><last>Pine</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>David</first><last>Huggins-Daines</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Eric</first><last>Joanis</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Patrick</first><last>Littell</last><affiliation>National Research Council of Canada</affiliation></author>
      <author><first>Marc</first><last>Tessier</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Delasie</first><last>Torkornoo</last><affiliation>Carleton University</affiliation></author>
      <author><first>Rebecca</first><last>Knowles</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Roland</first><last>Kuhn</last><affiliation>National Research Council of Canada</affiliation></author>
      <author><first>Delaney</first><last>Lothian</last><affiliation>Univesity of Alberta</affiliation></author>
      <pages>163-172</pages>
      <abstract>We develop an interactive web-based user interface for performing textspeech alignment and creating digital interactive “read-along audio books that highlight words as they are spoken and allow users to replay individual words when clicked. We build on an existing Python library for zero-shot multilingual textspeech alignment (Littell et al., 2022), extend it by exposing its functionality through a RESTful API, and rewrite the underlying speech recognition engine to run in the browser. The ReadAlong Studio Web App is open-source, user-friendly, prioritizes privacy and data sovereignty, allows for a variety of standard export formats, and is designed to work for the majority of the world’s languages.</abstract>
      <url hash="cc10d7ef">2023.bea-1.14</url>
      <bibkey>pine-etal-2023-readalong</bibkey>
      <doi>10.18653/v1/2023.bea-1.14</doi>
      <video href="2023.bea-1.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Labels are not necessary: Assessing peer-review helpfulness using domain adaptation based on self-training</title>
      <author><first>Chengyuan</first><last>Liu</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Divyang</first><last>Doshi</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Muskaan</first><last>Bhargava</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Ruixuan</first><last>Shang</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Jialin</first><last>Cui</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Dongkuan</first><last>Xu</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Edward</first><last>Gehringer</last><affiliation>North Carolina State University</affiliation></author>
      <pages>173-183</pages>
      <abstract>A peer-assessment system allows students to provide feedback on each other’s work. An effective peer assessment system urgently requires helpful reviews to facilitate students to make improvements and progress. Automated evaluation of review helpfulness, with the help of deep learning models and natural language processing techniques, gains much interest in the field of peer assessment. However, collecting labeled data with the “helpfulness” tag to build these prediction models remains challenging. A straightforward solution would be using a supervised learning algorithm to train a prediction model on a similar domain and apply it to our peer review domain for inference. But naively doing so can degrade the model performance in the presence of the distributional gap between domains. Such a distributional gap can be effectively addressed by Domain Adaptation (DA). Self-training has recently been shown as a powerful branch of DA to address the distributional gap. The first goal of this study is to evaluate the performance of self-training-based DA in predicting the helpfulness of peer reviews as well as the ability to overcome the distributional gap. Our second goal is to propose an advanced self-training framework to overcome the weakness of the existing self-training by tailoring knowledge distillation and noise injection, to further improve the model performance and better address the distributional gap.</abstract>
      <url hash="1a06806b">2023.bea-1.15</url>
      <bibkey>liu-etal-2023-labels</bibkey>
      <doi>10.18653/v1/2023.bea-1.15</doi>
    </paper>
    <paper id="16">
      <title>Generating Dialog Responses with Specified Grammatical Items for Second Language Learning</title>
      <author><first>Yuki</first><last>Okano</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Kotaro</first><last>Funakoshi</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Ryo</first><last>Nagata</last><affiliation>Konan University</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <pages>184-194</pages>
      <abstract>This paper proposes a new second language learning task of generating a response including specified grammatical items. We consider two approaches: 1) fine-tuning a pre-trained language model (DialoGPT) by reinforcement learning and 2) providing a few-shot prompt to a large language model (GPT-3). For reinforcement learning, we examine combinations of three reward functions that consider grammatical items, diversity, and fluency. Our experiments confirm that both approaches can generate responses including the specified grammatical items and that it is crucial to consider fluency rather than diversity as the reward function.</abstract>
      <url hash="8f8d4018">2023.bea-1.16</url>
      <bibkey>okano-etal-2023-generating</bibkey>
      <doi>10.18653/v1/2023.bea-1.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>UKP</fixed-case>-<fixed-case>SQ</fixed-case>u<fixed-case>ARE</fixed-case>: An Interactive Tool for Teaching Question Answering</title>
      <author><first>Haishuo</first><last>Fang</last><affiliation>UKP Lab, TU Darmstadt</affiliation></author>
      <author><first>Haritz</first><last>Puerto</last><affiliation>UKP Lab, TU Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische Universitt Darmstadt</affiliation></author>
      <pages>195-204</pages>
      <abstract>The exponential growth of question answering (QA) has made it an indispensable topic in any Natural Language Processing (NLP) course. Additionally, the breadth of QA derived from this exponential growth makes it an ideal scenario for teaching related NLP topics such as information retrieval, explainability, and adversarial attacks among others. In this paper, we introduce UKP-SQuARE as a platform for QA education. This platform provides an interactive environment where students can run, compare, and analyze various QA models from different perspectives, such as general behavior, explainability, and robustness. Therefore, students can get a first-hand experience in different QA techniques during the class. Thanks to this, we propose a learner-centered approach for QA education in which students proactively learn theoretical concepts and acquire problem-solving skills through interactive exploration, experimentation, and practical assignments, rather than solely relying on traditional lectures. To evaluate the effectiveness of UKP-SQuARE in teaching scenarios, we adopted it in a postgraduate NLP course and surveyed the students after the course. Their positive feedback shows the platform’s effectiveness in their course and invites a wider adoption.</abstract>
      <url hash="e5e14a0b">2023.bea-1.17</url>
      <bibkey>fang-etal-2023-ukp</bibkey>
      <doi>10.18653/v1/2023.bea-1.17</doi>
      <video href="2023.bea-1.17.mp4"/>
    </paper>
    <paper id="18">
      <title>Exploring Effectiveness of <fixed-case>GPT</fixed-case>-3 in Grammatical Error Correction: A Study on Performance and Controllability in Prompt-Based Methods</title>
      <author><first>Mengsay</first><last>Loem</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Masahiro</first><last>Kaneko</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Sho</first><last>Takase</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Naoaki</first><last>Okazaki</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <pages>205-219</pages>
      <abstract>Large-scale pre-trained language models such as GPT-3 have shown remarkable performance across various natural language processing tasks. However, applying prompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasks and their controllability remains underexplored. Controllability in GEC is crucial for real-world applications, particularly in educational settings, where the ability to tailor feedback according to learner levels and specific error types can significantly enhance the learning process. This paper investigates the performance and controllability of prompt-based methods with GPT-3 for GEC tasks using zero-shot and few-shot setting. We explore the impact of task instructions and examples on GPT-3’s output, focusing on controlling aspects such as minimal edits, fluency edits, and learner levels. Our findings demonstrate that GPT-3 could effectively perform GEC tasks, outperforming existing supervised and unsupervised approaches. We also showed that GPT-3 could achieve controllability when appropriate task instructions and examples are given.</abstract>
      <url hash="428356ca">2023.bea-1.18</url>
      <bibkey>loem-etal-2023-exploring</bibkey>
      <doi>10.18653/v1/2023.bea-1.18</doi>
      <video href="2023.bea-1.18.mp4"/>
    </paper>
    <paper id="19">
      <title>A Closer Look at k-Nearest Neighbors Grammatical Error Correction</title>
      <author><first>Justin</first><last>Vasselli</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>220-231</pages>
      <abstract>In various natural language processing tasks, such as named entity recognition and machine translation, example-based approaches have been used to improve performance by leveraging existing knowledge. However, the effectiveness of this approach for Grammatical Error Correction (GEC) is unclear. In this work, we explore how an example-based approach affects the accuracy and interpretability of the output of GEC systems and the trade-offs involved. The approach we investigate has shown great promise in machine translation by using the $k$-nearest translation examples to improve the results of a pretrained Transformer model. We find that using this technique increases precision by reducing the number of false positives, but recall suffers as the model becomes more conservative overall. Increasing the number of example sentences in the datastore does lead to better performing systems, but with diminishing returns and a high decoding cost. Synthetic data can be used as examples, but the effectiveness varies depending on the base model. Finally, we find that finetuning on a set of data may be more effective than using that data during decoding as examples.</abstract>
      <url hash="e707de8a">2023.bea-1.19</url>
      <bibkey>vasselli-watanabe-2023-closer</bibkey>
      <doi>10.18653/v1/2023.bea-1.19</doi>
      <video href="2023.bea-1.19.mp4"/>
    </paper>
    <paper id="20">
      <title>Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models</title>
      <author><first>James</first><last>Fiacco</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>David</first><last>Adamson</last><affiliation>Turnitin</affiliation></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>232-241</pages>
      <abstract>By aligning the functional components derived from the activations of transformer models trained for AES with external knowledge such as human-understandable feature groups, the proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for performing such analyses on further neural AES systems. The analysis focuses on models trained to score essays based on organization, main idea, support, and language. The findings provide insights into the models’ decision-making processes, biases, and limitations, contributing to the development of more transparent and reliable AES systems.</abstract>
      <url hash="0cc60850">2023.bea-1.20</url>
      <bibkey>fiacco-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.bea-1.20</doi>
    </paper>
    <paper id="21">
      <title>Analyzing Bias in Large Language Model Solutions for Assisted Writing Feedback Tools: Lessons from the Feedback Prize Competition Series</title>
      <author><first>Perpetual</first><last>Baffour</last><affiliation>The Learning Agency Lab</affiliation></author>
      <author><first>Tor</first><last>Saxberg</last><affiliation>The Learning Agency Lab</affiliation></author>
      <author><first>Scott</first><last>Crossley</last><affiliation>Vanderbilt University</affiliation></author>
      <pages>242-246</pages>
      <abstract>This paper analyzes winning solutions from the Feedback Prize competition series hosted from 2021-2022. The competition sought to improve Assisted Writing Feedback Tools (AWFTs) by crowdsourcing Large Language Model (LLM) solutions for evaluating student writing. The winning models are freely available for incorporation into educational applications, but the models need to be assessed for performance and other factors. This study reports the performance accuracy of Feedback Prize-winning models based on demographic factors such as student race/ethnicity, economic disadvantage, and English Language Learner status. Two competitions are analyzed. The first, which focused on identifying discourse elements, demonstrated minimal bias based on students’ demographic factors. However, the second competition, which aimed to predict discourse effectiveness, exhibited moderate bias.</abstract>
      <url hash="132868b9">2023.bea-1.21</url>
      <bibkey>baffour-etal-2023-analyzing</bibkey>
      <doi>10.18653/v1/2023.bea-1.21</doi>
      <video href="2023.bea-1.21.mp4"/>
    </paper>
    <paper id="22">
      <title>Improving Reading Comprehension Question Generation with Data Augmentation and Overgenerate-and-rank</title>
      <author><first>Nischal</first><last>Ashok Kumar</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Nigel</first><last>Fernandez</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Zichao</first><last>Wang</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Andrew</first><last>Lan</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>247-259</pages>
      <abstract>Reading comprehension is a crucial skill in many aspects of education, including language learning, cognitive development, and fostering early literacy skills in children. Automated answer-aware reading comprehension question generation has significant potential to scale up learner support in educational activities. One key technical challenge in this setting is that there can be multiple questions, sometimes very different from each other, with the same answer; a trained question generation method may not necessarily know which question human educators would prefer. To address this challenge, we propose 1) a data augmentation method that enriches the training dataset with diverse questions given the same context and answer and 2) an overgenerate-and-rank method to select the best question from a pool of candidates. We evaluate our method on the FairytaleQA dataset, showing a 5% absolute improvement in ROUGE-L over the best existing method. We also demonstrate the effectiveness of our method in generating harder, “implicit” questions, where the answers are not contained in the context as text spans.</abstract>
      <url hash="e3bb40c3">2023.bea-1.22</url>
      <bibkey>ashok-kumar-etal-2023-improving</bibkey>
      <doi>10.18653/v1/2023.bea-1.22</doi>
      <video href="2023.bea-1.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Assisting Language Learners: Automated Trans-Lingual Definition Generation via Contrastive Prompt Learning</title>
      <author><first>Hengyuan</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Dawei</first><last>Li</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Yanran</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Chenming</first><last>Shang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Chufan</first><last>Shi</last><affiliation>Shenzhen International Graduate School, Tsinghua University</affiliation></author>
      <author><first>Yong</first><last>Jiang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>260-274</pages>
      <abstract>The standard definition generation task requires to automatically produce mono-lingual definitions (e.g., English definitions for English words), but ignores that the generated definitions may also consist of unfamiliar words for language learners. In this work, we propose a novel task of Trans-Lingual Definition Generation (TLDG), which aims to generate definitions in another language, i.e., the native speaker’s language. Initially, we explore the unsupervised manner of this task and build up a simple implementation of fine-tuning the multi-lingual machine translation model. Then, we develop two novel methods, Prompt Combination and Contrastive Prompt Learning, for further enhancing the quality of the generation. Our methods are evaluated against the baseline Pipeline method in both rich- and low-resource settings, and we empirically establish its superiority in generating higher-quality trans-lingual definitions.</abstract>
      <url hash="5e870ac1">2023.bea-1.23</url>
      <bibkey>zhang-etal-2023-assisting</bibkey>
      <doi>10.18653/v1/2023.bea-1.23</doi>
    </paper>
    <paper id="24">
      <title>Predicting the Quality of Revisions in Argumentative Writing</title>
      <author><first>Zhexiong</first><last>Liu</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Diane</first><last>Litman</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Elaine</first><last>Wang</last><affiliation>RAND Corporation</affiliation></author>
      <author><first>Lindsay</first><last>Matsumura</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Richard</first><last>Correnti</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>275-287</pages>
      <abstract>The ability to revise in response to feedback is critical to students’ writing success. In the case of argument writing in specific, identifying whether an argument revision (AR) is successful or not is a complex problem because AR quality is dependent on the overall content of an argument. For example, adding the same evidence sentence could strengthen or weaken existing claims in different argument contexts (ACs). To address this issue we developed Chain-of-Thought prompts to facilitate ChatGPT-generated ACs for AR quality predictions. The experiments on two corpora, our annotated elementary essays and existing college essays benchmark, demonstrate the superiority of the proposed ACs over baselines.</abstract>
      <url hash="7b6c80dc">2023.bea-1.24</url>
      <bibkey>liu-etal-2023-predicting</bibkey>
      <doi>10.18653/v1/2023.bea-1.24</doi>
      <video href="2023.bea-1.24.mp4"/>
    </paper>
    <paper id="25">
      <title>Reconciling Adaptivity and Task Orientation in the Student Dashboard of an Intelligent Language Tutoring System</title>
      <author><first>Leona</first><last>Colling</last><affiliation>University of Tbingen</affiliation></author>
      <author><first>Tanja</first><last>Heck</last><affiliation>University of Tbingen</affiliation></author>
      <author><first>Detmar</first><last>Meurers</last><affiliation>Universitt Tbingen</affiliation></author>
      <pages>288-299</pages>
      <abstract>In intelligent language tutoring systems, student dashboards should display the learning progress and performance and support the navigation through the learning content. Designing an interface that transparently offers information on students’ learning in relation to specific learning targets while linking to the overarching functional goal, that motivates and organizes the practice in current foreign language teaching, is challenging. This becomes even more difficult in systems that adaptively expose students to different learning material and individualize system interactions. If such a system is used in an ecologically valid setting of blended learning, this generates additional requirements to incorporate the needs of students and teachers for control and customizability.We present the conceptual design of a student dashboard for a task-based, user-adaptive intelligent language tutoring system intended for use in real-life English classes in secondary schools. We highlight the key challenges and spell out open questions for future research.</abstract>
      <url hash="cfe1075f">2023.bea-1.25</url>
      <bibkey>colling-etal-2023-reconciling</bibkey>
      <doi>10.18653/v1/2023.bea-1.25</doi>
    </paper>
    <paper id="26">
      <title><fixed-case>G</fixed-case>roun<fixed-case>D</fixed-case>ialog: A Dataset for Repair and Grounding in Task-oriented Spoken Dialogues for Language Learning</title>
      <author><first>Xuanming</first><last>Zhang</last><affiliation>Columbia University</affiliation></author>
      <author><first>Rahul</first><last>Divekar</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Rutuja</first><last>Ubale</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>300-314</pages>
      <url hash="fbf0ed44">2023.bea-1.26</url>
      <bibkey>zhang-etal-2023-groundialog</bibkey>
      <abstract>Improving conversational proficiency is a key target for students learning a new language. While acquiring conversational proficiency, students must learn the linguistic mechanisms of Repair and Grounding (R\&amp;amp;G) to negotiate meaning and find common ground with their interlocutor so conversational breakdowns can be resolved. Task-oriented Spoken Dialogue Systems (SDS) have long been sought as a tool to hone conversational proficiency. However, the R&amp;amp;G patterns for language learners interacting with a task-oriented spoken dialogue system are not reflected explicitly in any existing datasets. Therefore, to move the needle in Spoken Dialogue Systems for language learning we present GrounDialog: an annotated dataset of spoken conversations where we elicit a rich set of R&amp;amp;G patterns.</abstract>
      <doi>10.18653/v1/2023.bea-1.26</doi>
      <video href="2023.bea-1.26.mp4"/>
    </paper>
    <paper id="27">
      <title><fixed-case>SIGHT</fixed-case>: A Large Annotated Dataset on Student Insights Gathered from Higher Education Transcripts</title>
      <author><first>Rose</first><last>Wang</last><affiliation>Stanford</affiliation></author>
      <author><first>Pawan</first><last>Wirawarn</last><affiliation>Stanford</affiliation></author>
      <author><first>Noah</first><last>Goodman</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dorottya</first><last>Demszky</last><affiliation>Stanford University</affiliation></author>
      <pages>315-351</pages>
      <abstract>Lectures are a learning experience for both students and teachers. Students learn from teachers about the subject material, while teachers learn from students about how to refine their instruction. Unfortunately, online student feedback is unstructured and abundant, making it challenging for teachers to learn and improve. We take a step towards tackling this challenge. First, we contribute a dataset for studying this problem: SIGHT is a large dataset of 288 math lecture transcripts and 15,784 comments collected from the Massachusetts Institute of Technology OpenCourseWare (MIT OCW) YouTube channel. Second, we develop a rubric for categorizing feedback types using qualitative analysis. Qualitative analysis methods are powerful in uncovering domain-specific insights, however they are costly to apply to large data sources. To overcome this challenge, we propose a set of best practices for using large language models (LLMs) to cheaply classify the comments at scale. We observe a striking correlation between the model’s and humans’ annotation: Categories with consistent human annotations (0.9 inter-rater reliability, IRR) also display higher human-model agreement (0.7), while categories with less consistent human annotations (0.7-0.8 IRR) correspondingly demonstrate lower human-model agreement (0.3-0.5). These techniques uncover useful student feedback from thousands of comments, costing around $0.002 per comment. We conclude by discussing exciting future directions on using online student feedback and improving automated annotation techniques for qualitative research.</abstract>
      <url hash="32484d5c">2023.bea-1.27</url>
      <bibkey>wang-etal-2023-sight</bibkey>
      <doi>10.18653/v1/2023.bea-1.27</doi>
      <video href="2023.bea-1.27.mp4"/>
    </paper>
    <paper id="28">
      <title>Recognizing Learner Handwriting Retaining Orthographic Errors for Enabling Fine-Grained Error Feedback</title>
      <author><first>Christian</first><last>Gold</last><affiliation>Fernuniversitaet Hagen</affiliation></author>
      <author><first>Ronja</first><last>Laarmann-Quante</last><affiliation>Ruhr University Bochum</affiliation></author>
      <author><first>Torsten</first><last>Zesch</last><affiliation>Computational Linguistics, FernUniversitt in Hagen</affiliation></author>
      <pages>352-360</pages>
      <abstract>This paper addresses the problem of providing automatic feedback on orthographic errors in handwritten text. Despite the availability of automatic error detection systems, the practical problem of digitizing the handwriting remains. Current handwriting recognition (HWR) systems produce highly accurate transcriptions but normalize away the very errors that are essential for providing useful feedback, e.g. orthographic errors. Our contribution is twofold:First, we create a comprehensive dataset of handwritten text with transcripts retaining orthographic errors by transcribing 1,350 pages from the German learner dataset FD-LEX. Second, we train a simple HWR system on our dataset, allowing it to transcribe words with orthographic errors. Thereby, we evaluate the effect of different dictionaries on recognition output, highlighting the importance of addressing spelling errors in these dictionaries.</abstract>
      <url hash="a35816e1">2023.bea-1.28</url>
      <bibkey>gold-etal-2023-recognizing</bibkey>
      <doi>10.18653/v1/2023.bea-1.28</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>E</fixed-case>x<fixed-case>ASAG</fixed-case>: Explainable Framework for Automatic Short Answer Grading</title>
      <author><first>Maximilian</first><last>Tornqvist</last><affiliation>Stockholm University</affiliation></author>
      <author><first>Mosleh</first><last>Mahamud</last><affiliation>Stockholm University</affiliation></author>
      <author><first>Erick</first><last>Mendez Guzman</last><affiliation>The University of Manchester</affiliation></author>
      <author><first>Alexandra</first><last>Farazouli</last><affiliation>Stockholm University</affiliation></author>
      <pages>361-371</pages>
      <abstract>As in other NLP tasks, Automatic Short Answer Grading (ASAG) systems have evolved from using rule-based and interpretable machine learning models to utilizing deep learning architectures to boost accuracy. Since proper feedback is critical to student assessment, explainability will be crucial for deploying ASAG in real-world applications. This paper proposes a framework to generate explainable outcomes for assessing question-answer pairs of a Data Mining course in a binary manner. Our framework utilizes a fine-tuned Transformer-based classifier and an explainability module using SHAP or Integrated Gradients to generate language explanations for each prediction. We assess the outcome of our framework by calculating accuracy-based metrics for classification performance. Furthermore, we evaluate the quality of the explanations by measuring their agreement with human-annotated justifications using Intersection-Over-Union at a token level to derive a plausibility score. Despite the relatively limited sample, results show that our framework derives explanations that are, to some degree, aligned with domain-expert judgment. Furthermore, both explainability methods perform similarly in their agreement with human-annotated explanations. A natural progression of our work is to analyze the use of our explainable ASAG framework on a larger sample to determine the feasibility of implementing a pilot study in a real-world setting.</abstract>
      <url hash="c59c9af7">2023.bea-1.29</url>
      <bibkey>tornqvist-etal-2023-exasag</bibkey>
      <doi>10.18653/v1/2023.bea-1.29</doi>
    </paper>
    <paper id="30">
      <title>You’ve Got a Friend in ... a Language Model? A Comparison of Explanations of Multiple-Choice Items of Reading Comprehension between <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> and Humans</title>
      <author><first>George</first><last>Duenas</last><affiliation>Universidad Pedagogica Nacional</affiliation></author>
      <author><first>Sergio</first><last>Jimenez</last><affiliation>Instituto Caro y Cuervo</affiliation></author>
      <author><first>Geral</first><last>Mateus Ferro</last><affiliation>Universidad Pedagogica Nacional</affiliation></author>
      <pages>372-381</pages>
      <abstract>Creating high-quality multiple-choice items requires careful attention to several factors, including ensuring that there is only one correct option, that options are independent of each other, that there is no overlap between options, and that each option is plausible. This attention is reflected in the explanations provided by human item-writers for each option. This study aimed to compare the creation of explanations of multiple-choice item options for reading comprehension by ChatGPT with those created by humans. We used two context-dependent multiple-choice item sets created based on EvidenceCentered Design. Results indicate that ChatGPT is capable of producing explanations with different type of information that are comparable to those created by humans. So that humans could benefit from additional information given to enhance their explanations. We conclude that ChatGPT ability to generate explanations for multiple-choice item options in reading comprehension tests is comparable to that of humans.</abstract>
      <url hash="c665a5d6">2023.bea-1.30</url>
      <bibkey>duenas-etal-2023-youve</bibkey>
      <doi>10.18653/v1/2023.bea-1.30</doi>
    </paper>
    <paper id="31">
      <title>Automatically Generated Summaries of Video Lectures May Enhance Students’ Learning Experience</title>
      <author><first>Hannah</first><last>Gonzalez</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Jiening</first><last>Li</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Helen</first><last>Jin</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Jiaxuan</first><last>Ren</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Hongyu</first><last>Zhang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Ayotomiwa</first><last>Akinyele</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Adrian</first><last>Wang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Eleni</first><last>Miltsakaki</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Ryan</first><last>Baker</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>382-393</pages>
      <abstract>We introduce a novel technique for automatically summarizing lecture videos using large language models such as GPT-3 and we present a user study investigating the effects on the studying experience when automatic summaries are added to lecture videos. We test students under different conditions and find that the students who are shown a summary next to a lecture video perform better on quizzes designed to test the course materials than the students who have access only to the video or the summary. Our findings suggest that adding automatic summaries to lecture videos enhances the learning experience. Qualitatively, students preferred summaries when studying under time constraints.</abstract>
      <url hash="2b9047b1">2023.bea-1.31</url>
      <bibkey>gonzalez-etal-2023-automatically</bibkey>
      <doi>10.18653/v1/2023.bea-1.31</doi>
      <video href="2023.bea-1.31.mp4"/>
    </paper>
    <paper id="32">
      <title>Automated evaluation of written discourse coherence using <fixed-case>GPT</fixed-case>-4</title>
      <author><first>Ben</first><last>Naismith</last><affiliation>Duolingo</affiliation></author>
      <author><first>Phoebe</first><last>Mulcaire</last><affiliation>Duolingo</affiliation></author>
      <author><first>Jill</first><last>Burstein</last><affiliation>Duolingo</affiliation></author>
      <pages>394-403</pages>
      <abstract>The popularization of large language models (LLMs) such as OpenAI’s GPT-3 and GPT-4 have led to numerous innovations in the field of AI in education. With respect to automated writing evaluation (AWE), LLMs have reduced challenges associated with assessing writing quality characteristics that are difficult to identify automatically, such as discourse coherence. In addition, LLMs can provide rationales for their evaluations (ratings) which increases score interpretability and transparency. This paper investigates one approach to producing ratings by training GPT-4 to assess discourse coherence in a manner consistent with expert human raters. The findings of the study suggest that GPT-4 has strong potential to produce discourse coherence ratings that are comparable to human ratings, accompanied by clear rationales. Furthermore, the GPT-4 ratings outperform traditional NLP coherence metrics with respect to agreement with human ratings. These results have implications for advancing AWE technology for learning and assessment.</abstract>
      <url hash="53b2d168">2023.bea-1.32</url>
      <bibkey>naismith-etal-2023-automated</bibkey>
      <doi>10.18653/v1/2023.bea-1.32</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>ALEXSIS</fixed-case>+: Improving Substitute Generation and Selection for Lexical Simplification with Information Retrieval</title>
      <author><first>Kai</first><last>North</last><affiliation>George Mason University</affiliation></author>
      <author><first>Alphaeus</first><last>Dmonte</last><affiliation>George Mason University</affiliation></author>
      <author><first>Tharindu</first><last>Ranasinghe</last><affiliation>Aston University</affiliation></author>
      <author><first>Matthew</first><last>Shardlow</last><affiliation>Manchester Metropolitan University</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <pages>404-413</pages>
      <abstract>Lexical simplification (LS) automatically replaces words that are deemed difficult to understand for a given target population with simpler alternatives, whilst preserving the meaning of the original sentence. The TSAR-2022 shared task on LS provided participants with a multilingual lexical simplification test set. It contained nearly 1,200 complex words in English, Portuguese, and Spanish and presented multiple candidate substitutions for each complex word. The competition did not make training data available; therefore, teams had to use either off-the-shelf pre-trained large language models (LLMs) or out-domain data to develop their LS systems. As such, participants were unable to fully explore the capabilities of LLMs by re-training and/or fine-tuning them on in-domain data. To address this important limitation, we present ALEXSIS+, a multilingual dataset in the aforementioned three languages, and ALEXSIS++, an English monolingual dataset that together contains more than 50,000 unique sentences retrieved from news corpora and annotated with cosine similarities to the original complex word and sentence. Using these additional contexts, we are able to generate new high-quality candidate substitutions that improve LS performance on the TSAR-2022 test set regardless of the language or model.</abstract>
      <url hash="eda081a2">2023.bea-1.33</url>
      <bibkey>north-etal-2023-alexsis</bibkey>
      <doi>10.18653/v1/2023.bea-1.33</doi>
    </paper>
    <paper id="34">
      <title>Generating Better Items for Cognitive Assessments Using Large Language Models</title>
      <author><first>Antonio</first><last>Laverghetta Jr.</last><affiliation>University of South Florida</affiliation></author>
      <author><first>John</first><last>Licato</last><affiliation>University of South Florida</affiliation></author>
      <pages>414-428</pages>
      <abstract>Writing high-quality test questions (items) is critical to building educational measures but has traditionally also been a time-consuming process. One promising avenue for alleviating this is automated item generation, whereby methods from artificial intelligence (AI) are used to generate new items with minimal human intervention. Researchers have explored using large language models (LLMs) to generate new items with equivalent psychometric properties to human-written ones. But can LLMs generate items with improved psychometric properties, even when existing items have poor validity evidence? We investigate this using items from a natural language inference (NLI) dataset. We develop a novel prompting strategy based on selecting items with both the best and worst properties to use in the prompt and use GPT-3 to generate new NLI items. We find that the GPT-3 items show improved psychometric properties in many cases, whilst also possessing good content, convergent and discriminant validity evidence. Collectively, our results demonstrate the potential of employing LLMs to ease the item development process and suggest that the careful use of prompting may allow for iterative improvement of item quality.</abstract>
      <url hash="c60d0bcc">2023.bea-1.34</url>
      <attachment type="attachment" hash="31fdaf27">2023.bea-1.34.attachment.zip</attachment>
      <bibkey>laverghetta-jr-licato-2023-generating</bibkey>
      <doi>10.18653/v1/2023.bea-1.34</doi>
    </paper>
    <paper id="35">
      <title>Span Identification of Epistemic Stance-Taking in Academic Written <fixed-case>E</fixed-case>nglish</title>
      <author><first>Masaki</first><last>Eguchi</last><affiliation>University of Oregon/Waseda University</affiliation></author>
      <author><first>Kristopher</first><last>Kyle</last><affiliation>University of Oregon</affiliation></author>
      <pages>429-442</pages>
      <abstract>Responding to the increasing need for automated writing evaluation (AWE) systems to assess language use beyond lexis and grammar (Burstein et al., 2016), we introduce a new approach to identify rhetorical features of stance in academic English writing. Drawing on the discourse-analytic framework of engagement in the Appraisal analysis (Martin &amp; White, 2005), we manually annotated 4,688 sentences (126,411 tokens) for eight rhetorical stance categories (e.g., PROCLAIM, ATTRIBUTION) and additional discourse elements. We then report an experiment to train machine learning models to identify and categorize the spans of these stance expressions. The best-performing model (RoBERTa + LSTM) achieved macro-averaged F1 of .7208 in the span identification of stance-taking expressions, slightly outperforming the intercoder reliability estimates before adjudication (F1 = .6629).</abstract>
      <url hash="798abf3a">2023.bea-1.35</url>
      <bibkey>eguchi-kyle-2023-span</bibkey>
      <doi>10.18653/v1/2023.bea-1.35</doi>
    </paper>
    <paper id="36">
      <title><fixed-case>ACTA</fixed-case>: Short-Answer Grading in High-Stakes Medical Exams</title>
      <author><first>King Yiu</first><last>Suen</last><affiliation>Nbme</affiliation></author>
      <author><first>Victoria</first><last>Yaneva</last><affiliation>National Board of Medical Examiners</affiliation></author>
      <author><first>Le An</first><last>Ha</last><affiliation>RGCL, RIILP, University of Wolverhampton</affiliation></author>
      <author><first>Janet</first><last>Mee</last><affiliation>National Board of Medical Examiners</affiliation></author>
      <author><first>Yiyun</first><last>Zhou</last><affiliation>Nbme</affiliation></author>
      <author><first>Polina</first><last>Harik</last><affiliation>Nbme</affiliation></author>
      <pages>443-447</pages>
      <abstract>This paper presents the ACTA system, which performs automated short-answer grading in the domain of high-stakes medical exams. The system builds upon previous work on neural similarity-based grading approaches by applying these to the medical domain and utilizing contrastive learning as a means to optimize the similarity metric. ACTA is evaluated against three strong baselines and is developed in alignment with operational needs, where low-confidence responses are flagged for human review. Learning curves are explored to understand the effects of training data on performance. The results demonstrate that ACTA leads to substantially lower number of responses being flagged for human review, while maintaining high classification accuracy.</abstract>
      <url hash="0f175eb5">2023.bea-1.36</url>
      <bibkey>suen-etal-2023-acta</bibkey>
      <doi>10.18653/v1/2023.bea-1.36</doi>
    </paper>
    <paper id="37">
      <title>Hybrid Models for Sentence Readability Assessment</title>
      <author><first>Fengkai</first><last>Liu</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>John</first><last>Lee</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>448-454</pages>
      <abstract>Automatic readability assessment (ARA) predicts how difficult it is for the reader to understand a text. While ARA has traditionally been performed at the passage level, there has been increasing interest in ARA at the sentence level, given its applications in downstream tasks such as text simplification and language exercise generation. Recent research has suggested the effectiveness of hybrid approaches for ARA, but they have yet to be applied on the sentence level. We present the first study that compares neural and hybrid models for sentence-level ARA. We conducted experiments on graded sentences from the Wall Street Journal (WSJ) and a dataset derived from the OneStopEnglish corpus. Experimental results show that both neural and hybrid models outperform traditional classifiers trained on linguistic features. Hybrid models obtained the best accuracy on both datasets, surpassing the previous best result reported on the WSJ dataset by almost 13% absolute.</abstract>
      <url hash="178dcaa2">2023.bea-1.37</url>
      <bibkey>liu-lee-2023-hybrid</bibkey>
      <doi>10.18653/v1/2023.bea-1.37</doi>
    </paper>
    <paper id="38">
      <title>Training for Grammatical Error Correction Without Human-Annotated <fixed-case>L</fixed-case>2 Learners’ Corpora</title>
      <author><first>Mikio</first><last>Oda</last><affiliation>National Institute of Technology, Kurume College</affiliation></author>
      <pages>455-465</pages>
      <abstract>Grammatical error correction (GEC) is a challenging task for non-native second language (L2) learners and learning machines. Data-driven GEC learning requires as much human-annotated genuine training data as possible. However, it is difficult to produce larger-scale human-annotated data, and synthetically generated large-scale parallel training data is valuable for GEC systems. In this paper, we propose a method for rebuilding a corpus of synthetic parallel data using target sentences predicted by a GEC model to improve performance. Experimental results show that our proposed pre-training outperforms that on the original synthetic datasets. Moreover, it is also shown that our proposed training without human-annotated L2 learners’ corpora is as practical as conventional full pipeline training with both synthetic datasets and L2 learners’ corpora in terms of accuracy.</abstract>
      <url hash="d8d206e1">2023.bea-1.38</url>
      <bibkey>oda-2023-training</bibkey>
      <doi>10.18653/v1/2023.bea-1.38</doi>
      <video href="2023.bea-1.38.mp4"/>
    </paper>
    <paper id="39">
      <title>Exploring a New Grammatico-functional Type of Measure as Part of a Language Learning Expert System</title>
      <author><first>Cyriel</first><last>Mallart</last><affiliation>LIDILE Universit Rennes 2</affiliation></author>
      <author><first>Andrew</first><last>Simpkin</last><affiliation>School of Mathematics, Statistics and Applied Mathematics, University of Galway</affiliation></author>
      <author><first>Rmi</first><last>Venant</last><affiliation>LIUM Universit du Mans</affiliation></author>
      <author><first>Nicolas</first><last>Ballier</last><affiliation>CLILLAC-ARP Universit de Paris Cit</affiliation></author>
      <author><first>Bernardo</first><last>Stearns</last><affiliation>Data Science Institute, University of Galway</affiliation></author>
      <author><first>Jen</first><last>Yu Li</last><affiliation>LIDILE Universit Rennes 2</affiliation></author>
      <author><first>Thomas</first><last>Gaillat</last><affiliation>Universit de Rennes 2</affiliation></author>
      <pages>466-476</pages>
      <abstract>This paper explores the use of L2-specific grammatical microsystems as elements of the domain knowledge of an Intelligent Computer-assisted Language Learning (ICALL) system. We report on the design of new grammatico-functional measures and their association with proficiency. We illustrate the approach with the design of the IT, THIS, THAT proform microsystem. The measures rely on the paradigmatic relations between words of the same linguistic functions. They are operationalised with one frequency-based and two probabilistic methods, i.e., the relative proportions of the forms and their likelihood of occurrence. Ordinal regression models show that the measures are significant in terms of association with CEFR levels, paving the way for their introduction in a specific proform microsystem expert model.</abstract>
      <url hash="650a6dce">2023.bea-1.39</url>
      <bibkey>mallart-etal-2023-exploring</bibkey>
      <doi>10.18653/v1/2023.bea-1.39</doi>
    </paper>
    <paper id="40">
      <title><fixed-case>J</fixed-case>apanese Lexical Complexity for Non-Native Readers: A New Dataset</title>
      <author><first>Yusuke</first><last>Ide</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Masato</first><last>Mita</last><affiliation>CyberAgent Inc.</affiliation></author>
      <author><first>Adam</first><last>Nohejl</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Hiroki</first><last>Ouchi</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>477-487</pages>
      <abstract>Lexical complexity prediction (LCP) is the task of predicting the complexity of words in a text on a continuous scale. It plays a vital role in simplifying or annotating complex words to assist readers. To study lexical complexity in Japanese, we construct the first Japanese LCP dataset. Our dataset provides separate complexity scores for Chinese/Korean annotators and others to address the readers’ L1-specific needs. In the baseline experiment, we demonstrate the effectiveness of a BERT-based system for Japanese LCP.</abstract>
      <url hash="60dbe289">2023.bea-1.40</url>
      <bibkey>ide-etal-2023-japanese</bibkey>
      <doi>10.18653/v1/2023.bea-1.40</doi>
      <video href="2023.bea-1.40.mp4"/>
    </paper>
    <paper id="41">
      <title>Grammatical Error Correction for Sentence-level Assessment in Language Learning</title>
      <author><first>Anisia</first><last>Katinskaia</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Roman</first><last>Yangarber</last><affiliation>University of Helsinki</affiliation></author>
      <pages>488-502</pages>
      <abstract>The paper presents experiments on using a Grammatical Error Correction (GEC) model to assess the correctness of answers that language learners give to grammar exercises. We explored whether a GEC model can be applied in the language learning context for a language with complex morphology. We empirically check a hypothesis that a GEC model corrects only errors and leaves correct answers unchanged. We perform a test on assessing learner answers in a real but constrained language-learning setup: the learners answer only fill-in-the-blank and multiple-choice exercises. For this purpose, we use ReLCo, a publicly available manually annotated learner dataset in Russian (Katinskaia et al., 2022). In this experiment, we fine-tune a large-scale T5 language model for the GEC task and estimate its performance on the RULEC-GEC dataset (Rozovskaya and Roth, 2019) to compare with top-performing models. We also release an updated version of the RULEC-GEC test set, manually checked by native speakers. Our analysis shows that the GEC model performs reasonably well in detecting erroneous answers to grammar exercises and potentially can be used for best-performing error types in a real learning setup. However, it struggles to assess answers which were tagged by human annotators as alternative-correct using the aforementioned hypothesis. This is in large part due to a still low recall in correcting errors, and the fact that the GEC model may modify even correct words—it may generate plausible alternatives, which are hard to evaluate against the gold-standard reference.</abstract>
      <url hash="1e42b3af">2023.bea-1.41</url>
      <bibkey>katinskaia-yangarber-2023-grammatical</bibkey>
      <doi>10.18653/v1/2023.bea-1.41</doi>
      <video href="2023.bea-1.41.mp4"/>
    </paper>
    <paper id="42">
      <title>“Geen makkie”: Interpretable Classification and Simplification of <fixed-case>D</fixed-case>utch Text Complexity</title>
      <author><first>Eliza</first><last>Hobo</last><affiliation>TNO, Netherlands Organisation for Applied Scientific Research</affiliation></author>
      <author><first>Charlotte</first><last>Pouw</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Lisa</first><last>Beinborn</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <pages>503-517</pages>
      <abstract>An inclusive society needs to facilitate access to information for all of its members, including citizens with low literacy and with non-native language skills. We present an approach to assess Dutch text complexity on the sentence level and conduct an interpretability analysis to explore the link between neural models and linguistic complexity features. Building on these findings, we develop the first contextual lexical simplification model for Dutch and publish a pilot dataset for evaluation. We go beyondprevious work which primarily targeted lexical substitution and propose strategies for adjusting the model’s linguistic register to generate simpler candidates. Our results indicate that continual pre-training and multi-task learning with conceptually related tasks are promising directions for ensuring the simplicity of the generated substitutions.</abstract>
      <url hash="ac8df894">2023.bea-1.42</url>
      <bibkey>hobo-etal-2023-geen</bibkey>
      <doi>10.18653/v1/2023.bea-1.42</doi>
      <video href="2023.bea-1.42.mp4"/>
    </paper>
    <paper id="43">
      <title><fixed-case>CEFR</fixed-case>-based Contextual Lexical Complexity Classifier in <fixed-case>E</fixed-case>nglish and <fixed-case>F</fixed-case>rench</title>
      <author><first>Desislava</first><last>Aleksandrova</last><affiliation>CBC/Radio-Canada</affiliation></author>
      <author><first>Vincent</first><last>Pouliot</last><affiliation>CBC/Radio-Canada</affiliation></author>
      <pages>518-527</pages>
      <abstract>This paper describes a CEFR-based classifier of single-word and multi-word lexical complexity in context from a second language learner perspective in English and in French, developed as an analytical tool for the pedagogical team of the language learning application Mauril. We provide an overview of the required corpora and the way we transformed it into rich contextual representations that allow the disambiguation and accurate labelling in context of polysemous occurrences of a given lexical item. We report evaluation results for all models, including two multi-lingual lexical classifiers evaluated on novel French datasets created for this experiment. Finally, we share the perspective of Mauril’s pedagogical team on the limitations of such systems.</abstract>
      <url hash="ae5eb9e1">2023.bea-1.43</url>
      <bibkey>aleksandrova-pouliot-2023-cefr</bibkey>
      <doi>10.18653/v1/2023.bea-1.43</doi>
    </paper>
    <paper id="44">
      <title>The <fixed-case>NCTE</fixed-case> Transcripts: A Dataset of Elementary Math Classroom Transcripts</title>
      <author><first>Dorottya</first><last>Demszky</last><affiliation>Stanford University</affiliation></author>
      <author><first>Heather</first><last>Hill</last><affiliation>Harvard</affiliation></author>
      <pages>528-538</pages>
      <abstract>Classroom discourse is a core medium of instruction analyzing it can provide a window into teaching and learning as well as driving the development of new tools for improving instruction. We introduce the largest dataset of mathematics classroom transcripts available to researchers, and demonstrate how this data can help improve instruction. The dataset consists of 1,660 45-60 minute long 4th and 5th grade elementary mathematics observations collected by the National Center for Teacher Effectiveness (NCTE) between 2010-2013. The anonymized transcripts represent data from 317 teachers across 4 school districts that serve largely historically marginalized students. The transcripts come with rich metadata, including turn-level annotations for dialogic discourse moves, classroom observation scores, demographic information, survey responses and student test scores. We demonstrate that our natural language processing model, trained on our turn-level annotations, can learn to identify dialogic discourse moves and these moves are correlated with better classroom observation scores and learning outcomes. This dataset opens up several possibilities for researchers, educators and policymakers to learn about and improve K-12 instruction. The dataset can be found at <url>https://github.com/ddemszky/classroom-transcript-analysis</url>.</abstract>
      <url hash="6ad3811c">2023.bea-1.44</url>
      <bibkey>demszky-hill-2023-ncte</bibkey>
      <doi>10.18653/v1/2023.bea-1.44</doi>
    </paper>
    <paper id="45">
      <title>Auto-req: Automatic detection of pre-requisite dependencies between academic videos</title>
      <author><first>Rushil</first><last>Thareja</last><affiliation>Extramarks Education Pvt. Ltd.</affiliation></author>
      <author><first>Ritik</first><last>Garg</last><affiliation>Extramarks Education Pvt. Ltd.</affiliation></author>
      <author><first>Shiva</first><last>Baghel</last><affiliation>Extramarks</affiliation></author>
      <author><first>Deep</first><last>Dwivedi</last><affiliation>Mr</affiliation></author>
      <author><first>Mukesh</first><last>Mohania</last><affiliation>Extramarks Education Pvt. Ltd.</affiliation></author>
      <author><first>Ritvik</first><last>Kulshrestha</last><affiliation>Extramarks Education Pvt. Ltd.</affiliation></author>
      <pages>539-549</pages>
      <abstract>Online learning platforms offer a wealth of educational material, but as the amount of content on these platforms grows, students may struggle to determine the most efficient order in which to cover the material to achieve a particular learning objective. In this paper, we propose a feature-based method for identifying pre-requisite dependencies between academic videos. Our approach involves using a transcript engine with a language model to transcribe domain-specific terms and then extracting novel similarity-based features to determine pre-requisite dependencies between video transcripts. This approach succeeds due to the development of a novel corpus of K-12 academic text, which was created using a proposed feature-based document parser. We evaluate our method on hand-annotated datasets for transcript extraction, video pre-requisites determination, and textbook parsing, which we have released. Our method for pre-requisite edge determination shows significant improvement (+4.7%-10.24% F1-score) compared to existing methods.</abstract>
      <url hash="503d2e56">2023.bea-1.45</url>
      <bibkey>thareja-etal-2023-auto</bibkey>
      <doi>10.18653/v1/2023.bea-1.45</doi>
      <video href="2023.bea-1.45.mp4"/>
    </paper>
    <paper id="46">
      <title>Transformer-based <fixed-case>H</fixed-case>ebrew <fixed-case>NLP</fixed-case> models for Short Answer Scoring in Biology</title>
      <author><first>Abigail</first><last>Gurin Schleifer</last><affiliation>Weizmann Institute of Science</affiliation></author>
      <author><first>Beata</first><last>Beigman Klebanov</last><affiliation>Educational Testing Service, Princeton, NJ, USA</affiliation></author>
      <author><first>Moriah</first><last>Ariely</last><affiliation>Weizmann Institute of Science, Rehovot, Israel</affiliation></author>
      <author><first>Giora</first><last>Alexandron</last><affiliation>Weizmann Institute of Science, Rehovot, Israel</affiliation></author>
      <pages>550-555</pages>
      <abstract>Pre-trained large language models (PLMs) are adaptable to a wide range of downstream tasks by fine-tuning their rich contextual embeddings to the task, often without requiring much task-specific data. In this paper, we explore the use of a recently developed Hebrew PLM aleph-BERT for automated short answer grading of high school biology items. We show that the alephBERT-based system outperforms a strong CNN-based baseline, and that it general-izes unexpectedly well in a zero-shot paradigm to items on an unseen topic that address the same underlying biological concepts, opening up the possibility of automatically assessing new items without item-specific fine-tuning.</abstract>
      <url hash="5777cc62">2023.bea-1.46</url>
      <bibkey>gurin-schleifer-etal-2023-transformer</bibkey>
      <doi>10.18653/v1/2023.bea-1.46</doi>
    </paper>
    <paper id="47">
      <title>Comparing Neural Question Generation Architectures for Reading Comprehension</title>
      <author><first>E. Margaret</first><last>Perkoff</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Abhidip</first><last>Bhattacharyya</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Jon</first><last>Cai</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Jie</first><last>Cao</last><affiliation>University of Colorado</affiliation></author>
      <pages>556-566</pages>
      <abstract>In recent decades, there has been a significant push to leverage technology to aid both teachers and students in the classroom. Language processing advancements have been harnessed to provide better tutoring services, automated feedback to teachers, improved peer-to-peer feedback mechanisms, and measures of student comprehension for reading. Automated question generation systems have the potential to significantly reduce teachers’ workload in the latter. In this paper, we compare three differ- ent neural architectures for question generation across two types of reading material: narratives and textbooks. For each architecture, we explore the benefits of including question attributes in the input representation. Our models show that a T5 architecture has the best overall performance, with a RougeL score of 0.536 on a narrative corpus and 0.316 on a textbook corpus. We break down the results by attribute and discover that the attribute can improve the quality of some types of generated questions, including Action and Character, but this is not true for all models.</abstract>
      <url hash="4f8609c0">2023.bea-1.47</url>
      <bibkey>perkoff-etal-2023-comparing</bibkey>
      <doi>10.18653/v1/2023.bea-1.47</doi>
    </paper>
    <paper id="48">
      <title>A dynamic model of lexical experience for tracking of oral reading fluency</title>
      <author><first>Beata</first><last>Beigman Klebanov</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Michael</first><last>Suhan</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Zuowei</first><last>Wang</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Tenaha</first><last>O’reilly</last><affiliation>Educational Testing Service</affiliation></author>
      <pages>567-575</pages>
      <abstract>We present research aimed at solving a problem in assessment of oral reading fluency using children’s oral reading data from our online book reading app. It is known that properties of the passage being read aloud impact fluency estimates; therefore, passage-based measures are used to remove passage-related variance when estimating growth in oral reading fluency. However, passage-based measures reported in the literature tend to treat passages as independent events, without explicitly modeling accumulation of lexical experience as one reads through a book. We propose such a model and show that it helps explain additional variance in the measurements of children’s fluency as they read through a book, improving over a strong baseline. These results have implications for measuring growth in oral reading fluency.</abstract>
      <url hash="a4c79b80">2023.bea-1.48</url>
      <bibkey>beigman-klebanov-etal-2023-dynamic</bibkey>
      <doi>10.18653/v1/2023.bea-1.48</doi>
    </paper>
    <paper id="49">
      <title>Rating Short <fixed-case>L</fixed-case>2 Essays on the <fixed-case>CEFR</fixed-case> Scale with <fixed-case>GPT</fixed-case>-4</title>
      <author><first>Kevin P.</first><last>Yancey</last><affiliation>Duolingo</affiliation></author>
      <author><first>Geoffrey</first><last>Laflair</last><affiliation>Duolingo</affiliation></author>
      <author><first>Anthony</first><last>Verardi</last><affiliation>Duolingo</affiliation></author>
      <author><first>Jill</first><last>Burstein</last><affiliation>Duolingo</affiliation></author>
      <pages>576-584</pages>
      <abstract>Essay scoring is a critical task used to evaluate second-language (L2) writing proficiency on high-stakes language assessments. While automated scoring approaches are mature and have been around for decades, human scoring is still considered the gold standard, despite its high costs and well-known issues such as human rater fatigue and bias. The recent introduction of large language models (LLMs) brings new opportunities for automated scoring. In this paper, we evaluate how well GPT-3.5 and GPT-4 can rate short essay responses written by L2 English learners on a high-stakes language assessment, computing inter-rater agreement with human ratings. Results show that when calibration examples are provided, GPT-4 can perform almost as well as modern Automatic Writing Evaluation (AWE) methods, but agreement with human ratings can vary depending on the test-taker’s first language (L1).</abstract>
      <url hash="af0c7e25">2023.bea-1.49</url>
      <bibkey>yancey-etal-2023-rating</bibkey>
      <doi>10.18653/v1/2023.bea-1.49</doi>
    </paper>
    <paper id="50">
      <title>Towards automatically extracting morphosyntactical error patterns from <fixed-case>L</fixed-case>1-<fixed-case>L</fixed-case>2 parallel dependency treebanks</title>
      <author><first>Arianna</first><last>Masciolini</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Elena</first><last>Volodina</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Dana</first><last>Dannlls</last><affiliation>University of Gothenburg</affiliation></author>
      <pages>585-597</pages>
      <abstract>L1-L2 parallel dependency treebanks are UD-annotated corpora of learner sentences paired with correction hypotheses. Automatic morphosyntactical annotation has the potential to remove the need for explicit manual error tagging and improve interoperability, but makes it more challenging to locate grammatical errors in the resulting datasets. We therefore propose a novel method for automatically extracting morphosyntactical error patterns and perform a preliminary bilingual evaluation of its first implementation through a similar example retrieval task. The resulting pipeline is also available as a prototype CALL application.</abstract>
      <url hash="ccda2c2a">2023.bea-1.50</url>
      <attachment type="attachment" hash="79f28776">2023.bea-1.50.attachment.zip</attachment>
      <bibkey>masciolini-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.bea-1.50</doi>
      <video href="2023.bea-1.50.mp4"/>
    </paper>
    <paper id="51">
      <title>Learning from Partially Annotated Data: Example-aware Creation of Gap-filling Exercises for Language Learning</title>
      <author><first>Semere Kiros</first><last>Bitew</last><affiliation>Ghent University - imec, IDLab</affiliation></author>
      <author><first>Johannes</first><last>Deleu</last><affiliation>Ghent University - imec</affiliation></author>
      <author><first>A. Seza</first><last>Doğruöz</last><affiliation>Universiteit Gent</affiliation></author>
      <author><first>Chris</first><last>Develder</last><affiliation>Ghent University</affiliation></author>
      <author><first>Thomas</first><last>Demeester</last><affiliation>Ghent University - imec</affiliation></author>
      <pages>598-609</pages>
      <abstract>Since performing exercises (including, e.g.,practice tests) forms a crucial component oflearning, and creating such exercises requiresnon-trivial effort from the teacher. There is agreat value in automatic exercise generationin digital tools in education. In this paper, weparticularly focus on automatic creation of gap-filling exercises for language learning, specifi-cally grammar exercises. Since providing anyannotation in this domain requires human ex-pert effort, we aim to avoid it entirely and ex-plore the task of converting existing texts intonew gap-filling exercises, purely based on anexample exercise, without explicit instructionor detailed annotation of the intended gram-mar topics. We contribute (i) a novel neuralnetwork architecture specifically designed foraforementioned gap-filling exercise generationtask, and (ii) a real-world benchmark datasetfor French grammar. We show that our modelfor this French grammar gap-filling exercisegeneration outperforms a competitive baselineclassifier by 8% in F1 percentage points, achiev-ing an average F1 score of 82%. Our model im-plementation and the dataset are made publiclyavailable to foster future research, thus offeringa standardized evaluation and baseline solutionof the proposed partially annotated data predic-tion task in grammar exercise creation.</abstract>
      <url hash="05c875de">2023.bea-1.51</url>
      <bibkey>bitew-etal-2023-learning</bibkey>
      <doi>10.18653/v1/2023.bea-1.51</doi>
      <video href="2023.bea-1.51.mp4"/>
    </paper>
    <paper id="52">
      <title>Evaluating Reading Comprehension Exercises Generated by <fixed-case>LLM</fixed-case>s: A Showcase of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> in Education Applications</title>
      <author><first>Changrong</first><last>Xiao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Sean Xin</first><last>Xu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Kunpeng</first><last>Zhang</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Yufang</first><last>Wang</last><affiliation>Beijing Xicheng Educational Research Institute</affiliation></author>
      <author><first>Lei</first><last>Xia</last><affiliation>Shawn Tech</affiliation></author>
      <pages>610-625</pages>
      <abstract>The recent advancement of pre-trained Large Language Models (LLMs), such as OpenAI’s ChatGPT, has led to transformative changes across fields. For example, developing intelligent systems in the educational sector that leverage the linguistic capabilities of LLMs demonstrates a visible potential. Though researchers have recently explored how ChatGPT could possibly assist in student learning, few studies have applied these techniques to real-world classroom settings involving teachers and students. In this study, we implement a reading comprehension exercise generation system that provides high-quality and personalized reading materials for middle school English learners in China. Extensive evaluations of the generated reading passages and corresponding exercise questions, conducted both automatically and manually, demonstrate that the system-generated materials are suitable for students and even surpass the quality of existing human-written ones. By incorporating first-hand feedback and suggestions from experienced educators, this study serves as a meaningful pioneering application of ChatGPT, shedding light on the future design and implementation of LLM-based systems in the educational context.</abstract>
      <url hash="a53151bc">2023.bea-1.52</url>
      <bibkey>xiao-etal-2023-evaluating</bibkey>
      <doi>10.18653/v1/2023.bea-1.52</doi>
      <video href="2023.bea-1.52.mp4"/>
    </paper>
    <paper id="53">
      <title>Is <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> a Good Teacher Coach? Measuring Zero-Shot Performance For Scoring and Providing Actionable Insights on Classroom Instruction</title>
      <author><first>Rose</first><last>Wang</last><affiliation>Stanford</affiliation></author>
      <author><first>Dorottya</first><last>Demszky</last><affiliation>Stanford University</affiliation></author>
      <pages>626-667</pages>
      <abstract>Coaching, which involves classroom observation and expert feedback, is a widespread and fundamental part of teacher training. However, the majority of teachers do not have access to consistent, high quality coaching due to limited resources and access to expertise. We explore whether generative AI could become a cost-effective complement to expert feedback by serving as an automated teacher coach. In doing so, we propose three teacher coaching tasks for generative AI: (A) scoring transcript segments based on classroom observation instruments, (B)identifying highlights and missed opportunities for good instructional strategies, and (C) providing actionable suggestions for eliciting more student reasoning. We recruit expert math teachers to evaluate the zero-shot performance of ChatGPT on each of these tasks for elementary math classroom transcripts. Our results reveal that ChatGPT generates responses that are relevant to improving instruction, but they are often not novel or insightful. For example, 82% of the model’s suggestions point to places in the transcript where the teacher is already implementing that suggestion. Our work highlights the challenges of producing insightful, novel and truthful feedback for teachers while paving the way for future research to address these obstacles and improve the capacity of generative AI to coach teachers.</abstract>
      <url hash="342d1177">2023.bea-1.53</url>
      <bibkey>wang-demszky-2023-chatgpt</bibkey>
      <doi>10.18653/v1/2023.bea-1.53</doi>
      <video href="2023.bea-1.53.mp4"/>
    </paper>
    <paper id="54">
      <title>Does <fixed-case>BERT</fixed-case> Exacerbate Gender or <fixed-case>L</fixed-case>1 Biases in Automated <fixed-case>E</fixed-case>nglish Speaking Assessment?</title>
      <author><first>Alexander</first><last>Kwako</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Yixin</first><last>Wan</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Jieyu</first><last>Zhao</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Mark</first><last>Hansen</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>Ucla</affiliation></author>
      <author><first>Li</first><last>Cai</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>668-681</pages>
      <abstract>In English speaking assessment, pretrained large language models (LLMs) such as BERT can score constructed response items as accurately as human raters. Less research has investigated whether LLMs perpetuate or exacerbate biases, which would pose problems for the fairness and validity of the test. This study examines gender and native language (L1) biases in human and automated scores, using an off-the-shelf (OOS) BERT model. Analyses focus on a specific type of bias known as differential item functioning (DIF), which compares examinees of similar English language proficiency. Results show that there is a moderate amount of DIF, based on examinees’ L1 background in grade band 912. DIF is higher when scored by an OOS BERT model, indicating that BERT may exacerbate this bias; however, in practical terms, the degree to which BERT exacerbates DIF is very small. Additionally, there is more DIF for longer speaking items and for older examinees, but BERT does not exacerbate these patterns of DIF.</abstract>
      <url hash="034b953c">2023.bea-1.54</url>
      <bibkey>kwako-etal-2023-bert</bibkey>
      <doi>10.18653/v1/2023.bea-1.54</doi>
      <video href="2023.bea-1.54.mp4"/>
    </paper>
    <paper id="55">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>QG</fixed-case>-<fixed-case>TI</fixed-case>: Towards Question Generation from Multi-modal Sources</title>
      <author><first>Zichao</first><last>Wang</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Richard</first><last>Baraniuk</last><affiliation>Rice University</affiliation></author>
      <pages>682-691</pages>
      <abstract>We study the new problem of automatic question generation (QG) from multi-modal sources containing images and texts, significantly expanding the scope of most of the existing work that focuses exclusively on QG from only textual sources. We propose a simple solution for our new problem, called MultiQG-TI, which enables a text-only question generator to process visual input in addition to textual input. Specifically, we leverage an image-to-text model and an optical character recognition model to obtain the textual description of the image and extract any texts in the image, respectively, and then feed them together with the input texts to the question generator. We only fine-tune the question generator while keeping the other components fixed. On the challenging ScienceQA dataset, we demonstrate that MultiQG-TI significantly outperforms ChatGPT with few-shot prompting, despite having hundred-times less trainable parameters. Additional analyses empirically confirm the necessity of both visual and textual signals for QG and show the impact of various modeling choices. Code is available at <url>https://anonymous.4open.science/r/multimodal-QG-47F2/</url></abstract>
      <url hash="d3e47911">2023.bea-1.55</url>
      <bibkey>wang-baraniuk-2023-multiqg</bibkey>
      <doi>10.18653/v1/2023.bea-1.55</doi>
    </paper>
    <paper id="56">
      <title>Inspecting Spoken Language Understanding from Kids for Basic Math Learning at Home</title>
      <author><first>Eda</first><last>Okur</last><affiliation>Intel Labs</affiliation></author>
      <author><first>Roddy</first><last>Fuentes Alba</last><affiliation>Intel Labs</affiliation></author>
      <author><first>Saurav</first><last>Sahay</last><affiliation>Intel Labs</affiliation></author>
      <author><first>Lama</first><last>Nachman</last><affiliation>Intel Labs</affiliation></author>
      <pages>692-708</pages>
      <abstract>Enriching the quality of early childhood education with interactive math learning at home systems, empowered by recent advances in conversational AI technologies, is slowly becoming a reality. With this motivation, we implement a multimodal dialogue system to support play-based learning experiences at home, guiding kids to master basic math concepts. This work explores Spoken Language Understanding (SLU) pipeline within a task-oriented dialogue system developed for Kid Space, with cascading Automatic Speech Recognition (ASR) and Natural Language Understanding (NLU) components evaluated on our home deployment data with kids going through gamified math learning activities. We validate the advantages of a multi-task architecture for NLU and experiment with a diverse set of pretrained language representations for Intent Recognition and Entity Extraction tasks in the math learning domain. To recognize kids’ speech in realistic home environments, we investigate several ASR systems, including the commercial Google Cloud and the latest open-source Whisper solutions with varying model sizes. We evaluate the SLU pipeline by testing our best-performing NLU models on noisy ASR output to inspect the challenges of understanding children for math learning in authentic homes.</abstract>
      <url hash="237cc878">2023.bea-1.56</url>
      <bibkey>okur-etal-2023-inspecting</bibkey>
      <doi>10.18653/v1/2023.bea-1.56</doi>
    </paper>
    <paper id="57">
      <title>Socratic Questioning of Novice Debuggers: A Benchmark Dataset and Preliminary Evaluations</title>
      <author><first>Erfan</first><last>Al-Hossami</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <author><first>Razvan</first><last>Bunescu</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <author><first>Ryan</first><last>Teehan</last><affiliation>New York University</affiliation></author>
      <author><first>Laurel</first><last>Powell</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <author><first>Khyati</first><last>Mahajan</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <author><first>Mohsen</first><last>Dorodchi</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <pages>709-726</pages>
      <abstract>Socratic questioning is a teaching strategy where the student is guided towards solving a problem on their own, instead of being given the solution directly. In this paper, we introduce a dataset of Socratic conversations where an instructor helps a novice programmer fix buggy solutions to simple computational problems. The dataset is then used for benchmarking the Socratic debugging abilities of GPT-based language models. While GPT-4 is observed to perform much better than GPT-3.5, its precision, and recall still fall short of human expert abilities, motivating further work in this area.</abstract>
      <url hash="fcd3cf50">2023.bea-1.57</url>
      <attachment type="attachment" hash="9d07cedd">2023.bea-1.57.attachment.zip</attachment>
      <bibkey>al-hossami-etal-2023-socratic</bibkey>
      <doi>10.18653/v1/2023.bea-1.57</doi>
    </paper>
    <paper id="58">
      <title>Beyond Black Box <fixed-case>AI</fixed-case> generated Plagiarism Detection: From Sentence to Document Level</title>
      <author><first>Ali</first><last>Quidwai</last><affiliation>New York University</affiliation></author>
      <author><first>Chunhui</first><last>Li</last><affiliation>Columbia University</affiliation></author>
      <author><first>Parijat</first><last>Dube</last><affiliation>Ibm</affiliation></author>
      <pages>727-735</pages>
      <abstract>The increasing reliance on large language models (LLMs) in academic writing has led to a rise in plagiarism. Existing AI-generated text classifiers have limited accuracy and often produce false positives. We propose a novel approach using natural language processing (NLP) techniques, offering quantifiable metrics at both sentence and document levels for easier interpretation by human evaluators. Our method employs a multi-faceted approach, generating multiple paraphrased versions of a given question and inputting them into the LLM to generate answers. By using a contrastive loss function based on cosine similarity, we match generated sentences with those from the student’s response. Our approach achieves up to 94% accuracy in classifying human and AI text, providing a robust and adaptable solution for plagiarism detection in academic settings. This method improves with LLM advancements, reducing the need for new model training or reconfiguration, and offers a more transparent way of evaluating and detecting AI-generated text.</abstract>
      <url hash="0668773a">2023.bea-1.58</url>
      <bibkey>quidwai-etal-2023-beyond</bibkey>
      <doi>10.18653/v1/2023.bea-1.58</doi>
    </paper>
    <paper id="59">
      <title>Enhancing Educational Dialogues: A Reinforcement Learning Approach for Generating <fixed-case>AI</fixed-case> Teacher Responses</title>
      <author><first>Thomas</first><last>Huber</last><affiliation>University of St. Gallen</affiliation></author>
      <author><first>Christina</first><last>Niklaus</last><affiliation>University of St. Gallen</affiliation></author>
      <author><first>Siegfried</first><last>Handschuh</last><affiliation>University of St. Gallen</affiliation></author>
      <pages>736-744</pages>
      <abstract>Reinforcement Learning remains an underutilized method of training and fine-tuning Language Models (LMs) despite recent successes. This paper presents a simple approach of fine-tuning a language model with Reinforcement Learning to achieve competitive performance on the BEA 2023 Shared Task whose goal is to automatically generate teacher responses in educational dialogues. We utilized the novel NLPO algorithm that masks out tokens during generation to direct the model towards generations that maximize a reward function. We show results for both the t5-base model with 220 million parameters from the HuggingFace repository submitted to the leaderboard that, despite its comparatively small size, has achieved a good performance on both test and dev set, as well as GPT-2 with 124 million parameters. The presented results show that despite maximizing only one of the metrics used in the evaluation as a reward function our model scores highly in the other metrics as well.</abstract>
      <url hash="6b08d90c">2023.bea-1.59</url>
      <bibkey>huber-etal-2023-enhancing</bibkey>
      <doi>10.18653/v1/2023.bea-1.59</doi>
      <video href="2023.bea-1.59.mp4"/>
    </paper>
    <paper id="60">
      <title>Assessing the efficacy of large language models in generating accurate teacher responses</title>
      <author><first>Yann</first><last>Hicke</last><affiliation>Cornell University</affiliation></author>
      <author><first>Abhishek</first><last>Masand</last><affiliation>Cornell University</affiliation></author>
      <author><first>Wentao</first><last>Guo</last><affiliation>Cornell University</affiliation></author>
      <author><first>Tushaar</first><last>Gangavarapu</last><affiliation>Cornell University</affiliation></author>
      <pages>745-755</pages>
      <abstract>(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT. We hypothesize that several dataset characteristics, including sampling, representativeness, and dialog completeness, pose significant challenges to fine-tuning, thus contributing to the poor generalizability of the fine-tuned models. Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model’s ability to showcase pedagogical skills.</abstract>
      <url hash="d5e9a58a">2023.bea-1.60</url>
      <bibkey>hicke-etal-2023-assessing</bibkey>
      <doi>10.18653/v1/2023.bea-1.60</doi>
    </paper>
    <paper id="61">
      <title><fixed-case>RETUYT</fixed-case>-<fixed-case>I</fixed-case>n<fixed-case>C</fixed-case>o at <fixed-case>BEA</fixed-case> 2023 Shared Task: Tuning Open-Source <fixed-case>LLM</fixed-case>s for Generating Teacher Responses</title>
      <author><first>Alexis</first><last>Baladón</last><affiliation>Instituto de Computación, Facultad de Ingeniería, Universidad de la República</affiliation></author>
      <author><first>Ignacio</first><last>Sastre</last><affiliation>Instituto de Computación, Facultad de Ingeniería, Universidad de la República</affiliation></author>
      <author><first>Luis</first><last>Chiruzzo</last><affiliation>Instituto de Computación, Facultad de Ingeniería, Universidad de la República</affiliation></author>
      <author><first>Aiala</first><last>Rosá</last><affiliation>Instituto de Computación, Facultad de Ingeniería, Universidad de la República</affiliation></author>
      <pages>756-765</pages>
      <abstract>This paper presents the results of our participation in the BEA 2023 shared task, which focuses on generating AI teacher responses in educational dialogues. We conducted experiments using several Open-Source Large Language Models (LLMs) and explored fine-tuning techniques along with prompting strategies, including Few-Shot and Chain-of-Thought approaches. Our best model was ranked 4.5 in the competition with a BertScore F1 of 0.71 and a DialogRPT final (avg) of 0.35. Nevertheless, our internal results did not exactly correlate with those obtained in the competition, which showed the difficulty in evaluating this task. Other challenges we faced were data leakage on the train set and the irregular format of the conversations.</abstract>
      <url hash="2f7940a1">2023.bea-1.61</url>
      <bibkey>baladon-etal-2023-retuyt</bibkey>
      <doi>10.18653/v1/2023.bea-1.61</doi>
      <video href="2023.bea-1.61.mp4"/>
    </paper>
    <paper id="62">
      <title>Empowering Conversational Agents using Semantic In-Context Learning</title>
      <author><first>Amin</first><last>Omidvar</last><affiliation>PhD student at the Department of Electrical Engineering and Computer Science, York University</affiliation></author>
      <author><first>Aijun</first><last>An</last><affiliation>York University</affiliation></author>
      <pages>766-771</pages>
      <abstract>Language models are one of the biggest game changers in downstream NLP applications, especially in conversational agents. In spite of their awesome capabilities to generated responses to solve the inquireis, there are still some big challenges to using them. One challenge is how to enable the LLMs to use the private internal data to solve inquires. And secondly, how to keep the LLMs updated with newly incoming data without the burden of fine-tuning as it is not only expensive but also not an available option for some commercial LLMs, such as ChatGPT. In this work, we propose Semantic In-Context Learning (S-ICL) to address the aforementioned challenges. Our approach was participated in the BEA 2023 shared task and ended up having the fourth place in both development and evaluation phases.</abstract>
      <url hash="868b7333">2023.bea-1.62</url>
      <bibkey>omidvar-an-2023-empowering</bibkey>
      <doi>10.18653/v1/2023.bea-1.62</doi>
    </paper>
    <paper id="63">
      <title><fixed-case>NAIST</fixed-case>eacher: A Prompt and Rerank Approach to Generating Teacher Utterances in Educational Dialogues</title>
      <author><first>Justin</first><last>Vasselli</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Christopher</first><last>Vasselli</last><affiliation>Serpenti Sei</affiliation></author>
      <author><first>Adam</first><last>Nohejl</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>772-784</pages>
      <abstract>This paper presents our approach to the BEA 2023 shared task of generating teacher responses in educational dialogues, using the Teacher-Student Chatroom Corpus. Our system prompts GPT-3.5-turbo to generate initial suggestions, which are then subjected to reranking. We explore multiple strategies for candidate generation, including prompting for multiple candidates and employing iterative few-shot prompts with negative examples. We aggregate all candidate responses and rerank them based on DialogRPT scores. To handle consecutive turns in the dialogue data, we divide the task of generating teacher utterances into two components: teacher replies to the student and teacher continuations of previously sent messages. Through our proposed methodology, our system achieved the top score on both automated metrics and human evaluation, surpassing the reference human teachers on the latter.</abstract>
      <url hash="97fb71f7">2023.bea-1.63</url>
      <bibkey>vasselli-etal-2023-naisteacher</bibkey>
      <doi>10.18653/v1/2023.bea-1.63</doi>
    </paper>
    <paper id="64">
      <title>The <fixed-case>BEA</fixed-case> 2023 Shared Task on Generating <fixed-case>AI</fixed-case> Teacher Responses in Educational Dialogues</title>
      <author><first>Anaïs</first><last>Tack</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Ekaterina</first><last>Kochmar</last><affiliation>Mbzuai</affiliation></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>King’s College London</affiliation></author>
      <author><first>Serge</first><last>Bibauw</last><affiliation>Universidad Central del Ecuador; UCLouvain</affiliation></author>
      <author><first>Chris</first><last>Piech</last><affiliation>Stanford University</affiliation></author>
      <pages>785-795</pages>
      <abstract>This paper describes the results of the first shared task on generation of teacher responses in educational dialogues. The goal of the task was to benchmark the ability of generative language models to act as AI teachers, replying to a student in a teacher-student dialogue. Eight teams participated in the competition hosted on CodaLab and experimented with a wide variety of state-of-the-art models, including Alpaca, Bloom, DialoGPT, DistilGPT-2, Flan-T5, GPT- 2, GPT-3, GPT-4, LLaMA, OPT-2.7B, and T5- base. Their submissions were automatically scored using BERTScore and DialogRPT metrics, and the top three among them were further manually evaluated in terms of pedagogical ability based on Tack and Piech (2022). The NAISTeacher system, which ranked first in both automated and human evaluation, generated responses with GPT-3.5 Turbo using an ensemble of prompts and DialogRPT-based ranking of responses for given dialogue contexts. Despite promising achievements of the participating teams, the results also highlight the need for evaluation metrics better suited to educational contexts.</abstract>
      <url hash="94ff1fc6">2023.bea-1.64</url>
      <bibkey>tack-etal-2023-bea</bibkey>
      <doi>10.18653/v1/2023.bea-1.64</doi>
    </paper>
    <paper id="65">
      <title>The <fixed-case>ADAIO</fixed-case> System at the <fixed-case>BEA</fixed-case>-2023 Shared Task: Shared Task Generating <fixed-case>AI</fixed-case> Teacher Responses in Educational Dialogues</title>
      <author><first>Adaeze</first><last>Adigwe</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>Istituto Italiano di Tecnologia, Universit di Ferrara, Italy</affiliation></author>
      <pages>796-804</pages>
      <abstract>This paper presents the ADAIO team’s system entry in the Building Educational Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. The task aims to assess the performance of state-of-the-art generative models as AI teachers in producing suitable responses within a student-teacher dialogue. Our system comprises evaluating various baseline models using OpenAI GPT-3 and designing diverse prompts to prompt the OpenAI models for teacher response generation. After the challenge, our system achieved second place by employing a few-shot prompt-based approach with the OpenAI text-davinci-003 model. The results highlight the few-shot learning capabilities of large-language models, particularly OpenAI’s GPT-3, in the role of AI teachers.</abstract>
      <url hash="52bfbf16">2023.bea-1.65</url>
      <bibkey>adigwe-yuan-2023-adaio</bibkey>
      <doi>10.18653/v1/2023.bea-1.65</doi>
    </paper>
  </volume>
</collection>
