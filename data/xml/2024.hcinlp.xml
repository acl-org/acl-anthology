<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.hcinlp">
  <volume id="1" ingest-date="2024-06-20" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Bridging Human--Computer Interaction and Natural Language Processing</booktitle>
      <editor><first>Su Lin</first><last>Blodgett</last></editor>
      <editor><first>Amanda</first><last>Cercas Curry</last></editor>
      <editor><first>Sunipa</first><last>Dev</last></editor>
      <editor><first>Michael</first><last>Madaio</last></editor>
      <editor><first>Ani</first><last>Nenkova</last></editor>
      <editor><first>Diyi</first><last>Yang</last></editor>
      <editor><first>Ziang</first><last>Xiao</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2024</year>
      <url hash="540f718e">2024.hcinlp-1</url>
      <venue>hcinlp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="ebde982b">2024.hcinlp-1.0</url>
      <bibkey>hcinlp-ws-2024-bridging</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Examining Prosody in Spoken Navigation Instructions for People with Disabilities</title>
      <author><first>Cathy</first><last>Jiao</last></author>
      <author><first>Aaron</first><last>Steinfeld</last><affiliation>Carnegie-Mellon University</affiliation></author>
      <author><first>Maxine</first><last>Eskenazi</last><affiliation>Carnegie-Mellon University</affiliation></author>
      <pages>1-12</pages>
      <abstract>The introduction of conversational systems have made synthesized speech technologies common tools for daily activities. However, not all synthetic speech systems are designed with the needs of people with disabilities in mind. This paper describes a study in which 198 people – 80 participants with self-reported disabilities and 118 participants without – were recruited to listen to navigation instructions from a spoken dialogue system with different prosodic features. Results showed that slowing down speech rate aids in participants’ number recall, but not in noun recall. From our results, we provide suggestions for developers for building accessible synthetic speech systems.</abstract>
      <url hash="494fd861">2024.hcinlp-1.1</url>
      <bibkey>jiao-etal-2024-examining</bibkey>
      <doi>10.18653/v1/2024.hcinlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Properties and Challenges of <fixed-case>LLM</fixed-case>-Generated Explanations</title>
      <author><first>Jenny</first><last>Kunz</last><affiliation>Linköping University</affiliation></author>
      <author><first>Marco</first><last>Kuhlmann</last><affiliation>Linköping University</affiliation></author>
      <pages>13-27</pages>
      <abstract>The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task-specific data sets.However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs.The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning.As the pre-training corpus includes a large amount of human-written explanations “in the wild”, we hypothesise that LLMs adopt common properties of human explanations.By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading.We discuss reasons and consequences of the properties’ presence or absence. In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system.</abstract>
      <url hash="fe3fab54">2024.hcinlp-1.2</url>
      <bibkey>kunz-kuhlmann-2024-properties</bibkey>
      <doi>10.18653/v1/2024.hcinlp-1.2</doi>
    </paper>
    <paper id="3">
      <title>This Reference Does Not Exist: An Exploration of <fixed-case>LLM</fixed-case> Citation Accuracy and Relevance</title>
      <author><first>Courtni</first><last>Byun</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>Piper</first><last>Vasicek</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>Kevin</first><last>Seppi</last><affiliation>Brigham Young University</affiliation></author>
      <pages>28-39</pages>
      <abstract>Citations are a fundamental and indispensable part of research writing. They provide support and lend credibility to research findings. Recent GPT-fueled interest in large language models (LLMs) has shone a spotlight on the capabilities and limitations of these models when generating relevant citations for a document. Recent work has focused largely on title and author accuracy. We underline this effort and expand on it with a preliminary exploration in relevance of model-recommended citations. We define three citation-recommendation tasks. We also collect and annotate a dataset of model-recommended citations for those tasks. We find that GPT-4 largely outperforms earlier models on both author and title accuracy in two markedly different CS venues, but may not recommend references that are more relevant than those recommended by the earlier models. The two venues we compare are CHI and EMNLP. All models appear to perform better at recommending EMNLP papers than CHI papers.</abstract>
      <url hash="dc649ce5">2024.hcinlp-1.3</url>
      <bibkey>byun-etal-2024-reference</bibkey>
      <doi>10.18653/v1/2024.hcinlp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Combining Multiple Metrics for Evaluating Retrieval-Augmented Conversations</title>
      <author><first>Jason Ingyu</first><last>Choi</last><affiliation>Amazon</affiliation></author>
      <author><first>Marcus</first><last>Collins</last><affiliation>Amazon</affiliation></author>
      <author><first>Eugene</first><last>Agichtein</last><affiliation>Amazon and Emory University</affiliation></author>
      <author><first>Oleg</first><last>Rokhlenko</last></author>
      <author><first>Shervin</first><last>Malmasi</last><affiliation>Amazon</affiliation></author>
      <pages>40-50</pages>
      <abstract>Conversational AI is a subtype of Human Computer Interaction that has gained wide adoption. These systems are typically powered by Large Language Models (LLMs) that use Retrieval Augmented Generation (RAG) to infuse external knowledge, which is effective against issues like hallucination. However, automatically evaluating retrieval augmented conversations with minimal human effort remains challenging, particularly in online settings. We address this challenge by proposing a lexical metric, and a novel method for combining it with other metrics, including semantic models. Our approach involves: (1) Conversational Information Utility (CIU), a new automated metric inspired by prior user studies on web search evaluation, to compute information overlap between conversation context and grounded information in an unsupervised, purely lexical way; and (2) a generalized reward model through Mixture-of-Experts (MoE-CIU) that dynamically ensembles CIU with other metrics, including learned ones, into a single reward. Evaluation against human ratings on two public datasets (Topical Chat and Persona Chat) shows that CIU improves correlation against human judgments by 2.0% and 0.9% respectively compared to the second best metric. When MoE is applied to combine lexical and learned semantic metrics, correlations further improve by 9.9% and 5.0%, suggesting that unified reward models are a promising approach.</abstract>
      <url hash="b024e70e">2024.hcinlp-1.4</url>
      <bibkey>choi-etal-2024-combining</bibkey>
      <doi>10.18653/v1/2024.hcinlp-1.4</doi>
    </paper>
    <paper id="5">
      <title>How Much Annotation is Needed to Compare Summarization Models?</title>
      <author><first>Chantal</first><last>Shaib</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Joe</first><last>Barrow</last><affiliation>Pattern Data</affiliation></author>
      <author><first>Alexa</first><last>Siu</last><affiliation>Adobe</affiliation></author>
      <author><first>Byron</first><last>Wallace</last><affiliation>Northeastern University, Brown University and Northeastern University</affiliation></author>
      <author><first>Ani</first><last>Nenkova</last><affiliation>Adobe Research</affiliation></author>
      <pages>51-59</pages>
      <abstract>Modern instruction-tuned models have become highly capable in text generation tasks such as summarization, and are expected to be released at a steady pace. In practice one may now wish to choose confidently, but with minimal effort, the best performing summarization model when applied to a new domain or purpose. In this work, we empirically investigate the test sample size necessary to select a preferred model in the context of news summarization. Empirical results reveal that comparative evaluation converges quickly for both automatic and human evaluation, with clear preferences for a system emerging from under 100 examples. The human preference data allows us to quantify how well automatic scores can reproduce preference rankings across a variety of downstream summarization tasks. We find that, while automatic metrics are stable at smaller sample sizes, only some automatic metrics are able to moderately predict model win rates according to human preference.</abstract>
      <url hash="4eb65b06">2024.hcinlp-1.5</url>
      <bibkey>shaib-etal-2024-much</bibkey>
      <doi>10.18653/v1/2024.hcinlp-1.5</doi>
    </paper>
    <paper id="6">
      <title>An Interactive Co-Pilot for Accelerated Research Ideation</title>
      <author><first>Harshit</first><last>Nigam</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Manasi</first><last>Patwardhan</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <author><first>Gautam</first><last>Shroff</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <pages>60-73</pages>
      <abstract>In the realm of research support tools, there exists a notable void in resources tailored specifically for aiding researchers during the crucial ideation phase of the research life-cycle. We address this gap by introducing ‘Acceleron’, a ‘Co-Pilot’ for researchers, designed specifically to accelerate the ideation phase of the research life-cycle. Leveraging the reasoning and domain-specific skills of Large Language Models (LLMs) within an agent-based architecture with distinct personas, Acceleron aids researchers through the formulation of a comprehensive research proposals. It emulates the ideation process, engaging researchers in an interactive fashion to validate the novelty of the proposal and generate plausible set-of hypotheses. Notably, it addresses challenges inherent in LLMs, such as hallucinations, implements a two-stage aspect-based retrieval to manage precision-recall trade-offs, and tackles issues of unanswerability. Our observations and end-user evaluations illustrate the efficacy of Acceleron as an enhancer of researcher’s productivity.</abstract>
      <url hash="16679004">2024.hcinlp-1.6</url>
      <bibkey>nigam-etal-2024-interactive</bibkey>
      <doi>10.18653/v1/2024.hcinlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Sensemaking of Socially-Mediated Crisis Information</title>
      <author><first>Vrushali</first><last>Koli</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <author><first>Jun</first><last>Yuan</last></author>
      <author><first>Aritra</first><last>Dasgupta</last><affiliation>New Jersey Institute of Technology and New Jersey Institute of Technology</affiliation></author>
      <pages>74-81</pages>
      <abstract>In times of crisis, the human mind is often a voracious information forager. It might not be immediately apparent what one wants or needs, and people frequently look for answers to their most pressing questions and worst fears. In that context, the pandemic has demonstrated that social media sources, like erstwhile Twitter, are a rich medium for data-driven communication between experts and the public.However, as lay users, we must find needles in a haystack to distinguish credible and actionable information signals from the noise. In this work, we leverage the literature on crisis communication to propose an AI-driven sensemaking model that bridges the gap between what people seek and what they need during a crisis. Our model learns to contrast social media messages concerning expert guidance with subjective opinion and enables semantic interpretation of message characteristics based on the communicative intent of the message author. We provide examples from our tweet collection and present a hypothetical social media usage scenario to demonstrate the efficacy of our proposed model.</abstract>
      <url hash="cc687798">2024.hcinlp-1.7</url>
      <bibkey>koli-etal-2024-sensemaking</bibkey>
      <doi>10.18653/v1/2024.hcinlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Blind Spots and Biases: Exploring the Role of Annotator Cognitive Biases in <fixed-case>NLP</fixed-case></title>
      <author><first>Sanjana</first><last>Gautam</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Mukund</first><last>Srinath</last></author>
      <pages>82-88</pages>
      <abstract>With the rapid proliferation of artificial intelligence, there is growing concern over its potential to exacerbate existing biases and societal disparities and introduce novel ones. This issue has prompted widespread attention from academia, policymakers, industry, and civil society. While evidence suggests that integrating human perspectives can mitigate bias-related issues in AI systems, it also introduces challenges associated with cognitive biases inherent in human decision-making. Our research focuses on reviewing existing methodologies and ongoing investigations aimed at understanding annotation attributes that contribute to bias.</abstract>
      <url hash="2c0ebbef">2024.hcinlp-1.8</url>
      <bibkey>gautam-srinath-2024-blind</bibkey>
      <doi>10.18653/v1/2024.hcinlp-1.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>LLMC</fixed-case>heckup: Conversational Examination of Large Language Models via Interpretability Tools and Self-Explanations</title>
      <author><first>Qianli</first><last>Wang</last></author>
      <author><first>Tatiana</first><last>Anikina</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Nils</first><last>Feldhus</last><affiliation>German Research Center for AI and German Research Center for AI</affiliation></author>
      <author><first>Josef</first><last>Genabith</last><affiliation>German Research Center for AI and Universität des Saarlandes</affiliation></author>
      <author><first>Leonhard</first><last>Hennig</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>89-104</pages>
      <abstract>Interpretability tools that offer explanations in the form of a dialogue have demonstrated their efficacy in enhancing users’ understanding (Slack et al., 2023; Shen et al., 2023), as one-off explanations may fall short in providing sufficient information to the user. Current solutions for dialogue-based explanations, however, often require external tools and modules and are not easily transferable to tasks they were not designed for. With <tex-math>\texttt{LLMCheckup}</tex-math>, we present an easily accessible tool that allows users to chat with any state-of-the-art large language model (LLM) about its behavior. We enable LLMs to generate explanations and perform user intent recognition without fine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI) methods, including white-box explainability tools such as feature attributions, and self-explanations (e.g., for rationale generation). LLM-based (self-)explanations are presented as an interactive dialogue that supports follow-up questions and generates suggestions. <tex-math>\texttt{LLMCheckup}</tex-math> provides tutorials for operations available in the system, catering to individuals with varying levels of expertise in XAI and supporting multiple input modalities. We introduce a new parsing strategy that substantially enhances the user intent recognition accuracy of the LLM. Finally, we showcase <tex-math>\texttt{LLMCheckup}</tex-math> for the tasks of fact checking and commonsense question answering. Our code repository: https://github.com/DFKI-NLP/LLMCheckup</abstract>
      <url hash="28d017a6">2024.hcinlp-1.9</url>
      <bibkey>wang-etal-2024-llmcheckup</bibkey>
      <doi>10.18653/v1/2024.hcinlp-1.9</doi>
    </paper>
  </volume>
</collection>
