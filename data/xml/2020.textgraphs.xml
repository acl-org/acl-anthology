<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.textgraphs">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</booktitle>
      <editor><first>Dmitry</first><last>Ustalov</last></editor>
      <editor><first>Swapna</first><last>Somasundaran</last></editor>
      <editor><first>Alexander</first><last>Panchenko</last></editor>
      <editor><first>Fragkiskos D.</first><last>Malliaros</last></editor>
      <editor><first>Ioana</first><last>Hulpuș</last></editor>
      <editor><first>Peter</first><last>Jansen</last></editor>
      <editor><first>Abhik</first><last>Jana</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="00d04105">2020.textgraphs-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>A survey of embedding models of entities and relationships for knowledge graph completion</title>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <pages>1–14</pages>
      <abstract>Knowledge graphs (KGs) of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge graphs are typically incomplete, it is useful to perform knowledge graph completion or link prediction, i.e. predict whether a relationship not in the knowledge graph is likely to be true. This paper serves as a comprehensive survey of embedding models of entities and relationships for knowledge graph completion, summarizing up-to-date experimental results on standard benchmark datasets and pointing out potential future research directions.</abstract>
      <url hash="645a9fe7">2020.textgraphs-1.1</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c48b2a8f">2020.textgraphs-1.1.OptionalSupplementaryMaterial.pdf</attachment>
    </paper>
    <paper id="2">
      <title>Graph-based Aspect Representation Learning for Entity Resolution</title>
      <author><first>Zhenqi</first><last>Zhao</last></author>
      <author><first>Yuchen</first><last>Guo</last></author>
      <author><first>Dingxian</first><last>Wang</last></author>
      <author><first>Yufan</first><last>Huang</last></author>
      <author><first>Xiangnan</first><last>He</last></author>
      <author><first>Bin</first><last>Gu</last></author>
      <pages>15–23</pages>
      <abstract>Entity Resolution (ER) identifies records that refer to the same real-world entity. Deep learning approaches improved the generalization ability of entity matching models, but hardly overcame the impact of noisy or incomplete data sources. In real scenes, an entity usually consists of multiple semantic facets, called aspects. In this paper, we focus on entity augmentation, namely retrieving the values of missing aspects. The relationship between aspects is naturally suitable to be represented by a knowledge graph, where entity augmentation can be modeled as a link prediction problem. Our paper proposes a novel graph-based approach to solve entity augmentation. Specifically, we apply a dedicated random walk algorithm, which uses node types to limit the traversal length, and encodes graph structure into low-dimensional embeddings. Thus, the missing aspects could be retrieved by a link prediction model. Furthermore, the augmented aspects with fixed orders are served as the input of a deep Siamese BiLSTM network for entity matching. We compared our method with state-of-the-art methods through extensive experiments on downstream ER tasks. According to the experiment results, our model outperforms other methods on evaluation metrics (accuracy, precision, recall, and f1-score) to a large extent, which demonstrates the effectiveness of our method.</abstract>
      <url hash="4f526fa9">2020.textgraphs-1.2</url>
    </paper>
    <paper id="3">
      <title>Merge and Recognize: A Geometry and 2<fixed-case>D</fixed-case> Context Aware Graph Model for Named Entity Recognition from Visual Documents</title>
      <author><first>Chuwei</first><last>Luo</last></author>
      <author><first>Yongpan</first><last>Wang</last></author>
      <author><first>Qi</first><last>Zheng</last></author>
      <author><first>Liangchen</first><last>Li</last></author>
      <author><first>Feiyu</first><last>Gao</last></author>
      <author><first>Shiyu</first><last>Zhang</last></author>
      <pages>24–34</pages>
      <abstract>Named entity recognition (NER) from visual documents, such as invoices, receipts or business cards, is a critical task for visual document understanding. Most classical approaches use a sequence-based model (typically BiLSTM-CRF framework) without considering document structure. Recent work on graph-based model using graph convolutional networks to encode visual and textual features have achieved promising performance on the task. However, few attempts take geometry information of text segments (text in bounding box) in visual documents into account. Meanwhile, existing methods do not consider that related text segments which need to be merged to form a complete entity in many real-world situations. In this paper, we present GraphNEMR, a graph-based model that uses graph convolutional networks to jointly merge text segments and recognize named entities. By incorporating geometry information from visual documents into our model, richer 2D context information is generated to improve document representations. To merge text segments, we introduce a novel mechanism that captures both geometry information as well as semantic information based on pre-trained language model. Experimental results show that the proposed GraphNEMR model outperforms both sequence-based and graph-based SOTA methods significantly.</abstract>
      <url hash="49c0166b">2020.textgraphs-1.3</url>
    </paper>
    <paper id="4">
      <title>Joint Learning of the Graph and the Data Representation for Graph-Based Semi-Supervised Learning</title>
      <author><first>Mariana</first><last>Vargas-Vieyra</last></author>
      <author><first>Aurélien</first><last>Bellet</last></author>
      <author><first>Pascal</first><last>Denis</last></author>
      <pages>35–45</pages>
      <abstract>Graph-based semi-supervised learning is appealing when labels are scarce but large amounts of unlabeled data are available. These methods typically use a heuristic strategy to construct the graph based on some fixed data representation, independently of the available labels. In this pa- per, we propose to jointly learn a data representation and a graph from both labeled and unlabeled data such that (i) the learned representation indirectly encodes the label information injected into the graph, and (ii) the graph provides a smooth topology with respect to the transformed data. Plugging the resulting graph and representation into existing graph-based semi-supervised learn- ing algorithms like label spreading and graph convolutional networks, we show that our approach outperforms standard graph construction methods on both synthetic data and real datasets.</abstract>
      <url hash="3b978675">2020.textgraphs-1.4</url>
    </paper>
    <paper id="5">
      <title>Contextual <fixed-case>BERT</fixed-case>: Conditioning the Language Model Using a Global State</title>
      <author><first>Timo I.</first><last>Denk</last></author>
      <author><first>Ana</first><last>Peleteiro Ramallo</last></author>
      <pages>46–50</pages>
      <abstract>BERT is a popular language model whose main pre-training task is to fill in the blank, i.e., predicting a word that was masked out of a sentence, based on the remaining words. In some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account. This motivates us to advance the BERT architecture by adding a global state for conditioning on a fixed-sized context. We present our two novel approaches and apply them to an industry use-case, where we complete fashion outfits with missing articles, conditioned on a specific customer. An experimental comparison to other methods from the literature shows that our methods improve personalization significantly.</abstract>
      <url hash="20609025">2020.textgraphs-1.5</url>
    </paper>
    <paper id="6">
      <title>Semi-supervised Word Sense Disambiguation Using Example Similarity Graph</title>
      <author><first>Rie</first><last>Yatabe</last></author>
      <author><first>Minoru</first><last>Sasaki</last></author>
      <pages>51–59</pages>
      <abstract>Word Sense Disambiguation (WSD) is a well-known problem in the natural language processing. In recent years, there has been increasing interest in applying neural net-works and machine learning techniques to solve WSD problems. However, these previ-ous supervised approaches often suffer from the lack of manually sense-tagged exam-ples. In this paper, to solve these problems, we propose a semi-supervised WSD method using graph embeddings based learning method in order to make effective use of labeled and unlabeled examples. The results of the experiments show that the proposed method performs better than the previous semi-supervised WSD method. Moreover, the graph structure between examples is effective for WSD and it is effective to utilize a graph structure obtained by fine-tuning BERT in the proposed method.</abstract>
      <url hash="4ac50b34">2020.textgraphs-1.6</url>
    </paper>
    <paper id="7">
      <title>Incorporating Temporal Information in Entailment Graph Mining</title>
      <author><first>Liane</first><last>Guillou</last></author>
      <author><first>Sander</first><last>Bijl de Vroe</last></author>
      <author><first>Mohammad Javad</first><last>Hosseini</last></author>
      <author><first>Mark</first><last>Johnson</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>60–71</pages>
      <abstract>We present a novel method for injecting temporality into entailment graphs to address the problem of spurious entailments, which may arise from similar but temporally distinct events involving the same pair of entities. We focus on the sports domain in which the same pairs of teams play on different occasions, with different outcomes. We present an unsupervised model that aims to learn entailments such as win/lose → play, while avoiding the pitfall of learning non-entailments such as win ̸→ lose. We evaluate our model on a manually constructed dataset, showing that incorporating time intervals and applying a temporal window around them, are effective strategies.</abstract>
      <url hash="20e99db0">2020.textgraphs-1.7</url>
    </paper>
    <paper id="8">
      <title>Graph-based Syntactic Word Embeddings</title>
      <author><first>Ragheb</first><last>Al-Ghezi</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <pages>72–78</pages>
      <abstract>We propose a simple and efficient framework to learn syntactic embeddings based on information derived from constituency parse trees. Using biased random walk methods, our embeddings not only encode syntactic information about words, but they also capture contextual information. We also propose a method to train the embeddings on multiple constituency parse trees to ensure the encoding of global syntactic representation. Quantitative evaluation of the embeddings show a competitive performance on POS tagging task when compared to other types of embeddings, and qualitative evaluation reveals interesting facts about the syntactic typology learned by these embeddings.</abstract>
      <url hash="4fdf2252">2020.textgraphs-1.8</url>
    </paper>
    <paper id="9">
      <title>Relation Specific Transformations for Open World Knowledge Graph Completion</title>
      <author><first>Haseeb</first><last>Shah</last></author>
      <author><first>Johannes</first><last>Villmow</last></author>
      <author><first>Adrian</first><last>Ulges</last></author>
      <pages>79–84</pages>
      <abstract>We propose an open-world knowledge graph completion model that can be combined with common closed-world approaches (such as ComplEx) and enhance them to exploit text-based representations for entities unseen in training. Our model learns relation-specific transformation functions from text-based to graph-based embedding space, where the closed-world link prediction model can be applied. We demonstrate state-of-the-art results on common open-world benchmarks and show that our approach benefits from relation-specific transformation functions (RST), giving substantial improvements over a relation-agnostic approach.</abstract>
      <url hash="27ada043">2020.textgraphs-1.9</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c44e1ff3">2020.textgraphs-1.9.OptionalSupplementaryMaterial.pdf</attachment>
    </paper>
    <paper id="10">
      <title><fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs 2020 Shared Task on Multi-Hop Inference for Explanation Regeneration</title>
      <author><first>Peter</first><last>Jansen</last></author>
      <author><first>Dmitry</first><last>Ustalov</last></author>
      <pages>85–97</pages>
      <abstract>The 2020 Shared Task on Multi-Hop Inference for Explanation Regeneration tasks participants with regenerating large detailed multi-fact explanations for standardized science exam questions. Given a question, correct answer, and knowledge base, models must rank each fact in the knowledge base such that facts most likely to appear in the explanation are ranked highest. Explanations consist of an average of 6 (and as many as 16) facts that span both core scientific knowledge and world knowledge, and form an explicit lexically-connected “explanation graph” describing how the facts interrelate. In this second iteration of the explanation regeneration shared task, participants are supplied with more than double the training and evaluation data of the first shared task, as well as a knowledge base nearly double in size, both of which expand into more challenging scientific topics that increase the difficulty of the task. In total 10 teams participated, and 5 teams submitted system description papers. The best-performing teams significantly increased state-of-the-art performance both in terms of ranking (mean average precision) and inference speed on this challenge task.</abstract>
      <url hash="623be095">2020.textgraphs-1.10</url>
    </paper>
    <paper id="11">
      <title><fixed-case>PGL</fixed-case> at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs 2020 Shared Task: Explanation Regeneration using Language and Graph Learning Methods</title>
      <author><first>Weibin</first><last>Li</last></author>
      <author><first>Yuxiang</first><last>Lu</last></author>
      <author><first>Zhengjie</first><last>Huang</last></author>
      <author><first>Weiyue</first><last>Su</last></author>
      <author><first>Jiaxiang</first><last>Liu</last></author>
      <author><first>Shikun</first><last>Feng</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <pages>98–102</pages>
      <abstract>This paper describes the system designed by the Baidu PGL Team which achieved the first place in the TextGraphs 2020 Shared Task. The task focuses on generating explanations for elementary science questions. Given a question and its corresponding correct answer, we are asked to select the facts that can explain why the answer is correct for the question and answering (QA) from a large knowledge base. To address this problem, we use a pre-trained language model to recall the top-K relevant explanations for each question. Then, we adopt a re-ranking approach based on a pre-trained language model to rank the candidate explanations. To further improve the rankings, we also develop an architecture consisting both powerful pre-trained transformers and GNNs to tackle the multi-hop inference problem. The official evaluation shows that, our system can outperform the second best system by 1.91 points.</abstract>
      <url hash="7c7fd439">2020.textgraphs-1.11</url>
    </paper>
    <paper id="12">
      <title><fixed-case>C</fixed-case>hi<fixed-case>S</fixed-case>quare<fixed-case>X</fixed-case> at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs 2020 Shared Task: Leveraging Pretrained Language Models for Explanation Regeneration</title>
      <author><first>Aditya Girish</first><last>Pawate</last></author>
      <author><first>Varun</first><last>Madhavan</last></author>
      <author><first>Devansh</first><last>Chandak</last></author>
      <pages>103–108</pages>
      <abstract>In this work, we describe the system developed by a group of undergraduates from the Indian Institutes of Technology for the Shared Task at TextGraphs-14 on Multi-Hop Inference Explanation Regeneration (Jansen and Ustalov, 2020). The shared task required participants to develop methods to reconstruct gold explanations for elementary science questions from the WorldTreeCorpus (Xie et al., 2020). Although our research was not funded by any organization and all the models were trained on freely available tools like Google Colab, which restricted our computational capabilities, we have managed to achieve noteworthy results, placing ourselves in 4th place with a MAPscore of 0.49021in the evaluation leaderboard and 0.5062 MAPscore on the post-evaluation-phase leaderboard using RoBERTa. We incorporated some of the methods proposed in the previous edition of Textgraphs-13 (Chia et al., 2019), which proved to be very effective, improved upon them, and built a model on top of it using powerful state-of-the-art pre-trained language models like RoBERTa (Liu et al., 2019), BART (Lewis et al., 2020), SciB-ERT (Beltagy et al., 2019) among others. Further optimization of our work can be done with the availability of better computational resources.</abstract>
      <url hash="e27a25fc">2020.textgraphs-1.12</url>
      <attachment type="OptionalSupplementaryMaterial" hash="f835c6a3">2020.textgraphs-1.12.OptionalSupplementaryMaterial.pdf</attachment>
    </paper>
    <paper id="13">
      <title>Explanation Regeneration via Multi-Hop <fixed-case>ILP</fixed-case> Inference over Knowledge Base</title>
      <author><first>Aayushee</first><last>Gupta</last></author>
      <author><first>Gopalakrishnan</first><last>Srinivasaraghavan</last></author>
      <pages>109–114</pages>
      <abstract>Textgraphs 2020 Workshop organized a shared task on ‘Explanation Regeneration’ that required reconstructing gold explanations for elementary science questions. This work describes our submission to the task which is based on multiple components: a BERT baseline ranking, an Integer Linear Program (ILP) based re-scoring and a regression model for re-ranking the explanation facts. Our system achieved a Mean Average Precision score of 0.3659.</abstract>
      <url hash="41fbef76">2020.textgraphs-1.13</url>
      <attachment type="OptionalSupplementaryMaterial" hash="20936250">2020.textgraphs-1.13.OptionalSupplementaryMaterial.pdf</attachment>
    </paper>
    <paper id="14">
      <title>Red Dragon <fixed-case>AI</fixed-case> at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs 2020 Shared Task : <fixed-case>LIT</fixed-case> : <fixed-case>LSTM</fixed-case>-Interleaved Transformer for Multi-Hop Explanation Ranking</title>
      <author><first>Yew Ken</first><last>Chia</last></author>
      <author><first>Sam</first><last>Witteveen</last></author>
      <author><first>Martin</first><last>Andrews</last></author>
      <pages>115–120</pages>
      <abstract>Explainable question answering for science questions is a challenging task that requires multi-hop inference over a large set of fact sentences. To counter the limitations of methods that view each query-document pair in isolation, we propose the LSTM-Interleaved Transformer which incorporates cross-document interactions for improved multi-hop ranking. The LIT architecture can leverage prior ranking positions in the re-ranking setting. Our model is competitive on the current leaderboard for the TextGraphs 2020 shared task, achieving a test-set MAP of 0.5607, and would have gained third place had we submitted before the competition deadline. Our code implementation is made available at [https://github.com/mdda/worldtree_corpus/tree/textgraphs_2020](https://github.com/mdda/worldtree_corpus/tree/textgraphs_2020).</abstract>
      <url hash="b46e1515">2020.textgraphs-1.14</url>
    </paper>
  </volume>
</collection>
