<?xml version='1.0' encoding='UTF-8'?>
<collection id="2008.jeptalnrecital">
  <volume id="long" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 15ème conférence sur le Traitement Automatique des Langues Naturelles. Articles longs</booktitle>
      <editor><first>Frédéric</first><last>Béchet</last></editor>
      <editor><first>Jean-Francois</first><last>Bonastre</last></editor>
      <publisher>ATALA</publisher>
      <address>Avignon, France</address>
      <month>June</month>
      <year>2008</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="454d6715">2008.jeptalnrecital-long.0</url>
      <bibkey>jep-taln-recital-2008-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The Mitkov algorithm for anaphora resolution in <fixed-case>P</fixed-case>ortuguese</title>
      <author><first>Amanda</first><last>Rocha-Chaves</last></author>
      <author><first>Lucia-Helena</first><last>Machado-Rino</last></author>
      <pages>1–10</pages>
      <abstract>This paper reports on the use of the Mitkov ́s algorithm for pronoun resolution in texts written in Brazilian Portuguese. Third person pronouns are the only ones focused upon here, with noun phrases as antecedents. A system for anaphora resolution in Brazilian Portuguese texts was built that embeds most of the Mitkov’s features. Some of his resolution factors were directly incorporated into the system; others had to be slightly modified for language adequacy. The resulting approach was intrinsically evaluated on hand-annotated corpora. It was also compared to Lappin &amp; Leass’s algorithm for pronoun resolution, also customized to Portuguese. Success rate was the evaluation measure used in both experiments. The results of both evaluations are discussed here.</abstract>
      <url hash="f32eb5cc">2008.jeptalnrecital-long.1</url>
      <bibkey>rocha-chaves-machado-rino-2008-mitkov</bibkey>
    </paper>
    <paper id="2">
      <title>Réécriture et Détection d’Implication Textuelle</title>
      <author><first>Paul</first><last>Bédaride</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>11–20</pages>
      <abstract>Nous présentons un système de normalisation de la variation syntaxique qui permet de mieux reconnaître la relation d’implication textuelle entre deux phrases. Le système est évalué sur une suite de tests comportant 2 520 paires test et les résultats montrent un gain en précision par rapport à un système de base variant entre 29.8 et 78.5 points la complexité des cas considérés.</abstract>
      <url hash="e96c190a">2008.jeptalnrecital-long.2</url>
      <language>fra</language>
      <bibkey>bedaride-gardent-2008-reecriture</bibkey>
    </paper>
    <paper id="3">
      <title>Représentation algébrique des expressions calendaires et vue calendaire d’un texte</title>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Javier</first><last>Couto</last></author>
      <author><first>Jean-Luc</first><last>Minel</last></author>
      <author><first>Sylviane</first><last>R. Schwer</last></author>
      <pages>21–30</pages>
      <abstract>Cet article aborde l’étude des expressions temporelles qui font référence directement à des unités de temps relatives aux divisions courantes des calendriers, que nous qualifions d’expressions calendaires (EC). Nous proposons une modélisation de ces expressions en définissant une algèbre d’opérateurs qui sont liés aux classes de marqueurs linguistiques qui apparaissent dans les EC. A partir de notre modélisation, une vue calendaire est construite dans la plate-forme de visualisation et navigation textuelle NaviTexte, visant le support à la lecture de textes. Enfin, nous concluons sur les perspectives offertes par le développement d’une première application de navigation temporelle.</abstract>
      <url hash="440cdace">2008.jeptalnrecital-long.3</url>
      <language>fra</language>
      <bibkey>battistelli-etal-2008-representation</bibkey>
    </paper>
    <paper id="4">
      <title>Annotation d’expressions temporelles et d’événements en français</title>
      <author><first>Gabriel</first><last>Parent</last></author>
      <author><first>Michel</first><last>Gagnon</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <pages>31–40</pages>
      <abstract>Dans cet article, nous proposons une méthode pour identifier, dans un texte en français, l’ensemble des expressions adverbiales de localisation temporelle, ainsi que tous les verbes, noms et adjectifs dénotant une éventualité (événement ou état). Cette méthode, en plus d’identifier ces expressions, extrait certaines informations sémantiques : la valeur de la localisation temporelle selon la norme TimeML et le type des éventualités. Pour les expressions adverbiales de localisation temporelle, nous utilisons une cascade d’automates, alors que pour l’identification des événements et états nous avons recours à une analyse complète de la phrase. Nos résultats sont proches de travaux comparables sur l’anglais, en l’absence d’évaluation quantitative similaire sur le français.</abstract>
      <url hash="e0b76752">2008.jeptalnrecital-long.4</url>
      <language>fra</language>
      <bibkey>parent-etal-2008-annotation</bibkey>
    </paper>
    <paper id="5">
      <title>Un modèle multi-sources pour la segmentation en sujets de journaux radiophoniques</title>
      <author><first>Stéphane</first><last>Huet</last></author>
      <author><first>Guillaume</first><last>Gravier</last></author>
      <author><first>Pascale</first><last>Sébillot</last></author>
      <pages>41–50</pages>
      <abstract>Nous présentons une méthode de segmentation de journaux radiophoniques en sujets, basée sur la prise en compte d’indices lexicaux, syntaxiques et acoustiques. Partant d’un modèle statistique existant de segmentation thématique, exploitant la notion de cohésion lexicale, nous étendons le formalisme pour y inclure des informations d’ordre syntaxique et acoustique. Les résultats expérimentaux montrent que le seul modèle de cohésion lexicale ne suffit pas pour le type de documents étudié en raison de la taille variable des segments et de l’absence d’un lien direct entre segment et thème. L’utilisation d’informations syntaxiques et acoustiques permet une amélioration substantielle de la segmentation obtenue.</abstract>
      <url hash="7a201cf6">2008.jeptalnrecital-long.5</url>
      <language>fra</language>
      <bibkey>huet-etal-2008-un</bibkey>
    </paper>
    <paper id="6">
      <title>Extraction automatique d’informations à partir de micro-textes non structurés</title>
      <author><first>Cédric</first><last>Vidrequin</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <author><first>Jean-Jacques</first><last>Schneider</last></author>
      <author><first>Marc</first><last>El-Bèze</last></author>
      <pages>51–60</pages>
      <abstract>Nous présentons dans cet article une méthode d’extraction automatique d’informations sur des textes de très petite taille, faiblement structurés. Nous travaillons sur des textes dont la rédaction n’est pas normalisée, avec très peu de mots pour caractériser chaque information. Les textes ne contiennent pas ou très peu de phrases. Il s’agit le plus souvent de morceaux de phrases ou d’expressions composées de quelques mots. Nous comparons plusieurs méthodes d’extraction, dont certaines sont entièrement automatiques. D’autres utilisent en partie une connaissance du domaine que nous voulons réduite au minimum, de façon à minimiser le travail manuel en amont. Enfin, nous présentons nos résultats qui dépassent ce dont il est fait état dans la littérature, avec une précision équivalente et un rappel supérieur.</abstract>
      <url hash="4d5fe223">2008.jeptalnrecital-long.6</url>
      <language>fra</language>
      <bibkey>vidrequin-etal-2008-extraction</bibkey>
    </paper>
    <paper id="7">
      <title>Quelles combinaisons de scores et de critères numériques pour un système de Questions/Réponses ?</title>
      <author><first>Laurent</first><last>Gillard</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <author><first>Marc</first><last>El-Bèze</last></author>
      <pages>61–70</pages>
      <abstract>Dans cet article, nous présentons une discussion sur la combinaison de différents scores et critères numériques pour la sélection finale d’une réponse dans la partie en charge des questions factuelles du système de Questions/Réponses développé au LIA. Ces scores et critères numériques sont dérivés de ceux obtenus en sortie de deux composants cruciaux pour notre système : celui de sélection des passages susceptibles de contenir une réponse et celui d’extraction et de sélection d’une réponse. Ils sont étudiés au regard de leur expressivité. Des comparaisons sont faites avec des approches de sélection de passages mettant en oeuvre des scores conventionnels en recherche d’information. Parallèlement, l’influence de la taille des contextes (en nombre de phrases) est évaluée. Cela permet de mettre en évidence que le choix de passages constitués de trois phrases autour d’une réponse candidate, avec une sélection des réponses basée sur une combinaison entre un score de passage de type Lucene ou Cosine et d’un score de compacité apparaît comme un compromis intéressant.</abstract>
      <url hash="c14d9d8c">2008.jeptalnrecital-long.7</url>
      <language>fra</language>
      <bibkey>gillard-etal-2008-quelles</bibkey>
    </paper>
    <paper id="8">
      <title>Contrôle rhétorique de la génération des connecteurs concessifs en dialogue homme-machine</title>
      <author><first>Vladimir</first><last>Popescu</last></author>
      <author><first>Jean</first><last>Caelen</last></author>
      <pages>71–80</pages>
      <abstract>Les connecteurs discursifs ont on rôle important dans l’interprétation des discours (dialogiques ou pas), donc lorsqu’il s’agit de produire des énoncés, le choix des mots qui relient les énoncés (par exemple, en dialogue oral) s’avère essentiel pour assurer la compréhension des visées illocutoires des locuteurs. En linguistique computationnelle, le problème a été abordé surtout au niveau de l’interprétation des discours monologiques, tandis que pour le dialogue, les recherches se sont limitées en général à établir une correspondance quasiment biunivoque entre relations rhétoriques et connecteurs. Dans ce papier nous proposons un mécanisme pour guider la génération des connecteurs concessifs en dialogue, à la fois du point de vue discursif et sémantique ; chaque connecteur considéré sera contraint par un ensemble de conditions qui prennent en compte la cohérence du discours et la pertinence sémantique de chaque mot concerné. Les contraintes discursives, exprimées dans un formalisme dérivé de la SDRT (« Segmented Discourse Representation Theory ») seront plongées dans des contraintes sémantiques sur les connecteurs, proposées par l’école genevoise (Moeschler), pour enfin évaluer la cohérence du discours résultant de l’emploi de ces connecteurs.</abstract>
      <url hash="a3edcd94">2008.jeptalnrecital-long.8</url>
      <language>fra</language>
      <bibkey>popescu-caelen-2008-controle</bibkey>
    </paper>
    <paper id="9">
      <title>Modélisation du principe d’ancrage pour la robustesse des systèmes de dialogue homme-machine finalisés</title>
      <author><first>Alexandre</first><last>Denis</last></author>
      <author><first>Matthieu</first><last>Quignard</last></author>
      <pages>81–90</pages>
      <abstract>Cet article présente une modélisation du principe d’ancrage (grounding) pour la robustesse des systèmes de dialogue finalisés. Ce principe, décrit dans (Clark &amp; Schaefer, 1989), suggère que les participants à un dialogue fournissent des preuves de compréhension afin d’atteindre la compréhension mutuelle. Nous explicitons une définition computationnelle du principe d’ancrage fondée sur des jugements de compréhension qui, contrairement à d’autres modèles, conserve une motivation pour l’expression de la compréhension. Nous déroulons enfin le processus d’ancrage sur un exemple tiré de l’implémentation du modèle.</abstract>
      <url hash="5d2ad37a">2008.jeptalnrecital-long.9</url>
      <language>fra</language>
      <bibkey>denis-quignard-2008-modelisation</bibkey>
    </paper>
    <paper id="10">
      <title>Enertex : un système basé sur l’énergie textuelle</title>
      <author><first>Silvia</first><last>Fernández</last></author>
      <author><first>Eric</first><last>Sanjuan</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <pages>91–100</pages>
      <abstract>Dans cet article, nous présentons des applications du système Enertex au Traitement Automatique de la Langue Naturelle. Enertex est basé sur l’énergie textuelle, une approche par réseaux de neurones inspirée de la physique statistique des systèmes magnétiques. Nous avons appliqué cette approche aux problèmes du résumé automatique multi-documents et de la détection de frontières thématiques. Les résultats, en trois langues : anglais, espagnol et français, sont très encourageants.</abstract>
      <url hash="207c4949">2008.jeptalnrecital-long.10</url>
      <language>fra</language>
      <bibkey>fernandez-etal-2008-enertex</bibkey>
    </paper>
    <paper id="11">
      <title>Intégration d’une étape de pré-filtrage et d’une fonction multiobjectif en vue d’améliorer le système <fixed-case>E</fixed-case>xtra<fixed-case>N</fixed-case>ews de résumé de documents multiples</title>
      <author><first>Fatma</first><last>Kallel Jaoua</last></author>
      <author><first>Lamia</first><last>Hadrich Belguith</last></author>
      <author><first>Maher</first><last>Jaoua</last></author>
      <author><first>Abdelmajid</first><last>Ben Hamadou</last></author>
      <pages>101–110</pages>
      <abstract>Dans cet article, nous présentons les améliorations que nous avons apportées au système ExtraNews de résumé automatique de documents multiples. Ce système se base sur l’utilisation d’un algorithme génétique qui permet de combiner les phrases des documents sources pour former les extraits, qui seront croisés et mutés pour générer de nouveaux extraits. La multiplicité des critères de sélection d’extraits nous a inspiré une première amélioration qui consiste à utiliser une technique d’optimisation multi-objectif en vue d’évaluer ces extraits. La deuxième amélioration consiste à intégrer une étape de pré-filtrage de phrases qui a pour objectif la réduction du nombre des phrases des textes sources en entrée. Une évaluation des améliorations apportées à notre système est réalisée sur les corpus de DUC’04 et DUC’07.</abstract>
      <url hash="3929fef0">2008.jeptalnrecital-long.11</url>
      <language>fra</language>
      <bibkey>kallel-jaoua-etal-2008-integration</bibkey>
    </paper>
    <paper id="12">
      <title>Recherche locale pour la traduction statistique à base de segments</title>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>Alexandre</first><last>Patry</last></author>
      <author><first>Fabrizio</first><last>Gotti</last></author>
      <pages>111–120</pages>
      <abstract>Dans cette étude, nous nous intéressons à des algorithmes de recherche locale pour la traduction statistique à base de segments (phrase-based machine translation). Les algorithmes que nous étudions s’appuient sur une formulation complète d’un état dans l’espace de recherche contrairement aux décodeurs couramment utilisés qui explorent l’espace des préfixes des traductions possibles. Nous montrons que la recherche locale seule, permet de produire des traductions proches en qualité de celles fournies par les décodeurs usuels, en un temps nettement inférieur et à un coût mémoire constant. Nous montrons également sur plusieurs directions de traduction qu’elle permet d’améliorer de manière significative les traductions produites par le système à l’état de l’art Pharaoh (Koehn, 2004).</abstract>
      <url hash="0358c0e0">2008.jeptalnrecital-long.12</url>
      <language>fra</language>
      <bibkey>langlais-etal-2008-recherche</bibkey>
    </paper>
    <paper id="13">
      <title>Transcrire les <fixed-case>SMS</fixed-case> comme on reconnaît la parole</title>
      <author><first>Catherine</first><last>Kobus</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <pages>121–130</pages>
      <abstract>Cet article présente une architecture inspirée des systèmes de reconnaissance vocale pour effectuer une normalisation orthographique de messages en « langage SMS ». Nous décrivons notre système de base, ainsi que diverses évolutions de ce système, qui permettent d’améliorer sensiblement la qualité des normalisations produites.</abstract>
      <url hash="1edfbede">2008.jeptalnrecital-long.13</url>
      <language>fra</language>
      <bibkey>kobus-etal-2008-transcrire</bibkey>
    </paper>
    <paper id="14">
      <title>Convertir des grammaires d’arbres adjoints à composantes multiples avec tuples d’arbres (<fixed-case>TT</fixed-case>-<fixed-case>MCTAG</fixed-case>) en grammaires à concaténation d’intervalles (<fixed-case>RCG</fixed-case>)</title>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Yannick</first><last>Parmentier</last></author>
      <pages>131–140</pages>
      <abstract>Cet article étudie la relation entre les grammaires d’arbres adjoints à composantes multiples avec tuples d’arbres (TT-MCTAG), un formalisme utilisé en linguistique informatique, et les grammaires à concaténation d’intervalles (RCG). Les RCGs sont connues pour décrire exactement la classe PTIME, il a en outre été démontré que les RCGs « simples » sont même équivalentes aux systèmes de réécriture hors-contextes linéaires (LCFRS), en d’autres termes, elles sont légèrement sensibles au contexte. TT-MCTAG a été proposé pour modéliser les langages à ordre des mots libre. En général ces langages sont NP-complets. Dans cet article, nous définissons une contrainte additionnelle sur les dérivations autorisées par le formalisme TT-MCTAG. Nous montrons ensuite comment cette forme restreinte de TT-MCTAG peut être convertie en une RCG simple équivalente. Le résultat est intéressant pour des raisons théoriques (puisqu’il montre que la forme restreinte de TT-MCTAG est légèrement sensible au contexte), mais également pour des raisons pratiques (la transformation proposée ici a été utilisée pour implanter un analyseur pour TT-MCTAG).</abstract>
      <url hash="cdbf5a10">2008.jeptalnrecital-long.14</url>
      <language>fra</language>
      <bibkey>kallmeyer-parmentier-2008-convertir</bibkey>
    </paper>
    <paper id="15">
      <title>Factorisation des contraintes syntaxiques dans un analyseur de dépendance</title>
      <author><first>Piet</first><last>Mertens</last></author>
      <pages>141–150</pages>
      <abstract>Cet article décrit un analyseur syntaxique pour grammaires de dépendance lexicalisées. Le formalisme syntaxique se caractérise par une factorisation des contraintes syntaxiques qui se manifeste dans la séparation entre dépendance et ordre linéaire, la spécification fonctionnelle (plutôt que syntagmatique) des dépendants, la distinction entre dépendants valenciels (la sous-catégorisation) et non valenciels (les circonstants) et la saturation progressive des arbres. Ceci résulte en une formulation concise de la grammaire à un niveau très abstrait et l’élimination de la reduplication redondante des informations due aux réalisations alternatives des dépendants ou à leur ordre. Les arbres élémentaires (obtenus à partir des formes dans l’entrée) et dérivés sont combinés entre eux par adjonction d’un arbre dépendant saturé à un arbre régissant, moyennant l’unification des noeuds et des relations. La dérivation est réalisée grâce à un analyseur chart bi-directionnel.</abstract>
      <url hash="4129d740">2008.jeptalnrecital-long.15</url>
      <language>fra</language>
      <bibkey>mertens-2008-factorisation</bibkey>
    </paper>
    <paper id="16">
      <title>Grammaires factorisées pour des dialectes apparentés</title>
      <author><first>Pascal</first><last>Vaillant</last></author>
      <pages>151–160</pages>
      <abstract>Pour la formalisation du lexique et de la grammaire de dialectes étroitement apparentés, il peut se révéler utile de factoriser une partie du travail de modélisation. Les soussystèmes linguistiques isomorphes dans les différents dialectes peuvent alors faire l’objet d’une description commune, les différences étant spécifiées par ailleurs. Cette démarche aboutit à un modèle de grammaire à couches : le noyau est commun à la famille de dialectes, et une couche superficielle détermine les caractéristiques de chacun. Nous appliquons ce procédé à la famille des langues créoles à base lexicale française de l’aire américano-caraïbe.</abstract>
      <url hash="e6796606">2008.jeptalnrecital-long.16</url>
      <language>fra</language>
      <bibkey>vaillant-2008-grammaires</bibkey>
    </paper>
    <paper id="17">
      <title>Expériences d’analyse syntaxique statistique du français</title>
      <author><first>Benoît</first><last>Crabbé</last></author>
      <author><first>Marie</first><last>Candito</last></author>
      <pages>161–170</pages>
      <abstract>Nous montrons qu’il est possible d’obtenir une analyse syntaxique statistique satisfaisante pour le français sur du corpus journalistique, à partir des données issues du French Treebank du laboratoire LLF, à l’aide d’un algorithme d’analyse non lexicalisé.</abstract>
      <url hash="4a830b3b">2008.jeptalnrecital-long.17</url>
      <language>fra</language>
      <bibkey>crabbe-candito-2008-experiences</bibkey>
    </paper>
    <paper id="18">
      <title>Construction d’un wordnet libre du français à partir de ressources multilingues</title>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Darja</first><last>Fišer</last></author>
      <pages>171–180</pages>
      <abstract>Cet article décrit la construction d’un Wordnet Libre du Français (WOLF) à partir du Princeton WordNet et de diverses ressources multilingues. Les lexèmes polysémiques ont été traités au moyen d’une approche reposant sur l’alignement en mots d’un corpus parallèle en cinq langues. Le lexique multilingue extrait a été désambiguïsé sémantiquement à l’aide des wordnets des langues concernées. Par ailleurs, une approche bilingue a été suffisante pour construire de nouvelles entrées à partir des lexèmes monosémiques. Nous avons pour cela extrait des lexiques bilingues à partir deWikipédia et de thésaurus. Le wordnet obtenu a été évalué par rapport au wordnet français issu du projet EuroWordNet. Les résultats sont encourageants, et des applications sont d’ores et déjà envisagées.</abstract>
      <url hash="7917581a">2008.jeptalnrecital-long.18</url>
      <language>fra</language>
      <bibkey>sagot-fiser-2008-construction</bibkey>
    </paper>
    <paper id="19">
      <title>Détermination des sens d’usage dans un réseau lexical construit à l’aide d’un jeu en ligne</title>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Alain</first><last>Joubert</last></author>
      <pages>181–191</pages>
      <abstract>Les informations lexicales, indispensables pour les tâches réalisées en TALN, sont difficiles à collecter. En effet, effectuée manuellement, cette tâche nécessite la compétence d’experts et la durée nécessaire peut être prohibitive, alors que réalisée automatiquement, les résultats peuvent être biaisés par les corpus de textes retenus. L’approche présentée ici consiste à faire participer un grand nombre de personnes à un projet contributif en leur proposant une application ludique accessible sur le web. A partir d’une base de termes préexistante, ce sont ainsi les joueurs qui vont construire le réseau lexical, en fournissant des associations qui ne sont validées que si elles sont proposées par au moins une paire d’utilisateurs. De plus, ces relations typées sont pondérées en fonction du nombre de paires d’utilisateurs qui les ont proposées. Enfin, nous abordons la question de la détermination des différents sens d’usage d’un terme, en analysant les relations entre ce terme et ses voisins immédiats dans le réseau lexical, avant de présenter brièvement la réalisation et les premiers résultats obtenus.</abstract>
      <url hash="243f6ef6">2008.jeptalnrecital-long.19</url>
      <language>fra</language>
      <bibkey>lafourcade-joubert-2008-determination</bibkey>
    </paper>
    <paper id="20">
      <title>Modélisation normalisée <fixed-case>LMF</fixed-case> des dictionnaires électroniques éditoriaux de l’arabe</title>
      <author><first>Feten</first><last>Baccar</last></author>
      <author><first>Aïda</first><last>Khemakhem</last></author>
      <author><first>Bilel</first><last>Gargouri</last></author>
      <author><first>Kais</first><last>Haddar</last></author>
      <author><first>Abdelmajid</first><last>Ben Hamadou</last></author>
      <pages>192–201</pages>
      <abstract>Le présent papier s’intéresse à l’élaboration des dictionnaires électroniques arabes à usage éditorial. Il propose un modèle unifié et normalisé de ces dictionnaires en se référant à la future norme LMF (Lexical Markup Framework) ISO 24613. Ce modèle permet de construire des dictionnaires extensibles, sur lesquels on peut réaliser, grâce à une structuration fine et standard, des fonctions de consultation génériques adaptées aux besoins des utilisateurs. La mise en oeuvre du modèle proposé est testée sur des dictionnaires existants de la langue arabe en utilisant, pour la consultation, le système ADIQTO (Arabic DIctionary Query TOols) que nous avons développé pour l’interrogation générique des dictionnaires normalisés de l’arabe.</abstract>
      <url hash="4d42c86f">2008.jeptalnrecital-long.20</url>
      <language>fra</language>
      <bibkey>baccar-etal-2008-modelisation</bibkey>
    </paper>
    <paper id="21">
      <title>La polysémie régulière dans <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Lucie</first><last>Barque</last></author>
      <author><first>François-Régis</first><last>Chaumartin</last></author>
      <pages>202–211</pages>
      <abstract>Cette étude propose une analyse et une modélisation des relations de polysémie dans le lexique électronique anglais WordNet. Elle exploite pour cela la hiérarchie des concepts (représentés par des synsets), et la définition associée à chacun de ces concepts. Le résultat est constitué d’un ensemble de règles qui nous ont permis d’identifier d’une façon largement automatisée, avec une précision voisine de 91%, plus de 2100 paires de synsets liés par une relation de polysémie régulière. Notre méthode permet aussi une désambiguïsation lexicale partielle des mots de la définition associée à ces synsets.</abstract>
      <url hash="1dbafdf9">2008.jeptalnrecital-long.21</url>
      <language>fra</language>
      <bibkey>barque-chaumartin-2008-la</bibkey>
    </paper>
    <paper id="22">
      <title>Une alternative aux modèles de traduction statistique d’<fixed-case>IBM</fixed-case>: Les triggers inter-langues</title>
      <author><first>Caroline</first><last>Lavecchia</last></author>
      <author><first>Kamel</first><last>Smaïli</last></author>
      <author><first>David</first><last>Langlois</last></author>
      <pages>212–221</pages>
      <abstract>Dans cet article, nous présentons une nouvelle approche pour la traduction automatique fondée sur les triggers inter-langues. Dans un premier temps, nous expliquons le concept de triggers inter-langues ainsi que la façon dont ils sont déterminés. Nous présentons ensuite les différentes expérimentations qui ont été menées à partir de ces triggers afin de les intégrer au mieux dans un processus complet de traduction automatique. Pour cela, nous construisons à partir des triggers inter-langues des tables de traduction suivant différentes méthodes. Nous comparons par la suite notre système de traduction fondé sur les triggers interlangues à un système état de l’art reposant sur le modèle 3 d’IBM (Brown &amp; al., 1993). Les tests menés ont montré que les traductions automatiques générées par notre système améliorent le score BLEU (Papineni &amp; al., 2001) de 2, 4% comparé à celles produites par le système état de l’art.</abstract>
      <url hash="83b72113">2008.jeptalnrecital-long.22</url>
      <language>fra</language>
      <bibkey>lavecchia-etal-2008-une</bibkey>
    </paper>
    <paper id="23">
      <title>Génération de reformulations locales par pivot pour l’aide à la révision</title>
      <author><first>Aurélien</first><last>Max</last></author>
      <pages>222–231</pages>
      <abstract>Cet article présente une approche pour obtenir des paraphrases pour de courts segments de texte qui peuvent aider un rédacteur à reformuler localement des textes. La ressource principale utilisée est une table d’alignements bilingues de segments d’un système de traduction automatique statistique. Un segment marqué par le rédacteur est tout d’abord traduit dans une langue pivot avant d’être traduit à nouveau dans la langue d’origine, ce qui est permis par la nature même de la ressource bilingue utilisée sans avoir recours à un processus de traduction complet. Le cadre proposé permet l’intégration et la combinaison de différents modèles d’estimation de la qualité des paraphrases. Des modèles linguistiques tentant de prendre en compte des caractéristiques des paraphrases de courts segments de textes sont proposés, et une évaluation est décrite et ses résultats analysés. Les domaines d’application possibles incluent, outre l’aide à la reformulation, le résumé et la réécriture des textes pour répondre à des conventions ou à des préférences stylistiques. L’approche est critiquée et des perspectives d’amélioration sont proposées.</abstract>
      <url hash="e0d06332">2008.jeptalnrecital-long.23</url>
      <language>fra</language>
      <bibkey>max-2008-generation</bibkey>
    </paper>
    <paper id="24">
      <title>Les architectures linguistiques et computationnelles en traduction automatique sont indépendantes</title>
      <author><first>Christian</first><last>Boitet</last></author>
      <pages>232–241</pages>
      <abstract>Contrairement à une idée répandue, les architectures linguistiques et computationnelles des systèmes de traduction automatique sont indépendantes. Les premières concernent le choix des représentations intermédiaires, les secondes le type d’algorithme, de programmation et de ressources utilisés. Il est ainsi possible d’utiliser des méthodes de calcul « expertes » ou « empiriques » pour construire diverses phases ou modules de systèmes d’architectures linguistiques variées. Nous terminons en donnant quelques éléments pour le choix de ces architectures en fonction des situations traductionnelles et des ressources disponibles, en termes de dictionnaires, de corpus, et de compétences humaines.</abstract>
      <url hash="ac8db0cb">2008.jeptalnrecital-long.24</url>
      <language>fra</language>
      <bibkey>boitet-2008-les</bibkey>
    </paper>
    <paper id="25">
      <title>Vérification sémantique pour l’annotation d’entités nommées</title>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>Caroline</first><last>Hagège</last></author>
      <pages>242–251</pages>
      <abstract>Dans cet article, nous proposons une méthode visant à corriger et à associer dynamiquement de nouveaux types sémantiques dans le cadre de systèmes de détection automatique d’entités nommées (EN). Après la détection des entités nommées et aussi de manière plus générale des noms propres dans les textes, une vérification de compatibilité de types sémantiques est effectuée non seulement pour confirmer ou corriger les résultats obtenus par le système de détection d’EN, mais aussi pour associer de nouveaux types non couverts par le système de détection d’EN. Cette vérification est effectuée en utilisant l’information syntaxique associée aux EN par un système d’analyse syntaxique robuste et en confrontant ces résultats avec la ressource sémantique WordNet. Les résultats du système de détection d’EN sont alors considérablement enrichis, ainsi que les étiquettes sémantiques associées aux EN, ce qui est particulièrement utile pour l’adaptation de systèmes de détection d’EN à de nouveaux domaines.</abstract>
      <url hash="b1477ff6">2008.jeptalnrecital-long.25</url>
      <language>fra</language>
      <bibkey>brun-hagege-2008-verification</bibkey>
    </paper>
    <paper id="26">
      <title>Exploitation de treillis de Galois en désambiguïsation non supervisée d’entités nommées</title>
      <author><first>Thomas</first><last>Girault</last></author>
      <pages>252–261</pages>
      <abstract>Nous présentons une méthode non supervisée de désambiguïsation d’entités nommées, basée sur l’exploitation des treillis de Galois. Nous réalisons une analyse de concepts formels à partir de relations entre des entités nommées et leurs contextes syntaxiques extraits d’un corpus d’apprentissage. Le treillis de Galois résultant fournit des concepts qui sont utilisés comme des étiquettes pour annoter les entités nommées et leurs contextes dans un corpus de test. Une évaluation en cascade montre qu’un système d’apprentissage supervisé améliore la classification des entités nommées lorsqu’il s’appuie sur l’annotation réalisée par notre système de désambiguïsation non supervisée.</abstract>
      <url hash="4dc05d57">2008.jeptalnrecital-long.26</url>
      <language>fra</language>
      <bibkey>girault-2008-exploitation</bibkey>
    </paper>
    <paper id="27">
      <title>Résolution de Métonymie des Entités Nommées : proposition d’une méthode hybride</title>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>Maud</first><last>Ehrmann</last></author>
      <author><first>Guillaume</first><last>Jacquet</last></author>
      <pages>262–271</pages>
      <abstract>Dans cet article, nous décrivons la méthode que nous avons développée pour la résolution de métonymie des entités nommées dans le cadre de la compétition SemEval 2007. Afin de résoudre les métonymies sur les noms de lieux et noms d’organisation, tel que requis pour cette tâche, nous avons mis au point un système hybride basé sur l’utilisation d’un analyseur syntaxique robuste combiné avec une méthode d’analyse distributionnelle. Nous décrivons cette méthode ainsi que les résultats obtenus par le système dans le cadre de la compétition SemEval 2007.</abstract>
      <url hash="4902428e">2008.jeptalnrecital-long.27</url>
      <language>fra</language>
      <bibkey>brun-etal-2008-resolution</bibkey>
    </paper>
    <paper id="28">
      <title>Etude de la corrélation entre morphosyntaxe et sémantique dans une perspective d’étiquetage automatique de textes médicaux arabes</title>
      <author><first>Tatiana</first><last>El-Khoury</last></author>
      <pages>272–281</pages>
      <abstract>Cet article se propose d’étudier les relations sémantiques reliant base et expansion au sein des termes médicaux arabes de type « N+N », particulièrement ceux dont la base est un déverbal. En étudiant les relations sémantiques établies par une base déverbale, ce travail tente d’attirer l’attention sur l’interpénétration du sémantique et du morphosyntaxique ; il montre que, dans une large mesure, la structure morphosyntaxique de la base détermine l’éventail des possibilités relationnelles. La découverte de régularités dans le comportement de la base déverbale permet de prédire le type de relations que peut établir cette base avec son expansion pavant ainsi la voie à un traitement automatique et un travail d’étiquetage sémantique des textes médicaux arabes.</abstract>
      <url hash="9842c191">2008.jeptalnrecital-long.28</url>
      <language>fra</language>
      <bibkey>el-khoury-2008-etude</bibkey>
    </paper>
    <paper id="29">
      <title>Influence de la qualité de l’étiquetage sur le chunking : une corrélation dépendant de la taille des chunks</title>
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Stéphane</first><last>Rauzy</last></author>
      <pages>282–291</pages>
      <abstract>Nous montrons dans cet article qu’il existe une corrélation étroite existant entre la qualité de l’étiquetage morpho-syntaxique et les performances des chunkers. Cette corrélation devient linéaire lorsque la taille des chunks est limitée. Nous appuyons notre démonstration sur la base d’une expérimentation conduite suite à la campagne d’évaluation Passage 2007 (de la Clergerie et al., 2008). Nous analysons pour cela les comportements de deux analyseurs ayant participé à cette campagne. L’interprétation des résultats montre que la tâche de chunking, lorsqu’elle vise des chunks courts, peut être assimilée à une tâche de “super-étiquetage”.</abstract>
      <url hash="ba89ff2f">2008.jeptalnrecital-long.29</url>
      <language>fra</language>
      <bibkey>blache-rauzy-2008-influence</bibkey>
    </paper>
    <paper id="30">
      <title>Apprentissage artificiel de règles d’indexation pour <fixed-case>MEDLINE</fixed-case></title>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>Vincent</first><last>Claveau</last></author>
      <pages>292–301</pages>
      <abstract>L’indexation est une composante importante de tout système de recherche d’information. Dans MEDLINE, la base documentaire de référence pour la littérature du domaine biomédical, le contenu des articles référencés est indexé à l’aide de descripteurs issus du thésaurus MeSH. Avec l’augmentation constante de publications à indexer pour maintenir la base à jour, le besoin d’outils automatiques se fait pressant pour les indexeurs. Dans cet article, nous décrivons l’utilisation et l’adaptation de la Programmation Logique Inductive (PLI) pour découvrir des règles d’indexation permettant de générer automatiquement des recommandations d’indexation pour MEDLINE. Les résultats obtenus par cette approche originale sont très satisfaisants comparés à ceux obtenus à l’aide de règles manuelles lorsque celles-ci existent. Ainsi, les jeux de règles obtenus par PLI devraient être prochainement intégrés au système produisant les recommandations d’indexation automatique pour MEDLINE.</abstract>
      <url hash="b2fd2042">2008.jeptalnrecital-long.30</url>
      <language>fra</language>
      <bibkey>neveol-claveau-2008-apprentissage</bibkey>
    </paper>
  </volume>
  <volume id="court" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 15ème conférence sur le Traitement Automatique des Langues Naturelles. Articles courts</booktitle>
      <editor><first>Frédéric</first><last>Béchet</last></editor>
      <editor><first>Jean-Francois</first><last>Bonastre</last></editor>
      <publisher>ATALA</publisher>
      <address>Avignon, France</address>
      <month>June</month>
      <year>2008</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="6be15c85">2008.jeptalnrecital-court.0</url>
      <bibkey>jep-taln-recital-2008-actes-de</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>Y</fixed-case> a-t-il une véritable équivalence entre les propositions syntaxiques du français et du japonais ?</title>
      <author><first>Yayoi</first><last>Nakamura-Delloye</last></author>
      <pages>1–10</pages>
      <abstract>La présente contribution part de nos constats réalisés à partir des résultats d’évaluation de notre système d’alignement des propositions de textes français-japonais. La présence importante de structures fondamentalement difficiles à aligner et les résultats peu satisfaisants de différentes méthodes de mise en correspondance des mots nous ont finalement amenés à remettre en cause l’existence même d’équivalence au niveau des propositions syntaxiques entre le français et le japonais. Afin de compenser les défauts que nous avons découverts, nous proposons des opérations permettant de restaurer l’équivalence des propositions alignées et d’améliorer la qualité des corpus alignés.</abstract>
      <url hash="81669fab">2008.jeptalnrecital-court.1</url>
      <language>fra</language>
      <bibkey>nakamura-delloye-2008-y</bibkey>
    </paper>
    <paper id="2">
      <title>Calculs d’unification sur les arbres de dérivation <fixed-case>TAG</fixed-case></title>
      <author><first>Sylvain</first><last>Schmitz</last></author>
      <author><first>Joseph</first><last>Le Roux</last></author>
      <pages>11–20</pages>
      <abstract>Nous définissons un formalisme, les grammaires rationnelles d’arbres avec traits, et une traduction des grammaires d’arbres adjoints avec traits vers ce nouveau formalisme. Cette traduction préserve les structures de dérivation de la grammaire d’origine en tenant compte de l’unification de traits. La construction peut être appliquée aux réalisateurs de surface qui se fondent sur les arbres de dérivation.</abstract>
      <url hash="8b16167d">2008.jeptalnrecital-court.2</url>
      <language>fra</language>
      <bibkey>schmitz-le-roux-2008-calculs</bibkey>
    </paper>
    <paper id="3">
      <title>Comparaison de méthodes lexicales et syntaxico-sémantiques dans la segmentation thématique de texte non supervisée</title>
      <author><first>Alexandre</first><last>Labadié</last></author>
      <author><first>Violaine</first><last>Prince</last></author>
      <pages>21–30</pages>
      <abstract>Cet article présente une méthode basée sur des calculs de distance et une analyse sémantique et syntaxique pour la segmentation thématique de texte. Pour évaluer cette méthode nous la comparons à un un algorithme lexical très connu : c99. Nous testons les deux méthodes sur un corpus de discours politique français et comparons les résultats. Les deux conclusions qui ressortent de notre expérience sont que les approches sont complémentaires et que les protocoles d’évaluation actuels sont inadaptés.</abstract>
      <url hash="4fb30432">2008.jeptalnrecital-court.3</url>
      <language>fra</language>
      <bibkey>labadie-prince-2008-comparaison</bibkey>
    </paper>
    <paper id="4">
      <title>Un modèle de langage pour le <fixed-case>DHM</fixed-case> : la Grammaire Sémantique Réversible</title>
      <author><first>Jérôme</first><last>Lehuen</last></author>
      <pages>31–40</pages>
      <abstract>Cet article propose un modèle de langage dédié au dialogue homme-machine, ainsi que des algorithmes d’analyse et de génération. L’originalité de notre approche est de faire reposer l’analyse et la génération sur les mêmes connaissances, essentiellement sémantiques. Celles-ci sont structurées sous la forme d’une bibliothèque de concepts, et de formes d’usage associées aux concepts. Les algorithmes, quant à eux, sont fondés sur un double principe de correspondance entre des offres et des attentes, et d’un calcul heuristique de score.</abstract>
      <url hash="6d0300bd">2008.jeptalnrecital-court.4</url>
      <language>fra</language>
      <bibkey>lehuen-2008-un</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>D</fixed-case>iscourse <fixed-case>R</fixed-case>epresentation <fixed-case>T</fixed-case>heory et graphes sémantiques : formalisation sémantique en contexte industriel</title>
      <author><first>Maxime</first><last>Amblard</last></author>
      <author><first>Johannes</first><last>Heinecke</last></author>
      <author><first>Estelle</first><last>Maillebuau</last></author>
      <pages>41–50</pages>
      <abstract>Ces travaux présentent une extension des représentations formelles pour la sémantique, de l’outil de traitement automatique des langues de Orange Labs1. Nous abordons ici uniquement des questions relatives à la construction des représentations sémantiques, dans le cadre de l’analyse linguistique. Afin d’obtenir des représentations plus fines de la structure argumentale des énoncés, nous incluons des concepts issus de la DRT dans le système de représentation basé sur les graphes sémantiques afin de rendre compte de la notion de portée.</abstract>
      <url hash="9e6c7107">2008.jeptalnrecital-court.5</url>
      <language>fra</language>
      <bibkey>amblard-etal-2008-discourse</bibkey>
    </paper>
    <paper id="6">
      <title>Sylva : plate-forme de validation multi-niveaux de lexiques</title>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <pages>51–60</pages>
      <abstract>La production de lexiques est une activité indispensable mais complexe, qui nécessite, quelle que soit la méthode de création utilisée (acquisition automatique ou manuelle), une validation humaine. Nous proposons dans ce but une plate-forme Web librement disponible, appelée Sylva (Systematic lexicon validator). Cette plate-forme a pour caractéristiques principales de permettre une validation multi-niveaux (par des validateurs, puis un expert) et une traçabilité de la ressource. La tâche de l’expert(e) linguiste en est allégée puisqu’il ne lui reste à considérer que les données sur lesquelles il n’y a pas d’accord inter-validateurs.</abstract>
      <url hash="9a5ce3c6">2008.jeptalnrecital-court.6</url>
      <language>fra</language>
      <bibkey>fort-guillaume-2008-sylva</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>E</fixed-case>-Gen : Profilage automatique de candidatures</title>
      <author><first>Rémy</first><last>Kessler</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <author><first>Marc</first><last>El-Bèze</last></author>
      <pages>61–70</pages>
      <abstract>La croissance exponentielle de l’Internet a permis le développement de sites d’offres d’emploi en ligne. Le système E-Gen (Traitement automatique d’offres d’emploi) a pour but de permettre l’analyse et la catégorisation d’offres d’emploi ainsi qu’une analyse et classification des réponses des candidats (Lettre de motivation et CV). Nous présentons les travaux réalisés afin de résoudre la seconde partie : on utilise une représentation vectorielle de texte pour effectuer une classification des pièces jointes contenus dans le mail à l’aide de SVM. Par la suite, une évaluation de la candidature est effectuée à l’aide de différents classifieurs (SVM et n-grammes de mots).</abstract>
      <url hash="04a1403d">2008.jeptalnrecital-court.7</url>
      <language>fra</language>
      <bibkey>kessler-etal-2008-e</bibkey>
    </paper>
    <paper id="8">
      <title>Typage, produit cartésien et unités d’analyse pour les modèles à états finis</title>
      <author><first>François</first><last>Barthélemy</last></author>
      <pages>71–80</pages>
      <abstract>Dans cet article, nous présentons un nouveau langage permettant d’écrire des relations rationnelles compilées en automates finis. Les deux caractéristiques innovantes de ce langage sont de pourvoir décrire des relations à plusieurs niveaux, pas nécessairement deux et d’utiliser diverses unités d’analyse pour exprimer les liens entre niveaux. Cela permet d’aligner de façon fine des représentations multiples.</abstract>
      <url hash="0f16f675">2008.jeptalnrecital-court.8</url>
      <language>fra</language>
      <bibkey>barthelemy-2008-typage</bibkey>
    </paper>
    <paper id="9">
      <title>Vers l’évaluation de systèmes de dialogue homme-machine : de l’oral au multimodal</title>
      <author><first>Frédéric</first><last>Landragin</last></author>
      <pages>81–90</pages>
      <abstract>L’évaluation pour le dialogue homme-machine ne se caractérise pas par l’efficacité, l’objectivité et le consensus que l’on observe dans d’autres domaines du traitement automatique des langues. Les systèmes de dialogue oraux et multimodaux restent cantonnés à des domaines applicatifs restreints, ce qui rend difficiles les évaluations comparatives ou normées. De plus, les avancées technologiques constantes rendent vite obsolètes les paradigmes d’évaluation et ont pour conséquence une multiplication de ceux-ci. Des solutions restent ainsi à trouver pour améliorer les méthodes existantes et permettre des diagnostics plus automatisés des systèmes. Cet article se veut un ensemble de réflexions autour de l’évaluation de la multimodalité dans les systèmes à forte composante linguistique. Des extensions des paradigmes existants sont proposées, en particulier DQR/DCR, sachant que certains sont mieux adaptés que d’autres au dialogue multimodal. Des conclusions et perspectives sont tirées sur l’avenir de l’évaluation pour le dialogue homme-machine.</abstract>
      <url hash="b0281028">2008.jeptalnrecital-court.9</url>
      <language>fra</language>
      <bibkey>landragin-2008-vers</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>POLYMOTS</fixed-case> : une base de données de constructions dérivationnelles en français à partir de radicaux phonologiques</title>
      <author><first>Nuria</first><last>Gala</last></author>
      <author><first>Véronique</first><last>Rey</last></author>
      <pages>91–100</pages>
      <abstract>Cet article présente POLYMOTS, une base de données lexicale contenant huit mille mots communs en français. L’originalité de l’approche proposée tient à l’analyse des mots. En effet, à la différence d’autres bases lexicales représentant la morphologie dérivationnelle des mots à partir d’affixes, ici l’idée a été d’isoler un radical commun à un ensemble de mots d’une même famille. Nous avons donc analysé les formes des mots et, par comparaison phonologique (forme phonique comparable) et morphologique (continuité de sens), nous avons regroupé les mots par familles, selon le type de radical phonologique. L’article présente les fonctionnalités de la base et inclut une discussion sur les applications et les perspectives d’une telle ressource.</abstract>
      <url hash="e7a7d618">2008.jeptalnrecital-court.10</url>
      <language>fra</language>
      <bibkey>gala-rey-2008-polymots</bibkey>
    </paper>
    <paper id="11">
      <title>Mesure de l’alternance entre préfixes pour la génération en traduction automatique</title>
      <author><first>Bruno</first><last>Cartoni</last></author>
      <pages>101–110</pages>
      <abstract>La génération de néologismes construits pose des problèmes dans un système de traduction automatique, notamment au moment de la sélection du préfixe dans les formations préfixées, quand certains préfixes paraissent pouvoir alterner. Nous proposons une étude « extensive », qui vise à rechercher dans de larges ressources textuelles (l’Internet) des formes préfixées générées automatiquement, dans le but d’individualiser les paramètres qui favorisent l’un des préfixes ou qui, au contraire, permettent cette alternance. La volatilité de cette ressource textuelle nécessite certaines précautions dans la méthodologie de décompte des données extraites.</abstract>
      <url hash="affc8a12">2008.jeptalnrecital-court.11</url>
      <language>fra</language>
      <bibkey>cartoni-2008-mesure</bibkey>
    </paper>
    <paper id="12">
      <title>Cascades de transducteurs pour le chunking de la parole conversationnelle : l’utilisation de la plateforme <fixed-case>C</fixed-case>as<fixed-case>S</fixed-case>ys dans le projet <fixed-case>EPAC</fixed-case></title>
      <author><first>Abdenour</first><last>Mokrane</last></author>
      <author><first>Nathalie</first><last>Friburger</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <pages>111–120</pages>
      <abstract>Cet article présente l’utilisation de la plate-forme CasSys pour la segmentation de la parole conversationnelle (chunking) à l’aide de cascades de transducteurs Unitex. Le système que nous présentons est utilisé dans le cadre du projet ANR EPAC. Ce projet a pour objectif l’indexation et l’annotation automatique de grands flux de parole issus d’émissions télévisées ou radiophoniques. Cet article présente tout d’abord l’adaptation à ce type de données d’un système antérieur de chunking (Romus) qui avait été développé pour le dialogue oral homme-machine. Il décrit ensuite les principaux problèmes qui se posent à l’analyse : traitement des disfluences de l’oral spontané, mais également gestion des erreurs dues aux étapes antérieures de reconnaissance de la parole et d’étiquetage morphosyntaxique.</abstract>
      <url hash="5b79d619">2008.jeptalnrecital-court.12</url>
      <language>fra</language>
      <bibkey>mokrane-etal-2008-cascades</bibkey>
    </paper>
    <paper id="13">
      <title>Regroupement automatique de documents en classes événementielles</title>
      <author><first>Aurélien</first><last>Bossard</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <pages>121–130</pages>
      <abstract>Cet article porte sur le regroupement automatique de documents sur une base événementielle. Après avoir précisé la notion d’événement, nous nous intéressons à la représentation des documents d’un corpus de dépêches, puis à une approche d’apprentissage pour réaliser les regroupements de manière non supervisée fondée sur k-means. Enfin, nous évaluons le système de regroupement de documents sur un corpus de taille réduite et nous discutons de l’évaluation quantitative de ce type de tâche.</abstract>
      <url hash="a6d34ed5">2008.jeptalnrecital-court.13</url>
      <language>fra</language>
      <bibkey>bossard-poibeau-2008-regroupement</bibkey>
    </paper>
    <paper id="14">
      <title>Comparing Constituency and Dependency Representations for <fixed-case>SMT</fixed-case> Phrase-Extraction</title>
      <author><first>Mary</first><last>Hearne</last></author>
      <author><first>Sylwia</first><last>Ozdowska</last></author>
      <author><first>John</first><last>Tinsley</last></author>
      <pages>131–140</pages>
      <abstract>We consider the value of replacing and/or combining string-basedmethods with syntax-based methods for phrase-based statistical machine translation (PBSMT), and we also consider the relative merits of using constituency-annotated vs. dependency-annotated training data. We automatically derive two subtree-aligned treebanks, dependency-based and constituency-based, from a parallel English–French corpus and extract syntactically motivated word- and phrase-pairs. We automatically measure PB-SMT quality. The results show that combining string-based and syntax-based word- and phrase-pairs can improve translation quality irrespective of the type of syntactic annotation. Furthermore, using dependency annotation yields greater translation quality than constituency annotation for PB-SMT.</abstract>
      <url hash="2c8001f1">2008.jeptalnrecital-court.14</url>
      <bibkey>hearne-etal-2008-comparing</bibkey>
    </paper>
    <paper id="15">
      <title>Repérage de citations, classification des styles de discours rapporté et identification des constituants citationnels en écrits journalistiques</title>
      <author><first>Fabien</first><last>Poulard</last></author>
      <author><first>Thierry</first><last>Waszak</last></author>
      <author><first>Nicolas</first><last>Hernandez</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <pages>141–150</pages>
      <abstract>Dans le contexte de la recherche de plagiat, le repérage de citations et de ses constituants est primordial puisqu’il peut amener à évaluer le caractère licite ou illicite d’une reprise (source citée ou non). Nous proposons ici une comparaison de méthodes automatiques pour le repérage de ces informations et rapportons une évaluation quantitative de celles-ci. Un corpus d’écrits journalistiques français a été manuellement annoté pour nous servir de base d’apprentissage et de test.</abstract>
      <url hash="f925e9ab">2008.jeptalnrecital-court.15</url>
      <language>fra</language>
      <bibkey>poulard-etal-2008-reperage</bibkey>
    </paper>
    <paper id="16">
      <title>Vers l’identification et le traitement des actes de dialogue composites</title>
      <author><first>Frédéric</first><last>Landragin</last></author>
      <pages>151–160</pages>
      <abstract>Il peut être difficile d’attribuer une seule valeur illocutoire à un énoncé dans un dialogue. En premier lieu, un énoncé peut comporter plusieurs segments de discours ayant chacun leur valeur illocutoire spécifique. De plus, un seul segment peut s’analyser en tant qu’acte de langage composite, regroupant par exemple la formulation d’une question et l’émission simultanée d’une information. Enfin, la structure du dialogue en termes d’échanges et de séquences peut être déterminante dans l’identification de l’acte, et peut également apporter une valeur illocutoire supplémentaire, comme celle de clore la séquence en cours. Dans le but de déterminer la réaction face à un tel acte de dialogue composite, nous présentons une approche théorique pour l’analyse des actes de dialogue en fonction du contexte de tâche et des connaissances des interlocuteurs. Nous illustrons sur un exemple nos choix de segmentation et d’identification des actes composites, et nous présentons les grandes lignes d’une stratégie pour déterminer la réaction qui semble être la plus pertinente.</abstract>
      <url hash="33054aa2">2008.jeptalnrecital-court.16</url>
      <language>fra</language>
      <bibkey>landragin-2008-vers-lidentification</bibkey>
    </paper>
    <paper id="17">
      <title>Représentation évènementielle des déplacements dans des dépêches épidémiologiques</title>
      <author><first>Manal</first><last>El Zant</last></author>
      <author><first>Jean</first><last>Royauté</last></author>
      <author><first>Michel</first><last>Roux</last></author>
      <pages>161–170</pages>
      <abstract>La représentation évènementielle des déplacements de personnes dans des dépêches épidémiologiques est d’une grande importance pour une compréhension détaillée du sens de ces dépêches. La dissémination des composants d’une telle représentation dans les dépêches rend difficile l’accès à leurs contenus. Ce papier décrit un système d’extraction d’information utilisant des cascades de transducteurs à nombre d’états fini qui ont permis la réalisation de trois tâches : la reconnaissance des entités nommées, l’annotation et la représentation des composants ainsi que la représentation des structures évènementielles. Nous avons obtenu une moyenne de rappel de 80, 93% pour la reconnaissance des entités nommées et de 97, 88% pour la représentation des composants. Ensuite, nous avons effectué un travail de normalisation de cette représentation par la résolution de certaines anaphores pronominales. Nous avons obtenu une valeur moyenne de précision de 81, 72% pour cette résolution.</abstract>
      <url hash="ed82805c">2008.jeptalnrecital-court.17</url>
      <language>fra</language>
      <bibkey>el-zant-etal-2008-representation</bibkey>
    </paper>
    <paper id="18">
      <title>Traduction multilingue : le projet <fixed-case>M</fixed-case>ul<fixed-case>T</fixed-case>ra</title>
      <author><first>Éric</first><last>Wehrli</last></author>
      <author><first>Luka</first><last>Nerima</last></author>
      <pages>171–178</pages>
      <abstract>L’augmentation rapide des échanges et des communications pluriculturels, en particulier sur internet, intensifie les besoins d’outils multilingues y compris de traduction. Cet article décrit un projet en cours au LATL pour le développement d’un système de traduction multilingue basé sur un modèle linguistique abstrait et largement générique, ainsi que sur un modèle logiciel basé sur la notion d’objet. Les langues envisagées dans la première phase de ce projet sont l’allemand, le français, l’italien, l’espagnol et l’anglais.</abstract>
      <url hash="3f9602ed">2008.jeptalnrecital-court.18</url>
      <language>fra</language>
      <bibkey>wehrli-nerima-2008-traduction</bibkey>
    </paper>
    <paper id="19">
      <title>Appariement d’entités nommées coréférentes : combinaisons de mesures de similarité par apprentissage supervisé</title>
      <author><first>Erwan</first><last>Moreau</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Olivier</first><last>Cappé</last></author>
      <pages>179–188</pages>
      <abstract>L’appariement d’entités nommées consiste à regrouper les différentes formes sous lesquelles apparaît une entité. Pour cela, des mesures de similarité textuelle sont généralement utilisées. Nous proposons de combiner plusieurs mesures afin d’améliorer les performances de la tâche d’appariement. À l’aide d’expériences menées sur deux corpus, nous montrons la pertinence de l’apprentissage supervisé dans ce but, particulièrement avec l’algorithme C4.5.</abstract>
      <url hash="10fefb3e">2008.jeptalnrecital-court.19</url>
      <language>fra</language>
      <bibkey>moreau-etal-2008-appariement</bibkey>
    </paper>
    <paper id="20">
      <title>Un sens logique pour les graphes sémantiques</title>
      <author><first>Renaud</first><last>Marlet</last></author>
      <pages>189–198</pages>
      <abstract>Nous discutons du sens des graphes sémantiques, notamment de ceux utilisés en Théorie Sens-Texte. Nous leur donnons un sens précis, éventuellement sous-spécifié, grâce à une traduction simple vers une formule de Minimal Recursion Semantics qui couvre les cas de prédications multiples sur plusieurs entités, de prédication d’ordre supérieur et de modalités.</abstract>
      <url hash="74e663f2">2008.jeptalnrecital-court.20</url>
      <language>fra</language>
      <bibkey>marlet-2008-un</bibkey>
    </paper>
    <paper id="21">
      <title>Annotation en Frames Sémantiques du corpus de dialogue <fixed-case>MEDIA</fixed-case></title>
      <author><first>Marie-Jean</first><last>Meurs</last></author>
      <author><first>Frédéric</first><last>Duvert</last></author>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <author><first>Fabrice</first><last>Lefèvre</last></author>
      <author><first>Renato</first><last>De Mori</last></author>
      <pages>199–208</pages>
      <abstract>Cet article présente un formalisme de représentation des connaissances qui a été utilisé pour fournir des annotations sémantiques de haut niveau pour le corpus de dialogue oral MEDIA. Ces annotations en structures sémantiques, basées sur le paradigme FrameNet, sont obtenues de manière incrémentale et partiellement automatisée. Nous décrivons le processus d’interprétation automatique qui permet d’obtenir des compositions sémantiques et de générer des hypothèses de frames par inférence. Le corpus MEDIA est un corpus de dialogues en langue française dont les tours de parole de l’utilisateur ont été manuellement transcrits et annotés (niveaux mots et constituants sémantiques de base). Le processus proposé utilise ces niveaux pour produire une annotation de haut niveau en frames sémantiques. La base de connaissances développée (définitions des frames et règles de composition) est présentée, ainsi que les résultats de l’annotation automatique.</abstract>
      <url hash="9f353f4a">2008.jeptalnrecital-court.21</url>
      <language>fra</language>
      <bibkey>meurs-etal-2008-annotation</bibkey>
    </paper>
    <paper id="22">
      <title>Dissymétrie entre l’indexation des documents et le traitement des requêtes pour la recherche d’information en langue arabe</title>
      <author><first>Ramzi</first><last>Abbès</last></author>
      <author><first>Malek</first><last>Boualem</last></author>
      <pages>209–218</pages>
      <abstract>Les moteurs de recherches sur le web produisent des résultats comparables et assez satisfaisants pour la recherche de documents écrits en caractères latins. Cependant, ils présentent de sérieuses lacunes dès que l’ont s’intéresse à des langues peu dotées ou des langues sémitiques comme l’arabe. Dans cet article nous présentons une étude analytique et qualitative de la recherche d’information en langue arabe en mettant l’accent sur l’insuffisance des outils de recherche actuels, souvent mal adaptés aux spécificités de la langue arabe. Pour argumenter notre analyse, nous présentons des résultats issus d’observations et de tests autour de certains phénomènes linguistiques de l’arabe écrit. Pour la validation des ces observations, nous avons testé essentiellement le moteur de recherche Google.</abstract>
      <url hash="f4ff6c4d">2008.jeptalnrecital-court.22</url>
      <language>fra</language>
      <bibkey>abbes-boualem-2008-dissymetrie</bibkey>
    </paper>
  </volume>
  <volume id="recital" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 15ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues</booktitle>
      <editor><first>Patrice</first><last>Bellot</last></editor>
      <editor><first>Marie-Laure</first><last>Guénot</last></editor>
      <publisher>ATALA</publisher>
      <address>Avignon, France</address>
      <month>June</month>
      <year>2008</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="b7f7f861">2008.jeptalnrecital-recital.0</url>
      <bibkey>jep-taln-recital-2008-actes-de-la</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Méthode de réordonnancement de réponses par transformation d’arbres : présentation et analyse des résultats</title>
      <author><first>Guillaume</first><last>Bernard</last></author>
      <pages>1–10</pages>
      <abstract>Dans cet article nous présentons une évaluation et une analyse des résultats d’une méthode de réordonnancement de réponses pour un système de questions-réponses. Cette méthode propose une sélection des réponses candidates à une question en calculant un coût par transformation d’arbres. Nous présentons une analyse des résultats obtenus sur le corpus Clef 2004-2005 et nos conclusions sur les voies d’amélioration possibles pour notre système.</abstract>
      <url hash="93df0d1c">2008.jeptalnrecital-recital.1</url>
      <language>fra</language>
      <bibkey>bernard-2008-methode</bibkey>
    </paper>
    <paper id="2">
      <title>Annotation des informations temporelles dans des textes en français</title>
      <author><first>André</first><last>Bittar</last></author>
      <pages>11–20</pages>
      <abstract>Le traitement des informations temporelles est crucial pour la compréhension de textes en langue naturelle. Le langage de spécification TimeML a été conçu afin de permettre le repérage et la normalisation des expressions temporelles et des événements dans des textes écrits en anglais. L’objectif des divers projets TimeML a été de formuler un schéma d’annotation pouvant s’appliquer à du texte libre, comme ce que l’on trouve sur le Web, par exemple. Des efforts ont été faits pour l’application de TimeML à d’autres langues que l’anglais, notamment le chinois, le coréen, l’italien, l’espagnol et l’allemand. Pour le français, il y a eu des efforts allant dans ce sens, mais ils sont encore un peu éparpillés. Dans cet article, nous détaillons nos travaux actuels qui visent à élaborer des ressources complètes pour l’annotation de textes en français selon TimeML - notamment un guide d’annotation, un corpus de référence (Gold Standard) et des modules d’annotation automatique.</abstract>
      <url hash="832c066b">2008.jeptalnrecital-recital.2</url>
      <language>fra</language>
      <bibkey>bittar-2008-annotation</bibkey>
    </paper>
    <paper id="3">
      <title>Morphosyntaxe de l’interrogation pour le système de question-réponse <fixed-case>RITEL</fixed-case></title>
      <author><first>Anne</first><last>Garcia-Fernandez</last></author>
      <author><first>Carole</first><last>Lailler</last></author>
      <pages>21–30</pages>
      <abstract>Nous proposons d’étudier le cas de l’interrogation en Dialogue Homme-Machine au sein d’un système de Question-Réponse à travers le prisme de la Grammaire Interactive. Celle-ci établit un rapport direct entre question et réponse et présuppose que la morphosyntaxe d’une interrogation dépend d’une « réponse escomptée »; l’interlocuteur humain ou machine ayant la possibilité de produire une réponse effective divergente. Nous proposons d’observer la présence des différentes formes de questions dans un corpus issu de l’utilisation du système RITEL. Et nous présentons une expérience menée sur des locuteurs natifs qui nous a permis de mettre en valeur la différence entre réponses effectives produites par nos sujets et réponses présupposées par le contenu intentionnel des questions. Les formalismes ainsi dégagés ont pour but de donner aux systèmes de DHM des fonctionnalités nouvelles comme la capacité à interpréter et à générer de la variabilité dans les énoncés produits.</abstract>
      <url hash="1bdd13e2">2008.jeptalnrecital-recital.3</url>
      <language>fra</language>
      <bibkey>garcia-fernandez-lailler-2008-morphosyntaxe</bibkey>
    </paper>
    <paper id="4">
      <title>Un système d’annotation des entités nommées du type personne pour la résolution de la référence</title>
      <author><first>Elzbieta</first><last>Gryglicka</last></author>
      <pages>31–40</pages>
      <abstract>Dans cet article nous présentons notre démarche pour l’annotation des expressions référentielles désignant les personnes et son utilisation pour la résolution partielle de la référence. Les choix effectués dans notre implémentation s’inspirent des travaux récents dans le domaine de l’extraction d’information et plus particulièrement de la reconnaissance des entités nommées. Nous utilisons les grammaires locales dans le but d’annoter les entités nommées du type Personne et pour construire, à partir des annotations produites, une base de connaissances extra-linguistiques. Les informations acquises par ce procédé sont ensuite utilisées pour implémenter une méthode de la résolution de la référence pour les syntagmes nominaux coréférentiels.</abstract>
      <url hash="2f81e99d">2008.jeptalnrecital-recital.4</url>
      <language>fra</language>
      <bibkey>gryglicka-2008-un</bibkey>
    </paper>
    <paper id="5">
      <title>Description de la structure de la phrase japonaise en vue d’une analyse syntaxique</title>
      <author><first>Alexis</first><last>Kauffmann</last></author>
      <pages>41–50</pages>
      <abstract>Nous décrivons la façon dont est formée la phrase japonaise, avec son contenu minimal, la structure des composants d’une phrase simple et l’ordre des mots dans ses composants, les différentes phrases complexes et les possibilités de changements modaux. Le but de cette description est de permettre l’analyse de la phrase japonaise selon des principes universels tout en restant fidèles aux particularités de la langue. L’analyseur syntaxique multilingue FIPS est en cours d’adaptation pour le japonais selon les règles de grammaire qui ont été définies. Bien qu’il fonctionnait alors uniquement pour des langues occidentales, les premiers résultats sont très positifs pour l’analyse des phrases simples, ce qui montre la capacité de Fips à s’adapter à des langues très différentes.</abstract>
      <url hash="34fe7d3e">2008.jeptalnrecital-recital.5</url>
      <language>fra</language>
      <bibkey>kauffmann-2008-description</bibkey>
    </paper>
    <paper id="6">
      <title>Adaptation d’un système de compréhension pour un robot compagnon</title>
      <author><first>Marc</first><last>Le Tallec</last></author>
      <pages>51–60</pages>
      <abstract>Le projet EmotiRob, financé par l’ANR, a pour but de réaliser un robot compagnon pour des enfants fragilisés. Le projet se décompose en deux sous parties que sont le module de compréhension pour comprendre ce que dit l’enfant et un module d’interaction émotionnelle pour apporter une réponse en simulant des émotions par les mouvements du corps, les traits du visage et par l’émission de petits sons simples. Le module de compréhension dont il est question ici réutilise les travaux du système Logus. La principale difficulté est de faire évoluer le système existant d’un dialogue homme-machine finalisé vers un domaine plus large et de détecter l’état émotionnel de l’enfant. Dans un premier temps, nous présentons le projet EmotiRob et ses spécificités. Ensuite, le système de compréhension de la parole Logus, sur lequel se base ce travail, est présenté en détail. Enfin, nous présentons les adaptations du système à la nouvelle tâche EmotiRob.</abstract>
      <url hash="c136b93f">2008.jeptalnrecital-recital.6</url>
      <language>fra</language>
      <bibkey>le-tallec-2008-adaptation</bibkey>
    </paper>
    <paper id="7">
      <title>Identification automatique de marques d’opinion dans des textes</title>
      <author><first>Aiala</first><last>Rosá</last></author>
      <pages>61–69</pages>
      <abstract>Nous présentons un modèle conceptuel pour la représentation d’opinions, en analysant les éléments qui les composent et quelques propriétés. Ce modèle conceptuel est implémenté et nous en décrivons le jeu d’annotations. Le processus automatique d’annotation de textes en espagnol est effectué par application de règles contextuelles. Un premier sous-ensemble de règles a été écrit pour l’identification de quelques éléments du modèle. Nous analysons les premiers résultats de leur application.</abstract>
      <url hash="ea0bffce">2008.jeptalnrecital-recital.7</url>
      <language>fra</language>
      <bibkey>rosa-2008-identification</bibkey>
    </paper>
    <paper id="8">
      <title>Transducteurs à fenêtre glissante pour l’induction lexicale</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <pages>70–79</pages>
      <abstract>Nous appliquons différents modèles de similarité graphique à la tâche de l’induction de lexiques bilingues entre un dialecte de Suisse allemande et l’allemand standard. Nous comparons des transducteurs stochastiques utilisant des fenêtres glissantes de 1 à 3 caractères, entraînés à l’aide de l’algorithme de maximisation de l’espérance avec des corpus d’entraînement de tailles différentes. Si les transducteurs à unigrammes donnent des résultats satisfaisants avec des corpus très petits, nous montrons que les transducteurs à bigrammes les dépassent à partir de 750 paires de mots d’entraînement. En général, les modèles entraînés nous ont permis d’améliorer la F-mesure de 7% à 15% par rapport à la distance de Levenshtein.</abstract>
      <url hash="21399c80">2008.jeptalnrecital-recital.8</url>
      <language>fra</language>
      <bibkey>scherrer-2008-transducteurs</bibkey>
    </paper>
    <paper id="9">
      <title>Génération intégrée localisée pour la production de documents</title>
      <author><first>Pierre</first><last>Hankach</last></author>
      <pages>80–89</pages>
      <abstract>Dans cet article, nous proposons une approche intégrée localisée pour la génération. Dans cette approche, le traitement intégré des décisions linguistiques est limité à la production des propositions dont les décisions qui concernent leurs générations sont dépendantes. La génération se fait par groupes de propositions de tailles limitées avec traitement intégré des décisions linguistiques qui concernent la production des propositions qui appartiennent au même groupe. Notre approche apporte une solution pour le problème de complexité computationnelle de la génération intégrée classique. Elle fournit ainsi une alternative à la génération séparée (séquentielle ou interactive) qui présente plusieurs défauts mais qui est implémentée de manière répandue dans les systèmes de générations existants.</abstract>
      <url hash="70ae456f">2008.jeptalnrecital-recital.9</url>
      <language>fra</language>
      <bibkey>hankach-2008-generation</bibkey>
    </paper>
    <paper id="10">
      <title>Un système de génération et étiquetage automatique de dictionnaires linguistiques de l’arabe</title>
      <author><first>Mohamed</first><last>Bouallegue</last></author>
      <author><first>Mohsen</first><last>Maraoui</last></author>
      <author><first>Mourad</first><last>Mars</last></author>
      <author><first>Mounir</first><last>Zrigui</last></author>
      <pages>90–99</pages>
      <abstract>L’objectif de cet article est la présentation d’un système de génération automatique de dictionnaires électroniques de la langue arabe classique, développé au sein de laboratoire UTIC (unité de Monastir). Dans cet article, nous présenterons, les différentes étapes de réalisation, et notamment la génération automatique de ces dictionnaires se basant sur une théorie originale : les Conditions de Structures Morphomatiques (CSM), et les matrices lexicales. Ce système rentre dans le cadre des deux projets MIRTO et OREILLODULE réalisés dans les deux laboratoires LIDILEM de Grenoble et UTIC Monastir de Tunisie</abstract>
      <url hash="bbab8fcd">2008.jeptalnrecital-recital.10</url>
      <language>fra</language>
      <bibkey>bouallegue-etal-2008-un</bibkey>
    </paper>
    <paper id="11">
      <title>Analyse quantitative et qualitative de citations extraites d’un corpus journalistique</title>
      <author><first>Fabien</first><last>Poulard</last></author>
      <pages>100–109</pages>
      <abstract>Dans le contexte de la détection de plagiats, le repérage de citations et de ses constituants est primordial puisqu’il peut aider à évaluer le caractère licite ou illicite d’une reprise (source citée ou non). Nous proposons ici une étude quantitative et qualitative des citations extraites d’un corpus que nous avons auparavant construit. Cette étude a pour but de tracer des axes de recherche vers une méthode de repérage automatique des citations.</abstract>
      <url hash="995b0af8">2008.jeptalnrecital-recital.11</url>
      <language>fra</language>
      <bibkey>poulard-2008-analyse</bibkey>
    </paper>
    <paper id="12">
      <title>Une structure pour les questions enchainées</title>
      <author><first>Kévin</first><last>Séjourné</last></author>
      <pages>110–119</pages>
      <abstract>Nous présentons des travaux réalisés dans le domaine des systèmes de questions réponses (SQR) utilisant des questions enchainées. La recherche des documents dans un SQR est perturbée par l’absence d’informations sur la valeur à accorder aux éléments de texte éventuellement utiles à la recherche d’informations qui figurent dans les questions liées. Les récentes campagnes d’évaluation montrent que ce problème est sous-estimé, et n’a pas fait l’oeuvre de technique dédiée. Afin d’améliorer la recherche des documents dans un SQR nous étudions une nouvelle méthode pour organiser les informations liées aux interactions entre questions. Celle-ci se base sur l’exploitation d’une structure de données adaptée à la transmission des informations des questions liées jusqu’au moteur d’interrogation.</abstract>
      <url hash="e8f606f2">2008.jeptalnrecital-recital.12</url>
      <language>fra</language>
      <bibkey>sejourne-2008-une</bibkey>
    </paper>
    <paper id="13">
      <title>Vers une nouvelle approche de la correction grammaticale automatique</title>
      <author><first>Agnès</first><last>Souque</last></author>
      <pages>120–129</pages>
      <abstract>La correction grammaticale automatique du français est une fonctionnalité qui fait cruellement défaut à la communauté des utilisateurs de logiciels libres. Dans le but de combler cette lacune, nous avons travaillé à l’adaptation au français d’un outil initialement développé pour une langue étrangère. Ce travail nous a permis de montrer que les approches classiques du traitement automatique des langues utilisées dans le domaine ne sont pas appropriées. Pour y remédier, nous proposons de faire évoluer les formalismes des correcteurs en intégrant les principes linguistiques de la segmentation en chunks et de l’unification. Bien qu’efficace, cette évolution n’est pas suffisante pour obtenir un bon correcteur grammatical du français. Nous envisageons alors une nouvelle approche de la problématique.</abstract>
      <url hash="47fc11d4">2008.jeptalnrecital-recital.13</url>
      <language>fra</language>
      <bibkey>souque-2008-vers</bibkey>
    </paper>
    <paper id="14">
      <title>Informations spatio-temporelles et objets touristiques dans des pages Web : repérage et annotation</title>
      <author><first>Stéphanie</first><last>Weiser</last></author>
      <pages>130–139</pages>
      <abstract>Cet article présente un projet de repérage, d’extraction et d’annotation d’informations temporelles, d’informations spatiales et d’objets touristiques dans des pages Web afin d’alimenter la base de connaissance d’un portail touristique. Nous portons une attention particulière aux différences qui distinguent le repérage d’information dans des pages Web du repérage d’informations dans des documents structurés. Après avoir introduit et classifié les différentes informations à extraire, nous nous intéressons à la façon de lier ces informations entre elles (par exemple apparier une information d’ouverture et un restaurant) et de les annoter. Nous présentons également le logiciel que nous avons réalisé afin d’effectuer cette opération d’annotation ainsi que les premiers résultats obtenus. Enfin, nous nous intéressons aux autres types de marques que l’on trouve dans les pages Web, les marques sémiotiques en particulier, dont l’analyse peut être utile à l’interprétation des pages.</abstract>
      <url hash="71f95e91">2008.jeptalnrecital-recital.14</url>
      <language>fra</language>
      <bibkey>weiser-2008-informations</bibkey>
    </paper>
  </volume>
</collection>
