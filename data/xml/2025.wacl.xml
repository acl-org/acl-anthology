<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.wacl">
  <volume id="1" ingest-date="2025-01-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th Workshop on Arabic Corpus Linguistics (WACL-4)</booktitle>
      <editor><first>Saad</first><last>Ezzini</last></editor>
      <editor><first>Hamza</first><last>Alami</last></editor>
      <editor><first>Ismail</first><last>Berrada</last></editor>
      <editor><first>Abdessamad</first><last>Benlahbib</last></editor>
      <editor><first>Abdelkader</first><last>El Mahdaouy</last></editor>
      <editor><first>Salima</first><last>Lamsiyah</last></editor>
      <editor><first>Hatim</first><last>Derrouz</last></editor>
      <editor><first>Amal</first><last>Haddad Haddad</last></editor>
      <editor><first>Mustafa</first><last>Jarrar</last></editor>
      <editor><first>Mo</first><last>El-Haj</last></editor>
      <editor><first>Ruslan</first><last>Mitkov</last></editor>
      <editor><first>Paul</first><last>Rayson</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, UAE</address>
      <month>January</month>
      <year>2025</year>
      <url hash="7adff051">2025.wacl-1</url>
      <venue>wacl</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="83c73d76">2025.wacl-1.0</url>
      <bibkey>wacl-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>A</fixed-case>rabic<fixed-case>S</fixed-case>ense: A Benchmark for Evaluating Commonsense Reasoning in <fixed-case>A</fixed-case>rabic with Large Language Models</title>
      <author><first>Salima</first><last>Lamsiyah</last></author>
      <author><first>Kamyar</first><last>Zeinalipour</last></author>
      <author><first>Samir</first><last>El amrany</last></author>
      <author><first>Matthias</first><last>Brust</last></author>
      <author><first>Marco</first><last>Maggini</last></author>
      <author><first>Pascal</first><last>Bouvry</last></author>
      <author><first>Christoph</first><last>Schommer</last></author>
      <pages>1–11</pages>
      <abstract>Recent efforts in natural language processing (NLP) commonsense reasoning research have led to the development of numerous new datasets and benchmarks. However, these resources have predominantly been limited to English, leaving a gap in evaluating commonsense reasoning in other languages. In this paper, we introduce the ArabicSense Benchmark, which is designed to thoroughly evaluate the world-knowledge commonsense reasoning abilities of large language models (LLMs) in Arabic. This benchmark includes three main tasks: first, it tests whether a system can distinguish between natural language statements that make sense and those that do not; second, it requires a system to identify the most crucial reason why a nonsensical statement fails to make sense; and third, it involves generating explanations for why statements do not make sense. We evaluate several Arabic BERT-based models and causal LLMs on these tasks. Experimental results demonstrate improvements after fine-tuning on our dataset. For instance, AraBERT v2 achieved an 87% F1 score on the second task, while Gemma and Mistral-7b achieved F1 scores of 95.5% and 94.8%, respectively. For the generation task, LLaMA-3 achieved the best performance with a BERTScore F1 of 77.3%, closely followed by Mistral-7b at 77.1%. All codes and the benchmark will be made publicly available at https://github.com/.</abstract>
      <url hash="64097bee">2025.wacl-1.1</url>
      <bibkey>lamsiyah-etal-2025-arabicsense</bibkey>
    </paper>
    <paper id="2">
      <title>Lahjawi: <fixed-case>A</fixed-case>rabic Cross-Dialect Translator</title>
      <author><first>Mohamed Motasim</first><last>Hamed</last></author>
      <author><first>Muhammad</first><last>Hreden</last></author>
      <author><first>Khalil</first><last>Hennara</last></author>
      <author><first>Zeina</first><last>Aldallal</last></author>
      <author><first>Sara</first><last>Chrouf</last></author>
      <author><first>Safwan</first><last>AlModhayan</last></author>
      <pages>12–24</pages>
      <abstract>In this paper, we explore the rich diversity of Arabic dialects by introducing a suite of pioneering models called Lahjawi. The primary model, Lahjawi-D2D, is the first designed for cross-dialect translation among 15 Arabic dialects. Furthermore, we introduce Lahjawi-D2MSA, a model designed to convert any Arabic dialect into Modern Standard Arabic (MSA). Both models are fine-tuned versions of Kuwain-1.5B an in-house built small language model, tailored for Arabic linguistic characteristics. We provide a detailed overview of Lahjawi’s architecture and training methods, along with a comprehensive evaluation of its performance. The results demonstrate Lahjawi’s success in preserving meaning and style, with BLEU scores of 9.62 for dialect-to-MSA and 9.88 for dialect-to- dialect tasks. Additionally, human evaluation reveals an accuracy score of 58% and a fluency score of 78%, underscoring Lahjawi’s robust handling of diverse dialectal nuances. This research sets a foundation for future advancements in Arabic NLP and cross-dialect communication technologies.</abstract>
      <url hash="9bedf483">2025.wacl-1.2</url>
      <bibkey>hamed-etal-2025-lahjawi</bibkey>
    </paper>
    <paper id="3">
      <title>Lost in Variation: An Unsupervised Methodology for Mining Lexico-syntactic Patterns in Middle <fixed-case>A</fixed-case>rabic Texts</title>
      <author><first>Julien</first><last>Bezançon</last></author>
      <author><first>Rimane</first><last>Karam</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <pages>25–37</pages>
      <abstract>While MSA and some dialects of Arabic have been extensively studied in NLP, Middle Arabic is still very much unknown to the field. However, Middle Arabic holds issues that are still not covered: it is characterized by variation since it mixes standard features, colloquial ones, as well as features that belong to neither of the two. Here, we introduce a methodology to identify, extract and rank variations of 13 manually retrieved formulas. Those formulas come from the nine first booklets of S ̄IRAT AL-MALIK AL-Z. ̄AHIR BAYBAR S., a corpus of Damascene popular literature written in Middle Arabic and composed of 53,843 sentences. In total, we ranked 20, sequences according to their similarity with the original formulas on multiple linguistic layers. We noticed that the variations in these formulas occur in a lexical, morphological and graphical level, but in opposition, the semantic and syntactic levels remain strictly invariable.</abstract>
      <url hash="f742d632">2025.wacl-1.3</url>
      <bibkey>bezancon-etal-2025-lost</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>SADSL</fixed-case>y<fixed-case>C</fixed-case>: A Corpus for Saudi <fixed-case>A</fixed-case>rabian Multi-dialect Identification through Song Lyrics</title>
      <author><first>Salwa Saad</first><last>Alahmari</last></author>
      <pages>38–43</pages>
      <abstract>This paper presents the Saudi Arabian Dialects Song Lyrics Corpus (SADSLyC), the first dataset featuring song lyrics from the five major Saudi dialects: Najdi (Central Region), Hijazi (Western Region), Shamali (Northern Region), Janoubi (Southern Region), and Shargawi (Eastern Region). The dataset consists of 31,358 sentences, with each sentence representing a self-contained verse in a song, totaling 151,841 words. Additionally, we present a baseline experiment using the SaudiBERT model to classify the fine-grained dialects in the SADSLyC Corpus. The model achieved an overall accuracy of 73% on the test dataset.</abstract>
      <url hash="fca04476">2025.wacl-1.4</url>
      <bibkey>alahmari-2025-sadslyc</bibkey>
    </paper>
    <paper id="5">
      <title>Enhancing Dialectal <fixed-case>A</fixed-case>rabic Intent Detection through Cross-Dialect Multilingual Input Augmentation</title>
      <author><first>Shehenaz</first><last>Hossain</last></author>
      <author><first>Fouad</first><last>Shammary</last></author>
      <author><first>Bahaulddin</first><last>Shammary</last></author>
      <author><first>Haithem</first><last>Afli</last></author>
      <pages>44–49</pages>
      <abstract>Addressing the challenges of Arabic intent detection amid extensive dialectal variation, this study presents a crossdialtectal, multilingual approach for classifying intents in banking and migration contexts. By augmenting dialectal inputs with Modern Standard Arabic (MSA) and English translations, our method leverages cross-lingual context to improve classification accuracy. We evaluate single-input (dialect-only), dual-input (dialect + MSA), and triple-input (dialect + MSA + English) models, applying language-specific tokenization for each. Results demonstrate that, in the migration dataset, our model achieved an accuracy gain of over 50% on Tunisian dialect, increasing from 43.3% with dialect-only input to 94% with the full multilingual setup. Similarly, in the PAL (Palestinian dialect) dataset, accuracy improved from 87.7% to 93.5% with translation augmentation, reflecting a gain of 5.8 percentage points. These findings underscore the effectiveness of our approach for intent detection across various Arabic dialects.</abstract>
      <url hash="eb6cd93d">2025.wacl-1.5</url>
      <bibkey>hossain-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>D</fixed-case>ial2<fixed-case>MSA</fixed-case>-Verified: A Multi-Dialect <fixed-case>A</fixed-case>rabic Social Media Dataset for Neural Machine Translation to <fixed-case>M</fixed-case>odern <fixed-case>S</fixed-case>tandard <fixed-case>A</fixed-case>rabic</title>
      <author><first>Abdullah</first><last>Khered</last></author>
      <author><first>Youcef</first><last>Benkhedda</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last></author>
      <pages>50–62</pages>
      <abstract>Social media has become an essential focus for Natural Language Processing (NLP) research due to its widespread use and unique linguistic characteristics. Normalising social media content, especially for morphologically rich languages like Arabic, remains a complex task due to limited parallel corpora. Arabic encompasses Modern Standard Arabic (MSA) and various regional dialects, collectively termed Dialectal Arabic (DA), which complicates NLP efforts due to their informal nature and variability. This paper presents Dial2MSA-Verified, an extension of the Dial2MSA dataset that includes verified translations for Gulf, Egyptian, Levantine, and Maghrebi dialects. We evaluate the performance of Seq2Seq models on this dataset, highlighting the effectiveness of state-of-the-art models in translating local Arabic dialects. We also provide insights through error analysis and outline future directions for enhancing Seq2Seq models and dataset development. The Dial2MSA-Verified dataset is publicly available to support further research.</abstract>
      <url hash="1664ba8d">2025.wacl-1.6</url>
      <bibkey>khered-etal-2025-dial2msa</bibkey>
    </paper>
    <paper id="7">
      <title>Web-Based Corpus Compilation of the Emirati <fixed-case>A</fixed-case>rabic Dialect</title>
      <author><first>Yousra A.</first><last>El-Ghawi</last></author>
      <pages>63–67</pages>
      <abstract>This paper displays some initial efforts conducted in the compilation pursuits of Arabic dialectal corpora in the form of raw text, the end purpose of which is to fine-tune existing Arabic large language models (LLM) to better understand and generate text in the Emirati dialect as instructed. The focus of the paper is on the process of compiling corpora from the web, which includes the exploration of possible methods, tools and techniques specific to web search, as well as examples of genres and domains to explore. The results of these efforts and the importance of native speaker contributions to corpus compilation for low-resource languages are also touched upon.</abstract>
      <url hash="c93f72fc">2025.wacl-1.7</url>
      <bibkey>el-ghawi-2025-web</bibkey>
    </paper>
    <paper id="8">
      <title>Evaluating Calibration of <fixed-case>A</fixed-case>rabic Pre-trained Language Models on Dialectal Text</title>
      <author><first>Ali</first><last>Al-Laith</last></author>
      <author><first>Rachida</first><last>Kebdani</last></author>
      <pages>68–76</pages>
      <abstract>While pre-trained language models have made significant progress in different classification tasks, little attention has been given to the reliability of their confidence scores. Calibration, how well model confidence aligns with actual accuracy, is essential for real-world applications where decisions rely on probabilistic outputs. This study addresses this gap in Arabic dialect identification by assessing the calibration of eight pre-trained language models, ensuring their predictions are not only accurate but also reliable for practical applications. We analyze two datasets: one with over 1 million text samples and the Nuanced Arabic Dialect Identification dataset(NADI-2023). Using Expected Calibration Error (ECE) as a metric, we reveal substantial variation in model calibration across dialects in both datasets, showing that prediction confidence can vary significantly depending on regional data. This research has implications for improving the reliability of Arabic dialect models in applications like sentiment analysis and social media monitoring.</abstract>
      <url hash="87c251fd">2025.wacl-1.8</url>
      <bibkey>al-laith-kebdani-2025-evaluating</bibkey>
    </paper>
    <paper id="9">
      <title>Empirical Evaluation of Pre-trained Language Models for Summarizing <fixed-case>M</fixed-case>oroccan <fixed-case>D</fixed-case>arija News Articles</title>
      <author><first>Azzedine</first><last>Aftiss</last></author>
      <author><first>Salima</first><last>Lamsiyah</last></author>
      <author><first>Christoph</first><last>Schommer</last></author>
      <author><first>Said Ouatik</first><last>El Alaoui</last></author>
      <pages>77–85</pages>
      <abstract>Moroccan Dialect (MD), or “Darija,” is a primary spoken variant of Arabic in Morocco, yet remains underrepresented in Natural Language Processing (NLP) research, particularly in tasks like summarization. Despite a growing volume of MD textual data online, there is a lack of robust resources and NLP models tailored to handle the unique linguistic challenges posed by MD. In response, we introduce .MA_v2, an expanded version of the GOUD.MA dataset, containing over 50k articles with their titles across 11 categories. This dataset provides a more comprehensive resource for developing summarization models. We evaluate the application of large language models (LLMs) for MD summarization, utilizing both fine-tuning and zero-shot prompting with encoder-decoder and causal LLMs, respectively. Our findings demonstrate that an expanded dataset improves summarization performance and highlights the capabilities of recent LLMs in handling MD text. We open-source our dataset, fine-tuned models, and all experimental code, establishing a foundation for future advancements in MD NLP. We release the code at https://github.com/AzzedineAftiss/Moroccan-Dialect-Summarization.</abstract>
      <url hash="860462d1">2025.wacl-1.9</url>
      <bibkey>aftiss-etal-2025-empirical</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>D</fixed-case>ialect2<fixed-case>SQL</fixed-case>: A Novel Text-to-<fixed-case>SQL</fixed-case> Dataset for <fixed-case>A</fixed-case>rabic Dialects with a Focus on <fixed-case>M</fixed-case>oroccan <fixed-case>D</fixed-case>arija</title>
      <author><first>Salmane</first><last>Chafik</last></author>
      <author><first>Saad</first><last>Ezzini</last></author>
      <author><first>Ismail</first><last>Berrada</last></author>
      <pages>86–92</pages>
      <abstract>The task of converting natural language questions into executable SQL queries, known as text-to-SQL, has gained significant interest in recent years, as it enables non-technical users to interact with relational databases. Many benchmarks, such as SPIDER and WikiSQL, have contributed to the development of new models and the evaluation of their performance. In addition, other datasets, like SEDE and BIRD, have introduced more challenges and complexities to better map real-world scenarios. However, these datasets primarily focus on high-resource languages such as English and Chinese. In this work, we introduce Dialect2SQL, the first large-scale, cross-domain text-to-SQL dataset in an Arabic dialect. It consists of 9,428 NLQ-SQL pairs across 69 databases in various domains. Along with SQL-related challenges such as long schemas, dirty values, and complex queries, our dataset also incorporates the complexities of the Moroccan dialect, which is known for its diverse source lan-guages, numerous borrowed words, and unique expressions. This demonstrates that our dataset will be a valuable contribution to both the text-to-SQL community and the development of resources for low-resource languages.</abstract>
      <url hash="13959048">2025.wacl-1.10</url>
      <bibkey>chafik-etal-2025-dialect2sql</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>A</fixed-case>ra<fixed-case>S</fixed-case>im: Optimizing <fixed-case>A</fixed-case>rabic Dialect Translation in Children’s Literature with <fixed-case>LLM</fixed-case>s and Similarity Scores</title>
      <author><first>Alaa</first><last>Bouomar</last></author>
      <author><first>Noorhan</first><last>Abbas</last></author>
      <pages>93–102</pages>
      <abstract>The goal of the paper is to address the linguistic gap faced by young Egyptian Arabic speakers through translating children stories from Modern Standard Arabic to the Egyptian Cairo dialect. Claude is used for initial translation, and a fine-tuned AraT5 model is used for backtranslation. The translation quality is assessed using semantic similarity and BLUE scores to compare the original texts and the translations. The resulting corpus contains 130 stories which were revised by native Egyptian speakers who are professional translators. The strengths of this paper are multiple: working on a less-resourced variety, addressing an important social issue, creating a dataset with potential real-life applications, and ensuring the quality of the produced dataset through human validation.</abstract>
      <url hash="af265bce">2025.wacl-1.11</url>
      <attachment type="OptionalSupplementaryMaterial" hash="0de5533a">2025.wacl-1.11.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>bouomar-abbas-2025-arasim</bibkey>
    </paper>
    <paper id="12">
      <title>Navigating Dialectal Bias and Ethical Complexities in <fixed-case>L</fixed-case>evantine <fixed-case>A</fixed-case>rabic Hate Speech Detection</title>
      <author><first>Ahmed</first><last>Haj Ahmed</last></author>
      <author><first>Rui-Jie</first><last>Yew</last></author>
      <author><first>Xerxes</first><last>Minocher</last></author>
      <author><first>Suresh</first><last>Venkatasubramanian</last></author>
      <pages>103–108</pages>
      <abstract>Social media platforms have become central to global communication, yet they also facilitate the spread of hate speech. For underrepresented dialects like Levantine Arabic, detecting hate speech presents unique cultural, ethical, and linguistic challenges. This paper explores the complex sociopolitical and linguistic landscape of Levantine Arabic and critically examines the limitations of current datasets used in hate speech detection. We highlight the scarcity of publicly available, diverse datasets and analyze the consequences of dialectal bias within existing resources. By emphasizing the need for culturally and contextually informed natural language processing (NLP) tools, we advocate for a more nuanced and inclusive approach to hate speech detection in the Arab world.</abstract>
      <url hash="2446d519">2025.wacl-1.12</url>
      <bibkey>haj-ahmed-etal-2025-navigating</bibkey>
    </paper>
  </volume>
</collection>
