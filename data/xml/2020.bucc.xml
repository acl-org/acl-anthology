<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.bucc">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 13th Workshop on Building and Using Comparable Corpora</booktitle>
      <editor><first>Reinhard</first><last>Rapp</last></editor>
      <editor><first>Pierre</first><last>Zweigenbaum</last></editor>
      <editor><first>Serge</first><last>Sharoff</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-42-9</isbn>
    </meta>
    <frontmatter>
      <url hash="a7cecf05">2020.bucc-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Line-a-line: A Tool for Annotating Word-Alignments</title>
      <author><first>Maria</first><last>Skeppstedt</last></author>
      <author><first>Magnus</first><last>Ahltorp</last></author>
      <author><first>Gunnar</first><last>Eriksson</last></author>
      <author><first>Rickard</first><last>Domeij</last></author>
      <pages>1–5</pages>
      <abstract>We here describe line-a-line, a web-based tool for manual annotation of word-alignments in sentence-aligned parallel corpora. The graphical user interface, which builds on a design template from the Jigsaw system for investigative analysis, displays the words from each sentence pair that is to be annotated as elements in two vertical lists. An alignment between two words is annotated by drag-and-drop, i.e. by dragging an element from the left-hand list and dropping it on an element in the right-hand list. The tool indicates that two words are aligned by lines that connect them and by highlighting associated words when the mouse is hovered over them. Line-a-line uses the efmaral library for producing pre-annotated alignments, on which the user can base the manual annotation. The tool is mainly planned to be used on moderately under-resourced languages, for which resources in the form of parallel corpora are scarce. The automatic word-alignment functionality therefore also incorporates information derived from non-parallel resources, in the form of pre-trained multilingual word embeddings from the MUSE library.</abstract>
      <url hash="c368f052">2020.bucc-1.1</url>
      <language>eng</language>
    </paper>
    <paper id="2">
      <title>Overview of the Fourth <fixed-case>BUCC</fixed-case> Shared Task: Bilingual Dictionary Induction from Comparable Corpora</title>
      <author><first>Reinhard</first><last>Rapp</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <author><first>Serge</first><last>Sharoff</last></author>
      <pages>6–13</pages>
      <abstract>The shared task of the 13th Workshop on Building and Using Comparable Corpora was devoted to the induction of bilingual dictionaries from comparable rather than parallel corpora. In this task, for a number of language pairs involving Chinese, English, French, German, Russian and Spanish, the participants were supposed to determine automatically the target language translations of several thousand source language test words of three frequency ranges. We describe here some background, the task definition, the training and test data sets and the evaluation used for ranking the participating systems. We also summarize the approaches used and present the results of the evaluation. In conclusion, the outcome of the competition are the results of a number of systems which provide surprisingly good solutions to the ambitious problem.</abstract>
      <url hash="0ff9d081">2020.bucc-1.2</url>
      <language>eng</language>
    </paper>
    <paper id="3">
      <title>Constructing a Bilingual Corpus of Parallel Tweets</title>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Sabit</first><last>Hassan</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <pages>14–21</pages>
      <abstract>In a bid to reach a larger and more diverse audience, Twitter users often post parallel tweets—tweets that contain the same content but are written in different languages. Parallel tweets can be an important resource for developing machine translation (MT) systems among other natural language processing (NLP) tasks. In this paper, we introduce a generic method for collecting parallel tweets. Using this method, we collect a bilingual corpus of English-Arabic parallel tweets and a list of Twitter accounts who post English-Arabictweets regularly. Since our method is generic, it can also be used for collecting parallel tweets that cover less-resourced languages such as Serbian and Urdu. Additionally, we annotate a subset of Twitter accounts with their countries of origin and topic of interest, which provides insights about the population who post parallel tweets. This latter information can also be useful for author profiling tasks.</abstract>
      <url hash="b3ac5e2f">2020.bucc-1.3</url>
      <language>eng</language>
    </paper>
    <paper id="4">
      <title>Automatic Creation of Correspondence Table of Meaning Tags from Two Dictionaries in One Language Using Bilingual Word Embedding</title>
      <author><first>Teruo</first><last>Hirabayashi</last></author>
      <author><first>Kanako</first><last>Komiya</last></author>
      <author><first>Masayuki</first><last>Asahara</last></author>
      <author><first>Hiroyuki</first><last>Shinnou</last></author>
      <pages>22–28</pages>
      <abstract>In this paper, we show how to use bilingual word embeddings (BWE) to automatically create a corresponding table of meaning tags from two dictionaries in one language and examine the effectiveness of the method. To do this, we had a problem: the meaning tags do not always correspond one-to-one because the granularities of the word senses and the concepts are different from each other. Therefore, we regarded the concept tag that corresponds to a word sense the most as the correct concept tag corresponding the word sense. We used two BWE methods, a linear transformation matrix and VecMap. We evaluated the most frequent sense (MFS) method and the corpus concatenation method for comparison. The accuracies of the proposed methods were higher than the accuracy of the random baseline but lower than those of the MFS and corpus concatenation methods. However, because our method utilized the embedding vectors of the word senses, the relations of the sense tags corresponding to concept tags could be examined by mapping the sense embeddings to the vector space of the concept tags. Also, our methods could be performed when we have only concept or word sense embeddings whereas the MFS method requires a parallel corpus and the corpus concatenation method needs two tagged corpora.</abstract>
      <url hash="3b5027b0">2020.bucc-1.4</url>
      <language>eng</language>
    </paper>
    <paper id="5">
      <title>Mining Semantic Relations from Comparable Corpora through Intersections of Word Embeddings</title>
      <author><first>Špela</first><last>Vintar</last></author>
      <author><first>Larisa</first><last>Grčić Simeunović</last></author>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <author><first>Uroš</first><last>Stepišnik</last></author>
      <pages>29–34</pages>
      <abstract>We report an experiment aimed at extracting words expressing a specific semantic relation using intersections of word embeddings. In a multilingual frame-based domain model, specific features of a concept are typically described through a set of non-arbitrary semantic relations. In karstology, our domain of choice which we are exploring though a comparable corpus in English and Croatian, karst phenomena such as landforms are usually described through their FORM, LOCATION, CAUSE, FUNCTION and COMPOSITION. We propose an approach to mine words pertaining to each of these relations by using a small number of seed adjectives, for which we retrieve closest words using word embeddings and then use intersections of these neighbourhoods to refine our search. Such cross-language expansion of semantically-rich vocabulary is a valuable aid in improving the coverage of a multilingual knowledge base, but also in exploring differences between languages in their respective conceptualisations of the domain.</abstract>
      <url hash="be499c28">2020.bucc-1.5</url>
      <language>eng</language>
    </paper>
    <paper id="6">
      <title>Benchmarking Multidomain <fixed-case>E</fixed-case>nglish-<fixed-case>I</fixed-case>ndonesian Machine Translation</title>
      <author><first>Tri Wahyu</first><last>Guntara</last></author>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <author><first>Radityo Eko</first><last>Prasojo</last></author>
      <pages>35–43</pages>
      <abstract>In the context of Machine Translation (MT) from-and-to English, Bahasa Indonesia has been considered a low-resource language, and therefore applying Neural Machine Translation (NMT) which typically requires large training dataset proves to be problematic. In this paper, we show otherwise by collecting large, publicly-available datasets from the Web, which we split into several domains: news, religion, general, and conversation, to train and benchmark some variants of transformer-based NMT models across the domains. We show using BLEU that our models perform well across them , outperform the baseline Statistical Machine Translation (SMT) models, and perform comparably with Google Translate. Our datasets (with the standard split for training, validation, and testing), code, and models are available on <url>https://github.com/gunnxx/indonesian-mt-data</url>
      </abstract>
      <url hash="7513e58c">2020.bucc-1.6</url>
      <language>eng</language>
    </paper>
    <paper id="7">
      <title>Reducing the Search Space for Parallel Sentences in Comparable Corpora</title>
      <author><first>Rémi</first><last>Cardon</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <pages>44–48</pages>
      <abstract>This paper describes and evaluates simple techniques for reducing the research space for parallel sentences in monolingual comparable corpora. Initially, when searching for parallel sentences between two comparable documents, all the possible sentence pairs between the documents have to be considered, which introduces a great degree of imbalance between parallel pairs and non-parallel pairs. This is a problem because even with a high performing algorithm, a lot of noise will be present in the extracted results, thus introducing a need for an extensive and costly manual check phase. We work on a manually annotated subset obtained from a French comparable corpus and show how we can drastically reduce the number of sentence pairs that have to be fed to a classifier so that the results can be manually handled.</abstract>
      <url hash="fe2455e4">2020.bucc-1.7</url>
      <language>eng</language>
    </paper>
    <paper id="8">
      <title><fixed-case>LMU</fixed-case> Bilingual Dictionary Induction System with Word Surface Similarity Scores for <fixed-case>BUCC</fixed-case> 2020</title>
      <author><first>Silvia</first><last>Severini</last></author>
      <author><first>Viktor</first><last>Hangya</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>49–55</pages>
      <abstract>The task of Bilingual Dictionary Induction (BDI) consists of generating translations for source language words which is important in the framework of machine translation (MT). The aim of the BUCC 2020 shared task is to perform BDI on various language pairs using comparable corpora. In this paper, we present our approach to the task of English-German and English-Russian language pairs. Our system relies on Bilingual Word Embeddings (BWEs) which are often used for BDI when only a small seed lexicon is available making them particularly effective in a low-resource setting. On the other hand, they perform well on high frequency words only. In order to improve the performance on rare words as well, we combine BWE based word similarity with word surface similarity methods, such as orthography In addition to the often used top-n translation method, we experiment with a margin based approach aiming for dynamic number of translations for each source word. We participate in both the open and closed tracks of the shared task and we show improved results of our method compared to simple vector similarity based approaches. Our system was ranked in the top-3 teams and achieved the best results for English-Russian.</abstract>
      <url hash="b20c67c8">2020.bucc-1.8</url>
      <language>eng</language>
    </paper>
    <paper id="9">
      <title><fixed-case>TALN</fixed-case>/<fixed-case>LS</fixed-case>2<fixed-case>N</fixed-case> Participation at the <fixed-case>BUCC</fixed-case> Shared Task: Bilingual Dictionary Induction from Comparable Corpora</title>
      <author><first>Martin</first><last>Laville</last></author>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <pages>56–60</pages>
      <abstract>This paper describes the TALN/LS2N system participation at the Building and Using Comparable Corpora (BUCC) shared task. We first introduce three strategies: (i) a word embedding approach based on fastText embeddings; (ii) a concatenation approach using both character Skip-Gram and character CBOW models, and finally (iii) a cognates matching approach based on an exact match string similarity. Then, we present the applied strategy for the shared task which consists in the combination of the embeddings concatenation and the cognates matching approaches. The covered languages are French, English, German, Russian and Spanish. Overall, our system mixing embeddings concatenation and perfect cognates matching obtained the best results while compared to individual strategies, except for English-Russian and Russian-English language pairs for which the concatenation approach was preferred.</abstract>
      <url hash="c0189300">2020.bucc-1.9</url>
      <language>eng</language>
    </paper>
    <paper id="10">
      <title>c<fixed-case>E</fixed-case>n<fixed-case>T</fixed-case>am: Creation and Validation of a New <fixed-case>E</fixed-case>nglish-<fixed-case>T</fixed-case>amil Bilingual Corpus</title>
      <author><first>Sanjanasri</first><last>JP</last></author>
      <author><first>Premjith</first><last>B</last></author>
      <author><first>Vijay Krishna</first><last>Menon</last></author>
      <author><first>Soman</first><last>KP</last></author>
      <pages>61–64</pages>
      <abstract>Natural Language Processing (NLP), is the field of artificial intelligence that gives the computer the ability to interpret, perceive and extract appropriate information from human languages. Contemporary NLP is predominantly a data driven process. It employs machine learning and statistical algorithms to learn language structures from textual corpus. While application of NLP in English, certain European languages such as Spanish, German, etc. and Chinese, Arabic has been tremendous, it is not so, in many Indian languages. There are obvious advantages in creating aligned bilingual and multilingual corpora. Machine translation, cross-lingual information retrieval, content availability and linguistic comparison are a few of the most sought after applications of such parallel corpora. This paper explains and validates a parallel corpus we created for English-Tamil bilingual pair.</abstract>
      <url hash="62dfeacf">2020.bucc-1.10</url>
      <language>eng</language>
    </paper>
    <paper id="11">
      <title><fixed-case>BUCC</fixed-case>2020: Bilingual Dictionary Induction using Cross-lingual Embedding</title>
      <author><first>Sanjanasri</first><last>JP</last></author>
      <author><first>Vijay Krishna</first><last>Menon</last></author>
      <author><first>Soman</first><last>KP</last></author>
      <pages>65–68</pages>
      <abstract>This paper presents a deep learning system for the BUCC 2020 shared task: Bilingual dictionary induction from comparable corpora. We have submitted two runs for this shared Task, German (de) and English (en) language pair for “closed track” and Tamil (ta) and English (en) for the “open track”. Our core approach focuses on quantifying the semantics of the language pairs, so that semantics of two different language pairs can be compared or transfer learned. With the advent of word embeddings, it is possible to quantify this. In this paper, we propose a deep learning approach which makes use of the supplied training data, to generate cross-lingual embedding. This is later used for inducting bilingual dictionary from comparable corpora.</abstract>
      <url hash="2e3ec71e">2020.bucc-1.11</url>
      <language>eng</language>
    </paper>
  </volume>
</collection>
