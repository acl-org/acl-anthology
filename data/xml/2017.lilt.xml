<?xml version='1.0' encoding='UTF-8'?>
<collection id="2017.lilt">
  <volume id="15">
    <meta>
      <booktitle>Linguistic Issues in Language Technology, Volume 15, 2017</booktitle>
      <publisher>CSLI Publications</publisher>
      <year>2017</year>
      <venue>lilt</venue>
    </meta>
    <paper id="1">
      <title>Lexical Factorization and Syntactic Behavior</title>
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Aravind</first><last>Joshi</last></author>
      <abstract>In this paper, we examine the correlation between lexical semantics and the syntactic realization of the different components of a word’s meaning in natural language. More specifically, we will explore the effect that lexical factorization in verb semantics has on the suppression or expression of semantic features within the sentence. Factorization was a common analytic tool employed in early generative linguistic approaches to lexical decomposition, and continues to play a role in contemporary semantics, in various guises and modified forms. Building on the unpublished analysis of verbs of seeing in Joshi (1972), we argue here that the significance of lexical factorization is twofold: first, current models of verb meaning owe much of their insight to factor-based theories of meaning; secondly, the factorization properties of a lexical item appear to influence, both directly and indirectly, the possible syntactic expressibility of arguments and adjuncts in sentence composition. We argue that this information can be used to compute what we call the factor expression likelihood (FEL) associated with a verb in a sentence. This is the likelihood that the overt syntactic expression of a factor will cooccur with the verb. This has consequences for the compositional mechanisms responsible for computing the meaning of the sentence, as well as significance in the creation of computational models attempting to capture linguistic behavior over large corpora.</abstract>
      <issue>1</issue>
      <url hash="1bb480bf">2017.lilt-15.1</url>
      <bibkey>pustejovsky-joshi-2017-lexical</bibkey>
    </paper>
    <paper id="2">
      <title>Factorization of Verbs: An Analysis of Verbs of Seeing</title>
      <author><first>Aravind</first><last>Joshi</last></author>
      <issue>1</issue>
      <url hash="5d79d50a">2017.lilt-15.2</url>
      <bibkey>joshi-2017-factorization</bibkey>
    </paper>
    <paper id="3">
      <title>Using Deep Neural Networks to Learn Syntactic Agreement</title>
      <author><first>Jean-Phillipe</first><last>Bernardy</last></author>
      <author><first>Shalom</first><last>Lappin</last></author>
      <abstract>We consider the extent to which different deep neural network (DNN) configurations can learn syntactic relations, by taking up Linzen et al.’s (2016) work on subject-verb agreement with LSTM RNNs. We test their methods on a much larger corpus than they used (a ⇠24 million example part of the WaCky corpus, instead of their ⇠1.35 million example corpus, both drawn from Wikipedia). We experiment with several different DNN architectures (LSTM RNNs, GRUs, and CNNs), and alternative parameter settings for these systems (vocabulary size, training to test ratio, number of layers, memory size, drop out rate, and lexical embedding dimension size). We also try out our own unsupervised DNN language model. Our results are broadly compatible with those that Linzen et al. report. However, we discovered some interesting, and in some cases, surprising features of DNNs and language models in their performance of the agreement learning task. In particular, we found that DNNs require large vocabularies to form substantive lexical embeddings in order to learn structural patterns. This finding has interesting consequences for our understanding of the way in which DNNs represent syntactic information. It suggests that DNNs learn syntactic patterns more efficiently through rich lexical embeddings, with semantic as well as syntactic cues, than from training on lexically impoverished strings that highlight structural patterns.</abstract>
      <issue>2</issue>
      <url hash="8a8a83f1">2017.lilt-15.3</url>
      <bibkey>bernardy-lappin-2017-using</bibkey>
    </paper>
    <paper id="4">
      <title>Dynamic Argument Structure</title>
      <author><first>Elisabetta</first><last>Jezek</last></author>
      <abstract>This paper presents a new classification of verbs of change and modification, proposing a dynamic interpretation of the lexical semantics of the predicate and its arguments. Adopting the model of dynamic event structure proposed in Pustejovsky (2013), and extending the model of dynamic selection outlined in Pustejovsky and Jezek (2011), we define a verb class in terms of its Dynamic Argument Structure (DAS), a representation which encodes how the participants involved in the change behave as the event unfolds. We address how the logical resources and results of change predicates are realized syntactically, if at all, as well as how the exploitation of the resource results in the initiation or termination of a new object, i.e. the result. We show how DAS can be associated with a dynamically encoded event structure representation, which measures the change making reference to a scalar component, modelled in terms of assignment and/or testing of values of attributes of participants.</abstract>
      <issue>3</issue>
      <url hash="bbb8f510">2017.lilt-15.4</url>
      <bibkey>jezek-2017-dynamic</bibkey>
    </paper>
  </volume>
</collection>
