<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.dlnld">
  <volume id="1" ingest-date="2024-05-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop on Deep Learning and Linked Data (DLnLD) @ LREC-COLING 2024</booktitle>
      <editor><first>Gilles</first><last>Sérasset</last></editor>
      <editor><first>Hugo Gonçalo</first><last>Oliveira</last></editor>
      <editor><first>Giedre Valunaite</first><last>Oleskeviciene</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="c81ea098">2024.dlnld-1</url>
      <venue>dlnld</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="2a4fdbd7">2024.dlnld-1.0</url>
      <bibkey>dlnld-2024-deep</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Investigating the Impact of Different Graph Representations for Relation Extraction with Graph Neural Networks</title>
      <author><first>Moritz</first><last>Blum</last></author>
      <author><first>Gennaro</first><last>Nolano</last></author>
      <author><first>Basil</first><last>Ell</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <pages>1–13</pages>
      <abstract>Graph Neural Networks(GNNs) have been applied successfully to various NLP tasks, particularly Relation Extraction(RE). Even though most of these approaches rely on the syntactic dependency tree of a sentence to derive a graph representation, the impact of this choice compared to other possible graph representations has not been evaluated. We examine the effect of representing text though a graph of different graph representations for GNNs that are applied to RE, considering, e.g., a fully connected graph of tokens, of semantic role structures, and combinations thereof. We further examine the impact of background knowledge injection from Knowledge Graphs(KGs) into the graph representation to achieve enhanced graph representations. Our results show that combining multiple graph representations can improve the model’s predictions. Moreover, the integration of background knowledge positively impacts scores, as enhancing the text graphs with Wikidata features or WordNet features can lead to an improvement of close to 0.1 points in F1.</abstract>
      <url hash="f39c3c41">2024.dlnld-1.1</url>
      <bibkey>blum-etal-2024-investigating</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>T</fixed-case>axo<fixed-case>C</fixed-case>ritic: Exploring Credit Assignment in Taxonomy Induction with Multi-Critic Reinforcement Learning</title>
      <author><first>Injy</first><last>Sarhan</last></author>
      <author><first>Bendegúz</first><last>Toth</last></author>
      <author><first>Pablo</first><last>Mosteiro</last></author>
      <author><first>Shihan</first><last>Wang</last></author>
      <pages>14–30</pages>
      <abstract>Taxonomies can serve as a vital foundation for several downstream tasks such as information retrieval and question answering, yet manual construction limits coverage and full potential. Automatic taxonomy induction, particularly using deep Reinforcement Learning (RL), is underexplored in Natural Language Processing (NLP). To address this gap, we present TaxoCritic, a novel approach that leverages deep multi-critic RL agents for taxonomy induction while incorporating credit assignment mechanisms. Our system uniquely assesses different sub-actions within the induction process, providing a granular analysis that aids in the precise attribution of credit and blame. We evaluate the effectiveness of multi-critic algorithms in experiments regarding both accuracy and robustness performance in edge identification. By providing a detailed comparison with state-of-the-art models and highlighting the strengths and limitations of our method, we aim to contribute to the ongoing</abstract>
      <url hash="9e15a68e">2024.dlnld-1.2</url>
      <bibkey>sarhan-etal-2024-taxocritic</bibkey>
    </paper>
    <paper id="3">
      <title>Combining Deep Learning Models and Lexical Linked Data: Some Insights from the Development of a Multilingual News Named Entity Recognition and Linking Dataset</title>
      <author><first>Emmanuel</first><last>Cartier</last></author>
      <author><first>Emile</first><last>Peetermans</last></author>
      <pages>31–44</pages>
      <abstract>This paper presents the methodology and outcomes of a Named Entity Recognition and Linking multilingual news benchmark that leverages both Deep learning approaches by using a fine-tuned transformer model to detect mentions of persons, locations and organisations in text, and Linguistic Linked Open Data, through the use of Wikidata to disambiguate mentions and link them to ontology entries. It shows all the advantages of combining both approaches, not only for building the benchmark but also for fine-tuning detection models. We also insist on several perspectives of research to improve the accuracy of a combining system and go further on leveraging the complementary approaches.</abstract>
      <url hash="d8da1006">2024.dlnld-1.3</url>
      <bibkey>cartier-peetermans-2024-combining</bibkey>
    </paper>
    <paper id="4">
      <title>Deductive Verification of <fixed-case>LLM</fixed-case> Generated <fixed-case>SPARQL</fixed-case> Queries</title>
      <author><first>Alexandre</first><last>Rademaker</last></author>
      <author><first>Guilherme</first><last>Lima</last></author>
      <author><first>Sandro Rama</first><last>Fiorini</last></author>
      <author><first>Viviane Torres</first><last>da Silva</last></author>
      <pages>45–52</pages>
      <abstract>Considering the increasing applications of Large Language Models (LLMs) to many natural language tasks, this paper presents preliminary findings on developing a verification component for detecting hallucinations of an LLM that produces SPARQL queries from natural language questions. We suggest a logic-based deductive verification of the generated SPARQL query by checking if the original NL question’s deep semantic representation entails the SPARQL’s semantic representation.</abstract>
      <url hash="2e536dc4">2024.dlnld-1.4</url>
      <bibkey>rademaker-etal-2024-deductive</bibkey>
    </paper>
    <paper id="5">
      <title>How to Turn Card Catalogs into <fixed-case>LLM</fixed-case> Fodder</title>
      <author><first>Mary Ann</first><last>Tan</last></author>
      <author><first>Shufan</first><last>Jiang</last></author>
      <author><first>Harald</first><last>Sack</last></author>
      <pages>53–65</pages>
      <abstract>Bibliographical metadata collections describing pre-modern objects suffer from incompleteness and inaccuracies. This hampers the identification of literary works. In addition, titles often contain voluminous descriptive texts that do not adhere to contemporary title conventions. This paper explores several NLP approaches where greater textual length in titles is leveraged to enhance descriptive information.</abstract>
      <url hash="d7bee583">2024.dlnld-1.5</url>
      <bibkey>tan-etal-2024-turn</bibkey>
    </paper>
    <paper id="6">
      <title>Evaluating Large Language Models for Linguistic Linked Data Generation</title>
      <author><first>Maria Pia</first><last>di Buono</last></author>
      <author><first>Blerina</first><last>Spahiu</last></author>
      <author><first>Verginica</first><last>Barbu Mititelu</last></author>
      <pages>66–75</pages>
      <abstract>Large language models (LLMs) have revolutionized human-machine interaction with their ability to converse and perform various language tasks. This study investigates the potential of LLMs for knowledge formalization using well-defined vocabularies, specifically focusing on OntoLex-Lemon. As a preliminary exploration, we test four languages (English, Italian, Albanian, Romanian) and analyze the formalization quality of nine words with varying characteristics applying a multidimensional evaluation approach. While manual validation provided initial insights, it highlights the need for developing scalable evaluation methods for future large-scale experiments. This research aims to initiate a discussion on the potential and challenges of utilizing LLMs for knowledge formalization within the Semantic Web framework.</abstract>
      <url hash="0841a12e">2024.dlnld-1.6</url>
      <bibkey>di-buono-etal-2024-evaluating</bibkey>
    </paper>
    <paper id="7">
      <title>Towards Automated Evaluation of Knowledge Encoded in Large Language Models</title>
      <author><first>Bruno Carlos Luís</first><last>Ferreira</last></author>
      <author><first>Catarina</first><last>Silva</last></author>
      <author><first>Hugo</first><last>Gonçalo Oliveira</last></author>
      <pages>76–85</pages>
      <abstract>Large Language Models (LLMs) have a significant user base and are gaining increasing interest and impact across various domains. Given their expanding influence, it is crucial to implement appropriate guardrails or controls to ensure ethical and responsible use. In this paper, we propose to automate the evaluation of the knowledge stored in LLMs. This is achieved by generating datasets tailored for this specific purpose, in any selected domain. Our approach consists of four major steps: (i) extraction of relevant entities; (ii) gathering of domain properties; (iii) dataset generation; and (iv) model evaluation. In order to materialize this vision, tools and resources were experimented for entity linking, knowledge acquisition, classification and prompt generation, yielding valuable insights and lessons. The generation of datasets for domain specific model evaluation has successfully proved that the approach can be a future tool for evaluating and moving LLMs “black-boxes” to human-interpretable knowledge bases.</abstract>
      <url hash="cf1143d5">2024.dlnld-1.7</url>
      <bibkey>ferreira-etal-2024-towards</bibkey>
    </paper>
    <paper id="8">
      <title>Self-Evaluation of Generative <fixed-case>AI</fixed-case> Prompts for Linguistic Linked Open Data Modelling in Diachronic Analysis</title>
      <author><first>Florentina</first><last>Armaselu</last></author>
      <author><first>Chaya</first><last>Liebeskind</last></author>
      <author><first>Giedre</first><last>Valunaite Oleskeviciene</last></author>
      <pages>86–91</pages>
      <abstract>This article addresses the question of evaluating generative AI prompts designed for specific tasks such as linguistic linked open data modelling and refining of word embedding results. The prompts were created to assist the pre-modelling phase in the construction of LLODIA, a linguistic linked open data model for diachronic analysis. We present a self-evaluation framework based on the method known in literature as LLM-Eval. The discussion includes prompts related to the RDF-XML conception of the model, and neighbour list refinement, dictionary alignment and contextualisation for the term revolution in French, Hebrew and Lithuanian, as a proof of concept.</abstract>
      <url hash="a952352e">2024.dlnld-1.8</url>
      <bibkey>armaselu-etal-2024-self</bibkey>
    </paper>
  </volume>
</collection>
