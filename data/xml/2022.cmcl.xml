<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.cmcl">
  <volume id="1" ingest-date="2022-05-15">
    <meta>
      <booktitle>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</booktitle>
      <editor><first>Emmanuele</first><last>Chersoni</last></editor>
      <editor><first>Nora</first><last>Hollenstein</last></editor>
      <editor><first>Cassandra</first><last>Jacobs</last></editor>
      <editor><first>Yohei</first><last>Oseki</last></editor>
      <editor><first>Laurent</first><last>Prévot</last></editor>
      <editor><first>Enrico</first><last>Santus</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="0281f3b4">2022.cmcl-1</url>
      <venue>cmcl</venue>
    </meta>
    <frontmatter>
      <url hash="b6c3a2f3">2022.cmcl-1.0</url>
      <bibkey>cmcl-2022-cognitive</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Seeing the advantage: visually grounding word embeddings to better capture human semantic knowledge</title>
      <author><first>Danny</first><last>Merkx</last></author>
      <author><first>Stefan</first><last>Frank</last></author>
      <author><first>Mirjam</first><last>Ernestus</last></author>
      <pages>1-11</pages>
      <abstract>Distributional semantic models capture word-level meaning that is useful in many natural language processing tasks and have even been shown to capture cognitive aspects of word meaning. The majority of these models are purely text based, even though the human sensory experience is much richer. In this paper we create visually grounded word embeddings by combining English text and images and compare them to popular text-based methods, to see if visual information allows our model to better capture cognitive aspects of word meaning. Our analysis shows that visually grounded embedding similarities are more predictive of the human reaction times in a large priming experiment than the purely text-based embeddings. The visually grounded embeddings also correlate well with human word similarity ratings. Importantly, in both experiments we show that the grounded embeddings account for a unique portion of explained variance, even when we include text-based embeddings trained on huge corpora. This shows that visual grounding allows our model to capture information that cannot be extracted using text as the only source of information.</abstract>
      <url hash="febc495c">2022.cmcl-1.1</url>
      <bibkey>merkx-etal-2022-seeing</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.1</doi>
      <video href="2022.cmcl-1.1.mp4"/>
      <pwccode url="https://github.com/DannyMerkx/speech2image" additional="false">DannyMerkx/speech2image</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="2">
      <title>A Neural Model for Compositional Word Embeddings and Sentence Processing</title>
      <author><first>Shalom</first><last>Lappin</last></author>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <pages>12-22</pages>
      <abstract>We propose a new neural model for word embeddings, which uses Unitary Matrices as the primary device for encoding lexical information. It uses simple matrix multiplication to derive matrices for large units, yielding a sentence processing model that is strictly compositional, does not lose information over time steps, and is transparent, in the sense that word embeddings can be analysed regardless of context. This model does not employ activation functions, and so the network is fully accessible to analysis by the methods of linear algebra at each point in its operation on an input sequence. We test it in two NLP agreement tasks and obtain rule like perfect accuracy, with greater stability than current state-of-the-art systems. Our proposed model goes some way towards offering a class of computationally powerful deep learning systems that can be fully understood and compared to human cognitive processes for natural language learning and representation.</abstract>
      <url hash="74b017db">2022.cmcl-1.2</url>
      <bibkey>lappin-bernardy-2022-neural</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.2</doi>
      <video href="2022.cmcl-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Visually Grounded Interpretation of Noun-Noun Compounds in <fixed-case>E</fixed-case>nglish</title>
      <author><first>Inga</first><last>Lang</last></author>
      <author><first>Lonneke</first><last>Plas</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <pages>23-35</pages>
      <abstract>Noun-noun compounds (NNCs) occur frequently in the English language. Accurate NNC interpretation, i.e. determining the implicit relationship between the constituents of a NNC, is crucial for the advancement of many natural language processing tasks. Until now, computational NNC interpretation has been limited to approaches involving linguistic representations only. However, much research suggests that grounding linguistic representations in vision or other modalities can increase performance on this and other tasks. Our work is a novel comparison of linguistic and visuo-linguistic representations for the task of NNC interpretation. We frame NNC interpretation as a relation classification task, evaluating on a large, relationally-annotated NNC dataset. We combine distributional word vectors with image vectors to investigate how visual information can help improve NNC interpretation systems. We find that adding visual vectors increases classification performance on our dataset in many cases.</abstract>
      <url hash="70b87ec6">2022.cmcl-1.3</url>
      <bibkey>lang-etal-2022-visually</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.3</doi>
      <video href="2022.cmcl-1.3.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="4">
      <title>Less Descriptive yet Discriminative: Quantifying the Properties of Multimodal Referring Utterances via <fixed-case>CLIP</fixed-case></title>
      <author><first>Ece</first><last>Takmaz</last></author>
      <author><first>Sandro</first><last>Pezzelle</last></author>
      <author><first>Raquel</first><last>Fernández</last></author>
      <pages>36-42</pages>
      <abstract>In this work, we use a transformer-based pre-trained multimodal model, CLIP, to shed light on the mechanisms employed by human speakers when referring to visual entities. In particular, we use CLIP to quantify the degree of descriptiveness (how well an utterance describes an image in isolation) and discriminativeness (to what extent an utterance is effective in picking out a single image among similar images) of human referring utterances within multimodal dialogues. Overall, our results show that utterances become less descriptive over time while their discriminativeness remains unchanged. Through analysis, we propose that this trend could be due to participants relying on the previous mentions in the dialogue history, as well as being able to distill the most discriminative information from the visual context. In general, our study opens up the possibility of using this and similar models to quantify patterns in human data and shed light on the underlying cognitive mechanisms.</abstract>
      <url hash="f6e0cfdc">2022.cmcl-1.4</url>
      <bibkey>takmaz-etal-2022-less</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.4</doi>
      <video href="2022.cmcl-1.4.mp4"/>
      <pwccode url="https://github.com/ecekt/clip-desc-disc" additional="false">ecekt/clip-desc-disc</pwccode>
    </paper>
    <paper id="5">
      <title>Codenames as a Game of Co-occurrence Counting</title>
      <author><first>Réka</first><last>Cserháti</last></author>
      <author><first>Istvan</first><last>Kollath</last></author>
      <author><first>András</first><last>Kicsi</last></author>
      <author><first>Gábor</first><last>Berend</last></author>
      <pages>43-53</pages>
      <abstract>Codenames is a popular board game, in which knowledge and cooperation between players play an important role. The task of a player playing as a spymaster is to find words (clues) that a teammate finds related to as many of some given words as possible, but not to other specified words. This is a hard challenge even with today’s advanced language technology methods.In our study, we create spymaster agents using four types of relatedness measures that require only a raw text corpus to produce. These include newly introduced ones based on co-occurrences, which outperform FastText cosine similarity on gold standard relatedness data. To generate clues in Codenames, we combine relatedness measures with four different scoring functions, for two languages, English and Hungarian. For testing, we collect decisions of human guesser players in an online game, and our configurations outperform previous agents among methods using raw corpora only.</abstract>
      <url hash="40b217b3">2022.cmcl-1.5</url>
      <bibkey>cserhati-etal-2022-codenames</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.5</doi>
      <video href="2022.cmcl-1.5.mp4"/>
      <pwccode url="https://github.com/xerevity/codenamesagent" additional="false">xerevity/codenamesagent</pwccode>
    </paper>
    <paper id="6">
      <title>Estimating word co-occurrence probabilities from pretrained static embeddings using a log-bilinear model</title>
      <author><first>Richard</first><last>Futrell</last></author>
      <pages>54-60</pages>
      <abstract>We investigate how to use pretrained static word embeddings to deliver improved estimates of bilexical co-occurrence probabilities: conditional probabilities of one word given a single other word in a specific relationship. Such probabilities play important roles in psycholinguistics, corpus linguistics, and usage-based cognitive modeling of language more generally. We propose a log-bilinear model taking pretrained vector representations of the two words as input, enabling generalization based on the distributional information contained in both vectors. We show that this model outperforms baselines in estimating probabilities of adjectives given nouns that they attributively modify, and probabilities of nominal direct objects given their head verbs, given limited training data in Arabic, English, Korean, and Spanish.</abstract>
      <url hash="30e183b7">2022.cmcl-1.6</url>
      <bibkey>futrell-2022-estimating</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.6</doi>
      <video href="2022.cmcl-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Modeling the Relationship between Input Distributions and Learning Trajectories with the Tolerance Principle</title>
      <author><first>Jordan</first><last>Kodner</last></author>
      <pages>61-67</pages>
      <abstract>Child language learners develop with remarkable uniformity, both in their learning trajectories and ultimate outcomes, despite major differences in their learning environments. In this paper, we explore the role that the frequencies and distributions of irregular lexical items in the input plays in driving learning trajectories. We conclude that while the Tolerance Principle, a type-based model of productivity learning, accounts for inter-learner uniformity, it also interacts with input distributions to drive cross-linguistic variation in learning trajectories.</abstract>
      <url hash="e9b22efe">2022.cmcl-1.7</url>
      <bibkey>kodner-2022-modeling</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.7</doi>
      <video href="2022.cmcl-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Predicting scalar diversity with context-driven uncertainty over alternatives</title>
      <author><first>Jennifer</first><last>Hu</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <author><first>Sebastian</first><last>Schuster</last></author>
      <pages>68-74</pages>
      <abstract>Scalar implicature (SI) arises when a speaker uses an expression (e.g., “some”) that is semantically compatible with a logically stronger alternative on the same scale (e.g., “all”), leading the listener to infer that they did not intend to convey the stronger meaning. Prior work has demonstrated that SI rates are highly variable across scales, raising the question of what factors determine the SI strength for a particular scale. Here, we test the hypothesis that SI rates depend on the listener’s confidence in the underlying scale, which we operationalize as uncertainty over the distribution of possible alternatives conditioned on the context. We use a T5 model fine-tuned on a text infilling task to estimate this distribution. We find that scale uncertainty predicts human SI rates, measured as entropy over the sampled alternatives and over latent classes among alternatives in sentence embedding space. Furthermore, we do not find a significant effect of the surprisal of the strong scalemate. Our results suggest that pragmatic inferences depend on listeners’ context-driven uncertainty over alternatives.</abstract>
      <url hash="f2351c8d">2022.cmcl-1.8</url>
      <bibkey>hu-etal-2022-predicting</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.8</doi>
      <video href="2022.cmcl-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences</title>
      <author><first>Joshua</first><last>Bensemann</last></author>
      <author><first>Alex</first><last>Peng</last></author>
      <author><first>Diana</first><last>Benavides-Prado</last></author>
      <author><first>Yang</first><last>Chen</last></author>
      <author><first>Neset</first><last>Tan</last></author>
      <author><first>Paul Michael</first><last>Corballis</last></author>
      <author><first>Patricia</first><last>Riddle</last></author>
      <author><first>Michael</first><last>Witbrock</last></author>
      <pages>75-87</pages>
      <abstract>Attention describes cognitive processes that are important to many human phenomena including reading. The term is also used to describe the way in which transformer neural networks perform natural language processing. While attention appears to be very different under these two contexts, this paper presents an analysis of the correlations between transformer attention and overt human attention during reading tasks. An extensive analysis of human eye tracking datasets showed that the dwell times of human eye movements were strongly correlated with the attention patterns occurring in the early layers of pre-trained transformers such as BERT. Additionally, the strength of a correlation was not related to the number of parameters within a transformer. This suggests that something about the transformers’ architecture determined how closely the two measures were correlated.</abstract>
      <url hash="01d4f289">2022.cmcl-1.9</url>
      <bibkey>bensemann-etal-2022-eye</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.9</doi>
      <video href="2022.cmcl-1.9.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/movieqa">MovieQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="10">
      <title>About Time: Do Transformers Learn Temporal Verbal Aspect?</title>
      <author><first>Eleni</first><last>Metheniti</last></author>
      <author><first>Tim</first><last>Van De Cruys</last></author>
      <author><first>Nabil</first><last>Hathout</last></author>
      <pages>88-101</pages>
      <abstract>Aspect is a linguistic concept that describes how an action, event, or state of a verb phrase is situated in time. In this paper, we explore whether different transformer models are capable of identifying aspectual features. We focus on two specific aspectual features: telicity and duration. Telicity marks whether the verb’s action or state has an endpoint or not (telic/atelic), and duration denotes whether a verb expresses an action (dynamic) or a state (stative). These features are integral to the interpretation of natural language, but also hard to annotate and identify with NLP methods. We perform experiments in English and French, and our results show that transformer models adequately capture information on telicity and duration in their vectors, even in their non-finetuned forms, but are somewhat biased with regard to verb tense and word order.</abstract>
      <url hash="fae10edd">2022.cmcl-1.10</url>
      <bibkey>metheniti-etal-2022-time</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.10</doi>
      <video href="2022.cmcl-1.10.mp4"/>
      <pwccode url="https://github.com/lenakmeth/telicity_classification" additional="false">lenakmeth/telicity_classification</pwccode>
    </paper>
    <paper id="11">
      <title>Poirot at <fixed-case>CMCL</fixed-case> 2022 Shared Task: Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models</title>
      <author><first>Harshvardhan</first><last>Srivastava</last></author>
      <pages>102-107</pages>
      <abstract>Eye tracking data during reading is a useful source of information to understand the cognitive processes that take place during language comprehension processes. Different languages account for different cognitive triggers, however there seems to be some uniform indicatorsacross languages. In this paper, we describe our submission to the CMCL 2022 shared task on predicting human reading patterns for multi-lingual dataset. Our model uses text representations from transformers and some hand engineered features with a regression layer on top to predict statistical measures of mean and standard deviation for 2 main eye-tracking features. We train an end-to-end model to extract meaningful information from different languages and test our model on two separate datasets. We compare different transformer models andshow ablation studies affecting model performance. Our final submission ranked 4th place for SubTask-1 and 1st place for SubTask-2 forthe shared task.</abstract>
      <url hash="2c12548e">2022.cmcl-1.11</url>
      <bibkey>srivastava-2022-poirot</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.11</doi>
      <video href="2022.cmcl-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title><fixed-case>NU</fixed-case> <fixed-case>HLT</fixed-case> at <fixed-case>CMCL</fixed-case> 2022 Shared Task: Multilingual and Crosslingual Prediction of Human Reading Behavior in Universal Language Space</title>
      <author><first>Joseph Marvin</first><last>Imperial</last></author>
      <pages>108-113</pages>
      <abstract>In this paper, we present a unified model that works for both multilingual and crosslingual prediction of reading times of words in various languages. The secret behind the success of this model is in the preprocessing step where all words are transformed to their universal language representation via the International Phonetic Alphabet (IPA). To the best of our knowledge, this is the first study to favorably exploit this phonological property of language for the two tasks. Various feature types were extracted covering basic frequencies, n-grams, information theoretic, and psycholinguistically-motivated predictors for model training. A finetuned Random Forest model obtained best performance for both tasks with 3.8031 and 3.9065 MAE scores for mean first fixation duration (FFDAvg) and mean total reading time (TRTAvg) respectively.</abstract>
      <url hash="8bd67a19">2022.cmcl-1.12</url>
      <bibkey>imperial-2022-nu</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.12</doi>
      <revision id="1" href="2022.cmcl-1.12v1" hash="69adb5a8"/>
      <revision id="2" href="2022.cmcl-1.12v2" hash="8bd67a19" date="2022-10-27">Fixed Acknowledgments.</revision>
      <video href="2022.cmcl-1.12.mp4"/>
      <pwccode url="https://github.com/imperialite/cmcl2022-unified-eye-tracking-ipa" additional="false">imperialite/cmcl2022-unified-eye-tracking-ipa</pwccode>
    </paper>
    <paper id="13">
      <title><fixed-case>H</fixed-case>k<fixed-case>A</fixed-case>msters at <fixed-case>CMCL</fixed-case> 2022 Shared Task: Predicting Eye-Tracking Data from a Gradient Boosting Framework with Linguistic Features</title>
      <author><first>Lavinia</first><last>Salicchi</last></author>
      <author><first>Rong</first><last>Xiang</last></author>
      <author><first>Yu-Yin</first><last>Hsu</last></author>
      <pages>114-120</pages>
      <abstract>Eye movement data are used in psycholinguistic studies to infer information regarding cognitive processes during reading. In this paper, we describe our proposed method for the Shared Task of Cognitive Modeling and Computational Linguistics (CMCL) 2022 - Subtask 1, which involves data from multiple datasets on 6 languages. We compared different regression models using features of the target word and its previous word, and target word surprisal as regression features. Our final system, using a gradient boosting regressor, achieved the lowest mean absolute error (MAE), resulting in the best system of the competition.</abstract>
      <url hash="d3e6acde">2022.cmcl-1.13</url>
      <bibkey>salicchi-etal-2022-hkamsters</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.13</doi>
      <video href="2022.cmcl-1.13.mp4"/>
    </paper>
    <paper id="14">
      <title><fixed-case>CMCL</fixed-case> 2022 Shared Task on Multilingual and Crosslingual Prediction of Human Reading Behavior</title>
      <author><first>Nora</first><last>Hollenstein</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Cassandra</first><last>Jacobs</last></author>
      <author><first>Yohei</first><last>Oseki</last></author>
      <author><first>Laurent</first><last>Prévot</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <pages>121-129</pages>
      <abstract>We present the second shared task on eye-tracking data prediction of the Cognitive Modeling and Computational Linguistics Workshop (CMCL). Differently from the previous edition, participating teams are asked to predict eye-tracking features from multiple languages, including a surprise language for which there were no available training data. Moreover, the task also included the prediction of standard deviations of feature values in order to account for individual differences between readers.A total of six teams registered to the task. For the first subtask on multilingual prediction, the winning team proposed a regression model based on lexical features, while for the second subtask on cross-lingual prediction, the winning team used a hybrid model based on a multilingual transformer embeddings as well as statistical features.</abstract>
      <url hash="3eb0c7f0">2022.cmcl-1.14</url>
      <bibkey>hollenstein-etal-2022-cmcl</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.14</doi>
      <video href="2022.cmcl-1.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Team <fixed-case>ÚFAL</fixed-case> at <fixed-case>CMCL</fixed-case> 2022 Shared Task: Figuring out the correct recipe for predicting Eye-Tracking features using Pretrained Language Models</title>
      <author><first>Sunit</first><last>Bhattacharya</last></author>
      <author><first>Rishu</first><last>Kumar</last></author>
      <author><first>Ondrej</first><last>Bojar</last></author>
      <pages>130-135</pages>
      <abstract>Eye-Tracking data is a very useful source of information to study cognition and especially language comprehension in humans. In this paper, we describe our systems for the CMCL 2022 shared task on predicting eye-tracking information. We describe our experiments withpretrained models like BERT and XLM and the different ways in which we used those representations to predict four eye-tracking features. Along with analysing the effect of using two different kinds of pretrained multilingual language models and different ways of pooling the token-level representations, we also explore how contextual information affects the performance of the systems. Finally, we also explore if factors like augmenting linguistic information affect the predictions. Our submissions achieved an average MAE of 5.72 and ranked 5th in the shared task. The average MAE showed further reduction to 5.25 in post task evaluation.</abstract>
      <url hash="69bed3a8">2022.cmcl-1.15</url>
      <bibkey>bhattacharya-etal-2022-team</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.15</doi>
      <video href="2022.cmcl-1.15.mp4"/>
    </paper>
    <paper id="16">
      <title>Team <fixed-case>DMG</fixed-case> at <fixed-case>CMCL</fixed-case> 2022 Shared Task: Transformer Adapters for the Multi- and Cross-Lingual Prediction of Human Reading Behavior</title>
      <author><first>Ece</first><last>Takmaz</last></author>
      <pages>136-144</pages>
      <abstract>In this paper, we present the details of our approaches that attained the second place in the shared task of the ACL 2022 Cognitive Modeling and Computational Linguistics Workshop. The shared task is focused on multi- and cross-lingual prediction of eye movement features in human reading behavior, which could provide valuable information regarding language processing. To this end, we train ‘adapters’ inserted into the layers of frozen transformer-based pretrained language models. We find that multilingual models equipped with adapters perform well in predicting eye-tracking features. Our results suggest that utilizing language- and task-specific adapters is beneficial and translating test sets into similar languages that exist in the training set could help with zero-shot transferability in the prediction of human reading behavior.</abstract>
      <url hash="d310f9d7">2022.cmcl-1.16</url>
      <bibkey>takmaz-2022-team</bibkey>
      <doi>10.18653/v1/2022.cmcl-1.16</doi>
      <video href="2022.cmcl-1.16.mp4"/>
    </paper>
  </volume>
</collection>
