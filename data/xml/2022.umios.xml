<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.umios">
  <volume id="1" ingest-date="2022-12-15" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop on Unimodal and Multimodal Induction of Linguistic Structures (UM-IoS)</booktitle>
      <editor><first>Wenjuan</first><last>Han</last></editor>
      <editor><first>Zilong</first><last>Zheng</last></editor>
      <editor><first>Zhouhan</first><last>Lin</last></editor>
      <editor><first>Lifeng</first><last>Jin</last></editor>
      <editor><first>Yikang</first><last>Shen</last></editor>
      <editor><first>Yoon</first><last>Kim</last></editor>
      <editor><first>Kewei</first><last>Tu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, United Arab Emirates (Hybrid)</address>
      <month>December</month>
      <year>2022</year>
      <url hash="73bb1526">2022.umios-1</url>
      <venue>umios</venue>
    </meta>
    <frontmatter>
      <url hash="8e1f0021">2022.umios-1.0</url>
      <bibkey>um-ios-2022-unimodal</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Named Entity Recognition as Structured Span Prediction</title>
      <author><first>Urchade</first><last>Zaratiana</last><affiliation>FI Group</affiliation></author>
      <author><first>Nadi</first><last>Tomeh</last><affiliation>LIPN, CNRS UMR 7030, France</affiliation></author>
      <author><first>Pierre</first><last>Holat</last><affiliation>FI Group</affiliation></author>
      <author><first>Thierry</first><last>Charnois</last><affiliation>LIPN, CNRS UMR 7030, France</affiliation></author>
      <pages>1-10</pages>
      <abstract>Named Entity Recognition (NER) is an important task in Natural Language Processing with applications in many domains. While the dominant paradigm of NER is sequence labelling, span-based approaches have become very popular in recent times but are less well understood. In this work, we study different aspects of span-based NER, namely the span representation, learning strategy, and decoding algorithms to avoid span overlap. We also propose an exact algorithm that efficiently finds the set of non-overlapping spans that maximizes a global score, given a list of candidate spans. We performed our study on three benchmark NER datasets from different domains. We make our code publicly available at <url>https://github.com/urchade/span-structured-prediction</url>.</abstract>
      <url hash="76929576">2022.umios-1.1</url>
      <bibkey>zaratiana-etal-2022-named</bibkey>
      <doi>10.18653/v1/2022.umios-1.1</doi>
    </paper>
    <paper id="2">
      <title>Global Span Selection for Named Entity Recognition</title>
      <author><first>Urchade</first><last>Zaratiana</last><affiliation>FI Group</affiliation></author>
      <author><first>Niama</first><last>El Elkhbir</last><affiliation>LIPN, CNRS UMR 7030, France</affiliation></author>
      <author><first>Pierre</first><last>Holat</last><affiliation>FI Group</affiliation></author>
      <author><first>Nadi</first><last>Tomeh</last><affiliation>LIPN, CNRS UMR 7030, France</affiliation></author>
      <author><first>Thierry</first><last>Charnois</last><affiliation>LIPN, CNRS UMR 7030, France</affiliation></author>
      <pages>11-17</pages>
      <abstract>Named Entity Recognition (NER) is an important task in Natural Language Processing with applications in many domains. In this paper, we describe a novel approach to named entity recognition, in which we output a set of spans (i.e., segmentations) by maximizing a global score. During training, we optimize our model by maximizing the probability of the gold segmentation. During inference, we use dynamic programming to select the best segmentation under a linear time complexity. We prove that our approach outperforms CRF and semi-CRF models for Named Entity Recognition. We will make our code publicly available.</abstract>
      <url hash="1578596e">2022.umios-1.2</url>
      <bibkey>zaratiana-etal-2022-global</bibkey>
      <video href="2022.umios-1.2.mp4"/>
      <doi>10.18653/v1/2022.umios-1.2</doi>
    </paper>
    <paper id="3">
      <title>Visual Grounding of Inter-lingual Word-Embeddings</title>
      <author><first>Wafaa</first><last>Mohammed</last><affiliation>University of Tübingen</affiliation></author>
      <author><first>Hassan</first><last>Shahmohammadi</last><affiliation>University of Tübingen</affiliation></author>
      <author><first>Hendrik P. A.</first><last>Lensch</last><affiliation>University of Tübingen</affiliation></author>
      <author><first>R. Harald</first><last>Baayen</last><affiliation>University of Tübingen</affiliation></author>
      <pages>18-28</pages>
      <abstract>Visual grounding of Language aims at enriching textual representations of language with multiple sources of visual knowledge such as images and videos. Although visual grounding is an area of intense research, inter-lingual aspects of visual grounding have not received much attention. The present study investigates the inter-lingual visual grounding of word embeddings. We propose an implicit alignment technique between the two spaces of vision and language in which inter-lingual textual information interacts in order to enrich pre-trained textual word embeddings. We focus on three languages in our experiments, namely, English, Arabic, and German. We obtained visually grounded vector representations for these languages and studied whether visual grounding on one or multiple languages improved the performance of embeddings on word similarity and categorization benchmarks. Our experiments suggest that inter-lingual knowledge improves the performance of grounded embeddings in similar languages such as German and English. However, inter-lingual grounding of German or English with Arabic led to a slight degradation in performance on word similarity benchmarks. On the other hand, we observed an opposite trend on categorization benchmarks where Arabic had the most improvement on English. In the discussion section, several reasons for those findings are laid out. We hope that our experiments provide a baseline for further research on inter lingual visual grounding.</abstract>
      <url hash="0dbcdd3a">2022.umios-1.3</url>
      <bibkey>mohammed-etal-2022-visual</bibkey>
      <doi>10.18653/v1/2022.umios-1.3</doi>
    </paper>
    <paper id="4">
      <title>A Subspace-Based Analysis of Structured and Unstructured Representations in Image-Text Retrieval</title>
      <author><first>Erica K.</first><last>Shimomoto</last><affiliation>National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Edison</first><last>Marrese-Taylor</last><affiliation>National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Ichiro</first><last>Kobayashi</last><affiliation>National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Yusuke</first><last>Miyao</last><affiliation>National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>29-44</pages>
      <abstract>In this paper, we specifically look at the image-text retrieval problem. Recent multimodal frameworks have shown that structured inputs and fine-tuning lead to consistent performance improvement. However, this paradigm has been challenged recently with newer Transformer-based models that can reach zero-shot state-of-the-art results despite not explicitly using structured data during pre-training. Since such strategies lead to increased computational resources, we seek to better understand their role in image-text retrieval by analyzing visual and text representations extracted with three multimodal frameworks – SGM, UNITER, and CLIP. To perform such analysis, we represent a single image or text as low-dimensional linear subspaces and perform retrieval based on subspace similarity. We chose this representation as subspaces give us the flexibility to model an entity based on feature sets, allowing us to observe how integrating or reducing information changes the representation of each entity. We analyze the performance of the selected models’ features on two standard benchmark datasets. Our results indicate that heavily pre-training models can already lead to features with critical information representing each entity, with zero-shot UNITER features performing consistently better than fine-tuned features. Furthermore, while models can benefit from structured inputs, learning representations for objects and relationships separately, such as in SGM, likely causes a loss of crucial contextual information needed to obtain a compact cluster that can effectively represent a single entity.</abstract>
      <url hash="f84da6ae">2022.umios-1.4</url>
      <bibkey>shimomoto-etal-2022-subspace</bibkey>
      <video href="2022.umios-1.4.mp4"/>
      <doi>10.18653/v1/2022.umios-1.4</doi>
    </paper>
    <paper id="5">
      <title>Discourse Relation Embeddings: Representing the Relations between Discourse Segments in Social Media</title>
      <author><first>Youngseo</first><last>Son</last><affiliation>Department of Computer Science, Stony Brook University</affiliation></author>
      <author><first>Vasudha</first><last>Varadarajan</last><affiliation>Department of Computer Science, Stony Brook University</affiliation></author>
      <author><first>H. Andrew</first><last>Schwartz</last><affiliation>Department of Computer Science, Stony Brook University</affiliation></author>
      <pages>45-55</pages>
      <abstract>Discourse relations are typically modeled as a discrete class that characterizes the relation between segments of text (e.g. causal explanations, expansions). However, such predefined discrete classes limit the universe of potential relationships and their nuanced differences. Adding higher-level semantic structure to contextual word embeddings, we propose representing discourse relations as points in high dimensional continuous space. However, unlike words, discourse relations often have no surface form (relations are in between two segments, often with no word or phrase in that gap) which presents a challenge for existing embedding techniques. We present a novel method for automatically creating discourse relation embeddings (DiscRE), addressing the embedding challenge through a weakly supervised, multitask approach to learn diverse and nuanced relations in social media. Results show DiscRE representations obtain the best performance on Twitter discourse relation classification (macro F1=0.76), social media causality prediction (from F1=0.79 to 0.81), and perform beyond modern sentence and word transformers at traditional discourse relation classification, capturing novel nuanced relations (e.g. relations at the intersection of causal explanations and counterfactuals).</abstract>
      <url hash="7a07a905">2022.umios-1.5</url>
      <bibkey>son-etal-2022-discourse</bibkey>
      <doi>10.18653/v1/2022.umios-1.5</doi>
    </paper>
    <paper id="6">
      <title>Understanding Cross-modal Interactions in <fixed-case>V</fixed-case>&amp;<fixed-case>L</fixed-case> Models that Generate Scene Descriptions</title>
      <author><first>Michele</first><last>Cafagna</last><affiliation>University of Malta, Institute of Linguistics and Language Technology</affiliation></author>
      <author><first>Kees</first><last>van Deemter</last><affiliation>Universiteit Utrecht, Information and Computing Sciences</affiliation></author>
      <author><first>Albert</first><last>Gatt</last><affiliation>University of Malta, Institute of Linguistics and Language Technology</affiliation></author>
      <pages>56-72</pages>
      <abstract>Image captioning models tend to describe images in an object-centric way, emphasising visible objects. But image descriptions can also abstract away from objects and describe the type of scene depicted. In this paper, we explore the potential of a state of the art Vision and Language model, VinVL, to caption images at the scene level using (1) a novel dataset which pairs images with both object-centric and scene descriptions. Through (2) an in-depth analysis of the effect of the fine-tuning, we show (3) that a small amount of curated data suffices to generate scene descriptions without losing the capability to identify object-level concepts in the scene; the model acquires a more holistic view of the image compared to when object-centric descriptions are generated. We discuss the parallels between these results and insights from computational and cognitive science research on scene perception.</abstract>
      <url hash="ab4dcc8d">2022.umios-1.6</url>
      <bibkey>cafagna-etal-2022-understanding</bibkey>
      <video href="2022.umios-1.6.mp4"/>
      <doi>10.18653/v1/2022.umios-1.6</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>D</fixed-case>eep<fixed-case>P</fixed-case>arliament: A Legal domain Benchmark &amp; Dataset for Parliament Bills Prediction</title>
      <author><first>Ankit</first><last>Pal</last><affiliation>Open Legal AI</affiliation></author>
      <pages>73-81</pages>
      <abstract>This paper introduces DeepParliament, a legal domain Benchmark Dataset that gathers bill documents and metadata and performs various bill status classification tasks. The proposed dataset text covers a broad range of bills from 1986 to the present and contains richer information on parliament bill content. Data collection, detailed statistics and analyses are provided in the paper. Moreover, we experimented with different types of models ranging from RNN to pretrained and reported the results. We are proposing two new benchmarks: Binary and Multi-Class Bill Status classification. Models developed for bill documents and relevant supportive tasks may assist Members of Parliament (MPs), presidents, and other legal practitioners. It will help review or prioritise bills, thus speeding up the billing process, improving the quality of decisions and reducing the time consumption in both houses. Considering that the foundation of the country”s democracy is Parliament and state legislatures, we anticipate that our research will be an essential addition to the Legal NLP community. This work will be the first to present a Parliament bill prediction task. In order to improve the accessibility of legal AI resources and promote reproducibility, we have made our code and dataset publicly accessible at github.com/monk1337/DeepParliament.</abstract>
      <url hash="ae5bd4de">2022.umios-1.8</url>
      <bibkey>pal-2022-deepparliament</bibkey>
      <doi>10.18653/v1/2022.umios-1.8</doi>
    </paper>
    <paper id="9">
      <title>Punctuation and case restoration in code mixed <fixed-case>I</fixed-case>ndian languages</title>
      <author><first>Subhashree</first><last>Tripathy</last><affiliation>Reverie Language Technologies, Bengaluru</affiliation></author>
      <author><first>Ashis</first><last>Samal</last><affiliation>Reverie Language Technologies, Bengaluru</affiliation></author>
      <pages>82-86</pages>
      <abstract>Automatic Speech Recognition (ASR) systems are taking over in different industries starting from producing video subtitles to interactive digital assistants. ASR output can be used in automatic indexing, categorizing, searching along with normal human readability. Raw transcripts from ASR systems are difficult to interpret since it usually produces text without punctuation and case information (all lower, all upper, camel case etc.), thus limiting the performance of downstream NLP tasks. We proposed an approach to restore the punctuation and case for both English and Hinglish (i.e Hindi vocabulary in Latin script) languages. We have performed a classification task using encoder-based transformers which is a mini BERT consisting of 4 encoder layers for punctuation and case restoration instead of the traditional Seq2Seq model considering the latency constraint in real world use cases. It consists of a total number of 15 distinct classes for the model which includes 5 punctuations i.e Period(.), Comma(,), Single Quote(‘), Double Quote(”) &amp; Question Mark(?) with different combinations of casing. The model is benchmarked on an internal dataset which was based on user conversation with the voice assistant and it achieves a F1(macro) score of 91.52% on the test set.</abstract>
      <url hash="2e61bab9">2022.umios-1.9</url>
      <bibkey>tripathy-samal-2022-punctuation</bibkey>
      <doi>10.18653/v1/2022.umios-1.9</doi>
    </paper>
    <paper id="10">
      <title>Probing Script Knowledge from Pre-Trained Models</title>
      <author><first>Zijia</first><last>Jin</last><affiliation>New York University</affiliation></author>
      <author><first>Xingyu</first><last>Zhang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Mo</first><last>Yu</last><affiliation>WeChat AI</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>Virginia Tech</affiliation></author>
      <pages>87-93</pages>
      <abstract>Adversarial attack of structured prediction models faces various challenges such as the difficulty of perturbing discrete words, the sentence quality issue, and the sensitivity of outputs to small perturbations. In this work, we introduce SHARP, a new attack method that formulates the black-box adversarial attack as a search-based optimization problem with a specially designed objective function considering sentence fluency, meaning preservation and attacking effectiveness. Additionally, three different searching strategies are analyzed and compared, , Beam Search, Metropolis-Hastings Sampling, and Hybrid Search. We demonstrate the effectiveness of our attacking strategies on two challenging structured prediction tasks: part-of-speech (POS) tagging and dependency parsing. Through automatic and human evaluations, we show that our method performs a more potent attack compared with pioneer arts. Moreover, the generated adversarial examples can be used to successfully boost the robustness and performance of the victim model via adversarial training.</abstract>
      <url hash="45ef1414">2022.umios-1.10</url>
      <bibkey>jin-etal-2022-probing</bibkey>
      <doi>10.18653/v1/2022.umios-1.10</doi>
    </paper>
  </volume>
</collection>
