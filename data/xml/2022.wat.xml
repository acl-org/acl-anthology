<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.wat">
  <volume id="1" ingest-date="2022-10-06">
    <meta>
      <booktitle>Proceedings of the 9th Workshop on Asian Translation</booktitle>
      <publisher>International Conference on Computational Linguistics</publisher>
      <address>Gyeongju, Republic of Korea</address>
      <month>October</month>
      <year>2022</year>
      <url hash="8d439a5d">2022.wat-1</url>
      <venue>wat</venue>
    </meta>
    <frontmatter>
      <url hash="13dbb019">2022.wat-1.0</url>
      <bibkey>wat-2022-asian</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Overview of the 9th Workshop on <fixed-case>A</fixed-case>sian Translation</title>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Hideya</first><last>Mino</last></author>
      <author><first>Isao</first><last>Goto</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Shohei</first><last>Higashiyama</last></author>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Akiko</first><last>Eriguchi</last></author>
      <author><first>Kaori</first><last>Abe</last></author>
      <author><first>Yusuke</first><last>Oda</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>1–36</pages>
      <abstract>This paper presents the results of the shared tasks from the 9th workshop on Asian translation (WAT2022). For the WAT2022, 8 teams submitted their translation results for the human evaluation. We also accepted 4 research papers. About 300 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated.</abstract>
      <url hash="7a73a87e">2022.wat-1.1</url>
      <bibkey>nakazawa-etal-2022-overview</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hindi-visual-genome">Hindi Visual Genome</pwcdataset>
    </paper>
    <paper id="2">
      <title>Comparing <fixed-case>BERT</fixed-case>-based Reward Functions for Deep Reinforcement Learning in Machine Translation</title>
      <author><first>Yuki</first><last>Nakatani</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <pages>37–43</pages>
      <abstract>In text generation tasks such as machine translation, models are generally trained using cross-entropy loss. However, mismatches between the loss function and the evaluation metric are often problematic. It is known that this problem can be addressed by direct optimization to the evaluation metric with reinforcement learning. In machine translation, previous studies have used BLEU to calculate rewards for reinforcement learning, but BLEU is not well correlated with human evaluation. In this study, we investigate the impact on machine translation quality through reinforcement learning based on evaluation metrics that are more highly correlated with human evaluation. Experimental results show that reinforcement learning with BERT-based rewards can improve various evaluation metrics.</abstract>
      <url hash="1259c697">2022.wat-1.2</url>
      <bibkey>nakatani-etal-2022-comparing</bibkey>
    </paper>
    <paper id="3">
      <title>Improving <fixed-case>J</fixed-case>ejueo-<fixed-case>K</fixed-case>orean Translation With Cross-Lingual Pretraining Using <fixed-case>J</fixed-case>apanese and <fixed-case>K</fixed-case>orean</title>
      <author><first>Francis</first><last>Zheng</last></author>
      <author><first>Edison</first><last>Marrese-Taylor</last></author>
      <author><first>Yutaka</first><last>Matsuo</last></author>
      <pages>44–50</pages>
      <abstract>Jejueo is a critically endangered language spoken on Jeju Island and is closely related to but mutually unintelligible with Korean. Parallel data between Jejueo and Korean is scarce, and translation between the two languages requires more attention, as current neural machine translation systems typically rely on large amounts of parallel training data. While low-resource machine translation has been shown to benefit from using additional monolingual data during the pretraining process, not as much research has been done on how to select languages other than the source and target languages for use during pretraining. We show that using large amounts of Korean and Japanese data during the pretraining process improves translation by 2.16 BLEU points for translation in the Jejueo → Korean direction and 1.34 BLEU points for translation in the Korean → Jejueo direction compared to the baseline.</abstract>
      <url hash="51ef8db4">2022.wat-1.3</url>
      <bibkey>zheng-etal-2022-improving</bibkey>
      <revision id="1" href="2022.wat-1.3v1" hash="dbf7f901"/>
      <revision id="2" href="2022.wat-1.3v2" hash="51ef8db4" date="2022-10-26">Fixed typos.</revision>
      <pwcdataset url="https://paperswithcode.com/dataset/jit-dataset">JIT Dataset</pwcdataset>
    </paper>
    <paper id="4">
      <title><fixed-case>TMU</fixed-case> <fixed-case>NMT</fixed-case> System with Automatic Post-Editing by Multi-Source <fixed-case>L</fixed-case>evenshtein Transformer for the Restricted Translation Task of <fixed-case>WAT</fixed-case> 2022</title>
      <author><first>Seiichiro</first><last>Kondo</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>51–58</pages>
      <abstract>In this paper, we describe our TMU English–Japanese systems submitted to the restricted translation task at WAT 2022 (Nakazawa et al., 2022). In this task, we translate an input sentence with the constraint that certain words or phrases (called restricted target vocabularies (RTVs)) should be contained in the output sentence. To satisfy this constraint, we address this task using a combination of two techniques. One is lexical-constraint-aware neural machine translation (LeCA) (Chen et al., 2020), which is a method of adding RTVs at the end of input sentences. The other is multi-source Levenshtein transformer (MSLevT) (Wan et al., 2020), which is a non-autoregressive method for automatic post-editing. Our system generates translations in two steps. First, we generate the translation using LeCA. Subsequently, we filter the sentences that do not satisfy the constraints and post-edit them with MSLevT. Our experimental results reveal that 100% of the RTVs can be included in the generated sentences while maintaining the translation quality of the LeCA model on both English to Japanese (En→Ja) and Japanese to English (Ja→En) tasks. Furthermore, the method used in previous studies requires an increase in the beam size to satisfy the constraints, which is computationally expensive. In contrast, the proposed method does not require a similar increase and can generate translations faster.</abstract>
      <url hash="27fbfb3d">2022.wat-1.4</url>
      <bibkey>kondo-komachi-2022-tmu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>H</fixed-case>w<fixed-case>T</fixed-case>sc<fixed-case>SU</fixed-case>’s Submissions on <fixed-case>WAT</fixed-case> 2022 Shared Task</title>
      <author><first>Yilun</first><last>Liu</last></author>
      <author><first>Zhen</first><last>Zhang</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <pages>59–63</pages>
      <abstract>In this paper we describe our submission to the shared tasks of the 9th Workshop on Asian Translation (WAT 2022) on NICT–SAP under the team name ”HwTscSU”. The tasks involve translation from 5 languages into English and vice-versa in two domains: IT domain and Wikinews domain. The purpose is to determine the feasibility of multilingualism, domain adaptation or document-level knowledge given very little to none clean parallel corpora for training. Our approach for all translation tasks mainly focused on pre-training NMT models on general datasets and fine-tuning them on domain-specific datasets. Due to the small amount of parallel corpora, we collected and cleaned the OPUS dataset including three IT domain corpora, i.e., GNOME, KDE4, and Ubuntu. We then trained Transformer models on the collected dataset and fine-tuned on corresponding dev set. The BLEU scores greatly improved in comparison with other systems. Our submission ranked 1st in all IT-domain tasks and in one out of eight ALT domain tasks.</abstract>
      <url hash="b190fdb0">2022.wat-1.5</url>
      <bibkey>liu-etal-2022-hwtscsus</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>NICT</fixed-case>’s Submission to the <fixed-case>WAT</fixed-case> 2022 Structured Document Translation Task</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <pages>64–67</pages>
      <abstract>We present our submission to the structured document translation task organized by WAT 2022. In structured document translation, the key challenge is the handling of inline tags, which annotate text. Specifically, the text that is annotated by tags, should be translated in such a way that in the translation should contain the tags annotating the translation. This challenge is further compounded by the lack of training data containing sentence pairs with inline XML tag annotated content. However, to our surprise, we find that existing multilingual NMT systems are able to handle the translation of text annotated with XML tags without any explicit training on data containing said tags. Specifically, massively multilingual translation models like M2M-100 perform well despite not being explicitly trained to handle structured content. This direct translation approach is often either as good as if not better than the traditional approach of “remove tag, translate and re-inject tag” also known as the “detag-and-project” approach.</abstract>
      <url hash="9aac4dcb">2022.wat-1.6</url>
      <bibkey>dabre-2022-nicts</bibkey>
    </paper>
    <paper id="7">
      <title>Rakuten’s Participation in <fixed-case>WAT</fixed-case> 2022: Parallel Dataset Filtering by Leveraging Vocabulary Heterogeneity</title>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <author><first>Johanes</first><last>Effendi</last></author>
      <author><first>Ohnmar</first><last>Htun</last></author>
      <author><first>Sunil</first><last>Yadav</last></author>
      <author><first>Dongzhe</first><last>Wang</last></author>
      <author><first>Saurabh</first><last>Jain</last></author>
      <pages>68–72</pages>
      <abstract>This paper introduces our neural machine translation system’s participation in the WAT 2022 shared translation task (team ID: sakura). We participated in the Parallel Data Filtering Task. Our approach based on Feature Decay Algorithms achieved +1.4 and +2.4 BLEU points for English to Japanese and Japanese to English respectively compared to the model trained on the full dataset, showing the effectiveness of FDA on in-domain data selection.</abstract>
      <url hash="0af309fd">2022.wat-1.7</url>
      <bibkey>poncelas-etal-2022-rakutens</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>NIT</fixed-case> Rourkela Machine Translation(<fixed-case>MT</fixed-case>) System Submission to <fixed-case>WAT</fixed-case> 2022 for <fixed-case>M</fixed-case>ulti<fixed-case>I</fixed-case>ndic<fixed-case>MT</fixed-case>: An Indic Language Multilingual Shared Task</title>
      <author><first>Sudhansu Bala</first><last>Das</last></author>
      <author><first>Atharv</first><last>Biradar</last></author>
      <author><first>Tapas Kumar</first><last>Mishra</last></author>
      <author><first>Bidyut Kumar</first><last>Patra</last></author>
      <pages>73–77</pages>
      <abstract>Multilingual Neural Machine Translation (MNMT) exhibits incredible performance with the development of a single translation model for many languages. Previous studies on multilingual translation reveal that multilingual training is effective for languages with limited corpus. This paper presents our submission (Team Id: NITR) in the WAT 2022 for “MultiIndicMT shared task” where the objective of the task is the translation between 5 Indic languages from OPUS Corpus (which are newly added in WAT 2022 corpus) into English and vice versa using the corpus provided by the organizer of WAT. Our system is based on a transformer-based NMT using fairseq modelling toolkit with ensemble techniques. Heuristic pre-processing approaches are carried out before keeping the model under training. Our multilingual NMT systems are trained with shared encoder and decoder parameters followed by assigning language embeddings to each token in both encoder and decoder. Our final multilingual system was examined by using BLEU and RIBES metrics scores. In future, we look forward to extend our research that will help in fine-tuning of both encoder and decoder during the monolingual unsupervised training in order to improve the quality of the synthetic data generated during the process.</abstract>
      <url hash="7b4c8b9b">2022.wat-1.8</url>
      <bibkey>das-etal-2022-nit</bibkey>
    </paper>
    <paper id="9">
      <title>Investigation of Multilingual Neural Machine Translation for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Sahinur Rahman</first><last>Laskar</last></author>
      <author><first>Riyanka</first><last>Manna</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>78–81</pages>
      <abstract>In the domain of natural language processing, machine translation is a well-defined task where one natural language is automatically translated to another natural language. The deep learning-based approach of machine translation, known as neural machine translation attains remarkable translational performance. However, it requires a sufficient amount of training data which is a critical issue for low-resource pair translation. To handle the data scarcity problem, the multilingual concept has been investigated in neural machine translation in different settings like many-to-one and one-to-many translation. WAT2022 (Workshop on Asian Translation 2022) organizes (hosted by the COLING 2022) Indic tasks: English-to-Indic and Indic-to-English translation tasks where we have participated as a team named CNLP-NITS-PP. Herein, we have investigated a transliteration-based approach, where Indic languages are transliterated into English script and shared sub-word level vocabulary during the training phase. We have attained BLEU scores of 2.0 (English-to-Bengali), 1.10 (English-to-Assamese), 4.50 (Bengali-to-English), and 3.50 (Assamese-to-English) translation, respectively.</abstract>
      <url hash="ab47c30f">2022.wat-1.9</url>
      <bibkey>laskar-etal-2022-investigation</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/samanantar">Samanantar</pwcdataset>
    </paper>
    <paper id="10">
      <title>Does partial pretranslation can improve low ressourced-languages pairs?</title>
      <author><first>Raoul</first><last>Blin</last></author>
      <pages>82–88</pages>
      <abstract>We study the effects of a local and punctual pretranslation of the source corpus on the performance of a Transformer translation model. The pretranslations are performed at the morphological (morpheme translation), lexical (word translation) and morphosyntactic (numeral groups and dates) levels. We focus on small and medium-sized training corpora (50K ~ 2.5M bisegments) and on a linguistically distant language pair (Japanese and French). We find that this type of pretranslation does not lead to significant progress. We describe the motivations of the approach, the specific difficulties of Japanese-French translation. We discuss the possible reasons for the observed underperformance.</abstract>
      <url hash="b8a7c1e9">2022.wat-1.10</url>
      <bibkey>blin-2022-partial</bibkey>
    </paper>
    <paper id="11">
      <title>Multimodal Neural Machine Translation with Search Engine Based Image Retrieval</title>
      <author><first>ZhenHao</first><last>Tang</last></author>
      <author><first>XiaoBing</first><last>Zhang</last></author>
      <author><first>Zi</first><last>Long</last></author>
      <author><first>XiangHua</first><last>Fu</last></author>
      <pages>89–98</pages>
      <abstract>Recently, numbers of works shows that the performance of neural machine translation (NMT) can be improved to a certain extent with using visual information. However, most of these conclusions are drawn from the analysis of experimental results based on a limited set of bilingual sentence-image pairs, such as Multi30K.In these kinds of datasets, the content of one bilingual parallel sentence pair must be well represented by a manually annotated image,which is different with the actual translation situation. we propose an open-vocabulary image retrieval methods to collect descriptive images for bilingual parallel corpus using image search engine, and we propose text-aware attentive visual encoder to filter incorrectly collected noise images. Experiment results on Multi30K and other two translation datasets show that our proposed method achieves significant improvements over strong baselines.</abstract>
      <url hash="93143a14">2022.wat-1.11</url>
      <bibkey>tang-etal-2022-multimodal</bibkey>
    </paper>
    <paper id="12">
      <title>Silo <fixed-case>NLP</fixed-case>’s Participation at <fixed-case>WAT</fixed-case>2022</title>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Subhadarshi</first><last>Panda</last></author>
      <author><first>Stig-Arne</first><last>Grönroos</last></author>
      <author><first>Mark</first><last>Granroth-Wilding</last></author>
      <author><first>Mika</first><last>Koistinen</last></author>
      <pages>99–105</pages>
      <abstract>This paper provides the system description of “Silo NLP’s” submission to the Workshop on Asian Translation (WAT2022). We have participated in the Indic Multimodal tasks (English-&gt;Hindi, English-&gt;Malayalam, and English-&gt;Bengali, Multimodal Translation). For text-only translation, we used the Transformer and fine-tuned the mBART. For multimodal translation, we used the same architecture and extracted object tags from the images to use as visual features concatenated with the text sequence for input. Our submission tops many tasks including English-&gt;Hindi multimodal translation (evaluation test), English-&gt;Malayalam text-only and multimodal translation (evaluation test), English-&gt;Bengali multimodal translation (challenge test), and English-&gt;Bengali text-only translation (evaluation test).</abstract>
      <url hash="76a5c639">2022.wat-1.12</url>
      <bibkey>parida-etal-2022-silo</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>PICT</fixed-case>@<fixed-case>WAT</fixed-case> 2022: Neural Machine Translation Systems for Indic Languages</title>
      <author><first>Anupam</first><last>Patil</last></author>
      <author><first>Isha</first><last>Joshi</last></author>
      <author><first>Dipali</first><last>Kadam</last></author>
      <pages>106–110</pages>
      <abstract>Translation entails more than simply translating words from one language to another. It is vitally essential for effective cross-cultural communication, thus making good translation systems an important requirement. We describe our systems in this paper, which were submitted to the WAT 2022 translation shared tasks. As part of the Multi-modal translation tasks’ text-only translation sub-tasks, we submitted three Neural Machine Translation systems based on Transformer models for English to Malayalam, English to Bengali, and English to Hindi text translation. We found significant results on the leaderboard for English-Indic (en-xx) systems utilizing BLEU and RIBES scores as comparative metrics in our studies. For the respective translations of English to Malayalam, Bengali, and Hindi, we obtained BLEU scores of 19.50, 32.90, and 41.80 for the challenge subset and 30.60, 39.80, and 42.90 on the benchmark evaluation subset data.</abstract>
      <url hash="d50fb749">2022.wat-1.13</url>
      <bibkey>patil-etal-2022-pict</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>E</fixed-case>nglish to <fixed-case>B</fixed-case>engali Multimodal Neural Machine Translation using Transliteration-based Phrase Pairs Augmentation</title>
      <author><first>Sahinur Rahman</first><last>Laskar</last></author>
      <author><first>Pankaj</first><last>Dadure</last></author>
      <author><first>Riyanka</first><last>Manna</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>111–116</pages>
      <abstract>Automatic translation of one natural language to another is a popular task of natural language processing. Although the deep learning-based technique known as neural machine translation (NMT) is a widely accepted machine translation approach, it needs an adequate amount of training data, which is a challenging issue for low-resource pair translation. Moreover, the multimodal concept utilizes text and visual features to improve low-resource pair translation. WAT2022 (Workshop on Asian Translation 2022) organizes (hosted by the COLING 2022) English to Bengali multimodal translation task where we have participated as a team named CNLP-NITS-PP in two tracks: 1) text-only and 2) multimodal translation. Herein, we have proposed a transliteration-based phrase pairs augmentation approach which shows improvement in the multimodal translation task and achieved benchmark results on Bengali Visual Genome 1.0 dataset. We have attained the best results on the challenge and evaluation test set for English to Bengali multimodal translation with BLEU scores of 28.70, 43.90 and RIBES scores of 0.688931, 0.780669, respectively.</abstract>
      <url hash="d580eca8">2022.wat-1.14</url>
      <bibkey>laskar-etal-2022-english</bibkey>
    </paper>
    <paper id="15">
      <title>Investigation of <fixed-case>E</fixed-case>nglish to <fixed-case>H</fixed-case>indi Multimodal Neural Machine Translation using Transliteration-based Phrase Pairs Augmentation</title>
      <author><first>Sahinur Rahman</first><last>Laskar</last></author>
      <author><first>Rahul</first><last>Singh</last></author>
      <author><first>Md Faizal</first><last>Karim</last></author>
      <author><first>Riyanka</first><last>Manna</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>117–122</pages>
      <abstract>Machine translation translates one natural language to another, a well-defined natural language processing task. Neural machine translation (NMT) is a widely accepted machine translation approach, but it requires a sufficient amount of training data, which is a challenging issue for low-resource pair translation. Moreover, the multimodal concept utilizes text and visual features to improve low-resource pair translation. WAT2022 (Workshop on Asian Translation 2022) organizes (hosted by the COLING 2022) English to Hindi multimodal translation task where we have participated as a team named CNLP-NITS-PP in two tracks: 1) text-only and 2) multimodal translation. Herein, we have proposed a transliteration-based phrase pairs augmentation approach, which shows improvement in the multimodal translation task. We have attained the second best results on the challenge test set for English to Hindi multimodal translation with BLEU score of 39.30, and a RIBES score of 0.791468.</abstract>
      <url hash="5c67feb4">2022.wat-1.15</url>
      <bibkey>laskar-etal-2022-investigation-english</bibkey>
    </paper>
  </volume>
</collection>
