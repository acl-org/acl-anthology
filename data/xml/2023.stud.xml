<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.stud">
  <volume id="1" ingest-date="2023-10-31" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 8th Student Research Workshop associated with the International Conference Recent Advances in Natural Language Processing</booktitle>
      <editor><first>Momchil</first><last>Hardalov</last></editor>
      <editor><first>Zara</first><last>Kancheva</last></editor>
      <editor><first>Boris</first><last>Velichkov</last></editor>
      <editor><first>Ivelina</first><last>Nikolova-Koleva</last></editor>
      <editor><first>Milena</first><last>Slavcheva</last></editor>
      <publisher>INCOMA Ltd., Shoumen, Bulgaria</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2023</year>
      <url hash="ef06c29f">2023.stud-1</url>
      <venue>stud</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="08309289">2023.stud-1.0</url>
      <bibkey>stud-2023-student</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Detecting <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>: A Survey of the State of Detecting <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>-Generated Text</title>
      <author><first>Mahdi</first><last>Dhaini</last></author>
      <author><first>Wessel</first><last>Poelman</last></author>
      <author><first>Ege</first><last>Erdogan</last></author>
      <pages>1–12</pages>
      <abstract>While recent advancements in the capabilities and widespread accessibility of generative language models, such as ChatGPT (OpenAI, 2022), have brought about various benefits by generating fluent human-like text, the task of distinguishing between human- and large language model (LLM) generated text has emerged as a crucial problem. These models can potentially deceive by generating artificial text that appears to be human-generated. This issue is particularly significant in domains such as law, education, and science, where ensuring the integrity of text is of the utmost importance. This survey provides an overview of the current approaches employed to differentiate between texts generated by humans and ChatGPT. We present an account of the different datasets constructed for detecting ChatGPT-generated text, the various methods utilized, what qualitative analyses into the characteristics of human versus ChatGPT-generated text have been performed, and finally, summarize our findings into general insights.</abstract>
      <url hash="98e67dab">2023.stud-1.1</url>
      <bibkey>dhaini-etal-2023-detecting</bibkey>
    </paper>
    <paper id="2">
      <title>Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models</title>
      <author><first>Lautaro</first><last>Estienne</last></author>
      <pages>13–22</pages>
      <abstract>A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only a few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach where calibration is performed without using any adaptation data.</abstract>
      <url hash="a948098c">2023.stud-1.2</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b2944fcc">2023.stud-1.2.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>estienne-2023-unsupervised</bibkey>
    </paper>
    <paper id="3">
      <title>Controllable Active-Passive Voice Generation using Prefix Tuning</title>
      <author><first>Valentin</first><last>Knappich</last></author>
      <author><first>Timo Pierre</first><last>Schrader</last></author>
      <pages>23–32</pages>
      <abstract>The prompting paradigm is an uprising trend in the field of Natural Language Processing (NLP) that aims to learn tasks by finding appropriate prompts rather than fine-tuning the model weights. Such prompts can express an intention, e.g., they can instruct a language model to generate a summary of a given event. In this paper, we study how to influence (”control”) the language generation process such that the outcome fulfills a requested linguistic property. More specifically, we look at controllable active-passive (AP) voice generation, i.e., we require the model to generate a sentence in the requested voice. We build upon the prefix tuning approach and introduce control tokens that are trained on controllable AP generation. We create an AP subset of the WebNLG dataset to fine-tune these control tokens. Among four different models, the one trained with a contrastive learning approach yields the best results in terms of AP accuracy ( 95%) but at the cost of decreased performance on the original WebNLG task.</abstract>
      <url hash="499c9345">2023.stud-1.3</url>
      <bibkey>knappich-schrader-2023-controllable</bibkey>
    </paper>
    <paper id="4">
      <title>Age-Specific Linguistic Features of Depression via Social Media</title>
      <author><first>Charlotte</first><last>Rosario</last></author>
      <pages>33–43</pages>
      <abstract>Social media data has become a crucial resource for understanding and detecting mental health challenges. However, there is a significant gap in our understanding of age-specific linguistic markers associated with classifying depression. This study bridges the gap by analyzing 25,241 text samples from 15,156 Reddit users with self-reported depression across two age groups: adolescents (13-20 year olds) and adults (21+). Through a quantitative exploratory analysis using LIWC, topic modeling, and data visualization, distinct patterns and topical differences emerged in the language of depression for adolescents and adults, including social concerns, temporal focuses, emotions, and cognition. These findings enhance our understanding of how depression is expressed on social media, bearing implications for accurate classification and tailored interventions across different age groups.</abstract>
      <url hash="2d21091b">2023.stud-1.4</url>
      <bibkey>rosario-2023-age</bibkey>
    </paper>
    <paper id="5">
      <title>Trigger Warnings: A Computational Approach to Understanding User-Tagged Trigger Warnings</title>
      <author><first>Sarthak</first><last>Tyagi</last></author>
      <author><first>Adwita</first><last>Arora</last></author>
      <author><first>Krish</first><last>Chopra</last></author>
      <author><first>Manan</first><last>Suri</last></author>
      <pages>44–54</pages>
      <abstract>Content and trigger warnings give information about the content of material prior to receiving it and are used by social media users to tag their content when discussing sensitive topics. Trigger warnings are known to yield benefits in terms of an increased individual agency to make an informed decision about engaging with content. At the same time, some studies contest the benefits of trigger warnings suggesting that they can induce anxiety and reinforce the traumatic experience of specific identities. Our study involves the analysis of the nature and implications of the usage of trigger warnings by social media users using empirical methods and machine learning. Further, we aim to study the community interactions associated with trigger warnings in online communities, precisely the diversity and content of responses and inter-user interactions. The domains of trigger warnings covered will include self-harm, drug abuse, suicide, and depression. The analysis of the above domains will assist in a better understanding of online behaviour associated with them and help in developing domain-specific datasets for further research</abstract>
      <url hash="f3a1c63c">2023.stud-1.5</url>
      <bibkey>tyagi-etal-2023-trigger</bibkey>
    </paper>
    <paper id="6">
      <title>Evaluating Hallucinations in Large Language Models for <fixed-case>B</fixed-case>ulgarian Language</title>
      <author><first>Melania</first><last>Berbatova</last></author>
      <author><first>Yoan</first><last>Salambashev</last></author>
      <pages>55–63</pages>
      <abstract>In this short paper, we introduce the task of evaluating the hallucination of large language models for the Bulgarian language. We first give definitions of what is a hallucination in large language models and what evaluation methods for measuring hallucinations exist. Next, we give an overview of the multilingual evaluation of the latest large language models, focusing on the evaluation of the performance in Bulgarian on tasks, related to hallucination. We then present a method to evaluate the level of hallucination in a given language with no reference data, and provide some initial experiments with this method in Bulgarian. Finally, we provide directions for future research on the topic.</abstract>
      <url hash="e37dc754">2023.stud-1.6</url>
      <bibkey>berbatova-salambashev-2023-evaluating</bibkey>
    </paper>
    <paper id="7">
      <title>Leveraging Probabilistic Graph Models in Nested Named Entity Recognition for <fixed-case>P</fixed-case>olish</title>
      <author><first>Jędrzej</first><last>Jamnicki</last></author>
      <pages>64–67</pages>
      <abstract>This paper presents ongoing work on leveraging probabilistic graph models, specifically conditional random fields and hidden Markov models, in nested named entity recognition for the Polish language. NER is a crucial task in natural language processing that involves identifying and classifying named entities in text documents. Nested NER deals with recognizing hierarchical structures of entities that overlap with one another, presenting additional challenges. The paper discusses the methodologies and approaches used in nested NER, focusing on CRF and HMM. Related works and their contributions are reviewed, and experiments using the KPWr dataset are conducted, particularly with the BiLSTM-CRF model and Word2Vec and HerBERT embeddings. The results show promise in addressing nested NER for Polish, but further research is needed to develop robust and accurate models for this complex task.</abstract>
      <url hash="545ceafb">2023.stud-1.7</url>
      <bibkey>jamnicki-2023-leveraging</bibkey>
    </paper>
    <paper id="8">
      <title>Crowdsourcing Veridicality Annotations in <fixed-case>S</fixed-case>panish: Can Speakers Actually Agree?</title>
      <author><first>Teresa</first><last>Martín Soeder</last></author>
      <pages>68–77</pages>
      <abstract>In veridicality studies, an area of research of Natural Language Inference (NLI), the factuality of different contexts is evaluated. This task, known to be a difficult one since often it is not clear what the interpretation should be Uma et al. (2021), is key for building any Natural Language Understanding (NLU) system that aims at making the right inferences. Here the results of a study that analyzes the veridicality of mood alternation and specificity in Spanish, and whose labels are based on those of Saurí and Pustejovsky (2009) are presented. It has an inter-annotator agreement of AC2 = 0.114, considerably lower than that of de Marneffe et al. (2012) (κ = 0.53), a main reference to this work; and a couple of mood-related significant effects. Due to this strong lack of agreement, an analysis of what factors cause disagreement is presented together with a discussion based on the work of de Marneffe et al. (2012) and Pavlick and Kwiatkowski (2019) about the quality of the annotations gathered and whether other types of analysis like entropy distribution could better represent this corpus. The annotations collected are available at https://github.com/narhim/veridicality_spanish.</abstract>
      <url hash="af99977c">2023.stud-1.8</url>
      <bibkey>martin-soeder-2023-crowdsourcing</bibkey>
    </paper>
    <paper id="9">
      <title>Weakly supervised learning for aspect based sentiment analysis of <fixed-case>U</fixed-case>rdu Tweets</title>
      <author><first>Zoya</first><last>Maqsood</last></author>
      <pages>78–86</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) is vital for text comprehension which benefits applications across various domains. This field involves the two main sub-tasks including aspect extraction and sentiment classification. Existing methods to tackle this problem normally address only one sub-task or utilize topic models that may result in overlapping concepts. Moreover, such algorithms often rely on extensive labeled data and external language resources, making their application costly and time-consuming in new domains and especially for resource-poor languages like Urdu. The lack of aspect mining studies in Urdu literature further exacerbates the inapplicability of existing methods for Urdu language. The primary challenge lies in the preprocessing of data to ensure its suitability for language comprehension by the model, as well as the availability of appropriate pre-trained models, domain embeddings, and tools. This paper implements an ABSA model (CITATION) for unlabeled Urdu tweets with minimal user guidance, utilizing a small set of seed words for each aspect and sentiment class. The model first learns sentiment and aspect joint topic embeddings in the word embedding space with regularization to encourage topic distinctiveness. Afterwards, it employs deep neural models for pre-training with embedding-based predictions and self-training on unlabeled data. Furthermore, we optimize the model for improved performance by substituting the CNN with the BiLSTM classifier for sentence-level sentiment and aspect classification. Our optimized model achieves significant improvements over baselines in aspect and sentiment classification for Urdu tweets with accuracy of 64.8% and 72.8% respectively, demonstrating its effectiveness in generating joint topics and addressing existing limitations in Urdu ABSA.</abstract>
      <url hash="601f56e5">2023.stud-1.9</url>
      <bibkey>maqsood-2023-weakly</bibkey>
    </paper>
    <paper id="10">
      <title>Exploring Low-resource Neural Machine Translation for <fixed-case>S</fixed-case>inhala-<fixed-case>T</fixed-case>amil Language Pair</title>
      <author><first>Ashmari</first><last>Pramodya</last></author>
      <pages>87–97</pages>
      <abstract>At present, Neural Machine Translation is a promising approach for machine translation. Transformer-based deep learning architectures in particular show a substantial performance increase in translating between various language pairs. However, many low-resource language pairs still struggle to lend themselves to Neural Machine Translation due to their data-hungry nature. In this article, we investigate methods of expanding the parallel corpus to enhance translation quality within a model training pipeline, starting from the initial collection of parallel data to the training process of baseline models. Grounded on state-of-the-art Neural Machine Translation approaches such as hyper-parameter tuning, and data augmentation with forward and backward translation, we define a set of best practices for improving Tamil-to-Sinhala machine translation and empirically validate our methods using standard evaluation metrics. Our results demonstrate that the Neural Machine Translation models trained on larger amounts of back-translated data outperform other synthetic data generation approaches in Transformer base training settings. We further demonstrate that, even for language pairs with limited resources, Transformer models are able to tune to outperform existing state-of-the-art Statistical Machine Translation models by as much as 3.28 BLEU points in the Tamil to Sinhala translation scenarios.</abstract>
      <url hash="0bc96722">2023.stud-1.10</url>
      <bibkey>pramodya-2023-exploring</bibkey>
    </paper>
    <paper id="11">
      <title>Prompting <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> to Draw Morphological Connections for New Word Comprehension</title>
      <author><first>Bianca-Madalina</first><last>Zgreaban</last></author>
      <author><first>Rishabh</first><last>Suresh</last></author>
      <pages>98–107</pages>
      <abstract>Though more powerful, Large Language Models need to be periodically retrained for updated information, consuming resources and energy. In this respect, prompt engineering can prove a possible solution to re-training. To explore this line of research, this paper uses a case study, namely, finding the best prompting strategy for asking ChatGPT to define new words based on morphological connections. To determine the best prompting strategy, each definition provided by the prompt was ranked in terms of plausibility and humanlikeness criteria. The findings of this paper show that adding contextual information, operationalised as the keywords ‘new’ and ‘morpheme’, significantly improve the performance of the model for any prompt. While no single prompt significantly outperformed all others, there were differences between performances on the two criteria for most prompts. ChatGPT also provided the most correct definitions with a persona-type prompt.</abstract>
      <url hash="b952d44d">2023.stud-1.11</url>
      <attachment type="OptionalSupplementaryMaterial" hash="58b2657c">2023.stud-1.11.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>zgreaban-suresh-2023-prompting</bibkey>
    </paper>
  </volume>
</collection>
