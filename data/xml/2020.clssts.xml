<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.clssts">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020)</booktitle>
      <editor><first>Kathy</first><last>McKeown</last></editor>
      <editor><first>Douglas W.</first><last>Oard</last></editor>
      <editor><first/><last>Elizabeth</last></editor>
      <editor><first>Richard</first><last>Schwartz</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-55-9</isbn>
      <venue>clssts</venue>
    </meta>
    <frontmatter>
      <url hash="27b49c1f">2020.clssts-1.0</url>
      <bibkey>clssts-2020-cross</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The Effect of Linguistic Parameters in <fixed-case>CLIR</fixed-case> Performance</title>
      <author><first>Carl</first><last>Rubino</last></author>
      <pages>1–6</pages>
      <abstract>This paper will detail how IARPA’s MATERIAL Cross-Language Information Retrieval (CLIR) program investigated certain linguistic parameters to guide language choice, data collection and partitioning, and understand evaluation results. Discerning which linguistic parameters correlated with overall performance enabled the evaluation of progress when different languages were measured, and also was an important factor in determining the most effective CLIR pipeline design, customized to handle language-specific properties deemed necessary to address.</abstract>
      <url hash="2bf28701">2020.clssts-1.1</url>
      <language>eng</language>
      <bibkey>rubino-2020-effect</bibkey>
    </paper>
    <paper id="2">
      <title>Corpora for Cross-Language Information Retrieval in Six Less-Resourced Languages</title>
      <author><first>Ilya</first><last>Zavorin</last></author>
      <author><first>Aric</first><last>Bills</last></author>
      <author><first>Cassian</first><last>Corey</last></author>
      <author><first>Michelle</first><last>Morrison</last></author>
      <author><first>Audrey</first><last>Tong</last></author>
      <author><first>Richard</first><last>Tong</last></author>
      <pages>7–13</pages>
      <abstract>The Machine Translation for English Retrieval of Information in Any Language (MATERIAL) research program, sponsored by the Intelligence Advanced Research Projects Activity (IARPA), focuses on rapid development of end-to-end systems capable of retrieving foreign language speech and text documents relevant to different types of English queries that may be further restricted by domain. Those systems also provide evidence of relevance of the retrieved content in the form of English summaries. The program focuses on Less-Resourced Languages and provides its performer teams very limited amounts of annotated training data. This paper describes the corpora that were created for system development and evaluation for the six languages released by the program to date: Tagalog, Swahili, Somali, Lithuanian, Bulgarian and Pashto. The corpora include build packs to train Machine Translation and Automatic Speech Recognition systems; document sets in three text and three speech genres annotated for domain and partitioned for analysis, development and evaluation; and queries of several types together with corresponding binary relevance judgments against the entire set of documents. The paper also describes a detection metric called Actual Query Weighted Value developed by the program to evaluate end-to-end system performance.</abstract>
      <url hash="7be88a35">2020.clssts-1.2</url>
      <language>eng</language>
      <bibkey>zavorin-etal-2020-corpora</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>MATERIAL</fixed-case>izing Cross-Language Information Retrieval: A Snapshot</title>
      <author><first>Petra</first><last>Galuscakova</last></author>
      <author><first>Douglas</first><last>Oard</last></author>
      <author><first>Joe</first><last>Barrow</last></author>
      <author><first>Suraj</first><last>Nair</last></author>
      <author><first>Shing</first><last>Han-Chin</last></author>
      <author><first>Elena</first><last>Zotkina</last></author>
      <author><first>Ramy</first><last>Eskander</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <pages>14–21</pages>
      <abstract>At about the midpoint of the IARPA MATERIAL program in October 2019, an evaluation was conducted on systems’ abilities to find Lithuanian documents based on English queries. Subsequently, both the Lithuanian test collection and results from all three teams were made available for detailed analysis. This paper capitalizes on that opportunity to begin to look at what’s working well at this stage of the program, and to identify some promising directions for future work.</abstract>
      <url hash="7d1034cf">2020.clssts-1.3</url>
      <language>eng</language>
      <bibkey>galuscakova-etal-2020-materializing</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>SEARCHER</fixed-case>: Shared Embedding Architecture for Effective Retrieval</title>
      <author><first>Joel</first><last>Barry</last></author>
      <author><first>Elizabeth</first><last>Boschee</last></author>
      <author><first>Marjorie</first><last>Freedman</last></author>
      <author><first>Scott</first><last>Miller</last></author>
      <pages>22–25</pages>
      <abstract>We describe an approach to cross lingual information retrieval that does not rely on explicit translation of either document or query terms. Instead, both queries and documents are mapped into a shared embedding space where retrieval is performed. We discuss potential advantages of the approach in handling polysemy and synonymy. We present a method for training the model, and give details of the model implementation. We present experimental results for two cases: Somali-English and Bulgarian-English CLIR.</abstract>
      <url hash="daff9c04">2020.clssts-1.4</url>
      <language>eng</language>
      <bibkey>barry-etal-2020-searcher</bibkey>
    </paper>
    <paper id="5">
      <title>Cross-lingual Information Retrieval with <fixed-case>BERT</fixed-case></title>
      <author><first>Zhuolin</first><last>Jiang</last></author>
      <author><first>Amro</first><last>El-Jaroudi</last></author>
      <author><first>William</first><last>Hartmann</last></author>
      <author><first>Damianos</first><last>Karakos</last></author>
      <author><first>Lingjun</first><last>Zhao</last></author>
      <pages>26–31</pages>
      <abstract>Multiple neural language models have been developed recently, e.g., BERT and XLNet, and achieved impressive results in various NLP tasks including sentence classification, question answering and document ranking. In this paper, we explore the use of the popular bidirectional language model, BERT, to model and learn the relevance between English queries and foreign-language documents in the task of cross-lingual information retrieval. A deep relevance matching model based on BERT is introduced and trained by finetuning a pretrained multilingual BERT model with weak supervision, using home-made CLIR training data derived from parallel corpora. Experimental results of the retrieval of Lithuanian documents against short English queries show that our model is effective and outperforms the competitive baseline approaches.</abstract>
      <url hash="632e3463">2020.clssts-1.5</url>
      <language>eng</language>
      <bibkey>jiang-etal-2020-cross</bibkey>
    </paper>
    <paper id="6">
      <title>A Comparison of Unsupervised Methods for Ad hoc Cross-Lingual Document Retrieval</title>
      <author><first>Elaine</first><last>Zosa</last></author>
      <author><first>Mark</first><last>Granroth-Wilding</last></author>
      <author><first>Lidia</first><last>Pivovarova</last></author>
      <pages>32–37</pages>
      <abstract>We address the problem of linking related documents across languages in a multilingual collection. We evaluate three diverse unsupervised methods to represent and compare documents: (1) multilingual topic model; (2) cross-lingual document embeddings; and (3) Wasserstein distance.We test the performance of these methods in retrieving news articles in Swedish that are known to be related to a given Finnish article.The results show that ensembles of the methods outperform the stand-alone methods, suggesting that they capture complementary characteristics of the documents</abstract>
      <url hash="ccf36348">2020.clssts-1.6</url>
      <language>eng</language>
      <bibkey>zosa-etal-2020-comparison</bibkey>
    </paper>
    <paper id="7">
      <title>Reformulating Information Retrieval from Speech and Text as a Detection Problem</title>
      <author><first>Damianos</first><last>Karakos</last></author>
      <author><first>Rabih</first><last>Zbib</last></author>
      <author><first>William</first><last>Hartmann</last></author>
      <author><first>Richard</first><last>Schwartz</last></author>
      <author><first>John</first><last>Makhoul</last></author>
      <pages>38–43</pages>
      <abstract>In the IARPA MATERIAL program, information retrieval (IR) is treated as a hard detection problem; the system has to output a single global ranking over all queries, and apply a hard threshold on this global list to come up with all the hypothesized relevant documents. This means that how queries are ranked relative to each other can have a dramatic impact on performance. In this paper, we study such a performance measure, the Average Query Weighted Value (AQWV), which is a combination of miss and false alarm rates. AQWV requires that the same detection threshold is applied to all queries. Hence, detection scores of different queries should be comparable, and, to do that, a score normalization technique (commonly used in keyword spotting from speech) should be used. We describe unsupervised methods for score normalization, which are borrowed from the speech field and adapted accordingly for IR, and demonstrate that they greatly improve AQWV on the task of cross-language information retrieval (CLIR), on three low-resource languages used in MATERIAL. We also present a novel supervised score normalization approach which gives additional gains.</abstract>
      <url hash="e7854247">2020.clssts-1.7</url>
      <language>eng</language>
      <bibkey>karakos-etal-2020-reformulating</bibkey>
    </paper>
    <paper id="8">
      <title>The 2019 <fixed-case>BBN</fixed-case> Cross-lingual Information Retrieval System</title>
      <author><first>Le</first><last>Zhang</last></author>
      <author><first>Damianos</first><last>Karakos</last></author>
      <author><first>William</first><last>Hartmann</last></author>
      <author><first>Manaj</first><last>Srivastava</last></author>
      <author><first>Lee</first><last>Tarlin</last></author>
      <author><first>David</first><last>Akodes</last></author>
      <author><first>Sanjay Krishna</first><last>Gouda</last></author>
      <author><first>Numra</first><last>Bathool</last></author>
      <author><first>Lingjun</first><last>Zhao</last></author>
      <author><first>Zhuolin</first><last>Jiang</last></author>
      <author><first>Richard</first><last>Schwartz</last></author>
      <author><first>John</first><last>Makhoul</last></author>
      <pages>44–51</pages>
      <abstract>In this paper, we describe a cross-lingual information retrieval (CLIR) system that, given a query in English, and a set of audio and text documents in a foreign language, can return a scored list of relevant documents, and present findings in a summary form in English. Foreign audio documents are first transcribed by a state-of-the-art pretrained multilingual speech recognition model that is finetuned to the target language. For text documents, we use multiple multilingual neural machine translation (MT) models to achieve good translation results, especially for low/medium resource languages. The processed documents and queries are then scored using a probabilistic CLIR model that makes use of the probability of translation from GIZA translation tables and scores from a Neural Network Lexical Translation Model (NNLTM). Additionally, advanced score normalization, combination, and thresholding schemes are employed to maximize the Average Query Weighted Value (AQWV) scores. The CLIR output, together with multiple translation renderings, are selected and translated into English snippets via a summarization model. Our turnkey system is language agnostic and can be quickly trained for a new low-resource language in few days.</abstract>
      <url hash="605f40f0">2020.clssts-1.8</url>
      <language>eng</language>
      <bibkey>zhang-etal-2020-2019</bibkey>
    </paper>
    <paper id="9">
      <title>What Set of Documents to Present to an Analyst?</title>
      <author><first>Richard</first><last>Schwartz</last></author>
      <author><first>John</first><last>Makhoul</last></author>
      <author><first>Lee</first><last>Tarlin</last></author>
      <author><first>Damianos</first><last>Karakos</last></author>
      <pages>52–57</pages>
      <abstract>We describe the human triage scenario envisioned in the Cross-Lingual Information Retrieval (CLIR) problem of the [REDUCT] Program. The overall goal is to maximize the quality of the set of documents that is given to a bilingual analyst, as measured by the AQWV score. The initial set of source documents that are retrieved by the CLIR system is summarized in English and presented to human judges who attempt to remove the irrelevant documents (false alarms); the resulting documents are then presented to the analyst. First, we describe the AQWV performance measure and show that, in our experience, if the acceptance threshold of the CLIR component has been optimized to maximize AQWV, the loss in AQWV due to false alarms is relatively constant across many conditions, which also limits the possible gain that can be achieved by any post filter (such as human judgments) that removes false alarms. Second, we analyze the likely benefits for the triage operation as a function of the initial CLIR AQWV score and the ability of the human judges to remove false alarms without removing relevant documents. Third, we demonstrate that we can increase the benefit for human judgments by combining the human judgment scores with the original document scores returned by the automatic CLIR system.</abstract>
      <url hash="2d35e020">2020.clssts-1.9</url>
      <language>eng</language>
      <bibkey>schwartz-etal-2020-set</bibkey>
    </paper>
    <paper id="10">
      <title>An Investigative Study of Multi-Modal Cross-Lingual Retrieval</title>
      <author><first>Piyush</first><last>Arora</last></author>
      <author><first>Dimitar</first><last>Shterionov</last></author>
      <author><first>Yasufumi</first><last>Moriya</last></author>
      <author><first>Abhishek</first><last>Kaushik</last></author>
      <author><first>Daria</first><last>Dzendzik</last></author>
      <author><first>Gareth</first><last>Jones</last></author>
      <pages>58–67</pages>
      <abstract>We describe work from our investigations of the novel area of multi-modal cross-lingual retrieval (MMCLIR) under low-resource conditions. We study the challenges associated with MMCLIR relating to: (i) data conversion between different modalities, for example speech and text, (ii) overcoming the language barrier between source and target languages; (iii) effectively scoring and ranking documents to suit the retrieval task; and (iv) handling low resource constraints that prohibit development of heavily tuned machine translation (MT) and automatic speech recognition (ASR) systems. We focus on the use case of retrieving text and speech documents in Swahili, using English queries which was the main focus of the OpenCLIR shared task. Our work is developed within the scope of this task. In this paper we devote special attention to the automatic translation (AT) component which is crucial for the overall quality of the MMCLIR system. We exploit a combination of dictionaries and phrase-based statistical machine translation (MT) systems to tackle effectively the subtask of query translation. We address each MMCLIR challenge individually, and develop separate components for automatic translation (AT), speech processing (SP) and information retrieval (IR). We find that results with respect to cross-lingual text retrieval are quite good relative to the task of cross-lingual speech retrieval. Overall we find that the task of MMCLIR and specifically cross-lingual speech retrieval is quite complex. Further we pinpoint open issues related to handling cross-lingual audio and text retrieval for low resource languages that need to be addressed in future research.</abstract>
      <url hash="770a75b5">2020.clssts-1.10</url>
      <language>eng</language>
      <bibkey>arora-etal-2020-investigative</bibkey>
    </paper>
    <paper id="11">
      <title>Subtitles to Segmentation: Improving Low-Resource Speech-to-<fixed-case>T</fixed-case>ext<fixed-case>T</fixed-case>ranslation Pipelines</title>
      <author><first>David</first><last>Wan</last></author>
      <author><first>Zhengping</first><last>Jiang</last></author>
      <author><first>Chris</first><last>Kedzie</last></author>
      <author><first>Elsbeth</first><last>Turcan</last></author>
      <author><first>Peter</first><last>Bell</last></author>
      <author><first>Kathy</first><last>McKeown</last></author>
      <pages>68–73</pages>
      <abstract>In this work, we focus on improving ASR output segmentation in the context of low-resource language speech-to-text translation. ASR output segmentation is crucial, as ASR systems segment the input audio using purely acoustic information and are not guaranteed to output sentence-like segments. Since most MT systems expect sentences as input, feeding in longer unsegmented passages can lead to sub-optimal performance. We explore the feasibility of using datasets of subtitles from TV shows and movies to train better ASR segmentation models. We further incorporate part-of-speech (POS) tag and dependency label information (derived from the unsegmented ASR outputs) into our segmentation model. We show that this noisy syntactic information can improve model accuracy. We evaluate our models intrinsically on segmentation quality and extrinsically on downstream MT performance, as well as downstream tasks including cross-lingual information retrieval (CLIR) tasks and human relevance assessments. Our model shows improved performance on downstream tasks for Lithuanian and Bulgarian.</abstract>
      <url hash="ef647954">2020.clssts-1.11</url>
      <language>eng</language>
      <bibkey>wan-etal-2020-subtitles</bibkey>
    </paper>
  </volume>
</collection>
