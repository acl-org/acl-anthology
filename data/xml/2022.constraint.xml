<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.constraint">
  <volume id="1" ingest-date="2022-05-15" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situations</booktitle>
      <editor><first>Tanmoy</first><last>Chakraborty</last></editor>
      <editor><first>Md. Shad</first><last>Akhtar</last></editor>
      <editor><first>Kai</first><last>Shu</last></editor>
      <editor><first>H. Russell</first><last>Bernard</last></editor>
      <editor><first>Maria</first><last>Liakata</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Aseem</first><last>Srivastava</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="c91f8ae7">2022.constraint-1</url>
      <venue>constraint</venue>
    </meta>
    <frontmatter>
      <url hash="16e2ed05">2022.constraint-1.0</url>
      <bibkey>constraint-2022-combating</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Findings of the <fixed-case>CONSTRAINT</fixed-case> 2022 Shared Task on Detecting the Hero, the Villain, and the Victim in Memes</title>
      <author><first>Shivam</first><last>Sharma</last></author>
      <author><first>Tharun</first><last>Suresh</last></author>
      <author><first>Atharva</first><last>Kulkarni</last></author>
      <author><first>Himanshi</first><last>Mathur</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Md. Shad</first><last>Akhtar</last></author>
      <author><first>Tanmoy</first><last>Chakraborty</last></author>
      <pages>1-11</pages>
      <abstract>We present the findings of the shared task at the CONSTRAINT 2022 Workshop: Hero, Villain, and Victim: Dissecting harmful memes for Semantic role labeling of entities. The task aims to delve deeper into the domain of meme comprehension by deciphering the connotations behind the entities present in a meme. In more nuanced terms, the shared task focuses on determining the victimizing, glorifying, and vilifying intentions embedded in meme entities to explicate their connotations. To this end, we curate HVVMemes, a novel meme dataset of about 7000 memes spanning the domains of COVID-19 and US Politics, each containing entities and their associated roles: hero, villain, victim, or none. The shared task attracted 105 participants, but eventually only 6 submissions were made. Most of the successful submissions relied on fine-tuning pre-trained language and multimodal models along with ensembles. The best submission achieved an F1-score of 58.67.</abstract>
      <url hash="90ddcd2d">2022.constraint-1.1</url>
      <bibkey>sharma-etal-2022-findings</bibkey>
      <doi>10.18653/v1/2022.constraint-1.1</doi>
      <video href="2022.constraint-1.1.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/harmeme">HarMeme</pwcdataset>
    </paper>
    <paper id="2">
      <title><fixed-case>DD</fixed-case>-<fixed-case>TIG</fixed-case> at Constraint@<fixed-case>ACL</fixed-case>2022: Multimodal Understanding and Reasoning for Role Labeling of Entities in Hateful Memes</title>
      <author><first>Ziming</first><last>Zhou</last></author>
      <author><first>Han</first><last>Zhao</last></author>
      <author><first>Jingjing</first><last>Dong</last></author>
      <author><first>Jun</first><last>Gao</last></author>
      <author><first>Xiaolong</first><last>Liu</last></author>
      <pages>12-18</pages>
      <abstract>The memes serve as an important tool in online communication, whereas some hateful memes endanger cyberspace by attacking certain people or subjects. Recent studies address hateful memes detection while further understanding of relationships of entities in memes remains unexplored. This paper presents our work at the Constraint@ACL2022 Shared Task: Hero, Villain and Victim: Dissecting harmful memes for semantic role labelling of entities. In particular, we propose our approach utilizing transformer-based multimodal models through a VCR method with data augmentation, continual pretraining, loss re-weighting, and ensemble learning. We describe the models used, the ways of preprocessing and experiments implementation. As a result, our best model achieves the Macro F1-score of 54.707 on the test set of this shared task.</abstract>
      <url hash="45307527">2022.constraint-1.2</url>
      <bibkey>zhou-etal-2022-dd</bibkey>
      <doi>10.18653/v1/2022.constraint-1.2</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/hateful-memes">Hateful Memes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
    </paper>
    <paper id="3">
      <title>Are you a hero or a villain? A semantic role labelling approach for detecting harmful memes.</title>
      <author><first>Shaik</first><last>Fharook</last></author>
      <author><first>Syed</first><last>Sufyan Ahmed</last></author>
      <author><first>Gurram</first><last>Rithika</last></author>
      <author><first>Sumith Sai</first><last>Budde</last></author>
      <author><first>Sunil</first><last>Saumya</last></author>
      <author><first>Shankar</first><last>Biradar</last></author>
      <pages>19-23</pages>
      <abstract>Identifying good and evil through representations of victimhood, heroism, and villainy (i.e., role labeling of entities) has recently caught the research community’s interest. Because of the growing popularity of memes, the amount of offensive information published on the internet is expanding at an alarming rate. It generated a larger need to address this issue and analyze the memes for content moderation. Framing is used to show the entities engaged as heroes, villains, victims, or others so that readers may better anticipate and understand their attitudes and behaviors as characters. Positive phrases are used to characterize heroes, whereas negative terms depict victims and villains, and terms that tend to be neutral are mapped to others. In this paper, we propose two approaches to role label the entities of the meme as hero, villain, victim, or other through Named-Entity Recognition(NER), Sentiment Analysis, etc. With an F1-score of 23.855, our team secured eighth position in the Shared Task @ Constraint 2022.</abstract>
      <url hash="3539bc50">2022.constraint-1.3</url>
      <bibkey>fharook-etal-2022-hero</bibkey>
      <doi>10.18653/v1/2022.constraint-1.3</doi>
      <video href="2022.constraint-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Logically at the Constraint 2022: Multimodal role labelling</title>
      <author><first>Ludovic</first><last>Kun</last></author>
      <author><first>Jayesh</first><last>Bankoti</last></author>
      <author><first>David</first><last>Kiskovski</last></author>
      <pages>24-34</pages>
      <abstract>This paper describes our system for the Constraint 2022 challenge at ACL 2022, whose goal is to detect which entities are glorified, vilified or victimised, within a meme . The task should be done considering the perspective of the meme’s author. In our work, the challenge is treated as a multi-class classification task. For a given pair of a meme and an entity, we need to classify whether the entity is being referenced as Hero, a Villain, a Victim or Other. Our solution combines (ensembling) different models based on Unimodal (Text only) model and Multimodal model (Text + Images). We conduct several experiments and benchmarks different competitive pre-trained transformers and vision models in this work. Our solution, based on an ensembling method, is ranked first on the leaderboard and obtains a macro F1-score of 0.58 on test set. The code for the experiments and results are available at <url>https://bitbucket.org/logicallydevs/constraint_2022/src/master/</url></abstract>
      <url hash="fd6cd88a">2022.constraint-1.4</url>
      <bibkey>kun-etal-2022-logically</bibkey>
      <doi>10.18653/v1/2022.constraint-1.4</doi>
      <video href="2022.constraint-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Combining Language Models and Linguistic Information to Label Entities in Memes</title>
      <author><first>Pranaydeep</first><last>Singh</last></author>
      <author><first>Aaron</first><last>Maladry</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <pages>35-42</pages>
      <abstract>This paper describes the system we developed for the shared task ‘Hero, Villain and Victim: Dissecting harmful memes for Semantic role labelling of entities’ organised in the framework of the Second Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situation (Constraint 2022). We present an ensemble approach combining transformer-based models and linguistic information, such as the presence of irony and implicit sentiment associated to the target named entities. The ensemble system obtains promising classification scores, resulting in a third place finish in the competition.</abstract>
      <url hash="b813ad58">2022.constraint-1.5</url>
      <bibkey>singh-etal-2022-combining</bibkey>
      <doi>10.18653/v1/2022.constraint-1.5</doi>
      <video href="2022.constraint-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Detecting the Role of an Entity in Harmful Memes: Techniques and their Limitations</title>
      <author><first>Rabindra Nath</first><last>Nandi</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>43-54</pages>
      <abstract>Harmful or abusive online content has been increasing over time and it has been raising concerns among social media platforms, government agencies, and policymakers. Such harmful or abusive content has a significant negative impact on society such as cyberbullying led to suicides, COVID-19 related rumors led to hundreds of deaths. The content that is posted and shared online can be textual, visual, a combination of both, or a meme. In this paper, we provide our study on detecting the roles of entities in harmful memes, which is part of the CONSTRAINT-2022 shared task. We report the results on the participated system. We further provide a comparative analysis on different experimental settings (i.e., unimodal, multimodal, attention, and augmentation).</abstract>
      <url hash="4837ea9a">2022.constraint-1.6</url>
      <bibkey>nandi-etal-2022-detecting</bibkey>
      <doi>10.18653/v1/2022.constraint-1.6</doi>
      <video href="2022.constraint-1.6.mp4"/>
      <pwccode url="https://github.com/robi56/harmful_memes_block_fusion" additional="false">robi56/harmful_memes_block_fusion</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hateful-memes">Hateful Memes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hateful-memes-challenge">Hateful Memes Challenge</pwcdataset>
    </paper>
    <paper id="7">
      <title>Fine-tuning and Sampling Strategies for Multimodal Role Labeling of Entities under Class Imbalance</title>
      <author><first>Syrielle</first><last>Montariol</last></author>
      <author><first>Étienne</first><last>Simon</last></author>
      <author><first>Arij</first><last>Riabi</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>55-65</pages>
      <abstract>We propose our solution to the multimodal semantic role labeling task from the CONSTRAINT’22 workshop. The task aims at classifying entities in memes into classes such as “hero” and “villain”. We use several pre-trained multi-modal models to jointly encode the text and image of the memes, and implement three systems to classify the role of the entities. We propose dynamic sampling strategies to tackle the issue of class imbalance. Finally, we perform qualitative analysis on the representations of the entities.</abstract>
      <url hash="bbb7091a">2022.constraint-1.7</url>
      <bibkey>montariol-etal-2022-fine</bibkey>
      <doi>10.18653/v1/2022.constraint-1.7</doi>
      <video href="2022.constraint-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Document Retrieval and Claim Verification to Mitigate <fixed-case>COVID</fixed-case>-19 Misinformation</title>
      <author><first>Megha</first><last>Sundriyal</last></author>
      <author><first>Ganeshan</first><last>Malhotra</last></author>
      <author><first>Md Shad</first><last>Akhtar</last></author>
      <author><first>Shubhashis</first><last>Sengupta</last></author>
      <author><first>Andrew</first><last>Fano</last></author>
      <author><first>Tanmoy</first><last>Chakraborty</last></author>
      <pages>66-74</pages>
      <abstract>During the COVID-19 pandemic, the spread of misinformation on online social media has grown exponentially. Unverified bogus claims on these platforms regularly mislead people, leading them to believe in half-baked truths. The current vogue is to employ manual fact-checkers to verify claims to combat this avalanche of misinformation. However, establishing such claims’ veracity is becoming increasingly challenging, partly due to the plethora of information available, which is difficult to process manually. Thus, it becomes imperative to verify claims automatically without human interventions. To cope up with this issue, we propose an automated claim verification solution encompassing two steps – document retrieval and veracity prediction. For the retrieval module, we employ a hybrid search-based system with BM25 as a base retriever and experiment with recent state-of-the-art transformer-based models for re-ranking. Furthermore, we use a BART-based textual entailment architecture to authenticate the retrieved documents in the later step. We report experimental findings, demonstrating that our retrieval module outperforms the best baseline system by 10.32 NDCG@100 points. We escort a demonstration to assess the efficacy and impact of our suggested solution. As a byproduct of this study, we present an open-source, easily deployable, and user-friendly Python API that the community can adopt.</abstract>
      <url hash="c0650c59">2022.constraint-1.8</url>
      <bibkey>sundriyal-etal-2022-document</bibkey>
      <doi>10.18653/v1/2022.constraint-1.8</doi>
      <video href="2022.constraint-1.8.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
    </paper>
    <paper id="9">
      <title><fixed-case>M</fixed-case>-<fixed-case>BAD</fixed-case>: A Multilabel Dataset for Detecting Aggressive Texts and Their Targets</title>
      <author><first>Omar</first><last>Sharif</last></author>
      <author><first>Eftekhar</first><last>Hossain</last></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last></author>
      <pages>75-85</pages>
      <abstract>Recently, detection and categorization of undesired (e. g., aggressive, abusive, offensive, hate) content from online platforms has grabbed the attention of researchers because of its detrimental impact on society. Several attempts have been made to mitigate the usage and propagation of such content. However, most past studies were conducted primarily for English, where low-resource languages like Bengali remained out of the focus. Therefore, to facilitate research in this arena, this paper introduces a novel multilabel Bengali dataset (named M-BAD) containing 15650 texts to detect aggressive texts and their targets. Each text of M-BAD went through rigorous two-level annotations. At the primary level, each text is labelled as either aggressive or non-aggressive. In the secondary level, the aggressive texts have been further annotated into five fine-grained target classes: religion, politics, verbal, gender and race. Baseline experiments are carried out with different machine learning (ML), deep learning (DL) and transformer models, where Bangla-BERT acquired the highest weighted <tex-math>f_1</tex-math>-score in both detection (0.92) and target identification (0.83) tasks. Error analysis of the models exhibits the difficulty to identify context-dependent aggression, and this work argues that further research is required to address these issues.</abstract>
      <url hash="cc16d6f4">2022.constraint-1.9</url>
      <bibkey>sharif-etal-2022-bad</bibkey>
      <doi>10.18653/v1/2022.constraint-1.9</doi>
      <video href="2022.constraint-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>How does fake news use a thumbnail? <fixed-case>CLIP</fixed-case>-based Multimodal Detection on the Unrepresentative News Image</title>
      <author><first>Hyewon</first><last>Choi</last></author>
      <author><first>Yejun</first><last>Yoon</last></author>
      <author><first>Seunghyun</first><last>Yoon</last></author>
      <author><first>Kunwoo</first><last>Park</last></author>
      <pages>86-94</pages>
      <abstract>This study investigates how fake news use the thumbnail image for a news article. We aim at capturing the degree of semantic incongruity between news text and image by using the pretrained CLIP representation. Motivated by the stylistic distinctiveness in fake news text, we examine whether fake news tends to use an irrelevant image to the news content. Results show that fake news tends to have a high degree of semantic incongruity than general news. We further attempt to detect such image-text incongruity by training classification models on a newly generated dataset. A manual evaluation suggests our method can find news articles of which the thumbnail image is semantically irrelevant to news text with an accuracy of 0.8. We also release a new dataset of image and news text pairs with the incongruity label, facilitating future studies on the direction.</abstract>
      <url hash="b659417f">2022.constraint-1.10</url>
      <bibkey>choi-etal-2022-fake</bibkey>
      <doi>10.18653/v1/2022.constraint-1.10</doi>
      <video href="2022.constraint-1.10.mp4"/>
      <pwccode url="https://github.com/ssu-humane/fake-news-thumbnail" additional="false">ssu-humane/fake-news-thumbnail</pwccode>
    </paper>
    <paper id="11">
      <title>Detecting False Claims in Low-Resource Regions: A Case Study of <fixed-case>C</fixed-case>aribbean Islands</title>
      <author><first>Jason</first><last>Lucas</last></author>
      <author><first>Limeng</first><last>Cui</last></author>
      <author><first>Thai</first><last>Le</last></author>
      <author><first>Dongwon</first><last>Lee</last></author>
      <pages>95-102</pages>
      <abstract>The COVID-19 pandemic has created threats to global health control. Misinformation circulated on social media and news outlets has undermined public trust towards Government and health agencies. This problem is further exacerbated in developing countries or low-resource regions, where the news is not equipped with abundant English fact-checking information. In this paper, we make the first attempt to detect COVID-19 misinformation (in English, Spanish, and Haitian French) populated in the Caribbean regions, using the fact-checked claims in the US (in English). We started by collecting a dataset of Caribbean real &amp; fake claims. Then we trained several classification and language models on COVID-19 in the high-resource language regions and transferred the knowledge to the Caribbean claim dataset. The experimental results of this paper reveal the limitations of current fake claim detection in low-resource regions and encourage further research on multi-lingual detection.</abstract>
      <url hash="09884df8">2022.constraint-1.11</url>
      <bibkey>lucas-etal-2022-detecting</bibkey>
      <doi>10.18653/v1/2022.constraint-1.11</doi>
      <video href="2022.constraint-1.11.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coaid">CoAID</pwcdataset>
    </paper>
  </volume>
</collection>
