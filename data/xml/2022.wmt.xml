<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.wmt">
  <volume id="1" ingest-date="2023-01-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Seventh Conference on Machine Translation (WMT)</booktitle>
      <editor><first>Philipp</first><last>Koehn</last></editor>
      <editor><first>Loïc</first><last>Barrault</last></editor>
      <editor><first>Ondřej</first><last>Bojar</last></editor>
      <editor><first>Fethi</first><last>Bougares</last></editor>
      <editor><first>Rajen</first><last>Chatterjee</last></editor>
      <editor><first>Marta R.</first><last>Costa-jussà</last></editor>
      <editor><first>Christian</first><last>Federmann</last></editor>
      <editor><first>Mark</first><last>Fishel</last></editor>
      <editor><first>Alexander</first><last>Fraser</last></editor>
      <editor><first>Markus</first><last>Freitag</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Roman</first><last>Grundkiewicz</last></editor>
      <editor><first>Paco</first><last>Guzman</last></editor>
      <editor><first>Barry</first><last>Haddow</last></editor>
      <editor><first>Matthias</first><last>Huck</last></editor>
      <editor><first>Antonio</first><last>Jimeno Yepes</last></editor>
      <editor><first>Tom</first><last>Kocmi</last></editor>
      <editor><first>André</first><last>Martins</last></editor>
      <editor><first>Makoto</first><last>Morishita</last></editor>
      <editor><first>Christof</first><last>Monz</last></editor>
      <editor><first>Masaaki</first><last>Nagata</last></editor>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Matteo</first><last>Negri</last></editor>
      <editor><first>Aurélie</first><last>Névéol</last></editor>
      <editor><first>Mariana</first><last>Neves</last></editor>
      <editor><first>Martin</first><last>Popel</last></editor>
      <editor><first>Marco</first><last>Turchi</last></editor>
      <editor><first>Marcos</first><last>Zampieri</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, United Arab Emirates (Hybrid)</address>
      <month>December</month>
      <year>2022</year>
      <url hash="7abe7444">2022.wmt-1</url>
      <venue>wmt</venue>
    </meta>
    <frontmatter>
      <url hash="3ca4c2cb">2022.wmt-1.0</url>
      <bibkey>wmt-2022-machine</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Findings of the 2022 Conference on Machine Translation (<fixed-case>WMT</fixed-case>22)</title>
      <author><first>Tom</first><last>Kocmi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Rachel</first><last>Bawden</last><affiliation>Inria</affiliation></author>
      <author><first>Ondřej</first><last>Bojar</last><affiliation>Charles University, MFF UFAL</affiliation></author>
      <author><first>Anton</first><last>Dvorkovich</last><affiliation>Yandex</affiliation></author>
      <author><first>Christian</first><last>Federmann</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mark</first><last>Fishel</last><affiliation>University of Tartu</affiliation></author>
      <author><first>Thamme</first><last>Gowda</last><affiliation>USC Information Sciences Institute</affiliation></author>
      <author><first>Yvette</first><last>Graham</last><affiliation>ADAPT, Trinity College Dublin</affiliation></author>
      <author><first>Roman</first><last>Grundkiewicz</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Rebecca</first><last>Knowles</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Philipp</first><last>Koehn</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Christof</first><last>Monz</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Makoto</first><last>Morishita</last><affiliation>NTT Communication Science Laboratories</affiliation></author>
      <author><first>Masaaki</first><last>Nagata</last><affiliation>NTT Corporation</affiliation></author>
      <author><first>Toshiaki</first><last>Nakazawa</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Michal</first><last>Novák</last><affiliation>Charles University, Faculty of Mathematics and Physics</affiliation></author>
      <author><first>Martin</first><last>Popel</last><affiliation>Charles University, Faculty of Mathematics and Physics, UFAL</affiliation></author>
      <author><first>Maja</first><last>Popović</last><affiliation>ADAPT, Dublin City University</affiliation></author>
      <pages>1-45</pages>
      <abstract>This paper presents the results of the General Machine Translation Task organised as part of the Conference on Machine Translation (WMT) 2022. In the general MT task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting of four different domains. We evaluate system outputs with human annotators using two different techniques: reference-based direct assessment and (DA) and a combination of DA and scalar quality metric (DA+SQM).</abstract>
      <url hash="3fb27977">2022.wmt-1.1</url>
      <bibkey>kocmi-etal-2022-findings</bibkey>
    </paper>
    <paper id="2">
      <title>Results of <fixed-case>WMT</fixed-case>22 Metrics Shared Task: Stop Using <fixed-case>BLEU</fixed-case> – Neural Metrics Are Better and More Robust</title>
      <author><first>Markus</first><last>Freitag</last><affiliation>Google Research</affiliation></author>
      <author><first>Ricardo</first><last>Rei</last><affiliation>Unbabel/INESC-ID</affiliation></author>
      <author><first>Nitika</first><last>Mathur</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Chi-kiu</first><last>Lo</last><affiliation>National Research Council of Canada</affiliation></author>
      <author><first>Craig</first><last>Stewart</last><affiliation>Unbabel</affiliation></author>
      <author><first>Eleftherios</first><last>Avramidis</last><affiliation>German Research Center for Artificial Intelligence (DFKI)</affiliation></author>
      <author><first>Tom</first><last>Kocmi</last><affiliation>Microsoft</affiliation></author>
      <author><first>George</first><last>Foster</last><affiliation>Google</affiliation></author>
      <author><first>Alon</first><last>Lavie</last><affiliation>Unbabel/Carnegie Mellon University</affiliation></author>
      <author><first>André F. T.</first><last>Martins</last><affiliation>Unbabel, Instituto de Telecomunicacoes</affiliation></author>
      <pages>46-68</pages>
      <abstract>This paper presents the results of the WMT22 Metrics Shared Task. Participants submitting automatic MT evaluation metrics were asked to score the outputs of the translation systems competing in the WMT22 News Translation Task on four different domains: news, social, ecommerce, and chat. All metrics were evaluated on how well they correlate with human ratings at the system and segment level. Similar to last year, we acquired our own human ratings based on expert-based human evaluation via Multidimensional Quality Metrics (MQM). This setup had several advantages, among other things: (i) expert-based evaluation is more reliable, (ii) we extended the pool of translations by 5 additional translations based on MBR decoding or rescoring which are challenging for current metrics. In addition, we initiated a challenge set subtask, where participants had to create contrastive test suites for evaluating metrics’ ability to capture and penalise specific types of translation errors. Finally, we present an extensive analysis on how well metrics perform on three language pairs: English to German, English to Russian and Chinese to English. The results demonstrate the superiority of neural-based learned metrics and demonstrate again that overlap metrics like Bleu, spBleu or chrf correlate poorly with human ratings. The results also reveal that neural-based metrics are remarkably robust across different domains and challenges.</abstract>
      <url hash="0169bc0d">2022.wmt-1.2</url>
      <bibkey>freitag-etal-2022-results</bibkey>
    </paper>
    <paper id="3">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2022 Shared Task on Quality Estimation</title>
      <author><first>Chrysoula</first><last>Zerva</last><affiliation>Instituto de Telecomunicações, Instituto Superior Técnico, University of Lisbon</affiliation></author>
      <author><first>Frédéric</first><last>Blain</last><affiliation>University of Wolverhampton</affiliation></author>
      <author><first>Ricardo</first><last>Rei</last><affiliation>Unbabel/INESC-ID</affiliation></author>
      <author><first>Piyawat</first><last>Lertvittayakumjorn</last><affiliation>Google</affiliation></author>
      <author><first>José G.</first><last>C. de Souza</last><affiliation>Unbabel</affiliation></author>
      <author><first>Steffen</first><last>Eger</last><affiliation>NLLG Lab, Bielefeld University</affiliation></author>
      <author><first>Diptesh</first><last>Kanojia</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Duarte</first><last>Alves</last><affiliation>Instituto Superior Técnico / Unbabel</affiliation></author>
      <author><first>Constantin</first><last>Orăsan</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Marina</first><last>Fomicheva</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>André F. T.</first><last>Martins</last><affiliation>Unbabel, Instituto de Telecomunicacoes</affiliation></author>
      <author><first>Lucia</first><last>Specia</last><affiliation>Imperial College London</affiliation></author>
      <pages>69-99</pages>
      <abstract>We report the results of the WMT 2022 shared task on Quality Estimation, in which the challenge is to predict the quality of the output of neural machine translation systems at the word and sentence levels, without access to reference translations. This edition introduces a few novel aspects and extensions that aim to enable more fine-grained, and explainable quality estimation approaches. We introduce an updated quality annotation scheme using Multidimensional Quality Metrics to obtain sentence- and word-level quality scores for three language pairs. We also extend the Direct Assessments and post-edit data (MLQE-PE) to new language pairs: we present a novel and large dataset on English-Marathi, as well as a zero-shot test set on English-Yoruba. Further, we include an explainability sub-task for all language pairs and present a new format of a critical error detection task for two new language pairs. Participants from 11 different teams submitted altogether 991 systems to different task variants and language pairs.</abstract>
      <url hash="4e4f22c8">2022.wmt-1.3</url>
      <bibkey>zerva-etal-2022-findings</bibkey>
    </paper>
    <paper id="4">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2022 Shared Task on Efficient Translation</title>
      <author><first>Kenneth</first><last>Heafield</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Biao</first><last>Zhang</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Graeme</first><last>Nail</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Jelmer</first><last>Van Der Linde</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Nikolay</first><last>Bogoychev</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>100-108</pages>
      <abstract>The machine translation efficiency task challenges participants to make their systems faster and smaller with minimal impact on translation quality. How much quality to sacrifice for efficiency depends upon the application, so participants were encouraged to make multiple submissions covering the space of trade-offs. In total, there were 76 submissions from 5 teams. The task covers GPU, single-core CPU, and multi-core CPU hardware tracks as well as batched throughput or single-sentence latency conditions. Submissions showed hundreds of millions of words can be translated for a dollar, average latency is 3.5–25 ms, and models fit in 7.5–900 MB.</abstract>
      <url hash="3975e7da">2022.wmt-1.4</url>
      <bibkey>heafield-etal-2022-findings</bibkey>
    </paper>
    <paper id="5">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2022 Shared Task on Automatic Post-Editing</title>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>IIT Bombay</affiliation></author>
      <author><first>Rajen</first><last>Chatterjee</last><affiliation>Apple Inc.</affiliation></author>
      <author><first>Markus</first><last>Freitag</last><affiliation>Google Research</affiliation></author>
      <author><first>Diptesh</first><last>Kanojia</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Marco</first><last>Turchi</last><affiliation>Zoom Video Communications</affiliation></author>
      <pages>109-117</pages>
      <abstract>We present the results from the 8th round of the WMT shared task on MT Automatic PostEditing, which consists in automatically correcting the output of a “black-box” machine translation system by learning from human corrections. This year, the task focused on a new language pair (English→Marathi) and on data coming from multiple domains (healthcare, tourism, and general/news). Although according to several indicators this round was of medium-high difficulty compared to the past,the best submission from the three participating teams managed to significantly improve (with an error reduction of 3.49 TER points) the original translations produced by a generic neural MT system.</abstract>
      <url hash="e2286ddd">2022.wmt-1.5</url>
      <bibkey>bhattacharyya-etal-2022-findings</bibkey>
    </paper>
    <paper id="6">
      <title>Embarrassingly Easy Document-Level <fixed-case>MT</fixed-case> Metrics: How to Convert Any Pretrained Metric into a Document-Level Metric</title>
      <author><first>Giorgos</first><last>Vernikos</last><affiliation>École Polytechnique Fédérale de Lausanne</affiliation></author>
      <author><first>Brian</first><last>Thompson</last><affiliation>Amazon</affiliation></author>
      <author><first>Prashant</first><last>Mathur</last><affiliation>Amazon</affiliation></author>
      <author><first>Marcello</first><last>Federico</last><affiliation>AWS AI Labs</affiliation></author>
      <pages>118-128</pages>
      <abstract>We present a very simple method for extending pretrained machine translation metrics to incorporate document-level context. We apply our method to four popular metrics: BERTScore, Prism, COMET, and the reference-free metric COMET-QE. We evaluate our document-level metrics on the MQM annotations from the WMT 2021 metrics shared task and find that the document-level metrics outperform their sentence-level counterparts in about 85% of the tested conditions, when excluding results on low-quality human references. Additionally, we show that our document-level extension of COMET-QE dramatically improves accuracy on discourse phenomena tasks, supporting our hypothesis that our document-level metrics are resolving ambiguities in the reference sentence by using additional context.</abstract>
      <url hash="7119a963">2022.wmt-1.6</url>
      <bibkey>vernikos-etal-2022-embarrassingly</bibkey>
      <video href="2022.wmt-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Searching for a Higher Power in the Human Evaluation of <fixed-case>MT</fixed-case></title>
      <author><first>Johnny</first><last>Wei</last><affiliation>Information Sciences Institute, University of Southern California</affiliation></author>
      <author><first>Tom</first><last>Kocmi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Christian</first><last>Federmann</last><affiliation>Microsoft</affiliation></author>
      <pages>129-139</pages>
      <abstract>In MT evaluation, pairwise comparisons are conducted to identify the better system. In conducting the comparison, the experimenter must allocate a budget to collect Direct Assessment (DA) judgments. We provide a cost effective way to spend the budget, but show that typical budget sizes often do not allow for solid comparison. Taking the perspective that the basis of solid comparison is in achieving statistical significance, we study the power (rate of achieving significance) on a large collection of pairwise DA comparisons. Due to the nature of statistical estimation, power is low for differentiating less than 1-2 DA points, and to achieve a notable increase in power requires at least 2-3x more samples. Applying variance reduction alone will not yield these gains, so we must face the reality of undetectable differences and spending increases. In this context, we propose interim testing, an “early stopping” collection procedure that yields more power per judgment collected, which adaptively focuses the budget on pairs that are borderline significant. Interim testing can achieve up to a 27% efficiency gain when spending 3x the current budget, or 18% savings at the current evaluation power.</abstract>
      <url hash="46d9dc8a">2022.wmt-1.7</url>
      <bibkey>wei-etal-2022-searching</bibkey>
      <video href="2022.wmt-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Test Set Sampling Affects System Rankings: Expanded Human Evaluation of <fixed-case>WMT</fixed-case>20 <fixed-case>E</fixed-case>nglish-<fixed-case>I</fixed-case>nuktitut Systems</title>
      <author><first>Rebecca</first><last>Knowles</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Chi-kiu</first><last>Lo</last><affiliation>National Research Council of Canada</affiliation></author>
      <pages>140-153</pages>
      <abstract>We present a collection of expanded human annotations of the WMT20 English-Inuktitut machine translation shared task, covering the Nunavut Hansard portion of the dataset. Additionally, we recompute News rankings to take into account the completed set of human annotations and certain irregularities in the annotation task construction. We show the effect of these changes on the downstream task of the evaluation of automatic metrics. Finally, we demonstrate that character-level metrics correlate well with human judgments for the task of automatically evaluating translation into this polysynthetic language.</abstract>
      <url hash="af29b2f2">2022.wmt-1.8</url>
      <bibkey>knowles-lo-2022-test</bibkey>
      <video href="2022.wmt-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Continuous Rating as Reliable Human Evaluation of Simultaneous Speech Translation</title>
      <author><first>Dávid</first><last>Javorský</last><affiliation>Charles Univerzity, Faculty of Mathematics and Physics</affiliation></author>
      <author><first>Dominik</first><last>Macháček</last><affiliation>Charles University, MFF UFAL</affiliation></author>
      <author><first>Ondřej</first><last>Bojar</last><affiliation>Charles University, MFF UFAL</affiliation></author>
      <pages>154-164</pages>
      <abstract>Simultaneous speech translation (SST) can be evaluated on simulated online events where human evaluators watch subtitled videos and continuously express their satisfaction by pressing buttons (so called Continuous Rating). Continuous Rating is easy to collect, but little is known about its reliability, or relation to comprehension of foreign language document by SST users. In this paper, we contrast Continuous Rating with factual questionnaires on judges with different levels of source language knowledge. Our results show that Continuous Rating is easy and reliable SST quality assessment if the judges have at least limited knowledge of the source language. Our study indicates users’ preferences on subtitle layout and presentation style and, most importantly, provides a significant evidence that users with advanced source language knowledge prefer low latency over fewer re-translations.</abstract>
      <url hash="fb3fe140">2022.wmt-1.9</url>
      <bibkey>javorsky-etal-2022-continuous</bibkey>
      <video href="2022.wmt-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Gender Bias Mitigation for <fixed-case>NMT</fixed-case> Involving Genderless Languages</title>
      <author><first>Ander</first><last>Corral</last><affiliation>Orai NLP Technologies</affiliation></author>
      <author><first>Xabier</first><last>Saralegi</last><affiliation>Orai NLP technologies</affiliation></author>
      <pages>165-176</pages>
      <abstract>It has been found that NMT systems have a strong preference towards social defaults and biases when translating certain occupations, which due to their widespread use, can unintentionally contribute to amplifying and perpetuating these patterns. In that sense, this work focuses on sentence-level gender agreement between gendered entities and occupations when translating from genderless languages to languages with grammatical gender. Specifically, we address the Basque to Spanish translation direction for which bias mitigation has not been addressed. Gender information in Basque is explicit in neither the grammar nor the morphology. It is only present in a limited number of gender specific common nouns and person proper names. We propose a template-based fine-tuning strategy with explicit gender tags to provide a stronger gender signal for the proper inflection of occupations. This strategy is compared against systems fine-tuned on real data extracted from Wikipedia biographies. We provide a detailed gender bias assessment analysis and perform a template ablation study to determine the optimal set of templates. We report a substantial gender bias mitigation (up to 50% on gender bias scores) while keeping the original translation quality.</abstract>
      <url hash="bd305e46">2022.wmt-1.10</url>
      <bibkey>corral-saralegi-2022-gender</bibkey>
      <video href="2022.wmt-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Exploring the Benefits and Limitations of Multilinguality for Non-autoregressive Machine Translation</title>
      <author><first>Sweta</first><last>Agrawal</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Julia</first><last>Kreutzer</last><affiliation>Google</affiliation></author>
      <author><first>Colin</first><last>Cherry</last><affiliation>Google</affiliation></author>
      <pages>177-187</pages>
      <abstract>Non-autoregressive (NAR) machine translation has recently received significant developments and now achieves comparable quality with autoregressive (AR) models on some benchmarks while providing an efficient alternative to AR inference. However, while AR translation is often used to implement multilingual models that benefit from transfer between languages and from improved serving efficiency, multilingual NAR models remain relatively unexplored. Taking Connectionist Temporal Classification as an example NAR model and IMPUTER as a semi-NAR model, we present a comprehensive empirical study of multilingual NAR. We test its capabilities with respect to positive transfer between related languages and negative transfer under capacity constraints. As NAR models require distilled training sets, we carefully study the impact of bilingual versus multilingual teachers. Finally, we fit a scaling law for multilingual NAR to determine capacity bottlenecks, which quantifies its performance relative to the AR model as the model scale increases.</abstract>
      <url hash="a694b206">2022.wmt-1.11</url>
      <bibkey>agrawal-etal-2022-exploring</bibkey>
      <video href="2022.wmt-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Learning an Artificial Language for Knowledge-Sharing in Multilingual Translation</title>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>Karlsruhe Institut of Technology</affiliation></author>
      <pages>188-202</pages>
      <abstract>The cornerstone of multilingual neural translation is shared representations across languages. Given the theoretically infinite representation power of neural networks, semantically identical sentences are likely represented differently. While representing sentences in the continuous latent space ensures expressiveness, it introduces the risk of capturing of irrelevant features which hinders the learning of a common representation. In this work, we discretize the encoder output latent space of multilingual models by assigning encoder states to entries in a codebook,which in effect represents source sentences in a new artificial language. This discretization process not only offers a new way to interpret the otherwise black-box model representations,but, more importantly, gives potential for increasing robustness in unseen testing conditions. We validate our approach on large-scale experiments with realistic data volumes and domains. When tested in zero-shot conditions, our approach is competitive with two strong alternatives from the literature. We also use the learned artificial language to analyze model behavior, and discover that using a similar bridge language increases knowledge-sharing among the remaining languages.</abstract>
      <url hash="fcc57370">2022.wmt-1.12</url>
      <bibkey>liu-niehues-2022-learning</bibkey>
      <video href="2022.wmt-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Don’t Discard Fixed-Window Audio Segmentation in Speech-to-Text Translation</title>
      <author><first>Chantal</first><last>Amrhein</last><affiliation>Department of Computational Linguistics, University of Zurich</affiliation></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>203-219</pages>
      <abstract>For real-life applications, it is crucial that end-to-end spoken language translation models perform well on continuous audio, without relying on human-supplied segmentation. For online spoken language translation, where models need to start translating before the full utterance is spoken,most previous work has ignored the segmentation problem. In this paper, we compare various methods for improving models’ robustness towards segmentation errors and different segmentation strategies in both offline and online settings and report results on translation quality, flicker and delay. Our findings on five different language pairs show that a simple fixed-window audio segmentation can perform surprisingly well given the right conditions.</abstract>
      <url hash="65c3aa76">2022.wmt-1.13</url>
      <bibkey>amrhein-haddow-2022-dont</bibkey>
      <video href="2022.wmt-1.13.mp4"/>
    </paper>
    <paper id="14">
      <title>Additive Interventions Yield Robust Multi-Domain Machine Translation Models</title>
      <author><first>Elijah</first><last>Rippeth</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Matt</first><last>Post</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>220-232</pages>
      <abstract>Additive interventions are a recently-proposed mechanism for controlling target-side attributes in neural machine translation by modulating the encoder’s representation of a source sequence as opposed to manipulating the raw source sequence as seen in most previous tag-based approaches. In this work we examine the role of additive interventions in a large-scale multi-domain machine translation setting and compare its performance in various inference scenarios. We find that while the performance difference is small between intervention-based systems and tag-based systems when the domain label matches the test domain, intervention-based systems are robust to label error, making them an attractive choice under label uncertainty. Further, we find that the superiority of single-domain fine-tuning comes under question when training data is scaled, contradicting previous findings.</abstract>
      <url hash="1b46f17c">2022.wmt-1.14</url>
      <bibkey>rippeth-post-2022-additive</bibkey>
      <video href="2022.wmt-1.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Inria-<fixed-case>ALMA</fixed-case>na<fixed-case>CH</fixed-case> at <fixed-case>WMT</fixed-case> 2022: Does Transcription Help Cross-Script Machine Translation?</title>
      <author><first>Jesujoba</first><last>Alabi</last><affiliation>Saarland University</affiliation></author>
      <author><first>Lydia</first><last>Nishimwe</last><affiliation>Inria</affiliation></author>
      <author><first>Benjamin</first><last>Muller</last><affiliation>Inria</affiliation></author>
      <author><first>Camille</first><last>Rey</last><affiliation>Inria</affiliation></author>
      <author><first>Benoît</first><last>Sagot</last><affiliation>Inria</affiliation></author>
      <author><first>Rachel</first><last>Bawden</last><affiliation>Inria</affiliation></author>
      <pages>233-243</pages>
      <abstract>This paper describes the Inria ALMAnaCH team submission to the WMT 2022 general translation shared task. Participating in the language directions cs,ru,uk→en and cs↔uk, we experiment with the use of a dedicated Latin-script transcription convention aimed at representing all Slavic languages involved in a way that maximises character- and word-level correspondences between them as well as with the English language. Our hypothesis was that bringing the source and target language closer could have a positive impact on machine translation results. We provide multiple comparisons, including bilingual and multilingual baselines, with and without transcription. Initial results indicate that the transcription strategy was not successful, resulting in lower results than baselines. We nevertheless submitted our multilingual, transcribed models as our primary systems, and in this paper provide some indications as to why we got these negative results.</abstract>
      <url hash="026dc3e0">2022.wmt-1.15</url>
      <bibkey>alabi-etal-2022-inria</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>NAIST</fixed-case>-<fixed-case>NICT</fixed-case>-<fixed-case>TIT</fixed-case> <fixed-case>WMT</fixed-case>22 General <fixed-case>MT</fixed-case> Task Submission</title>
      <author><first>Hiroyuki</first><last>Deguchi</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Kenji</first><last>Imamura</last><affiliation>National Institute of Information and Communications Technology</affiliation></author>
      <author><first>Masahiro</first><last>Kaneko</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Yuto</first><last>Nishida</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Justin</first><last>Vasselli</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Huy Hien</first><last>Vu</last><affiliation>Nara Institute of Science and Techlogy</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>244-250</pages>
      <abstract>In this paper, we describe our NAIST-NICT-TIT submission to the WMT22 general machine translation task. We participated in this task for the English ↔ Japanese language pair. Our system is characterized as an ensemble of Transformer big models, k-nearest-neighbor machine translation (kNN-MT) (Khandelwal et al., 2021), and reranking.In our translation system, we construct the datastore for kNN-MT from back-translated monolingual data and integrate kNN-MT into the ensemble model. We designed a reranking system to select a translation from the n-best translation candidates generated by the translation system. We also use a context-aware model to improve the document-level consistency of the translation.</abstract>
      <url hash="d307d3d3">2022.wmt-1.16</url>
      <bibkey>deguchi-etal-2022-naist</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>S</fixed-case>amsung <fixed-case>R</fixed-case>&amp;<fixed-case>D</fixed-case> Institute <fixed-case>P</fixed-case>oland Participation in <fixed-case>WMT</fixed-case> 2022</title>
      <author><first>Adam</first><last>Dobrowolski</last><affiliation>Samsung R&amp;D Institute Poland</affiliation></author>
      <author><first>Mateusz</first><last>Klimaszewski</last><affiliation>Warsaw University of Technology</affiliation></author>
      <author><first>Adam</first><last>Myśliwy</last><affiliation>Samsung R&amp;D Institute Poland</affiliation></author>
      <author><first>Marcin</first><last>Szymański</last><affiliation>Psnc</affiliation></author>
      <author><first>Jakub</first><last>Kowalski</last><affiliation>Samsung R&amp;D Institute Poland</affiliation></author>
      <author><first>Kornelia</first><last>Szypuła</last><affiliation>Samsung R&amp;D Institute Poland</affiliation></author>
      <author><first>Paweł</first><last>Przewłocki</last><affiliation>Samsung R&amp;D Institute Poland</affiliation></author>
      <author><first>Paweł</first><last>Przybysz</last><affiliation>Samsung R&amp;D Institute Poland</affiliation></author>
      <pages>251-259</pages>
      <abstract>This paper presents the system description of Samsung R&amp;D Institute Poland participation in WMT 2022 for General MT solution for medium and low resource languages: Russian and Croatian. Our approach combines iterative noised/tagged back-translation and iterative distillation. We investigated different monolingual resources and compared their influence on final translations. We used available BERT-likemodels for text classification and for extracting domains of texts. Then we prepared an ensemble of NMT models adapted to multiple domains. Finally we attempted to predict ensemble weight vectors from the BERT-based domain classifications for individual sentences. Our final trained models reached quality comparable to best online translators using only limited constrained resources during training.</abstract>
      <url hash="21f81b86">2022.wmt-1.17</url>
      <bibkey>dobrowolski-etal-2022-samsung</bibkey>
    </paper>
    <paper id="18">
      <title>Tencent <fixed-case>AI</fixed-case> Lab - Shanghai Jiao Tong University Low-Resource Translation System for the <fixed-case>WMT</fixed-case>22 Translation Task</title>
      <author><first>Zhiwei</first><last>He</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Xing</first><last>Wang</last><affiliation>Tencent</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>260-267</pages>
      <abstract>This paper describes Tencent AI Lab - Shanghai Jiao Tong University (TAL-SJTU) Low-Resource Translation systems for the WMT22 shared task. We participate in the general translation task on English-Livonian.Our system is based on M2M100 with novel techniques that adapt it to the target language pair.(1) Cross-model word embedding alignment: inspired by cross-lingual word embedding alignment, we successfully transfer a pre-trained word embedding to M2M100, enabling it to support Livonian.(2) Gradual adaptation strategy: we exploit Estonian and Latvian as auxiliary languages for many-to-many translation training and then adapt to English-Livonian.(3) Data augmentation: to enlarge the parallel data for English-Livonian, we construct pseudo-parallel data with Estonian and Latvian as pivot languages.(4) Fine-tuning: to make the most of all available data, we fine-tune the model with the validation set and online back-translation, further boosting the performance. In model evaluation: (1) We find that previous work underestimated the translation performance of Livonian due to inconsistent Unicode normalization, which may cause a discrepancy of up to 14.9 BLEU score.(2) In addition to the standard validation set, we also employ round-trip BLEU to evaluate the models, which we find more appropriate for this task. Finally, our unconstrained system achieves BLEU scores of 17.0 and 30.4 for English to/from Livonian.</abstract>
      <url hash="725e00dd">2022.wmt-1.18</url>
      <bibkey>he-etal-2022-tencent</bibkey>
    </paper>
    <paper id="19">
      <title>Lan-Bridge <fixed-case>MT</fixed-case>’s Participation in the <fixed-case>WMT</fixed-case> 2022 General Translation Shared Task</title>
      <author><first>Bing</first><last>Han</last><affiliation>Lan-Bridge</affiliation></author>
      <author><first>Yangjian</first><last>Wu</last><affiliation>Lan-bridge</affiliation></author>
      <author><first>Gang</first><last>Hu</last><affiliation>Lan-bridge</affiliation></author>
      <author><first>Qiulin</first><last>Chen</last><affiliation>Lan-Bridge</affiliation></author>
      <pages>268-274</pages>
      <abstract>This paper describes Lan-Bridge Translation systems for the WMT 2022 General Translation shared task. We participate in 18 language directions: English to and from Czech, German, Ukrainian, Japanese, Russian, Chinese, English to Croatian, French to German, Yakut to and from Russian and Ukrainian to and from Czech.To develop systems covering all these direc_x0002_tions, we mainly focus on multilingual mod_x0002_els. In general, we apply data corpus filtering, scaling model size, sparse expert model (in par_x0002_ticular, Transformer with adapters), large scale backtranslation and language model rerankingtechniques. Our system ranks first in 6 directions based on automatic evaluation.</abstract>
      <url hash="af45c45a">2022.wmt-1.19</url>
      <bibkey>han-etal-2022-lan</bibkey>
    </paper>
    <paper id="20">
      <title>Manifold’s <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>hinese System at <fixed-case>WMT</fixed-case>22 General <fixed-case>MT</fixed-case> Task</title>
      <author><first>Chang</first><last>Jin</last><affiliation>Soochow University</affiliation></author>
      <author><first>Tingxun</first><last>Shi</last><affiliation>OPPO Research Institute</affiliation></author>
      <author><first>Zhengshan</first><last>Xue</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Xiaodong</first><last>Lin</last><affiliation>OPPO Research Institute</affiliation></author>
      <pages>275-279</pages>
      <abstract>Manifold’s English-Chinese System at WMT22 is an ensemble of 4 models trained by different configurations with scheduled sampling-based fine-tuning. The four configurations are DeepBig (XenC), DeepLarger (XenC), DeepBig-TalkingHeads (XenC) and DeepBig (LaBSE). Concretely, DeepBig extends Transformer-Big to 24 encoder layers. DeepLarger has 20 encoder layers and its feed-forward network (FFN) dimension is 8192. TalkingHeads applies the talking-heads trick. For XenC configs, we selected monolingual and parallel data that is similar to the past newstest datasets using XenC, and for LaBSE, we cleaned the officially provided parallel data using LaBSE pretrained model. According to the officially released autonomic metrics leaderboard, our final constrained system ranked 1st among all others when evaluated by bleu-all, chrf-all and COMET-B, 2nd by COMET-A.</abstract>
      <url hash="754b856e">2022.wmt-1.20</url>
      <bibkey>jin-etal-2022-manifolds</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>CUNI</fixed-case>-Bergamot Submission at <fixed-case>WMT</fixed-case>22 General Translation Task</title>
      <author><first>Josef</first><last>Jon</last><affiliation>Charles University</affiliation></author>
      <author><first>Martin</first><last>Popel</last><affiliation>Charles University, Faculty of Mathematics and Physics, UFAL</affiliation></author>
      <author><first>Ondřej</first><last>Bojar</last><affiliation>Charles University, MFF UFAL</affiliation></author>
      <pages>280-289</pages>
      <abstract>We present the CUNI-Bergamot submission for the WMT22 General translation task. We compete in English-Czech direction. Our submission further explores block backtranslation techniques. Compared to the previous work, we measure performance in terms of COMET score and named entities translation accuracy. We evaluate performance of MBR decoding compared to traditional mixed backtranslation training and we show a possible synergy when using both of the techniques simultaneously. The results show that both approaches are effective means of improving translation quality and they yield even better results when combined.</abstract>
      <url hash="622c141f">2022.wmt-1.21</url>
      <bibkey>jon-etal-2022-cuni</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>KYB</fixed-case> General Machine Translation Systems for <fixed-case>WMT</fixed-case>22</title>
      <author><first>Shivam</first><last>Kalkar</last><affiliation>NRI Digital, Ltd.</affiliation></author>
      <author><first>Yoko</first><last>Matsuzaki</last><affiliation>NRI Digital, Ltd.</affiliation></author>
      <author><first>Ben</first><last>Li</last><affiliation>NRI Digital, Ltd.</affiliation></author>
      <pages>290-294</pages>
      <abstract>We here describe our neural machine translation system for general machine translation shared task in WMT 2022. Our systems are based on the Transformer (Vaswani et al., 2017) with base settings. We explore the high-efficiency model training strategies, aimed to train a model with high-accuracy by using small model and a reasonable amount of data. We performed fine-tuning and ensembling with N-best ranking in English to/from Japanese directions. We found that fine-tuning by filtered JParaCrawl data set leads to better translations for both of direction in English to/from Japanese models. In English to Japanese direction model, ensembling and N-best ranking of 10 different checkpoints improved translations. By comparing with other online translation service, we found that our model achieved a great translation quality.</abstract>
      <url hash="449be8ce">2022.wmt-1.22</url>
      <bibkey>kalkar-etal-2022-kyb</bibkey>
    </paper>
    <paper id="23">
      <title>Analyzing the Use of Influence Functions for Instance-Specific Data Filtering in Neural Machine Translation</title>
      <author><first>Tsz Kin</first><last>Lam</last><affiliation>Heidelberg University</affiliation></author>
      <author><first>Eva</first><last>Hasler</last><affiliation>Amazon</affiliation></author>
      <author><first>Felix</first><last>Hieber</last><affiliation>Amazon</affiliation></author>
      <pages>295-309</pages>
      <abstract>Customer feedback can be an important signal for improving commercial machine translation systems. One solution for fixing specific translation errors is to remove the related erroneous training instances followed by re-training of the machine translation system, which we refer to as instance-specific data filtering. Influence functions (IF) have been shown to be effective in finding such relevant training examples for classification tasks such as image classification, toxic speech detection and entailment task. Given a probing instance, IF find influential training examples by measuring the similarity of the probing instance with a set of training examples in gradient space. In this work, we examine the use of influence functions for Neural Machine Translation (NMT). We propose two effective extensions to a state of the art influence function and demonstrate on the sub-problem of copied training examples that IF can be applied more generally than hand-crafted regular expressions.</abstract>
      <url hash="2ef623ab">2022.wmt-1.23</url>
      <bibkey>lam-etal-2022-analyzing</bibkey>
    </paper>
    <paper id="24">
      <title>The <fixed-case>AISP</fixed-case>-<fixed-case>SJTU</fixed-case> Translation System for <fixed-case>WMT</fixed-case> 2022</title>
      <author><first>Guangfeng</first><last>Liu</last><affiliation>AI Speech Co., Ltd.</affiliation></author>
      <author><first>Qinpei</first><last>Zhu</last><affiliation>AI Speech Co., Ltd.</affiliation></author>
      <author><first>Xingyu</first><last>Chen</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Renjie</first><last>Feng</last><affiliation>Soochow University</affiliation></author>
      <author><first>Jianxin</first><last>Ren</last><affiliation>AI Speech Co., Ltd.</affiliation></author>
      <author><first>Renshou</first><last>Wu</last><affiliation>AI Speech Co., Ltd.</affiliation></author>
      <author><first>Qingliang</first><last>Miao</last><affiliation>AI Speech Co., Ltd.</affiliation></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Kai</first><last>Yu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>310-317</pages>
      <abstract>This paper describes AISP-SJTU’s participation in WMT 2022 shared general MT task. In this shared task, we participated in four translation directions: English-Chinese, Chinese-English, English-Japanese and Japanese-English. Our systems are based on the Transformer architecture with several novel and effective variants, including network depth and internal structure. In our experiments, we employ data filtering, large-scale back-translation, knowledge distillation, forward-translation, iterative in-domain knowledge finetune and model ensemble. The constrained systems achieve 48.8, 29.7, 39.3 and 22.0 case-sensitive BLEU scores on EN-ZH, ZH-EN, EN-JA and JA-EN, respectively.</abstract>
      <url hash="c6c4f1ee">2022.wmt-1.24</url>
      <bibkey>liu-etal-2022-aisp</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>NT</fixed-case>5 at <fixed-case>WMT</fixed-case> 2022 General Translation Task</title>
      <author><first>Makoto</first><last>Morishita</last><affiliation>NTT Communication Science Laboratories</affiliation></author>
      <author><first>Keito</first><last>Kudo</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Yui</first><last>Oka</last><affiliation>NTT Communication Science Laboratories</affiliation></author>
      <author><first>Katsuki</first><last>Chousa</last><affiliation>Ntt</affiliation></author>
      <author><first>Shun</first><last>Kiyono</last><affiliation>LINE Corporation</affiliation></author>
      <author><first>Sho</first><last>Takase</last><affiliation>LINE Corporation</affiliation></author>
      <author><first>Jun</first><last>Suzuki</last><affiliation>Tohoku University / RIKEN Center for AIP</affiliation></author>
      <pages>318-325</pages>
      <abstract>This paper describes the NTT-Tohoku-TokyoTech-RIKEN (NT5) team’s submission system for the WMT’22 general translation task. This year, we focused on the English-to-Japanese and Japanese-to-English translation tracks. Our submission system consists of an ensemble of Transformer models with several extensions. We also applied data augmentation and selection techniques to obtain potentially effective training data for training individual Transformer models in the pre-training and fine-tuning scheme. Additionally, we report our trial of incorporating a reranking module and the reevaluated results of several techniques that have been recently developed and published.</abstract>
      <url hash="192e36f7">2022.wmt-1.25</url>
      <bibkey>morishita-etal-2022-nt5</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>A</fixed-case>dam <fixed-case>M</fixed-case>ickiewicz <fixed-case>U</fixed-case>niversity at <fixed-case>WMT</fixed-case> 2022: <fixed-case>NER</fixed-case>-Assisted and Quality-Aware Neural Machine Translation</title>
      <author><first>Artur</first><last>Nowakowski</last><affiliation>Adam Mickiewicz University</affiliation></author>
      <author><first>Gabriela</first><last>Pałka</last><affiliation>Adam Mickiewicz University</affiliation></author>
      <author><first>Kamil</first><last>Guttmann</last><affiliation>Adam Mickiewicz University</affiliation></author>
      <author><first>Mikołaj</first><last>Pokrywka</last><affiliation>Adam Mickiewicz University</affiliation></author>
      <pages>326-334</pages>
      <abstract>This paper presents Adam Mickiewicz University’s (AMU) submissions to the constrained track of the WMT 2022 General MT Task. We participated in the Ukrainian ↔ Czech translation directions. The systems are a weighted ensemble of four models based on the Transformer (big) architecture. The models use source factors to utilize the information about named entities present in the input. Each of the models in the ensemble was trained using only the data provided by the shared task organizers. A noisy back-translation technique was used to augment the training corpora. One of the models in the ensemble is a document-level model, trained on parallel and synthetic longer sequences. During the sentence-level decoding process, the ensemble generated the n-best list. The n-best list was merged with the n-best list generated by a single document-level model which translated multiple sentences at a time. Finally, existing quality estimation models and minimum Bayes risk decoding were used to rerank the n-best list so that the best hypothesis was chosen according to the COMET evaluation metric. According to the automatic evaluation results, our systems rank first in both translation directions.</abstract>
      <url hash="58b26a7f">2022.wmt-1.26</url>
      <bibkey>nowakowski-etal-2022-adam</bibkey>
    </paper>
    <paper id="27">
      <title>Evaluating Corpus Cleanup Methods in the <fixed-case>WMT</fixed-case>’22 News Translation Task</title>
      <author><first>Marilena</first><last>Malli</last><affiliation>Department of Informatics and Telecommunications, University of Athens</affiliation></author>
      <author><first>George</first><last>Tambouratzis</last><affiliation>ILSP/Athena R.C.</affiliation></author>
      <pages>335-341</pages>
      <abstract>This submission to the WMT22: General MT Task, consists of translations produced from a series of NMT models of the following two language pairs: german-to-english and german-to-french. All the models are trained using only the parallel training data specified by WMT22, and no monolingual training data was used. The models follow the transformer architecture employing 8 attention heads and 6 layers in both the encoder and decoder. It is also worth mentioning that, in order to limit the computational resources that we would use during the training process, we decided to train the majority of models by limiting the training to 21 epochs. Moreover, the translations submitted at WMT22 have been produced using the test data released by the WMT22.The aim of our experiments has been to evaluate methods for cleaning-up a parallel corpus to determine if this will lead to a translation model producing more accurate translations. For each language pair, the base NMT models has been trained from raw parallel training corpora, while the additional NMT models have been trained with corpora subjected to a special cleaning process with the following tools: Bifixer and Bicleaner. It should be mentioned that the Bicleaner repository doesn’t provide pre-trained classifiers for the above language pairs, consequently we trained probabilistic dictionaries in order to produce new models. The fundamental differences between these NMT models produced are mainly related to the quality and the quantity of the training data, while there are very few differences in the training parameters. To complete this work, we used the following three software packages: (i) MARIAN NMT (Version: v1.11.5), which was used for the training of the neural machine translation models and (ii) Bifixer and (iii) Bicleaner, which were used in order to correct and clean the parallel training data. Concerning the Bifixer and Bicleaner tools, we followed all the steps as described meticulously in the following article: “Ramírez-Sánchez, G., Zaragoza-Bernabeu, J., Bañón, M., &amp; Rojas, S.O. (2020). Bifixer and Bicleaner: two open-source tools to clean your parallel data. EAMT. ” and also in the official github pages: <url>https://github.com/bitextor/bifixer</url>, <url>https://github.com/bitextor/bicleaner</url>.</abstract>
      <url hash="c8c85b61">2022.wmt-1.27</url>
      <bibkey>malli-tambouratzis-2022-evaluating</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>PROMT</fixed-case> Systems for <fixed-case>WMT</fixed-case>22 General Translation Task</title>
      <author><first>Alexander</first><last>Molchanov</last><affiliation>Promt Llc</affiliation></author>
      <author><first>Vladislav</first><last>Kovalenko</last><affiliation>Promt Llc</affiliation></author>
      <author><first>Natalia</first><last>Makhamalkina</last><affiliation>Promt Llc</affiliation></author>
      <pages>342-345</pages>
      <abstract>The PROMT systems are trained with the MarianNMT toolkit. All systems use the transformer-big configuration. We use BPE for text encoding, the vocabulary sizes vary from 24k to 32k for different language pairs. All systems are unconstrained. We use all data provided by the WMT organizers, all publicly available data and some private data. We participate in four directions: English-Russian, English-German and German-English, Ukrainian-English.</abstract>
      <url hash="f175af0a">2022.wmt-1.28</url>
      <bibkey>molchanov-etal-2022-promt</bibkey>
    </paper>
    <paper id="29">
      <title>e<fixed-case>T</fixed-case>ranslation’s Submissions to the <fixed-case>WMT</fixed-case>22 General Machine Translation Task</title>
      <author><first>Csaba</first><last>Oravecz</last><affiliation>European Commission, Directorate-General for Translation</affiliation></author>
      <author><first>Katina</first><last>Bontcheva</last><affiliation>Sogeti</affiliation></author>
      <author><first>David</first><last>Kolovratnìk</last><affiliation>Fujitsu</affiliation></author>
      <author><first>Bogomil</first><last>Kovachev</last><affiliation>European Commission, Directorate-General for Translation</affiliation></author>
      <author><first>Christopher</first><last>Scott</last><affiliation>European Commission, Directorate-General for Translation</affiliation></author>
      <pages>346-351</pages>
      <abstract>The paper describes the NMT models for French-German, English-Ukranian and English-Russian, submitted by the eTranslation team to the WMT22 general machine translation shared task. In the WMT news task last year, multilingual systems with deep and complex architectures utilizing immense amount of data and resources were dominant. This year with the task extended to cover less domain specific text we expected even more dominance of such systems. In the hope to produce competitive (constrained) systems despite our limited resources, this time we selected only medium resource language pairs, which are serviced in the European Commission’s eTranslation system. We took the approach of exploring less resource intensive strategies focusing on data selection and filtering to improve the performance of baseline systems. With our submitted systems our approach scored competitively according to the automatic rankings, except for the the English–Russian model where our submission was only a baseline reference model developed as a by-product of the multilingual setup we built focusing primarily on the English-Ukranian language pair.</abstract>
      <url hash="cbaa1594">2022.wmt-1.29</url>
      <bibkey>oravecz-etal-2022-etranslations</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>CUNI</fixed-case> Systems for the <fixed-case>WMT</fixed-case> 22 <fixed-case>C</fixed-case>zech-<fixed-case>U</fixed-case>krainian Translation Task</title>
      <author><first>Martin</first><last>Popel</last><affiliation>Charles University, Faculty of Mathematics and Physics, UFAL</affiliation></author>
      <author><first>Jindřich</first><last>Libovický</last><affiliation>Charles Univeristy</affiliation></author>
      <author><first>Jindřich</first><last>Helcl</last><affiliation>Charles University in Prague</affiliation></author>
      <pages>352-357</pages>
      <abstract>We present Charles University submissions to the WMT 22 GeneralTranslation Shared Task on Czech-Ukrainian and Ukrainian-Czech machine translation. We present two constrained submissions based on block back-translation and tagged back-translation and experiment with rule-basedromanization of Ukrainian. Our results show that the romanization onlyhas a minor effect on the translation quality. Further, we describe Charles Translator,a system that was developed in March 2022 as a response to the migrationfrom Ukraine to the Czech Republic. Compared to our constrained systems,it did not use the romanization and used some proprietary data sources.</abstract>
      <url hash="ff87212d">2022.wmt-1.30</url>
      <bibkey>popel-etal-2022-cuni</bibkey>
    </paper>
    <paper id="31">
      <title>The <fixed-case>ARC</fixed-case>-<fixed-case>NKUA</fixed-case> Submission for the <fixed-case>E</fixed-case>nglish-<fixed-case>U</fixed-case>krainian General Machine Translation Shared Task at <fixed-case>WMT</fixed-case>22</title>
      <author><first>Dimitrios</first><last>Roussis</last><affiliation>ILSP/Athena RC</affiliation></author>
      <author><first>Vassilis</first><last>Papavassiliou</last><affiliation>ILSP/Athena RC</affiliation></author>
      <pages>358-365</pages>
      <abstract>The ARC-NKUA (“Athena” Research Center - National and Kapodistrian University of Athens) submission to the WMT22 General Machine Translation shared task concerns the unconstrained tracks of the English-Ukrainian and Ukrainian-English translation directions. The two Neural Machine Translation systems are based on Transformer models and our primary submissions were determined through experimentation with (a) ensemble decoding, (b) selected fine-tuning with a subset of the training data, (c) data augmentation with back-translated monolingual data, and (d) post-processing of the translation outputs. Furthermore, we discuss filtering techniques and the acquisition of additional data used for training the systems.</abstract>
      <url hash="e70d2862">2022.wmt-1.31</url>
      <bibkey>roussis-papavassiliou-2022-arc</bibkey>
    </paper>
    <paper id="32">
      <title>The <fixed-case>N</fixed-case>iu<fixed-case>T</fixed-case>rans Machine Translation Systems for <fixed-case>WMT</fixed-case>22</title>
      <author><first>Weiqiao</first><last>Shan</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Zhiquan</first><last>Cao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yuchen</first><last>Han</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Siming</first><last>Wu</last><affiliation>NiuTrans</affiliation></author>
      <author><first>Yimin</first><last>Hu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jie</first><last>Wang</last><affiliation>NiuTrans</affiliation></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>NiuTrans</affiliation></author>
      <author><first>Hou</first><last>Baoyu</last><affiliation>Neu</affiliation></author>
      <author><first>Hang</first><last>Cao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Chenghao</first><last>Gao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xiaowen</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Anxiang</first><last>Ma</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jingbo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <pages>366-374</pages>
      <abstract>This paper describes the NiuTrans neural machine translation systems of the WMT22 General MT constrained task. We participate in four directions, including Chinese→English, English→Croatian, and Livonian↔English. Our models are based on several advanced Transformer variants, e.g., Transformer-ODE, Universal Multiscale Transformer (UMST). The main workflow consists of data filtering, large-scale data augmentation (i.e., iterative back-translation, iterative knowledge distillation), and specific-domain fine-tuning. Moreover, we try several multi-domain methods, such as a multi-domain model structure and a multi-domain data clustering method, to rise to this year’s newly proposed multi-domain test set challenge. For low-resource scenarios, we build a multi-language translation model to enhance the performance, and try to use the pre-trained language model (mBERT) to initialize the translation model.</abstract>
      <url hash="1c4416d9">2022.wmt-1.32</url>
      <bibkey>shan-etal-2022-niutrans</bibkey>
    </paper>
    <paper id="33">
      <title>Teaching Unseen Low-resource Languages to Large Translation Models</title>
      <author><first>Maali</first><last>Tars</last><affiliation>University of Tartu</affiliation></author>
      <author><first>Taido</first><last>Purason</last><affiliation>University of Tartu</affiliation></author>
      <author><first>Andre</first><last>Tättar</last><affiliation>University of Tartu</affiliation></author>
      <pages>375-380</pages>
      <abstract>In recent years, large multilingual pre-trained neural machine translation model research has grown and it is common for these models to be publicly available for usage and fine-tuning. Low-resource languages benefit from the pre-trained models, because of knowledge transfer from high- to medium-resource languages. The recently available M2M-100 model is our starting point for cross-lingual transfer learning to Finno-Ugric languages, like Livonian. We participate in the WMT22 General Machine Translation task, where we focus on the English-Livonian language pair. We leverage data from other Finno-Ugric languages and through that, we achieve high scores for English-Livonian translation directions. Overall, instead of training a model from scratch, we use transfer learning and back-translation as the main methods and fine-tune a publicly available pre-trained model. This in turn reduces the cost and duration of training high-quality multilingual neural machine translation models.</abstract>
      <url hash="6a03b9fc">2022.wmt-1.33</url>
      <bibkey>tars-etal-2022-teaching</bibkey>
    </paper>
    <paper id="34">
      <title>Can Domains Be Transferred across Languages in Multi-Domain Multilingual Neural Machine Translation?</title>
      <author><first>Thuy-trang</first><last>Vu</last><affiliation>Monash University</affiliation></author>
      <author><first>Shahram</first><last>Khadivi</last><affiliation>eBay</affiliation></author>
      <author><first>Xuanli</first><last>He</last><affiliation>Monash University</affiliation></author>
      <author><first>Dinh</first><last>Phung</last><affiliation>Monash University, Australia</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University</affiliation></author>
      <pages>381-396</pages>
      <abstract>Previous works mostly focus on either multilingual or multi-domain aspects of neural machine translation (NMT). This paper investigates whether the domain information can be transferred across languages on the composition of multi-domain and multilingual NMT, particularly for the incomplete data condition where in-domain bitext is missing for some language pairs. Our results in the curated leave-one-domain-out experiments show that multi-domain multilingual (MDML) NMT can boost zero-shot translation performance up to +10 gains on BLEU, as well as aid the generalisation of multi-domain NMT to the missing domain. We also explore strategies for effective integration of multilingual and multi-domain NMT, including language and domain tag combination and auxiliary task training. We find that learning domain-aware representations and adding target-language tags to the encoder leads to effective MDML-NMT.</abstract>
      <url hash="a7050285">2022.wmt-1.34</url>
      <bibkey>vu-etal-2022-domains</bibkey>
      <video href="2022.wmt-1.34.mp4"/>
    </paper>
    <paper id="35">
      <title><fixed-case>DUTNLP</fixed-case> Machine Translation System for <fixed-case>WMT</fixed-case>22 General <fixed-case>MT</fixed-case> Task</title>
      <author><first>Ting</first><last>Wang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Huan</first><last>Liu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Junpeng</first><last>Liu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Degen</first><last>Huang</last><affiliation>Dalian University of Technology</affiliation></author>
      <pages>397-402</pages>
      <abstract>This paper describes DUTNLP Lab’s submission to the WMT22 General MT Task on four translation directions: English to/from Chinese and English to/from Japanese under the constrained condition. Our primary system are built on several Transformer variants which employ wider FFN layer or deeper encoder layer. The bilingual data are filtered by detailed data pre-processing strategies and four data augmentation methods are combined to enlarge the training data with the provided monolingual data. Several common methods are also employed to further improve the model performance, such as fine-tuning, model ensemble and post-editing. As a result, our constrained systems achieve 29.01, 63.87, 41.84, and 24.82 BLEU scores on Chinese-to-English, English-to-Chinese, English-to-Japanese, and Japanese-to-English, respectively.</abstract>
      <url hash="48891ad5">2022.wmt-1.35</url>
      <bibkey>wang-etal-2022-dutnlp</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Submissions to the <fixed-case>WMT</fixed-case> 2022 General Machine Translation Shared Task</title>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Yuanchang</first><last>Luo</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Yuhao</first><last>Xie</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Jinlong</first><last>Yang</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Miaomiao</first><last>Ma</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Lizhi</first><last>Lei</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <author><first>Ying</first><last>Qin</last><affiliation>Huawei Translation Service Center</affiliation></author>
      <pages>403-410</pages>
      <abstract>This paper presents the submissions of Huawei Translate Services Center (HW-TSC) to the WMT 2022 General Machine Translation Shared Task. We participate in 6 language pairs, including Zh↔En, Ru↔En, Uk↔En, Hr↔En, Uk↔Cs and Liv↔En. We use Transformer architecture and obtain the best performance via multiple variants with larger parameter sizes. We perform fine-grained pre-processing and filtering on the provided large-scale bilingual and monolingual datasets. For medium and highresource languages, we mainly use data augmentation strategies, including Back Translation, Self Training, Ensemble Knowledge Distillation, Multilingual, etc. For low-resource languages such as Liv, we use pre-trained machine translation models, and then continue training with Regularization Dropout (R-Drop). The previous mentioned data augmentation methods are also used. Our submissions obtain competitive results in the final evaluation.</abstract>
      <url hash="fe2fad28">2022.wmt-1.36</url>
      <bibkey>wei-etal-2022-hw</bibkey>
    </paper>
    <paper id="37">
      <title>Vega-<fixed-case>MT</fixed-case>: The <fixed-case>JD</fixed-case> Explore Academy Machine Translation System for <fixed-case>WMT</fixed-case>22</title>
      <author><first>Changtong</first><last>Zan</last><affiliation>China University of Petroleum(East China)</affiliation></author>
      <author><first>Keqin</first><last>Peng</last><affiliation>Beihang University</affiliation></author>
      <author><first>Liang</first><last>Ding</last><affiliation>JD Explore Academy, JD.com Inc. &amp; The University of Sydney</affiliation></author>
      <author><first>Baopu</first><last>Qiu</last><affiliation>Nanjing University,State Key Laboratory for Novel Software Technology</affiliation></author>
      <author><first>Boan</first><last>Liu</last><affiliation>JD Explore Academy</affiliation></author>
      <author><first>Shwai</first><last>He</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Qingyu</first><last>Lu</last><affiliation>School of Automation, Southeast University</affiliation></author>
      <author><first>Zheng</first><last>Zhang</last><affiliation>Whu</affiliation></author>
      <author><first>Chuang</first><last>Liu</last><affiliation>Whu</affiliation></author>
      <author><first>Weifeng</first><last>Liu</last><affiliation>China University of Petroleum</affiliation></author>
      <author><first>Yibing</first><last>Zhan</last><affiliation>JD Explore Academy</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>School of Computer Science, The University of Sydney</affiliation></author>
      <pages>411-422</pages>
      <url hash="7406c0fd">2022.wmt-1.37</url>
      <bibkey>zan-etal-2022-vega</bibkey>
      <abstract>We describe the JD Explore Academy’s submission of the WMT 2022 shared general translation task. We participated in all high-resource tracks and one medium-resource track, including Chinese-English, German-English, Czech-English, Russian-English, and Japanese-English. We push the limit of our previous work – bidirectional training for translation by scaling up two main factors, i.e. language pairs and model sizes, namely the <b>Vega-MT</b> system. As for language pairs, we scale the “bidirectional” up to the “multidirectional” settings, covering all participating languages, to exploit the common knowledge across languages, and transfer them to the downstream bilingual tasks. As for model sizes, we scale the Transformer-Big up to the extremely large model that owns nearly 4.7 Billion parameters, to fully enhance the model capacity for our Vega-MT. Also, we adopt the data augmentation strategies, e.g. cycle translation for monolingual data, and bidirectional self-training for bilingual and monolingual data, to comprehensively exploit the bilingual and monolingual data. To adapt our Vega-MT to the general domain test set, generalization tuning is designed. Based on the official automatic scores of constrained systems, in terms of the sacreBLEU shown in Figure-1, we got the 1st place on Zh-En (33.5), En-Zh (49.7), De-En (33.7), En-De (37.8), Cs-En (54.9), En-Cs (41.4) and En-Ru (32.7), 2nd place on Ru-En (45.1) and Ja-En (25.6), and 3rd place on En-Ja(41.5), respectively; W.R.T the COMET, we got the 1st place on Zh-En (45.1), En-Zh (61.7), De-En (58.0), En-De (63.2), Cs-En (74.7), Ru-En (64.9), En-Ru (69.6) and En-Ja (65.1), 2nd place on En-Cs (95.3) and Ja-En (40.6), respectively. Models will be released to facilitate the MT community through GitHub and OmniForce Platform.</abstract>
      <revision id="1" href="2022.wmt-1.37v1" hash="11b3ea6a"/>
      <revision id="2" href="2022.wmt-1.37v2" hash="1ac889d3" date="2024-03-18">Change in acknowledgment.</revision>
      <revision id="3" href="2022.wmt-1.37v3" hash="7406c0fd" date="2024-03-29">A sponsor was added to the Acknowledgments section.</revision>
    </paper>
    <paper id="38">
      <title>No Domain Left behind</title>
      <author><first>Hui</first><last>Zeng</last><affiliation>Jiaguyi Language Technology Co., Ltd</affiliation></author>
      <pages>423-427</pages>
      <abstract>We participated in the WMT General MT task and focus on four high resource language pairs: English to Chinese, Chinese to English, English to Japanese and Japanese to English). The submitted systems (LanguageX) focus on data cleaning, data selection, data mixing and TM-augmented NMT. Rules and multilingual language model are used for data filtering and data selection. In the automatic evaluation, our best submitted English to Chinese system achieved 54.3 BLEU score and 63.8 COMET score, which is the highest among all the submissions.</abstract>
      <url hash="351973cf">2022.wmt-1.38</url>
      <bibkey>zeng-2022-domain</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>GTCOM</fixed-case> Neural Machine Translation Systems for <fixed-case>WMT</fixed-case>22</title>
      <author><first>Hao</first><last>Zong</last><affiliation>Global Tone Communication Technology Co., Ltd</affiliation></author>
      <author><first>Chao</first><last>Bei</last><affiliation>Global Tone Communication Technology Co.,Ltd.</affiliation></author>
      <pages>428-431</pages>
      <abstract>GTCOM participates in five directions: English to/from Ukrainian, Ukrainian to/from Czech, English to Chinese and English to Croatian. Our submitted systems are unconstrained and focus on backtranslation, multilingual translation model and finetuning. Multilingual translation model focus on X to one and one to X. We also apply rules and language model to filter monolingual, parallel sentences and synthetic sentences.</abstract>
      <url hash="cbcab642">2022.wmt-1.39</url>
      <bibkey>zong-bei-2022-gtcom</bibkey>
    </paper>
    <paper id="40">
      <title>Linguistically Motivated Evaluation of the 2022 State-of-the-art Machine Translation Systems for Three Language Directions</title>
      <author><first>Vivien</first><last>Macketanz</last><affiliation>German Research Center for Artificial Intelligence (DFKI)</affiliation></author>
      <author><first>Shushen</first><last>Manakhimova</last><affiliation>Dfki</affiliation></author>
      <author><first>Eleftherios</first><last>Avramidis</last><affiliation>German Research Center for Artificial Intelligence (DFKI)</affiliation></author>
      <author><first>Ekaterina</first><last>Lapshinova-koltunski</last><affiliation>Stiftung Universität Hildesheim</affiliation></author>
      <author><first>Sergei</first><last>Bagdasarov</last><affiliation>Universit�t des Saarlandes</affiliation></author>
      <author><first>Sebastian</first><last>Möller</last><affiliation>Quality and Usability Lab, TU Berlin</affiliation></author>
      <pages>432-449</pages>
      <abstract>This document describes a fine-grained linguistically motivated analysis of 29 machine translation systems submitted at the Shared Task of the 7th Conference of Machine Translation (WMT22). This submission expands the test suite work of previous years by adding the language direction of English–Russian. As a result, evaluation takes place for the language directions of German–English, English–German, and English–Russian. We find that the German–English systems suffer in translating idioms, some tenses of modal verbs, and resultative predicates, the English–German ones in idioms, transitive-past progressive, and middle voice, whereas the English–Russian ones in pseudogapping and idioms.</abstract>
      <url hash="3d150f96">2022.wmt-1.40</url>
      <bibkey>macketanz-etal-2022-linguistically-motivated</bibkey>
    </paper>
    <paper id="41">
      <title>Automated Evaluation Metric for Terminology Consistency in <fixed-case>MT</fixed-case></title>
      <author><first>Kirill</first><last>Semenov</last><affiliation>Higher School of Economics - Moscow</affiliation></author>
      <author><first>Ondřej</first><last>Bojar</last><affiliation>Charles University, MFF UFAL</affiliation></author>
      <pages>450-457</pages>
      <abstract>The most widely used metrics for machine translation tackle sentence-level evaluation. However, at least for professional domains such as legal texts, it is crucial to measure the consistency of the translation of the terms throughout the whole text. This paper introduces an automated metric for the term consistency evaluation in machine translation (MT). To demonstrate the metric’s performance, we used the Czech-to-English translated texts from the ELITR 2021 agreement corpus and the outputs of the MT systems that took part in WMT21 News Task. We show different modes of our evaluation algorithm and try to interpret the differences in the ranking of the translation systems based on sentence-level metrics and our approach. We also demonstrate that the proposed metric scores significantly differ from the widespread automated metric scores, and correlate with the human assessment.</abstract>
      <url hash="159f2d57">2022.wmt-1.41</url>
      <bibkey>semenov-bojar-2022-automated</bibkey>
    </paper>
    <paper id="42">
      <title>Test Suite Evaluation: Morphological Challenges and Pronoun Translation</title>
      <author><first>Marion</first><last>Weller-di Marco</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>458-468</pages>
      <abstract>This paper summarizes the results of our test suite evaluation with a main focus on morphology for the language pairs English to/from German. We look at the translation of morphologically complex words (DE–EN), and evaluatewhether English noun phrases are translated as compounds vs. phrases into German. Furthermore, we investigate the preservation of morphological features (gender in EN–DE pronoun translation and number in morpho-syntacticallycomplex structures for DE–EN). Our results indicate that systems are able to interpret linguistic structures to obtain relevant information, but also that translation becomes more challenging with increasing complexity, as seen, for example, when translating words with negation or non-concatenative properties, and for the morecomplex cases of the pronoun translation task.</abstract>
      <url hash="bf60fe9b">2022.wmt-1.42</url>
      <bibkey>weller-di-marco-fraser-2022-test</bibkey>
    </paper>
    <paper id="43">
      <title>Robust <fixed-case>MT</fixed-case> Evaluation with Sentence-level Multilingual Augmentation</title>
      <author><first>Duarte</first><last>Alves</last><affiliation>Instituto Superior Técnico / Unbabel</affiliation></author>
      <author><first>Ricardo</first><last>Rei</last><affiliation>Unbabel/INESC-ID</affiliation></author>
      <author><first>Ana C</first><last>Farinha</last><affiliation>Unbabel</affiliation></author>
      <author><first>José G.</first><last>C. de Souza</last><affiliation>Unbabel</affiliation></author>
      <author><first>André F. T.</first><last>Martins</last><affiliation>Unbabel, Instituto de Telecomunicacoes</affiliation></author>
      <pages>469-478</pages>
      <abstract>Automatic translations with critical errors may lead to misinterpretations and pose several risks for the user. As such, it is important that Machine Translation (MT) Evaluation systems are robust to these errors in order to increase the reliability and safety of Machine Translation systems. Here we introduce SMAUG a novel Sentence-level Multilingual AUGmentation approach for generating translations with critical errors and apply this approach to create a test set to evaluate the robustness of MT metrics to these errors. We show that current State-of-the-Art metrics are improving their capability to distinguish translations with and without critical errors and to penalize the first accordingly. We also show that metrics tend to struggle with errors related to named entities and numbers and that there is a high variance in the robustness of current methods to translations with critical errors.</abstract>
      <url hash="8cc84d13">2022.wmt-1.43</url>
      <attachment type="software" hash="f42c89e0">2022.wmt-1.43.software.zip</attachment>
      <bibkey>alves-etal-2022-robust</bibkey>
    </paper>
    <paper id="44">
      <title><fixed-case>ACES</fixed-case>: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics</title>
      <author><first>Chantal</first><last>Amrhein</last><affiliation>Department of Computational Linguistics, University of Zurich</affiliation></author>
      <author><first>Nikita</first><last>Moghe</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Liane</first><last>Guillou</last><affiliation>The University of Edinburgh</affiliation></author>
      <pages>479-513</pages>
      <abstract>As machine translation (MT) metrics improve their correlation with human judgement every year, it is crucial to understand the limitations of these metrics at the segment level. Specifically, it is important to investigate metric behaviour when facing accuracy errors in MT because these can have dangerous consequences in certain contexts (e.g., legal, medical). We curate ACES, a translation accuracy challenge set, consisting of 68 phenomena ranging from simple perturbations at the word/character level to more complex errors based on discourse and real-world knowledge. We use ACES to evaluate a wide range of MT metrics including the submissions to the WMT 2022 metrics shared task and perform several analyses leading to general recommendations for metric developers. We recommend: a) combining metrics with different strengths, b) developing metrics that give more weight to the source and less to surface-level overlap with the reference and c) explicitly modelling additional language-specific information beyond what is available via multilingual embeddings.</abstract>
      <url hash="26a22e0f">2022.wmt-1.44</url>
      <bibkey>amrhein-etal-2022-aces</bibkey>
      <revision id="1" href="2022.wmt-1.44v1" hash="2e00cec3"/>
      <revision id="2" href="2022.wmt-1.44v2" hash="26a22e0f" date="2023-02-21">Corrected tables.</revision>
    </paper>
    <paper id="45">
      <title>Linguistically Motivated Evaluation of Machine Translation Metrics Based on a Challenge Set</title>
      <author><first>Eleftherios</first><last>Avramidis</last><affiliation>German Research Center for Artificial Intelligence (DFKI)</affiliation></author>
      <author><first>Vivien</first><last>Macketanz</last><affiliation>German Research Center for Artificial Intelligence (DFKI)</affiliation></author>
      <pages>514-529</pages>
      <abstract>We employ a linguistically motivated challenge set in order to evaluate the state-of-the-art machine translation metrics submitted to the Metrics Shared Task of the 7th Conference for Machine Translation. The challenge set includes about 20,000 items extracted from 145 MT systems for two language directions (German-English, English-German), covering more than 100 linguistically-motivated phenomena organized in 14 categories. The best performing metrics are YiSi-1, BERTScore and COMET-22 for German-English, and UniTE, UniTE-ref, XL-DA and xxl-DA19 for English-German.Metrics in both directions are performing worst when it comes to named-entities &amp; terminology and particularly measuring units. Particularly in German-English they are weak at detecting issues at punctuation, polar questions, relative clauses, dates and idioms. In English-German, they perform worst at present progressive of transitive verbs, future II progressive of intransitive verbs, simple present perfect of ditransitive verbs and focus particles.</abstract>
      <url hash="ee495fcf">2022.wmt-1.45</url>
      <bibkey>avramidis-macketanz-2022-linguistically</bibkey>
    </paper>
    <paper id="46">
      <title>Exploring Robustness of Machine Translation Metrics: A Study of Twenty-Two Automatic Metrics in the <fixed-case>WMT</fixed-case>22 Metric Task</title>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ting</first><last>Zhu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Mengli</first><last>Zhu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ning</first><last>Xie</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Lizhi</first><last>Lei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Shimin</first><last>Tao</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ying</first><last>Qin</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <pages>530-540</pages>
      <abstract>Contextual word embeddings extracted from pre-trained models have become the basis for many downstream NLP tasks, including machine translation automatic evaluations. Metrics that leverage embeddings claim better capture of synonyms and changes in word orders, and thus better correlation with human ratings than surface-form matching metrics (e.g. BLEU). However, few studies have been done to examine robustness of these metrics. This report uses a challenge set to uncover the brittleness of reference-based and reference-free metrics. Our challenge set1 aims at examining metrics’ capability to correlate synonyms in different areas and to discern catastrophic errors at both word- and sentence-levels. The results show that although embedding-based metrics perform relatively well on discerning sentence-level negation/affirmation errors, their performances on relating synonyms are poor. In addition, we find that some metrics are susceptible to text styles so their generalizability compromised.</abstract>
      <url hash="4dc199a0">2022.wmt-1.46</url>
      <bibkey>chen-etal-2022-exploring</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>MS</fixed-case>-<fixed-case>COMET</fixed-case>: More and Better Human Judgements Improve Metric Performance</title>
      <author><first>Tom</first><last>Kocmi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hitokazu</first><last>Matsushita</last><affiliation>Microsoft</affiliation></author>
      <author><first>Christian</first><last>Federmann</last><affiliation>Microsoft</affiliation></author>
      <pages>541-548</pages>
      <abstract>We develop two new metrics that build on top of the COMET architecture. The main contribution is collecting a ten-times larger corpus of human judgements than COMET and investigating how to filter out problematic human judgements. We propose filtering human judgements where human reference is statistically worse than machine translation. Furthermore, we average scores of all equal segments evaluated multiple times. The results comparing automatic metrics on source-based DA and MQM-style human judgement show state-of-the-art performance on a system-level pair-wise system ranking. We release both of our metrics for public use.</abstract>
      <url hash="20b6bce2">2022.wmt-1.47</url>
      <bibkey>kocmi-etal-2022-ms</bibkey>
    </paper>
    <paper id="48">
      <title>Partial Could Be Better than Whole. <fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> 2022 Submission for the Metrics Shared Task</title>
      <author><first>Yilun</first><last>Liu</last><affiliation>2012 Lab, Huawei co. LTD</affiliation></author>
      <author><first>Xiaosong</first><last>Qiao</last><affiliation>Huawei Technologies Co Ltd</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Su</first><last>Chang</last><affiliation>Huawei TSC</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Huawei</affiliation></author>
      <author><first>Yanqing</first><last>Zhao</last><affiliation>Huawei</affiliation></author>
      <author><first>Song</first><last>Peng</last><affiliation>Huawei</affiliation></author>
      <author><first>Shimin</first><last>Tao</last><affiliation>Huawei</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <author><first>Ying</first><last>Qin</last><affiliation>Huawei Technologies</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei</affiliation></author>
      <author><first>Minghan</first><last>Wang</last><affiliation>Huawei</affiliation></author>
      <author><first>Yinglu</first><last>Li</last><affiliation>Huawei</affiliation></author>
      <author><first>Peng</first><last>Li</last><affiliation>Huawei</affiliation></author>
      <author><first>Xiaofeng</first><last>Zhao</last><affiliation>Huawei</affiliation></author>
      <pages>549-557</pages>
      <abstract>In this paper, we present the contribution of HW-TSC to WMT 2022 Metrics Shared Task. We propose one reference-based metric, HWTSC-EE-BERTScore*, and four referencefree metrics including HWTSC-Teacher-Sim, HWTSC-TLM, KG-BERTScore and CROSSQE. Among these metrics, HWTSC-Teacher-Sim and CROSS-QE are supervised, whereas HWTSC-EE-BERTScore*, HWTSC-TLM and KG-BERTScore are unsupervised. We use these metrics in the segment-level and systemlevel tracks. Overall, our systems achieve strong results for all language pairs on previous test sets and a new state-of-the-art in many sys-level case sets.</abstract>
      <url hash="d9ae83a1">2022.wmt-1.48</url>
      <bibkey>liu-etal-2022-partial</bibkey>
    </paper>
    <paper id="49">
      <title>Unsupervised Embedding-based Metric for <fixed-case>MT</fixed-case> Evaluation with Improved Human Correlation</title>
      <author><first>Ananya</first><last>Mukherjee</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Manish</first><last>Shrivastava</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <pages>558-563</pages>
      <abstract>In this paper, we describe our submission to the WMT22 metrics shared task. Our metric focuses on computing contextual and syntactic equivalences along with lexical, morphological, and semantic similarity. The intent is to capture the fluency and context of the MT outputs along with their adequacy. Fluency is captured using syntactic similarity and context is captured using sentence similarity leveraging sentence embeddings. The final sentence translation score is the weighted combination of three similarity scores: a) Syntactic Similarity b) Lexical, Morphological and Semantic Similarity, and c) Contextual Similarity. This paper outlines two improved versions of MEE i.e., MEE2 and MEE4. Additionally, we report our experiments on language pairs of en-de, en-ru and zh-en from WMT17-19 testset and further depict the correlation with human assessments.</abstract>
      <url hash="717c4735">2022.wmt-1.49</url>
      <bibkey>mukherjee-shrivastava-2022-unsupervised</bibkey>
    </paper>
    <paper id="50">
      <title><fixed-case>REUSE</fixed-case>: <fixed-case>RE</fixed-case>ference-free <fixed-case>U</fixed-case>n<fixed-case>S</fixed-case>upervised Quality Estimation Metric</title>
      <author><first>Ananya</first><last>Mukherjee</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Manish</first><last>Shrivastava</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <pages>564-568</pages>
      <abstract>This paper describes our submission to the WMT2022 shared metrics task. Our unsupervised metric estimates the translation quality at chunk-level and sentence-level. Source and target sentence chunks are retrieved by using a multi-lingual chunker. The chunk-level similarity is computed by leveraging BERT contextual word embeddings and sentence similarity scores are calculated by leveraging sentence embeddings of Language-Agnostic BERT models. The final quality estimation score is obtained by mean pooling the chunk-level and sentence-level similarity scores. This paper outlines our experiments and also reports the correlation with human judgements for en-de, en-ru and zh-en language pairs of WMT17, WMT18 and WMT19 test sets.</abstract>
      <url hash="7613cdd8">2022.wmt-1.50</url>
      <bibkey>mukherjee-shrivastava-2022-reuse</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>M</fixed-case>a<fixed-case>TES</fixed-case>e: Machine Translation Evaluation as a Sequence Tagging Problem</title>
      <author><first>Stefano</first><last>Perrella</last><affiliation>Sapienza University of Rome</affiliation></author>
      <author><first>Lorenzo</first><last>Proietti</last><affiliation>Sapienza University of Rome</affiliation></author>
      <author><first>Alessandro</first><last>Scirè</last><affiliation>Babelscape and Sapienza University of Rome</affiliation></author>
      <author><first>Niccolò</first><last>Campolungo</last><affiliation>La Sapienza University</affiliation></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>569-577</pages>
      <abstract>Starting from last year, WMT human evaluation has been performed within the Multidimensional Quality Metrics (MQM) framework, where human annotators are asked to identify error spans in translations, alongside an error category and a severity. In this paper, we describe our submission to the WMT 2022 Metrics Shared Task, where we propose using the same paradigm for automatic evaluation: we present the MaTESe metrics, which reframe machine translation evaluation as a sequence tagging problem. Our submission also includes a reference-free metric, denominated MaTESe-QE. Despite the paucity of the openly available MQM data, our metrics obtain promising results, showing high levels of correlation with human judgements, while also enabling an evaluation that is interpretable. Moreover, MaTESe-QE can also be employed in settings where it is infeasible to curate reference translations manually.</abstract>
      <url hash="b5728b64">2022.wmt-1.51</url>
      <bibkey>perrella-etal-2022-matese</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>COMET</fixed-case>-22: Unbabel-<fixed-case>IST</fixed-case> 2022 Submission for the Metrics Shared Task</title>
      <author><first>Ricardo</first><last>Rei</last><affiliation>Unbabel/INESC-ID</affiliation></author>
      <author><first>José G.</first><last>C. de Souza</last><affiliation>Unbabel</affiliation></author>
      <author><first>Duarte</first><last>Alves</last><affiliation>Instituto Superior Técnico / Unbabel</affiliation></author>
      <author><first>Chrysoula</first><last>Zerva</last><affiliation>Instituto de Telecomunicações, Instituto Superior Técnico, University of Lisbon</affiliation></author>
      <author><first>Ana C</first><last>Farinha</last><affiliation>Unbabel</affiliation></author>
      <author><first>Taisiya</first><last>Glushkova</last><affiliation>Instituto de Telecomunicações, Instituto Superior Técnico, University of Lisbon</affiliation></author>
      <author><first>Alon</first><last>Lavie</last><affiliation>Unbabel/Carnegie Mellon University</affiliation></author>
      <author><first>Luisa</first><last>Coheur</last><affiliation>INESC-ID/Instituto Superior Técnico</affiliation></author>
      <author><first>André F. T.</first><last>Martins</last><affiliation>Unbabel, Instituto de Telecomunicacoes</affiliation></author>
      <pages>578-585</pages>
      <abstract>In this paper, we present the joint contribution of Unbabel and IST to the WMT 2022 Metrics Shared Task. Our primary submission – dubbed COMET-22 – is an ensemble between a COMET estimator model trained with Direct Assessments and a newly proposed multitask model trained to predict sentence-level scores along with OK/BAD word-level tags derived from Multidimensional Quality Metrics error annotations. These models are ensembled together using a hyper-parameter search that weights different features extracted from both evaluation models and combines them into a single score. For the reference-free evaluation, we present CometKiwi. Similarly to our primary submission, CometKiwi is an ensemble between two models. A traditional predictor-estimator model inspired by OpenKiwi and our new multitask model trained on Multidimensional Quality Metrics which can also be used without references. Both our submissions show improved correlations compared to state-of-the-art metrics from last year as well as increased robustness to critical errors.</abstract>
      <url hash="738309e0">2022.wmt-1.52</url>
      <bibkey>rei-etal-2022-comet</bibkey>
    </paper>
    <paper id="53">
      <title><fixed-case>A</fixed-case>libaba-Translate <fixed-case>C</fixed-case>hina’s Submission for <fixed-case>WMT</fixed-case>2022 Metrics Shared Task</title>
      <author><first>Yu</first><last>Wan</last><affiliation>NLP2CT Lab, University of Macau</affiliation></author>
      <author><first>Keqin</first><last>Bao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Dayiheng</first><last>Liu</last><affiliation>Alibaba DAMO Academy</affiliation></author>
      <author><first>Baosong</first><last>Yang</last><affiliation>Alibaba Damo Academy, Alibaba Inc.</affiliation></author>
      <author><first>Derek F.</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Lidia S.</first><last>Chao</last><affiliation>University of Macau</affiliation></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Jun</first><last>Xie</last><affiliation>Alibaba DAMO Academy</affiliation></author>
      <pages>586-592</pages>
      <abstract>In this report, we present our submission to the WMT 2022 Metrics Shared Task. We build our system based on the core idea of UNITE (Unified Translation Evaluation), which unifies source-only, reference-only, and source- reference-combined evaluation scenarios into one single model. Specifically, during the model pre-training phase, we first apply the pseudo-labeled data examples to continuously pre-train UNITE. Notably, to reduce the gap between pre-training and fine-tuning, we use data cropping and a ranking-based score normalization strategy. During the fine-tuning phase, we use both Direct Assessment (DA) and Multidimensional Quality Metrics (MQM) data from past years’ WMT competitions. Specially, we collect the results from models with different pre-trained language model backbones, and use different ensembling strategies for involved translation directions.</abstract>
      <url hash="4db4290a">2022.wmt-1.53</url>
      <bibkey>wan-etal-2022-alibaba</bibkey>
    </paper>
    <paper id="54">
      <title>Quality Estimation via Backtranslation at the <fixed-case>WMT</fixed-case> 2022 Quality Estimation Task</title>
      <author><first>Sweta</first><last>Agrawal</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Nikita</first><last>Mehandru</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Niloufar</first><last>Salehi</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Marine</first><last>Carpuat</last><affiliation>University of Maryland</affiliation></author>
      <pages>593-596</pages>
      <abstract>This paper describes submission to the WMT 2022 Quality Estimation shared task (Task 1: sentence-level quality prediction). We follow a simple and intuitive approach, which consists of estimating MT quality by automatically back-translating hypotheses into the source language using a multilingual MT system. We then compare the resulting backtranslation with the original source using standard MT evaluation metrics. We find that even the best-performing backtranslation-based scores perform substantially worse than supervised QE systems, including the organizers’ baseline. However, combining backtranslation-based metrics with off-the-shelf QE scorers improves correlation with human judgments, suggesting that they can indeed complement a supervised QE system.</abstract>
      <url hash="09a2caaa">2022.wmt-1.54</url>
      <bibkey>agrawal-etal-2022-quality</bibkey>
      <revision id="1" href="2022.wmt-1.54v1" hash="9d0eee09"/>
      <revision id="2" href="2022.wmt-1.54v2" hash="09a2caaa" date="2023-02-24">Corrected Acknowledgement.</revision>
    </paper>
    <paper id="55">
      <title><fixed-case>A</fixed-case>libaba-Translate <fixed-case>C</fixed-case>hina’s Submission for <fixed-case>WMT</fixed-case> 2022 Quality Estimation Shared Task</title>
      <author><first>Keqin</first><last>Bao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yu</first><last>Wan</last><affiliation>NLP2CT Lab, University of Macau</affiliation></author>
      <author><first>Dayiheng</first><last>Liu</last><affiliation>Alibaba DAMO Academy</affiliation></author>
      <author><first>Baosong</first><last>Yang</last><affiliation>Alibaba Damo Academy, Alibaba Inc.</affiliation></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Xiangnan</first><last>He</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Derek F.</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Jun</first><last>Xie</last><affiliation>Alibaba DAMO Academy</affiliation></author>
      <pages>597-605</pages>
      <url hash="e70c930d">2022.wmt-1.55</url>
      <bibkey>bao-etal-2022-alibaba</bibkey>
      <abstract>In this paper, we present our submission to the sentence-level MQM benchmark at Quality Estimation Shared Task, named UniTE (Unified Translation Evaluation). Specifically, our systems employ the framework of UniTE, which combined three types of input formats during training with a pre-trained language model. First, we apply the pseudo-labeled data examples for the continuously pre-training phase. Notably, to reduce the gap between pre-training and fine-tuning, we use data cropping and a ranking-based score normalization strategy. For the fine-tuning phase, we use both Direct Assessment (DA) and Multidimensional Quality Metrics (MQM) data from past years’ WMT competitions. Finally, we collect the source-only evaluation results, and ensemble the predictions generated by two UniTE models, whose backbones are XLM-R and infoXLM, respectively. Results show that our models reach 1st overall ranking in the Multilingual and English-Russian settings, and 2nd overall ranking in English-German and Chinese-English settings, showing relatively strong performances in this year’s quality estimation competition.</abstract>
    </paper>
    <paper id="56">
      <title><fixed-case>KU</fixed-case> <fixed-case>X</fixed-case> Upstage’s Submission for the <fixed-case>WMT</fixed-case>22 Quality Estimation: Critical Error Detection Shared Task</title>
      <author><first>Sugyeong</first><last>Eo</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Upstage</affiliation></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaehyung</first><last>Seo</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>606-614</pages>
      <abstract>This paper presents KU X Upstage’s submission to the quality estimation (QE): critical error detection (CED) shared task in WMT22. We leverage the XLM-RoBERTa large model without utilizing any additional parallel data. To the best of our knowledge, we apply prompt-based fine-tuning to the QE task for the first time. To maximize the model’s language understanding capability, we reformulate the CED task to be similar to the masked language model objective, which is a pre-training strategy of the language model. We design intuitive templates and label words, and include auxiliary descriptions such as demonstration or Google Translate results in the input sequence. We further improve the performance through the template ensemble, and as a result of the shared task, our approach achieve the best performance for both English-German and Portuguese-English language pairs in an unconstrained setting.</abstract>
      <url hash="3599d093">2022.wmt-1.56</url>
      <bibkey>eo-etal-2022-ku</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>NJUNLP</fixed-case>’s Participation for the <fixed-case>WMT</fixed-case>2022 Quality Estimation Shared Task</title>
      <author><first>Xiang</first><last>Geng</last><affiliation>National Key Laboratory for Novel Software Technology, Nanjing University</affiliation></author>
      <author><first>Yu</first><last>Zhang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>National Key Laboratory for Novel Software Technology, Nanjing University</affiliation></author>
      <author><first>Shimin</first><last>Tao</last><affiliation>Huawei</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <pages>615-620</pages>
      <abstract>This paper presents submissions of the NJUNLP team in WMT 2022Quality Estimation shared task 1, where the goal is to predict the sentence-level and word-level quality for target machine translations. Our system explores pseudo data and multi-task learning. We propose several novel methods to generate pseudo data for different annotations using the conditional masked language model and the neural machine translation model. The proposed methods control the decoding process to generate more real pseudo translations. We pre-train the XLMR-large model with pseudo data and then fine-tune this model with real data both in the way of multi-task learning. We jointly learn sentence-level scores (with regression and rank tasks) and word-level tags (with a sequence tagging task). Our system obtains competitive results on different language pairs and ranks first place on both sentence- and word-level sub-tasks of the English-German language pair.</abstract>
      <url hash="62c2c8f3">2022.wmt-1.57</url>
      <bibkey>geng-etal-2022-njunlps</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>BJTU</fixed-case>-Toshiba’s Submission to <fixed-case>WMT</fixed-case>22 Quality Estimation Shared Task</title>
      <author><first>Hui</first><last>Huang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Hui</first><last>Di</last><affiliation>Toshiba (China) Co., Ltd.</affiliation></author>
      <author><first>Chunyou</first><last>Li</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Hanming</first><last>Wu</last><affiliation>BeiJing Jiaotong University</affiliation></author>
      <author><first>Kazushige</first><last>Ouchi</last><affiliation>Toshiba (China) Co., Ltd.</affiliation></author>
      <author><first>Yufeng</first><last>Chen</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jian</first><last>Liu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>621-626</pages>
      <abstract>This paper presents the BJTU-Toshiba joint submission for WMT 2022 quality estimation shared task. We only participate in Task 1 (quality prediction) of the shared task, focusing on the sentence-level MQM prediction. The techniques we experimented with include the integration of monolingual language models and the pre-finetuning of pre-trained representations. We tried two styles of pre-finetuning, namely Translation Language Modeling and Replaced Token Detection. We demonstrate the competitiveness of our system compared to the widely adopted XLM-RoBERTa baseline. Our system is also the top-ranking system on the Sentence-level MQM Prediction for the English-German language pairs.</abstract>
      <url hash="2d91c90b">2022.wmt-1.58</url>
      <bibkey>huang-etal-2022-bjtu</bibkey>
    </paper>
    <paper id="59">
      <title>Papago’s Submission to the <fixed-case>WMT</fixed-case>22 Quality Estimation Shared Task</title>
      <author><first>Seunghyun</first><last>Lim</last><affiliation>Naver Papago</affiliation></author>
      <author><first>Jeonghyeok</first><last>Park</last><affiliation>Naver Corp.</affiliation></author>
      <pages>627-633</pages>
      <abstract>This paper describes anonymous submission to the WMT 2022 Quality Estimation shared task. We participate in Task 1: Quality Prediction for both sentence and word-level quality prediction tasks. Our system is a multilingual and multi-task model, whereby a single system can infer both sentence and word-level quality on multiple language pairs. Our system’s architecture consists of Pretrained Language Model (PLM) and task layers, and is jointly optimized for both sentence and word-level quality prediction tasks using multilingual dataset. We propose novel auxiliary tasks for training and explore diverse sources of additional data to demonstrate further improvements on performance. Through ablation study, we examine the effectiveness of proposed components and find optimal configurations to train our submission systems under each language pair and task settings. Finally, submission systems are trained and inferenced using K-folds ensemble. Our systems greatly outperform task organizer’s baseline and achieve comparable performance against other participants’ submissions in both sentence and word-level quality prediction tasks.</abstract>
      <url hash="3c75e29c">2022.wmt-1.59</url>
      <bibkey>lim-park-2022-papagos</bibkey>
    </paper>
    <paper id="60">
      <title><fixed-case>C</fixed-case>omet<fixed-case>K</fixed-case>iwi: <fixed-case>IST</fixed-case>-Unbabel 2022 Submission for the Quality Estimation Shared Task</title>
      <author><first>Ricardo</first><last>Rei</last><affiliation>Unbabel/INESC-ID</affiliation></author>
      <author><first>Marcos</first><last>Treviso</last><affiliation>Instituto de Telecomunicações</affiliation></author>
      <author><first>Nuno M.</first><last>Guerreiro</last><affiliation>Instituto de Telecomunicações</affiliation></author>
      <author><first>Chrysoula</first><last>Zerva</last><affiliation>Instituto de Telecomunicações, Instituto Superior Técnico, University of Lisbon</affiliation></author>
      <author><first>Ana C</first><last>Farinha</last><affiliation>Unbabel</affiliation></author>
      <author><first>Christine</first><last>Maroti</last><affiliation>Unbabel</affiliation></author>
      <author><first>José G.</first><last>C. de Souza</last><affiliation>Unbabel</affiliation></author>
      <author><first>Taisiya</first><last>Glushkova</last><affiliation>Instituto de Telecomunicações, Instituto Superior Técnico, University of Lisbon</affiliation></author>
      <author><first>Duarte</first><last>Alves</last><affiliation>Instituto Superior Técnico / Unbabel</affiliation></author>
      <author><first>Luisa</first><last>Coheur</last><affiliation>INESC-ID/Instituto Superior Técnico</affiliation></author>
      <author><first>Alon</first><last>Lavie</last><affiliation>Unbabel/Carnegie Mellon University</affiliation></author>
      <author><first>André F. T.</first><last>Martins</last><affiliation>Unbabel, Instituto de Telecomunicacoes</affiliation></author>
      <pages>634-645</pages>
      <abstract>We present the joint contribution of IST and Unbabel to the WMT 2022 Shared Task on Quality Estimation (QE). Our team participated in all three subtasks: (i) Sentence and Word-level Quality Prediction; (ii) Explainable QE; and (iii) Critical Error Detection. For all tasks we build on top of the COMET framework, connecting it with the predictor-estimator architecture of OpenKiwi, and equipping it with a word-level sequence tagger and an explanation extractor. Our results suggest that incorporating references during pretraining improves performance across several language pairs on downstream tasks, and that jointly training with sentence and word-level objectives yields a further boost. Furthermore, combining attention and gradient information proved to be the top strategy for extracting good explanations of sentence-level QE models. Overall, our submissions achieved the best results for all three tasks for almost all language pairs by a considerable margin.</abstract>
      <url hash="813cd8c4">2022.wmt-1.60</url>
      <bibkey>rei-etal-2022-cometkiwi</bibkey>
    </paper>
    <paper id="61">
      <title><fixed-case>C</fixed-case>ross<fixed-case>QE</fixed-case>: <fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> 2022 Submission for the Quality Estimation Shared Task</title>
      <author><first>Shimin</first><last>Tao</last><affiliation>Huawei</affiliation></author>
      <author><first>Su</first><last>Chang</last><affiliation>Huawei TSC</affiliation></author>
      <author><first>Ma</first><last>Miaomiao</last><affiliation>Huawei TSC</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <author><first>Xiang</first><last>Geng</last><affiliation>National Key Laboratory for Novel Software Technology, Nanjing University</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>National Key Laboratory for Novel Software Technology, Nanjing University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Huawei</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei TSC</affiliation></author>
      <author><first>Minghan</first><last>Wang</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Yinglu</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <pages>646-652</pages>
      <abstract>Quality estimation (QE) is a crucial method to investigate automatic methods for estimating the quality of machine translation results without reference translations. This paper presents Huawei Translation Services Center’s (HW-TSC’s) work called CrossQE in WMT 2022 QE shared tasks 1 and 2, namely sentence- and word- level quality prediction and explainable QE.CrossQE employes the framework of predictor-estimator for task 1, concretely with a pre-trained cross-lingual XLM-RoBERTa large as predictor and task-specific classifier or regressor as estimator. An extensive set of experimental results show that after adding bottleneck adapter layer, mean teacher loss, masked language modeling task loss and MC dropout methods in CrossQE, the performance has improved to a certain extent. For task 2, CrossQE calculated the cosine similarity between each word feature in the target and each word feature in the source by task 1 sentence-level QE system’s predictor, and used the inverse value of maximum similarity between each word in the target and the source as the word translation error risk value. Moreover, CrossQE has outstanding performance on QE test sets of WMT 2022.</abstract>
      <url hash="eb700267">2022.wmt-1.61</url>
      <bibkey>tao-etal-2022-crossqe</bibkey>
    </paper>
    <paper id="62">
      <title>Welocalize-<fixed-case>ARC</fixed-case>/<fixed-case>NKUA</fixed-case>’s Submission to the <fixed-case>WMT</fixed-case> 2022 Quality Estimation Shared Task</title>
      <author><first>Eirini</first><last>Zafeiridou</last><affiliation>Welocalize, National and Kapodistrian University of Athens</affiliation></author>
      <author><first>Sokratis</first><last>Sofianopoulos</last><affiliation>Researcher</affiliation></author>
      <pages>653-660</pages>
      <abstract>This paper presents our submission to the WMT 2022 quality estimation shared task and more specifically to the quality prediction sentence-level direct assessment (DA) subtask. We build a multilingual system based on the predictor–estimator architecture by using the XLM-RoBERTa transformer for feature extraction and a regression head on top of the final model to estimate the <tex-math>z</tex-math>-standardized DA labels. Furthermore, we use pretrained models to extract useful knowledge that reflect various criteria of quality assessment and demonstrate good correlation with human judgements. We optimize the performance of our model by incorporating this information as additional external features in the input data and by applying Monte Carlo dropout during both training and inference.</abstract>
      <url hash="6ed81c00">2022.wmt-1.62</url>
      <bibkey>zafeiridou-sofianopoulos-2022-welocalize</bibkey>
    </paper>
    <paper id="63">
      <title><fixed-case>E</fixed-case>dinburgh’s Submission to the <fixed-case>WMT</fixed-case> 2022 Efficiency Task</title>
      <author><first>Nikolay</first><last>Bogoychev</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Maximiliana</first><last>Behnke</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Jelmer</first><last>Van Der Linde</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Graeme</first><last>Nail</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Kenneth</first><last>Heafield</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Biao</first><last>Zhang</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Sidharth</first><last>Kashyap</last><affiliation>Intel Corporation</affiliation></author>
      <pages>661-667</pages>
      <abstract>We participated in all tracks of the WMT 2022 efficient machine translation task: single-core CPU, multi-core CPU, and GPU hardware with throughput and latency conditions. Our submissions explores a number of several efficiency strategies: knowledge distillation, a simpler simple recurrent unit (SSRU) decoder with one or two layers, shortlisting, deep encoder, shallow decoder, pruning and bidirectional decoder. For the CPU track, we used quantized 8-bit models. For the GPU track, we used FP16 quantisation. We explored various pruning strategies and combination of one or more of the above methods.</abstract>
      <url hash="c45f8caf">2022.wmt-1.63</url>
      <bibkey>bogoychev-etal-2022-edinburghs</bibkey>
    </paper>
    <paper id="64">
      <title><fixed-case>CUNI</fixed-case> Non-Autoregressive System for the <fixed-case>WMT</fixed-case> 22 Efficient Translation Shared Task</title>
      <author><first>Jindřich</first><last>Helcl</last><affiliation>Charles University in Prague</affiliation></author>
      <pages>668-670</pages>
      <abstract>We present a non-autoregressive system submission to the WMT 22 Efficient Translation Shared Task. Our system was used by Helcl et al. (2022) in an attempt to provide fair comparison between non-autoregressive and autoregressive models. This submission is an effort to establish solid baselines along with sound evaluation methodology, particularly in terms of measuring the decoding speed. The model itself is a 12-layer Transformer model trained with connectionist temporal classification on knowledge-distilled dataset by a strong autoregressive teacher model.</abstract>
      <url hash="bf091683">2022.wmt-1.64</url>
      <bibkey>helcl-2022-cuni</bibkey>
    </paper>
    <paper id="65">
      <title>The <fixed-case>R</fixed-case>oyal<fixed-case>F</fixed-case>lush System for the <fixed-case>WMT</fixed-case> 2022 Efficiency Task</title>
      <author><first>Bo</first><last>Qin</last><affiliation>Hithink RoyalFlush AI Research Institute, China</affiliation></author>
      <author><first>Aixin</first><last>Jia</last><affiliation>Hithink RoyalFlush AI Research Institute, China</affiliation></author>
      <author><first>Qiang</first><last>Wang</last><affiliation>Hithink RoyalFlush AI Research Institute, China</affiliation></author>
      <author><first>Jianning</first><last>Lu</last><affiliation>Hithink RoyalFlush AI Research Institute, China</affiliation></author>
      <author><first>Shuqin</first><last>Pan</last><affiliation>Hithink RoyalFlush AI Research Institute, China</affiliation></author>
      <author><first>Haibo</first><last>Wang</last><affiliation>Hithink RoyalFlush AI Research Institute, China</affiliation></author>
      <author><first>Ming</first><last>Chen</last><affiliation>Hithink RoyalFlush AI Research Institute, China</affiliation></author>
      <pages>671-676</pages>
      <abstract>This paper describes the submission of the RoyalFlush neural machine translation system for the WMT 2022 translation efficiency task. Unlike the commonly used autoregressive translation system, we adopted a two-stage translation paradigm called Hybrid Regression Translation (HRT) to combine the advantages of autoregressive and non-autoregressive translation. Specifically, HRT first autoregressively generates a discontinuous sequence (e.g., make a prediction every k tokens, k1) and then fills in all previously skipped tokens at once in a non-autoregressive manner. Thus, we can easily trade off the translation quality and speed by adjusting k. In addition, by integrating other modeling techniques (e.g., sequence-level knowledge distillation and deep-encoder-shallow-decoder layer allocation strategy) and a mass of engineering efforts, HRT improves 80% inference speed and achieves equivalent translation performance with the same-capacity AT counterpart. Our fastest system reaches 6k+ words/second on the GPU latency setting, estimated to be about 3.1x faster than the last year’s winner.</abstract>
      <url hash="44357abf">2022.wmt-1.65</url>
      <bibkey>qin-etal-2022-royalflush</bibkey>
    </paper>
    <paper id="66">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Submission for the <fixed-case>WMT</fixed-case>22 Efficiency Task</title>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ting</first><last>Hu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xianzhi</first><last>Yu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jianfei</first><last>Feng</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ting</first><last>Zhu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Lizhi</first><last>Lei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Shimin</first><last>Tao</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ying</first><last>Qin</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jinlong</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <pages>677-681</pages>
      <abstract>This paper presents the submission of Huawei Translation Services Center (HW-TSC) to WMT 2022 Efficiency Shared Task. For this year’s task, we still apply sentence-level distillation strategy to train small models with different configurations. Then, we integrate the average attention mechanism into the lightweight RNN model to pursue more efficient decoding. We tried adding a retrain step to our 8-bit and 4-bit models to achieve a balance between model size and quality. We still use Huawei Noah’s Bolt for INT8 inference and 4-bit storage. Coupled with Bolt’s support for batch inference and multi-core parallel computing, we finally submit models with different configurations to the CPU latency and throughput tracks to explore the Pareto frontiers.</abstract>
      <url hash="a2938a6d">2022.wmt-1.66</url>
      <bibkey>shang-etal-2022-hw</bibkey>
    </paper>
    <paper id="67">
      <title><fixed-case>IIT</fixed-case> <fixed-case>B</fixed-case>ombay’s <fixed-case>WMT</fixed-case>22 Automatic Post-Editing Shared Task Submission</title>
      <author><first>Sourabh</first><last>Deoghare</last><affiliation>IIT Bombay</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology Bombay and Patna</affiliation></author>
      <pages>682-688</pages>
      <url hash="86c1ea62">2022.wmt-1.67</url>
      <bibkey>deoghare-bhattacharyya-2022-iit</bibkey>
      <abstract>This paper describes IIT Bombay’s submission to the WMT22 Automatic Post-Editing (APE) shared task for the English-Marathi (En-Mr) language pair. We follow the curriculum training strategy to train our APE system. First, we train an encoder-decoder model to perform translation from English to Marathi. Next, we add another encoder to the model and train the resulting <i>dual-encoder single-decoder</i> model for the APE task. This involves training the model using the synthetic APE data in multiple training stages and then fine-tuning it using the real APE data. We use the LaBSE technique to ensure the quality of the synthetic APE data. For data augmentation, along with using candidates obtained from an external machine translation (MT) system, we also use the phrase-level APE triplets generated using phrase table injection. As APE systems are prone to the problem of ‘over-correction’, we use a sentence-level quality estimation (QE) system to select the final output between an original translation and the corresponding output generated by the APE model. Our approach improves the TER and BLEU scores on the development set by -3.92 and +4.36 points, respectively. Also, the final results on the test set show that our APE system outperforms the baseline system by -3.49 TER points and +5.37 BLEU points.</abstract>
    </paper>
    <paper id="68">
      <title><fixed-case>LUL</fixed-case>’s <fixed-case>WMT</fixed-case>22 Automatic Post-Editing Shared Task Submission</title>
      <author><first>Xiaoying</first><last>Huang</last><affiliation>Communication University of China</affiliation></author>
      <author><first>Xingrui</first><last>Lou</last><affiliation>Communication University of China</affiliation></author>
      <author><first>Fan</first><last>Zhang</last><affiliation>Communication University of China</affiliation></author>
      <author><first>Tu</first><last>Mei</last><affiliation>Samsung Research Institute China - Beijing</affiliation></author>
      <pages>689-693</pages>
      <abstract>By learning the human post-edits, the automatic post-editing (APE) models are often used to modify the output of the machine translation (MT) system to make it as close as possible to human translation. We introduce the system used in our submission of WMT’22 Automatic Post-Editing (APE) English-Marathi (En-Mr) shared task. In this task, we first train the MT system of En-Mr to generate additional machine-translation sentences. Then we use the additional triple to bulid our APE model and use APE dataset to further fine-tuning. Inspired by the mixture of experts (MoE), we use GMM algorithm to roughly divide the text of APE dataset into three categories. After that, the experts are added to the APE model and different domain data are sent to different experts. Finally, we ensemble the models to get better performance. Our APE system significantly improves the translations of provided MT results by -2.848 and +3.74 on the development dataset in terms of TER and BLEU, respectively. Finally, the TER and BLEU scores are improved by -1.22 and +2.41 respectively on the blind test set.</abstract>
      <url hash="d64acc34">2022.wmt-1.68</url>
      <bibkey>huang-etal-2022-luls</bibkey>
    </paper>
    <paper id="69">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2022 Biomedical Translation Shared Task: Monolingual Clinical Case Reports</title>
      <author><first>Mariana</first><last>Neves</last><affiliation>German Federal Institute for Risk Assessment</affiliation></author>
      <author><first>Antonio</first><last>Jimeno Yepes</last><affiliation>RMIT University, Australia</affiliation></author>
      <author><first>Amy</first><last>Siu</last><affiliation>Berliner Hochschule fuer Technik, German</affiliation></author>
      <author><first>Roland</first><last>Roller</last><affiliation>German Research Center for Artificial Intelligence (DFKI), Berlin, Germa</affiliation></author>
      <author><first>Philippe</first><last>Thomas</last><affiliation>German Research Center for Artificial Intelligence (DFKI), Berlin, Germa</affiliation></author>
      <author><first>Maika</first><last>Vicente Navarro</last><affiliation>Leica Biosystems, Australia</affiliation></author>
      <author><first>Lana</first><last>Yeganova</last><affiliation>NCBI/NLM/NIH, Bethesda, USA</affiliation></author>
      <author><first>Dina</first><last>Wiemann</last><affiliation>Novartis AG, Basel, Switzerland</affiliation></author>
      <author><first>Giorgio Maria</first><last>Di Nunzio</last><affiliation>University of Padua, Italy</affiliation></author>
      <author><first>Federica</first><last>Vezzani</last><affiliation>University of Padua, Italy</affiliation></author>
      <author><first>Christel</first><last>Gerardin</last><affiliation>Sorbonne Universit�, Franc</affiliation></author>
      <author><first>Rachel</first><last>Bawden</last><affiliation>Inria, Paris, France</affiliation></author>
      <author><first>Darryl Johan</first><last>Estrada</last><affiliation>Barcelona Supercomputing Center, Spain</affiliation></author>
      <author><first>Salvador</first><last>Lima-lopez</last><affiliation>Barcelona Supercomputing Center, Spain</affiliation></author>
      <author><first>Eulalia</first><last>Farre-maduel</last><affiliation>Barcelona Supercomputing Center, Spain</affiliation></author>
      <author><first>Martin</first><last>Krallinger</last><affiliation>Barcelona Supercomputing Center, Spai</affiliation></author>
      <author><first>Cristian</first><last>Grozea</last><affiliation>Fraunhofer Institute FOKUS, Berlin, Germany</affiliation></author>
      <author><first>Aurelie</first><last>Neveol</last><affiliation>Universit� Paris-Saclay, CNRS, LISN, Orsay, France</affiliation></author>
      <pages>694-723</pages>
      <abstract>In the seventh edition of the WMT Biomedical Task, we addressed a total of seven languagepairs, namely English/German, English/French, English/Spanish, English/Portuguese, English/Chinese, English/Russian, English/Italian. This year’s test sets covered three types of biomedical text genre. In addition to scientific abstracts and terminology items used in previous editions, we released test sets of clinical cases. The evaluation of clinical cases translations were given special attention by involving clinicians in the preparation of reference translations and manual evaluation. For the main MEDLINE test sets, we received a total of 609 submissions from 37 teams. For the ClinSpEn sub-task, we had the participation of five teams.</abstract>
      <url hash="c0d27a70">2022.wmt-1.69</url>
      <bibkey>neves-etal-2022-findings</bibkey>
    </paper>
    <paper id="70">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2022 Shared Task on Chat Translation</title>
      <author><first>Ana C</first><last>Farinha</last><affiliation>Unbabel</affiliation></author>
      <author><first>M. Amin</first><last>Farajian</last><affiliation>Unbabel</affiliation></author>
      <author><first>Marianna</first><last>Buchicchio</last><affiliation>Unbabel</affiliation></author>
      <author><first>Patrick</first><last>Fernandes</last><affiliation>Carnegie Mellon University, Instituto de Telecomunicações</affiliation></author>
      <author><first>José G.</first><last>C. de Souza</last><affiliation>Unbabel</affiliation></author>
      <author><first>Helena</first><last>Moniz</last><affiliation>INESC-ID, University of Lisbon</affiliation></author>
      <author><first>André F. T.</first><last>Martins</last><affiliation>Unbabel, Instituto de Telecomunicacoes</affiliation></author>
      <pages>724-743</pages>
      <abstract>This paper reports the findings of the second edition of the Chat Translation Shared Task. Similarly to the previous WMT 2020 edition, the task consisted of translating bilingual customer support conversational text. However, unlike the previous edition, in which the bilingual data was created from a synthetic monolingual English corpus, this year we used a portion of the newly released Unbabel’s MAIA corpus, which contains genuine bilingual conversations between agents and customers. We also expanded the language pairs to English↔German (en↔de), English↔French (en↔fr), and English↔Brazilian Portuguese (en↔pt-br).Given that the main goal of the shared task is to translate bilingual conversations, participants were encouraged to train and test their models specifically for this environment. In total, we received 18 submissions from 4 different teams. All teams participated in both directions of en↔de. One of the teams also participated in en↔fr and en↔pt-br. We evaluated the submissions with automatic metrics as well as human judgments via Multidimensional Quality Metrics (MQM) on both directions. The official ranking of the systems is based on the overall MQM scores of the participating systems on both directions, i.e. agent and customer.</abstract>
      <url hash="92b9d98c">2022.wmt-1.70</url>
      <bibkey>farinha-etal-2022-findings</bibkey>
    </paper>
    <paper id="71">
      <title>Findings of the First <fixed-case>WMT</fixed-case> Shared Task on Sign Language Translation (<fixed-case>WMT</fixed-case>-<fixed-case>SLT</fixed-case>22)</title>
      <author><first>Mathias</first><last>Müller</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Sarah</first><last>Ebling</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Eleftherios</first><last>Avramidis</last><affiliation>German Research Center for Artificial Intelligence (DFKI)</affiliation></author>
      <author><first>Alessia</first><last>Battisti</last><affiliation>University of Zurich, Switzerland</affiliation></author>
      <author><first>Michèle</first><last>Berger</last><affiliation>HfH</affiliation></author>
      <author><first>Richard</first><last>Bowden</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Annelies</first><last>Braffort</last><affiliation>LISN, CNRS, Université Paris-Saclay</affiliation></author>
      <author><first>Necati</first><last>Cihan Camgöz</last><affiliation>Meta</affiliation></author>
      <author><first>Cristina</first><last>España-bonet</last><affiliation>DFKI GmbH</affiliation></author>
      <author><first>Roman</first><last>Grundkiewicz</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Zifan</first><last>Jiang</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Oscar</first><last>Koller</last><affiliation>Microsoft</affiliation></author>
      <author><first>Amit</first><last>Moryossef</last><affiliation>Bar-Ilan university, University of Zurich, ETH Zurich</affiliation></author>
      <author><first>Regula</first><last>Perrollaz</last><affiliation>Hochschule fuer Heilpaedogik</affiliation></author>
      <author><first>Sabine</first><last>Reinhard</last><affiliation>Hochschule für Heilpädagogik HfH</affiliation></author>
      <author><first>Annette</first><last>Rios</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Dimitar</first><last>Shterionov</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Sandra</first><last>Sidler-miserez</last><affiliation>University of Teacher Education in Special Needs (HfH)</affiliation></author>
      <author><first>Katja</first><last>Tissi</last><affiliation>Hochschule fuer Heilpaedagogik</affiliation></author>
      <pages>744-772</pages>
      <abstract>This paper presents the results of the First WMT Shared Task on Sign Language Translation (WMT-SLT22).This shared task is concerned with automatic translation between signed and spoken languages. The task is novel in the sense that it requires processing visual information (such as video frames or human pose estimation) beyond the well-known paradigm of text-to-text machine translation (MT).The task featured two tracks, translating from Swiss German Sign Language (DSGS) to German and vice versa. Seven teams participated in this first edition of the task, all submitting to the DSGS-to-German track. Besides a system ranking and system papers describing state-of-the-art techniques, this shared task makes the following scientific contributions: novel corpora, reproducible baseline systems and new protocols and software for human evaluation. Finally, the task also resulted in the first publicly available set of system outputs and human evaluation scores for sign language translation.</abstract>
      <url hash="77c19451">2022.wmt-1.71</url>
      <bibkey>muller-etal-2022-findings</bibkey>
    </paper>
    <paper id="72">
      <title>Findings of the <fixed-case>WMT</fixed-case>’22 Shared Task on Large-Scale Machine Translation Evaluation for <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>David</first><last>Adelani</last><affiliation>University College London</affiliation></author>
      <author><first>Md Mahfuz Ibn</first><last>Alam</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>George Mason University</affiliation></author>
      <author><first>Akshita</first><last>Bhagia</last><affiliation>Ai2</affiliation></author>
      <author><first>Marta R.</first><last>Costa-jussà</last><affiliation>Meta AI</affiliation></author>
      <author><first>Jesse</first><last>Dodge</last><affiliation>Ai2</affiliation></author>
      <author><first>Fahim</first><last>Faisal</last><affiliation>George Mason University</affiliation></author>
      <author><first>Christian</first><last>Federmann</last><affiliation>Microsoft</affiliation></author>
      <author><first>Natalia</first><last>Fedorova</last><affiliation>Toloka</affiliation></author>
      <author><first>Francisco</first><last>Guzmán</last><affiliation>Meta AI</affiliation></author>
      <author><first>Sergey</first><last>Koshelev</last><affiliation>Toloka</affiliation></author>
      <author><first>Jean</first><last>Maillard</last><affiliation>Meta AI</affiliation></author>
      <author><first>Vukosi</first><last>Marivate</last><affiliation>Department of Computer Science, University of Pretoria</affiliation></author>
      <author><first>Jonathan</first><last>Mbuya</last><affiliation>George Mason University</affiliation></author>
      <author><first>Alexandre</first><last>Mourachko</last><affiliation>Meta AI</affiliation></author>
      <author><first>Safiyyah</first><last>Saleem</last><affiliation>Meta AI</affiliation></author>
      <author><first>Holger</first><last>Schwenk</last><affiliation>Meta AI Research</affiliation></author>
      <author><first>Guillaume</first><last>Wenzek</last><affiliation>Meta AI</affiliation></author>
      <pages>773-800</pages>
      <abstract>We present the results of the WMT’22 SharedTask on Large-Scale Machine Translation Evaluation for African Languages. The shared taskincluded both a data and a systems track, alongwith additional innovations, such as a focus onAfrican languages and extensive human evaluation of submitted systems. We received 14system submissions from 8 teams, as well as6 data track contributions. We report a largeprogress in the quality of translation for Africanlanguages since the last iteration of this sharedtask: there is an increase of about 7.5 BLEUpoints across 72 language pairs, and the average BLEU scores went from 15.09 to 22.60.</abstract>
      <url hash="5dcee569">2022.wmt-1.72</url>
      <bibkey>adelani-etal-2022-findings</bibkey>
    </paper>
    <paper id="73">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2022 Shared Tasks in Unsupervised <fixed-case>MT</fixed-case> and Very Low Resource Supervised <fixed-case>MT</fixed-case></title>
      <author><first>Marion</first><last>Weller-di Marco</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>801-805</pages>
      <abstract>We present the findings of the WMT2022Shared Tasks in Unsupervised MT and VeryLow Resource Supervised MT with experiments on the language pairs German to/fromUpper Sorbian, German to/from Lower Sorbian and Lower Sorbian to/from Upper Sorbian. Upper and Lower Sorbian are minoritylanguages spoken in the Eastern parts of Germany. There are active language communitiesworking on the preservation of the languageswho also made the data used in this Shared Taskavailable.In total, four teams participated on this SharedTask, with submissions from three teams for theunsupervised sub task, and submissions fromall four teams for the supervised sub task. Inthis overview paper, we present and discuss theresults.</abstract>
      <url hash="8ce23d9d">2022.wmt-1.73</url>
      <bibkey>weller-di-marco-fraser-2022-findings</bibkey>
    </paper>
    <paper id="74">
      <title>Overview and Results of <fixed-case>M</fixed-case>ix<fixed-case>MT</fixed-case> Shared-Task at <fixed-case>WMT</fixed-case> 2022</title>
      <author><first>Vivek</first><last>Srivastava</last><affiliation>TCS Research</affiliation></author>
      <author><first>Mayank</first><last>Singh</last><affiliation>IIT Gandhinagar</affiliation></author>
      <pages>806-811</pages>
      <abstract>In this paper, we present an overview of the WMT 2022 shared task on code-mixed machine translation (MixMT). In this shared task, we hosted two code-mixed machine translation subtasks in the following settings: (i) monolingual to code-mixed translation and (ii) code-mixed to monolingual translation. In both the subtasks, we received registration and participation from teams across the globe showing an interest and need to immediately address the challenges with machine translation involving code-mixed and low-resource languages.</abstract>
      <url hash="6f858631">2022.wmt-1.74</url>
      <bibkey>srivastava-singh-2022-overview</bibkey>
    </paper>
    <paper id="75">
      <title>Findings of the Word-Level <fixed-case>A</fixed-case>uto<fixed-case>C</fixed-case>ompletion Shared Task in <fixed-case>WMT</fixed-case> 2022</title>
      <author><first>Francisco</first><last>Casacuberta</last><affiliation>Universitat Politècnica de València</affiliation></author>
      <author><first>George</first><last>Foster</last><affiliation>Google</affiliation></author>
      <author><first>Guoping</first><last>Huang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Philipp</first><last>Koehn</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Geza</first><last>Kovacs</last><affiliation>Lilt</affiliation></author>
      <author><first>Lemao</first><last>Liu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>812-820</pages>
      <abstract>Recent years have witnessed rapid advancements in machine translation, but the state-of-the-art machine translation system still can not satisfy the high requirements in some rigorous translation scenarios. Computer-aided translation (CAT) provides a promising solution to yield a high-quality translation with a guarantee. Unfortunately, due to the lack of popular benchmarks, the research on CAT is not well developed compared with machine translation. In this year, we hold a new shared task called Word-level AutoCompletion (WLAC) for CAT in WMT. Specifically, we introduce some resources to train a WLAC model, and particularly we collect data from CAT systems as a part of test data for this shared task. In addition, we employ both automatic and human evaluations to measure the performance of the submitted systems, and our final evaluation results reveal some findings for the WLAC task.</abstract>
      <url hash="c59f8005">2022.wmt-1.75</url>
      <bibkey>casacuberta-etal-2022-findings</bibkey>
    </paper>
    <paper id="76">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2022 Shared Task on Translation Suggestion</title>
      <author><first>Zhen</first><last>Yang</last><affiliation>Tencent.com</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Yingxue</first><last>Zhang</last><affiliation>Pattern Recognition Center, WeChat AI, Tencent</affiliation></author>
      <author><first>Ernan</first><last>Li</last><affiliation>Pattern Recognition Center, WeChat AI, Tencent</affiliation></author>
      <author><first>Jie</first><last>Zhou</last><affiliation>Tencent Inc.</affiliation></author>
      <pages>821-829</pages>
      <abstract>We report the result of the first edition of the WMT shared task on Translation Suggestion (TS). The task aims to provide alternatives for specific words or phrases given the entire documents generated by machine translation (MT). It consists two sub-tasks, namely, the naive translation suggestion and translation suggestion with hints. The main difference is that some hints are provided in sub-task two, therefore, it is easier for the model to generate more accurate suggestions. For sub-task one, we provide the corpus for the language pairs English-German and English-Chinese. And only English-Chinese corpus is provided for the sub-task two. We received 92 submissions from 5 participating teams in sub-task one and 6 submissions for the sub-task 2, most of them covering all of the translation directions. We used the automatic metric BLEU for evaluating the performance of each submission.</abstract>
      <url hash="ebaecc00">2022.wmt-1.76</url>
      <bibkey>yang-etal-2022-findings</bibkey>
    </paper>
    <paper id="77">
      <title>Focused Concatenation for Context-Aware Neural Machine Translation</title>
      <author><first>Lorenzo</first><last>Lupo</last><affiliation>Lig</affiliation></author>
      <author><first>Marco</first><last>Dinarelli</last><affiliation>Lig</affiliation></author>
      <author><first>Laurent</first><last>Besacier</last><affiliation>Naver Labs Europe</affiliation></author>
      <pages>830-842</pages>
      <abstract>A straightforward approach to context-aware neural machine translation consists in feeding the standard encoder-decoder architecture with a window of consecutive sentences, formed by the current sentence and a number of sentences from its context concatenated to it. In this work, we propose an improved concatenation approach that encourages the model to focus on the translation of the current sentence, discounting the loss generated by target context. We also propose an additional improvement that strengthen the notion of sentence boundaries and of relative sentence distance, facilitating model compliance to the context-discounted objective. We evaluate our approach with both average-translation quality metrics and contrastive test sets for the translation of inter-sentential discourse phenomena, proving its superiority to the vanilla concatenation approach and other sophisticated context-aware systems.</abstract>
      <url hash="31941209">2022.wmt-1.77</url>
      <bibkey>lupo-etal-2022-focused</bibkey>
    </paper>
    <paper id="78">
      <title>Does Sentence Segmentation Matter for Machine Translation?</title>
      <author><first>Rachel</first><last>Wicks</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Matt</first><last>Post</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>843-854</pages>
      <abstract>For the most part, NLP applications operate at the sentence level. Since sentences occur most naturally in documents, they must be extracted and segmented via the use of a segmenter, of which there are a handful of options. There has been some work evaluating the performance of segmenters on intrinsic metrics, that look at their ability to recover human-segmented sentence boundaries, but there has been no work looking at the effect of segmenters on downstream tasks. We ask the question, “does segmentation matter?” and attempt to answer it on the task of machine translation. We consider two settings: the application of segmenters to a black-box system whose training segmentation is mostly unknown, as well as the variation in performance when segmenters are applied to the training process, too. We find that the choice of segmenter largely does not matter, so long as its behavior is not one of extreme under- or over-segmentation. For such settings, we provide some qualitative analysis examining their harms, and point the way towards document-level processing.</abstract>
      <url hash="547458f1">2022.wmt-1.78</url>
      <bibkey>wicks-post-2022-sentence</bibkey>
    </paper>
    <paper id="79">
      <title>Revisiting Locality Sensitive Hashing for Vocabulary Selection in Fast Neural Machine Translation</title>
      <author><first>Hieu</first><last>Hoang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Marcin</first><last>Junczys-dowmunt</last><affiliation>Microsoft</affiliation></author>
      <author><first>Roman</first><last>Grundkiewicz</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Huda</first><last>Khayrallah</last><affiliation>Microsoft</affiliation></author>
      <pages>855-869</pages>
      <abstract>Neural machine translation models often contain large target vocabularies. The calculation of logits, softmax and beam search is computationally costly over so many classes. We investigate the use of locality sensitive hashing (LSH) to reduce the number of vocabulary items that must be evaluated and explore the relationship between the hashing algorithm, translation speed and quality. Compared to prior work, our LSH-based solution does not require additional augmentation via word-frequency lists or alignments. We propose a training procedure that produces models, which, when combined with our LSH inference algorithm increase translation speed by up to 87% over the baseline, while maintaining translation quality as measured by BLEU. Apart from just using BLEU, we focus on minimizing search errors compared to the full softmax, a much harsher quality criterion.</abstract>
      <url hash="b3e3b0d9">2022.wmt-1.79</url>
      <bibkey>hoang-etal-2022-revisiting</bibkey>
    </paper>
    <paper id="80">
      <title>Too Brittle to Touch: Comparing the Stability of Quantization and Distillation towards Developing Low-Resource <fixed-case>MT</fixed-case> Models</title>
      <author><first>Harshita</first><last>Diddee</last><affiliation>Microsoft Research India</affiliation></author>
      <author><first>Sandipan</first><last>Dandapat</last><affiliation>Microsoft</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Tanuja</first><last>Ganu</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Kalika</first><last>Bali</last><affiliation>Microsoft Research Labs</affiliation></author>
      <pages>870-885</pages>
      <abstract>Leveraging shared learning through Massively Multilingual Models, state-of-the-art Machine translation (MT) models are often able to adapt to the paucity of data for low-resource languages. However, this performance comes at the cost of significantly bloated models which aren’t practically deployable. Knowledge Distillation is one popular technique to develop competitive lightweight models: In this work, we first evaluate its use in compressing MT models, focusing specifically on languages with extremely limited training data. Through our analysis across 8 languages, we find that the variance in the performance of the distilled models due to their dependence on priors including the amount of synthetic data used for distillation, the student architecture, training hyper-parameters and confidence of the teacher models, makes distillation a brittle compression mechanism. To mitigate this, we further explore the use of post-training quantization for the compression of these models. Here, we find that while Distillation provides gains across some low-resource languages, Quantization provides more consistent performance trends for the entire range of languages, especially the lowest-resource languages in our target set.</abstract>
      <url hash="72f4acc1">2022.wmt-1.80</url>
      <bibkey>diddee-etal-2022-brittle</bibkey>
    </paper>
    <paper id="81">
      <title>Data Augmentation for Inline Tag-Aware Neural Machine Translation</title>
      <author><first>Yonghyun</first><last>Ryu</last><affiliation>Samsung Electronics</affiliation></author>
      <author><first>Yoonjung</first><last>Choi</last><affiliation>Samsung Research</affiliation></author>
      <author><first>Sangha</first><last>Kim</last><affiliation>Samsung Research</affiliation></author>
      <pages>886-894</pages>
      <abstract>Despite the wide use of inline formatting, not much has been studied on translating sentences with inline formatted tags. The detag-and-project approach using word alignments is one solution to translating a tagged sentence. However, the method has a limitation: tag reinsertion is not considered in the translation process. Another solution is to use an end-to-end model which takes text with inline tags as inputs and translates them into a tagged sentence. This approach can alleviate the problems of the aforementioned method, but there is no sufficient parallel corpus dedicated to such a task. To solve this problem, an automatic data augmentation method by tag injection is suggested, but it is computationally expensive and augmentation is limited since the model is based on isolated translation for all fragments. In this paper, we propose an efficient and effective tag augmentation method based on word alignment. Our experiments show that our approach outperforms the detag-and-project methods. We also introduce a metric to evaluate the placement of tags and show that the suggested metric is reasonable for our task. We further analyze the effectiveness of each implementation detail.</abstract>
      <url hash="cc46e475">2022.wmt-1.81</url>
      <bibkey>ryu-etal-2022-data</bibkey>
    </paper>
    <paper id="82">
      <title>The <fixed-case>SPECTRANS</fixed-case> System Description for the <fixed-case>WMT</fixed-case>22 Biomedical Task</title>
      <author><first>Nicolas</first><last>Ballier</last><affiliation>Université de Paris</affiliation></author>
      <author><first>Jean-baptiste</first><last>Yunès</last><affiliation>Université Paris Cité</affiliation></author>
      <author><first>Guillaume</first><last>Wisniewski</last><affiliation>Université Paris Cité and LLF</affiliation></author>
      <author><first>Lichao</first><last>Zhu</last><affiliation>Université Paris Cité</affiliation></author>
      <author><first>Maria</first><last>Zimina</last><affiliation>Université Paris Cité</affiliation></author>
      <pages>895-900</pages>
      <abstract>This paper describes the SPECTRANS submission for the WMT 2022 biomedical shared task. We present the results of our experiments using the training corpora and the JoeyNMT (Kreutzer et al., 2019) and SYSTRAN Pure Neural Server/ Advanced Model Studio toolkits for the language directions English to French and French to English. We compare the pre- dictions of the different toolkits. We also use JoeyNMT to fine-tune the model with a selection of texts from WMT, Khresmoi and UFAL data sets. We report our results and assess the respective merits of the different translated texts.</abstract>
      <url hash="2e61423c">2022.wmt-1.82</url>
      <bibkey>ballier-etal-2022-spectrans</bibkey>
    </paper>
    <paper id="83">
      <title><fixed-case>SRT</fixed-case>’s Neural Machine Translation System for <fixed-case>WMT</fixed-case>22 Biomedical Translation Task</title>
      <author><first>Yoonjung</first><last>Choi</last><affiliation>Samsung Research</affiliation></author>
      <author><first>Jiho</first><last>Shin</last><affiliation>Samsung Electronics</affiliation></author>
      <author><first>Yonghyun</first><last>Ryu</last><affiliation>Samsung Electronics</affiliation></author>
      <author><first>Sangha</first><last>Kim</last><affiliation>Samsung Research</affiliation></author>
      <pages>901-907</pages>
      <abstract>This paper describes the Samsung Research’s Translation system (SRT) submitted to the WMT22 biomedical translation task in two language directions: English to Spanish and Spanish to English. To improve the overall quality, we adopt the deep transformer architecture and employ the back-translation strategy for monolingual corpus. One of the issues in the domain translation is to translate domain-specific terminologies well. To address this issue, we apply the soft-constrained terminology translation based on biomedical terminology dictionaries. In this paper, we provide the performance of our system with WMT20 and WMT21 biomedical testsets. Compared to the best model in WMT20 and WMT21, our system shows equal or better performance. According to the official evaluation results in terms of BLEU scores, our systems get the highest scores in both directions.</abstract>
      <url hash="43bb5707">2022.wmt-1.83</url>
      <bibkey>choi-etal-2022-srts</bibkey>
    </paper>
    <paper id="84">
      <title>Examining Large Pre-Trained Language Models for Machine Translation: What You Don’t Know about It</title>
      <author><first>Lifeng</first><last>Han</last><affiliation>The University of Manchester</affiliation></author>
      <author><first>Gleb</first><last>Erofeev</last><affiliation>Logrus Global</affiliation></author>
      <author><first>Irina</first><last>Sorokina</last><affiliation>Logrus Global</affiliation></author>
      <author><first>Serge</first><last>Gladkoff</last><affiliation>Logrus Global</affiliation></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <pages>908-919</pages>
      <abstract>Pre-trained language models (PLMs) often take advantage of the monolingual and multilingual dataset that is freely available online to acquire general or mixed domain knowledge before deployment into specific tasks. Extra-large PLMs (xLPLMs) are proposed very recently to claim supreme performances over smaller-sized PLMs such as in machine translation (MT) tasks. These xLPLMs include Meta-AI’s wmt21-dense-24-wide-en-X (2021) and NLLB (2022). In this work, we examine if xLPLMs are absolutely superior to smaller-sized PLMs in fine-tuning toward domain-specific MTs. We use two different in-domain data of different sizes: commercial automotive in-house data and clinical shared task data from the ClinSpEn2022 challenge at WMT2022. We choose the popular Marian Helsinki as smaller sized PLM and two massive-sized Mega-Transformers from Meta-AI as xLPLMs.Our experimental investigation shows that 1) on smaller-sized in-domain commercial automotive data, xLPLM wmt21-dense-24-wide-en-X indeed shows much better evaluation scores using SacreBLEU and hLEPOR metrics than smaller-sized Marian, even though its score increase rate is lower than Marian after fine-tuning; 2) on relatively larger-size well prepared clinical data fine-tuning, the xLPLM NLLB tends to lose its advantage over smaller-sized Marian on two sub-tasks (clinical terms and ontology concepts) using ClinSpEn offered metrics METEOR, COMET, and ROUGE-L, and totally lost to Marian on Task-1 (clinical cases) on all official metrics including SacreBLEU and BLEU; 3) metrics do not always agree with each other on the same tasks using the same model outputs; 4) clinic-Marian ranked No.2 on Task- 1 (via SacreBLEU/BLEU) and Task-3 (via METEOR and ROUGE) among all submissions.</abstract>
      <url hash="c951d3dd">2022.wmt-1.84</url>
      <bibkey>han-etal-2022-examining</bibkey>
      <video href="2022.wmt-1.84.mp4"/>
    </paper>
    <paper id="85">
      <title>Summer: <fixed-case>W</fixed-case>e<fixed-case>C</fixed-case>hat Neural Machine Translation Systems for the <fixed-case>WMT</fixed-case>22 Biomedical Translation Task</title>
      <author><first>Ernan</first><last>Li</last><affiliation>Pattern Recognition Center, WeChat AI, Tencent</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Jie</first><last>Zhou</last><affiliation>Tencent Inc.</affiliation></author>
      <pages>920-924</pages>
      <abstract>This paper introduces WeChat’s participation in WMT 2022 shared biomedical translationtask on Chinese→English. Our systems are based on the Transformer(Vaswani et al., 2017),and use several different Transformer structures to improve the quality of translation. In our experiments, we employ data filtering, data generation, several variants of Transformer,fine-tuning and model ensemble. Our Chinese→English system, named Summer, achieves the highest BLEU score among all submissions.</abstract>
      <url hash="13929439">2022.wmt-1.85</url>
      <bibkey>li-etal-2022-summer</bibkey>
    </paper>
    <paper id="86">
      <title>Optum’s Submission to <fixed-case>WMT</fixed-case>22 Biomedical Translation Tasks</title>
      <author><first>Sahil</first><last>Manchanda</last><affiliation>Optum</affiliation></author>
      <author><first>Saurabh</first><last>Bhagwat</last><affiliation>Optum</affiliation></author>
      <pages>925-929</pages>
      <abstract>This paper describes Optum’s submission to the Biomedical Translation task of the seventh conference on Machine Translation (WMT22). The task aims at promoting the development and evaluation of machine translation systems in their ability to handle challenging domain-specific biomedical data. We made submissions to two sub-tracks of ClinSpEn 2022, namely, ClinSpEn-CC (clinical cases) and ClinSpEn-OC (ontology concepts). These sub-tasks aim to test translation from English to Spanish. Our approach involves fine-tuning a pre-trained transformer model using in-house clinical domain data and the biomedical data provided by WMT. The fine-tuned model results in a test BLEU score of 38.12 in the ClinSpEn-CC (clinical cases) subtask, which is a gain of 1.23 BLEU compared to the pre-trained model.</abstract>
      <url hash="a2646b8f">2022.wmt-1.86</url>
      <bibkey>manchanda-bhagwat-2022-optums</bibkey>
    </paper>
    <paper id="87">
      <title>Huawei <fixed-case>B</fixed-case>abel<fixed-case>T</fixed-case>ar <fixed-case>NMT</fixed-case> at <fixed-case>WMT</fixed-case>22 Biomedical Translation Task: How We Further Improve Domain-specific <fixed-case>NMT</fixed-case></title>
      <author><first>Weixuan</first><last>Wang</last><affiliation>Artificial Intelligence Application Research Center, Huawei Technologies</affiliation></author>
      <author><first>Xupeng</first><last>Meng</last><affiliation>Artificial Intelligence Application Research Center, Huawei Technologies</affiliation></author>
      <author><first>Suqing</first><last>Yan</last><affiliation>Artificial Intelligence Application Research Center, Huawei Technologies</affiliation></author>
      <author><first>Ye</first><last>Tian</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Wei</first><last>Peng</last><affiliation>Artificial Intelligence Application Research Center, Huawei Technologies</affiliation></author>
      <pages>930-935</pages>
      <abstract>This paper describes Huawei Artificial Intelligence Application Research Center’s neural machine translation system (“BabelTar”). Our submission to the WMT22 biomedical translation shared task covers language directions between English and the other seven languages (French, German, Italian, Spanish, Portuguese, Russian, and Chinese). During the past four years, our participation in this domain-specific track has witnessed a paradigm shift of methodology from a purely data-driven focus to embracing diversified techniques, including pre-trained multilingual NMT models, homograph disambiguation, ensemble learning, and preprocessing methods. We illustrate practical insights and measured performance improvements relating to how we further improve our domain-specific NMT system.</abstract>
      <url hash="ed15493f">2022.wmt-1.87</url>
      <bibkey>wang-etal-2022-huawei</bibkey>
    </paper>
    <paper id="88">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> Translation Systems for the <fixed-case>WMT</fixed-case>22 Biomedical Translation Task</title>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jinlong</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ming</first><last>Zhu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuanchang</first><last>Luo</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuhao</first><last>Xie</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Miaomiao</first><last>Ma</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ting</first><last>Zhu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Lizhi</first><last>Lei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Song</first><last>Peng</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ying</first><last>Qin</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <pages>936-942</pages>
      <abstract>This paper describes the translation systems trained by Huawei translation services center (HW-TSC) for the WMT22 biomedical translation task in five language pairs: English↔German (en↔de), English↔French (en↔fr), English↔Chinese (en↔zh), English↔Russian (en↔ru) and Spanish→English (es→en). Our primary systems are built on deep Transformer with a large filter size. We also utilize R-Drop, data diversification, forward translation, back translation, data selection, finetuning and ensemble to improve the system performance. According to the official evaluation results in OCELoT or CodaLab, our unconstrained systems in en→de, de→en, en→fr, fr→en, en→zh and es→en (clinical terminology sub-track) get the highest BLEU scores among all submissions for the WMT22 biomedical translation task.</abstract>
      <url hash="5abf1b35">2022.wmt-1.88</url>
      <bibkey>wu-etal-2022-hw</bibkey>
    </paper>
    <paper id="89">
      <title>Unbabel-<fixed-case>IST</fixed-case> at the <fixed-case>WMT</fixed-case> Chat Translation Shared Task</title>
      <author><first>João</first><last>Alves</last><affiliation>Unbabel</affiliation></author>
      <author><first>Pedro Henrique</first><last>Martins</last><affiliation>Instituto de Telecomunicações, Instituto Superior Técnico</affiliation></author>
      <author><first>José G.</first><last>C. de Souza</last><affiliation>Unbabel</affiliation></author>
      <author><first>M. Amin</first><last>Farajian</last><affiliation>Unbabel</affiliation></author>
      <author><first>André F. T.</first><last>Martins</last><affiliation>Unbabel, Instituto de Telecomunicacoes</affiliation></author>
      <pages>943-948</pages>
      <abstract>We present the joint contribution of IST and Unbabel to the WMT 2022 Chat Translation Shared Task. We participated in all six language directions (English ↔ German, English ↔ French, English ↔ Brazilian Portuguese). Due to the lack of domain-specific data, we use mBART50, a large pretrained language model trained on millions of sentence-pairs, as our base model. We fine-tune it using a two step fine-tuning process. In the first step, we fine-tune the model on publicly available data. In the second step, we use the validation set. After having a domain specific model, we explore the use of kNN-MT as a way of incorporating domain-specific data at decoding time.</abstract>
      <url hash="85381386">2022.wmt-1.89</url>
      <bibkey>alves-etal-2022-unbabel</bibkey>
    </paper>
    <paper id="90">
      <title>Investigating Effectiveness of Multi-Encoder for Conversational Neural Machine Translation</title>
      <author><first>Baban</first><last>Gain</last><affiliation>Indian Institute of Technology, Patna</affiliation></author>
      <author><first>Ramakrishna</first><last>Appicharla</last><affiliation>Indian Institute of Technology Patna</affiliation></author>
      <author><first>Soumya</first><last>Chennabasavaraj</last><affiliation>Flipkart</affiliation></author>
      <author><first>Nikesh</first><last>Garera</last><affiliation>Flipkart</affiliation></author>
      <author><first>Asif</first><last>Ekbal</last><affiliation>IIT Patna</affiliation></author>
      <author><first>Muthusamy</first><last>Chelliah</last><affiliation>Flipkart</affiliation></author>
      <pages>949-954</pages>
      <abstract>Multilingual chatbots are the need of the hour for modern business. There is increasing demand for such systems all over the world. A multilingual chatbot can help to connect distant parts of the world together, without sharing a common language. We participated in WMT22 Chat Translation Shared Task. In this paper, we report descriptions of methodologies used for participation. We submit outputs from multi-encoder based transformer model, where one encoder is for context and another for source utterance. We consider one previous utterance as context. We obtain COMET scores of 0.768 and 0.907 on English-to-German and German-to-English directions, respectively. We submitted outputs without using context at all, which generated worse results in English-to-German direction. While for German-to-English, the model achieved a lower COMET score but slightly higher chrF and BLEU scores. Further, to understand the effectiveness of the context encoder, we submitted a run after removing the context encoder during testing and we obtain similar results.</abstract>
      <url hash="ad059569">2022.wmt-1.90</url>
      <bibkey>gain-etal-2022-investigating</bibkey>
      <revision id="1" href="2022.wmt-1.90v1" hash="1fcf991c"/>
      <revision id="2" href="2022.wmt-1.90v2" hash="ad059569" date="2023-03-13">Corrected author name on the pdf.</revision>
    </paper>
    <paper id="91">
      <title><fixed-case>BJTU</fixed-case>-<fixed-case>W</fixed-case>e<fixed-case>C</fixed-case>hat’s Systems for the <fixed-case>WMT</fixed-case>22 Chat Translation Task</title>
      <author><first>Yunlong</first><last>Liang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yufeng</first><last>Chen</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jie</first><last>Zhou</last><affiliation>Tencent Inc.</affiliation></author>
      <pages>955-961</pages>
      <abstract>This paper introduces the joint submission of the Beijing Jiaotong University and WeChat AI to the WMT’22 chat translation task for English-German. Based on the Transformer, we apply several effective variants. In our experiments, we apply the pre-training-then-fine-tuning paradigm. In the first pre-training stage, we employ data filtering and synthetic data generation (i.e., back-translation, forward-translation, and knowledge distillation). In the second fine-tuning stage, we investigate speaker-aware in-domain data generation, speaker adaptation, prompt-based context modeling, target denoising fine-tuning, and boosted self-COMET-based model ensemble. Our systems achieve 81.0 and 94.6 COMET scores on English-German and German-English, respectively. The COMET scores of English-German and German-English are the highest among all submissions.</abstract>
      <url hash="6066c4f5">2022.wmt-1.91</url>
      <bibkey>liang-etal-2022-bjtu</bibkey>
    </paper>
    <paper id="92">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> Translation Systems for the <fixed-case>WMT</fixed-case>22 Chat Translation Task</title>
      <author><first>Jinlong</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuhao</first><last>Xie</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuanchang</first><last>Luo</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ting</first><last>Zhu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yanqing</first><last>Zhao</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Lizhi</first><last>Lei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ying</first><last>Qin</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <pages>962-968</pages>
      <abstract>This paper describes the submissions of Huawei Translation Services Center (HW-TSC) to WMT22 chat translation shared task on English-Germany (en-de) bidirection with results of zore-shot and few-shot tracks. We use the deep transformer architecture with a lager parameter size. Our submissions to the WMT21 News Translation task are used as the baselines. We adopt strategies such as back translation, forward translation, domain transfer, data selection, and noisy forward translation in task, and achieve competitive results on the development set. We also test the effectiveness of document translation on chat tasks. Due to the lack of chat data, the results on the development set show that it is not as effective as sentence-level translation models.</abstract>
      <url hash="7afa70be">2022.wmt-1.92</url>
      <bibkey>yang-etal-2022-hw</bibkey>
    </paper>
    <paper id="93">
      <title>Clean Text and Full-Body Transformer: <fixed-case>M</fixed-case>icrosoft’s Submission to the <fixed-case>WMT</fixed-case>22 Shared Task on Sign Language Translation</title>
      <author><first>Subhadeep</first><last>Dey</last><affiliation>Microsoft</affiliation></author>
      <author><first>Abhilash</first><last>Pal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Cyrine</first><last>Chaabani</last><affiliation>Microsoft</affiliation></author>
      <author><first>Oscar</first><last>Koller</last><affiliation>Microsoft</affiliation></author>
      <pages>969-976</pages>
      <abstract>This paper describes Microsoft’s submission to the first shared task on sign language translation at WMT 2022, a public competition tackling sign language to spoken language translation for Swiss German sign language. The task is very challenging due to data scarcity and an unprecedented vocabulary size of more than 20k words on the target side. Moreover, the data is taken from real broadcast news, includes native signing and covers scenarios of long videos. Motivated by recent advances in action recognition, we incorporate full body information by extracting features from a pre-trained I3D model and applying a standard transformer network. The accuracy of the system is furtherimproved by applying careful data cleaning on the target text. We obtain BLEU scores of 0.6 and 0.78 on the test and dev set respectively, which is the best score among the participants of the shared task. Also in the human evaluation the submission reaches the first place. The BLEU score is further improved to 1.08 on the dev set by applying features extracted from a lip reading model.</abstract>
      <url hash="480a8865">2022.wmt-1.93</url>
      <bibkey>dey-etal-2022-clean</bibkey>
    </paper>
    <paper id="94">
      <title>Spatio-temporal Sign Language Representation and Translation</title>
      <author><first>Yasser</first><last>Hamidullah</last><affiliation>Dfki</affiliation></author>
      <author><first>Josef</first><last>Van Genabith</last><affiliation>Dfki</affiliation></author>
      <author><first>Cristina</first><last>España-bonet</last><affiliation>DFKI GmbH</affiliation></author>
      <pages>977-982</pages>
      <abstract>This paper describes the DFKI-MLT submission to the WMT-SLT 2022 sign language translation (SLT) task from Swiss German Sign Language (video) into German (text). State-of-the-art techniques for SLT use a generic seq2seq architecture with customized input embeddings. Instead of word embeddings as used in textual machine translation, SLT systems use features extracted from video frames. Standard approaches often do not benefit from temporal features. In our participation, we present a system that learns spatio-temporal feature representations and translation in a single model, resulting in a real end-to-end architecture expected to better generalize to new data sets. Our best system achieved <tex-math>5 \pm 1</tex-math> BLEU points on the development set, but the performance on the test dropped to <tex-math>0.11 \pm 0.06</tex-math> BLEU points.</abstract>
      <url hash="34491d08">2022.wmt-1.94</url>
      <bibkey>hamidullah-etal-2022-spatio</bibkey>
    </paper>
    <paper id="95">
      <title>Experimental Machine Translation of the <fixed-case>S</fixed-case>wiss <fixed-case>G</fixed-case>erman Sign Language via 3<fixed-case>D</fixed-case> Augmentation of Body Keypoints</title>
      <author><first>Lorenz</first><last>Hufe</last><affiliation>Dfki</affiliation></author>
      <author><first>Eleftherios</first><last>Avramidis</last><affiliation>German Research Center for Artificial Intelligence (DFKI)</affiliation></author>
      <pages>983-988</pages>
      <abstract>This paper describes the participation of DFKI-SLT at the Sign Language Translation Task of the Seventh Conference of Machine Translation (WMT22). The system focuses on the translation direction from the Swiss German Sign Language (DSGS) to written German. The original videos of the sign language were analyzed with computer vision models to provide 3D body keypoints. A deep-learning sequence-to-sequence model is trained on a parallel corpus of these body keypoints aligned to written German sentences. Geometric data augmentation occurs during the training process. The body keypoints are augmented by artificial rotation in the three dimensional space. The 3D-transformation is calculated with different angles on every batch of the training process.</abstract>
      <url hash="672656fc">2022.wmt-1.95</url>
      <bibkey>hufe-avramidis-2022-experimental</bibkey>
    </paper>
    <paper id="96">
      <title><fixed-case>TTIC</fixed-case>’s <fixed-case>WMT</fixed-case>-<fixed-case>SLT</fixed-case> 22 Sign Language Translation System</title>
      <author><first>Bowen</first><last>Shi</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Diane</first><last>Brentari</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Gregory</first><last>Shakhnarovich</last><affiliation>TTI-Chicago</affiliation></author>
      <author><first>Karen</first><last>Livescu</last><affiliation>TTI-Chicago</affiliation></author>
      <pages>989-993</pages>
      <abstract>We describe TTIC’s model submission to WMT-SLT 2022 task on sign language translation (Swiss-German Sign Language (DSGS) - German). Our model consists of an I3D backbone for image encoding and a Transformerbased encoder-decoder model for sequence modeling. The I3D is pre-trained with isolated sign recognition using the WLASL dataset. The model is based on RGB images alone and does not rely on the pre-extracted human pose. We explore a few different strategies for model training in this paper. Our system achieves 0.3 BLEU score and 0.195 Chrf score on the official test set.</abstract>
      <url hash="50c1d94d">2022.wmt-1.96</url>
      <bibkey>shi-etal-2022-ttics</bibkey>
    </paper>
    <paper id="97">
      <title>Tackling Low-Resourced Sign Language Translation: <fixed-case>UPC</fixed-case> at <fixed-case>WMT</fixed-case>-<fixed-case>SLT</fixed-case> 22</title>
      <author><first>Laia</first><last>Tarres</last><affiliation>Upc</affiliation></author>
      <author><first>Gerard I.</first><last>Gállego</last><affiliation>Universitat Politècnica de Catalunya</affiliation></author>
      <author><first>Xavier</first><last>Giro-i-nieto</last><affiliation>Universitat Politecnica de Catalunya</affiliation></author>
      <author><first>Jordi</first><last>Torres</last><affiliation>UPC Barcelona Tech and BSC</affiliation></author>
      <pages>994-1000</pages>
      <abstract>This paper describes the system developed at the Universitat Politècnica de Catalunya for the Workshop on Machine Translation 2022 Sign Language Translation Task, in particular, for the sign-to-text direction. We use a Transformer model implemented with the Fairseq modeling toolkit. We have experimented with the vocabulary size, data augmentation techniques and pretraining the model with the PHOENIX-14T dataset. Our system obtains 0.50 BLEU score for the test set, improving the organizers’ baseline by 0.38 BLEU. We remark the poor results for both the baseline and our system, and thus, the unreliability of our findings.</abstract>
      <url hash="f4850018">2022.wmt-1.97</url>
      <bibkey>tarres-etal-2022-tackling</bibkey>
    </paper>
    <paper id="98">
      <title>Separating Grains from the Chaff: Using Data Filtering to Improve Multilingual Translation for Low-Resourced <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Idris</first><last>Abdulmumin</last><affiliation>Ahmadu Bello University, Zaria</affiliation></author>
      <author><first>Michael</first><last>Beukman</last><affiliation>University of the Witwatersrand</affiliation></author>
      <author><first>Jesujoba</first><last>Alabi</last><affiliation>Saarland University</affiliation></author>
      <author><first>Chris Chinenye</first><last>Emezue</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Everlyn</first><last>Chimoto</last><affiliation>University of Cape Town</affiliation></author>
      <author><first>Tosin</first><last>Adewumi</last><affiliation>Luleå University of Technology</affiliation></author>
      <author><first>Shamsuddeen</first><last>Muhammad</last><affiliation>Bayero University, Kano</affiliation></author>
      <author><first>Mofetoluwa</first><last>Adeyemi</last><affiliation>Defence Space Administration, Abuja.</affiliation></author>
      <author><first>Oreen</first><last>Yousuf</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Sahib</first><last>Singh</last><affiliation>Ford Motor Company</affiliation></author>
      <author><first>Tajuddeen</first><last>Gwadabe</last><affiliation>University of Chinese Academy of Science</affiliation></author>
      <pages>1001-1014</pages>
      <abstract>We participated in the WMT 2022 Large-Scale Machine Translation Evaluation for the African Languages Shared Task. This work describes our approach, which is based on filtering the given noisy data using a sentence-pair classifier that was built by fine-tuning a pre-trained language model. To train the classifier, we obtain positive samples (i.e. high-quality parallel sentences) from a gold-standard curated dataset and extract negative samples (i.e. low-quality parallel sentences) from automatically aligned parallel data by choosing sentences with low alignment scores. Our final machine translation model was then trained on filtered data, instead of the entire noisy dataset. We empirically validate our approach by evaluating on two common datasets and show that data filtering generally improves overall translation quality, in some cases even significantly.</abstract>
      <url hash="b801bae1">2022.wmt-1.98</url>
      <bibkey>abdulmumin-etal-2022-separating</bibkey>
    </paper>
    <paper id="99">
      <title>Language Adapters for Large-Scale <fixed-case>MT</fixed-case>: The <fixed-case>GMU</fixed-case> System for the <fixed-case>WMT</fixed-case> 2022 Large-Scale Machine Translation Evaluation for <fixed-case>A</fixed-case>frican Languages Shared Task</title>
      <author><first>Md Mahfuz Ibn</first><last>Alam</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>George Mason University</affiliation></author>
      <pages>1015-1033</pages>
      <abstract>This report describes GMU’s machine translation systems for the WMT22 shared task on large-scale machine translation evaluation for African languages. We participated in the constrained translation track where only the data listed on the shared task page were allowed, including submissions accepted to the Data track. Our approach uses models initialized with DeltaLM, a generic pre-trained multilingual encoder-decoder model, and fine-tuned correspondingly with the allowed data sources. Our best submission incorporates language family and language-specific adapter units; ranking ranked second under the constrained setting.</abstract>
      <url hash="d274666e">2022.wmt-1.99</url>
      <bibkey>alam-anastasopoulos-2022-language</bibkey>
    </paper>
    <paper id="100">
      <title><fixed-case>S</fixed-case>amsung Research <fixed-case>P</fixed-case>hilippines - Datasaur <fixed-case>AI</fixed-case>’s Submission for the <fixed-case>WMT</fixed-case>22 Large Scale Multilingual Translation Task</title>
      <author><first>Jan Christian Blaise</first><last>Cruz</last><affiliation>Samsung Research Philippines (SRPH)</affiliation></author>
      <author><first>Lintang</first><last>Sutawika</last><affiliation>Konvergen AI</affiliation></author>
      <pages>1034-1038</pages>
      <abstract>This paper describes the submission of the joint Samsung Research Philippines - Datasaur AI team for the WMT22 Large Scale Multilingual African Translation shared task. We approach the contest as a way to explore task composition as a solution for low-resource multilingual translation, using adapter fusion to combine multiple task adapters that learn subsets of the total translation pairs. Our final model shows performance improvements in 32 out of the 44 translation directions that we participate in when compared to a single model system trained on multiple directions at once.</abstract>
      <url hash="3a161834">2022.wmt-1.100</url>
      <bibkey>cruz-sutawika-2022-samsung</bibkey>
    </paper>
    <paper id="101">
      <title>University of Cape Town’s <fixed-case>WMT</fixed-case>22 System: Multilingual Machine Translation for <fixed-case>S</fixed-case>outhern <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Khalid</first><last>Elmadani</last><affiliation>University of Cape Town</affiliation></author>
      <author><first>Francois</first><last>Meyer</last><affiliation>University of Cape Town</affiliation></author>
      <author><first>Jan</first><last>Buys</last><affiliation>University of Cape Town</affiliation></author>
      <pages>1039-1048</pages>
      <abstract>The paper describes the University of Cape Town’s submission to the constrained track of the WMT22 Shared Task: Large-Scale Machine Translation Evaluation for African Languages. Our system is a single multilingual translation model that translates between English and 8 South / South East African Languages, as well as between specific pairs of the African languages. We used several techniques suited for low-resource machine translation (MT), including overlap BPE, back-translation, synthetic training data generation, and adding more translation directions during training. Our results show the value of these techniques, especially for directions where very little or no bilingual training data is available.</abstract>
      <url hash="2784a352">2022.wmt-1.101</url>
      <bibkey>elmadani-etal-2022-university</bibkey>
    </paper>
    <paper id="102">
      <title>Tencent’s Multilingual Machine Translation System for <fixed-case>WMT</fixed-case>22 Large-Scale <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Wenxiang</first><last>Jiao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Jiarui</first><last>Li</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Wenxuan</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jen-tse</first><last>Huang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>1049-1056</pages>
      <abstract>This paper describes Tencent’s multilingual machine translation systems for the WMT22 shared task on Large-Scale Machine Translation Evaluation for African Languages. We participated in the constrained translation track in which only the data and pretrained models provided by the organizer are allowed. The task is challenging due to three problems, including the absence of training data for some to-be-evaluated language pairs, the uneven optimization of language pairs caused by data imbalance, and the curse of multilinguality. To address these problems, we adopt data augmentation, distributionally robust optimization, and language family grouping, respectively, to develop our multilingual neural machine translation (MNMT) models. Our submissions won the 1st place on the blind test sets in terms of the automatic evaluation metrics. Codes, models, and detailed competition results are available at <url>https://github.com/wxjiao/WMT2022-Large-Scale-African</url>.</abstract>
      <url hash="b70195b2">2022.wmt-1.102</url>
      <bibkey>jiao-etal-2022-tencents</bibkey>
    </paper>
    <paper id="103">
      <title><fixed-case>DENTRA</fixed-case>: Denoising and Translation Pre-training for Multilingual Machine Translation</title>
      <author><first>Samta</first><last>Kamboj</last><affiliation>Inception Institute of Artificial Intelligence, Abu Dhabi, UAE</affiliation></author>
      <author><first>Sunil Kumar</first><last>Sahu</last><affiliation>Inception Institute of Artificial Intelligence</affiliation></author>
      <author><first>Neha</first><last>Sengupta</last><affiliation>Inception Institute of Artificial Intelligence</affiliation></author>
      <pages>1057-1067</pages>
      <abstract>In this paper, we describe our submission to the WMT-2022: Large-Scale Machine Translation Evaluation for African Languages under the Constrained Translation track. We introduce DENTRA, a novel pre-training strategy for a multilingual sequence-to-sequence transformer model. DENTRA pre-training combines denoising and translation objectives to incorporate both monolingual and bitext corpora in 24 African, English, and French languages. To evaluate the quality of DENTRA, we fine-tuned it with two multilingual machine translation configurations, one-to-many and many-to-one. In both pre-training and fine-tuning, we employ only the datasets provided by the organizers. We compare DENTRA against a strong baseline, M2M-100, in different African multilingual machine translation scenarios and show gains in 3 out of 4 subtasks.</abstract>
      <url hash="50e6a382">2022.wmt-1.103</url>
      <bibkey>kamboj-etal-2022-dentra</bibkey>
    </paper>
    <paper id="104">
      <title>The <fixed-case>V</fixed-case>olc<fixed-case>T</fixed-case>rans System for <fixed-case>WMT</fixed-case>22 Multilingual Machine Translation Task</title>
      <author><first>Xian</first><last>Qian</last><affiliation>ByteDance AI LAB</affiliation></author>
      <author><first>Kai</first><last>Hu</last><affiliation>ByteDance AI LAB</affiliation></author>
      <author><first>Jiaqiang</first><last>Wang</last><affiliation>ByteDance</affiliation></author>
      <author><first>Yifeng</first><last>Liu</last><affiliation>Institute for Interdisciplinary Information Sciences, Tsinghua University</affiliation></author>
      <author><first>Xingyuan</first><last>Pan</last><affiliation>Department of Computer Science and Engineering, Wuhan University</affiliation></author>
      <author><first>Jun</first><last>Cao</last><affiliation>ByteDance AI Lab</affiliation></author>
      <author><first>Mingxuan</first><last>Wang</last><affiliation>Bytedance AI Lab</affiliation></author>
      <pages>1068-1075</pages>
      <abstract>This report describes our VolcTrans system for the WMT22 shared task on large-scale multilingual machine translation. We participated in the unconstrained track which allows the use of external resources. Our system is a transformer-based multilingual model trained on data from multiple sources including the public training set from the data track, NLLB data provided by Meta AI, self-collected parallel corpora, and pseudo bitext from back-translation. Both bilingual and monolingual texts are cleaned by a series of heuristic rules. On the official test set, our system achieves 17.3 BLEU, 21.9 spBLEU, and 41.9 chrF2++ on average over all language pairs. Averaged inference speed is 11.5 sentences per second using a single Nvidia Tesla V100 GPU.</abstract>
      <url hash="6c64b097">2022.wmt-1.104</url>
      <attachment type="dataset" hash="0423b40d">2022.wmt-1.104.dataset.zip</attachment>
      <bibkey>qian-etal-2022-volctrans</bibkey>
    </paper>
    <paper id="105">
      <title><fixed-case>W</fixed-case>eb<fixed-case>C</fixed-case>rawl <fixed-case>A</fixed-case>frican : A Multilingual Parallel Corpora for <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Pavanpankaj</first><last>Vegi</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <author><first>Sivabhavani</first><last>J</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <author><first>Biswajit</first><last>Paul</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <author><first>Abhinav</first><last>Mishra</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <author><first>Prashant</first><last>Banjare</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <author><first>Prasanna</first><last>K R</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <author><first>Chitra</first><last>Viswanathan</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <pages>1076-1089</pages>
      <abstract>WebCrawl African is a mixed domain multilingual parallel corpora for a pool of African languages compiled by ANVITA machine translation team of Centre for Artificial Intelligence and Robotics Lab, primarily for accelerating research on low-resource and extremely low-resource machine translation and is part of the submission to WMT 2022 shared task on Large-Scale Machine Translation Evaluation for African Languages under the data track. The corpora is compiled through web data mining and comprises 695K parallel sentences spanning 74 different language pairs from English and 15 African languages, many of which fall under low and extremely low resource categories. As a measure of corpora usefulness, a MNMT model for 24 African languages to English is trained by combining WebCrawl African corpora with existing corpus and evaluation on FLORES200 shows that inclusion of WebCrawl African corpora could improve BLEU score by 0.01-1.66 for 12 out of 15 African to English translation directions and even by 0.18-0.68 for the 4 out of 9 African to English translation directions which are not part of WebCrawl African corpora. WebCrawl African corpora includes more parallel sentences for many language pairs in comparison to OPUS public repository. This data description paper captures creation of corpora and results obtained along with datasheets. The WebCrawl African corpora is hosted on github repository.</abstract>
      <url hash="8c2322b6">2022.wmt-1.105</url>
      <bibkey>vegi-etal-2022-webcrawl</bibkey>
    </paper>
    <paper id="106">
      <title><fixed-case>ANVITA</fixed-case>-<fixed-case>A</fixed-case>frican: A Multilingual Neural Machine Translation System for <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Pavanpankaj</first><last>Vegi</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <author><first>Sivabhavani</first><last>J</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <author><first>Biswajit</first><last>Paul</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <author><first>Prasanna</first><last>K R</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <author><first>Chitra</first><last>Viswanathan</last><affiliation>Center for Artficial Intelligence and Robotics</affiliation></author>
      <pages>1090-1097</pages>
      <abstract>This paper describes ANVITA African NMT system submitted by team ANVITA for WMT 2022 shared task on Large-Scale Machine Translation Evaluation for African Languages under the constrained translation track. The team participated in 24 African languages to English MT directions. For better handling of relatively low resource language pairs and effective transfer learning, models are trained in multilingual setting. Heuristic based corpus filtering is applied and it improved performance by 0.04-2.06 BLEU across 22 out of 24 African to English directions and also improved training time by 5x. Use of deep transformer with 24 layers of encoder and 6 layers of decoder significantly improved performance by 1.1-7.7 BLEU across all the 24 African to English directions compared to base transformer. For effective selection of source vocabulary in multilingual setting, joint and language wise vocabulary selection strategies are explored at the source side. Use of language wise vocabulary selection however did not consistently improve performance of low resource languages in comparison to joint vocabulary selection. Empirical results indicate that training using deep transformer with filtered corpora seems to be a better choice than using base transformer on the whole corpora both in terms of accuracy and training time.</abstract>
      <url hash="ef6ffa31">2022.wmt-1.106</url>
      <bibkey>vegi-etal-2022-anvita</bibkey>
    </paper>
    <paper id="107">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> Systems for <fixed-case>WMT</fixed-case>22 Very Low Resource Supervised <fixed-case>MT</fixed-case> Task</title>
      <author><first>Shaojun</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuanchang</first><last>Luo</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jinlong</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuhao</first><last>Xie</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Lizhi</first><last>Lei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ying</first><last>Qin</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <pages>1098-1103</pages>
      <abstract>This paper describes the submissions of Huawei translation services center (HW-TSC) to the WMT22 Very Low Resource Supervised MT task. We participate in all 6 supervised tracks including all combinations between Upper/Lower Sorbian (Hsb/Dsb) and German (De). Our systems are build on deep Transformer with a large filter size. We use multilingual transfer with German-Czech (De-Cs) and German-Polish (De-Pl) parallel data. We also utilize regularized dropout (R-Drop), back translation, fine-tuning and ensemble to improve the system performance. According to the official evaluation results on OCELoT, our supervised systems of all 6 language directions get the highest BLEU scores among all submissions. Our pre-trained multilingual model for unsupervised De2Dsb and Dsb2De translation also gain highest BLEU.</abstract>
      <url hash="3ee191ce">2022.wmt-1.107</url>
      <bibkey>li-etal-2022-hw-tsc-systems</bibkey>
    </paper>
    <paper id="108">
      <title>Unsupervised and Very-Low Resource Supervised Translation on <fixed-case>G</fixed-case>erman and Sorbian Variant Languages</title>
      <author><first>Rahul</first><last>Tangsali</last><affiliation>Pune Institute of Computer Technology</affiliation></author>
      <author><first>Aditya</first><last>Vyawahare</last><affiliation>Pune Institute of Computer Technology</affiliation></author>
      <author><first>Aditya</first><last>Mandke</last><affiliation>University of California San Diego</affiliation></author>
      <author><first>Onkar</first><last>Litake</last><affiliation>University of California San Diego</affiliation></author>
      <author><first>Dipali</first><last>Kadam</last><affiliation>Pune Institute of Computer Technology</affiliation></author>
      <pages>1104-1110</pages>
      <abstract>This paper presents the work of team PICT-NLP for the shared task on unsupervised and very low-resource supervised machine translation, organized by the Workshop on Machine Translation, a workshop in collocation with the Conference on Empirical Methods in Natural Language Processing (EMNLP 2022). The paper delineates the approaches we implemented for supervised and unsupervised translation between the following 6 language pairs: German-Lower Sorbian (de-dsb), Lower Sorbian-German (dsb-de), Lower Sorbian-Upper Sorbian (dsb-hsb), Upper Sorbian-Lower Sorbian (hsb-dsb), German-Upper Sorbian (de-hsb), and Upper Sorbian-German (hsb-de). For supervised learning, we implemented the transformer architecture from scratch using the Fairseq library. Whereas for unsupervised learning, we implemented Facebook’s XLM masked language modeling approach. We discuss the training details for the models we used, and the results obtained from our approaches. We used the BLEU and chrF metrics for evaluating the accuracies of the generated translations on our systems.</abstract>
      <url hash="6bd34660">2022.wmt-1.108</url>
      <bibkey>tangsali-etal-2022-unsupervised</bibkey>
    </paper>
    <paper id="109">
      <title><fixed-case>MUNI</fixed-case>-<fixed-case>NLP</fixed-case> Systems for <fixed-case>L</fixed-case>ower <fixed-case>S</fixed-case>orbian-<fixed-case>G</fixed-case>erman and <fixed-case>L</fixed-case>ower <fixed-case>S</fixed-case>orbian-<fixed-case>U</fixed-case>pper <fixed-case>S</fixed-case>orbian Machine Translation @ <fixed-case>WMT</fixed-case>22</title>
      <author><first>Edoardo</first><last>Signoroni</last><affiliation>Faculty of Informatics, Masaryk University</affiliation></author>
      <author><first>Pavel</first><last>Rychlý</last><affiliation>NLP Centre, Faculty of Informatics, Masaryk University</affiliation></author>
      <pages>1111-1116</pages>
      <abstract>We describe our neural machine translation systems for the WMT22 shared task on unsupervised MT and very low resource supervised MT. We submit supervised NMT systems for Lower Sorbian-German and Lower Sorbian-Upper Sorbian translation in both directions. By using a novel tokenization algorithm, data augmentation techniques, such as Data Diversification (DD), and parameter optimization we improve on our baselines by 10.5-10.77 BLEU for Lower Sorbian-German and by 1.52-1.88 BLEU for Lower Sorbian-Upper Sorbian.</abstract>
      <url hash="4222a912">2022.wmt-1.109</url>
      <bibkey>signoroni-rychly-2022-muni</bibkey>
    </paper>
    <paper id="110">
      <title>The <fixed-case>AIC</fixed-case> System for the <fixed-case>WMT</fixed-case> 2022 Unsupervised <fixed-case>MT</fixed-case> and Very Low Resource Supervised <fixed-case>MT</fixed-case> Task</title>
      <author><first>Ahmad</first><last>Shapiro</last><affiliation>Applied Innovation Center, MCIT - Egypt</affiliation></author>
      <author><first>Mahmoud</first><last>Salama</last><affiliation>Applied Innovation Center, MCIT - Egypt</affiliation></author>
      <author><first>Omar</first><last>Abdelhakim</last><affiliation>Applied Innovation Center, MCIT - Egypt</affiliation></author>
      <author><first>Mohamed</first><last>Fayed</last><affiliation>Applied Innovation Center, MCIT - Egypt</affiliation></author>
      <author><first>Ayman</first><last>Khalafallah</last><affiliation>Applied Innovation Center, MCIT - Egypt</affiliation></author>
      <author><first>Noha</first><last>Adly</last><affiliation>Applied Innovation Center, MCIT - Egypt</affiliation></author>
      <pages>1117-1121</pages>
      <abstract>This paper presents our submissions to WMT 22 shared task in the Unsupervised and Very Low Resource Supervised Machine Translation tasks. The task revolves around translating between German ↔ Upper Sorbian (de ↔ hsb), German ↔ Lower Sorbian (de ↔ dsb) and Upper Sorbian ↔ Lower Sorbian (hsb ↔ dsb) in both unsupervised and supervised manner. For the unsupervised system, we trained an unsupervised phrase-based statistical machine translation (UPBSMT) system on each pair independently. We pretrained a De-Salvic mBART model on the following languages Polish (pl), Czech (cs), German (de), Upper Sorbian (hsb), Lower Sorbian (dsb). We then fine-tuned our mBART on the synthetic parallel data generated by the (UPBSMT) model along with authentic parallel data (de ↔ pl, de ↔ cs). We further fine-tuned our unsupervised system on authentic parallel data (hsb ↔ dsb, de ↔ dsb, de ↔ hsb) to submit our supervised low-resource system.</abstract>
      <url hash="e089883b">2022.wmt-1.110</url>
      <bibkey>shapiro-etal-2022-aic</bibkey>
    </paper>
    <paper id="111">
      <title><fixed-case>NICT</fixed-case> at <fixed-case>M</fixed-case>ix<fixed-case>MT</fixed-case> 2022: Synthetic Code-Mixed Pre-training and Multi-way Fine-tuning for <fixed-case>H</fixed-case>inglish–<fixed-case>E</fixed-case>nglish Translation</title>
      <author><first>Raj</first><last>Dabre</last><affiliation>Nict</affiliation></author>
      <pages>1122-1125</pages>
      <abstract>In this paper, we describe our submission to the Code-mixed Machine Translation (MixMT) shared task. In MixMT, the objective is to translate Hinglish to English and vice versa. For our submissions, we focused on code-mixed pre-training and multi-way fine-tuning. Our submissions achieved rank 4 in terms of automatic evaluation score. For Hinglish to English translation, our submission achieved rank 4 as well.</abstract>
      <url hash="02cef907">2022.wmt-1.111</url>
      <bibkey>dabre-2022-nict</bibkey>
    </paper>
    <paper id="112">
      <title>Gui at <fixed-case>M</fixed-case>ix<fixed-case>MT</fixed-case> 2022 : <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>inglish : An <fixed-case>MT</fixed-case> Approach for Translation of Code Mixed Data</title>
      <author><first>Akshat</first><last>Gahoi</last><affiliation>International Institute of Information Technology - Hyderabad</affiliation></author>
      <author><first>Jayant</first><last>Duneja</last><affiliation>International Institute of Information Technology - Hyderabad</affiliation></author>
      <author><first>Anshul</first><last>Padhi</last><affiliation>International Institute of Information Technology - Hyderabad</affiliation></author>
      <author><first>Shivam</first><last>Mangale</last><affiliation>International Institute of Information Technology - Hyderabad</affiliation></author>
      <author><first>Saransh</first><last>Rajput</last><affiliation>International Institute of Information Technology, Hyderabad</affiliation></author>
      <author><first>Tanvi</first><last>Kamble</last><affiliation>International Institute of Information Technology - Hyderabad</affiliation></author>
      <author><first>Dipti</first><last>Sharma</last><affiliation>International Institute of Information Technology - Hyderabad</affiliation></author>
      <author><first>Vasudev</first><last>Varma</last><affiliation>International Institute of Information Technology - Hyderabad</affiliation></author>
      <pages>1126-1130</pages>
      <abstract>Code-mixed machine translation has become an important task in multilingual communities and extending the task of machine translation to code mixed data has become a common task for these languages. In the shared tasks of EMNLP 2022, we try to tackle the same for both English + Hindi to Hinglish and Hinglish to English. The first task dealt with both Roman and Devanagari script as we had monolingual data in both English and Hindi whereas the second task only had data in Roman script. To our knowledge, we achieved one of the top ROUGE-L and WER scores for the first task of Monolingual to Code-Mixed machine translation. In this paper, we discuss the use of mBART with some special pre-processing and post-processing (transliteration from Devanagari to Roman) for the first task in detail and the experiments that we performed for the second task of translating code-mixed Hinglish to monolingual English.</abstract>
      <url hash="f5f0dc74">2022.wmt-1.112</url>
      <bibkey>gahoi-etal-2022-gui</bibkey>
    </paper>
    <paper id="113">
      <title><fixed-case>MUCS</fixed-case>@<fixed-case>M</fixed-case>ix<fixed-case>MT</fixed-case>: <fixed-case>I</fixed-case>ndic<fixed-case>T</fixed-case>rans-based Machine Translation for <fixed-case>H</fixed-case>inglish Text</title>
      <author><first>Asha</first><last>Hegde</last><affiliation>Mangalore University</affiliation></author>
      <author><first>Shashirekha</first><last>Lakshmaiah</last><affiliation>Mangalore University</affiliation></author>
      <pages>1131-1135</pages>
      <url hash="9e2ff8ed">2022.wmt-1.113</url>
      <bibkey>hegde-lakshmaiah-2022-mucs</bibkey>
      <abstract>Code-mixing is the phenomena of mixing various linguistic units such as paragraphs, sentences, phrases, words, etc., of one language with that of the other language in any text. This code-mixing is predominantly used by social media users who know more than one language. Processing code-mixed text is challenging because of its characteristics and lack of tools that supports such data. Further, pretrained models can be used for the formal text and not for the informal text such as code-mixed. Developing efficient Machine Translation (MT) systems for code-mixed text is challenging due to lack of code-mixed training data. Further, existing MT systems developed to translate monolingual data are not portable to translate code-mixed text mainly due to its informal nature. To address the MT challenges of code-mixed text, this paper describes the proposed MT models submitted by our team MUCS, to the Code-mixed Machine Translation (MixMT) shared task in the Workshop on Machine Translation (WMT) organized in connection with Empirical models in Natural Language Processing (EMNLP) 2022. This shared has two subtasks: i) subtask 1 - to translate English sentences and their corresponding Hindi translations into Hinglish text and ii) subtask 2 - to translate Hinglish text into English text. The proposed models that translate the code-mixed English text to Hinglish (English-Hindli code-mixed text) and vice-versa, comprises of i) transliterating Hinglish text from Latin to Devanagari script and vice-versa, ii) pseudo translation generation using existing models, and iii) efficient target generation by combining the pseudo translations along with the training data provided by the shared task organizers. The proposed models obtained <tex-math>5^{th}</tex-math> and <tex-math>3^{rd}</tex-math> rank with Recall-Oriented Under-study for Gisting Evaluation (ROUGE) scores of 0.35806 and 0.55453 for subtask 1 and subtask 2 respectively.</abstract>
    </paper>
    <paper id="114">
      <title><fixed-case>SIT</fixed-case> at <fixed-case>M</fixed-case>ix<fixed-case>MT</fixed-case> 2022: Fluent Translation Built on Giant Pre-trained Models</title>
      <author><first>Abdul</first><last>Khan</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Hrishikesh</first><last>Kanade</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Girish</first><last>Budhrani</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Preet</first><last>Jhanglani</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Jia</first><last>Xu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <pages>1136-1144</pages>
      <abstract>This paper describes the Stevens Institute of Technology’s submission for the WMT 2022 Shared Task: Code-mixed Machine Translation (MixMT). The task consisted of two subtasks, subtask 1 Hindi/English to Hinglish and subtask 2 Hinglish to English translation. Our findings lie in the improvements made through the use of large pre-trained multilingual NMT models and in-domain datasets, as well as back-translation and ensemble techniques. The translation output is automatically evaluated against the reference translations using ROUGE-L and WER. Our system achieves the 1st position on subtask 2 according to ROUGE-L, WER, and human evaluation, 1st position on subtask 1 according to WER and human evaluation, and 3rd position on subtask 1 with respect to ROUGE-L metric.</abstract>
      <url hash="3ba06dae">2022.wmt-1.114</url>
      <bibkey>khan-etal-2022-sit</bibkey>
    </paper>
    <paper id="115">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh’s Submission to the <fixed-case>WMT</fixed-case>22 Code-Mixing Shared Task (<fixed-case>M</fixed-case>ix<fixed-case>MT</fixed-case>)</title>
      <author><first>Faheem</first><last>Kirefu</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Vivek</first><last>Iyer</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Pinzhen</first><last>Chen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Laurie</first><last>Burchell</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>1145-1157</pages>
      <abstract>The University of Edinburgh participated in the WMT22 shared task on code-mixed translation. This consists of two subtasks: i) generating code-mixed Hindi/English (Hinglish) text generation from parallel Hindi and English sentences and ii) machine translation from Hinglish to English. As both subtasks are considered low-resource, we focused our efforts on careful data generation and curation, especially the use of backtranslation from monolingual resources. For subtask 1 we explored the effects of constrained decoding on English and transliterated subwords in order to produce Hinglish. For subtask 2, we investigated different pretraining techniques, namely comparing simple initialisation from existing machine translation models and aligned augmentation. For both subtasks, we found that our baseline systems worked best. Our systems for both subtasks were one of the overall top-performing submissions.</abstract>
      <url hash="62205c02">2022.wmt-1.115</url>
      <bibkey>kirefu-etal-2022-university</bibkey>
    </paper>
    <paper id="116">
      <title><fixed-case>CNLP</fixed-case>-<fixed-case>NITS</fixed-case>-<fixed-case>PP</fixed-case> at <fixed-case>M</fixed-case>ix<fixed-case>MT</fixed-case> 2022: <fixed-case>H</fixed-case>inglish-<fixed-case>E</fixed-case>nglish Code-Mixed Machine Translation</title>
      <author><first>Sahinur Rahman</first><last>Laskar</last><affiliation>National Institute of Technology Silchar</affiliation></author>
      <author><first>Rahul</first><last>Singh</last><affiliation>National Institute of Technology Silchar</affiliation></author>
      <author><first>Shyambabu</first><last>Pandey</last><affiliation>National Institute of Technology Silchar</affiliation></author>
      <author><first>Riyanka</first><last>Manna</last><affiliation>Adamas University</affiliation></author>
      <author><first>Partha</first><last>Pakray</last><affiliation>National Institute of Technology Silchar</affiliation></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last><affiliation>National Institute of Technology Silchar</affiliation></author>
      <pages>1158-1161</pages>
      <abstract>The mixing of two or more languages in speech or text is known as code-mixing. In this form of communication, users mix words and phrases from multiple languages. Code-mixing is very common in the context of Indian languages due to the presence of multilingual societies. The probability of the existence of code-mixed sentences in almost all Indian languages since in India English is the dominant language for social media textual communication platforms. We have participated in the WMT22 shared task of code-mixed machine translation with the team name: CNLP-NITS-PP. In this task, we have prepared a synthetic Hinglish–English parallel corpus using transliteration of original Hindi sentences to tackle the limitation of the parallel corpus, where, we mainly considered sentences that have named-entity (proper noun) from the available English-Hindi parallel corpus. With the addition of synthetic bi-text data to the original parallel corpus (train set), our transformer-based neural machine translation models have attained recall-oriented understudy for gisting evaluation (ROUGE-L) scores of 0.23815, 0.33729, and word error rate (WER) scores of 0.95458, 0.88451 at Sub-Task-1 (English-to-Hinglish) and Sub-Task-2 (Hinglish-to-English) for test set results respectively.</abstract>
      <url hash="712aa364">2022.wmt-1.116</url>
      <bibkey>laskar-etal-2022-cnlp-nits</bibkey>
    </paper>
    <paper id="117">
      <title>Domain Curricula for Code-Switched <fixed-case>MT</fixed-case> at <fixed-case>M</fixed-case>ix<fixed-case>MT</fixed-case> 2022</title>
      <author><first>Lekan</first><last>Raheem</last><affiliation>African Institute for Mathematical Sciences (AIMS)</affiliation></author>
      <author><first>Maab</first><last>Elrashid</last><affiliation>African Institute for Mathematical Sciences (AIMS)</affiliation></author>
      <author><first>Melvin</first><last>Johnson</last><affiliation>Google Research</affiliation></author>
      <author><first>Julia</first><last>Kreutzer</last><affiliation>Google Research</affiliation></author>
      <pages>1162-1169</pages>
      <abstract>In multilingual colloquial settings, it is a habitual occurrence to compose expressions of text or speech containing tokens or phrases of different languages, a phenomenon popularly known as code-switching or code-mixing (CMX). We present our approach and results for the Code-mixed Machine Translation (MixMT) shared task at WMT 2022: the task consists of two subtasks, monolingual to code-mixed machine translation (Subtask-1) and code-mixed to monolingual machine translation (Subtask-2). Most non-synthetic code-mixed data are from social media but gathering a significant amount of this kind of data would be laborious and this form of data has more writing variation than other domains, so for both subtasks, we experimented with data schedules for out-of-domain data. We jointly learn multiple domains of text by pretraining and fine-tuning, combined with a sentence alignment objective. We found that switching between domains caused improved performance in the domains seen earliest during training, but depleted the performance on the remaining domains. A continuous training run with strategically dispensed data of different domains showed a significantly improved performance over fine-tuning.</abstract>
      <url hash="cb316c18">2022.wmt-1.117</url>
      <bibkey>raheem-etal-2022-domain</bibkey>
    </paper>
    <paper id="118">
      <title>Lingua Custodia’s Participation at the <fixed-case>WMT</fixed-case> 2022 Word-Level Auto-completion Shared Task</title>
      <author><first>Melissa</first><last>Ailem</last><affiliation>Lingua Custodia</affiliation></author>
      <author><first>Jingshu</first><last>Liu</last><affiliation>Lingua Custodia</affiliation></author>
      <author><first>Jean-gabriel</first><last>Barthelemy</last><affiliation>Lingua Custodia</affiliation></author>
      <author><first>Raheel</first><last>Qader</last><affiliation>Lingua Custodia</affiliation></author>
      <pages>1170-1175</pages>
      <abstract>This paper presents Lingua Custodia’s submission to the WMT22 shared task on Word Level Auto-completion (WLAC). We consider two directions, namely German-English and English-German.The WLAC task in Neural Machine Translation (NMT) consists in predicting a target word given few human typed characters, the source sentence to translate, as well as some translation context. Inspired by recent work in terminology control, we propose to treat the human typed sequence as a constraint to predict the right word starting by the latter. To do so, the source side of the training data is augmented with both the constraints and the translation context. In addition, following new advances in WLAC, we use a joint optimization strategy taking into account several types of translation context. The automatic as well as human accuracy obtained with the submitted systems show the effectiveness of the proposed method.</abstract>
      <url hash="d36c7e31">2022.wmt-1.118</url>
      <bibkey>ailem-etal-2022-lingua</bibkey>
    </paper>
    <paper id="119">
      <title>Translation Word-Level Auto-Completion: What Can We Achieve Out of the Box?</title>
      <author><first>Yasmin</first><last>Moslem</last><affiliation>ADAPT Centre, Dublin City University</affiliation></author>
      <author><first>Rejwanul</first><last>Haque</last><affiliation>National College of Ireland</affiliation></author>
      <author><first>Andy</first><last>Way</last><affiliation>ADAPT Centre, Dublin City University</affiliation></author>
      <pages>1176-1181</pages>
      <abstract>Research on Machine Translation (MT) has achieved important breakthroughs in several areas. While there is much more to be done in order to build on this success, we believe that the language industry needs better ways to take full advantage of current achievements. Due to a combination of factors, including time, resources, and skills, businesses tend to apply pragmatism into their AI workflows. Hence, they concentrate more on outcomes, e.g. delivery, shipping, releases, and features, and adopt high-level working production solutions, where possible. Among the features thought to be helpful for translators are sentence-level and word-level translation auto-suggestion and auto-completion. Suggesting alternatives can inspire translators and limit their need to refer to external resources, which hopefully boosts their productivity. This work describes our submissions to WMT’s shared task on word-level auto-completion, for the Chinese-to-English, English-to-Chinese, German-to-English, and English-to-German language directions. We investigate the possibility of using pre-trained models and out-of-the-box features from available libraries. We employ random sampling to generate diverse alternatives, which reveals good results. Furthermore, we introduce our open-source API, based on CTranslate2, to serve translations, auto-suggestions, and auto-completions.</abstract>
      <url hash="0e49cf4c">2022.wmt-1.119</url>
      <bibkey>moslem-etal-2022-translation</bibkey>
    </paper>
    <paper id="120">
      <title><fixed-case>PRHLT</fixed-case>’s Submission to <fixed-case>WLAC</fixed-case> 2022</title>
      <author><first>Angel</first><last>Navarro</last><affiliation>Prhlt</affiliation></author>
      <author><first>Miguel</first><last>Domingo</last><affiliation>Universitat Politècnica de València</affiliation></author>
      <author><first>Francisco</first><last>Casacuberta</last><affiliation>Universitat Politècnica de València</affiliation></author>
      <pages>1182-1186</pages>
      <abstract>This paper describes our submission to the Word-Level AutoCompletion shared task of WMT22. We participated in the English–German and German–English categories. We proposed a segment-based interactive machine translation approach whose central core is a machine translation (MT) model which generates a complete translation from the context provided by the task. From there, we obtain the word which corresponds to the autocompletion. With this approach, we aim to show that it is possible to use the MT models in the autocompletion task by simply performing minor changes at the decoding step, obtaining satisfactory results.</abstract>
      <url hash="f59fd596">2022.wmt-1.120</url>
      <bibkey>navarro-etal-2022-prhlts</bibkey>
    </paper>
    <paper id="121">
      <title><fixed-case>IIGROUP</fixed-case> Submissions for <fixed-case>WMT</fixed-case>22 Word-Level <fixed-case>A</fixed-case>uto<fixed-case>C</fixed-case>ompletion Task</title>
      <author><first>Cheng</first><last>Yang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Siheng</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Chufan</first><last>Shi</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>1187-1191</pages>
      <abstract>This paper presents IIGroup’s submission to the WMT22 Word-Level AutoCompletion(WLAC) Shared Task in four language directions. We propose to use a Generate-then-Rerank framework to solve this task. More specifically, the generator is used to generate candidate words and recall as many positive candidates as possible. To facilitate the training process of the generator, we propose a span-level mask prediction task. Once we get the candidate words, we take the top-K candidates and feed them into the reranker. The reranker is used to select the most confident candidate. The experimental results in four language directions demonstrate the effectiveness of our systems. Our systems achieve competitive performance ranking 1st in English to Chinese subtask and 2nd in Chinese to English subtask.</abstract>
      <url hash="a2a52152">2022.wmt-1.121</url>
      <bibkey>yang-etal-2022-iigroup</bibkey>
    </paper>
    <paper id="122">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Submissions to the <fixed-case>WMT</fixed-case>22 Word-Level Auto Completion Task</title>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xianghui</first><last>He</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jinlong</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuanchang</first><last>Luo</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuhao</first><last>Xie</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Lizhi</first><last>Lei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ying</first><last>Qin</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <pages>1192-1197</pages>
      <url hash="7c450325">2022.wmt-1.122</url>
      <bibkey>yang-etal-2022-hw-tscs</bibkey>
      <abstract>This paper presents the submissions of Huawei Translation Services Center (HW-TSC) to WMT 2022 Word-Level AutoCompletion Task. We propose an end-to-end autoregressive model with bi-context based on Transformer to solve current task. The model uses a mixture of subword and character encoding units to realize the joint encoding of human input, the context of the target side and the decoded sequence, which ensures full utilization of information. We uses one model to solve four types of data structures in the task. During training, we try using a machine translation model as the pre-trained model and fine-tune it for the task. We also add BERT-style MLM data at the fine-tuning stage to improve model performance. We participate in zh<tex-math>\rightarrow</tex-math>en, en<tex-math>\rightarrow</tex-math>de, and de<tex-math>\rightarrow</tex-math>en directions and win the first place in all the three tracks. Particularly, we outperform the second place by more than 5% in terms of accuracy on the zh<tex-math>\rightarrow</tex-math>en and en<tex-math>\rightarrow</tex-math>de tracks. The result is buttressed by human evaluations as well, demonstrating the effectiveness of our model.</abstract>
    </paper>
    <paper id="123">
      <title><fixed-case>TSM</fixed-case>ind: <fixed-case>A</fixed-case>libaba and Soochow University’s Submission to the <fixed-case>WMT</fixed-case>22 Translation Suggestion Task</title>
      <author><first>Xin</first><last>Ge</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ke</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jiayi</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Nini</first><last>Xiao</last><affiliation>Soochow University</affiliation></author>
      <author><first>Xiangyu</first><last>Duan</last><affiliation>Soochow University</affiliation></author>
      <author><first>Yu</first><last>Zhao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yuqi</first><last>Zhang</last><affiliation>Alibaba inc.</affiliation></author>
      <pages>1198-1204</pages>
      <abstract>This paper describes the joint submission of Alibaba and Soochow University to the WMT 2022 Shared Task on Translation Suggestion (TS). We participate in the English to/from German and English to/from Chinese tasks. Basically, we utilize the model paradigm fine-tuning on the downstream tasks based on large-scale pre-trained models, which has recently achieved great success. We choose FAIR’s WMT19 English to/from German news translation system and MBART50 for English to/from Chinese as our pre-trained models. Considering the task’s condition of limited use of training data, we follow the data augmentation strategies provided by Yang to boost our TS model performance. And we further involve the dual conditional cross-entropy model and GPT-2 language model to filter augmented data. The leader board finally shows that our submissions are ranked first in three of four language directions in the Naive TS task of the WMT22 Translation Suggestion task.</abstract>
      <url hash="bf8b0eba">2022.wmt-1.123</url>
      <bibkey>ge-etal-2022-tsmind</bibkey>
    </paper>
    <paper id="124">
      <title>Transn’s Submissions to the <fixed-case>WMT</fixed-case>22 Translation Suggestion Task</title>
      <author><first>Mao</first><last>Hongbao</last><affiliation>Transn IOL Technology Co.,Ltd.</affiliation></author>
      <author><first>Zhang</first><last>Wenbo</last><affiliation>Transn IOL Technology Co.,Ltd.</affiliation></author>
      <author><first>Cai</first><last>Jie</last><affiliation>Transn IOL Technology Co.,Ltd.</affiliation></author>
      <author><first>Cheng</first><last>Jianwei</last><affiliation>Transn IOL Technology Co.,Ltd.</affiliation></author>
      <pages>1205-1210</pages>
      <abstract>This paper describes the Transn’s submissions to the WMT2022 shared task on TranslationSuggestion. Our team participated on two tasks: Naive Translation Suggestion and TranslationSuggestion with Hints, focusing on two language directions Zh→En and En→Zh. Apart from the golden training data provided by the shared task, we utilized synthetic corpus to fine-tune on DeltaLM (∆LM), which is a pre-trained encoder-decoder language model. We applied two-stage training strategy on ∆LM and several effective methods to generate synthetic corpus, which contribute a lot to the results. According to the official evaluation results in terms of BLEU scores, our submissions in Naive Translation Suggestion En→Zh and Translation Suggestion with Hints (both Zh→En and En→Zh) ranked 1st, and Naive Translation Suggestion Zh→En also achieved comparable result to the best score.</abstract>
      <url hash="aed7ffbc">2022.wmt-1.124</url>
      <bibkey>hongbao-etal-2022-transns</bibkey>
    </paper>
    <paper id="125">
      <title>Improved Data Augmentation for Translation Suggestion</title>
      <author><first>Hongxiao</first><last>Zhang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Siyu</first><last>Lai</last><affiliation>BeiJing JiaoTong University</affiliation></author>
      <author><first>Songming</first><last>Zhang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Hui</first><last>Huang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yufeng</first><last>Chen</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jian</first><last>Liu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>1211-1216</pages>
      <abstract>Translation suggestion (TS) models are used to automatically provide alternative suggestions for incorrect spans in sentences generated by machine translation. This paper introduces the system used in our submission to the WMT’22 Translation Suggestion shared task. Our system is based on the ensemble of different translation architectures, including Transformer, SA-Transformer, and DynamicConv. We use three strategies to construct synthetic data from parallel corpora to compensate for the lack of supervised data. In addition, we introduce a multi-phase pre-training strategy, adding an additional pre-training phase with in-domain data. We rank second and third on the English-German and English-Chinese bidirectional tasks, respectively.</abstract>
      <url hash="d818f220">2022.wmt-1.125</url>
      <bibkey>zhang-etal-2022-improved</bibkey>
    </paper>
  </volume>
</collection>
