<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.nllp">
  <volume id="1" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the Natural Legal Language Processing Workshop 2021</booktitle>
      <editor><first>Nikolaos</first><last>Aletras</last></editor>
      <editor><first>Ion</first><last>Androutsopoulos</last></editor>
      <editor><first>Leslie</first><last>Barrett</last></editor>
      <editor><first>Catalina</first><last>Goanta</last></editor>
      <editor><first>Daniel</first><last>Preotiuc-Pietro</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
      <venue>nllp</venue>
    </meta>
    <frontmatter>
      <url hash="db09a562">2021.nllp-1.0</url>
      <bibkey>nllp-2021-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Corpus for Multilingual Analysis of Online Terms of Service</title>
      <author><first>Kasper</first><last>Drawzeski</last></author>
      <author><first>Andrea</first><last>Galassi</last></author>
      <author><first>Agnieszka</first><last>Jablonowska</last></author>
      <author><first>Francesca</first><last>Lagioia</last></author>
      <author><first>Marco</first><last>Lippi</last></author>
      <author><first>Hans Wolfgang</first><last>Micklitz</last></author>
      <author><first>Giovanni</first><last>Sartor</last></author>
      <author><first>Giacomo</first><last>Tagiuri</last></author>
      <author><first>Paolo</first><last>Torroni</last></author>
      <pages>1–8</pages>
      <abstract>We present the first annotated corpus for multilingual analysis of potentially unfair clauses in online Terms of Service. The data set comprises a total of 100 contracts, obtained from 25 documents annotated in four different languages: English, German, Italian, and Polish. For each contract, potentially unfair clauses for the consumer are annotated, for nine different unfairness categories. We show how a simple yet efficient annotation projection technique based on sentence embeddings could be used to automatically transfer annotations across languages.</abstract>
      <url hash="3b404792">2021.nllp-1.1</url>
      <bibkey>drawzeski-etal-2021-corpus</bibkey>
      <doi>10.18653/v1/2021.nllp-1.1</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/multilingual-terms-of-service">Multilingual Terms of Service</pwcdataset>
    </paper>
    <paper id="2">
      <title>Named Entity Recognition in the <fixed-case>R</fixed-case>omanian Legal Domain</title>
      <author><first>Vasile</first><last>Pais</last></author>
      <author><first>Maria</first><last>Mitrofan</last></author>
      <author><first>Carol Luca</first><last>Gasan</last></author>
      <author><first>Vlad</first><last>Coneschi</last></author>
      <author><first>Alexandru</first><last>Ianov</last></author>
      <pages>9–18</pages>
      <abstract>Recognition of named entities present in text is an important step towards information extraction and natural language understanding. This work presents a named entity recognition system for the Romanian legal domain. The system makes use of the gold annotated LegalNERo corpus. Furthermore, the system combines multiple distributional representations of words, including word embeddings trained on a large legal domain corpus. All the resources, including the corpus, model and word embeddings are open sourced. Finally, the best system is available for direct usage in the RELATE platform.</abstract>
      <url hash="f10db3b4">2021.nllp-1.2</url>
      <bibkey>pais-etal-2021-named</bibkey>
      <doi>10.18653/v1/2021.nllp-1.2</doi>
      <pwccode url="https://github.com/racai-ai/LegalNER" additional="false">racai-ai/LegalNER</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/legalnero">LegalNERo</pwcdataset>
    </paper>
    <paper id="3">
      <title><fixed-case>S</fixed-case>wiss-Judgment-Prediction: A Multilingual Legal Judgment Prediction Benchmark</title>
      <author><first>Joel</first><last>Niklaus</last></author>
      <author><first>Ilias</first><last>Chalkidis</last></author>
      <author><first>Matthias</first><last>Stürmer</last></author>
      <pages>19–35</pages>
      <abstract>In many jurisdictions, the excessive workload of courts leads to high delays. Suitable predictive AI models can assist legal professionals in their work, and thus enhance and speed up the process. So far, Legal Judgment Prediction (LJP) datasets have been released in English, French, and Chinese. We publicly release a multilingual (German, French, and Italian), diachronic (2000-2020) corpus of 85K cases from the Federal Supreme Court of Switzer- land (FSCS). We evaluate state-of-the-art BERT-based methods including two variants of BERT that overcome the BERT input (text) length limitation (up to 512 tokens). Hierarchical BERT has the best performance (approx. 68-70% Macro-F1-Score in German and French). Furthermore, we study how several factors (canton of origin, year of publication, text length, legal area) affect performance. We release both the benchmark dataset and our code to accelerate future research and ensure reproducibility.</abstract>
      <url hash="1c06f7a9">2021.nllp-1.3</url>
      <bibkey>niklaus-etal-2021-swiss</bibkey>
      <doi>10.18653/v1/2021.nllp-1.3</doi>
      <pwccode url="https://github.com/joelniklaus/swissjudgementprediction" additional="false">joelniklaus/swissjudgementprediction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/echr">ECHR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ecthr">ECtHR</pwcdataset>
    </paper>
    <paper id="4">
      <title>Automated Extraction of Sentencing Decisions from Court Cases in the <fixed-case>H</fixed-case>ebrew Language</title>
      <author><first>Mohr</first><last>Wenger</last></author>
      <author><first>Tom</first><last>Kalir</last></author>
      <author><first>Noga</first><last>Berger</last></author>
      <author><first>Carmit Klar</first><last>Chalamish</last></author>
      <author><first>Renana</first><last>Keydar</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <pages>36–45</pages>
      <abstract>We present the task of Automated Punishment Extraction (APE) in sentencing decisions from criminal court cases in Hebrew. Addressing APE will enable the identification of sentencing patterns and constitute an important stepping stone for many follow up legal NLP applications in Hebrew, including the prediction of sentencing decisions. We curate a dataset of sexual assault sentencing decisions and a manually-annotated evaluation dataset, and implement rule-based and supervised models. We find that while supervised models can identify the sentence containing the punishment with good accuracy, rule-based approaches outperform them on the full APE task. We conclude by presenting a first analysis of sentencing patterns in our dataset and analyze common models’ errors, indicating avenues for future work, such as distinguishing between probation and actual imprisonment punishment. We will make all our resources available upon request, including data, annotation, and first benchmark models.</abstract>
      <url hash="c019e15a">2021.nllp-1.4</url>
      <bibkey>wenger-etal-2021-automated</bibkey>
      <doi>10.18653/v1/2021.nllp-1.4</doi>
    </paper>
    <paper id="5">
      <title>A Multilingual Approach to Identify and Classify Exceptional Measures against <fixed-case>COVID</fixed-case>-19</title>
      <author><first>Georgios</first><last>Tziafas</last></author>
      <author><first>Eugenie</first><last>de Saint-Phalle</last></author>
      <author><first>Wietse</first><last>de Vries</last></author>
      <author><first>Clara</first><last>Egger</last></author>
      <author><first>Tommaso</first><last>Caselli</last></author>
      <pages>46–62</pages>
      <abstract>The COVID-19 pandemic has witnessed the implementations of exceptional measures by governments across the world to counteract its impact. This work presents the initial results of an on-going project, EXCEPTIUS, aiming to automatically identify, classify and com- pare exceptional measures against COVID-19 across 32 countries in Europe. To this goal, we created a corpus of legal documents with sentence-level annotations of eight different classes of exceptional measures that are im- plemented across these countries. We evalu- ated multiple multi-label classifiers on a manu- ally annotated corpus at sentence level. The XLM-RoBERTa model achieves highest per- formance on this multilingual multi-label clas- sification task, with a macro-average F1 score of 59.8%.</abstract>
      <url hash="ad6b41f8">2021.nllp-1.5</url>
      <bibkey>tziafas-etal-2021-multilingual</bibkey>
      <doi>10.18653/v1/2021.nllp-1.5</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="6">
      <title>Multi-granular Legal Topic Classification on <fixed-case>G</fixed-case>reek Legislation</title>
      <author><first>Christos</first><last>Papaloukas</last></author>
      <author><first>Ilias</first><last>Chalkidis</last></author>
      <author><first>Konstantinos</first><last>Athinaios</last></author>
      <author><first>Despina</first><last>Pantazi</last></author>
      <author><first>Manolis</first><last>Koubarakis</last></author>
      <pages>63–75</pages>
      <abstract>In this work, we study the task of classifying legal texts written in the Greek language. We introduce and make publicly available a novel dataset based on Greek legislation, consisting of more than 47 thousand official, categorized Greek legislation resources. We experiment with this dataset and evaluate a battery of advanced methods and classifiers, ranging from traditional machine learning and RNN-based methods to state-of-the-art Transformer-based methods. We show that recurrent architectures with domain-specific word embeddings offer improved overall performance while being competitive even to transformer-based models. Finally, we show that cutting-edge multilingual and monolingual transformer-based models brawl on the top of the classifiers’ ranking, making us question the necessity of training monolingual transfer learning models as a rule of thumb. To the best of our knowledge, this is the first time the task of Greek legal text classification is considered in an open research project, while also Greek is a language with very limited NLP resources in general.</abstract>
      <url hash="37bdb43b">2021.nllp-1.6</url>
      <bibkey>papaloukas-etal-2021-multi</bibkey>
      <doi>10.18653/v1/2021.nllp-1.6</doi>
      <pwccode url="https://github.com/christospi/glc-nllp-21" additional="false">christospi/glc-nllp-21</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/eurlex57k">EURLEX57K</pwcdataset>
    </paper>
    <paper id="7">
      <title>Machine Extraction of Tax Laws from Legislative Texts</title>
      <author><first>Elliott</first><last>Ash</last></author>
      <author><first>Malka</first><last>Guillot</last></author>
      <author><first>Luyang</first><last>Han</last></author>
      <pages>76–85</pages>
      <abstract>Using a corpus of compiled codes from U.S. states containing labeled tax law sections, we train text classifiers to automatically tag tax-law documents and, further, to identify the associated revenue source (e.g. income, property, or sales). After evaluating classifier performance in held-out test data, we apply them to an historical corpus of U.S. state legislation to extract the flow of relevant laws over the years 1910 through 2010. We document that the classifiers are effective in the historical corpus, for example by automatically detecting establishments of state personal income taxes. The trained models with replication code are published at https://github.com/luyang521/tax-classification.</abstract>
      <url hash="84eb99d4">2021.nllp-1.7</url>
      <attachment type="Software" hash="fbb842fe">2021.nllp-1.7.Software.zip</attachment>
      <bibkey>ash-etal-2021-machine</bibkey>
      <doi>10.18653/v1/2021.nllp-1.7</doi>
      <pwccode url="https://github.com/luyang521/tax-classification" additional="false">luyang521/tax-classification</pwccode>
    </paper>
    <paper id="8">
      <title>jur<fixed-case>BERT</fixed-case>: A <fixed-case>R</fixed-case>omanian <fixed-case>BERT</fixed-case> Model for Legal Judgement Prediction</title>
      <author><first>Mihai</first><last>Masala</last></author>
      <author><first>Radu Cristian Alexandru</first><last>Iacob</last></author>
      <author><first>Ana Sabina</first><last>Uban</last></author>
      <author><first>Marina</first><last>Cidota</last></author>
      <author><first>Horia</first><last>Velicu</last></author>
      <author><first>Traian</first><last>Rebedea</last></author>
      <author><first>Marius</first><last>Popescu</last></author>
      <pages>86–94</pages>
      <abstract>Transformer-based models have become the de facto standard in the field of Natural Language Processing (NLP). By leveraging large unlabeled text corpora, they enable efficient transfer learning leading to state-of-the-art results on numerous NLP tasks. Nevertheless, for low resource languages and highly specialized tasks, transformer models tend to lag behind more classical approaches (e.g. SVM, LSTM) due to the lack of aforementioned corpora. In this paper we focus on the legal domain and we introduce a Romanian BERT model pre-trained on a large specialized corpus. Our model outperforms several strong baselines for legal judgement prediction on two different corpora consisting of cases from trials involving banks in Romania.</abstract>
      <url hash="5d96be6c">2021.nllp-1.8</url>
      <bibkey>masala-etal-2021-jurbert</bibkey>
      <doi>10.18653/v1/2021.nllp-1.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>J</fixed-case>uri<fixed-case>BERT</fixed-case>: A Masked-Language Model Adaptation for <fixed-case>F</fixed-case>rench Legal Text</title>
      <author><first>Stella</first><last>Douka</last></author>
      <author><first>Hadi</first><last>Abdine</last></author>
      <author><first>Michalis</first><last>Vazirgiannis</last></author>
      <author><first>Rajaa</first><last>El Hamdani</last></author>
      <author><first>David</first><last>Restrepo Amariles</last></author>
      <pages>95–101</pages>
      <abstract>Language models have proven to be very useful when adapted to specific domains. Nonetheless, little research has been done on the adaptation of domain-specific BERT models in the French language. In this paper, we focus on creating a language model adapted to French legal text with the goal of helping law professionals. We conclude that some specific tasks do not benefit from generic language models pre-trained on large amounts of data. We explore the use of smaller architectures in domain-specific sub-languages and their benefits for French legal text. We prove that domain-specific pre-trained models can perform better than their equivalent generalised ones in the legal domain. Finally, we release JuriBERT, a new set of BERT models adapted to the French legal domain.</abstract>
      <url hash="9b9303ae">2021.nllp-1.9</url>
      <bibkey>douka-etal-2021-juribert</bibkey>
      <doi>10.18653/v1/2021.nllp-1.9</doi>
    </paper>
    <paper id="10">
      <title>Few-shot and Zero-shot Approaches to Legal Text Classification: A Case Study in the Financial Sector</title>
      <author><first>Rajdeep</first><last>Sarkar</last></author>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <author><first>Jay</first><last>Megaro</last></author>
      <author><first>John</first><last>Mariano</last></author>
      <author><first>Vall</first><last>Herard</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <pages>102–106</pages>
      <abstract>The application of predictive coding techniques to legal texts has the potential to greatly reduce the cost of legal review of documents, however, there is such a wide array of legal tasks and continuously evolving legislation that it is hard to construct sufficient training data to cover all cases. In this paper, we investigate few-shot and zero-shot approaches that require substantially less training data and introduce a triplet architecture, which for promissory statements produces performance close to that of a supervised system. This method allows predictive coding methods to be rapidly developed for new regulations and markets.</abstract>
      <url hash="994c7657">2021.nllp-1.10</url>
      <bibkey>sarkar-etal-2021-shot</bibkey>
      <doi>10.18653/v1/2021.nllp-1.10</doi>
    </paper>
    <paper id="11">
      <title>A Free Format Legal Question Answering System</title>
      <author><first>Soha</first><last>Khazaeli</last></author>
      <author><first>Janardhana</first><last>Punuru</last></author>
      <author><first>Chad</first><last>Morris</last></author>
      <author><first>Sanjay</first><last>Sharma</last></author>
      <author><first>Bert</first><last>Staub</last></author>
      <author><first>Michael</first><last>Cole</last></author>
      <author><first>Sunny</first><last>Chiu-Webster</last></author>
      <author><first>Dhruv</first><last>Sakalley</last></author>
      <pages>107–113</pages>
      <abstract>We present an information retrieval-based question answer system to answer legal questions. The system is not limited to a predefined set of questions or patterns and uses both sparse vector search and embeddings for input to a BERT-based answer re-ranking system. A combination of general domain and legal domain data is used for training. This natural question answering system is in production and is used commercially.</abstract>
      <url hash="3a813ff5">2021.nllp-1.11</url>
      <bibkey>khazaeli-etal-2021-free</bibkey>
      <doi>10.18653/v1/2021.nllp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Searching for Legal Documents at Paragraph Level: Automating Label Generation and Use of an Extended Attention Mask for Boosting Neural Models of Semantic Similarity</title>
      <author><first>Li</first><last>Tang</last></author>
      <author><first>Simon</first><last>Clematide</last></author>
      <pages>114–122</pages>
      <abstract>Searching for legal documents is a specialized Information Retrieval task that is relevant for expert users (lawyers and their assistants) and for non-expert users. By searching previous court decisions (cases), a user can better prepare the legal reasoning of a new case. Being able to search using a natural language text snippet instead of a more artificial query could help to prevent query formulation issues. Also, if semantic similarity could be modeled beyond exact lexical matches, more relevant results can be found even if the query terms don’t match exactly. For this domain, we formulated a task to compare different ways of modeling semantic similarity at paragraph level, using neural and non-neural systems. We compared systems that encode the query and the search collection paragraphs as vectors, enabling the use of cosine similarity for results ranking. After building a German dataset for cases and statutes from Switzerland, and extracting citations from cases to statutes, we developed an algorithm for estimating semantic similarity at paragraph level, using a link-based similarity method. When evaluating different systems in this way, we find that semantic similarity modeling by neural systems can be boosted with an extended attention mask that quenches noise in the inputs.</abstract>
      <url hash="eaaba371">2021.nllp-1.12</url>
      <bibkey>tang-clematide-2021-searching</bibkey>
      <doi>10.18653/v1/2021.nllp-1.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>G</fixed-case>er<fixed-case>D</fixed-case>a<fixed-case>LIR</fixed-case>: A <fixed-case>G</fixed-case>erman Dataset for Legal Information Retrieval</title>
      <author><first>Marco</first><last>Wrzalik</last></author>
      <author><first>Dirk</first><last>Krechel</last></author>
      <pages>123–128</pages>
      <abstract>We present GerDaLIR, a German Dataset for Legal Information Retrieval based on case documents from the open legal information platform Open Legal Data. The dataset consists of 123K queries, each labelled with at least one relevant document in a collection of 131K case documents. We conduct several baseline experiments including BM25 and a state-of-the-art neural re-ranker. With our dataset, we aim to provide a standardized benchmark for German LIR and promote open research in this area. Beyond that, our dataset comprises sufficient training data to be used as a downstream task for German or multilingual language models.</abstract>
      <url hash="224f30de">2021.nllp-1.13</url>
      <bibkey>wrzalik-krechel-2021-gerdalir</bibkey>
      <doi>10.18653/v1/2021.nllp-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>SP</fixed-case>a<fixed-case>R</fixed-case>.txt, a Cheap Shallow Parsing Approach for Regulatory Texts</title>
      <author><first>Ruben</first><last>Kruiper</last></author>
      <author><first>Ioannis</first><last>Konstas</last></author>
      <author><first>Alasdair J.G.</first><last>Gray</last></author>
      <author><first>Farhad</first><last>Sadeghineko</last></author>
      <author><first>Richard</first><last>Watson</last></author>
      <author><first>Bimal</first><last>Kumar</last></author>
      <pages>129–143</pages>
      <abstract>Automated Compliance Checking (ACC) systems aim to semantically parse building regulations to a set of rules. However, semantic parsing is known to be hard and requires large amounts of training data. The complexity of creating such training data has led to research that focuses on small sub-tasks, such as shallow parsing or the extraction of a limited subset of rules. This study introduces a shallow parsing task for which training data is relatively cheap to create, with the aim of learning a lexicon for ACC. We annotate a small domain-specific dataset of 200 sentences, SPaR.txt, and train a sequence tagger that achieves 79,93 F1-score on the test set. We then show through manual evaluation that the model identifies most (89,84%) defined terms in a set of building regulation documents, and that both contiguous and discontiguous Multi-Word Expressions (MWE) are discovered with reasonable accuracy (70,3%).</abstract>
      <url hash="e1b3d09e">2021.nllp-1.14</url>
      <bibkey>kruiper-etal-2021-spar</bibkey>
      <doi>10.18653/v1/2021.nllp-1.14</doi>
      <pwccode url="https://github.com/rubenkruiper/spar.txt" additional="false">rubenkruiper/spar.txt</pwccode>
    </paper>
    <paper id="15">
      <title>Capturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser</title>
      <author><first>Yuta</first><last>Koreeda</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>144–154</pages>
      <abstract>While many NLP pipelines assume raw, clean texts, many texts we encounter in the wild, including a vast majority of legal documents, are not so clean, with many of them being visually structured documents (VSDs) such as PDFs. Conventional preprocessing tools for VSDs mainly focused on word segmentation and coarse layout analysis, whereas fine-grained logical structure analysis (such as identifying paragraph boundaries and their hierarchies) of VSDs is underexplored. To that end, we proposed to formulate the task as prediction of “transition labels” between text fragments that maps the fragments to a tree, and developed a feature-based machine learning system that fuses visual, textual and semantic cues. Our system is easily customizable to different types of VSDs and it significantly outperformed baselines in identifying different structures in VSDs. For example, our system obtained a paragraph boundary detection F1 score of 0.953 which is significantly better than a popular PDF-to-text tool with an F1 score of 0.739.</abstract>
      <url hash="e8035863">2021.nllp-1.15</url>
      <bibkey>koreeda-manning-2021-capturing</bibkey>
      <doi>10.18653/v1/2021.nllp-1.15</doi>
      <pwccode url="https://github.com/stanfordnlp/pdf-struct" additional="false">stanfordnlp/pdf-struct</pwccode>
    </paper>
    <paper id="16">
      <title>Legal Terminology Extraction with the Termolator</title>
      <author><first>Nhi</first><last>Pham</last></author>
      <author><first>Lachlan</first><last>Pham</last></author>
      <author><first>Adam L.</first><last>Meyers</last></author>
      <pages>155–162</pages>
      <abstract>Domain-specific terminology is ubiquitous in legal documents. Despite potential utility in populating glossaries and ontologies or as arguments in information extraction and document classification tasks, there has been limited work done for legal terminology extraction. This paper describes some work to remedy this omission. In the described research, we make some modifications to the Termolator, a high-performing, open-source terminology extractor which has been tuned to scientific articles. Our changes are designed to improve the Termolator’s results when applied to United States Supreme Court decisions. Unaltered and using the recommended settings, the original Termolator provides a list of terminology with a precision of 23% and 25% for the categories of economic activity (development set) and criminal procedures (test set) respectively. These were the most frequently occurring broad issues in Washington University in St. Louis Database corpus, a database of Supreme Court decisions that have been manually classified by topic. Our contribution includes the introduction of several legal domain-specific filtration steps and changes to the web search relevance score; each incrementally improved precision culminating in a combined precision of 63% and 65%. We also evaluated the baseline version of the Termolator on more specific subcategories and on broad issues with fewer cases. Our results show that a narrowed scope as well as smaller document numbers significantly lower the precision. In both cases, the modifications to the Termolator improve precision.</abstract>
      <url hash="34970b99">2021.nllp-1.16</url>
      <bibkey>pham-etal-2021-legal</bibkey>
      <doi>10.18653/v1/2021.nllp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Supervised Identification of Participant Slots in Contracts</title>
      <author><first>Dan</first><last>Simonson</last></author>
      <pages>163–171</pages>
      <abstract>This paper presents a technique for the identification of participant slots in English language contracts. Taking inspiration from unsupervised slot extraction techniques, the system presented here uses a supervised approach to identify terms used to refer to a genre-specific slot in novel contracts. We evaluate the system in multiple feature configurations to demonstrate that the best performing system in both genres of contracts omits the exact mention form from consideration—even though such mention forms are often the name of the slot under consideration—and is instead based solely on the dependency label and parent; in other words, a more reliable quantification of a party’s role in a contract is found in what they do rather than what they are named.</abstract>
      <url hash="1652370c">2021.nllp-1.17</url>
      <bibkey>simonson-2021-supervised</bibkey>
      <doi>10.18653/v1/2021.nllp-1.17</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="18">
      <title>Named Entity Recognition in Historic Legal Text: A Transformer and State Machine Ensemble Method</title>
      <author><first>Fernando</first><last>Trias</last></author>
      <author><first>Hongming</first><last>Wang</last></author>
      <author><first>Sylvain</first><last>Jaume</last></author>
      <author><first>Stratos</first><last>Idreos</last></author>
      <pages>172–179</pages>
      <abstract>Older legal texts are often scanned and digitized via Optical Character Recognition (OCR), which results in numerous errors. Although spelling and grammar checkers can correct much of the scanned text automatically, Named Entity Recognition (NER) is challenging, making correction of names difficult. To solve this, we developed an ensemble language model using a transformer neural network architecture combined with a finite state machine to extract names from English-language legal text. We use the US-based English language Harvard Caselaw Access Project for training and testing. Then, the extracted names are subjected to heuristic textual analysis to identify errors, make corrections, and quantify the extent of problems. With this system, we are able to extract most names, automatically correct numerous errors and identify potential mistakes that can later be reviewed for manual correction.</abstract>
      <url hash="24d1967c">2021.nllp-1.18</url>
      <bibkey>trias-etal-2021-named</bibkey>
      <doi>10.18653/v1/2021.nllp-1.18</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="19">
      <title>Summarization of <fixed-case>G</fixed-case>erman Court Rulings</title>
      <author><first>Ingo</first><last>Glaser</last></author>
      <author><first>Sebastian</first><last>Moser</last></author>
      <author><first>Florian</first><last>Matthes</last></author>
      <pages>180–189</pages>
      <abstract>Historically speaking, the German legal language is widely neglected in NLP research, especially in summarization systems, as most of them are based on English newspaper articles. In this paper, we propose the task of automatic summarization of German court rulings. Due to their complexity and length, it is of critical importance that legal practitioners can quickly identify the content of a verdict and thus be able to decide on the relevance for a given legal case. To tackle this problem, we introduce a new dataset consisting of 100k German judgments with short summaries. Our dataset has the highest compression ratio among the most common summarization datasets. German court rulings contain much structural information, so we create a pre-processing pipeline tailored explicitly to the German legal domain. Additionally, we implement multiple extractive as well as abstractive summarization systems and build a wide variety of baseline models. Our best model achieves a ROUGE-1 score of 30.50. Therefore with this work, we are laying the crucial groundwork for further research on German summarization systems.</abstract>
      <url hash="d1cd0d0e">2021.nllp-1.19</url>
      <bibkey>glaser-etal-2021-summarization</bibkey>
      <doi>10.18653/v1/2021.nllp-1.19</doi>
      <pwccode url="https://github.com/sebimo/legalsum" additional="false">sebimo/legalsum</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bigpatent">BigPatent</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="20">
      <title>Learning from Limited Labels for Long Legal Dialogue</title>
      <author><first>Jenny</first><last>Hong</last></author>
      <author><first>Derek</first><last>Chong</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>190–204</pages>
      <abstract>We study attempting to achieve high accuracy information extraction of case factors from a challenging dataset of parole hearings, which, compared to other legal NLP datasets, has longer texts, with fewer labels. On this corpus, existing work directly applying pretrained neural models has failed to extract all but a few relatively basic items with little improvement over rule-based extraction. We address two challenges posed by existing work: training on long documents and reasoning over complex speech patterns. We use a similar approach to the two-step open-domain question answering approach by using a Reducer to extract relevant text segments and a Producer to generate both extractive answers and non-extractive classifications. In a context like ours, with limited labeled data, we show that a superior approach for strong performance within limited development time is to use a combination of a rule-based Reducer and a neural Producer. We study four representative tasks from the parole dataset. On all four, we improve extraction from the previous benchmark of 0.41–0.63 to 0.83–0.89 F1.</abstract>
      <url hash="980f16c3">2021.nllp-1.20</url>
      <bibkey>hong-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.nllp-1.20</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="21">
      <title>Automating Claim Construction in Patent Applications: The <fixed-case>CMU</fixed-case>mine Dataset</title>
      <author><first>Ozan</first><last>Tonguz</last></author>
      <author><first>Yiwei</first><last>Qin</last></author>
      <author><first>Yimeng</first><last>Gu</last></author>
      <author><first>Hyun Hannah</first><last>Moon</last></author>
      <pages>205–209</pages>
      <abstract>Intellectual Property (IP) in the form of issued patents is a critical and very desirable element of innovation in high-tech. In this position paper, we explore the possibility of automating the legal task of Claim Construction in patent applications via Natural Language Processing (NLP) and Machine Learning (ML). To this end, we first create a large dataset known as CMUmine™and then demonstrate that, using NLP and ML techniques the Claim Construction in patent applications, a crucial legal task currently performed by IP attorneys, can be automated. To the best of our knowledge, this is the first public patent application dataset. Our results look very promising in automating the patent application process.</abstract>
      <url hash="1529b8c2">2021.nllp-1.21</url>
      <bibkey>tonguz-etal-2021-automating</bibkey>
      <doi>10.18653/v1/2021.nllp-1.21</doi>
    </paper>
    <paper id="22">
      <title>Effectively Leveraging <fixed-case>BERT</fixed-case> for Legal Document Classification</title>
      <author><first>Nut</first><last>Limsopatham</last></author>
      <pages>210–216</pages>
      <abstract>Bidirectional Encoder Representations from Transformers (BERT) has achieved state-of-the-art performances on several text classification tasks, such as GLUE and sentiment analysis. Recent work in the legal domain started to use BERT on tasks, such as legal judgement prediction and violation prediction. A common practise in using BERT is to fine-tune a pre-trained model on a target task and truncate the input texts to the size of the BERT input (e.g. at most 512 tokens). However, due to the unique characteristics of legal documents, it is not clear how to effectively adapt BERT in the legal domain. In this work, we investigate how to deal with long documents, and how is the importance of pre-training on documents from the same domain as the target task. We conduct experiments on the two recent datasets: ECHR Violation Dataset and the Overruling Task Dataset, which are multi-label and binary classification tasks, respectively. Importantly, on average the number of tokens in a document from the ECHR Violation Dataset is more than 1,600. While the documents in the Overruling Task Dataset are shorter (the maximum number of tokens is 204). We thoroughly compare several techniques for adapting BERT on long documents and compare different models pre-trained on the legal and other domains. Our experimental results show that we need to explicitly adapt BERT to handle long documents, as the truncation leads to less effective performance. We also found that pre-training on the documents that are similar to the target task would result in more effective performance on several scenario.</abstract>
      <url hash="7b5a2276">2021.nllp-1.22</url>
      <bibkey>limsopatham-2021-effectively</bibkey>
      <doi>10.18653/v1/2021.nllp-1.22</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/echr">ECHR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/overruling">Overruling</pwcdataset>
    </paper>
    <paper id="23">
      <title>Semi-automatic Triage of Requests for Free Legal Assistance</title>
      <author><first>Meladel</first><last>Mistica</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Brayden</first><last>Merrifield</last></author>
      <author><first>Kate</first><last>Fazio</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>217–227</pages>
      <abstract>Free legal assistance is critically under-resourced, and many of those who seek legal help have their needs unmet. A major bottleneck in the provision of free legal assistance to those most in need is the determination of the precise nature of the legal problem. This paper describes a collaboration with a major provider of free legal assistance, and the deployment of natural language processing models to assign area-of-law categories to real-world requests for legal assistance. In particular, we focus on an investigation of models to generate efficiencies in the triage process, but also the risks associated with naive use of model predictions, including fairness across different user demographics.</abstract>
      <url hash="80f93c57">2021.nllp-1.23</url>
      <bibkey>mistica-etal-2021-semi</bibkey>
      <doi>10.18653/v1/2021.nllp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Automatic Resolution of Domain Name Disputes</title>
      <author><first>Wayan Oger</first><last>Vihikan</last></author>
      <author><first>Meladel</first><last>Mistica</last></author>
      <author><first>Inbar</first><last>Levy</last></author>
      <author><first>Andrew</first><last>Christie</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>228–238</pages>
      <abstract>We introduce the new task of domain name dispute resolution (DNDR), that predicts the outcome of a process for resolving disputes about legal entitlement to a domain name. TheICANN UDRP establishes a mandatory arbitration process for a dispute between a trade-mark owner and a domain name registrant pertaining to a generic Top-Level Domain (gTLD) name (one ending in .COM, .ORG, .NET, etc). The nature of the problem leads to a very skewed data set, which stems from being able to register a domain name with extreme ease, very little expense, and no need to prove an entitlement to it. In this paper, we describe thetask and associated data set. We also present benchmarking results based on a range of mod-els, which show that simple baselines are in general difficult to beat due to the skewed data distribution, but in the specific case of the respondent having submitted a response, a fine-tuned BERT model offers considerable improvements over a majority-class model</abstract>
      <url hash="212b58e8">2021.nllp-1.24</url>
      <bibkey>vihikan-etal-2021-automatic</bibkey>
      <doi>10.18653/v1/2021.nllp-1.24</doi>
      <pwccode url="https://github.com/vihikan/automatic-resolution-of-domain-name-disputes" additional="false">vihikan/automatic-resolution-of-domain-name-disputes</pwccode>
    </paper>
  </volume>
</collection>
