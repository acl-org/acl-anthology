<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.teicai">
  <volume id="1" ingest-date="2024-03-04" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Worskhop on Towards Ethical and Inclusive Conversational AI: Language Attitudes, Linguistic Diversity, and Language Rights (TEICAI 2024)</booktitle>
      <editor><first>Nina</first><last>Hosseini-Kivanani</last></editor>
      <editor><first>Sviatlana</first><last>Höhn</last></editor>
      <editor><first>Dimitra</first><last>Anastasiou</last></editor>
      <editor><first>Bettina</first><last>Migge</last></editor>
      <editor><first>Angela</first><last>Soltan</last></editor>
      <editor><first>Doris</first><last>Dippold</last></editor>
      <editor><first>Ekaterina</first><last>Kamlovskaya</last></editor>
      <editor><first>Fred</first><last>Philippy</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St Julians, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="1c507d0a">2024.teicai-1</url>
      <venue>teicai</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="e9f78837">2024.teicai-1.0</url>
      <bibkey>teicai-ws-2024-worskhop</bibkey>
    </frontmatter>
    <paper id="1">
      <title>How Do Conversational Agents in Healthcare Impact on Patient Agency?</title>
      <author><first>Kerstin</first><last>Denecke</last></author>
      <pages>1-8</pages>
      <abstract>In healthcare, agency refers to the ability of patients to actively participate in and control their health through collaborating with providers, informed decision-making and understanding health information. Conversational agents (CAs) are increasingly used for realizing digital health interventions, but it is still unclear how they are enhancing patient agency. This paper explores which technological components are required to enable CAs impacting on patient agency, and identifies metrics for measuring and evaluating this impact. We do this by drawing on existing work related to developing and evaluating healthcare CAs and through analysis of a concrete example of a CA. As a result, we identify five main areas where CAs enhance patient agency, namely by: improved access to health information, personalized advice, increased engagement, emotional support and reduced barriers to care. For each of these areas, specific technological functions have to be integrated into CAs such as sentiment and emotion analysis methods that allow a CA to support emotionally.</abstract>
      <url hash="f095bf64">2024.teicai-1.1</url>
      <bibkey>denecke-2024-conversational</bibkey>
    </paper>
    <paper id="2">
      <title>Why academia should cut back general enthusiasm about <fixed-case>CA</fixed-case>s</title>
      <author><first>Alessia</first><last>Giulimondi</last></author>
      <pages>9-15</pages>
      <abstract>This position paper will analyze LLMs, the core technology of CAs, from a socio-technical and linguistic perspective in order to argue for a limitation of its use in academia, which should be reflected in a more cautious adoption of CAs in private spaces. The article describes how machine learning technologies like LLMs are inserted into a more general process of platformization (van Dijck, 2021), negatively affecting autonomy of research (Kersessens and van Dijck, 2022). Moreover, fine-tuning practices, as means to polish language models (Kasirzadeh and Gabriel, 2023) are questioned, explaining how these foster a deterministic approach to language. A leading role of universities in this general gain of awareness is strongly advocated, as institutions that support transparent and open science, in order to foster and protect democratic values in our societies.</abstract>
      <url hash="942d0043">2024.teicai-1.2</url>
      <bibkey>giulimondi-2024-academia</bibkey>
    </paper>
    <paper id="3">
      <title>Bridging the Language Gap: Integrating Language Variations into Conversational <fixed-case>AI</fixed-case> Agents for Enhanced User Engagement</title>
      <author><first>Marcellus</first><last>Amadeus</last></author>
      <author><first>Jose Roberto</first><last>Homeli da Silva</last></author>
      <author><first>Joao Victor</first><last>Pessoa Rocha</last></author>
      <pages>16-20</pages>
      <abstract>This paper presents the initial steps taken to integrate language variations into conversational AI agents to enhance user engagement. The study is built upon sociolinguistic and pragmatic traditions and involves the creation of an annotation taxonomy. The taxonomy includes eleven classes, ranging from concrete to abstract, and the covered aspects are the instance itself, time, sentiment, register, state, region, type, grammar, part of speech, meaning, and language. The paper discusses the challenges of incorporating vernacular language into AI agents, the procedures for data collection, and the taxonomy organization. It also outlines the next steps, including the database expansion and the computational implementation. The authors believe that integrating language variation into conversational AI will build near-real language inventories and boost user engagement. The paper concludes by discussing the limitations and the importance of building rapport with users through their own vernacular.</abstract>
      <url hash="617a3ade">2024.teicai-1.3</url>
      <bibkey>amadeus-etal-2024-bridging</bibkey>
    </paper>
    <paper id="4">
      <title>Socio-cultural adapted chatbots: Harnessing Knowledge Graphs and Large Language Models for enhanced context awarenes</title>
      <author><first>Jader</first><last>Camboim de Sá</last></author>
      <author><first>Dimitra</first><last>Anastasiou</last></author>
      <author><first>Marcos</first><last>Da Silveira</last></author>
      <author><first>Cédric</first><last>Pruski</last></author>
      <pages>21-27</pages>
      <abstract>Understanding the socio-cultural context is crucial in machine translation (MT). Although conversational AI systems and chatbots, in particular, are not designed for translation, they can be used for MT purposes. Yet, chatbots often struggle to identify any socio-cultural context during user interactions. In this paper, we highlight this challenge with real-world examples from popular chatbots. We advocate for the use of knowledge graphs as an external source of information that can potentially encapsulate socio-cultural contexts, aiding chatbots in enhancing translation. We further present a method to exploit external knowledge and extract contextual information that can significantly improve text translation, as evidenced by our interactions with these chatbots.</abstract>
      <url hash="41d28881">2024.teicai-1.4</url>
      <bibkey>camboim-de-sa-etal-2024-socio</bibkey>
    </paper>
    <paper id="5">
      <title>How should Conversational Agent systems respond to sexual harassment?</title>
      <author><first>Laura</first><last>De Grazia</last></author>
      <author><first>Alex</first><last>Peiró Lilja</last></author>
      <author><first>Mireia</first><last>Farrús Cabeceran</last></author>
      <author><first>Mariona</first><last>Taulé</last></author>
      <pages>28-35</pages>
      <abstract>This paper investigates the appropriate responses that Conversational Agent systems (CAs) should employ when subjected to sexual harassment by users. Previous studies indicate that conventional CAs often respond neutrally or evade such requests. Enhancing the responsiveness of CAs to offensive speech is crucial, as users might carry over these interactions into their social interactions. To address this issue, we selected evaluators to compare a series of responses to sexual harassment from four commercial CAs (Amazon Alexa, Apple Siri, Google Home, and Microsoft Cortana) with alternative responses we realized based on insights from psychological and sociological studies. Focusing on CAs with a female voice, given their increased likelihood of encountering offensive language, we conducted two experiments involving 22 evaluators (11 females and 11 males). In the initial experiment, participants assessed the responses in a textual format, while the second experiment involved the evaluation of responses generated with a synthetic voice exhibiting three different intonations (angry, neutral, and assertive). Results from the first experiment revealed a general preference for the responses we formulated. For the most voted replies, female evaluators exhibited a tendency towards responses with an assertive intent, emphasizing the sexually harassing nature of the request. Conversely, male evaluators leaned towards a more neutral response, aligning with prior findings that highlight gender-based differences in the perception of sexual harassment. The second experiment underscored a preference for assertive responses. The study’s outcomes highlight the need to develop new, educational responses from CAs to instances of sexual harassment, aiming to discourage harmful behavior.</abstract>
      <url hash="1a8bfaef">2024.teicai-1.5</url>
      <bibkey>de-grazia-etal-2024-conversational</bibkey>
    </paper>
    <paper id="6">
      <title>Non-Referential Functions of Language in Social Agents: The Case of Social Proximity</title>
      <author><first>Sviatlana</first><last>Höhn</last></author>
      <pages>36-41</pages>
      <abstract>Non-referential functions of language such as setting group boundaries, identity construction and regulation of social proximity have rarely found place in the language technology creation process. Nevertheless, their importance has been postulated in literature. While multiple methods to include social information in large language models (LLM) cover group properties (gender, age, geographic relations, professional characteristics), a combination of group social characteristics and individual features of an agent (natural or artificial) play a role in social interaction but have not been studied in generated language. This article explores the orchestration of prompt engineering and retrieval-augmented generation techniques to linguistic features of social proximity and distance in language generated by an LLM. The study uses the immediacy/distance model from literature to analyse language generated by an LLM for different recipients. This research reveals that kinship terms are almost the only way of displaying immediacy in LLM-made conversations.</abstract>
      <url hash="64e92a7e">2024.teicai-1.6</url>
      <bibkey>hohn-2024-non</bibkey>
    </paper>
    <paper id="7">
      <title>Making a Long Story Short in Conversation Modeling</title>
      <author><first>Yufei</first><last>Tao</last></author>
      <author><first>Tiernan</first><last>Mines</last></author>
      <author><first>Ameeta</first><last>Agrawal</last></author>
      <pages>42-49</pages>
      <abstract>Conversation systems accommodate diverse users with unique personalities and distinct writing styles. Within the domain of multi-turn dialogue modeling, this work studies the impact of varied utterance lengths on the quality of subsequent responses generated by conversation models. Using GPT-3 as the base model, multiple dialogue datasets, and several metrics, we conduct a thorough exploration of this aspect of conversational models. Our analysis sheds light on the complex relationship between utterance lengths and the quality of follow-up responses generated by dialogue systems. Empirical findings suggests that, for certain types of conversations, utterance lengths can be reduced by up to 72% without any noticeable difference in the quality of follow-up responses.</abstract>
      <url hash="c4f69374">2024.teicai-1.7</url>
      <bibkey>tao-etal-2024-making</bibkey>
    </paper>
  </volume>
</collection>
