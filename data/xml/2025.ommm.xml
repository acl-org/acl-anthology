<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.ommm">
  <volume id="1" ingest-date="2026-01-07" type="proceedings">
    <meta>
      <booktitle>Proceedings of Interdisciplinary Workshop on Observations of Misunderstood, Misguided and Malicious Use of Language Models</booktitle>
      <editor><first>Piotr</first><last>Przybyła</last></editor>
      <editor><first>Matthew</first><last>Shardlow</last></editor>
      <editor><first>Clara</first><last>Colombatto</last></editor>
      <editor><first>Nanna</first><last>Inie</last></editor>
      <publisher>INCOMA Ltd., Shoumen, Bulgaria</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2025</year>
      <url hash="5bb9441d">2025.ommm-1</url>
      <venue>ommm</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="cbac2286">2025.ommm-1.0</url>
      <bibkey>ommm-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Bias in, Bias out: Annotation Bias in Multilingual Large Language Models</title>
      <author><first>Xia</first><last>Cui</last></author>
      <author><first>Ziyi</first><last>Huang</last></author>
      <author><first>Naeemeh</first><last>Adel</last></author>
      <pages>1–16</pages>
      <abstract>Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.</abstract>
      <url hash="fc3e90f8">2025.ommm-1.1</url>
      <bibkey>cui-etal-2025-bias</bibkey>
    </paper>
    <paper id="2">
      <title>Freeze and Reveal: Exposing Modality Bias in Vision-Language Models</title>
      <author><first>Vivek Hruday</first><last>Kavuri</last></author>
      <author><first>Vysishtya Karanam</first><last>Karanam</last></author>
      <author><first>Venkamsetty Venkata</first><last>Jahnavi</last></author>
      <author><first>Kriti</first><last>Madumadukala</last></author>
      <author><first>Balaji Lakshmipathi</first><last>Darur</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <pages>17–26</pages>
      <abstract>Vision-Language Models (VLMs) achieve impressive multimodal performance but often inherit gender biases from their training data. This bias might be coming from both the vision and text modalities. In this work, we dissect the contributions of vision and text backbones to these biases by applying targeted debiasing—Counterfactual Data Augmentation (CDA) and Task Vector methods. Inspired by data-efficient approaches in hate speech classification, we introduce a novel metric, Degree of Stereotypicality (DoS), and a corresponding debiasing method, Data Augmentation Using DoS (DAUDoS), to reduce bias with minimal computational cost. We curate a gender-annotated dataset and evaluate all methods on the VisoGender benchmark to quantify improvements and identify the dominant source of bias. Our results show that CDA reduces the gender gap by 6% and DAUDoS by 3% but using only one‐third the data. Both methods also improve the model’s ability to correctly identify gender in images by 3%, with DAUDoS achieving this improvement using only almost one-third of training data. From our experiments, we observed that CLIP’s vision encoder is more biased whereas PaliGemma2’s text encoder is more biased. By identifying whether the bias stems more from the vision or text encoders, our work enables more targeted and effective bias mitigation strategies in future multi-modal systems.</abstract>
      <url hash="7440de20">2025.ommm-1.2</url>
      <bibkey>kavuri-etal-2025-freeze</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>A</fixed-case>nthro<fixed-case>S</fixed-case>et: a Challenge Dataset for Anthropomorphic Language Detection</title>
      <author><first>Dorielle</first><last>Lonke</last></author>
      <author><first>Jelke</first><last>Bloem</last></author>
      <author><first>Pia</first><last>Sommerauer</last></author>
      <pages>27–39</pages>
      <abstract>This paper addresses the challenge of detecting anthropomorphic language in AI research. We introduce AnthroSet, a novel dataset of 600 manually annotated utterances covering various linguistic structures. Through the evaluation of two current approaches for anthropomorphism and atypical animacy detection, we highlight the limitations of a masked language model approach, arising from masking constraints as well as increasingly anthropomorphizing AI-related terminology. Our findings underscore the need for more targeted methods and a robust definition of anthropomorphism.</abstract>
      <url hash="465ba118">2025.ommm-1.3</url>
      <bibkey>lonke-etal-2025-anthroset</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>FLARE</fixed-case>: An Error Analysis Framework for Diagnosing <fixed-case>LLM</fixed-case> Classification Failures</title>
      <author><first>Keerthana</first><last>Madhavan</last></author>
      <author><first>Luiza</first><last>Antonie</last></author>
      <author><first>Stacey</first><last>Scott</last></author>
      <pages>40–44</pages>
      <abstract>When Large Language Models return “Inconclusive” in classification tasks, practitioners are left without insight into what went wrong. This diagnostic gap can delay medical decisions, undermine content moderation, and mislead downstream systems. We present FLARE (Failure Location and Reasoning Evaluation), a framework that transforms opaque failures into seven actionable categories. Applied to 5,400 election-misinformation classifications, FLARE reveals a surprising result: Few-Shot prompting—widely considered a best practice—produced 38× more failures than Zero-Shot, with 70.8% due to simple parsing issues. By exposing hidden failure modes, FLARE addresses critical misunderstandings in LLM deployment with implications across domains.</abstract>
      <url hash="b4ff1288">2025.ommm-1.4</url>
      <bibkey>madhavan-etal-2025-flare</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>B</fixed-case>u<fixed-case>ST</fixed-case>: A <fixed-case>S</fixed-case>iamese Transformer Model for <fixed-case>AI</fixed-case> Text Detection in <fixed-case>B</fixed-case>ulgarian</title>
      <author><first>Andrii</first><last>Maslo</last></author>
      <author><first>Silvia</first><last>Gargova</last></author>
      <pages>45–52</pages>
      <abstract>We introduce BuST (Bulgarian Siamese Transformer), a novel method for detecting machine-generated Bulgarian text using paraphrase-based semantic similarity. Inspired by the RAIDAR approach, BuST employs a Siamese Transformer architecture to compare input texts with their LLM-generated paraphrases, identifying subtle linguistic patterns that indicate synthetic origin. In pilot experiments, BuST achieved 88.79% accuracy and an F1-score of 88.0%, performing competitively with strong baselines. While BERT reached higher raw scores, BuST offers a model-agnostic and adaptable framework for low-resource settings, demonstrating the promise of paraphrase-driven detection strategies.</abstract>
      <url hash="abbc2fb9">2025.ommm-1.5</url>
      <bibkey>maslo-gargova-2025-bust</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>F</fixed-case>*ck Around and Find Out: Quasi-Malicious Interactions with <fixed-case>LLM</fixed-case>s as a Site of Situated Learning</title>
      <author><first>Sarah</first><last>ONeill</last></author>
      <pages>53–58</pages>
      <abstract>This work-in-progress paper proposes a cross-disciplinary perspective on “malicious” interactions with large language models (LLMs), reframing it from only a threat to be mitigated, we ask whether certain adversarial interactions can also serve as productive learning encounters that demystify the opaque workings of AI systems to novice users. We ground this inquiry in an anecdotal observation of a student who deliberately sabotaged a machine-learning robot’s training process in order to understand its underlying logic. We outline this observation with a conceptual framework for learning with, through, and from the material quirks of LLMs grounded in Papert’s constructionism and Hasse’s ultra-social learning theory. Finally, we present the preliminary design of a research-through-workshop where non-experts will jailbreak various LLM chatbots, investigating this encounter as a situated learning process. We share this early-stage research as an invitation for feedback on reimagining inappropriate and harmful interactions with LLMs not merely as problems, but as opportunities for engagement and education.</abstract>
      <url hash="175fd249">2025.ommm-1.6</url>
      <bibkey>oneill-2025-f</bibkey>
    </paper>
    <paper id="7">
      <title>&lt;think&gt; So let’s replace this phrase with insult... &lt;/think&gt; Lessons learned from generation of toxic texts with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sergey</first><last>Pletenev</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Daniil</first><last>Moskovskiy</last></author>
      <pages>59–63</pages>
      <abstract>Modern Large Language Models (LLMs) are excellent at generating synthetic data. However, their performance in sensitive domains such as text detoxification has not received proper attention from the scientific community. This paper explores the possibility of using LLM-generated synthetic toxic data as an alternative to human-generated data for training models for detoxification. Using Llama 3 and Qwen activation-patched models, we generated synthetic toxic counterparts for neutral texts from ParaDetox and SST-2 datasets. Our experiments show that models fine-tuned on synthetic data consistently perform worse than those trained on human data, with a drop in performance of up to 30% in joint metrics. The root cause is identified as a critical lexical diversity gap: LLMs generate toxic content using a small, repetitive vocabulary of insults that fails to capture the nuances and variety of human toxicity. These findings highlight the limitations of current LLMs in this domain and emphasize the continued importance of diverse, human-annotated data for building robust detoxification systems.</abstract>
      <url hash="daf1cdec">2025.ommm-1.7</url>
      <bibkey>pletenev-etal-2025-think</bibkey>
    </paper>
    <paper id="8">
      <title>Anthropomorphizing <fixed-case>AI</fixed-case>: A Multi-Label Analysis of Public Discourse on Social Media</title>
      <author><first>Muhammad Owais</first><last>Raza</last></author>
      <author><first>Areej Fatemah</first><last>Meghji</last></author>
      <pages>64–73</pages>
      <abstract>As the anthropomorphization of AI in public discourse usually reflects a complex interplay of metaphors, media framing, and societal perceptions, it is increasingly being used to shape and influence public perception on a variety of topics. To explore public perception and investigate how AI is personified, emotionalized, and interpreted in public discourse, we develop a custom multi-labeled dataset from the title and description of YouTube videos discussing artificial intelligence (AI) and large language models (LLMs). This was accomplished using a hybrid annotation pipeline that combined human-in-the-loop validation with AI assisted pre-labeling. This research introduces a novel taxonomy of narrative and epistemic dimensions commonly found in social media content on AI / LLM. Employing two modeling techniques based on traditional machine learning and transformer-based models for classification, the experimental results indicate that the fine-tuned transformer models, particularly AnthroRoBERTa and AnthroDistilBERT, generally outperform traditional machine learning approaches in anthropomorphization focused classification.</abstract>
      <url hash="6f4d50ef">2025.ommm-1.8</url>
      <bibkey>raza-meghji-2025-anthropomorphizing</bibkey>
    </paper>
    <paper id="9">
      <title>Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jonathan Hvithamar</first><last>Rystrøm</last></author>
      <author><first>Hannah Rose</first><last>Kirk</last></author>
      <author><first>Scott</first><last>Hale</last></author>
      <pages>74–85</pages>
      <abstract>Large Language Models (LLMs) are becoming increasingly capable across global languages. However, the ability to communicate across languages does not necessarily translate to appropriate cultural representations. A key concern is US-centric bias, where LLMs reflect US rather than local cultural values. We propose a novel methodology that compares LLM-generated response distributions against population-level opinion data from the World Value Survey across four languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear mixed-effects regression framework, we compare three families of models: Google’s Gemma models (2B-27B parameters), AI2’s OLMo models (7B-32B parameters), and successive iterations of OpenAI’s turbo-series. Across the families of models, we find no consistent relationships between language capabilities and cultural alignment. While the Gemma models have a positive correlation between language capability and cultural alignment across all languages, the OpenAI and OLMo models are inconsistent. Our results demonstrate that achieving meaningful cultural alignment requires dedicated effort beyond improving general language capabilities.</abstract>
      <url hash="c522f309">2025.ommm-1.9</url>
      <bibkey>rystrom-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="10">
      <title>Learn, Achieve, Predict, Propose, Forget, Suffer: Analysing and Classifying Anthropomorphisms of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Matthew</first><last>Shardlow</last></author>
      <author><first>Ashley</first><last>Williams</last></author>
      <author><first>Charlie</first><last>Roadhouse</last></author>
      <author><first>Filippos Karolos</first><last>Ventirozos</last></author>
      <author><first>Piotr</first><last>Przybyła</last></author>
      <pages>86–94</pages>
      <abstract>Anthropomorphism is a literary device where human-like characteristics are used to refer to non-human entities. However, the use of anthropomorphism in the scientific description and public communication of large language models could lead to misunderstanding amongst scientists and lay-people regarding the technical capabilities and limitations of these models. In this study, we present an analysis of anthropomorphised language commonly used to describe LLMs, showing that the presence of terms such as ‘learn’, ‘achieve’, ‘predict’ and ‘can’ are typically correlated with human labels of anthropomorphism. We also perform experiments to develop a classification system for anthropomorphic descriptions of LLMs in scientific writing at the sentence level. We find that whilst a supervised Roberta-based system identifies anthropomorphisms with F1-score of 0.564, state-of-the-art LLM-based approaches regularly overfit to the task.</abstract>
      <url hash="8d8a186d">2025.ommm-1.10</url>
      <bibkey>shardlow-etal-2025-learn</bibkey>
    </paper>
    <paper id="11">
      <title>Leveraging the Scala type system for secure <fixed-case>LLM</fixed-case>-generated code</title>
      <author><first>Alexander</first><last>Sternfeld</last></author>
      <author><first>Ljiljana</first><last>Dolamic</last></author>
      <author><first>Andrei</first><last>Kucharavy</last></author>
      <pages>95–103</pages>
      <abstract>Large language models (LLMs) have shown remarkable proficiency in code generation tasks across various programming languages. However, their outputs often contain subtle but critical vulnerabilities, posing significant risks when deployed in security-sensitive or mission-critical systems. This paper introduces an agentic AI framework designed to enhance the security and robustness of LLM-generated code by leveraging strongly typed and verifiable languages, using Scala as a representative example. We evaluate the effectiveness of our approach in two settings: formal verification with the Stainless framework and general-purpose secure code generation. Our experiments with leading open-source LLMs reveal that while direct code generation often fails to enforce safety constraints, just as naive prompting for more secure code, our type-focused agentic pipeline substantially mitigates input validation and injection vulnerabilities. The results demonstrate the potential of structured, type-guided LLM workflows to improve the SotA of the trustworthiness of automated code generation in high-assurance domains.</abstract>
      <url hash="2f90848a">2025.ommm-1.11</url>
      <attachment type="OptionalSupplementaryMaterial" hash="d3211b16">2025.ommm-1.11.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>sternfeld-etal-2025-leveraging</bibkey>
    </paper>
  </volume>
</collection>
