<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.nlp4dh">
  <volume id="1" ingest-date="2024-11-05" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th International Conference on Natural Language Processing for Digital Humanities</booktitle>
      <editor><first>Mika</first><last>Hämäläinen</last></editor>
      <editor><first>Emily</first><last>Öhman</last></editor>
      <editor><first>So</first><last>Miyagawa</last></editor>
      <editor><first>Khalid</first><last>Alnajjar</last></editor>
      <editor><first>Yuri</first><last>Bizzoni</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Miami, USA</address>
      <month>November</month>
      <year>2024</year>
      <url hash="1699d0ba">2024.nlp4dh-1</url>
      <venue>nlp4dh</venue>
    </meta>
    <frontmatter>
      <url hash="6f2349b4">2024.nlp4dh-1.0</url>
      <bibkey>nlp4dh-2024-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Text Length and the Function of Intentionality: A Case Study of Contrastive Subreddits</title>
      <author><first>Emily Sofi</first><last>Ohman</last></author>
      <author><first>Aatu</first><last>Liimatta</last></author>
      <pages>1–8</pages>
      <abstract>Text length is of central concern in natural language processing (NLP) tasks, yet it is very much under-researched. In this paper, we use social media data, specifically Reddit, to explore the function of text length and intentionality by contrasting subreddits of the same topic where one is considered more serious/professional/academic and the other more relaxed/beginner/layperson. We hypothesize that word choices are more deliberate and intentional in the more in-depth and professional subreddits with texts subsequently becoming longer as a function of this intentionality. We argue that this has deep implications for many applied NLP tasks such as emotion and sentiment analysis, fake news and disinformation detection, and other modeling tasks focused on social media and similar platforms where users interact with each other via the medium of text.</abstract>
      <url hash="1f68e11e">2024.nlp4dh-1.1</url>
      <bibkey>ohman-liimatta-2024-text</bibkey>
    </paper>
    <paper id="2">
      <title>Tracing the Genealogies of Ideas with Sentence Embeddings</title>
      <author><first>Lucian</first><last>Li</last></author>
      <pages>9–16</pages>
      <abstract>Detecting intellectual influence in unstructured text is an important problem for a wide range of fields, including intellectual history, social science, and bibliometrics. A wide range of previous studies in computational social science and digital humanities have attempted to resolve this through a range of dictionary, embedding, and language model based methods. I introduce an approach which leverages a sentence embedding index to efficiently search for similar ideas in a large historical corpus. This method remains robust in conditions of high OCR error found in real mass digitized historical corpora that disrupt previous published methods, while also capturing paraphrase and indirect influence. I evaluate this method on a large corpus of 250,000 nonfiction texts from the 19th century, and find that discovered influence is in line with history of science literature. By expanding the scope of our search for influence and the origins of ideas beyond traditional structured corpora and canonical works and figures, we can get a more nuanced perspective on influence and idea dissemination that can encompass epistemically marginalized groups.</abstract>
      <url hash="0a0c88a1">2024.nlp4dh-1.2</url>
      <bibkey>li-2024-tracing</bibkey>
    </paper>
    <paper id="3">
      <title>Evaluating Computational Representations of Character: An Austen Character Similarity Benchmark</title>
      <author><first>Funing</first><last>Yang</last></author>
      <author><first>Carolyn Jane</first><last>Anderson</last></author>
      <pages>17–30</pages>
      <abstract>Several systems have been developed to extract information about characters to aid computational analysis of English literature. We propose character similarity grouping as a holistic evaluation task for these pipelines. We present AustenAlike, a benchmark suite of character similarities in Jane Austen’s novels. Our benchmark draws on three notions of character similarity: a structurally defined notion of similarity; a socially defined notion of similarity; and an expert defined set extracted from literary criticism. We use AustenAlike to evaluate character features extracted using two pipelines, BookNLP and FanfictionNLP. We build character representations from four kinds of features and compare them to the three AustenAlike benchmarks and to GPT-4 similarity rankings. We find that though computational representations capture some broad similarities based on shared social and narrative roles, the expert pairings in our third benchmark are challenging for all systems, highlighting the subtler aspects of similarity noted by human readers.</abstract>
      <url hash="5364f486">2024.nlp4dh-1.3</url>
      <bibkey>yang-anderson-2024-evaluating</bibkey>
    </paper>
    <paper id="4">
      <title>Investigating Expert-in-the-Loop <fixed-case>LLM</fixed-case> Discourse Patterns for Ancient Intertextual Analysis</title>
      <author><first>Ray</first><last>Umphrey</last></author>
      <author><first>Jesse</first><last>Roberts</last></author>
      <author><first>Lindsey</first><last>Roberts</last></author>
      <pages>31–40</pages>
      <abstract>This study explores the potential of large language models (LLMs) for identifying and examining intertextual relationships within biblical, koine Greek texts. By evaluating the performance of LLMs on various intertextuality scenarios the study demonstrates that these models can detect direct quotations, allusions, and echoes between texts. The LLM’s ability to generate novel intertextual observations and connections highlights its potential to uncover new insights. However, the model also struggles with long query passages and the inclusion of false intertextual dependences, emphasizing the importance of expert evaluation. The expert-in-the-loop methodology presented offers a scalable approach for intertextual research into the complex web of intertextuality within and beyond the biblical corpus.</abstract>
      <url hash="9752ed8e">2024.nlp4dh-1.4</url>
      <bibkey>umphrey-etal-2024-investigating</bibkey>
    </paper>
    <paper id="5">
      <title>Extracting Relations from Ecclesiastical Cultural Heritage Texts</title>
      <author><first>Giulia</first><last>Cruciani</last></author>
      <pages>41–50</pages>
      <abstract>Motivated by the increasing volume of data and the necessity of getting valuable insights, this research describes the process of extracting entities and relations from Italian texts in the context of ecclesiastical cultural heritage data. Named Entity Recognition (NER) and Relation Extraction (RE) are paramount tasks in Natural Language Processing. This paper presents a traditional methodology based on a two-step procedure: firstly, a custom model for Named Entity Recognition extracts entities from data, and then, a multi-input neural network model is trained to perform Relation Classification as a multi-label classification problem. Data are provided by IDS&amp;Unitelm (technological partner of the IT Services and National Office for Ecclesiastical Cultural Heritage and Religious Buildings of CEI, the Italian Episcopal Conference) and concerns biographical texts of 9,982 entities of type person, which can be accessed by the online portal BeWeb. This approach aims to enhance the organization and accessibility of ecclesiastical cultural heritage data, offering deeper insights into historical biographical records.</abstract>
      <url hash="2801d82c">2024.nlp4dh-1.5</url>
      <bibkey>cruciani-2024-extracting</bibkey>
    </paper>
    <paper id="6">
      <title>Constructing a Sentiment-Annotated Corpus of <fixed-case>A</fixed-case>ustrian Historical Newspapers: Challenges, Tools, and Annotator Experience</title>
      <author><first>Lucija</first><last>Krusic</last></author>
      <pages>51–62</pages>
      <abstract>This study presents the development of a sentiment-annotated corpus of historical newspaper texts in Austrian German, addressing a gap in annotated corpora for Natural Language Processing in the field of Digital Humanities. Three annotators categorised 1005 sentences from two 19th-century periodicals into four sentiment categories: positive, negative, neutral, and mixed. The annotators, Masters and PhD students in Linguistics and Digital Humanities, are considered semi-experts and have received substantial training during this annotation study. Three tools were used and compared in the annotation process: Google Sheets, Google Forms and Doccano, and resulted in a gold standard corpus. The analysis revealed a fair to moderate inter-rater agreement (Fleiss’ kappa = 0.405) and an average percentage agreement of 45.7% for full consensus and 92.5% for majority vote. As majority vote is needed for the creation of a gold standard corpus, these results are considered sufficient, and the annotations reliable. The study also introduced comprehensive guidelines for sentiment annotation, which were essential to overcome the challenges posed by historical language and context. The annotators’ experience was assessed through a combination of standardised usability tests (NASA-TLX and UEQ-S) and a detailed custom-made user experience questionnaire, which provided qualitative insights into the difficulties and usability of the tools used. The questionnaire is an additional resource that can be used to assess usability and user experience assessments in future annotation studies. The findings demonstrate the effectiveness of semi-expert annotators and dedicated tools in producing reliable annotations and provide valuable resources, including the annotated corpus, guidelines, and a user experience questionnaire, for future sentiment analysis and annotation of Austrian historical texts. The sentiment-annotated corpus will be used as the gold standard for fine-tuning and evaluating machine learning models for sentiment analysis of Austrian historical newspapers with the topic of migration and minorities in a subsequent study.</abstract>
      <url hash="d2b22a2b">2024.nlp4dh-1.6</url>
      <bibkey>krusic-2024-constructing</bibkey>
    </paper>
    <paper id="7">
      <title>It is a Truth Individually Acknowledged: Cross-references On Demand</title>
      <author><first>Piper</first><last>Vasicek</last></author>
      <author><first>Courtni</first><last>Byun</last></author>
      <author><first>Kevin</first><last>Seppi</last></author>
      <pages>63–74</pages>
      <abstract>Cross-references link source passages of text to other passages that elucidate the source passage in some way and can deepen human understanding. Despite their usefulness, however, good cross-references are hard to find, and extensive sets of cross-references only exist for the few most highly studied books such as the Bible, for which scholars have been collecting cross-references for hundreds of years. Therefore, we propose a new task: generate cross-references for user-selected text on demand. We define a metric, coverage, to evaluate task performance. We adapt several models to generate cross references, including an Anchor Words topic model, SBERT SentenceTransformers, and ChatGPT, and evaluate their coverage in both English and German on existing cross-reference datasets. While ChatGPT outperforms other models on these datasets, this is likely due to data contamination. We hand-evaluate performance on the well-known works of Jane Austen and a less-known science fiction series Sons of the Starfarers by Joe Vasicek, finding that ChatGPT does not perform as well on these works; sentence embeddings perform best. We experiment with newer LLMs and large context windows, and suggest that future work should focus on deploying cross-references on-demand with readers to determine their effectiveness in the wild.</abstract>
      <url hash="b3ce607d">2024.nlp4dh-1.7</url>
      <bibkey>vasicek-etal-2024-truth</bibkey>
    </paper>
    <paper id="8">
      <title>Extracting position titles from unstructured historical job advertisements</title>
      <author><first>Klara</first><last>Venglarova</last></author>
      <author><first>Raven</first><last>Adam</last></author>
      <author><first>Georg</first><last>Vogeler</last></author>
      <pages>75–84</pages>
      <abstract>This paper explores the automated extraction of job titles from unstructured historical job advertisements, using a corpus of digitized German-language newspapers from 1850-1950. The study addresses the challenges of working with unstructured, OCR-processed historical data, contrasting with contemporary approaches that often use structured, digitally-born datasets when dealing with this text type. We compare four extraction methods: a dictionary-based approach, a rule-based approach, a named entity recognition (NER) mode, and a text-generation method. The NER approach, trained on manually annotated data, achieved the highest F1 score (0.944 using transformers model trained on GPU, 0.884 model trained on CPU), demonstrating its flexibility and ability to correctly identify job titles. The text-generation approach performs similarly (0.920). However, the rule-based (0.69) and dictionary-based (0.632) methods reach relatively high F1 Scores as well, while offering the advantage of not requiring extensive labeling of training data. The results highlight the complexities of extracting meaningful job titles from historical texts, with implications for further research into labor market trends and occupational history.</abstract>
      <url hash="6fac1b91">2024.nlp4dh-1.8</url>
      <bibkey>venglarova-etal-2024-extracting</bibkey>
    </paper>
    <paper id="9">
      <title>Language Resources From Prominent Born-Digital Humanities Texts are Still Needed in the Age of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Natalie</first><last>Hervieux</last></author>
      <author><first>Peiran</first><last>Yao</last></author>
      <author><first>Susan</first><last>Brown</last></author>
      <author><first>Denilson</first><last>Barbosa</last></author>
      <pages>85–104</pages>
      <abstract>The digital humanities (DH) community fundamentally embraces the use of computerized tools for the study and creation of knowledge related to language, history, culture, and human values, in which natural language plays a prominent role. Many successful DH tools rely heavily on Natural Language Processing methods, and several efforts exist within the DH community to promote the use of newer and better tools. Nevertheless, most NLP research is driven by web corpora that are noticeably different from texts commonly found in DH artifacts, which tend to use richer language and refer to rarer entities. Thus, the near-human performance achieved by state-of-the-art NLP tools on web texts might not be achievable on DH texts. We introduce a dataset carefully created by computer scientists and digital humanists intended to serve as a reference point for the development and evaluation of NLP tools. The dataset is a subset of a born-digital textbase resulting from a prominent and ongoing experiment in digital literary history, containing thousands of multi-sentence excerpts that are suited for information extraction tasks. We fully describe the dataset and show that its language is demonstrably different than the corpora normally used in training language resources in the NLP community.</abstract>
      <url hash="5429afac">2024.nlp4dh-1.9</url>
      <bibkey>hervieux-etal-2024-language</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>NLP</fixed-case> for Digital Humanities: Processing Chronological Text Corpora</title>
      <author><first>Adam</first><last>Pawłowski</last></author>
      <author><first>Tomasz</first><last>Walkowiak</last></author>
      <pages>105–112</pages>
      <abstract>The paper focuses on the integration of Natural Language Processing (NLP) techniques to analyze extensive chronological text corpora. This research underscores the synergy between humanistic inquiry and computational methods, especially in the processing and analysis of sequential textual data known as lexical series. A reference workflow for chronological corpus analysis is introduced, outlining the methodologies applicable to the ChronoPress corpus, a data set that encompasses 22 years of Polish press from 1945 to 1966. The study showcases the potential of this approach in uncovering cultural and historical patterns through the analysis of lexical series. The findings highlight both the challenges and opportunities present in leveraging lexical series analysis within Digital Humanities, emphasizing the necessity for advanced data filtering and anomaly detection algorithms to effectively manage the vast and intricate datasets characteristic of this field.</abstract>
      <url hash="cf672d5e">2024.nlp4dh-1.10</url>
      <bibkey>pawlowski-walkowiak-2024-nlp</bibkey>
    </paper>
    <paper id="11">
      <title>A Multi-task Framework with Enhanced Hierarchical Attention for Sentiment Analysis on Classical <fixed-case>C</fixed-case>hinese Poetry: Utilizing Information from Short Lines</title>
      <author><first>Quanqi</first><last>Du</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>113–122</pages>
      <abstract>Classical Chinese poetry has a long history, dating back to the 11th century BC. By investigating the sentiment expressed in the poetry, we can gain more insights in the emotional life and history development in ancient Chinese culture. To help improve the sentiment analysis performance in the field of classical Chinese poetry, we propose to utilize the unique information from the individual short lines that compose the poem, and introduce a multi-task framework with hierarchical attention enhanced with short line sentiment labels. Specifically, the multi-task framework comprises sentiment analysis for both the overall poem and the short lines, while the hierarchical attention consists of word- and sentence-level attention, with the latter enhanced with additional information from short line sentiments. Our experimental results showcase that our approach leveraging more fine-grained information from short lines outperforms the state-of-the-art, achieving an accuracy score of 72.88% and an F1-macro score of 71.05%.</abstract>
      <url hash="15651f93">2024.nlp4dh-1.11</url>
      <bibkey>du-hoste-2024-multi</bibkey>
    </paper>
    <paper id="12">
      <title>Exploring Similarity Measures and Intertextuality in <fixed-case>V</fixed-case>edic <fixed-case>S</fixed-case>anskrit Literature</title>
      <author><first>So</first><last>Miyagawa</last></author>
      <author><first>Yuki</first><last>Kyogoku</last></author>
      <author><first>Yuzuki</first><last>Tsukagoshi</last></author>
      <author><first>Kyoko</first><last>Amano</last></author>
      <pages>123–131</pages>
      <abstract>This paper examines semantic similarity and intertextuality in selected texts from the Vedic Sanskrit corpus, specifically the Maitrāyaṇī Saṃhitā (MS) and Kāṭhaka-Saṃhitā (KS). Three computational methods are employed: Word2Vec for word embeddings, stylo package for stylometric analysis, and TRACER for text reuse detection. By comparing various sections of the texts at different granularities, patterns of similarity and structural alignment are uncovered, providing insights into textual relationships and chronology. Word embeddings capture semantic similarities, while stylometric analysis reveals clusters and components that differentiate the texts. TRACER identifies parallel passages, indicating probable instances of text reuse. The computational analysis corroborates previous philological studies, suggesting a shared period of composition between MS.1.9 and MS.1.7. This research highlights the potential of computational methods in studying ancient Sanskrit literature, complementing traditional approaches. The agreement among the methods strengthens the validity of the findings, and the visualizations offer a nuanced understanding of textual connections. The study demonstrates that smaller chunk sizes are more effective for detecting intertextual parallels, showcasing the power of these techniques in unraveling the complexities of ancient texts.</abstract>
      <url hash="f5cd7682">2024.nlp4dh-1.12</url>
      <bibkey>miyagawa-etal-2024-exploring</bibkey>
    </paper>
    <paper id="13">
      <title>Historical Ink: 19th Century <fixed-case>L</fixed-case>atin <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>panish Newspaper Corpus with <fixed-case>LLM</fixed-case> <fixed-case>OCR</fixed-case> Correction</title>
      <author><first>Laura</first><last>Manrique-Gomez</last></author>
      <author><first>Tony</first><last>Montes</last></author>
      <author><first>Arturo</first><last>Rodriguez Herrera</last></author>
      <author><first>Ruben</first><last>Manrique</last></author>
      <pages>132–139</pages>
      <abstract>This paper presents two significant contributions: First, it introduces a novel dataset of 19th-century Latin American newspaper texts, addressing a critical gap in specialized corpora for historical and linguistic analysis in this region. Second, it develops a flexible framework that utilizes a Large Language Model for OCR error correction and linguistic surface form detection in digitized corpora. This semi-automated framework is adaptable to various contexts and datasets and is applied to the newly created dataset.</abstract>
      <url hash="09a9eeaa">2024.nlp4dh-1.13</url>
      <bibkey>manrique-gomez-etal-2024-historical</bibkey>
    </paper>
    <paper id="14">
      <title>Canonical Status and Literary Influence: A Comparative Study of <fixed-case>D</fixed-case>anish Novels from the Modern Breakthrough (1870–1900)</title>
      <author><first>Pascale</first><last>Feldkamp</last></author>
      <author><first>Alie</first><last>Lassche</last></author>
      <author><first>Jan</first><last>Kostkan</last></author>
      <author><first>Márton</first><last>Kardos</last></author>
      <author><first>Kenneth</first><last>Enevoldsen</last></author>
      <author><first>Katrine</first><last>Baunvig</last></author>
      <author><first>Kristoffer</first><last>Nielbo</last></author>
      <pages>140–155</pages>
      <abstract>We examine the relationship between the canonization of Danish novels and their textual innovation and influence, taking the Danish Modern Breakthrough era (1870–1900) as a case study. We evaluate whether canonical novels introduced a significant textual novelty in their time, and explore their influence on the overall literary trend of the period. By analyzing the positions of canonical versus non-canonical novels in semantic space, we seek to better understand the link between a novel’s canonical status and its literary impact. Additionally, we examine the overall diversification of Modern Breakthrough novels during this significant period of rising literary readership. We find that canonical novels stand out from both the historical novel genre and non-canonical novels of the period. Our findings on diversification within and across groups indicate that the novels now regarded as canonical served as literary trendsetters of their time.</abstract>
      <url hash="73e383b3">2024.nlp4dh-1.14</url>
      <bibkey>feldkamp-etal-2024-canonical</bibkey>
    </paper>
    <paper id="15">
      <title>Deciphering psycho-social effects of Eating Disorder : Analysis of <fixed-case>R</fixed-case>eddit Posts using Large Language Model(<fixed-case>LLM</fixed-case>)s and Topic Modeling</title>
      <author><first>Medini</first><last>Chopra</last></author>
      <author><first>Anindita</first><last>Chatterjee</last></author>
      <author><first>Lipika</first><last>Dey</last></author>
      <author><first>Partha Pratim</first><last>Das</last></author>
      <pages>156–164</pages>
      <abstract>Eating disorders are a global health concern as they manifest in increasing numbers across all sections of society. Social network platforms have emerged as a dependable source of information about the disease, its effect, and its prevalence among different sections. This work lays the foundation for large-scale analysis of social media data using large language models (LLMs). We show that using LLMs can drastically reduce the time and resource requirements for garnering insights from large data repositories. With respect to ED, this work focuses on understanding its psychological impacts on both patients and those who live in their proximity. Social scientists can utilize the proposed approach to design more focused studies with better representative groups.</abstract>
      <url hash="0783b185">2024.nlp4dh-1.15</url>
      <bibkey>chopra-etal-2024-deciphering</bibkey>
    </paper>
    <paper id="16">
      <title>Topic-Aware Causal Intervention for Counterfactual Detection</title>
      <author><first>Thong Thanh</first><last>Nguyen</last></author>
      <author><first>Truc-My</first><last>Nguyen</last></author>
      <pages>165–176</pages>
      <abstract>Counterfactual statements, which describe events that did not or cannot take place, are beneficial to numerous NLP applications. Hence, we consider the problem of counterfactual detection (CFD) and seek to enhance the CFD models. Previous models are reliant on clue phrases to predict counterfactuality, so they suffer from significant performance drop when clue phrase hints do not exist during testing. Moreover, these models tend to predict non-counterfactuals over counterfactuals. To address these issues, we propose to integrate neural topic model into the CFD model to capture the global semantics of the input statement. We continue to causally intervene the hidden representations of the CFD model to balance the effect of the class labels. Extensive experiments show that our approach outperforms previous state-of-the-art CFD and bias-resolving methods in both the CFD and other bias-sensitive tasks.</abstract>
      <url hash="b4a3f416">2024.nlp4dh-1.16</url>
      <bibkey>nguyen-nguyen-2024-topic</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>UD</fixed-case> for <fixed-case>G</fixed-case>erman Poetry</title>
      <author><first>Stefanie</first><last>Dipper</last></author>
      <author><first>Ronja</first><last>Laarmann-Quante</last></author>
      <pages>177–188</pages>
      <abstract>This article deals with the syntactic analysis of German-language poetry from different centuries. We use Universal Dependencies (UD) as our syntactic framework. We discuss particular challenges of the poems in terms of tokenization, sentence boundary recognition and special syntactic constructions. Our annotated corpus currently consists of 20 poems with a total of 2,162 tokens, which originate from the PoeTree.de corpus. We present some statistics on our annotations and also evaluate the automatic UD annotation from PoeTree.de using our annotations.</abstract>
      <url hash="07768ddf">2024.nlp4dh-1.17</url>
      <bibkey>dipper-laarmann-quante-2024-ud</bibkey>
    </paper>
    <paper id="18">
      <title>Molyé: A Corpus-based Approach to Language Contact in Colonial <fixed-case>F</fixed-case>rance</title>
      <author><first>Rasul</first><last>Dent</last></author>
      <author><first>Juliette</first><last>Janes</last></author>
      <author><first>Thibault</first><last>Clerice</last></author>
      <author><first>Pedro</first><last>Ortiz Suarez</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>189–199</pages>
      <abstract>Whether or not several Creole languages which developed during the early modern period can be considered genetic descendants of European languages has been the subject of intense debate. This is in large part due to the absence of evidence of intermediate forms. This work introduces a new open corpus, the Molyé corpus, which combines stereotypical representations of three kinds of language variation in Europe with early attestations of French-based Creole languages across a period of 400 years. It is intended to facilitate future research on the continuity between contact situations in Europe and Creolophone (former) colonies.</abstract>
      <url hash="b0ba8975">2024.nlp4dh-1.18</url>
      <bibkey>dent-etal-2024-molye</bibkey>
    </paper>
    <paper id="19">
      <title>Vector Poetics: Parallel Couplet Detection in Classical <fixed-case>C</fixed-case>hinese Poetry</title>
      <author><first>Maciej</first><last>Kurzynski</last></author>
      <author><first>Xiaotong</first><last>Xu</last></author>
      <author><first>Yu</first><last>Feng</last></author>
      <pages>200–208</pages>
      <abstract>This paper explores computational approaches for detecting parallelism in classical Chinese poetry, a rhetorical device where two verses mirror each other in syntax, meaning, tone, and rhythm. We experiment with five classification methods: (1) verb position matching, (2) integrated semantic, syntactic, and word-segmentation analysis, (3) difference-based character embeddings, (4) structured examples (inner/outer couplets), and (5) GPT-guided classification. We use a manually annotated dataset, containing 6,125 pentasyllabic couplets, to evaluate performance. The results indicate that parallelism detection poses a significant challenge even for powerful LLMs such as GPT-4o, with the highest F1 score below 0.72. Nevertheless, each method contributes valuable insights into the art of parallelism in Chinese poetry, suggesting a new understanding of parallelism as a verbal expression of principal components in a culturally defined vector space.</abstract>
      <url hash="7201a94c">2024.nlp4dh-1.19</url>
      <bibkey>kurzynski-etal-2024-vector</bibkey>
    </paper>
    <paper id="20">
      <title>Adapting Measures of Literality for Use with Historical Language Data</title>
      <author><first>Adam</first><last>Roussel</last></author>
      <pages>209–215</pages>
      <abstract>This paper concerns the adaptation of two existing computational measures relating to the estimation of the literality of expressions to enable their use in scenarios where data is scarce, as is usually the case with historical language data. Being able to determine an expression’s literality via statistical means could support a range of linguistic annotation tasks, such as those relating to metaphor, metonymy, and idiomatic expressions, however making this judgment is especially difficult for modern annotators of historical and ancient texts. Therefore we re-implement these measures using smaller corpora and count-based vectors more suited to these amounts of training data. The adapted measures are evaluated against an existing data set of particle verbs annotated with degrees of literality. The results were inconclusive, yielding low correlations between 0.05 and 0.10 (Spearman’s ρ). Further work is needed to determine which measures and types of data correspond to which aspects of literality.</abstract>
      <url hash="335e84d7">2024.nlp4dh-1.20</url>
      <bibkey>roussel-2024-adapting</bibkey>
    </paper>
    <paper id="21">
      <title>Improving <fixed-case>L</fixed-case>atin Dependency Parsing by Combining Treebanks and Predictions</title>
      <author><first>Hanna-Mari Kristiina</first><last>Kupari</last></author>
      <author><first>Erik</first><last>Henriksson</last></author>
      <author><first>Veronika</first><last>Laippala</last></author>
      <author><first>Jenna</first><last>Kanerva</last></author>
      <pages>216–228</pages>
      <abstract>This paper introduces new models designed to improve the morpho-syntactic parsing of the five largest Latin treebanks in the Universal Dependencies (UD) framework. First, using two state-of-the-art parsers, Trankit and Stanza, along with our custom UD tagger, we train new models on the five treebanks both individually and by combining them into novel merged datasets. We also test the models on the CIRCSE test set. In an additional experiment, we evaluate whether this set can be accurately tagged using the novel LASLA corpus (https://github.com/CIRCSE/LASLA). Second, we aim to improve the results by combining the predictions of different models through an atomic morphological feature voting system. The results of our two main experiments demonstrate significant improvements, particularly for the smaller treebanks, with LAS scores increasing by 16.10 and 11.85%-points for UDante and Perseus, respectively (Gamba and Zeman, 2023a). Additionally, the voting system for morphological features (FEATS) brings improvements, especially for the smaller Latin treebanks: Perseus 3.15% and CIRCSE 2.47%-points. Tagging the CIRCSE set with our custom model using the LASLA model improves POS 6.71 and FEATS 11.04%-points respectively, compared to our best-performing UD PROIEL model. Our results show that larger datasets and ensemble predictions can significantly improve performance.</abstract>
      <url hash="5ac677a9">2024.nlp4dh-1.21</url>
      <bibkey>kupari-etal-2024-improving</bibkey>
    </paper>
    <paper id="22">
      <title>From N-grams to Pre-trained Multilingual Models For Language Identification</title>
      <author><first>Thapelo Andrew</first><last>Sindane</last></author>
      <author><first>Vukosi</first><last>Marivate</last></author>
      <pages>229–239</pages>
      <abstract>In this paper, we investigate the use of N-gram models and Large Pre-trained Multilingual models for Language Identification (LID) across 11 South African languages. For N-gram models, this study shows that effective data size selection remains crucial for establishing effective frequency distributions of the target languages, that efficiently model each language, thus, improving language ranking. For pre-trained multilingual models, we conduct extensive experiments covering a diverse set of massively pre-trained multilingual (PLM) models – mBERT, RemBERT, XLM-r, and Afri-centric multilingual models – AfriBERTa, Afro-XLMr, AfroLM, and Serengeti. We further compare these models with available large-scale Language Identification tools: Compact Language Detector v3 (CLD V3), AfroLID, GlotLID, and OpenLID to highlight the importance of focused-based LID. From these, we show that Serengeti is a superior model across models: N-grams to Transformers on average. Moreover, we propose a lightweight BERT-based LID model (za_BERT_lid) trained with NHCLT + Vukzenzele corpus, which performs on par with our best-performing Afri-centric models.</abstract>
      <url hash="3f12159f">2024.nlp4dh-1.22</url>
      <bibkey>sindane-marivate-2024-n</bibkey>
    </paper>
    <paper id="23">
      <title>Visualising Changes in Semantic Neighbourhoods of <fixed-case>E</fixed-case>nglish Noun Compounds over Time</title>
      <author><first>Malak</first><last>Rassem</last></author>
      <author><first>Myrto</first><last>Tsigkouli</last></author>
      <author><first>Chris W.</first><last>Jenkins</last></author>
      <author><first>Filip</first><last>Miletić</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>240–246</pages>
      <abstract>This paper provides a framework and tool set for computing and visualising dynamic, time- specific semantic neighbourhoods of English noun-noun compounds and their constituents over time. Our framework not only identifies salient vector-space dimensions and neighbours in notoriously sparse data: we specifically bring together changes in meaning aspects and degrees of (non-)compositionality.</abstract>
      <url hash="54a497bd">2024.nlp4dh-1.23</url>
      <bibkey>rassem-etal-2024-visualising</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>SEFLAG</fixed-case>: Systematic Evaluation Framework for <fixed-case>NLP</fixed-case> Models and Datasets in <fixed-case>L</fixed-case>atin and <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek</title>
      <author><first>Konstantin</first><last>Schulz</last></author>
      <author><first>Florian</first><last>Deichsler</last></author>
      <pages>247–258</pages>
      <abstract>Literary scholars of Latin and Ancient Greek increasingly use natural language processing for their work, but many models and datasets are hard to use due to a lack of sustainable research data management. This paper introduces the Systematic Evaluation Framework for natural language processing models and datasets in Latin and Ancient Greek (SEFLAG), which consistently assesses language resources using common criteria, such as specific evaluation metrics, metadata and risk analysis. The framework, a work in progress in its initial phase, currently covers lemmatization and named entity recognition for both languages, with plans for adding dependency parsing and other tasks. For increased transparency and sustainability, a thorough documentation is included as well as an integration into the HuggingFace ecosystem. The combination of these efforts is designed to support researchers in their search for suitable models.</abstract>
      <url hash="88bf6277">2024.nlp4dh-1.24</url>
      <bibkey>schulz-deichsler-2024-seflag</bibkey>
    </paper>
    <paper id="25">
      <title>A Two-Model Approach for Humour Style Recognition</title>
      <author><first>Mary Ogbuka</first><last>Kenneth</last></author>
      <author><first>Foaad</first><last>Khosmood</last></author>
      <author><first>Abbas</first><last>Edalat</last></author>
      <pages>259–274</pages>
      <abstract>Humour, a fundamental aspect of human communication, manifests itself in various styles that significantly impact social interactions and mental health. Recognising different humour styles poses challenges due to the lack of established datasets and machine learning (ML) models. To address this gap, we present a new text dataset for humour style recognition, comprising 1463 instances across four styles (self-enhancing, self-deprecating, affiliative, and aggressive) and non-humorous text, with lengths ranging from 4 to 229 words. Our research employs various computational methods, including classic machine learning classifiers, text embedding models, and DistilBERT, to establish baseline performance. Additionally, we propose a two-model approach to enhance humour style recognition, particularly in distinguishing between affiliative and aggressive styles. Our method demonstrates an 11.61% improvement in f1-score for affiliative humour classification, with consistent improvements in the 14 models tested. Our findings contribute to the computational analysis of humour in text, offering new tools for studying humour in literature, social media, and other textual sources.</abstract>
      <url hash="4f58fd6e">2024.nlp4dh-1.25</url>
      <bibkey>kenneth-etal-2024-two</bibkey>
    </paper>
    <paper id="26">
      <title>N-gram-Based Preprocessing for Sandhi Reversion in <fixed-case>V</fixed-case>edic <fixed-case>S</fixed-case>anskrit</title>
      <author><first>Yuzuki</first><last>Tsukagoshi</last></author>
      <author><first>Ikki</first><last>Ohmukai</last></author>
      <pages>275–279</pages>
      <abstract>This study aims to address the challenges posed by sandhi in Vedic Sanskrit, a phenomenon that complicates the computational analysis of Sanskrit texts. By focusing on sandhi reversion, the research seeks to improve the accuracy of processing Vedic Sanskrit, an older layer of the language. Sandhi, a phonological phenomenon, poses challenges for text processing in Sanskrit due to the fusion of word boundaries or the sound change around word boundaries. In this research, we developed a transformer-based model with a novel n-gram preprocessing strategy to improve the accuracy of sandhi reversion for Vedic. We created character-based n-gram texts of varying lengths (n = 2, 3, 4, 5, 6) from the Rigveda, the oldest Vedic text, and trained models on these texts to perform machine translation from post-sandhi to pre-sandhi forms. In the results, we found that the model trained with 5-gram text achieved the highest accuracy. This success is likely due to the 5-gram’s ability to capture the maximum phonemic context in which Vedic sandhi occurs, making it more effective for the task. These findings suggest that by leveraging the inherent characteristics of phonological changes in language, even simple preprocessing methods like n-gram segmentation can significantly improve the accuracy of complex linguistic tasks.</abstract>
      <url hash="33d6f011">2024.nlp4dh-1.26</url>
      <bibkey>tsukagoshi-ohmukai-2024-n</bibkey>
    </paper>
    <paper id="27">
      <title>Enhancing <fixed-case>S</fixed-case>wedish Parliamentary Data: Annotation, Accessibility, and Application in Digital Humanities</title>
      <author><first>Shafqat Mumtaz</first><last>Virk</last></author>
      <author><first>Claes</first><last>Ohlsson</last></author>
      <author><first>Nina</first><last>Tahmasebi</last></author>
      <author><first>Henrik</first><last>Björck</last></author>
      <author><first>Leif</first><last>Runefelt</last></author>
      <pages>280–288</pages>
      <abstract>The Swedish bicameral parliament data presents a valuable textual resource that is of interest for many researches and scholars. The parliamentary texts offer many avenues for research including the study of how various affairs were run by governments over time. The Parliament proceedings are available in textual format, but in their original form, they are noisy and unstructured and thus hard to explore and investigate. In this paper, we report the transformation of the raw bicameral parliament data (1867-1970) into a structured lexical resource annotated with various word and document level attributes. The annotated data is then made searchable through two modern corpus infrastructure components which provide a wide array of corpus exploration, visualization, and comparison options. To demonstrate the practical utility of this resource, we present a case study examining the transformation of the concept of ‘market’ over time from a tangible physical entity to an abstract idea.</abstract>
      <url hash="a5de7229">2024.nlp4dh-1.27</url>
      <bibkey>virk-etal-2024-enhancing</bibkey>
    </paper>
    <paper id="28">
      <title>Evaluating Open-Source <fixed-case>LLM</fixed-case>s in Low-Resource Languages: Insights from <fixed-case>L</fixed-case>atvian High School Exams</title>
      <author><first>Roberts</first><last>Darģis</last></author>
      <author><first>Guntis</first><last>Bārzdiņš</last></author>
      <author><first>Inguna</first><last>Skadiņa</last></author>
      <author><first>Baiba</first><last>Saulite</last></author>
      <pages>289–293</pages>
      <abstract>The latest large language models (LLM) have significantly advanced natural language processing (NLP) capabilities across various tasks. However, their performance in low-resource languages, such as Latvian with 1.5 million native speakers, remains substantially underexplored due to both limited training data and the absence of comprehensive evaluation benchmarks. This study addresses this gap by conducting a systematic assessment of prominent open-source LLMs on natural language understanding (NLU) and natural language generation (NLG) tasks in Latvian. We utilize standardized high school centralized graduation exams as a benchmark dataset, offering relatable and diverse evaluation scenarios that encompass multiple-choice questions and complex text analysis tasks. Our experimental setup involves testing models from the leading LLM families, including Llama, Qwen, Gemma, and Mistral, with OpenAI’s GPT-4 serving as a performance reference. The results reveal that certain open-source models demonstrate competitive performance in NLU tasks, narrowing the gap with GPT-4. However, all models exhibit notable deficiencies in NLG tasks, specifically in generating coherent and contextually appropriate text analyses, highlighting persistent challenges in NLG for low-resource languages. These findings contribute to efforts to develop robust multilingual benchmarks and improve LLM performance in diverse linguistic contexts.</abstract>
      <url hash="3ec45a29">2024.nlp4dh-1.28</url>
      <bibkey>dargis-etal-2024-evaluating</bibkey>
    </paper>
    <paper id="29">
      <title>Computational Methods for the Analysis of Complementizer Variability in Language and Literature: The Case of <fixed-case>H</fixed-case>ebrew “she-” and “ki”</title>
      <author><first>Avi</first><last>Shmidman</last></author>
      <author><first>Aynat</first><last>Rubinstein</last></author>
      <pages>294–307</pages>
      <abstract>We demonstrate a computational method for analyzing complementizer variability within language and literature, focusing on Hebrew as a test case. The primary complementizers in Hebrew are “she-” and “ki”. We first run a large-scale corpus analysis to determine the relative preference for one or the other of these complementizers given the preceding verb. On top of this foundation, we leverage clustering methods to measure the degree of interchangeability between the complementizers for each verb. The resulting tables, which provide this information for all common complement-taking verbs in Hebrew, are a first-of-its-kind lexical resource which we provide to the NLP community. Upon this foundation, we demonstrate a computational method to analyze literary works for unusual and unexpected complementizer usages deserving of literary analysis.</abstract>
      <url hash="fb80dbdf">2024.nlp4dh-1.29</url>
      <bibkey>shmidman-rubinstein-2024-computational</bibkey>
    </paper>
    <paper id="30">
      <title>From Discrete to Continuous Classes: A Situational Analysis of Multilingual Web Registers with <fixed-case>LLM</fixed-case> Annotations</title>
      <author><first>Erik</first><last>Henriksson</last></author>
      <author><first>Amanda</first><last>Myntti</last></author>
      <author><first>Saara</first><last>Hellström</last></author>
      <author><first>Selcen</first><last>Erten-Johansson</last></author>
      <author><first>Anni</first><last>Eskelinen</last></author>
      <author><first>Liina</first><last>Repo</last></author>
      <author><first>Veronika</first><last>Laippala</last></author>
      <pages>308–318</pages>
      <abstract>In corpus linguistics, registers–language varieties suited to different contexts–have traditionally been defined by their situations of use, yet recent studies reveal significant situational variation within registers. Previous quantitative studies, however, have been limited to English, leaving this variation in other languages largely unexplored. To address this gap, we apply a quantitative situational analysis to a large multilingual web register corpus, using large language models (LLMs) to annotate texts in English, Finnish, French, Swedish, and Turkish for 23 situational parameters. Using clustering techniques, we identify six situational text types, such as “Advice”, “Opinion” and “Marketing”, each characterized by distinct situational features. We explore the relationship between these text types and traditional register categories, finding partial alignment, though no register maps perfectly onto a single cluster. These results support the quantitative approach to situational analysis and are consistent with earlier findings for English. Cross-linguistic comparisons show that language accounts for only a small part of situational variation within registers, suggesting registers are situationally similar across languages. This study demonstrates the utility of LLMs in multilingual register analysis and deepens our understanding of situational variation within registers.</abstract>
      <url hash="38fd7eea">2024.nlp4dh-1.30</url>
      <bibkey>henriksson-etal-2024-discrete</bibkey>
    </paper>
    <paper id="31">
      <title>Testing and Adapting the Representational Abilities of Large Language Models on Folktales in Low-Resource Languages</title>
      <author><first>J. A.</first><last>Meaney</last></author>
      <author><first>Beatrice</first><last>Alex</last></author>
      <author><first>William</first><last>Lamb</last></author>
      <pages>319–324</pages>
      <abstract>Folktales are a rich resource of knowledge about the society and culture of a civilisation. Digital folklore research aims to use automated techniques to better understand these folktales, and it relies on abstract representations of the textual data. Although a number of large language models (LLMs) claim to be able to represent low-resource langauges such as Irish and Gaelic, we present two classification tasks to explore how useful these representations are, and three adaptations to improve the performance of these models. We find that adapting the models to work with longer sequences, and continuing pre-training on the domain of folktales improves classification performance, although these findings are tempered by the impressive performance of a baseline SVM with non-contextual features.</abstract>
      <url hash="35ee0c93">2024.nlp4dh-1.31</url>
      <bibkey>meaney-etal-2024-testing</bibkey>
    </paper>
    <paper id="32">
      <title>Examining Language Modeling Assumptions Using an Annotated Literary Dialect Corpus</title>
      <author><first>Craig</first><last>Messner</last></author>
      <author><first>Thomas</first><last>Lippincott</last></author>
      <pages>325–330</pages>
      <abstract>We present a dataset of 19th century American literary orthovariant tokens with a novel layer of human-annotated dialect group tags designed to serve as the basis for computational experiments exploring literarily meaningful orthographic variation. We perform an initial broad set of experiments over this dataset using both token (BERT) and character (CANINE)-level contextual language models. We find indications that the “dialect effect” produced by intentional orthographic variation employs multiple linguistic channels, and that these channels are able to be surfaced to varied degrees given particular language modelling assumptions. Specifically, we find evidence showing that choice of tokenization scheme meaningfully impact the type of orthographic information a model is able to surface.</abstract>
      <url hash="1fac4d0b">2024.nlp4dh-1.32</url>
      <bibkey>messner-lippincott-2024-examining</bibkey>
    </paper>
    <paper id="33">
      <title>Evaluating Language Models in Location Referring Expression Extraction from Early Modern and Contemporary <fixed-case>J</fixed-case>apanese Texts</title>
      <author><first>Ayuki</first><last>Katayama</last></author>
      <author><first>Yusuke</first><last>Sakai</last></author>
      <author><first>Shohei</first><last>Higashiyama</last></author>
      <author><first>Hiroki</first><last>Ouchi</last></author>
      <author><first>Ayano</first><last>Takeuchi</last></author>
      <author><first>Ryo</first><last>Bando</last></author>
      <author><first>Yuta</first><last>Hashimoto</last></author>
      <author><first>Toshinobu</first><last>Ogiso</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <pages>331–338</pages>
      <abstract>Automatic extraction of geographic information, including Location Referring Expressions (LREs), can aid humanities research in analyzing large collections of historical texts. In this study, to investigate how accurate pretrained Transformer language models (LMs) can extract LREs from historical texts, we evaluate two representative types of LMs, namely, masked language model and causal language model, using early modern and contemporary Japanese datasets. Our experimental results demonstrated the potential of contemporary LMs for historical texts, but also suggest the need for further model enhancement, such as pretraining on historical texts.</abstract>
      <url hash="7093577b">2024.nlp4dh-1.33</url>
      <bibkey>katayama-etal-2024-evaluating</bibkey>
    </paper>
    <paper id="34">
      <title>Evaluating <fixed-case>LLM</fixed-case> Performance in Character Analysis: A Study of Artificial Beings in Recent <fixed-case>K</fixed-case>orean Science Fiction</title>
      <author><first>Woori</first><last>Jang</last></author>
      <author><first>Seohyon</first><last>Jung</last></author>
      <pages>339–351</pages>
      <abstract>Literary works present diverse and complex character behaviors, often implicit or intentionally obscured, making character analysis an inherently challenging task. This study explores LLMs’ capability to identify and interpret behaviors of artificial beings in 11 award-winning contemporary Korean science fiction short stories. Focusing on artificial beings as a distinct class of characters, rather than on conventional human characters, adds to the multi-layered complexity of analysis. We compared two LLMs, Claude 3.5 Sonnet and GPT-4o, with human experts using a custom eight-label system and a unique agreement metric developed to capture the cognitive intricacies of literary interpretation. Human inter-annotator agreement was around 50%, confirming the subjectivity of literary comprehension. LLMs differed from humans in selected text spans but demonstrated high agreement in label assignment for correctly identified spans. LLMs notably excelled at discerning ‘actions’ as semantic units rather than isolated grammatical components. This study reaffirms literary interpretation’s multifaceted nature while expanding the boundaries of NLP, contributing to discussions about AI’s capacity to understand and interpret creative works.</abstract>
      <url hash="2e34a3d5">2024.nlp4dh-1.34</url>
      <bibkey>jang-jung-2024-evaluating</bibkey>
    </paper>
    <paper id="35">
      <title>Text vs. Transcription: A Study of Differences Between the Writing and Speeches of <fixed-case>U</fixed-case>.<fixed-case>S</fixed-case>. Presidents</title>
      <author><first>Mina</first><last>Rajaei Moghadam</last></author>
      <author><first>Mosab</first><last>Rezaei</last></author>
      <author><first>Gülşat</first><last>Aygen</last></author>
      <author><first>Reva</first><last>Freedman</last></author>
      <pages>352–361</pages>
      <abstract>Even after many years of research, answering the question of the differences between spoken and written text remains open. This paper aims to study syntactic features that can serve as distinguishing factors. To do so, we focus on the transcribed speeches and written books of United States presidents. We conducted two experiments to analyze high-level syntactic features. In the first experiment, we examine these features while controlling for the effect of sentence length. In the second experiment, we compare the high-level syntactic features with low-level ones. The results indicate that adding high-level syntactic features enhances model performance, particularly in longer sentences. Moreover, the importance of the prepositional phrases in a sentence increases with sentence length. We also find that these longer sentences with more prepositional phrases are more likely to appear in speeches than in written books by U.S. presidents.</abstract>
      <url hash="395209ae">2024.nlp4dh-1.35</url>
      <bibkey>rajaei-moghadam-etal-2024-text</bibkey>
    </paper>
    <paper id="36">
      <title>Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language</title>
      <author><first>Xinmeng</first><last>Hou</last></author>
      <pages>362–376</pages>
      <abstract>This study introduces a prescriptive annotation benchmark grounded in humanities research to ensure consistent, unbiased labeling of offensive language, particularly for casual and non-mainstream language uses. We contribute two newly annotated datasets that achieve higher inter-annotator agreement between human and language model (LLM) annotations compared to original datasets based on descriptive instructions. Our experiments show that LLMs can serve as effective alternatives when professional annotators are unavailable. Moreover, smaller models fine-tuned on multi-source LLM-annotated data outperform models trained on larger, single-source human-annotated datasets. These findings highlight the value of structured guidelines in reducing subjective variability, maintaining performance with limited data, and embracing language diversity. Content Warning: This article only analyzes offensive language for academic purposes. Discretion is advised.</abstract>
      <url hash="9dee7f91">2024.nlp4dh-1.36</url>
      <bibkey>hou-2024-mitigating</bibkey>
    </paper>
    <paper id="37">
      <title>Classification of Buddhist Verses: The Efficacy and Limitations of Transformer-Based Models</title>
      <author><first>Nikita</first><last>Neveditsin</last></author>
      <author><first>Ambuja</first><last>Salgaonkar</last></author>
      <author><first>Pawan</first><last>Lingras</last></author>
      <author><first>Vijay</first><last>Mago</last></author>
      <pages>377–385</pages>
      <abstract>This study assesses the ability of machine learning to classify verses from Buddhist texts into two categories: Therigatha and Theragatha, attributed to female and male authors, respectively. It highlights the difficulties in data preprocessing and the use of Transformer-based models on Devanagari script due to limited vocabulary, demonstrating that simple statistical models can be equally effective. The research suggests areas for future exploration, provides the dataset for further study, and acknowledges existing limitations and challenges.</abstract>
      <url hash="35b80208">2024.nlp4dh-1.37</url>
      <bibkey>neveditsin-etal-2024-classification</bibkey>
    </paper>
    <paper id="38">
      <title>Intersecting Register and Genre: Understanding the Contents of Web-Crawled Corpora</title>
      <author><first>Amanda</first><last>Myntti</last></author>
      <author><first>Liina</first><last>Repo</last></author>
      <author><first>Elian</first><last>Freyermuth</last></author>
      <author><first>Antti</first><last>Kanner</last></author>
      <author><first>Veronika</first><last>Laippala</last></author>
      <author><first>Erik</first><last>Henriksson</last></author>
      <pages>386–397</pages>
      <abstract>Web-scale corpora present valuable research opportunities but often lack detailed metadata, making them challenging to use in linguistics and social sciences. This study tackles this problem by exploring automatic methods to classify web corpora into specific categories, focusing on text registers such as Interactive Discussion and literary genres such as Politics and Social Sciences. We train two machine learning models to classify documents from the large web-crawled OSCAR dataset: a register classifier using the multilingual, manually annotated CORE corpus, and a genre classifier using a dataset based on Kindle US&amp;UK. Fine-tuned from XLM-R Large, the register and genre classifiers achieved F1-scores of 0.74 and 0.70, respectively. Our analysis includes evaluating the distribution of the predicted text classes and examining the intersection of genre-register pairs using topic modelling. The results show expected combinations between certain registers and genres, such as the Lyrical register often aligning with the Literature &amp; Fiction genre. However, most registers, such as Interactive Discussion, are divided across multiple genres, like Engineering &amp; Transportation and Politics &amp; Social Sciences, depending on the discussion topic. This enriched metadata provides valuable insights and supports new ways of studying digital cultural heritage.</abstract>
      <url hash="701119c3">2024.nlp4dh-1.38</url>
      <bibkey>myntti-etal-2024-intersecting</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>S</fixed-case>ui Generis: Large Language Models for Authorship Attribution and Verification in <fixed-case>L</fixed-case>atin</title>
      <author><first>Svetlana</first><last>Gorovaia</last></author>
      <author><first>Gleb</first><last>Schmidt</last></author>
      <author><first>Ivan P.</first><last>Yamshchikov</last></author>
      <pages>398–412</pages>
      <abstract>This paper evaluates the performance of Large Language Models (LLMs) in authorship attribu- tion and authorship verification tasks for Latin texts of the Patristic Era. The study showcases that LLMs can be robust in zero-shot author- ship verification even on short texts without sophisticated feature engineering. Yet, the mod- els can also be easily “mislead” by semantics. The experiments also demonstrate that steering the model’s authorship analysis and decision- making is challenging, unlike what is reported in the studies dealing with high-resource mod- ern languages. Although LLMs prove to be able to beat, under certain circumstances, the traditional baselines, obtaining a nuanced and truly explainable decision requires at best a lot of experimentation.</abstract>
      <url hash="25d29a46">2024.nlp4dh-1.39</url>
      <bibkey>gorovaia-etal-2024-sui</bibkey>
    </paper>
    <paper id="40">
      <title>Enhancing Neural Machine Translation for <fixed-case>A</fixed-case>inu-<fixed-case>J</fixed-case>apanese: A Comprehensive Study on the Impact of Domain and Dialect Integration</title>
      <author><first>Ryo</first><last>Igarashi</last></author>
      <author><first>So</first><last>Miyagawa</last></author>
      <pages>413–422</pages>
      <abstract>Neural Machine Translation (NMT) has revolutionized language translation, yet significant challenges persist for low-resource languages, particularly those with high dialectal variation and limited standardization. This comprehensive study focuses on the Ainu language, a critically endangered indigenous language of northern Japan, which epitomizes these challenges. We address the limitations of previous research through two primary strategies: (1) extensive corpus expansion encompassing diverse domains and dialects, and (2) development of innovative methods to incorporate dialect and domain information directly into the translation process. Our approach yielded substantial improvements in translation quality, with BLEU scores increasing from 32.90 to 39.06 (+6.16) for Japanese → Ainu and from 10.45 to 31.83 (+21.38) for Ainu → Japanese. Through rigorous experimentation and analysis, we demonstrate the crucial importance of integrating linguistic variation information in NMT systems for languages characterized by high diversity and limited resources. Our findings have broad implications for improving machine translation for other low-resource languages, potentially advancing preservation and revitalization efforts for endangered languages worldwide.</abstract>
      <url hash="d6a61e80">2024.nlp4dh-1.40</url>
      <bibkey>igarashi-miyagawa-2024-enhancing</bibkey>
    </paper>
    <paper id="41">
      <title>Exploring Large Language Models for Qualitative Data Analysis</title>
      <author><first>Tim</first><last>Fischer</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>423–437</pages>
      <abstract>This paper explores the potential of Large Language Models (LLMs) to enhance qualitative data analysis (QDA) workflows within the open-source QDA platform developed at our university. We identify several opportunities within a typical QDA workflow where AI assistance can boost researcher productivity and translate these opportunities into corresponding NLP tasks: document classification, information extraction, span classification, and text generation. A benchmark tailored to these QDA activities is constructed, utilizing English and German datasets that align with relevant use cases. Focusing on efficiency and accessibility, we evaluate the performance of three prominent open-source LLMs - Llama 3.1, Gemma 2, and Mistral NeMo - on this benchmark. Our findings reveal the promise of LLM integration for streamlining QDA workflows, particularly for English-language projects. Consequently, we have implemented the LLM Assistant as an opt-in feature within our platform and report the implementation details. With this, we hope to further democratize access to AI capabilities for qualitative data analysis.</abstract>
      <url hash="e8035a13">2024.nlp4dh-1.41</url>
      <bibkey>fischer-biemann-2024-exploring</bibkey>
    </paper>
    <paper id="42">
      <title>Cross-Dialectal Transfer and Zero-Shot Learning for <fixed-case>A</fixed-case>rmenian Varieties: A Comparative Analysis of <fixed-case>RNN</fixed-case>s, Transformers and <fixed-case>LLM</fixed-case>s</title>
      <author><first>Chahan</first><last>Vidal-Gorène</last></author>
      <author><first>Nadi</first><last>Tomeh</last></author>
      <author><first>Victoria</first><last>Khurshudyan</last></author>
      <pages>438–449</pages>
      <abstract>This paper evaluates lemmatization, POS-tagging, and morphological analysis for four Armenian varieties: Classical Armenian, Modern Eastern Armenian, Modern Western Armenian, and the under-documented Getashen dialect. It compares traditional RNN models, multilingual models like mDeBERTa, and large language models (ChatGPT) using supervised, transfer learning, and zero/few-shot learning approaches. The study finds that RNN models are particularly strong in POS-tagging, while large language models demonstrate high adaptability, especially in handling previously unseen dialect variations. The research highlights the value of cross-variational and in-context learning for enhancing NLP performance in low-resource languages, offering crucial insights into model transferability and supporting the preservation of endangered dialects.</abstract>
      <url hash="b478dbac">2024.nlp4dh-1.42</url>
      <bibkey>vidal-gorene-etal-2024-cross</bibkey>
    </paper>
    <paper id="43">
      <title>Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference for Cost-Effective Cultural Heritage Dataset Generation</title>
      <author><first>William</first><last>Thorne</last></author>
      <author><first>Ambrose</first><last>Robinson</last></author>
      <author><first>Bohua</first><last>Peng</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Diana</first><last>Maynard</last></author>
      <pages>450–462</pages>
      <abstract>As the cultural heritage sector increasingly adopts technologies like Retrieval-Augmented Generation (RAG) to provide more personalised search experiences and enable conversations with collections data, the demand for specialised evaluation datasets has grown. While end-to-end system testing is essential, it’s equally important to assess individual components. We target the final, answering task, which is well-suited to Machine Reading Comprehension (MRC). Although existing MRC datasets address general domains, they lack the specificity needed for cultural heritage information. Unfortunately, the manual creation of such datasets is prohibitively expensive for most heritage institutions. This paper presents a cost-effective approach for generating domain-specific MRC datasets with increased difficulty using Reinforcement Learning from Human Feedback (RLHF) from synthetic preference data. Our method leverages the performance of existing question-answering models on a subset of SQuAD to create a difficulty metric, assuming that more challenging questions are answered correctly less frequently. This research contributes: (1) A methodology for increasing question difficulty using PPO and synthetic data; (2) Empirical evidence of the method’s effectiveness, including human evaluation; (3) An in-depth error analysis and study of emergent phenomena; and (4) An open-source codebase and set of three llama-2-chat adapters for reproducibility and adaptation.</abstract>
      <url hash="a840052c">2024.nlp4dh-1.43</url>
      <bibkey>thorne-etal-2024-increasing</bibkey>
    </paper>
    <paper id="44">
      <title>Assessing Large Language Models in Translating <fixed-case>C</fixed-case>optic and <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek Ostraca</title>
      <author><first>Audric-Charles</first><last>Wannaz</last></author>
      <author><first>So</first><last>Miyagawa</last></author>
      <pages>463–471</pages>
      <abstract>The advent of Large Language Models (LLMs) substantially raised the quality and lowered the cost of Machine Translation (MT). Can scholars working with ancient languages draw benefits from this new technology? More specifically, can current MT facilitate multilingual digital papyrology? To answer this question, we evaluate 9 LLMs in the task of MT with 4 Coptic and 4 Ancient Greek ostraca into English using 6 NLP metrics. We argue that some models have already reached a performance apt to assist human experts. As can be expected from the difference in training corpus size, all models seem to perform better with Ancient Greek than with Coptic, where hallucinations are markedly more common. In the Coptic texts, the specialised Coptic Translator (CT) competes closely with Claude 3 Opus for the rank of most promising tool, while Claude 3 Opus and GPT-4o compete for the same position in the Ancient Greek texts. We argue that MT now substantially heightens the incentive to work on multilingual corpora. This could have a positive and long-lasting effect on Classics and Egyptology and help reduce the historical bias in translation availability. In closing, we reflect upon the need to meet AI-generated translations with an adequate critical stance.</abstract>
      <url hash="5f5db340">2024.nlp4dh-1.44</url>
      <bibkey>wannaz-miyagawa-2024-assessing</bibkey>
    </paper>
    <paper id="45">
      <title>The Social Lives of Literary Characters: Combining citizen science and language models to understand narrative social networks</title>
      <author><first>Andrew</first><last>Piper</last></author>
      <author><first>Michael</first><last>Xu</last></author>
      <author><first>Derek</first><last>Ruths</last></author>
      <pages>472–482</pages>
      <abstract>Characters and their interactions are central to the fabric of narratives, playing a crucial role in developing readers’ social cognition. In this paper, we introduce a novel annotation framework that distinguishes between five types of character interactions, including bilateral and unilateral classifications. Leveraging the crowd-sourcing framework of citizen science, we collect a large dataset of manual annotations (N=13,395). Using this data, we explore how genre and audience factors influence social network structures in a sample of contemporary books. Our findings demonstrate that fictional narratives tend to favor more embodied interactions and exhibit denser and less modular social networks. Our work not only enhances the understanding of narrative social networks but also showcases the potential of integrating citizen science with NLP methodologies for large-scale narrative analysis.</abstract>
      <url hash="286d791d">2024.nlp4dh-1.45</url>
      <bibkey>piper-etal-2024-social</bibkey>
    </paper>
    <paper id="46">
      <title>Multi-word expressions in biomedical abstracts and their plain <fixed-case>E</fixed-case>nglish adaptations</title>
      <author><first>Sergei</first><last>Bagdasarov</last></author>
      <author><first>Elke</first><last>Teich</last></author>
      <pages>483–488</pages>
      <abstract>This study analyzes the use of multi-word expressions (MWEs), prefabricated sequences of words (e.g. in this case, this means that, healthcare service, follow up) in biomedical abstracts and their plain language adaptations. While English academic writing became highly specialized and complex from the late 19th century onwards, recent decades have seen a rising demand for a lay-friendly language in scientific content, especially in the health domain, to bridge a communication gap between experts and laypersons. Based on previous research showing that MWEs are easier to process than non-formulaic word sequences of comparable length, we hypothesize that they can potentially be used to create a more reader-friendly language. Our preliminary results suggest some significant differences between complex and plain abstracts when it comes to the usage patterns and informational load of MWEs.</abstract>
      <url hash="7bc664dc">2024.nlp4dh-1.46</url>
      <bibkey>bagdasarov-teich-2024-multi</bibkey>
    </paper>
    <paper id="47">
      <title>Assessing the Performance of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>-4, Fine-tuned <fixed-case>BERT</fixed-case> and Traditional <fixed-case>ML</fixed-case> Models on <fixed-case>M</fixed-case>oroccan <fixed-case>A</fixed-case>rabic Sentiment Analysis</title>
      <author><first>Mohamed</first><last>Hannani</last></author>
      <author><first>Abdelhadi</first><last>Soudi</last></author>
      <author><first>Kristof</first><last>Van Laerhoven</last></author>
      <pages>489–498</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks across different languages. However, their performance in low-resource languages and dialects, such as Moroccan Arabic (MA), requires further investigation. This study evaluates the performance of ChatGPT-4, different fine-tuned BERT models, FastText as text representation, and traditional machine learning models on MA sentiment analysis. Experiments were done on two open source MA datasets: an X(Twitter) Moroccan Arabic corpus (MAC) and a Moroccan Arabic YouTube corpus (MYC) datasets to assess their capabilities on sentiment text classification. We compare the performance of fully fine-tuned and pre-trained Arabic BERT-based models with ChatGPT-4 in zero-shot settings.</abstract>
      <url hash="2d4b86d7">2024.nlp4dh-1.47</url>
      <bibkey>hannani-etal-2024-assessing</bibkey>
    </paper>
    <paper id="48">
      <title>Analyzing Pokémon and Mario Streamers’ Twitch Chat with <fixed-case>LLM</fixed-case>-based User Embeddings</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Jack</first><last>Rueter</last></author>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <pages>499–503</pages>
      <abstract>We present a novel digital humanities method for representing our Twitch chatters as user embeddings created by a large language model (LLM). We cluster these embeddings automatically using affinity propagation and further narrow this clustering down through manual analysis. We analyze the chat of one stream by each Twitch streamer: SmallAnt, DougDoug and PointCrow. Our findings suggest that each streamer has their own type of chatters, however two categories emerge for all of the streamers: supportive viewers and emoji and reaction senders. Repetitive message spammers is a shared chatter category for two of the streamers.</abstract>
      <url hash="805b19d8">2024.nlp4dh-1.48</url>
      <bibkey>hamalainen-etal-2024-analyzing</bibkey>
    </paper>
    <paper id="49">
      <title>Corpus Development Based on Conflict Structures in the Security Field and <fixed-case>LLM</fixed-case> Bias Verification</title>
      <author><first>Keito</first><last>Inoshita</last></author>
      <pages>504–512</pages>
      <abstract>This study investigates the presence of biases in large language models (LLMs), specifically focusing on how these models process and reflect inter-state conflict structures. Previous research has often lacked the standardized datasets necessary for a thorough and consistent evaluation of biases in this context. Without such datasets, it is challenging to accurately assess the impact of these biases on critical applications. To address this gap, we developed a diverse and high-quality corpus using a four-phase process. This process included generating texts based on international conflict-related keywords, enhancing emotional diversity to capture a broad spectrum of sentiments, validating the coherence and connections between texts, and conducting final quality assurance through human reviewers who are experts in natural language processing. Our analysis, conducted using this newly developed corpus, revealed subtle but significant negative biases in LLMs, particularly towards Eastern bloc countries such as Russia and China. These biases have the potential to influence decision-making processes in fields like national security and international relations, where accurate, unbiased information is crucial. The findings underscore the importance of evaluating and mitigating these biases to ensure the reliability and fairness of LLMs when applied in sensitive areas.</abstract>
      <url hash="42c59fe8">2024.nlp4dh-1.49</url>
      <bibkey>inoshita-2024-corpus</bibkey>
    </paper>
    <paper id="50">
      <title>Generating Interpretations of Policy Announcements</title>
      <author><first>Andreas</first><last>Marfurt</last></author>
      <author><first>Ashley</first><last>Thornton</last></author>
      <author><first>David</first><last>Sylvan</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <pages>513–520</pages>
      <abstract>Recent advances in language modeling have focused on (potentially multiple-choice) question answering, open-ended generation, or math and coding problems. We look at a more nuanced task: the interpretation of statements of political actors. To this end, we present a dataset of policy announcements and corresponding annotated interpretations, on the topic of US foreign policy relations with Russia in the years 1993 up to 2016. We analyze the performance of finetuning standard sequence-to-sequence models of varying sizes on predicting the annotated interpretations and compare them to few-shot prompted large language models. We find that 1) model size is not the main factor for success on this task, 2) finetuning smaller models provides both quantitatively and qualitatively superior results to in-context learning with large language models, but 3) large language models pick up the annotation format and approximate the category distribution with just a few in-context examples.</abstract>
      <url hash="eb588f62">2024.nlp4dh-1.50</url>
      <bibkey>marfurt-etal-2024-generating</bibkey>
    </paper>
    <paper id="51">
      <title>Order Up! Micromanaging Inconsistencies in <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>-4o Text Analyses</title>
      <author><first>Erkki</first><last>Mervaala</last></author>
      <author><first>Ilona</first><last>Kousa</last></author>
      <pages>521–535</pages>
      <abstract>Large language model (LLM) applications have taken the world by storm in the past two years, and the academic sphere has not been an exception. One common, cumbersome task for researchers to attempt to automatise has been text annotation and, to an extent, analysis. Popular LLMs such as ChatGPT have been examined as a research assistant and as an analysis tool, and several discrepancies regarding both transparency and the generative content have been uncovered. Our research approaches the usability and trustworthiness of ChatGPT for text analysis from the point of view of an “out-of-the-box” zero-shot or few-shot setting, focusing on how the context window and mixed text types affect the analyses generated. Results from our testing indicate that both the types of the texts and the ordering of different kinds of texts do affect the ChatGPT analysis, but also that the context-building is less likely to cause analysis deterioration when analysing similar texts. Though some of these issues are at the core of how LLMs function, many of these caveats can be addressed by transparent research planning.</abstract>
      <url hash="431dc36d">2024.nlp4dh-1.51</url>
      <bibkey>mervaala-kousa-2024-order</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>CIPHE</fixed-case>: A Framework for Document Cluster Interpretation and Precision from Human Exploration</title>
      <author><first>Anton</first><last>Eklund</last></author>
      <author><first>Mona</first><last>Forsman</last></author>
      <author><first>Frank</first><last>Drewes</last></author>
      <pages>536–548</pages>
      <abstract>Document clustering models serve unique application purposes, which turns model quality into a property that depends on the needs of the individual investigator. We propose a framework, Cluster Interpretation and Precision from Human Exploration (CIPHE), for collecting and quantifying human interpretations of cluster samples. CIPHE tasks survey participants to explore actual document texts from cluster samples and records their perceptions. It also includes a novel inclusion task that is used to calculate the cluster precision in an indirect manner. A case study on news clusters shows that CIPHE reveals which clusters have multiple interpretation angles, aiding the investigator in their exploration.</abstract>
      <url hash="69410fff">2024.nlp4dh-1.52</url>
      <bibkey>eklund-etal-2024-ciphe</bibkey>
    </paper>
    <paper id="53">
      <title>Empowering Teachers with Usability-Oriented <fixed-case>LLM</fixed-case>-Based Tools for Digital Pedagogy</title>
      <author><first>Melany Vanessa</first><last>Macias</last></author>
      <author><first>Lev</first><last>Kharlashkin</last></author>
      <author><first>Leo Einari</first><last>Huovinen</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <pages>549–557</pages>
      <abstract>We present our work on two LLM-based tools that utilize artificial intelligence and creative technology to improve education. The first tool is a Moodle AI plugin, which helps teachers manage their course content more efficiently using AI-driven analysis, content generation, and an interactive chatbot. The second one is a curriculum planning tool that provides insight into the sustainability, work-life relevance, and workload of each course. Both of these tools have the common goal of integrating sustainable development goals (UN SDGs) into teaching, among other things. We will describe the usability-focused and user-centric approach we have embraced when developing these tools.</abstract>
      <url hash="528e09f6">2024.nlp4dh-1.53</url>
      <bibkey>macias-etal-2024-empowering</bibkey>
    </paper>
  </volume>
</collection>
