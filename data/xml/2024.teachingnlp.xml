<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.teachingnlp">
  <volume id="1" ingest-date="2024-07-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Sixth Workshop on Teaching NLP</booktitle>
      <editor><first>Sana</first><last>Al-azzawi</last></editor>
      <editor><first>Laura</first><last>Biester</last></editor>
      <editor><first>György</first><last>Kovács</last></editor>
      <editor><first>Ana</first><last>Marasović</last></editor>
      <editor><first>Leena</first><last>Mathur</last></editor>
      <editor><first>Margot</first><last>Mieskes</last></editor>
      <editor><first>Leonie</first><last>Weissweiler</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="76fcc069">2024.teachingnlp-1</url>
      <venue>teachingnlp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="658e3be0">2024.teachingnlp-1.0</url>
      <bibkey>teachingnlp-2024-teaching</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Documenting the Unwritten Curriculum of Student Research</title>
      <author><first>Shomir</first><last>Wilson</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>1-3</pages>
      <abstract>Graduate and undergraduate student researchers in natural language processing (NLP) often need mentoring to learn the norms of research. While methodological and technical knowledge are essential, there is also a “hidden curriculum” of experiential knowledge about topics like work strategies, common obstacles, collaboration, conferences, and scholarly writing. As a professor, I have written a set of guides that cover typically unwritten customs and procedures for academic research. I share them with advisees to help them understand research norms and to help us focus on their specific questions and interests. This paper describes these guides, which are freely accessible on the web (https://shomir.net/advice), and I provide recommendations to faculty who are interested in creating similar materials for their advisees.</abstract>
      <url hash="b61b3076">2024.teachingnlp-1.1</url>
      <bibkey>wilson-2024-documenting-unwritten</bibkey>
    </paper>
    <paper id="2">
      <title>Example-Driven Course Slides on Natural Language Processing Concepts</title>
      <author><first>Natalie</first><last>Parde</last><affiliation>University of Illinois Chicago</affiliation></author>
      <pages>4-6</pages>
      <abstract>Natural language processing (NLP) is a fast-paced field and a popular course topic in many undergraduate and graduate programs. This paper presents a comprehensive suite of example-driven course slides covering NLP concepts, ranging from fundamental building blocks to modern state-of-the-art approaches. In contributing these slides, I hope to alleviate burden for those starting out as faculty or in need of course material updates. The slides are publicly available for external use and are updated regularly to incorporate new advancements.</abstract>
      <url hash="1444aff3">2024.teachingnlp-1.2</url>
      <bibkey>parde-2024-example-driven</bibkey>
    </paper>
    <paper id="3">
      <title>Industry vs Academia: Running a Course on Transformers in Two Setups</title>
      <author><first>Irina</first><last>Nikishina</last></author>
      <author><first>Maria</first><last>Tikhonova</last><affiliation>Higher School of Economics</affiliation></author>
      <author><first>Viktoriia</first><last>Chekalina</last></author>
      <author><first>Alexey</first><last>Zaytsev</last><affiliation>BIMSA</affiliation></author>
      <author><first>Artem</first><last>Vazhentsev</last><affiliation>Skolkovo Institute of Science and Technology and Artificial Intelligence Research Institute</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <pages>7-22</pages>
      <abstract>This paper presents a course on neural networks based on the Transformer architecture targeted at diverse groups of people from academia and industry with experience in Python, Machine Learning, and Deep Learning but little or no experience with Transformers. The course covers a comprehensive overview of the Transformers NLP applications and their use for other data types. The course features 15 sessions, each consisting of a lecture and a practical part, and two homework assignments organized as CodaLab competitions. The first six sessions of the course are devoted to the Transformer and the variations of this architecture (e.g., encoders, decoders, encoder-decoders) as well as different techniques of model tuning. Subsequent sessions are devoted to multilingualism, multimodality (e.g., texts and images), efficiency, event sequences, and tabular data.We ran the course for different audiences: academic students and people from industry. The first run was held in 2022. During the subsequent iterations until 2024, it was constantly updated and extended with recently emerged findings on GPT-4, LLMs, RLHF, etc. Overall, it has been ran six times (four times in industry and twice in academia) and received positive feedback from academic and industry students.</abstract>
      <url hash="da351aa7">2024.teachingnlp-1.3</url>
      <bibkey>nikishina-etal-2024-industry-vs</bibkey>
    </paper>
    <paper id="4">
      <title>Striking a Balance between Classical and Deep Learning Approaches in Natural Language Processing Pedagogy</title>
      <author><first>Aditya</first><last>Joshi</last><affiliation>UNSW</affiliation></author>
      <author><first>Jake</first><last>Renzella</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Saurav</first><last>Jha</last></author>
      <author><first>Xiangyu</first><last>Zhang</last></author>
      <pages>23-32</pages>
      <abstract>While deep learning approaches represent the state-of-the-art of natural language processing (NLP) today, classical algorithms and approaches still find a place in NLP textbooks and courses of recent years. This paper discusses the perspectives of conveners of two introductory NLP courses taught in Australia and India, and examines how classical and deep learning approaches can be balanced within the lecture plan and assessments of the courses. We also draw parallels with the objects-first and objects-later debate in CS1 education. We observe that teaching classical approaches adds value to student learning by building an intuitive understanding of NLP problems, potential solutions, and even deep learning models themselves. Despite classical approaches not being state-of-the-art, the paper makes a case for their inclusion in NLP courses today.</abstract>
      <url hash="1bb16831">2024.teachingnlp-1.4</url>
      <bibkey>joshi-etal-2024-striking-balance</bibkey>
    </paper>
    <paper id="5">
      <title>Co-Creational Teaching of Natural Language Processing</title>
      <author><first>John</first><last>McCrae</last><affiliation>National University of Ireland Galway</affiliation></author>
      <pages>33-42</pages>
      <abstract>Traditional lectures have poorer outcomes compared to active learning methodologies, yet many natural language processing classes in higher education still follow this outdated methodology. In this paper, we present, co-creational teaching, a methodology that encourages partnership between staff and lecturers and show how this can be applied to teach natural language processing. As a fast-moving and dynamic area of study with high interest from students, natural language processing is an ideal subject for innovative teaching methodologies to improve student outcomes. We detail our experience with teaching natural language processing through partnership with students and provide detailed descriptions of methodologies that can be used by others in their teaching, including considerations of diverse student populations.</abstract>
      <url hash="cbff15ea">2024.teachingnlp-1.5</url>
      <bibkey>mccrae-2024-co-creational</bibkey>
    </paper>
    <paper id="6">
      <title>Collaborative Development of Modular Open Source Educational Resources for Natural Language Processing</title>
      <author><first>Matthias</first><last>Aßenmacher</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Andreas</first><last>Stephan</last></author>
      <author><first>Leonie</first><last>Weissweiler</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Erion</first><last>Çano</last><affiliation>Universität Paderborn and Universität Vienna</affiliation></author>
      <author><first>Ingo</first><last>Ziegler</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>Marwin</first><last>Härttrich</last></author>
      <author><first>Bernd</first><last>Bischl</last><affiliation>LMU</affiliation></author>
      <author><first>Benjamin</first><last>Roth</last><affiliation>Universität Vienna</affiliation></author>
      <author><first>Christian</first><last>Heumann</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>43-53</pages>
      <abstract>In this work, we present a collaboratively and continuously developed open-source educational resource (OSER) for teaching natural language processing at two different universities. We shed light on the principles we followed for the initial design of the course and the rationale for ongoing developments, followed by a reflection on the inter-university collaboration for designing and maintaining teaching material. When reflecting on the latter, we explicitly emphasize the considerations that need to be made when facing heterogeneous groups and when having to accommodate multiple examination regulations within one single course framework. Relying on the fundamental principles of OSER developments as defined by Bothmann et al. (2023) proved to be an important guideline during this process. The final part pertains to open-sourcing our teaching material, coping with the increasing speed of developments in the field, and integrating the course digitally, also addressing conflicting priorities and challenges we are currently facing.</abstract>
      <url hash="cfb12c3c">2024.teachingnlp-1.6</url>
      <bibkey>assenmacher-etal-2024-collaborative-development</bibkey>
    </paper>
    <paper id="7">
      <title>From Hate Speech to Societal Empowerment: A Pedagogical Journey Through Computational Thinking and <fixed-case>NLP</fixed-case> for High School Students</title>
      <author><first>Alessandra Teresa</first><last>Cignarella</last><affiliation>aequa-tech srl</affiliation></author>
      <author><first>Elisa</first><last>Chierchiello</last></author>
      <author><first>Chiara</first><last>Ferrando</last></author>
      <author><first>Simona</first><last>Frenda</last><affiliation>University of Turin</affiliation></author>
      <author><first>Soda Marem</first><last>Lo</last></author>
      <author><first>Andrea</first><last>Marra</last></author>
      <pages>54-65</pages>
      <abstract>The teaching laboratory we have created integrates methodologies to address the topic of hate speech on social media among students while fostering computational thinking and AI education for societal impact. We provide a foundational understanding of hate speech and introduce computational concepts using matrices, bag of words, and practical exercises in platforms like Colaboratory. Additionally, we emphasize the application of AI, particularly in NLP, to address real-world challenges. Through retrospective evaluation, we assess the efficacy of our approach, aiming to empower students as proactive contributors to societal betterment. With this paper we present an overview of the laboratory’s structure, the primary materials used, and insights gleaned from six editions conducted to the present date.</abstract>
      <url hash="6728b272">2024.teachingnlp-1.7</url>
      <bibkey>cignarella-etal-2024-hate-speech</bibkey>
    </paper>
    <paper id="8">
      <title>Tightly Coupled Worksheets and Homework Assignments for <fixed-case>NLP</fixed-case></title>
      <author><first>Laura</first><last>Biester</last><affiliation>Middlebury College</affiliation></author>
      <author><first>Winston</first><last>Wu</last><affiliation>University of Hawaii at Hilo</affiliation></author>
      <pages>66-68</pages>
      <abstract>In natural language processing courses, students often struggle to debug their code. In this paper, we present three homework assignments that are tightly coupled with in-class worksheets. The worksheets allow students to confirm their understanding of the algorithms on paper before trying to write code. Then, as students complete the coding portion of the assignments, the worksheets aid students in the debugging process as test cases for the code, allowing students to seamlessly compare their results to those from the correct execution of the algorithm.</abstract>
      <url hash="89c7a96f">2024.teachingnlp-1.8</url>
      <bibkey>biester-wu-2024-tightly-coupled</bibkey>
    </paper>
    <paper id="9">
      <title>Teaching <fixed-case>LLM</fixed-case>s at <fixed-case>C</fixed-case>harles <fixed-case>U</fixed-case>niversity: Assignments and Activities</title>
      <author><first>Jindřich</first><last>Helcl</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Zdeněk</first><last>Kasner</last></author>
      <author><first>Ondřej</first><last>Dušek</last><affiliation>Charles University, Prague</affiliation></author>
      <author><first>Tomasz</first><last>Limisiewicz</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>Dominik</first><last>Macháček</last><affiliation>Charles University</affiliation></author>
      <author><first>Tomáš</first><last>Musil</last><affiliation>Charles University, Prague</affiliation></author>
      <author><first>Jindřich</first><last>Libovický</last><affiliation>Charles University Prague</affiliation></author>
      <pages>69-72</pages>
      <abstract>This paper presents teaching materials, particularly assignments and ideas for classroom activities, from a new course on large language modelsThe assignments include experiments with LLM inference for weather report generation and machine translation.The classroom activities include class quizzes, focused research on downstream tasks and datasets, and an interactive “best paper” session aimed at reading and comprehension of research papers.</abstract>
      <url hash="46310a9e">2024.teachingnlp-1.9</url>
      <bibkey>helcl-etal-2024-teaching-llms</bibkey>
    </paper>
    <paper id="10">
      <title>Empowering the Future with Multilinguality and Language Diversity</title>
      <author><first>En-Shiun</first><last>Lee</last></author>
      <author><first>Kosei</first><last>Uemura</last></author>
      <author><first>Syed</first><last>Wasti</last></author>
      <author><first>Mason</first><last>Shipton</last></author>
      <pages>73-76</pages>
      <abstract>The rapid advancements and the widespread transformation of Large Language Models, have made it necessary to incorporate these cutting-edge techniques into the educational curricula of Natural Language Processing (NLP) with limited computing resources. This paper presents an applied NLP course designed for upper-year computer science undergraduate students on state-of-the-art techniques with an emphasis on multilinguality and language diversity. We hope to empower learners to advance their language community while preparing for industry.</abstract>
      <url hash="2a2b2470">2024.teachingnlp-1.10</url>
      <bibkey>lee-etal-2024-empowering-future</bibkey>
    </paper>
    <paper id="11">
      <title>A Course Shared Task on Evaluating <fixed-case>LLM</fixed-case> Output for Clinical Questions</title>
      <author><first>Yufang</first><last>Hou</last><affiliation>Technische Universität Darmstadt and IBM Research Ireland</affiliation></author>
      <author><first>Thy Thy</first><last>Tran</last></author>
      <author><first>Doan Nam Long</first><last>Vu</last></author>
      <author><first>Yiwen</first><last>Cao</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <author><first>Kai</first><last>Li</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <author><first>Lukas</first><last>Rohde</last></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>77-80</pages>
      <abstract>This paper presents a shared task that we organized at the Foundations of Language Technology (FoLT) course in 2023/2024 at the Technical University of Darmstadt, which focuses on evaluating the output of Large Language Models (LLMs) in generating harmful answers to health-related clinical questions. We describe the task design considerations and report the feedback we received from the students. We expect the task and the findings reported in this paper to be relevant for instructors teaching natural language processing (NLP).</abstract>
      <url hash="ae833955">2024.teachingnlp-1.11</url>
      <bibkey>hou-etal-2024-course-shared</bibkey>
    </paper>
    <paper id="12">
      <title>A Prompting Assignment for Exploring Pretrained <fixed-case>LLM</fixed-case>s</title>
      <author><first>Carolyn</first><last>Anderson</last><affiliation>Wellesley College</affiliation></author>
      <pages>81-84</pages>
      <abstract>As the scale of publicly-available large language models (LLMs) has increased, so has interest in few-shot prompting methods. This paper presents an assignment that asks students to explore three aspects of large language model capabilities (commonsense reasoning, factuality, and wordplay) with a prompt engineering focus. The assignment consists of three tasks designed to share a common programming framework, so that students can reuse and adapt code from earlier tasks. Two of the tasks also involve dataset construction: students are asked to construct a simple dataset for the wordplay task, and a more challenging dataset for the factuality task. In addition, the assignment includes reflection questions that ask students to think critically about what they observe.</abstract>
      <url hash="6d45548e">2024.teachingnlp-1.12</url>
      <bibkey>anderson-2024-prompting-assignment</bibkey>
    </paper>
    <paper id="13">
      <title>Teaching Natural Language Processing in Law School</title>
      <author><first>Daniel</first><last>Braun</last><affiliation>University of Twente</affiliation></author>
      <pages>85-90</pages>
      <abstract>Fuelled by technical advances, the interest in Natural Language Processing in the legal domain has rapidly increased over the last months and years. The design, usage, and testing of domain-specific systems, but also assessing these systems from a legal perspective, needs competencies at the intersection of law and Natural Language Processing. While the demand for such competencies is high among students, only a few law schools, particularly in Europe, teach such competencies. In this paper, we present the design for a Natural Language Processing course for postgraduate law students that is based on the principle of constructive alignment and has proven to be successful over the last three years.</abstract>
      <url hash="8834ff1d">2024.teachingnlp-1.13</url>
      <bibkey>braun-2024-teaching-natural</bibkey>
    </paper>
    <paper id="14">
      <title>Exploring Language Representation through a Resource Inventory Project</title>
      <author><first>Carolyn</first><last>Anderson</last><affiliation>Wellesley College</affiliation></author>
      <pages>91-93</pages>
      <abstract>The increasing scale of large language models has led some students to wonder what contributions can be made in academia. However, students are often unaware that LLM-based approaches are not feasible for the majority of the world’s languages due to lack of data availability. This paper presents a research project in which students explore the issue of language representation by creating an inventory of the data, preprocessing, and model resources available for a less-resourced language. Students are put into small groups and assigned a language to research. Within the group, students take on one of three roles: dataset investigator, preprocessing investigator, or downstream task investigator. Students then work together to create a 7-page research report about their language.</abstract>
      <url hash="9a004e70">2024.teachingnlp-1.14</url>
      <bibkey>anderson-2024-exploring-language</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>BELT</fixed-case>: Building Endangered Language Technology</title>
      <author><first>Michael</first><last>Ginn</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>David</first><last>Saavedra-Beltrán</last></author>
      <author><first>Camilo</first><last>Robayo</last><affiliation>Universidad Nacional de Colombia</affiliation></author>
      <author><first>Alexis</first><last>Palmer</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <pages>94-104</pages>
      <abstract>The development of language technology (LT) for an endangered language is often identified as a goal in language revitalization efforts, but developing such technologies is typically subject to additional methodological challenges as well as social and ethical concerns. In particular, LT development has too often taken on colonialist qualities, extracting language data, relying on outside experts, and denying the speakers of a language sovereignty over the technologies produced.We seek to avoid such an approach through the development of the Building Endangered Language Technology (BELT) website, an educational resource designed for speakers and community members with limited technological experience to develop LTs for their own language. Specifically, BELT provides interactive lessons on basic Python programming, coupled with projects to develop specific language technologies, such as spellcheckers or word games. In this paper, we describe BELT’s design, the motivation underlying many key decisions, and preliminary responses from learners.</abstract>
      <url hash="4772a6d1">2024.teachingnlp-1.15</url>
      <bibkey>ginn-etal-2024-belt-building</bibkey>
    </paper>
    <paper id="16">
      <title>Training an <fixed-case>NLP</fixed-case> Scholar at a Small Liberal Arts College: A Backwards Designed Course Proposal</title>
      <author><first>Grusha</first><last>Prasad</last><affiliation>Colgate University</affiliation></author>
      <author><first>Forrest</first><last>Davis</last><affiliation>Colgate University</affiliation></author>
      <pages>105-118</pages>
      <abstract>The rapid growth in natural language processing (NLP) over the last couple yearshas generated student interest and excitement in learning more about the field. In this paper, we present two types of students that NLP courses might want to train. First, an “NLP engineer” who is able to flexibly design, build and apply new technologies in NLP for a wide range of tasks. Second, an “NLP scholar” who is able to pose, refine and answer questions in NLP and how it relates to the society, while also learning to effectively communicate these answers to a broader audience. While these two types of skills are not mutually exclusive — NLP engineers should be able to think critically, and NLP scholars should be able to build systems — we think that courses can differ in the balance of these skills. As educators at Small Liberal Arts Colleges, the strengths of our students and our institution favors an approach that is better suited to train NLP scholars. In this paper we articulate what kinds of skills an NLP scholar should have, and then adopt a backwards design to propose course components that can aid the acquisition of these skills.</abstract>
      <url hash="6fecea0c">2024.teachingnlp-1.16</url>
      <bibkey>prasad-davis-2024-training-nlp</bibkey>
    </paper>
    <paper id="17">
      <title>An Interactive Toolkit for Approachable <fixed-case>NLP</fixed-case></title>
      <author><first>AriaRay</first><last>Brown</last></author>
      <author><first>Julius</first><last>Steuer</last></author>
      <author><first>Marius</first><last>Mosbach</last><affiliation>McGill University and Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last><affiliation>Saarland University</affiliation></author>
      <pages>119-127</pages>
      <abstract>We present a novel tool designed for teaching and interfacing the information-theoretic modeling abilities of large language models. The Surprisal Toolkit allows students from diverse linguistic and programming backgrounds to learn about measures of information theory and natural language processing (NLP) through an online interactive tool. In addition, the interface provides a valuable research mechanism for obtaining measures of surprisal. We implement the toolkit as part of a classroom tutorial in three different learning scenarios and discuss the overall receptive student feedback. We suggest this toolkit and similar applications as resourceful supplements to instruction in NLP topics, especially for the purpose of balancing conceptual understanding with technical instruction, grounding abstract topics, and engaging students with varying coding abilities.</abstract>
      <url hash="5e4ca972">2024.teachingnlp-1.17</url>
      <bibkey>brown-etal-2024-interactive-toolkit</bibkey>
    </paper>
    <paper id="18">
      <title>Occam’s Razor and Bender and Koller’s Octopus</title>
      <author><first>Michael</first><last>Guerzhoy</last><affiliation>University of Toronto</affiliation></author>
      <pages>128-129</pages>
      <abstract>We discuss the teaching of the controversy surrounding Bender and Koller’s prominent 2020 paper, “Climbing toward NLU: On Meaning, Form, and Understanding in the Age of Data” (ACL 2020)We present what we understand to be the main contentions of the paper, and then recommend that the students engage with the natural counter-arguments to the claims in the paper.We attach teaching materials that we use to facilitate teaching this topic to undergraduate students.</abstract>
      <url hash="cfe94dba">2024.teachingnlp-1.18</url>
      <bibkey>guerzhoy-2024-occams-razor</bibkey>
    </paper>
  </volume>
</collection>
