<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.trustnlp">
  <volume id="1" ingest-date="2023-07-21" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)</booktitle>
      <editor><first>Anaelia</first><last>Ovalle</last></editor>
      <editor><first>Kai-Wei</first><last>Chang</last></editor>
      <editor><first>Ninareh</first><last>Mehrabi</last></editor>
      <editor><first>Yada</first><last>Pruksachatkun</last></editor>
      <editor><first>Aram</first><last>Galystan</last></editor>
      <editor><first>Jwala</first><last>Dhamala</last></editor>
      <editor><first>Apurv</first><last>Verma</last></editor>
      <editor><first>Trista</first><last>Cao</last></editor>
      <editor><first>Anoop</first><last>Kumar</last></editor>
      <editor><first>Rahul</first><last>Gupta</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="b924af08">2023.trustnlp-1</url>
      <venue>trustnlp</venue>
    </meta>
    <frontmatter>
      <url hash="65095a3b">2023.trustnlp-1.0</url>
      <bibkey>trustnlp-2023-trustworthy</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Towards Faithful Explanations for Text Classification with Robustness Improvement and Explanation Guided Training</title>
      <author><first>Dongfang</first><last>Li</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Baotian</first><last>Hu</last><affiliation>Harbin Institue of Technology, Shenzhen</affiliation></author>
      <author><first>Qingcai</first><last>Chen</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Shan</first><last>He</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>1-14</pages>
      <abstract>Feature attribution methods highlight the important input tokens as explanations to model predictions, which have been widely applied to deep neural networks towards trustworthy AI. However, recent works show that explanations provided by these methods face challenges of being faithful and robust. In this paper, we propose a method with Robustness improvement and Explanation Guided training towards more faithful EXplanations (REGEX) for text classification. First, we improve model robustness by input gradient regularization technique and virtual adversarial training. Secondly, we use salient ranking to mask noisy tokens and maximize the similarity between model attention and feature attribution, which can be seen as a self-training procedure without importing other external information. We conduct extensive experiments on six datasets with five attribution methods, and also evaluate the faithfulness in the out-of-domain setting. The results show that REGEX improves fidelity metrics of explanations in all settings and further achieves consistent gains based on two randomization tests. Moreover, we show that using highlight explanations produced by REGEX to train select-then-predict models results in comparable task performance to the end-to-end method.</abstract>
      <url hash="b155473a">2023.trustnlp-1.1</url>
      <bibkey>li-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Driving Context into Text-to-Text Privatization</title>
      <author><first>Stefan</first><last>Arnold</last><affiliation>FAU Erlangen-Nrnberg</affiliation></author>
      <author><first>Dilara</first><last>Yesilbas</last><affiliation>University of Erlangen-Nuremberg</affiliation></author>
      <author><first>Sven</first><last>Weinzierl</last><affiliation>FAU Erlangen-Nrnberg</affiliation></author>
      <pages>15-25</pages>
      <abstract>Metric Differential Privacy enables text-to-text privatization by adding calibrated noise to the vector of a word derived from an embedding space and projecting this noisy vector back to a discrete vocabulary using a nearest neighbor search. Since words are substituted without context, this mechanism is expected to fall short at finding substitutes for words with ambiguous meanings, such as ‘bank’. To account for these ambiguous words, we leverage a sense embedding and incorporate a sense disambiguation step prior to noise injection. We encompass our modification to the privatization mechanism with an estimation of privacy and utility. For word sense disambiguation on the Words in Context dataset, we demonstrate a substantial increase in classification accuracy by 6.05%.</abstract>
      <url hash="d42003e6">2023.trustnlp-1.2</url>
      <attachment type="SupplementaryMaterial" hash="08298fde">2023.trustnlp-1.2.SupplementaryMaterial.zip</attachment>
      <bibkey>arnold-etal-2023-driving</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.2</doi>
      <video href="2023.trustnlp-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models</title>
      <author><first>Pranav</first><last>Narayanan Venkit</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Mukund</first><last>Srinath</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Shomir</first><last>Wilson</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>26-34</pages>
      <abstract>We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the Bias Identification Test in Sentiment (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.</abstract>
      <url hash="34a0b7d7">2023.trustnlp-1.3</url>
      <bibkey>narayanan-venkit-etal-2023-automated</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Pay Attention to the Robustness of <fixed-case>C</fixed-case>hinese Minority Language Models! Syllable-level Textual Adversarial Attack on <fixed-case>T</fixed-case>ibetan Script</title>
      <author><first>Xi</first><last>Cao</last><affiliation>1.School of Information Science and Technology,Tibet University;2.Collaborative Innovation Center for Tibet Informatization by MOE and Tibet Autonomous Region</affiliation></author>
      <author><first>Dolma</first><last>Dawa</last><affiliation>1.School of Information Science and Technology,Tibet University;2.Collaborative Innovation Center for Tibet Informatization by MOE and Tibet Autonomous Region</affiliation></author>
      <author><first>Nuo</first><last>Qun</last><affiliation>1.School of Information Science and Technology,Tibet University;2.Collaborative Innovation Center for Tibet Informatization by MOE and Tibet Autonomous Region</affiliation></author>
      <author><first>Trashi</first><last>Nyima</last><affiliation>1.School of Information Science and Technology,Tibet University;2.Collaborative Innovation Center for Tibet Informatization by MOE and Tibet Autonomous Region</affiliation></author>
      <pages>35-46</pages>
      <abstract>The textual adversarial attack refers to an attack method in which the attacker adds imperceptible perturbations to the original texts by elaborate design so that the NLP (natural language processing) model produces false judgments. This method is also used to evaluate the robustness of NLP models. Currently, most of the research in this field focuses on English, and there is also a certain amount of research on Chinese. However, to the best of our knowledge, there is little research targeting Chinese minority languages. Textual adversarial attacks are a new challenge for the information processing of Chinese minority languages. In response to this situation, we propose a Tibetan syllable-level black-box textual adversarial attack called TSAttacker based on syllable cosine distance and scoring mechanism. And then, we conduct TSAttacker on six models generated by fine-tuning two PLMs (pre-trained language models) for three downstream tasks. The experiment results show that TSAttacker is effective and generates high-quality adversarial samples. In addition, the robustness of the involved models still has much room for improvement.</abstract>
      <url hash="763efb99">2023.trustnlp-1.4</url>
      <attachment type="SupplementaryMaterial" hash="9aa4415a">2023.trustnlp-1.4.SupplementaryMaterial.zip</attachment>
      <bibkey>cao-etal-2023-pay-attention</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.4</doi>
    </paper>
    <paper id="5">
      <title>Can we trust the evaluation on <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>?</title>
      <author><first>Rachith</first><last>Aiyappa</last><affiliation>Indiana University Bloomington</affiliation></author>
      <author><first>Jisun</first><last>An</last><affiliation>Indiana University Bloomington</affiliation></author>
      <author><first>Haewoon</first><last>Kwak</last><affiliation>Indiana University Bloomington</affiliation></author>
      <author><first>Yong-yeol</first><last>Ahn</last><affiliation>Indiana University</affiliation></author>
      <pages>47-54</pages>
      <abstract>ChatGPT, the first large language model with mass adoption, has demonstrated remarkableperformance in numerous natural language tasks. Despite its evident usefulness, evaluatingChatGPT’s performance in diverse problem domains remains challenging due to the closednature of the model and its continuous updates via Reinforcement Learning from HumanFeedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study in stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.</abstract>
      <url hash="710fb135">2023.trustnlp-1.5</url>
      <bibkey>aiyappa-etal-2023-trust</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.5</doi>
      <video href="2023.trustnlp-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Improving Factuality of Abstractive Summarization via Contrastive Reward Learning</title>
      <author><first>I-chun</first><last>Chern</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zhiruo</first><last>Wang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Sanjan</first><last>Das</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Bhavuk</first><last>Sharma</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>55-60</pages>
      <url hash="76c1efb8">2023.trustnlp-1.6</url>
      <attachment type="SupplementaryMaterial" hash="15d8ab09">2023.trustnlp-1.6.SupplementaryMaterial.zip</attachment>
      <bibkey>chern-etal-2023-improving</bibkey>
      <abstract>Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries. Code and human evaluation results will be publicly available at \url{https://github.com/EthanC111/factuality_summarization}.</abstract>
      <doi>10.18653/v1/2023.trustnlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Examining the Causal Impact of First Names on Language Models: The Case of Social Commonsense Reasoning</title>
      <author><first>Sullam</first><last>Jeoung</last><affiliation>UIUC</affiliation></author>
      <author><first>Jana</first><last>Diesner</last><affiliation>UIUC</affiliation></author>
      <author><first>Halil</first><last>Kilicoglu</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>61-72</pages>
      <abstract>As language models continue to be integrated into applications of personal and societal relevance, ensuring these models’ trustworthiness is crucial, particularly with respect to producing consistent outputs regardless of sensitive attributes. Given that first names may serve as proxies for (intersectional) socio-demographic representations, it is imperative to examine the impact of first names on commonsense reasoning capabilities. In this paper, we study whether a model’s reasoning given a specific input differs based on the first names provided. Our underlying assumption is that the reasoning about Alice should not differ from the reasoning about James. We propose and implement a controlled experimental framework to measure the causal effect of first names on commonsense reasoning, enabling us to distinguish between model predictions due to chance and caused by actual factors of interest. Our results indicate that the frequency of first names has a direct effect on model prediction, with less frequent names yielding divergent predictions compared to more frequent names. To gain insights into the internal mechanisms of models that are contributing to these behaviors, we also conduct an in-depth explainable analysis. Overall, our findings suggest that to ensure model robustness, it is essential to augment datasets with more diverse first names during the configuration stage.</abstract>
      <url hash="b81932de">2023.trustnlp-1.7</url>
      <attachment type="SupplementaryMaterial" hash="371cb5a3">2023.trustnlp-1.7.SupplementaryMaterial.zip</attachment>
      <bibkey>jeoung-etal-2023-examining</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Reliability Check: An Analysis of <fixed-case>GPT</fixed-case>-3’s Response to Sensitive Topics and Prompt Wording</title>
      <author><first>Aisha</first><last>Khatun</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Daniel</first><last>Brown</last><affiliation>University of Waterloo</affiliation></author>
      <pages>73-95</pages>
      <abstract>Large language models (LLMs) have become mainstream technology with their versatile use cases and impressive performance. Despite the countless out-of-the-box applications, LLMs are still not reliable. A lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and Reinforcement Learning with Human Feedback (RLHF), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. We find that GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes mistakes with common Misconceptions and Controversies. The model responses are inconsistent across prompts and settings, highlighting GPT-3’s unreliability.</abstract>
      <url hash="b931ed21">2023.trustnlp-1.8</url>
      <bibkey>khatun-brown-2023-reliability</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.8</doi>
      <video href="2023.trustnlp-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Sample Attackability in Natural Language Adversarial Attacks</title>
      <author><first>Vyas</first><last>Raina</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Mark</first><last>Gales</last><affiliation>Cambridge University</affiliation></author>
      <pages>96-107</pages>
      <abstract>Adversarial attack research in natural language processing (NLP) has made significant progress in designing powerful attack methods and defence approaches. However, few efforts have sought to identify which source samples are the most attackable or robust, i.e. can we determine for an unseen target model, which samples are the most vulnerable to an adversarial attack. This work formally extends the definition of sample attackability/robustness for NLP attacks. Experiments on two popular NLP datasets, four state of the art models and four different NLP adversarial attack methods, demonstrate that sample uncertainty is insufficient for describing characteristics of attackable/robust samples and hence a deep learning based detector can perform much better at identifying the most attackable and robust samples for an unseen target model. Nevertheless, further analysis finds that there is little agreement in which samples are considered the most attackable/robust across different NLP attack methods, explaining a lack of portability of attackability detection methods across attack methods.</abstract>
      <url hash="9c99c214">2023.trustnlp-1.9</url>
      <attachment type="SupplementaryMaterial" hash="d8a9fbbf">2023.trustnlp-1.9.SupplementaryMaterial.zip</attachment>
      <bibkey>raina-gales-2023-sample</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.9</doi>
      <video href="2023.trustnlp-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>A Keyword Based Approach to Understanding the Overpenalization of Marginalized Groups by <fixed-case>E</fixed-case>nglish Marginal Abuse Models on <fixed-case>T</fixed-case>witter</title>
      <author><first>Kyra</first><last>Yee</last><affiliation>NA</affiliation></author>
      <author><first>Alice</first><last>Schoenauer Sebag</last><affiliation>unaffiliated</affiliation></author>
      <author><first>Olivia</first><last>Redfield</last><affiliation/></author>
      <author><first>Matthias</first><last>Eck</last><affiliation>Twitter</affiliation></author>
      <author><first>Emily</first><last>Sheng</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Luca</first><last>Belli</last><affiliation>Twitter</affiliation></author>
      <pages>108-120</pages>
      <abstract>Harmful content detection models tend to have higher false positive rates for content from marginalized groups. In the context of marginal abuse modeling on Twitter, such disproportionate penalization poses the risk of reduced visibility, where marginalized communities lose the opportunity to voice their opinion on the platform. Current approaches to algorithmic harm mitigation, and bias detection for NLP models are often very ad hoc and subject to human bias. We make two main contributions in this paper. First, we design a novel methodology, which provides a principled approach to detecting and measuring the severity of potential harms associated with a text-based model. Second, we apply our methodology to audit Twitter’s English marginal abuse model, which is used for removing amplification eligibility of marginally abusive content. Without utilizing demographic labels or dialect classifiers, we are still able to detect and measure the severity of issues related to the over-penalization of the speech of marginalized communities, such as the use of reclaimed speech, counterspeech, and identity related terms. In order to mitigate the associated harms, we experiment with adding additional true negative examples and find that doing so provides improvements to our fairness metrics without large degradations in model performance.</abstract>
      <url hash="4d1c1aa1">2023.trustnlp-1.10</url>
      <bibkey>yee-etal-2023-keyword</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.10</doi>
    </paper>
    <paper id="11">
      <title>An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models</title>
      <author><first>Saghar</first><last>Hosseini</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Hamid</first><last>Palangi</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last><affiliation>Microsoft Research</affiliation></author>
      <pages>121-134</pages>
      <abstract>Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from massive human-written data which contains latent societal biases and toxic contents. In this paper, we leverage the primary task of PTLMs, i.e., language modeling, and propose a new metric to quantify manifested implicit representational harms in PTLMs towards 13 marginalized demographics. Using this metric, we conducted an empirical analysis of 24 widely used PTLMs. Our analysis provides insights into the correlation between the proposed metric in this work and other related metrics for representational harm. We observe that our metric correlates with most of the gender-specific metrics in the literature. Through extensive experiments, we explore the connections between PTLMs architectures and representational harms across two dimensions: depth and width of the networks. We found that prioritizing depth over width, mitigates representational harms in some PTLMs. Our code and data can be found at [place holder].</abstract>
      <url hash="bf4fe558">2023.trustnlp-1.11</url>
      <bibkey>hosseini-etal-2023-empirical</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.11</doi>
      <video href="2023.trustnlp-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Linguistic Properties of Truthful Response</title>
      <author><first>Bruce W.</first><last>Lee</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Benedict Florance</first><last>Arockiaraj</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Helen</first><last>Jin</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>135-140</pages>
      <abstract>We investigate the phenomenon of an LLM’s untruthful response using a large set of 220 handcrafted linguistic features. We focus on GPT-3 models and find that the linguistic profiles of responses are similar across model sizes. That is, how varying-sized LLMs respond to given prompts stays similar on the linguistic properties level. We expand upon this finding by training support vector machines that rely only upon the stylistic components of model responses to classify the truthfulness of statements. Though the dataset size limits our current findings, we present promising evidence that truthfulness detection is possible without evaluating the content itself. We release our code and raw data.</abstract>
      <url hash="3ed02f8c">2023.trustnlp-1.12</url>
      <attachment type="SupplementaryMaterial" hash="a555ea42">2023.trustnlp-1.12.SupplementaryMaterial.zip</attachment>
      <bibkey>lee-etal-2023-linguistic</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Debunking Biases in Attention</title>
      <author><first>Shijing</first><last>Chen</last><affiliation>The University of New South Wales</affiliation></author>
      <author><first>Usman</first><last>Naseem</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Imran</first><last>Razzak</last><affiliation>UNSW</affiliation></author>
      <pages>141-150</pages>
      <abstract>Despite the remarkable performances in various applications, machine learning (ML) models could potentially discriminate. They may result in biasness in decision-making, leading to an impact negatively on individuals and society. Recently, various methods have been developed to mitigate biasness and achieve significant performance. Attention mechanisms are a fundamental component of many state-of-the-art ML models and may potentially impact the fairness of ML models. However, how they explicitly influence fairness has yet to be thoroughly explored. In this paper, we investigate how different attention mechanisms affect the fairness of ML models, focusing on models used in Natural Language Processing (NLP) models. We evaluate the performance of fairness of several models with and without different attention mechanisms on widely used benchmark datasets. Our results indicate that the majority of attention mechanisms that have been assessed can improve the fairness performance of Bidirectional Gated Recurrent Unit (BiGRU) and Bidirectional Long Short-Term Memory (BiLSTM) in all three datasets regarding religious and gender-sensitive groups, however, with varying degrees of trade-offs in accuracy measures. Our findings highlight the possibility of fairness being affected by adopting specific attention mechanisms in machine learning models for certain datasets</abstract>
      <url hash="3830531a">2023.trustnlp-1.13</url>
      <attachment type="SupplementaryMaterial" hash="3a522f5d">2023.trustnlp-1.13.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="3a522f5d">2023.trustnlp-1.13.SupplementaryMaterial.zip</attachment>
      <bibkey>chen-etal-2023-debunking</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.13</doi>
    </paper>
    <paper id="14">
      <title>Guiding Text-to-Text Privatization by Syntax</title>
      <author><first>Stefan</first><last>Arnold</last><affiliation>FAU Erlangen-Nrnberg</affiliation></author>
      <author><first>Dilara</first><last>Yesilbas</last><affiliation>University of Erlangen-Nuremberg</affiliation></author>
      <author><first>Sven</first><last>Weinzierl</last><affiliation>FAU Erlangen-Nrnberg</affiliation></author>
      <pages>151-162</pages>
      <abstract>Metric Differential Privacy is a generalization of differential privacy tailored to address the unique challenges of text-to-text privatization. By adding noise to the representation of words in the geometric space of embeddings, words are replaced with words located in the proximity of the noisy representation. Since embeddings are trained based on word co-occurrences, this mechanism ensures that substitutions stem from a common semantic context. Without considering the grammatical category of words, however, this mechanism cannot guarantee that substitutions play similar syntactic roles. We analyze the capability of text-to-text privatization to preserve the grammatical category of words after substitution and find that surrogate texts consist almost exclusively of nouns. Lacking the capability to produce surrogate texts that correlate with the structure of the sensitive texts, we encompass our analysis by transforming the privatization step into a candidate selection problem in which substitutions are directed to words with matching grammatical properties. We demonstrate a substantial improvement in the performance of downstream tasks by up to 4.66% while retaining comparative privacy guarantees.</abstract>
      <url hash="61c812f8">2023.trustnlp-1.14</url>
      <attachment type="SupplementaryMaterial" hash="7f0c4445">2023.trustnlp-1.14.SupplementaryMaterial.zip</attachment>
      <bibkey>arnold-etal-2023-guiding</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Are fairness metric scores enough to assess discrimination biases in machine learning?</title>
      <author><first>Fanny</first><last>Jourdan</last><affiliation>IRIT Institut de Recherche en Informatique de Toulouse</affiliation></author>
      <author><first>Laurent</first><last>Risser</last><affiliation>CNRS, Institut de Mathematiques de Toulouse</affiliation></author>
      <author><first>Jean-michel</first><last>Loubes</last><affiliation>Universit Toulouse 3, Institut de Mathmatiques</affiliation></author>
      <author><first>Nicholas</first><last>Asher</last><affiliation>CNRS Institut de Recherche en Informatique de Toulouse</affiliation></author>
      <pages>163-174</pages>
      <abstract>This paper presents novel experiments shedding light on the shortcomings of current metrics for assessing biases of gender discrimination made by machine learning algorithms on textual data. We focus on the Bios dataset, and our learning task is to predict the occupation of individuals, based on their biography. Such prediction tasks are common in commercial Natural Language Processing (NLP) applications such as automatic job recommendations. We address an important limitation of theoretical discussions dealing with group-wise fairness metrics: they focus on large datasets, although the norm in many industrial NLP applications is to use small to reasonably large linguistic datasets for which the main practical constraint is to get a good prediction accuracy. We then question how reliable are different popular measures of bias when the size of the training set is simply sufficient to learn reasonably accurate predictions. Our experiments sample the Bios dataset and learn more than 200 models on different sample sizes. This allows us to statistically study our results and to confirm that common gender bias indices provide diverging and sometimes unreliable results when applied to relatively small training and test samples. This highlights the crucial importance of variance calculations for providing sound results in this field.</abstract>
      <url hash="857a0d70">2023.trustnlp-1.15</url>
      <bibkey>jourdan-etal-2023-fairness</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>DEPTH</fixed-case>+: An Enhanced Depth Metric for <fixed-case>W</fixed-case>ikipedia Corpora Quality</title>
      <author><first>Saied</first><last>Alshahrani</last><affiliation>Clarkson University</affiliation></author>
      <author><first>Norah</first><last>Alshahrani</last><affiliation>Clarkson University</affiliation></author>
      <author><first>Jeanna</first><last>Matthews</last><affiliation>Clarkson University</affiliation></author>
      <pages>175-189</pages>
      <abstract>Wikipedia articles are a common source of training data for Natural Language Processing (NLP) research, especially as a source for corpora in languages other than English. However, research has shown that not all Wikipedia editions are produced organically by native speakers, and there are substantial levels of automation and translation activities in the Wikipedia project that could negatively impact the degree to which they truly represent the language and the culture of native speakers. To encourage transparency in the Wikipedia project, Wikimedia Foundation introduced the depth metric as an indication of the degree of collaboration or how frequently users edit a Wikipedia edition’s articles. While a promising start, this depth metric suffers from a few serious problems, like a lack of adequate handling of inflation of edits metric and a lack of full utilization of users-related metrics. In this paper, we propose the DEPTH+ metric, provide its mathematical definitions, and describe how it reflects a better representation of the depth of human collaborativeness. We also quantify the bot activities in Wikipedia and offer a bot-free depth metric after the removal of the bot-created articles and the bot-made edits on the Wikipedia articles.</abstract>
      <url hash="e5f2fc3b">2023.trustnlp-1.16</url>
      <attachment type="SupplementaryMaterial" hash="1688db20">2023.trustnlp-1.16.SupplementaryMaterial.zip</attachment>
      <bibkey>alshahrani-etal-2023-depth</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Distinguishing Fact from Fiction: A Benchmark Dataset for Identifying Machine-Generated Scientific Papers in the <fixed-case>LLM</fixed-case> Era.</title>
      <author><first>Edoardo</first><last>Mosca</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Mohamed Hesham Ibrahim</first><last>Abdalla</last><affiliation>Technical University of Munich (TUM)</affiliation></author>
      <author><first>Paolo</first><last>Basso</last><affiliation>Politecnico di Milano</affiliation></author>
      <author><first>Margherita</first><last>Musumeci</last><affiliation>Politecnico di Milano</affiliation></author>
      <author><first>Georg</first><last>Groh</last><affiliation>TUM</affiliation></author>
      <pages>190-207</pages>
      <abstract>As generative NLP can now produce content nearly indistinguishable from human writing, it becomes difficult to identify genuine research contributions in academic writing and scientific publications. Moreover, information in NLP-generated text can potentially be factually wrong or even entirely fabricated. This study introduces a novel benchmark dataset, containing human-written and machine-generated scientific papers from SCIgen, GPT-2, GPT-3, ChatGPT, and Galactica. After describing the generation and extraction pipelines, we also experiment with four distinct classifiers as a baseline for detecting the authorship of scientific text. A strong focus is put on generalization capabilities and explainability to highlight the strengths and weaknesses of detectors. We believe our work serves as an important step towards creating more robust methods for distinguishing between human-written and machine-generated scientific papers, ultimately ensuring the integrity of scientific literature.</abstract>
      <url hash="fb23cf37">2023.trustnlp-1.17</url>
      <bibkey>mosca-etal-2023-distinguishing</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.17</doi>
    </paper>
    <paper id="18">
      <title>Detecting Personal Information in Training Corpora: an Analysis</title>
      <author><first>Nishant</first><last>Subramani</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Sasha</first><last>Luccioni</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Jesse</first><last>Dodge</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Margaret</first><last>Mitchell</last><affiliation>Hugging Face</affiliation></author>
      <pages>208-220</pages>
      <abstract>Large language models are trained on increasing quantities of unstructured text, the largest sources of which are scraped from the Web. These Web scrapes are mainly composed of heterogeneous collections of text from multiple domains with minimal documentation. While some work has been done to identify and remove toxic, biased, or sexual language, the topic of personal information (PI) in textual data used for training Natural Language Processing (NLP) models is relatively under-explored. In this work, we draw from definitions of PI across multiple countries to define the first PI taxonomy of its kind, categorized by type and risk level. We then conduct a case study on the Colossal Clean Crawled Corpus (C4) and the Pile, to detect some of the highest-risk personal information, such as email addresses and credit card numbers, and examine the differences between automatic and regular expression-based approaches for their detection. We identify shortcomings in modern approaches for PI detection, and propose a reframing of the problem that is informed by global perspectives and the goals in personal information detection.</abstract>
      <url hash="b595ebb3">2023.trustnlp-1.18</url>
      <bibkey>subramani-etal-2023-detecting</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Enhancing textual counterfactual explanation intelligibility through Counterfactual Feature Importance</title>
      <author><first>Milan</first><last>Bhan</last><affiliation>Sorbonne University</affiliation></author>
      <author><first>Jean-noel</first><last>Vittaut</last><affiliation>Sorbonne University</affiliation></author>
      <author><first>Nicolas</first><last>Chesneau</last><affiliation>Ekimetrics</affiliation></author>
      <author><first>Marie-jeanne</first><last>Lesot</last><affiliation>Sorbonne University</affiliation></author>
      <pages>221-231</pages>
      <abstract>Textual counterfactual examples explain a prediction by modifying the tokens of an initial instance in order to flip the outcome of a classifier. Even under sparsity constraint, counterfactual generation can lead to numerous changes from the initial text, making the explanation hard to understand. We propose Counterfactual Feature Importance, a method to make non-sparse counterfactual explanations more intelligible. Counterfactual Feature Importance assesses token change importance between an instance to explain and its counterfactual example. We develop two ways of computing Counterfactual Feature Importance, respectively based on classifier gradient computation and counterfactual generator loss evolution during counterfactual search. Then we design a global version of Counterfactual Feature Importance, providing rich information about semantic fields globally impacting classifier predictions. Counterfactual Feature Importance enables to focus on impacting parts of counterfactual explanations, making counterfactual explanations involving numerous changes more understandable.</abstract>
      <url hash="cac01307">2023.trustnlp-1.19</url>
      <attachment type="SupplementaryMaterial" hash="e38c8501">2023.trustnlp-1.19.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="e38c8501">2023.trustnlp-1.19.SupplementaryMaterial.zip</attachment>
      <bibkey>bhan-etal-2023-enhancing</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.19</doi>
    </paper>
    <paper id="20">
      <title>Privacy- and Utility-Preserving <fixed-case>NLP</fixed-case> with Anonymized data: A case study of Pseudonymization</title>
      <author><first>Oleksandr</first><last>Yermilov</last><affiliation>Ukrainian Catholic University</affiliation></author>
      <author><first>Vipul</first><last>Raheja</last><affiliation>Grammarly</affiliation></author>
      <author><first>Artem</first><last>Chernodub</last><affiliation>Grammarly</affiliation></author>
      <pages>232-241</pages>
      <abstract>This work investigates the effectiveness of different pseudonymization techniques, ranging from rule-based substitutions to using pre-trained Large Language Models (LLMs), on a variety of datasets and models used for two widely used NLP tasks: text classification and summarization. Our work provides crucial insights into the gaps between original and anonymized data (focusing on the pseudonymization technique) and model quality and fosters future research into higher-quality anonymization techniques better to balance the trade-offs between data protection and utility preservation. We make our code, pseudonymized datasets, and downstream models publicly available.</abstract>
      <url hash="479575d2">2023.trustnlp-1.20</url>
      <bibkey>yermilov-etal-2023-privacy</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>GPT</fixed-case>s Don’t Keep Secrets: Searching for Backdoor Watermark Triggers in Autoregressive Language Models</title>
      <author><first>Evan</first><last>Lucas</last><affiliation>Michigan Technological University</affiliation></author>
      <author><first>Timothy</first><last>Havens</last><affiliation>Michigan Technological University</affiliation></author>
      <pages>242-248</pages>
      <abstract>This work analyzes backdoor watermarks in an autoregressive transformer fine-tuned to perform a generative sequence-to-sequence task, specifically summarization. We propose and demonstrate an attack to identify trigger words or phrases by analyzing open ended generations from autoregressive models that have backdoor watermarks inserted. It is shown in our work that triggers based on random common words are easier to identify than those based on single, rare tokens. The attack proposed is easy to implement and only requires access to the model weights. Code used to create the backdoor watermarked models and analyze their outputs is shared at [github link to be inserted for camera ready version].</abstract>
      <url hash="2180368e">2023.trustnlp-1.21</url>
      <bibkey>lucas-havens-2023-gpts</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.21</doi>
      <video href="2023.trustnlp-1.21.mp4"/>
    </paper>
    <paper id="22">
      <title>Make Text Unlearnable: Exploiting Effective Patterns to Protect Personal Data</title>
      <author><first>Xinzhe</first><last>Li</last><affiliation>Deakin University</affiliation></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Deakin University</affiliation></author>
      <pages>249-259</pages>
      <abstract>This paper addresses the ethical concerns arising from the use of unauthorized public data in deep learning models and proposes a novel solution. Specifically, building on the work of Huang et al. (2021), we extend their bi-level optimization approach to generate unlearnable text using a gradient-based search technique. However, although effective, this approach faces practical limitations, including the requirement of batches of instances and model architecture knowledge that is not readily accessible to ordinary users with limited access to their own data. Furthermore, even with semantic-preserving constraints, unlearnable noise can alter the text’s semantics. To address these challenges, we extract simple patterns from unlearnable text produced by bi-level optimization and demonstrate that the data remains unlearnable for unknown models. Additionally, these patterns are not instance- or dataset-specific, allowing users to readily apply them to text classification and question-answering tasks, even if only a small proportion of users implement them on their public content. We also open-source codes to generate unlearnable text and assess unlearnable noise to benefit the public and future studies.</abstract>
      <url hash="c6e182c4">2023.trustnlp-1.22</url>
      <bibkey>li-liu-2023-make</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.22</doi>
    </paper>
    <paper id="23">
      <title>Training Data Extraction From Pre-trained Language Models: A Survey</title>
      <author><first>Shotaro</first><last>Ishihara</last><affiliation>Nikkei Inc.</affiliation></author>
      <pages>260-275</pages>
      <abstract>As the deployment of pre-trained language models (PLMs) expands, pressing security concerns have arisen regarding the potential for malicious extraction of training data, posing a threat to data privacy. This study is the first to provide a comprehensive survey of training data extraction from PLMs.Our review covers more than 100 key papers in fields such as natural language processing and security. First, preliminary knowledge is recapped and a taxonomy of various definitions of memorization is presented. The approaches for attack and defense are then systemized. Furthermore, the empirical findings of several quantitative studies are highlighted. Finally, future research directions based on this review are suggested.</abstract>
      <url hash="65652033">2023.trustnlp-1.23</url>
      <bibkey>ishihara-2023-training</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Expanding Scope: Adapting <fixed-case>E</fixed-case>nglish Adversarial Attacks to <fixed-case>C</fixed-case>hinese</title>
      <author><first>Hanyu</first><last>Liu</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Chengyuan</first><last>Cai</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Yanjun</first><last>Qi</last><affiliation>Faculty, CS, UVA</affiliation></author>
      <pages>276-286</pages>
      <abstract>Recent studies have revealed that NLP predictive models are vulnerable to adversarial attacks. Most existing studies focused on designing attacks to evaluate the robustness of NLP models in the English language alone. Literature has seen an increasing need for NLP solutions for other languages. We, therefore, ask one natural question whether state-of-the-art (SOTA) attack methods generalize to other languages. This paper investigates how to adapt SOTA adversarial attack algorithms in English to the Chinese language. Our experiments show that attack methods previously applied to English NLP can generate high-quality adversarial examples in Chinese when combined with proper text segmentation and linguistic constraints. In addition, we demonstrate that the generated adversarial examples can achieve high fluency and sentiment consistency by focusing on the Chinese language’s morphology and phonology, which in turn can be used to improve the adversarial robustness of Chinese NLP models.</abstract>
      <url hash="501cec89">2023.trustnlp-1.24</url>
      <bibkey>liu-etal-2023-expanding</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>IMBERT</fixed-case>: Making <fixed-case>BERT</fixed-case> Immune to Insertion-based Backdoor Attacks</title>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London</affiliation></author>
      <author><first>Jun</first><last>Wang</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Benjamin</first><last>Rubinstein</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Trevor</first><last>Cohn</last><affiliation>University of Melbourne</affiliation></author>
      <pages>287-301</pages>
      <abstract>Backdoor attacks are an insidious security threat against machine learning models. Adversaries can manipulate the predictions of compromised models by inserting triggers into the training phase. Various backdoor attacks have been devised which can achieve nearly perfect attack success without affecting model predictions for clean inputs. Means of mitigating such vulnerabilities are underdeveloped, especially in natural language processing. To fill this gap, we introduce IMBERT, which uses either gradients or self-attention scores derived from victim models to self-defend against backdoor attacks at inference time. Our empirical studies demonstrate that IMBERT can effectively identify up to 98.5% of inserted triggers. Thus, it significantly reduces the attack success rate while attaining competitive accuracy on the clean dataset across widespread insertion-based attacks compared to two baselines. Finally, we show that our approach is model-agnostic, and can be easily ported to several pre-trained transformer models.</abstract>
      <url hash="6da9b4fa">2023.trustnlp-1.25</url>
      <bibkey>he-etal-2023-imbert</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.25</doi>
    </paper>
    <paper id="26">
      <title>On The Real-world Performance of Machine Translation: Exploring Social Media Post-authors’ Perspectives</title>
      <author><first>Ananya</first><last>Gupta</last><affiliation>Clemson University</affiliation></author>
      <author><first>Jae</first><last>Takeuchi</last><affiliation>Clemson University</affiliation></author>
      <author><first>Bart</first><last>Knijnenburg</last><affiliation>Clemson University</affiliation></author>
      <pages>302-310</pages>
      <abstract>Many social networking sites (SNS) offer machine translation of posts in an effort to increase understanding, engagement, and connectivity between users across language barriers. However, the translations of these posts are still not 100% accurate and can be a cause of misunderstandings that can harm post-authors’ professional or personal relationships. An exacerbating factor is on most SNS, authors cannot view the translation of their own posts, nor make corrections to inaccurate translations. This paper reports findings from a survey (N = 189) and an interview (N = 15) to explore users’ concerns regarding this automatic form of machine translation. Our findings show that users are concerned about potential inaccuracies in the meaning of the translations of their posts, and would thus appreciate being able to view and potentially correct such translations. Additionally, we found that when users write posts in their native language, they write them for specific audiences, so they do not always want them translated. This underscores the urgency of providing users with more control over the translation of their posts.</abstract>
      <url hash="049be9a7">2023.trustnlp-1.26</url>
      <attachment type="SupplementaryMaterial" hash="4970d274">2023.trustnlp-1.26.SupplementaryMaterial.zip</attachment>
      <bibkey>gupta-etal-2023-real</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.26</doi>
    </paper>
    <paper id="27">
      <title>Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values</title>
      <author><first>Yejin</first><last>Bang</last><affiliation>the Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tiezheng</first><last>Yu</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Andrea</first><last>Madotto</last><affiliation>Meta</affiliation></author>
      <author><first>Zhaojiang</first><last>Lin</last><affiliation>Meta</affiliation></author>
      <author><first>Mona</first><last>Diab</last><affiliation>Meta Responsible AI</affiliation></author>
      <author><first>Pascale</first><last>Fung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>311-325</pages>
      <abstract>Many NLP classification tasks, such as sexism/racism detection or toxicity detection, are based on human values. Yet, human values can vary under diverse cultural conditions. Therefore, we introduce a framework for value-aligned classification that performs prediction based on explicitly written human values in the command. Along with the task, we propose a practical approach that distills value-aligned knowledge from large-scale language models (LLMs) to construct value-aligned classifiers in two steps. First, we generate value-aligned training data from LLMs by prompt-based few-shot learning. Next, we fine-tune smaller classification models with the generated data for the task. Empirical results show that our VA-Models surpass multiple baselines by at least 15.56% on the F1-score, including few-shot learning with OPT-175B and existing text augmentation methods. We suggest that using classifiers with explicit human value input improves both inclusivity &amp; explainability in AI.</abstract>
      <url hash="f79c760b">2023.trustnlp-1.27</url>
      <bibkey>bang-etal-2023-enabling</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.27</doi>
    </paper>
    <paper id="28">
      <title>Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement</title>
      <author><first>Gwenyth</first><last>Portillo Wightman</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Alexandra</first><last>Delucia</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Mark</first><last>Dredze</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>326-362</pages>
      <abstract>Large language models have achieved impressive few-shot performance on a wide variety of tasks. However, in many settings, users require confidence estimates for model predictions. While traditional classifiers produce scores for each label, language models instead produce scores for the generation which may not be well calibrated. We compare generations across diverse prompts and show that these can be used to create confidence scores. By utilizing more prompts we can get more precise confidence estimates and use response diversity as a proxy for confidence. We evaluate this approach across ten multiple-choice question-answering datasets using three models: T0, FLAN-T5, and GPT-3. In addition to analyzing multiple human written prompts, we automatically generate more prompts using a language model in order to produce finer-grained confidence estimates. Our method produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt. These improvements could benefit users who rely on prediction confidence for integration into a larger system or in decision-making processes.</abstract>
      <url hash="ea4049e6">2023.trustnlp-1.28</url>
      <bibkey>portillo-wightman-etal-2023-strength</bibkey>
      <doi>10.18653/v1/2023.trustnlp-1.28</doi>
    </paper>
  </volume>
</collection>
