<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.sltu">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)</booktitle>
      <editor><first>Dorothee</first><last>Beermann</last></editor>
      <editor><first>Laurent</first><last>Besacier</last></editor>
      <editor><first>Sakriani</first><last>Sakti</last></editor>
      <editor><first>Claudia</first><last>Soria</last></editor>
      <publisher>European Language Resources association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-35-1</isbn>
      <venue>sltu</venue>
    </meta>
    <frontmatter>
      <url hash="f2d4a780">2020.sltu-1.0</url>
      <bibkey>sltu-2020-joint</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Neural Models for Predicting <fixed-case>C</fixed-case>eltic Mutations</title>
      <author><first>Kevin</first><last>Scannell</last></author>
      <pages>1–8</pages>
      <abstract>The Celtic languages share a common linguistic phenomenon known as initial mutations; these consist of pronunciation and spelling changes that occur at the beginning of some words, triggered in certain semantic or syntactic contexts. Initial mutations occur quite frequently and all non-trivial NLP systems for the Celtic languages must learn to handle them properly. In this paper we describe and evaluate neural network models for predicting mutations in two of the six Celtic languages: Irish and Scottish Gaelic. We also discuss applications of these models to grammatical error detection and language modeling.</abstract>
      <url hash="33f99972">2020.sltu-1.1</url>
      <language>eng</language>
      <bibkey>scannell-2020-neural</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>E</fixed-case>idos: An Open-Source Auditory Periphery Modeling Toolkit and Evaluation of Cross-Lingual Phonemic Contrasts</title>
      <author><first>Alexander</first><last>Gutkin</last></author>
      <pages>9–20</pages>
      <abstract>Many analytical models that mimic, in varying degree of detail, the basic auditory processes involved in human hearing have been developed over the past decades. While the auditory periphery mechanisms responsible for transducing the sound pressure wave into the auditory nerve discharge are relatively well understood, the models that describe them are usually very complex because they try to faithfully simulate the behavior of several functionally distinct biological units involved in hearing. Because of this, there is a relative scarcity of toolkits that support combining publicly-available auditory models from multiple sources. We address this shortcoming by presenting an open-source auditory toolkit that integrates multiple models of various stages of human auditory processing into a simple and easily configurable pipeline, which supports easy switching between ten available models. The auditory representations that the pipeline produces can serve as machine learning features and provide analytical benchmark for comparing against auditory filters learned from the data. Given a low- and high-resource language pair, we evaluate several auditory representations on a simple multilingual phonemic contrast task to determine whether contrasts that are meaningful within a language are also empirically robust across languages.</abstract>
      <url hash="b18fedda">2020.sltu-1.2</url>
      <language>eng</language>
      <bibkey>gutkin-2020-eidos</bibkey>
    </paper>
    <paper id="3">
      <title>Open-Source High Quality Speech Datasets for <fixed-case>B</fixed-case>asque, <fixed-case>C</fixed-case>atalan and <fixed-case>G</fixed-case>alician</title>
      <author><first>Oddur</first><last>Kjartansson</last></author>
      <author><first>Alexander</first><last>Gutkin</last></author>
      <author><first>Alena</first><last>Butryna</last></author>
      <author><first>Isin</first><last>Demirsahin</last></author>
      <author><first>Clara</first><last>Rivera</last></author>
      <pages>21–27</pages>
      <abstract>This paper introduces new open speech datasets for three of the languages of Spain: Basque, Catalan and Galician. Catalan is furthermore the official language of the Principality of Andorra. The datasets consist of high-quality multi-speaker recordings of the three languages along with the associated transcriptions. The resulting corpora include over 33 hours of crowd-sourced recordings of 132 male and female native speakers. The recording scripts also include material for elicitation of global and local place names, personal and business names. The datasets are released under a permissive license and are available for free download for commercial, academic and personal use. The high-quality annotated speech datasets described in this paper can be used to, among other things, build text-to-speech systems, serve as adaptation data in automatic speech recognition and provide useful phonetic and phonological insights in corpus linguistics.</abstract>
      <url hash="7c050393">2020.sltu-1.3</url>
      <language>eng</language>
      <bibkey>kjartansson-etal-2020-open</bibkey>
    </paper>
    <paper id="4">
      <title>Two <fixed-case>LRL</fixed-case> &amp; Distractor Corpora from Web Information Retrieval and a Small Case Study in Language Identification without Training Corpora</title>
      <author><first>Armin</first><last>Hoenen</last></author>
      <author><first>Cemre</first><last>Koc</last></author>
      <author><first>Marc</first><last>Rahn</last></author>
      <pages>28–35</pages>
      <abstract>In recent years, low resource languages (LRLs) have seen a surge in interest after certain tasks have been solved for larger ones and as they present various challenges (data sparsity, sparsity of experts and expertise, unusual structural properties etc.). For a larger number of them in the wake of this interest resources and technologies have been created. However, there are very small languages for which this has not yet led to a significant change. We focus here one such language (Nogai) and one larger small language (Maori). Since especially smaller languages often face the situation of having very similar siblings or a larger small sister language which is more accessible, the rate of noise in data gathered on them so far is often high. Therefore, we present small corpora for our 2 case study languages which we obtained through web information retrieval and likewise for their noise inducing distractor languages and conduct a small language identification experiment where we identify documents in a boolean way as either belonging or not to the target language. We release our test corpora for two such scenarios in the format of the An Crubadan project (Scannell, 2007) and a tool for unsupervised language identification using alphabet and toponym information.</abstract>
      <url hash="ba80ea4d">2020.sltu-1.4</url>
      <language>eng</language>
      <bibkey>hoenen-etal-2020-two</bibkey>
    </paper>
    <paper id="5">
      <title>Morphological Disambiguation of <fixed-case>S</fixed-case>outh <fixed-case>S</fixed-case>ámi with <fixed-case>FST</fixed-case>s and Neural Networks</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Linda</first><last>Wiechetek</last></author>
      <pages>36–40</pages>
      <abstract>We present a method for conducting morphological disambiguation for South Sámi, which is an endangered language. Our method uses an FST-based morphological analyzer to produce an ambiguous set of morphological readings for each word in a sentence. These readings are disambiguated with a Bi-RNN model trained on the related North Sámi UD Treebank and some synthetically generated South Sámi data. The disambiguation is done on the level of morphological tags ignoring word forms and lemmas; this makes it possible to use North Sámi training data for South Sámi without the need for a bilingual dictionary or aligned word embeddings. Our approach requires only minimal resources for South Sámi, which makes it usable and applicable in the contexts of any other endangered language as well.</abstract>
      <url hash="f80e78b8">2020.sltu-1.5</url>
      <language>eng</language>
      <bibkey>hamalainen-wiechetek-2020-morphological</bibkey>
    </paper>
    <paper id="6">
      <title>Effects of Language Relatedness for Cross-lingual Transfer Learning in Character-Based Language Models</title>
      <author><first>Mittul</first><last>Singh</last></author>
      <author><first>Peter</first><last>Smit</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <pages>41–45</pages>
      <abstract>Character-based Neural Network Language Models (NNLM) have the advantage of smaller vocabulary and thus faster training times in comparison to NNLMs based on multi-character units. However, in low-resource scenarios, both the character and multi-character NNLMs suffer from data sparsity. In such scenarios, cross-lingual transfer has improved multi-character NNLM performance by allowing information transfer from a source to the target language. In the same vein, we propose to use cross-lingual transfer for character NNLMs applied to low-resource Automatic Speech Recognition (ASR). However, applying cross-lingual transfer to character NNLMs is not as straightforward. We observe that relatedness of the source language plays an important role in cross-lingual pretraining of character NNLMs. We evaluate this aspect on ASR tasks for two target languages: Finnish (with English and Estonian as source) and Swedish (with Danish, Norwegian, and English as source). Prior work has observed no difference between using the related or unrelated language for multi-character NNLMs. We, however, show that for character-based NNLMs, only pretraining with a related language improves the ASR performance, and using an unrelated language may deteriorate it. We also observe that the benefits are larger when there is much lesser target data than source data.</abstract>
      <url hash="adf5c404">2020.sltu-1.6</url>
      <language>eng</language>
      <bibkey>singh-etal-2020-effects</bibkey>
    </paper>
    <paper id="7">
      <title>Multilingual Graphemic Hybrid <fixed-case>ASR</fixed-case> with Massive Data Augmentation</title>
      <author><first>Chunxi</first><last>Liu</last></author>
      <author><first>Qiaochu</first><last>Zhang</last></author>
      <author><first>Xiaohui</first><last>Zhang</last></author>
      <author><first>Kritika</first><last>Singh</last></author>
      <author><first>Yatharth</first><last>Saraf</last></author>
      <author><first>Geoffrey</first><last>Zweig</last></author>
      <pages>46–52</pages>
      <abstract>Towards developing high-performing ASR for low-resource languages, approaches to address the lack of resources are to make use of data from multiple languages, and to augment the training data by creating acoustic variations. In this work we present a single grapheme-based ASR model learned on 7 geographically proximal languages, using standard hybrid BLSTM-HMM acoustic models with lattice-free MMI objective. We build the single ASR grapheme set via taking the union over each language-specific grapheme set, and we find such multilingual graphemic hybrid ASR model can perform language-independent recognition on all 7 languages, and substantially outperform each monolingual ASR model. Secondly, we evaluate the efficacy of multiple data augmentation alternatives within language, as well as their complementarity with multilingual modeling. Overall, we show that the proposed multilingual graphemic hybrid ASR with various data augmentation can not only recognize any within training set languages, but also provide large ASR performance improvements.</abstract>
      <url hash="f285cb96">2020.sltu-1.7</url>
      <language>eng</language>
      <bibkey>liu-etal-2020-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/audioset">AudioSet</pwcdataset>
    </paper>
    <paper id="8">
      <title>Neural Text-to-Speech Synthesis for an Under-Resourced Language in a Diglossic Environment: the Case of <fixed-case>G</fixed-case>ascon <fixed-case>O</fixed-case>ccitan</title>
      <author><first>Ander</first><last>Corral</last></author>
      <author><first>Igor</first><last>Leturia</last></author>
      <author><first>Aure</first><last>Séguier</last></author>
      <author><first>Michäel</first><last>Barret</last></author>
      <author><first>Benaset</first><last>Dazéas</last></author>
      <author><first>Philippe</first><last>Boula de Mareüil</last></author>
      <author><first>Nicolas</first><last>Quint</last></author>
      <pages>53–60</pages>
      <abstract>Occitan is a minority language spoken in Southern France, some Alpine Valleys of Italy, and the Val d’Aran in Spain, which only very recently started developing language and speech technologies. This paper describes the first project for designing a Text-to-Speech synthesis system for one of its main regional varieties, namely Gascon. We used a state-of-the-art deep neural network approach, the Tacotron2-WaveGlow system. However, we faced two additional difficulties or challenges: on the one hand, we wanted to test if it was possible to obtain good quality results with fewer recording hours than is usually reported for such systems; on the other hand, we needed to achieve a standard, non-Occitan pronunciation of French proper names, therefore we needed to record French words and test phoneme-based approaches. The evaluation carried out over the various developed systems and approaches shows promising results with near production-ready quality. It has also allowed us to detect the phenomena for which some flaws or fall of quality occur, pointing at the direction of future work to improve the quality of the actual system and for new systems for other language varieties and voices.</abstract>
      <url hash="51050726">2020.sltu-1.8</url>
      <language>eng</language>
      <bibkey>corral-etal-2020-neural</bibkey>
    </paper>
    <paper id="9">
      <title>Transfer Learning for Less-Resourced <fixed-case>S</fixed-case>emitic Languages Speech Recognition: the Case of <fixed-case>A</fixed-case>mharic</title>
      <author><first>Yonas</first><last>Woldemariam</last></author>
      <pages>61–69</pages>
      <abstract>While building automatic speech recognition (ASR) requires a large amount of speech and text data, the problem gets worse for less-resourced languages. In this paper, we investigate a model adaptation method, namely transfer learning for a less-resourced Semitic language i.e., Amharic, to solve resource scarcity problems in speech recognition development and improve the Amharic ASR model. In our experiments, we transfer acoustic models trained on two different source languages (English and Mandarin) to Amharic using very limited resources. The experimental results show that a significant WER (Word Error Rate) reduction has been achieved by transferring the hidden layers of the trained source languages neural networks. In the best case scenario, the Amharic ASR model adapted from English yields the best WER reduction from 38.72% to 24.50% (an improvement of 14.22% absolute). Adapting the Mandarin model improves the baseline Amharic model with a WER reduction of 10.25% (absolute). Our analysis also reveals that, the speech recognition performance of the adapted acoustic model is highly influenced by the relatedness (in a relative sense) between the source and the target languages than other considered factors (e.g. the quality of source models). Furthermore, other Semitic as well as Afro-Asiatic languages could benefit from the methodology presented in this study.</abstract>
      <url hash="5536e6c7">2020.sltu-1.9</url>
      <language>eng</language>
      <bibkey>woldemariam-2020-transfer</bibkey>
    </paper>
    <paper id="10">
      <title>Semi-supervised Acoustic Modelling for Five-lingual Code-switched <fixed-case>ASR</fixed-case> using Automatically-segmented Soap Opera Speech</title>
      <author><first>Nick</first><last>Wilkinson</last></author>
      <author><first>Astik</first><last>Biswas</last></author>
      <author><first>Emre</first><last>Yilmaz</last></author>
      <author><first>Febe</first><last>De Wet</last></author>
      <author><first>Ewald</first><last>Van der westhuizen</last></author>
      <author><first>Thomas</first><last>Niesler</last></author>
      <pages>70–78</pages>
      <abstract>This paper considers the impact of automatic segmentation on the fully-automatic, semi-supervised training of automatic speech recog-nition (ASR) systems for five-lingual code-switched (CS) speech. Four automatic segmentation techniques were evaluated in terms ofthe recognition performance of an ASR system trained on the resulting segments in a semi-supervised manner. For comparative purposesa semi-supervised syste Three of these use a newly proposed convolutional neural network (CNN) model for framewise classification,and include a novel form of HMM smoothing of the CNN outputs. Automatic segmentation was applied in combination with automaticspeaker diarization. The best-performing segmentation technique was also evaluated without speaker diarization. An evaluation basedon 248 unsegmented soap opera episodes indicated that voice activity detection (VAD) based on a CNN followed by Gaussian mixturemodel-hidden Markov model smoothing (CNN-GMM-HMM) yields the best ASR performance. The semi-supervised system trainedwith the best automatic segmentation achieved an overall WER improvement of 1.1% absolute over a semi-supervised system trainedwith manually created segments. Furthermore, we found that recognition rates improved even further when the automatic segmentationwas used in conjunction with speaker diarization.</abstract>
      <url hash="48b852d4">2020.sltu-1.10</url>
      <language>eng</language>
      <bibkey>wilkinson-etal-2020-semi</bibkey>
    </paper>
    <paper id="11">
      <title>Investigating Language Impact in Bilingual Approaches for Computational Language Documentation</title>
      <author><first>Marcely</first><last>Zanon Boito</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>79–87</pages>
      <abstract>For endangered languages, data collection campaigns have to accommodate the challenge that many of them are from oral tradition, and producing transcriptions is costly. Therefore, it is fundamental to translate them into a widely spoken language to ensure interpretability of the recordings. In this paper we investigate how the choice of translation language affects the posterior documentation work and potential automatic approaches which will work on top of the produced bilingual corpus. For answering this question, we use the MaSS multilingual speech corpus (Boito et al., 2020) for creating 56 bilingual pairs that we apply to the task of low-resource unsupervised word segmentation and alignment. Our results highlight that the choice of language for translation influences the word segmentation performance, and that different lexicons are learned by using different aligned translations. Lastly, this paper proposes a hybrid approach for bilingual word segmentation, combining boundary clues extracted from a non-parametric Bayesian model (Goldwater et al., 2009a) with the attentional word segmentation neural model from Godard et al. (2018). Our results suggest that incorporating these clues into the neural models’ input representation increases their translation and alignment quality, specially for challenging language pairs.</abstract>
      <url hash="9d94083e">2020.sltu-1.11</url>
      <language>eng</language>
      <bibkey>zanon-boito-etal-2020-investigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mass">MaSS</pwcdataset>
    </paper>
    <paper id="12">
      <title>Design and evaluation of a smartphone keyboard for <fixed-case>P</fixed-case>lains <fixed-case>C</fixed-case>ree syllabics</title>
      <author><first>Eddie</first><last>Santos</last></author>
      <author><first>Atticus</first><last>Harrigan</last></author>
      <pages>88–96</pages>
      <abstract>Plains Cree is a less-resourced language in Canada. To promote its usage online, we describe previous keyboard layouts for typing Plains Cree syllabics on smartphones. We describe our own solution whose development was guided by ergonomics research and corpus statistics. We then describe a case study in which three participants used a previous layout and our own, and we collected quantitative and qualitative data. We conclude that, despite observing accuracy improvements in user testing, introducing a brand new paradigm for typing Plains Cree syllabics may not be ideal for the community.</abstract>
      <url hash="27f5d716">2020.sltu-1.12</url>
      <language>eng</language>
      <bibkey>santos-harrigan-2020-design</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>S</fixed-case>eg: Parallel Data and Subword Information for Learning Bilingual Embeddings in Low Resource Scenarios</title>
      <author><first>Efsun</first><last>Sarioglu Kayi</last></author>
      <author><first>Vishal</first><last>Anand</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>97–105</pages>
      <abstract>Distributed word embeddings have become ubiquitous in natural language processing as they have been shown to improve performance in many semantic and syntactic tasks. Popular models for learning cross-lingual word embeddings do not consider the morphology of words. We propose an approach to learn bilingual embeddings using parallel data and subword information that is expressed in various forms, i.e. character n-grams, morphemes obtained by unsupervised morphological segmentation and byte pair encoding. We report results for three low resource morphologically rich languages (Swahili, Tagalog, and Somali) and a high resource language (German) in a simulated a low-resource scenario. Our results show that our method that leverages subword information outperforms the model without subword information, both in intrinsic and extrinsic evaluations of the learned embeddings. Specifically, analogy reasoning results show that using subwords helps capture syntactic characteristics. Semantically, word similarity results and intrinsically, word translation scores demonstrate superior performance over existing methods. Finally, qualitative analysis also shows better-quality cross-lingual embeddings particularly for morphological variants in both languages.</abstract>
      <url hash="2e8fa546">2020.sltu-1.13</url>
      <language>eng</language>
      <bibkey>sarioglu-kayi-etal-2020-multiseg</bibkey>
      <pwccode url="https://github.com/vishalanand/MultiSeg" additional="false">vishalanand/MultiSeg</pwccode>
    </paper>
    <paper id="14">
      <title>Poio Text Prediction: Lessons on the Development and Sustainability of <fixed-case>LT</fixed-case>s for Endangered Languages</title>
      <author><first>Gema</first><last>Zamora Fernández</last></author>
      <author><first>Vera</first><last>Ferreira</last></author>
      <author><first>Pedro</first><last>Manha</last></author>
      <pages>106–110</pages>
      <abstract>2019, the International Year of Indigenous Languages (IYIL), marked a crucial milestone for a diverse community united by a strong sense of urgency. In this presentation, we evaluate the impact of IYIL’s outcomes in the development of LTs for endangered languages. We give a brief description of the field of Language Documentation, whose experts have led the research and data collection efforts surrounding endangered languages for the past 30 years. We introduce the work of the Interdisciplinary Centre for Social and Language Documentation and we look at Poio as an example of an LT developed specifically with speakers of endangered languages in mind. This example illustrates how the deeper systemic causes of language endangerment are reflected in the development of LTs. Additionally, we share some of the strategic decisions that have led the development of this project. Finally, we advocate the importance of bridging the divide between research and activism, pushing for the inclusion of threatened languages in the world of LTs, and doing so in close collaboration with the speaker community.</abstract>
      <url hash="9a0d1bde">2020.sltu-1.14</url>
      <language>eng</language>
      <bibkey>zamora-fernandez-etal-2020-poio</bibkey>
    </paper>
    <paper id="15">
      <title>Text Corpora and the Challenge of Newly Written Languages</title>
      <author><first>Alice</first><last>Millour</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <pages>111–120</pages>
      <abstract>Text corpora represent the foundation on which most natural language processing systems rely. However, for many languages, collecting or building a text corpus of a sufficient size still remains a complex issue, especially for corpora that are accessible and distributed under a clear license allowing modification (such as annotation) and further resharing. In this paper, we review the sources of text corpora usually called upon to fill the gap in low-resource contexts, and how crowdsourcing has been used to build linguistic resources. Then, we present our own experiments with crowdsourcing text corpora and an analysis of the obstacles we encountered. Although the results obtained in terms of participation are still unsatisfactory, we advocate that the effort towards a greater involvement of the speakers should be pursued, especially when the language of interest is newly written.</abstract>
      <url hash="27f44afd">2020.sltu-1.15</url>
      <language>eng</language>
      <bibkey>millour-fort-2020-text</bibkey>
    </paper>
    <paper id="16">
      <title>Scaling Language Data Import/Export with a Data Transformer Interface</title>
      <author><first>Nicholas</first><last>Buckeridge</last></author>
      <author><first>Ben</first><last>Foley</last></author>
      <pages>121–125</pages>
      <abstract>This paper focuses on the technical improvement of Elpis, a language technology which assists people in the process of transcription, particularly for low-resource language documentation situations. To provide better support for the diversity of file formats encountered by people working to document the world’s languages, a Data Transformer interface has been developed to abstract the complexities of designing individual data import scripts. This work took place as part of a larger project of code quality improvement and the publication of template code that can be used for development of other language technologies.</abstract>
      <url hash="007f0046">2020.sltu-1.16</url>
      <language>eng</language>
      <bibkey>buckeridge-foley-2020-scaling</bibkey>
    </paper>
    <paper id="17">
      <title>Fully Convolutional <fixed-case>ASR</fixed-case> for Less-Resourced Endangered Languages</title>
      <author><first>Bao</first><last>Thai</last></author>
      <author><first>Robert</first><last>Jimerson</last></author>
      <author><first>Raymond</first><last>Ptucha</last></author>
      <author><first>Emily</first><last>Prud’hommeaux</last></author>
      <pages>126–130</pages>
      <abstract>The application of deep learning to automatic speech recognition (ASR) has yielded dramatic accuracy increases for languages with abundant training data, but languages with limited training resources have yet to see accuracy improvements on this scale. In this paper, we compare a fully convolutional approach for acoustic modelling in ASR with a variety of established acoustic modeling approaches. We evaluate our method on Seneca, a low-resource endangered language spoken in North America. Our method yields word error rates up to 40% lower than those reported using both standard GMM-HMM approaches and established deep neural methods, with a substantial reduction in training time. These results show particular promise for languages like Seneca that are both endangered and lack extensive documentation.</abstract>
      <url hash="7d7a2f11">2020.sltu-1.17</url>
      <language>eng</language>
      <bibkey>thai-etal-2020-fully</bibkey>
    </paper>
    <paper id="18">
      <title>Cross-Lingual Machine Speech Chain for <fixed-case>J</fixed-case>avanese, <fixed-case>S</fixed-case>undanese, <fixed-case>B</fixed-case>alinese, and <fixed-case>B</fixed-case>ataks Speech Recognition and Synthesis</title>
      <author><first>Sashi</first><last>Novitasari</last></author>
      <author><first>Andros</first><last>Tjandra</last></author>
      <author><first>Sakriani</first><last>Sakti</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>131–138</pages>
      <abstract>Even though over seven hundred ethnic languages are spoken in Indonesia, the available technology remains limited that could support communication within indigenous communities as well as with people outside the villages. As a result, indigenous communities still face isolation due to cultural barriers; languages continue to disappear. To accelerate communication, speech-to-speech translation (S2ST) technology is one approach that can overcome language barriers. However, S2ST systems require machine translation (MT), speech recognition (ASR), and synthesis (TTS) that rely heavily on supervised training and a broad set of language resources that can be difficult to collect from ethnic communities. Recently, a machine speech chain mechanism was proposed to enable ASR and TTS to assist each other in semi-supervised learning. The framework was initially implemented only for monolingual languages. In this study, we focus on developing speech recognition and synthesis for these Indonesian ethnic languages: Javanese, Sundanese, Balinese, and Bataks. We first separately train ASR and TTS of standard Indonesian in supervised training. We then develop ASR and TTS of ethnic languages by utilizing Indonesian ASR and TTS in a cross-lingual machine speech chain framework with only text or only speech data removing the need for paired speech-text data of those ethnic languages.</abstract>
      <url hash="c21b8541">2020.sltu-1.18</url>
      <language>eng</language>
      <bibkey>novitasari-etal-2020-cross</bibkey>
    </paper>
    <paper id="19">
      <title>Automatic <fixed-case>M</fixed-case>yanmar Image Captioning using <fixed-case>CNN</fixed-case> and <fixed-case>LSTM</fixed-case>-Based Language Model</title>
      <author><first>San</first><last>Pa Pa Aung</last></author>
      <author><first>Win</first><last>Pa Pa</last></author>
      <author><first>Tin Lay</first><last>Nwe</last></author>
      <pages>139–143</pages>
      <abstract>An image captioning system involves modules on computer vision as well as natural language processing. Computer vision module is for detecting salient objects or extracting features of images and Natural Language Processing (NLP) module is for generating correct syntactic and semantic image captions. Although many image caption datasets such as Flickr8k, Flickr30k and MSCOCO are publicly available, most of the datasets are captioned in English language. There is no image caption corpus for Myanmar language. Myanmar image caption corpus is manually built as part of the Flickr8k dataset in this current work. Furthermore, a generative merge model based on Convolutional Neural Network (CNN) and Long-Short Term Memory (LSTM) is applied especially for Myanmar image captioning. Next, two conventional feature extraction models Visual Geometry Group (VGG) OxfordNet 16-layer and 19-layer are compared. The performance of this system is evaluated on Myanmar image caption corpus using BLEU scores and 10-fold cross validation.</abstract>
      <url hash="d7e96ad0">2020.sltu-1.19</url>
      <language>eng</language>
      <bibkey>pa-pa-aung-etal-2020-automatic</bibkey>
    </paper>
    <paper id="20">
      <title>Phoneme Boundary Analysis using Multiway Geometric Properties of Waveform Trajectories</title>
      <author><first>Bhagath</first><last>Parabattina</last></author>
      <author><first>Pradip K.</first><last>Das</last></author>
      <pages>144–152</pages>
      <abstract>Automatic phoneme segmentation is an important problem in speech processing. It helps in improving the recognition quality by providing a proper segmentation information for phonemes or phonetic units. Inappropriate segmentation may lead to recognition falloff. The problem is essential not only for recognition but also for annotation purpose also. In general, segmentation algorithms rely on training large data sets where data is observed to find the patterns among them. But this process is not straight forward for languages that are under resourced because of less availability of datasets. In this paper, we propose a method that uses geometrical properties of waveform trajectory where intra signal variations are studied and used for segmentation. The method does not rely on large datasets for training. The geometric properties are extracted as linear structural changes in a raw waveform. The methods and findings of the study are presented.</abstract>
      <url hash="689fc45b">2020.sltu-1.20</url>
      <language>eng</language>
      <bibkey>parabattina-das-2020-phoneme</bibkey>
    </paper>
    <paper id="21">
      <title>Natural Language Processing Chains Inside a Cross-lingual Event-Centric Knowledge Pipeline for <fixed-case>E</fixed-case>uropean <fixed-case>U</fixed-case>nion Under-resourced Languages</title>
      <author><first>Diego</first><last>Alves</last></author>
      <author><first>Gaurish</first><last>Thakkar</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <pages>153–158</pages>
      <abstract>This article presents the strategy for developing a platform containing Language Processing Chains for European Union languages, consisting of Tokenization to Parsing, also including Named Entity recognition and with addition of Sentiment Analysis. These chains are part of the first step of an event-centric knowledge processing pipeline whose aim is to process multilingual media information about major events that can cause an impact in Europe and the rest of the world. Due to the differences in terms of availability of language resources for each language, we have built this strategy in three steps, starting with processing chains for the well-resourced languages and finishing with the development of new modules for the under-resourced ones. In order to classify all European Union official languages in terms of resources, we have analysed the size of annotated corpora as well as the existence of pre-trained models in mainstream Language Processing tools, and we have combined this information with the proposed classification published at META-NET whitepaper series.</abstract>
      <url hash="c953eb1b">2020.sltu-1.21</url>
      <language>eng</language>
      <bibkey>alves-etal-2020-natural</bibkey>
    </paper>
    <paper id="22">
      <title>Component Analysis of Adjectives in <fixed-case>L</fixed-case>uxembourgish for Detecting Sentiments</title>
      <author><first>Joshgun</first><last>Sirajzade</last></author>
      <author><first>Daniela</first><last>Gierschek</last></author>
      <author><first>Christoph</first><last>Schommer</last></author>
      <pages>159–166</pages>
      <abstract>The aim of this paper is to investigate the role of Luxembourgish adjectives in expressing sentiments in user comments written at the web presence of rtl.lu (RTL is the abbreviation for Radio Television Letzebuerg). Alongside many textual features or representations,adjectives could be used in order to detect sentiment, even on a sentence or comment level. In fact, they are also by themselves one of the best ways to describe a sentiment, despite the fact that other word classes such as nouns, verbs, adverbs or conjunctions can also be utilized for this purpose. The empirical part of this study focuses on a list of adjectives that were extracted from an annotated corpus. The corpus contains the part of speech tags of individual words and sentiment annotation on the adjective, sentence and comment level. Suffixes of Luxembourgish adjectives like -esch, -eg, -lech, -al, -el, -iv, -ent, -los, -barand the prefixon- were explicitly investigated, especially by paying attention to their role in regards to building a model by applying classical machine learning techniques. We also considered the interaction of adjectives with other grammatical means, especially other part of speeches, e.g. negations, which can completely reverse the meaning, thus the sentiment of an utterance.</abstract>
      <url hash="5a299e04">2020.sltu-1.22</url>
      <language>eng</language>
      <bibkey>sirajzade-etal-2020-component</bibkey>
    </paper>
    <paper id="23">
      <title>Acoustic-Phonetic Approach for <fixed-case>ASR</fixed-case> of Less Resourced Languages Using Monolingual and Cross-Lingual Information</title>
      <author><first>Shweta</first><last>Bansal</last></author>
      <pages>167–171</pages>
      <abstract>The exploration of speech processing for endangered languages has substantially increased in the past epoch of time. In this paper, we present the acoustic-phonetic approach for automatic speech recognition (ASR) using monolingual and cross-lingual information with application to under-resourced Indian languages, Punjabi, Nepali and Hindi. The challenging task while developing the ASR was the collection of the acoustic corpus for under-resourced languages. We have described here, in brief, the strategies used for designing the corpus and also highlighted the issues pertaining while collecting data for these languages. The bootstrap GMM-UBM based approach is used, which integrates pronunciation lexicon, language model and acoustic-phonetic model. Mel Frequency Cepstral Coefficients were used for extracting the acoustic signal features for training in monolingual and cross-lingual settings. The experimental result shows the overall performance of ASR for cross-lingual and monolingual. The phone substitution plays a key role in the cross-lingual as well as monolingual recognition. The result obtained by cross-lingual recognition compared with other baseline system and it has been found that the performance of the recognition system is based on phonemic units . The recognition rate of cross-lingual generally declines as compared with the monolingual.</abstract>
      <url hash="7fa62cee">2020.sltu-1.23</url>
      <language>eng</language>
      <bibkey>bansal-2020-acoustic</bibkey>
    </paper>
    <paper id="24">
      <title>An Annotation Framework for <fixed-case>L</fixed-case>uxembourgish Sentiment Analysis</title>
      <author><first>Joshgun</first><last>Sirajzade</last></author>
      <author><first>Daniela</first><last>Gierschek</last></author>
      <author><first>Christoph</first><last>Schommer</last></author>
      <pages>172–176</pages>
      <abstract>The aim of this paper is to present a framework developed for crowdsourcing sentiment annotation for the low-resource language Luxembourgish. Our tool is easily accessible through a web interface and facilitates sentence-level annotation of several annotators in parallel. In the heart of our framework is an XML database, which serves as central part linking several components. The corpus in the database consists of news articles and user comments. One of the components is LuNa, a tool for linguistic preprocessing of the data set. It tokenizes the text, splits it into sentences and assigns POS-tags to the tokens. After that, the preprocessed text is stored in XML format into the database. The Sentiment Annotation Tool, which is a browser-based tool, then enables the annotation of split sentences from the database. The Sentiment Engine, a separate module, is trained with this material in order to annotate the whole data set and analyze the sentiment of the comments over time and in relationship to the news articles. The gained knowledge can again be used to improve the sentiment classification on the one hand and on the other hand to understand the sentiment phenomenon from the linguistic point of view.</abstract>
      <url hash="7994927b">2020.sltu-1.24</url>
      <language>eng</language>
      <bibkey>sirajzade-etal-2020-annotation</bibkey>
    </paper>
    <paper id="25">
      <title>A Sentiment Analysis Dataset for Code-Mixed <fixed-case>M</fixed-case>alayalam-<fixed-case>E</fixed-case>nglish</title>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Navya</first><last>Jose</last></author>
      <author><first>Shardul</first><last>Suryawanshi</last></author>
      <author><first>Elizabeth</first><last>Sherly</last></author>
      <author><first>John Philip</first><last>McCrae</last></author>
      <pages>177–184</pages>
      <abstract>There is an increasing demand for sentiment analysis of text from social media which are mostly code-mixed. Systems trained on monolingual data fail for code-mixed data due to the complexity of mixing at different levels of the text. However, very few resources are available for code-mixed data to create models specific for this data. Although much research in multilingual and cross-lingual sentiment analysis has used semi-supervised or unsupervised methods, supervised methods still performs better. Only a few datasets for popular languages such as English-Spanish, English-Hindi, and English-Chinese are available. There are no resources available for Malayalam-English code-mixed data. This paper presents a new gold standard corpus for sentiment analysis of code-mixed text in Malayalam-English annotated by voluntary annotators. This gold standard corpus obtained a Krippendorff’s alpha above 0.8 for the dataset. We use this new corpus to provide the benchmark for sentiment analysis in Malayalam-English code-mixed texts.</abstract>
      <url hash="59bc1465">2020.sltu-1.25</url>
      <language>eng</language>
      <bibkey>chakravarthi-etal-2020-sentiment</bibkey>
      <pwccode url="https://github.com/bharathichezhiyan/MalayalamMixSentiment" additional="false">bharathichezhiyan/MalayalamMixSentiment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/malayalammixsentiment">MalayalamMixSentiment</pwcdataset>
    </paper>
    <paper id="26">
      <title>Speech-Emotion Detection in an <fixed-case>I</fixed-case>ndonesian Movie</title>
      <author><first>Fahmi</first><last>Fahmi</last></author>
      <author><first>Meganingrum Arista</first><last>Jiwanggi</last></author>
      <author><first>Mirna</first><last>Adriani</last></author>
      <pages>185–193</pages>
      <abstract>The growing demand to develop an automatic emotion recognition system for the Human-Computer Interaction field had pushed some research in speech emotion detection. Although it is growing, there is still little research about automatic speech emotion detection in Bahasa Indonesia. Another issue is the lack of standard corpus for this research area in Bahasa Indonesia. This study proposed several approaches to detect speech-emotion in the dialogs of an Indonesian movie by classifying them into 4 different emotion classes i.e. happiness, sadness, anger, and neutral. There are two different speech data representations used in this study i.e. statistical and temporal/sequence representations. This study used Artificial Neural Network (ANN), Recurrent Neural Network (RNN) with Long Short Term Memory (LSTM) variation, word embedding, and also the hybrid of three to perform the classification task. The best accuracies given by one-vs-rest scenario for each emotion class with speech-transcript pairs using hybrid of non-temporal and embedding approach are 1) happiness: 76.31%; 2) sadness: 86.46%; 3) anger: 82.14%; and 4) neutral: 68.51%. The multiclass classification resulted in 64.66% of precision, 66.79% of recall, and 64.83% of F1-score.</abstract>
      <url hash="d1c14584">2020.sltu-1.26</url>
      <language>eng</language>
      <bibkey>fahmi-etal-2020-speech</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>M</fixed-case>acsen: A Voice Assistant for Speakers of a Lesser Resourced Language</title>
      <author><first>Dewi</first><last>Jones</last></author>
      <pages>194–201</pages>
      <abstract>This paper reports on the development of a voice assistant mobile app for speakers of a lesser resourced language – Welsh. An assistant with a smaller set of effective but useful skills is both desirable and urgent for the wider Welsh speaking community. Descriptions of the app’s skills, architecture, design decisions and user interface is provided before elaborating on the most recent research and activities in open source speech technology for Welsh. The paper reports on the progress to date on crowdsourcing Welsh speech data in Mozilla Common Voice and of its suitability for training Mozilla’s DeepSpeech speech recognition for a voice assistant application according to conventional and transfer learning methods. We demonstrate that with smaller datasets of speech data, transfer learning and a domain specific language model, acceptable speech recognition is achievable that facilitates, as confirmed by beta users, a practical and useful voice assistant for Welsh speakers. We hope that this work informs and serves as a model to researchers and developers in other lesser-resourced linguistic communities and helps bring into being voice assistant apps for their languages.</abstract>
      <url hash="21215a5f">2020.sltu-1.27</url>
      <language>eng</language>
      <bibkey>jones-2020-macsen</bibkey>
      <pwccode url="https://github.com/techiaith/docker-deepspeech-cy" additional="true">techiaith/docker-deepspeech-cy</pwccode>
    </paper>
    <paper id="28">
      <title>Corpus Creation for Sentiment Analysis in Code-Mixed <fixed-case>T</fixed-case>amil-<fixed-case>E</fixed-case>nglish Text</title>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Vigneshwaran</first><last>Muralidaran</last></author>
      <author><first>Ruba</first><last>Priyadharshini</last></author>
      <author><first>John Philip</first><last>McCrae</last></author>
      <pages>202–210</pages>
      <abstract>Understanding the sentiment of a comment from a video or an image is an essential task in many applications. Sentiment analysis of a text can be useful for various decision-making processes. One such application is to analyse the popular sentiments of videos on social media based on viewer comments. However, comments from social media do not follow strict rules of grammar, and they contain mixing of more than one language, often written in non-native scripts. Non-availability of annotated code-mixed data for a low-resourced language like Tamil also adds difficulty to this problem. To overcome this, we created a gold standard Tamil-English code-switched, sentiment-annotated corpus containing 15,744 comment posts from YouTube. In this paper, we describe the process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark.</abstract>
      <url hash="240990a5">2020.sltu-1.28</url>
      <language>eng</language>
      <bibkey>chakravarthi-etal-2020-corpus</bibkey>
      <pwccode url="https://github.com/bharathichezhiyan/TamilMixSentiment" additional="false">bharathichezhiyan/TamilMixSentiment</pwccode>
    </paper>
    <paper id="29">
      <title>Gender Detection from Human Voice Using Tensor Analysis</title>
      <author><first>Prasanta</first><last>Roy</last></author>
      <author><first>Parabattina</first><last>Bhagath</last></author>
      <author><first>Pradip</first><last>Das</last></author>
      <pages>211–217</pages>
      <abstract>Speech-based communication is one of the most preferred modes of communication for humans. The human voice contains several important information and clues that help in interpreting the voice message. The gender of the speaker can be accurately guessed by a person based on the received voice of a speaker. The knowledge of the speaker’s gender can be a great aid to design accurate speech recognition systems. GMM based classifier is a popular choice used for gender detection. In this paper, we propose a Tensor-based approach for detecting the gender of a speaker and discuss its implementation details for low resourceful languages. Experiments were conducted using the TIMIT and SHRUTI dataset. An average gender detection accuracy of 91% is recorded. Analysis of the results with the proposed method is presented in this paper.</abstract>
      <url hash="d708a748">2020.sltu-1.29</url>
      <language>eng</language>
      <bibkey>roy-etal-2020-gender</bibkey>
    </paper>
    <paper id="30">
      <title>Data-Driven Parametric Text Normalization: Rapidly Scaling Finite-State Transduction Verbalizers to New Languages</title>
      <author><first>Sandy</first><last>Ritchie</last></author>
      <author><first>Eoin</first><last>Mahon</last></author>
      <author><first>Kim</first><last>Heiligenstein</last></author>
      <author><first>Nikos</first><last>Bampounis</last></author>
      <author><first>Daan</first><last>van Esch</last></author>
      <author><first>Christian</first><last>Schallhart</last></author>
      <author><first>Jonas</first><last>Mortensen</last></author>
      <author><first>Benoit</first><last>Brard</last></author>
      <pages>218–225</pages>
      <abstract>This paper presents a methodology for rapidly generating FST-based verbalizers for ASR and TTS systems by efficiently sourcing language-specific data. We describe a questionnaire which collects the necessary data to bootstrap the number grammar induction system and parameterize the verbalizer templates described in Ritchie et al. (2019), and a machine-readable data store which allows the data collected through the questionnaire to be supplemented by additional data from other sources. This system allows us to rapidly scale technologies such as ASR and TTS to more languages, including low-resource languages.</abstract>
      <url hash="2fb9504b">2020.sltu-1.30</url>
      <language>eng</language>
      <bibkey>ritchie-etal-2020-data</bibkey>
    </paper>
    <paper id="31">
      <title>Lenition and Fortition of Stop Codas in <fixed-case>R</fixed-case>omanian</title>
      <author><first>Mathilde</first><last>Hutin</last></author>
      <author><first>Oana</first><last>Niculescu</last></author>
      <author><first>Ioana</first><last>Vasilescu</last></author>
      <author><first>Lori</first><last>Lamel</last></author>
      <author><first>Martine</first><last>Adda-Decker</last></author>
      <pages>226–234</pages>
      <abstract>The present paper aims at providing a first study of lenition- and fortition-type phenomena in coda position in Romanian, a language that can be considered as less-resourced. Our data show that there are two contexts for devoicing in Romanian: before a voiceless obstruent, which means that there is regressive voicelessness assimilation in the language, and before pause, which means that there is a tendency towards final devoicing proper. The data also show that non-canonical voicing is an instance of voicing assimilation, as it is observed mainly before voiced consonants (voiced obstruents and sonorants alike). Two conclusions can be drawn from our analyses. First, from a phonetic point of view, the two devoicing phenomena exhibit the same behavior regarding place of articulation of the coda, while voicing assimilation displays the reverse tendency. In particular, alveolars, which tend to devoice the most, also voice the least. Second, the two assimilation processes have similarities that could distinguish them from final devoicing as such. Final devoicing seems to be sensitive to speech style and gender of the speaker, while assimilation processes do not. This may indicate that the two kinds of processes are phonologized at two different degrees in the language, assimilation being more accepted and generalized than final devoicing.</abstract>
      <url hash="344cdb25">2020.sltu-1.31</url>
      <language>eng</language>
      <bibkey>hutin-etal-2020-lenition-fortition</bibkey>
    </paper>
    <paper id="32">
      <title>Adapting a <fixed-case>W</fixed-case>elsh Terminology Tool to Develop a <fixed-case>C</fixed-case>ornish Dictionary</title>
      <author><first>Delyth</first><last>Prys</last></author>
      <pages>235–239</pages>
      <abstract>Cornish and Welsh are closely related Celtic languages and this paper provides a brief description of a recent project to publish an online bilingual English/Cornish dictionary, the Gerlyver Kernewek, based on similar work previously undertaken for Welsh. Both languages are endangered, Cornish critically so, but both can benefit from the use of language technology. Welsh has previous experience of using language technologies for language revitalization, and this is now being used to help the Cornish language create new tools and resources, including lexicographical ones, helping a dispersed team of language specialists and editors, many of them in a voluntary capacity, to work collaboratively online. Details are given of the Maes T dictionary writing and publication platform, originally developed for Welsh, and of some of the adaptations that had to be made to accommodate the specific needs of Cornish, including their use of Middle and Late varieties due to its development as a revived language.</abstract>
      <url hash="48eef228">2020.sltu-1.32</url>
      <language>eng</language>
      <bibkey>prys-2020-adapting</bibkey>
    </paper>
    <paper id="33">
      <title>Multiple Segmentations of <fixed-case>T</fixed-case>hai Sentences for Neural Machine Translation</title>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <author><first>Wichaya</first><last>Pidchamook</last></author>
      <author><first>Chao-Hong</first><last>Liu</last></author>
      <author><first>James</first><last>Hadley</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>240–244</pages>
      <abstract>Thai is a low-resource language, so it is often the case that data is not available in sufficient quantities to train an Neural Machine Translation (NMT) model which perform to a high level of quality. In addition, the Thai script does not use white spaces to delimit the boundaries between words, which adds more complexity when building sequence to sequence models. In this work, we explore how to augment a set of English–Thai parallel data by replicating sentence-pairs with different word segmentation methods on Thai, as training data for NMT model training. Using different merge operations of Byte Pair Encoding, different segmentations of Thai sentences can be obtained. The experiments show that combining these datasets, performance is improved for NMT models trained with a dataset that has been split using a supervised splitting tool.</abstract>
      <url hash="f0e2df18">2020.sltu-1.33</url>
      <language>eng</language>
      <bibkey>poncelas-etal-2020-multiple</bibkey>
    </paper>
    <paper id="34">
      <title>Automatic Extraction of Verb Paradigms in Regional Languages: the case of the Linguistic Crescent varieties</title>
      <author><first>Elena</first><last>Knyazeva</last></author>
      <author><first>Gilles</first><last>Adda</last></author>
      <author><first>Philippe</first><last>Boula de Mareüil</last></author>
      <author><first>Maximilien</first><last>Guérin</last></author>
      <author><first>Nicolas</first><last>Quint</last></author>
      <pages>245–249</pages>
      <abstract>Language documentation is crucial for endangered varieties all over the world. Verb conjugation is a key aspect of this documentation for Romance varieties such as those spoken in central France, in the area of the Linguistic Crescent, which extends overs significant portions of the old provinces of Marche and Bourbonnais. We present a first methodological experiment using automatic speech processing tools for the extraction of verbal paradigms collected and recorded during fieldworks sessions made in situ. In order to prove the feasibility of the approach, we test it with different protocols, on good quality data, and we offer possible ways of extension for this research.</abstract>
      <url hash="cb9ae4a7">2020.sltu-1.34</url>
      <language>eng</language>
      <bibkey>knyazeva-etal-2020-automatic</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>FST</fixed-case> Morphology for the Endangered <fixed-case>S</fixed-case>kolt <fixed-case>S</fixed-case>ami Language</title>
      <author><first>Jack</first><last>Rueter</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <pages>250–257</pages>
      <abstract>We present advances in the development of a FST-based morphological analyzer and generator for Skolt Sami. Like other minority Uralic languages, Skolt Sami exhibits a rich morphology, on the one hand, and there is little golden standard material for it, on the other. This makes NLP approaches for its study difficult without a solid morphological analysis. The language is severely endangered and the work presented in this paper forms a part of a greater whole in its revitalization efforts. Furthermore, we intersperse our description with facilitation and description practices not well documented in the infrastructure. Currently, the analyzer covers over 30,000 Skolt Sami words in 148 inflectional paradigms and over 12 derivational forms.</abstract>
      <url hash="95419412">2020.sltu-1.35</url>
      <language>eng</language>
      <bibkey>rueter-hamalainen-2020-fst</bibkey>
      <pwccode url="https://github.com/giellalt/lang-sms" additional="false">giellalt/lang-sms</pwccode>
    </paper>
    <paper id="36">
      <title>Voted-Perceptron Approach for <fixed-case>K</fixed-case>azakh Morphological Disambiguation</title>
      <author><first>Gulmira</first><last>Tolegen</last></author>
      <author><first>Alymzhan</first><last>Toleu</last></author>
      <author><first>Rustam</first><last>Mussabayev</last></author>
      <pages>258–264</pages>
      <abstract>This paper presents an approach of voted perceptron for morphological disambiguation for the case of Kazakh language. Guided by the intuition that the feature value from the correct path of analyses must be higher than the feature value of non-correct path of analyses, we propose the voted perceptron algorithm with Viterbi decoding manner for disambiguation. The approach can use arbitrary features to learn the feature vector for a sequence of analyses, which plays a vital role for disambiguation. Experimental results show that our approach outperforms other statistical and rule-based models. Moreover, we manually annotated a new morphological disambiguation corpus for Kazakh language.</abstract>
      <url hash="434e8b2b">2020.sltu-1.36</url>
      <language>eng</language>
      <bibkey>tolegen-etal-2020-voted</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>DNN</fixed-case>-Based Multilingual Automatic Speech Recognition for <fixed-case>W</fixed-case>olaytta using <fixed-case>O</fixed-case>romo Speech</title>
      <author><first>Martha Yifiru</first><last>Tachbelie</last></author>
      <author><first>Solomon Teferra</first><last>Abate</last></author>
      <author><first>Tanja</first><last>Schultz</last></author>
      <pages>265–270</pages>
      <abstract>It is known that Automatic Speech Recognition (ASR) is very useful for human-computer interaction in all the human languages. However, due to its requirement for a big speech corpus, which is very expensive, it has not been developed for most of the languages. Multilingual ASR (MLASR) has been suggested to share existing speech corpora among related languages to develop an ASR for languages which do not have the required speech corpora. Literature shows that phonetic relatedness goes across language families. We have, therefore, conducted experiments on MLASR taking two language families: one as source (Oromo from Cushitic) and the other as target (Wolaytta from Omotic). Using Oromo Deep Neural Network (DNN) based acoustic model, Wolaytta pronunciation dictionary and language model we have achieved Word Error Rate (WER) of 48.34% for Wolaytta. Moreover, our experiments show that adding only 30 minutes of speech data from the target language (Wolaytta) to the whole training data (22.8 hours) of the source language (Oromo) results in a relative WER reduction of 32.77%. Our results show the possibility of developing ASR system for a language, if we have pronunciation dictionary and language model, using an existing speech corpus of another language irrespective of their language family.</abstract>
      <url hash="1d9d0016">2020.sltu-1.37</url>
      <language>eng</language>
      <bibkey>tachbelie-etal-2020-dnn</bibkey>
    </paper>
    <paper id="38">
      <title>Building Language Models for Morphological Rich Low-Resource Languages using Data from Related Donor Languages: the Case of <fixed-case>U</fixed-case>yghur</title>
      <author><first>Ayimunishagu</first><last>Abulimiti</last></author>
      <author><first>Tanja</first><last>Schultz</last></author>
      <pages>271–276</pages>
      <abstract>Huge amounts of data are needed to build reliable statistical language models. Automatic speech processing tasks in low-resource languages typically suffer from lower performances due to weak or unreliable language models. Furthermore, language modeling for agglutinative languages is very challenging, as the morphological richness results in higher Out Of Vocabulary (OOV) rate. In this work, we show our effort to build word-based as well as morpheme-based language models for Uyghur, a language that combines both challenges, i.e. it is a low-resource and agglutinative language. Fortunately, there exists a closely-related rich-resource language, namely Turkish. Here, we present our work on leveraging Turkish text data to improve Uyghur language models. To maximize the overlap between Uyghur and Turkish words, the Turkish data is pre-processed on the word surface level, which results in 7.76% OOV-rate reduction on the Uyghur development set. To investigate various levels of low-resource conditions, different subsets of Uyghur data are generated. Morpheme-based language models trained with bilingual data achieved up to 40.91% relative perplexity reduction over the language models trained only with Uyghur data.</abstract>
      <url hash="20c3c747">2020.sltu-1.38</url>
      <language>eng</language>
      <bibkey>abulimiti-schultz-2020-building</bibkey>
    </paper>
    <paper id="39">
      <title>Basic Language Resources for 31 Languages (Plus <fixed-case>E</fixed-case>nglish): The <fixed-case>LORELEI</fixed-case> Representative and Incident Language Packs</title>
      <author><first>Jennifer</first><last>Tracey</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <pages>277–284</pages>
      <abstract>This paper documents and describes the thirty-one basic language resource packs created for the DARPA LORELEI program for use in development and testing of systems capable of providing language-independent situational awareness in emerging scenarios in a low resource language context. Twenty-four Representative Language Packs cover a broad range of language families and typologies, providing large volumes of monolingual and parallel text, smaller volumes of entity and semantic annotations, and a variety of grammatical resources and tools designed to support research into language universals and cross-language transfer. Seven Incident Language Packs provide test data to evaluate system capabilities on a previously unseen low resource language. We discuss the makeup of Representative and Incident Language Packs, the methods used to produce them, and the evolution of their design and implementation over the course of the multi-year LORELEI program. We conclude with a summary of the final language packs including their low-cost publication in the LDC catalog.</abstract>
      <url hash="6a9cd549">2020.sltu-1.39</url>
      <language>eng</language>
      <bibkey>tracey-strassel-2020-basic</bibkey>
    </paper>
    <paper id="40">
      <title>On the Exploration of <fixed-case>E</fixed-case>nglish to <fixed-case>U</fixed-case>rdu Machine Translation</title>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>Syeda</first><last>Abida</last></author>
      <author><first>Noor-e-</first><last>Hira</last></author>
      <author><first>Syeda</first><last>Zahra</last></author>
      <author><first>Dania</first><last>Parvez</last></author>
      <author><first>Javeria</first><last>Bashir</last></author>
      <author><first>Qurat-ul-ain</first><last>Majid</last></author>
      <pages>285–293</pages>
      <abstract>Machine Translation is the inevitable technology to reduce communication barriers in today’s world. It has made substantial progress in recent years and is being widely used in commercial as well as non-profit sectors. Such is only the case for European and other high resource languages. For English-Urdu language pair, the technology is in its infancy stage due to scarcity of resources. Present research is an important milestone in English-Urdu machine translation, as we present results for four major domains including Biomedical, Religious, Technological and General using Statistical and Neural Machine Translation. We performed series of experiments in attempts to optimize the performance of each system and also to study the impact of data sources on the systems. Finally, we established a comparison of the data sources and the effect of language model size on statistical machine translation performance.</abstract>
      <url hash="d45e0926">2020.sltu-1.40</url>
      <language>eng</language>
      <bibkey>abdul-rauf-etal-2020-exploration</bibkey>
    </paper>
    <paper id="41">
      <title>Developing a <fixed-case>T</fixed-case>wi (<fixed-case>A</fixed-case>sante) Dictionary from <fixed-case>A</fixed-case>kan Interlinear Glossed Texts</title>
      <author><first>Dorothee</first><last>Beermann</last></author>
      <author><first>Lars</first><last>Hellan</last></author>
      <author><first>Pavel</first><last>Mihaylov</last></author>
      <author><first>Anna</first><last>Struck</last></author>
      <pages>294–297</pages>
      <abstract>Traditionally, a lexicographer identifies the lexical items to be added to a dictionary. Here we present a corpus-based approach to dictionary compilation and describe a procedure that derives a Twi dictionary from a TypeCraft corpus of Interlinear Glossed Texts. We first extracted a list of unique words. We excluded words belonging to different dialects of Akan (mostly Fante and Abron). We corrected misspellings and distinguished English loan words to be integrated in our dictionary from instances of code switching. Next to the dictionary itself, one other resource arising from our work is a lexicographical model for Akan which represents the lexical resource itself, and the extended morphological and word class inventories that provide information to be aggregated. We also represent external resources such as the corpus that serves as the source and word level audio files. The Twi dictionary consists at present of 1367 words; it will be available online and from an open mobile app.</abstract>
      <url hash="46414bb3">2020.sltu-1.41</url>
      <language>eng</language>
      <bibkey>beermann-etal-2020-developing</bibkey>
    </paper>
    <paper id="42">
      <title>Adapting Language Specific Components of Cross-Media Analysis Frameworks to Less-Resourced Languages: the Case of <fixed-case>A</fixed-case>mharic</title>
      <author><first>Yonas</first><last>Woldemariam</last></author>
      <author><first>Adam</first><last>Dahlgren</last></author>
      <pages>298–305</pages>
      <abstract>We present an ASR based pipeline for Amharic that orchestrates NLP components within a cross media analysis framework (CMAF). One of the major challenges that are inherently associated with CMAFs is effectively addressing multi-lingual issues. As a result, many languages remain under-resourced and fail to leverage out of available media analysis solutions. Although spoken natively by over 22 million people and there is an ever-increasing amount of Amharic multimedia content on the Web, querying them with simple text search is difficult. Searching for, especially audio/video content with simple key words, is even hard as they exist in their raw form. In this study, we introduce a spoken and textual content processing workflow into a CMAF for Amharic. We design an ASR-named entity recognition (NER) pipeline that includes three main components: ASR, a transliterator and NER. We explore various acoustic modeling techniques and develop an OpenNLP-based NER extractor along with a transliterator that interfaces between ASR and NER. The designed ASR-NER pipeline for Amharic promotes the multi-lingual support of CMAFs. Also, the state-of-the art design principles and techniques employed in this study shed light for other less-resourced languages, particularly the Semitic ones.</abstract>
      <url hash="d9c253ee">2020.sltu-1.42</url>
      <language>eng</language>
      <bibkey>woldemariam-dahlgren-2020-adapting</bibkey>
    </paper>
    <paper id="43">
      <title>Phonemic Transcription of Low-Resource Languages: To What Extent can Preprocessing be Automated?</title>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>Séverine</first><last>Guillaume</last></author>
      <author><first>Alexis</first><last>Michaud</last></author>
      <pages>306–315</pages>
      <abstract>Automatic Speech Recognition for low-resource languages has been an active field of research for more than a decade. It holds promise for facilitating the urgent task of documenting the world’s dwindling linguistic diversity. Various methodological hurdles are encountered in the course of this exciting development, however. A well-identified difficulty is that data preprocessing is not at all trivial: data collected in classical fieldwork are usually tailored to the needs of the linguist who collects them, and there is baffling diversity in formats and annotation schema, even among fieldworkers who use the same software package (such as ELAN). The tests reported here (on Yongning Na and other languages from the Pangloss Collection, an open archive of endangered languages) explore some possibilities for automating the process of data preprocessing: assessing to what extent it is possible to bypass the involvement of language experts for menial tasks of data preparation for Natural Language Processing (NLP) purposes. What is at stake is the accessibility of language archive data for a range of NLP tasks and beyond.</abstract>
      <url hash="d7dc84be">2020.sltu-1.43</url>
      <language>eng</language>
      <bibkey>wisniewski-etal-2020-phonemic</bibkey>
    </paper>
    <paper id="44">
      <title>Manual Speech Synthesis Data Acquisition - From Script Design to Recording Speech</title>
      <author><first>Atli</first><last>Sigurgeirsson</last></author>
      <author><first>Gunnar</first><last>Örnólfsson</last></author>
      <author><first>Jón</first><last>Guðnason</last></author>
      <pages>316–320</pages>
      <abstract>Atli Þór Sigurgeirsson, atlithors@ru.is, Reykjavik University Gunnar Thor Örnólfsson, gunnarthor@hi.is, Árni Magnússon institute of Icelandic studies Dr. Jón Guðnason, jg@ru.is In this paper we present the work of collecting a large amount of high quality speech synthesis data for Icelandic. 8 speakers will be recorded for 20 hours each. A script design strategy is proposed and three scripts have been generated to maximize diphone coverage, varying in length. The largest reading script contains 14,400 prompts and includes 87.3% of all Icelandic diphones at least once and 81% of all Icelandic diphones at least twenty times. A recording client was developed to facilitate recording sessions. The client supports easily importing scripts and maintaining multiple collections in parallel. The recorded data can be downloaded straight from the client. Recording sessions are carried out in a professional studio under supervision and started October of 2019. As of writing, 58.7 hours of high quality speech data has been collected. The scripts, the recording software and the speech data will later be released under a CC-BY 4.0 license.</abstract>
      <url hash="0cac16a6">2020.sltu-1.44</url>
      <language>eng</language>
      <bibkey>sigurgeirsson-etal-2020-manual</bibkey>
    </paper>
    <paper id="45">
      <title>Owóksape - An Online Language Learning Platform for <fixed-case>L</fixed-case>akota</title>
      <author><first>Jan</first><last>Ullrich</last></author>
      <author><first>Elliot</first><last>Thornton</last></author>
      <author><first>Peter</first><last>Vieira</last></author>
      <author><first>Logan</first><last>Swango</last></author>
      <author><first>Marek</first><last>Kupiec</last></author>
      <pages>321–329</pages>
      <abstract>This paper presents Owóksape, an online language learning platform for the under-resourced language Lakota. The Lakota language (Lakȟótiyapi) is a Siouan language native to the United States with fewer than 2000 fluent speakers. Owóksape was developed by The Language Conservancy to support revitalization efforts, including reaching younger generations and providing a tool to complement traditional teaching methods. This project grew out of various multimedia resources in order to combine their most effective aspects into a single, self-paced learning tool. The first section of this paper discusses the motivation for and background of Owóksape. Section two details the linguistic features and language documentation principles that form the backbone of the platform. Section three lays out the unique integration of cultural aspects of the Lakota people into the visual design of the application. Section four explains the pedagogical principles of Owóksape. Application features and exercise types are then discussed in detail with visual examples, followed by an overview of the software design, as well as the effort required to develop the platform. Finally, a description of future features and considerations is presented.</abstract>
      <url hash="26a7b211">2020.sltu-1.45</url>
      <language>eng</language>
      <bibkey>ullrich-etal-2020-owoksape</bibkey>
    </paper>
    <paper id="46">
      <title>A Corpus of the <fixed-case>S</fixed-case>orani <fixed-case>K</fixed-case>urdish Folkloric Lyrics</title>
      <author><first>Sina</first><last>Ahmadi</last></author>
      <author><first>Hossein</first><last>Hassani</last></author>
      <author><first>Kamaladdin</first><last>Abedi</last></author>
      <pages>330–335</pages>
      <abstract>Kurdish poetry and prose narratives were historically transmitted orally and less in a written form. Being an essential medium of oral narration and literature, Kurdish lyrics have had a unique attribute in becoming a vital resource for different types of studies, including Digital Humanities, Computational Folkloristics and Computational Linguistics. As an initial study of its kind for the Kurdish language, this paper presents our efforts in transcribing and collecting Kurdish folk lyrics as a corpus that covers various Kurdish musical genres, in particular Beyt, Gorani, Bend, and Heyran. We believe that this corpus contributes to Kurdish language processing in several ways, such as compensation for the lack of a long history of written text by incorporating oral literature, presenting an unexplored realm in Kurdish language processing, and assisting the initiation of Kurdish computational folkloristics. Our corpus contains 49,582 tokens in the Sorani dialect of Kurdish. The corpus is publicly available in the Text Encoding Initiative (TEI) format for non-commercial use.</abstract>
      <url hash="0049280e">2020.sltu-1.46</url>
      <language>eng</language>
      <bibkey>ahmadi-etal-2020-corpus</bibkey>
      <pwccode url="https://github.com/KurdishBLARK/KurdishLyricsCorpus" additional="false">KurdishBLARK/KurdishLyricsCorpus</pwccode>
    </paper>
    <paper id="47">
      <title>Improving the Language Model for Low-Resource <fixed-case>ASR</fixed-case> with Online Text Corpora</title>
      <author><first>Nils</first><last>Hjortnaes</last></author>
      <author><first>Timofey</first><last>Arkhangelskiy</last></author>
      <author><first>Niko</first><last>Partanen</last></author>
      <author><first>Michael</first><last>Rießler</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>336–341</pages>
      <abstract>In this paper, we expand on previous work on automatic speech recognition in a low-resource scenario typical of data collected by field linguists. We train DeepSpeech models on 35 hours of dialectal Komi speech recordings and correct the output using language models constructed from various sources. Previous experiments showed that transfer learning using DeepSpeech can improve the accuracy of a speech recognizer for Komi, though the error rate remained very high. In this paper we present further experiments with language models created using KenLM from text materials available online. These are constructed from two corpora, one containing literary texts, one for social media content, and another combining the two. We then trained the model using each language model to explore the impact of the language model data source on the speech recognition model. Our results show significant improvements of over 25% in character error rate and nearly 20% in word error rate. This offers important methodological insight into how ASR results can be improved under low-resource conditions: transfer learning can be used to compensate the lack of training data in the target language, and online texts are a very useful resource when developing language models in this context.</abstract>
      <url hash="c27dfe91">2020.sltu-1.47</url>
      <language>eng</language>
      <bibkey>hjortnaes-etal-2020-improving</bibkey>
    </paper>
    <paper id="48">
      <title>A Summary of the First Workshop on Language Technology for Language Documentation and Revitalization</title>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Shruti</first><last>Rijhwani</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <author><first>Jordan</first><last>MacKenzie</last></author>
      <author><first>Hilaria</first><last>Cruz</last></author>
      <author><first>Xinjian</first><last>Li</last></author>
      <author><first>Matthew</first><last>Lee</last></author>
      <author><first>Aditi</first><last>Chaudhary</last></author>
      <author><first>Luke</first><last>Gessler</last></author>
      <author><first>Steven</first><last>Abney</last></author>
      <author><first>Shirley Anugrah</first><last>Hayati</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Olga</first><last>Zamaraeva</last></author>
      <author><first>Emily</first><last>Prud’hommeaux</last></author>
      <author><first>Jennette</first><last>Child</last></author>
      <author><first>Sara</first><last>Child</last></author>
      <author><first>Rebecca</first><last>Knowles</last></author>
      <author><first>Sarah</first><last>Moeller</last></author>
      <author><first>Jeffrey</first><last>Micher</last></author>
      <author><first>Yiyuan</first><last>Li</last></author>
      <author><first>Sydney</first><last>Zink</last></author>
      <author><first>Mengzhou</first><last>Xia</last></author>
      <author><first>Roshan S</first><last>Sharma</last></author>
      <author><first>Patrick</first><last>Littell</last></author>
      <pages>342–351</pages>
      <abstract>Despite recent advances in natural language processing and other language technology, the application of such technology to language documentation and conservation has been limited. In August 2019, a workshop was held at Carnegie Mellon University in Pittsburgh, PA, USA to attempt to bring together language community members, documentary linguists, and technologists to discuss how to bridge this gap and create prototypes of novel and practical language revitalization technologies. The workshop focused on developing technologies to aid language documentation and revitalization in four areas: 1) spoken language (speech transcription, phone to orthography decoding, text-to-speech and text-speech forced alignment), 2) dictionary extraction and management, 3) search tools for corpora, and 4) social media (language learning bots and social media analysis). This paper reports the results of this workshop, including issues discussed, and various conceived and implemented technologies for nine languages: Arapaho, Cayuga, Inuktitut, Irish Gaelic, Kidaw’ida, Kwak’wala, Ojibwe, San Juan Quiahije Chatino, and Seneca.</abstract>
      <url hash="e9cc23b9">2020.sltu-1.48</url>
      <language>eng</language>
      <bibkey>neubig-etal-2020-summary</bibkey>
    </paper>
    <paper id="49">
      <title>“A Passage to <fixed-case>I</fixed-case>ndia”: Pre-trained Word Embeddings for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Saurav</first><last>Kumar</last></author>
      <author><first>Saunack</first><last>Kumar</last></author>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>352–357</pages>
      <abstract>Dense word vectors or ‘word embeddings’ which encode semantic properties of words, have now become integral to NLP tasks like Machine Translation (MT), Question Answering (QA), Word Sense Disambiguation (WSD), and Information Retrieval (IR). In this paper, we use various existing approaches to create multiple word embeddings for 14 Indian languages. We place these embeddings for all these languages, <i>viz.</i>, Assamese, Bengali, Gujarati, Hindi, Kannada, Konkani, Malayalam, Marathi, Nepali, Odiya, Punjabi, Sanskrit, Tamil, and Telugu in a single repository. Relatively newer approaches that emphasize catering to context (BERT, ELMo, <i>etc.</i>) have shown significant improvements, but require a large amount of resources to generate usable models. We release pre-trained embeddings generated using both contextual and non-contextual approaches. We also use MUSE and XLM to train cross-lingual embeddings for all pairs of the aforementioned languages. To show the efficacy of our embeddings, we evaluate our embedding models on XPOS, UPOS and NER tasks for all these languages. We release a total of 436 models using 8 different approaches. We hope they are useful for the resource-constrained Indian language NLP. The title of this paper refers to the famous novel “A Passage to India” by E.M. Forster, published initially in 1924.</abstract>
      <url hash="71204e25">2020.sltu-1.49</url>
      <language>eng</language>
      <bibkey>kumar-etal-2020-passage</bibkey>
    </paper>
    <paper id="50">
      <title>A Counselling Corpus in <fixed-case>C</fixed-case>antonese</title>
      <author><first>John</first><last>Lee</last></author>
      <author><first>Tianyuan</first><last>Cai</last></author>
      <author><first>Wenxiu</first><last>Xie</last></author>
      <author><first>Lam</first><last>Xing</last></author>
      <pages>358–361</pages>
      <abstract>Virtual agents are increasingly used for delivering health information in general, and mental health assistance in particular. This paper presents a corpus designed for training a virtual counsellor in Cantonese, a variety of Chinese. The corpus consists of a domain-independent subcorpus that supports small talk for rapport building with users, and a domain-specific subcorpus that provides material for a particular area of counselling. The former consists of ELIZA style responses, chitchat expressions, and a dataset of general dialog, all of which are reusable across counselling domains. The latter consists of example user inputs and appropriate chatbot replies relevant to the specific domain. In a case study, we created a chatbot with a domain-specific subcorpus that addressed 25 issues in test anxiety, with 436 inputs solicited from native speakers of Cantonese and 150 chatbot replies harvested from mental health websites. Preliminary evaluations show that Word Mover’s Distance achieved 56% accuracy in identifying the issue in user input, outperforming a number of baselines.</abstract>
      <url hash="c22f57ef">2020.sltu-1.50</url>
      <language>eng</language>
      <bibkey>lee-etal-2020-counselling</bibkey>
    </paper>
    <paper id="51">
      <title>Speech Transcription Challenges for Resource Constrained Indigenous Language <fixed-case>C</fixed-case>ree</title>
      <author><first>Vishwa</first><last>Gupta</last></author>
      <author><first>Gilles</first><last>Boulianne</last></author>
      <pages>362–367</pages>
      <abstract>Cree is one of the most spoken Indigenous languages in Canada. From a speech recognition perspective, it is a low-resource language, since very little data is available for either acoustic or language modeling. This has prevented development of speech technology that could help revitalize the language. We describe our experiments with available Cree data to improve automatic transcription both in speaker- independent and dependent scenarios. While it was difficult to get low speaker-independent word error rates with only six speakers, we were able to get low word and phoneme error rates in the speaker-dependent scenario. We compare our phoneme recognition with two state-of-the-art open-source phoneme recognition toolkits, which use end-to-end training and sequence-to-sequence modeling. Our phoneme error rate (8.7%) is significantly lower than that achieved by the best of these systems (15.1%). With these systems and varying amounts of transcribed and text data, we show that pre-training on other languages is important for speaker-independent recognition, and even small amounts of additional text-only documents are useful. These results can guide practical language documentation work, when deciding how much transcribed and text data is needed to achieve useful phoneme accuracies.</abstract>
      <url hash="5d9bcbea">2020.sltu-1.51</url>
      <language>eng</language>
      <bibkey>gupta-boulianne-2020-speech</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>T</fixed-case>urkish Emotion Voice Database (<fixed-case>T</fixed-case>ur<fixed-case>EV</fixed-case>-<fixed-case>DB</fixed-case>)</title>
      <author><first>Salih Firat</first><last>Canpolat</last></author>
      <author><first>Zuhal</first><last>Ormanoğlu</last></author>
      <author><first>Deniz</first><last>Zeyrek</last></author>
      <pages>368–375</pages>
      <abstract>We introduce the Turkish Emotion-Voice Database (TurEV-DB) which involves a corpus of over 1700 tokens based on 82 words uttered by human subjects in four different emotions (<i>angry, calm, happy, sad</i>). Three machine learning experiments are run on the corpus data to classify the emotions using a convolutional neural network (CNN) model and a support vector machine (SVM) model. We report the performance of the machine learning models, and for evaluation, compare machine learning results with the judgements of humans.</abstract>
      <url hash="108219be">2020.sltu-1.52</url>
      <language>eng</language>
      <bibkey>canpolat-etal-2020-turkish</bibkey>
    </paper>
  </volume>
</collection>
