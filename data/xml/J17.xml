<?xml version='1.0' encoding='UTF-8'?>
<collection id="J17">
  <volume id="1">
    <meta>
      <booktitle>Computational Linguistics, Volume 43, Issue 1 - <fixed-case>A</fixed-case>pril 2017</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>April</month>
      <year>2017</year>
      <venue>cl</venue>
    </meta>
    <frontmatter>
      <bibkey>cl-2017-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Statistical, Grammar-Based Approach to Microplanning</title>
      <author><first>Claire</first><last>Gardent</last></author>
      <author><first>Laura</first><last>Perez-Beltrachini</last></author>
      <abstract>Although there has been much work in recent years on data-driven natural language generation, little attention has been paid to the fine-grained interactions that arise during microplanning between aggregation, surface realization, and sentence segmentation. In this article, we propose a hybrid symbolic/statistical approach to jointly model the constraints regulating these interactions. Our approach integrates a small handwritten grammar, a statistical hypertagger, and a surface realization algorithm. It is applied to the verbalization of knowledge base queries and tested on 13 knowledge bases to demonstrate domain independence. We evaluate our approach in several ways. A quantitative analysis shows that the hybrid approach outperforms a purely symbolic approach in terms of both speed and coverage. Results from a human study indicate that users find the output of this hybrid statistic/symbolic system more fluent than both a template-based and a purely symbolic grammar-based approach. Finally, we illustrate by means of examples that our approach can account for various factors impacting aggregation, sentence segmentation, and surface realization.</abstract>
      <pages>1-30</pages>
      <doi>10.1162/COLI_a_00273</doi>
      <url hash="ac887648">J17-1001</url>
      <bibkey>gardent-perez-beltrachini-2017-statistical</bibkey>
    </paper>
    <paper id="2">
      <title>A Game-Theoretic Approach to Word Sense Disambiguation</title>
      <author><first>Rocco</first><last>Tripodi</last></author>
      <author><first>Marcello</first><last>Pelillo</last></author>
      <abstract>This article presents a new model for word sense disambiguation formulated in terms of evolutionary game theory, where each word to be disambiguated is represented as a node on a graph whose edges represent word relations and senses are represented as classes. The words simultaneously update their class membership preferences according to the senses that neighboring words are likely to choose. We use distributional information to weigh the influence that each word has on the decisions of the others and semantic similarity information to measure the strength of compatibility among the choices. With this information we can formulate the word sense disambiguation problem as a constraint satisfaction problem and solve it using tools derived from game theory, maintaining the textual coherence. The model is based on two ideas: Similar words should be assigned to similar classes and the meaning of a word does not depend on all the words in a text but just on some of them. The article provides an in-depth motivation of the idea of modeling the word sense disambiguation problem in terms of game theory, which is illustrated by an example. The conclusion presents an extensive analysis on the combination of similarity measures to use in the framework and a comparison with state-of-the-art systems. The results show that our model outperforms state-of-the-art algorithms and can be applied to different tasks and in different scenarios.</abstract>
      <pages>31-70</pages>
      <doi>10.1162/COLI_a_00274</doi>
      <url hash="b79d9681">J17-1002</url>
      <bibkey>tripodi-pelillo-2017-game</bibkey>
    </paper>
    <paper id="3">
      <title>Multilingual Metaphor Processing: Experiments with Semi-Supervised and Unsupervised Learning</title>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <author><first>Lin</first><last>Sun</last></author>
      <author><first>Elkin</first><last>Darío Gutiérrez</last></author>
      <author><first>Patricia</first><last>Lichtenstein</last></author>
      <author><first>Srini</first><last>Narayanan</last></author>
      <abstract>Highly frequent in language and communication, metaphor represents a significant challenge for Natural Language Processing (NLP) applications. Computational work on metaphor has traditionally evolved around the use of hand-coded knowledge, making the systems hard to scale. Recent years have witnessed a rise in statistical approaches to metaphor processing. However, these approaches often require extensive human annotation effort and are predominantly evaluated within a limited domain. In contrast, we experiment with weakly supervised and unsupervised techniques—with little or no annotation—to generalize higher-level mechanisms of metaphor from distributional properties of concepts. We investigate different levels and types of supervision (learning from linguistic examples vs. learning from a given set of metaphorical mappings vs. learning without annotation) in flat and hierarchical, unconstrained and constrained clustering settings. Our aim is to identify the optimal type of supervision for a learning algorithm that discovers patterns of metaphorical association from text. In order to investigate the scalability and adaptability of our models, we applied them to data in three languages from different language groups—English, Spanish, and Russian—achieving state-of-the-art results with little supervision. Finally, we demonstrate that statistical methods can facilitate and scale up cross-linguistic research on metaphor.</abstract>
      <pages>71-123</pages>
      <doi>10.1162/COLI_a_00275</doi>
      <url hash="224e6445">J17-1003</url>
      <bibkey>shutova-etal-2017-multilingual</bibkey>
    </paper>
    <paper id="4">
      <title>Argumentation Mining in User-Generated Web Discourse</title>
      <author><first>Ivan</first><last>Habernal</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <abstract>The goal of argumentation mining, an evolving research field in computational linguistics, is to design methods capable of analyzing people’s argumentation. In this article, we go beyond the state of the art in several ways. (i) We deal with actual Web data and take up the challenges given by the variety of registers, multiple domains, and unrestricted noisy user-generated Web discourse. (ii) We bridge the gap between normative argumentation theories and argumentation phenomena encountered in actual data by adapting an argumentation model tested in an extensive annotation study. (iii) We create a new gold standard corpus (90k tokens in 340 documents) and experiment with several machine learning methods to identify argument components. We offer the data, source codes, and annotation guidelines to the community under free licenses. Our findings show that argumentation mining in user-generated Web discourse is a feasible but challenging task.</abstract>
      <pages>125-179</pages>
      <doi>10.1162/COLI_a_00276</doi>
      <url hash="9f356eaa">J17-1004</url>
      <bibkey>habernal-gurevych-2017-argumentation</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ukp">UKP</pwcdataset>
    </paper>
    <paper id="5">
      <title>Hashtag Sense Clustering Based on Temporal Similarity</title>
      <author><first>Giovanni</first><last>Stilo</last></author>
      <author><first>Paola</first><last>Velardi</last></author>
      <abstract>Hashtags are creative labels used in micro-blogs to characterize the topic of a message/discussion. Regardless of the use for which they were originally intended, hashtags cannot be used as a means to cluster messages with similar content. First, because hashtags are created in a spontaneous and highly dynamic way by users in multiple languages, the same topic can be associated with different hashtags, and conversely, the same hashtag may refer to different topics in different time periods. Second, contrary to common words, hashtag disambiguation is complicated by the fact that no sense catalogs (e.g., Wikipedia or WordNet) are available; and, furthermore, hashtag labels are difficult to analyze, as they often consist of acronyms, concatenated words, and so forth. A common way to determine the meaning of hashtags has been to analyze their context, but, as we have just pointed out, hashtags can have multiple and variable meanings. In this article, we propose a temporal sense clustering algorithm based on the idea that semantically related hashtags have similar and synchronous usage patterns.</abstract>
      <pages>181-200</pages>
      <doi>10.1162/COLI_a_00277</doi>
      <url hash="ea9e95f2">J17-1005</url>
      <bibkey>stilo-velardi-2017-hashtag</bibkey>
    </paper>
    <paper id="6">
      <title>Evaluative Language Beyond Bags of Words: Linguistic Insights and Computational Applications</title>
      <author><first>Farah</first><last>Benamara</last></author>
      <author><first>Maite</first><last>Taboada</last></author>
      <author><first>Yannick</first><last>Mathieu</last></author>
      <abstract>The study of evaluation, affect, and subjectivity is a multidisciplinary enterprise, including sociology, psychology, economics, linguistics, and computer science. A number of excellent computational linguistics and linguistic surveys of the field exist. Most surveys, however, do not bring the two disciplines together to show how methods from linguistics can benefit computational sentiment analysis systems. In this survey, we show how incorporating linguistic insights, discourse information, and other contextual phenomena, in combination with the statistical exploitation of data, can result in an improvement over approaches that take advantage of only one of these perspectives. We first provide a comprehensive introduction to evaluative language from both a linguistic and computational perspective. We then argue that the standard computational definition of the concept of evaluative language neglects the dynamic nature of evaluation, in which the interpretation of a given evaluation depends on linguistic and extra-linguistic contextual factors. We thus propose a dynamic definition that incorporates update functions. The update functions allow for different contextual aspects to be incorporated into the calculation of sentiment for evaluative words or expressions, and can be applied at all levels of discourse. We explore each level and highlight which linguistic aspects contribute to accurate extraction of sentiment. We end the review by outlining what we believe the future directions of sentiment analysis are, and the role that discourse and contextual information need to play.</abstract>
      <pages>201-264</pages>
      <doi>10.1162/COLI_a_00278</doi>
      <url hash="b19a09c2">J17-1006</url>
      <bibkey>benamara-etal-2017-evaluative</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
    </paper>
    <paper id="7">
      <title>Book Review: Biomedical Natural Language Processing by Kevin Bretonnel <fixed-case>C</fixed-case>ohen and Dina Demner-Fushman</title>
      <author><first>Jin-Dong</first><last>Kim</last></author>
      <pages>265-267</pages>
      <doi>10.1162/COLI_r_00281</doi>
      <url hash="1afd8b16">J17-1007</url>
      <bibkey>kim-2017-book</bibkey>
    </paper>
    <paper id="8">
      <title>Book Review: Automatic Detection of Verbal Deception by Eileen Fitzpatrick, Joan Bachenko and Tommaso Fornaciari</title>
      <author><first>Yoong</first><last>Keok Lee</last></author>
      <pages>269-271</pages>
      <doi>10.1162/COLI_r_00282</doi>
      <url hash="9e272402">J17-1008</url>
      <bibkey>keok-lee-2017-book</bibkey>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Computational Linguistics, Volume 43, Issue 2 - June 2017</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>June</month>
      <year>2017</year>
      <venue>cl</venue>
    </meta>
    <frontmatter>
      <bibkey>cl-2017-linguistics-43</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Comprehensive Analysis of Bilingual Lexicon Induction</title>
      <author><first>Ann</first><last>Irvine</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <abstract>Bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages. In this article we present the most comprehensive analysis of bilingual lexicon induction to date. We present experiments on a wide range of languages and data sizes. We examine translation into English from 25 foreign languages: Albanian, Azeri, Bengali, Bosnian, Bulgarian, Cebuano, Gujarati, Hindi, Hungarian, Indonesian, Latvian, Nepali, Romanian, Serbian, Slovak, Somali, Spanish, Swedish, Tamil, Telugu, Turkish, Ukrainian, Uzbek, Vietnamese, and Welsh. We analyze the behavior of bilingual lexicon induction on low-frequency words, rather than testing solely on high-frequency words, as previous research has done. Low-frequency words are more relevant to statistical machine translation, where systems typically lack translations of rare words that fall outside of their training data. We systematically explore a wide range of features and phenomena that affect the quality of the translations discovered by bilingual lexicon induction. We provide illustrative examples of the highest ranking translations for orthogonal signals of translation equivalence like contextual similarity and temporal similarity. We analyze the effects of frequency and burstiness, and the sizes of the seed bilingual dictionaries and the monolingual training corpora. Additionally, we introduce a novel discriminative approach to bilingual lexicon induction. Our discriminative model is capable of combining a wide variety of features that individually provide only weak indications of translation equivalence. When feature weights are discriminatively set, these signals produce dramatically higher translation quality than previous approaches that combined signals in an unsupervised fashion (e.g., using minimum reciprocal rank). We also directly compare our model’s performance against a sophisticated generative approach, the matching canonical correlation analysis (MCCA) algorithm used by Haghighi et al. (2008). Our algorithm achieves an accuracy of 42% versus MCCA’s 15%.</abstract>
      <pages>273–310</pages>
      <doi>10.1162/COLI_a_00284</doi>
      <url hash="ac50ccab">J17-2001</url>
      <bibkey>irvine-callison-burch-2017-comprehensive</bibkey>
    </paper>
    <paper id="2">
      <title>Greedy Transition-Based Dependency Parsing with Stack <fixed-case>LSTM</fixed-case>s</title>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <abstract>We introduce a greedy transition-based parser that learns to represent parser states using recurrent neural networks. Our primary innovation that enables us to do this efficiently is a new control structure for sequential neural networks—the stack long short-term memory unit (LSTM). Like the conventional stack data structures used in transition-based parsers, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. Our model captures three facets of the parser’s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of transition actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. In addition, we compare two different word representations: (i) standard word vectors based on look-up tables and (ii) character-based models of words. Although standard word embedding models work well in all languages, the character-based models improve the handling of out-of-vocabulary words, particularly in morphologically rich languages. Finally, we discuss the use of dynamic oracles in training the parser. During training, dynamic oracles alternate between sampling parser states from the training data and from the model as it is being learned, making the model more robust to the kinds of errors that will be made at test time. Training our model with dynamic oracles yields a linear-time greedy parser with very competitive performance.</abstract>
      <pages>311–347</pages>
      <doi>10.1162/COLI_a_00285</doi>
      <url hash="857e3f6c">J17-2002</url>
      <bibkey>ballesteros-etal-2017-greedy</bibkey>
    </paper>
    <paper id="3">
      <title>Statistical Models for Unsupervised, Semi-Supervised Supervised Transliteration Mining</title>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <author><first>Helmut</first><last>Schmid</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <abstract>We present a generative model that efficiently mines transliteration pairs in a consistent fashion in three different settings: unsupervised, semi-supervised, and supervised transliteration mining. The model interpolates two sub-models, one for the generation of transliteration pairs and one for the generation of non-transliteration pairs (i.e., noise). The model is trained on noisy unlabeled data using the EM algorithm. During training the transliteration sub-model learns to generate transliteration pairs and the fixed non-transliteration model generates the noise pairs. After training, the unlabeled data is disambiguated based on the posterior probabilities of the two sub-models. We evaluate our transliteration mining system on data from a transliteration mining shared task and on parallel corpora. For three out of four language pairs, our system outperforms all semi-supervised and supervised systems that participated in the NEWS 2010 shared task. On word pairs extracted from parallel corpora with fewer than 2% transliteration pairs, our system achieves up to 86.7% F-measure with 77.9% precision and 97.8% recall.</abstract>
      <pages>349–375</pages>
      <doi>10.1162/COLI_a_00286</doi>
      <url hash="0102b781">J17-2003</url>
      <bibkey>sajjad-etal-2017-statistical</bibkey>
    </paper>
    <paper id="4">
      <title>Identifying and Avoiding Confusion in Dialogue with People with <fixed-case>A</fixed-case>lzheimer’s Disease</title>
      <author><first>Hamidreza</first><last>Chinaei</last></author>
      <author><first>Leila Chan</first><last>Currie</last></author>
      <author><first>Andrew</first><last>Danks</last></author>
      <author><first>Hubert</first><last>Lin</last></author>
      <author><first>Tejas</first><last>Mehta</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <abstract>Alzheimer’s disease (AD) is an increasingly prevalent cognitive disorder in which memory, language, and executive function deteriorate, usually in that order. There is a growing need to support individuals with AD and other forms of dementia in their daily lives, and our goal is to do so through speech-based interaction. Given that 33% of conversations with people with middle-stage AD involve a breakdown in communication, it is vital that automated dialogue systems be able to identify those breakdowns and, if possible, avoid them. In this article, we discuss several linguistic features that are verbal indicators of confusion in AD (including vocabulary richness, parse tree structures, and acoustic cues) and apply several machine learning algorithms to identify dialogue-relevant confusion from speech with up to 82% accuracy. We also learn dialogue strategies to avoid confusion in the first place, which is accomplished using a partially observable Markov decision process and which obtains accuracies (up to 96.1%) that are significantly higher than several baselines. This work represents a major step towards automated dialogue systems for individuals with dementia.</abstract>
      <pages>377–406</pages>
      <doi>10.1162/COLI_a_00290</doi>
      <url hash="991b7296">J17-2004</url>
      <bibkey>chinaei-etal-2017-identifying</bibkey>
    </paper>
    <paper id="5">
      <title>Framing <fixed-case>QA</fixed-case> as Building and Ranking Intersentence Answer Justifications</title>
      <author><first>Peter</first><last>Jansen</last></author>
      <author><first>Rebecca</first><last>Sharp</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <abstract>We propose a question answering (QA) approach for standardized science exams that both identifies correct answers and produces compelling human-readable justifications for why those answers are correct. Our method first identifies the actual information needed in a question using psycholinguistic concreteness norms, then uses this information need to construct answer justifications by aggregating multiple sentences from different knowledge bases using syntactic and lexical information. We then jointly rank answers and their justifications using a reranking perceptron that treats justification quality as a latent variable. We evaluate our method on 1,000 multiple-choice questions from elementary school science exams, and empirically demonstrate that it performs better than several strong baselines, including neural network approaches. Our best configuration answers 44% of the questions correctly, where the top justifications for 57% of these correct answers contain a compelling human-readable justification that explains the inference required to arrive at the correct answer. We include a detailed characterization of the justification quality for both our method and a strong baseline, and show that information aggregation is key to addressing the information need in complex questions.</abstract>
      <pages>407–449</pages>
      <doi>10.1162/COLI_a_00287</doi>
      <url hash="52beb243">J17-2005</url>
      <bibkey>jansen-etal-2017-framing</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>S</fixed-case>quib: Effects of Cognitive Effort on the Resolution of Overspecified Descriptions</title>
      <author><first>Ivandré</first><last>Paraboni</last></author>
      <author><first>Alex Gwo Jen</first><last>Lan</last></author>
      <author><first>Matheus Mendes</first><last>de Sant’Ana</last></author>
      <author><first>Flávio Luiz</first><last>Coutinho</last></author>
      <abstract>Studies in referring expression generation (REG) have shown different effects of referential overspecification on the resolution of certain descriptions. To further investigate effects of this kind, this article reports two eye-tracking experiments that measure the time required to recognize target objects based on different kinds of information. Results suggest that referential overspecification may be either helpful or detrimental to identification depending on the kind of information that is actually overspecified, an insight that may be useful for the design of more informed hearer-oriented REG algorithms.</abstract>
      <pages>451–459</pages>
      <doi>10.1162/COLI_a_00288</doi>
      <url hash="59ddaba8">J17-2006</url>
      <bibkey>paraboni-etal-2017-squib</bibkey>
    </paper>
    <paper id="7">
      <title>Book Review: Linked Lexical Knowledge Bases Foundations and Applications by Iryna <fixed-case>G</fixed-case>urevych, Judith Eckle-er and <fixed-case>M</fixed-case>ichael Matuschek</title>
      <author><first>Maud</first><last>Ehrmann</last></author>
      <pages>461–463</pages>
      <doi>10.1162/COLI_r_00289</doi>
      <url hash="9c878c47">J17-2007</url>
      <bibkey>ehrmann-2017-book</bibkey>
    </paper>
  </volume>
  <volume id="3">
    <meta>
      <booktitle>Computational Linguistics, Volume 43, Issue 3 - September 2017</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>September</month>
      <year>2017</year>
      <venue>cl</venue>
    </meta>
    <frontmatter>
      <bibkey>cl-2017-linguistics-43-issue</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Hybrid Grammars for Parsing of Discontinuous Phrase Structures and Non-Projective Dependency Structures</title>
      <author><first>Kilian</first><last>Gebhardt</last></author>
      <author><first>Mark-Jan</first><last>Nederhof</last></author>
      <author><first>Heiko</first><last>Vogler</last></author>
      <abstract>We explore the concept of hybrid grammars, which formalize and generalize a range of existing frameworks for dealing with discontinuous syntactic structures. Covered are both discontinuous phrase structures and non-projective dependency structures. Technically, hybrid grammars are related to synchronous grammars, where one grammar component generates linear structures and another generates hierarchical structures. By coupling lexical elements of both components together, discontinuous structures result. Several types of hybrid grammars are characterized. We also discuss grammar induction from treebanks. The main advantage over existing frameworks is the ability of hybrid grammars to separate discontinuity of the desired structures from time complexity of parsing. This permits exploration of a large variety of parsing algorithms for discontinuous structures, with different properties. This is confirmed by the reported experimental results, which show a wide variety of running time, accuracy, and frequency of parse failures.</abstract>
      <pages>465–520</pages>
      <doi>10.1162/COLI_a_00291</doi>
      <url hash="b0a3e8db">J17-3001</url>
      <bibkey>gebhardt-etal-2017-hybrid</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="2">
      <title>Translation Divergences in <fixed-case>C</fixed-case>hinese–<fixed-case>E</fixed-case>nglish Machine Translation: An Empirical Investigation</title>
      <author><first>Dun</first><last>Deng</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <abstract>In this article, we conduct an empirical investigation of translation divergences between Chinese and English relying on a parallel treebank. To do this, we first devise a hierarchical alignment scheme where Chinese and English parse trees are aligned in a way that eliminates conflicts and redundancies between word alignments and syntactic parses to prevent the generation of spurious translation divergences. Using this Hierarchically Aligned Chinese–English Parallel Treebank (HACEPT), we are able to semi-automatically identify and categorize the translation divergences between the two languages and quantify each type of translation divergence. Our results show that the translation divergences are much broader than described in previous studies that are largely based on anecdotal evidence and linguistic knowledge. The distribution of the translation divergences also shows that some high-profile translation divergences that motivate previous research are actually very rare in our data, whereas other translation divergences that have previously received little attention actually exist in large quantities. We also show that HACEPT allows the extraction of syntax-based translation rules, most of which are expressive enough to capture the translation divergences, and point out that the syntactic annotation in existing treebanks is not optimal for extracting such translation rules. We also discuss the implications of our study for attempts to bridge translation divergences by devising shared semantic representations across languages. Our quantitative results lend further support to the observation that although it is possible to bridge some translation divergences with semantic representations, other translation divergences are open-ended, thus building a semantic representation that captures all possible translation divergences may be impractical.</abstract>
      <pages>521–565</pages>
      <doi>10.1162/COLI_a_00292</doi>
      <url hash="405354c6">J17-3002</url>
      <bibkey>deng-xue-2017-translation</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/amr-bank">AMR Bank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="3">
      <title>A Kernel Independence Test for Geographical Language Variation</title>
      <author><first>Dong</first><last>Nguyen</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <abstract>Quantifying the degree of spatial dependence for linguistic variables is a key task for analyzing dialectal variation. However, existing approaches have important drawbacks. First, they are based on parametric models of dependence, which limits their power in cases where the underlying parametric assumptions are violated. Second, they are not applicable to all types of linguistic data: Some approaches apply only to frequencies, others to boolean indicators of whether a linguistic variable is present. We present a new method for measuring geographical language variation, which solves both of these problems. Our approach builds on Reproducing Kernel Hilbert Space (RKHS) representations for nonparametric statistics, and takes the form of a test statistic that is computed from pairs of individual geotagged observations without aggregation into predefined geographical bins. We compare this test with prior work using synthetic data as well as a diverse set of real data sets: a corpus of Dutch tweets, a Dutch syntactic atlas, and a data set of letters to the editor in North American newspapers. Our proposed test is shown to support robust inferences across a broad range of scenarios and types of data.</abstract>
      <pages>567–592</pages>
      <doi>10.1162/COLI_a_00293</doi>
      <url hash="a5da865a">J17-3003</url>
      <bibkey>nguyen-eisenstein-2017-kernel</bibkey>
      <pwccode url="https://github.com/dongpng/geo-independence-testing" additional="false">dongpng/geo-independence-testing</pwccode>
    </paper>
    <paper id="4">
      <title><fixed-case>A</fixed-case>uto<fixed-case>E</fixed-case>xtend: Combining Word Embeddings with Semantic Resources</title>
      <author><first>Sascha</first><last>Rothe</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <abstract>We present AutoExtend, a system that combines word embeddings with semantic resources by learning embeddings for non-word objects like synsets and entities and learning word embeddings that incorporate the semantic information from the resource. The method is based on encoding and decoding the word embeddings and is flexible in that it can take any word embeddings as input and does not need an additional training corpus. The obtained embeddings live in the same vector space as the input word embeddings. A sparse tensor formalization guarantees efficiency and parallelizability. We use WordNet, GermaNet, and Freebase as semantic resources. AutoExtend achieves state-of-the-art performance on Word-in-Context Similarity and Word Sense Disambiguation tasks.</abstract>
      <pages>593–617</pages>
      <doi>10.1162/COLI_a_00294</doi>
      <url hash="bb9fa638">J17-3004</url>
      <bibkey>rothe-schutze-2017-autoextend</bibkey>
    </paper>
    <paper id="5">
      <title>Parsing Argumentation Structures in Persuasive Essays</title>
      <author><first>Christian</first><last>Stab</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <abstract>In this article, we present a novel approach for parsing argumentation structures. We identify argument components using sequence labeling at the token level and apply a new joint model for detecting argumentation structures. The proposed model globally optimizes argument component types and argumentative relations using Integer Linear Programming. We show that our model significantly outperforms challenging heuristic baselines on two different types of discourse. Moreover, we introduce a novel corpus of persuasive essays annotated with argumentation structures. We show that our annotation scheme and annotation guidelines successfully guide human annotators to substantial agreement.</abstract>
      <pages>619–659</pages>
      <doi>10.1162/COLI_a_00295</doi>
      <url hash="2006196c">J17-3005</url>
      <bibkey>stab-gurevych-2017-parsing</bibkey>
    </paper>
    <paper id="6">
      <title>The Agreement Measure γcat a Complement to γ Focused on Categorization of a Continuum</title>
      <author><first>Yann</first><last>Mathet</last></author>
      <abstract>Agreement on unitizing, where several annotators freely put units of various sizes and categories on a continuum, is difficult to assess because of the simultaneaous discrepancies in positioning and categorizing. The recent agreement measure γ offers an overall solution that simultaneously takes into account positions and categories. In this article, I propose the additional coefficient γcat, which complements γ by assessing the agreement on categorization of a continuum, putting aside positional discrepancies. When applied to pure categorization (with predefined units), γcat behaves the same way as the famous dedicated Krippendorff’s α, even with missing values, which proves its consistency. A variation of γcat is also proposed that provides an in-depth assessment of categorizing for each individual category. The entire family of γ coefficients is implemented in free software.</abstract>
      <pages>661–681</pages>
      <doi>10.1162/COLI_a_00296</doi>
      <url hash="f7241c3a">J17-3006</url>
      <bibkey>mathet-2017-agreement</bibkey>
    </paper>
  </volume>
  <volume id="4">
    <meta>
      <booktitle>Computational Linguistics, Volume 43, Issue 4 - <fixed-case>D</fixed-case>ecember 2017</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>December</month>
      <year>2017</year>
      <venue>cl</venue>
    </meta>
    <frontmatter>
      <bibkey>cl-2017-linguistics-43-issue-4</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Discourse Structure in Machine Translation Evaluation</title>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>Lluís</first><last>Màrquez</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <abstract>In this article, we explore the potential of using sentence-level discourse structure for machine translation evaluation. We first design discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory (RST). Then, we show that a simple linear combination with these measures can help improve various existing machine translation evaluation metrics regarding correlation with human judgments both at the segment level and at the system level. This suggests that discourse information is complementary to the information used by many of the existing evaluation metrics, and thus it could be taken into account when developing richer evaluation metrics, such as the WMT-14 winning combined metric DiscoTKparty. We also provide a detailed analysis of the relevance of various discourse elements and relations from the RST parse trees for machine translation evaluation. In particular, we show that (i) all aspects of the RST tree are relevant, (ii) nuclearity is more useful than relation type, and (iii) the similarity of the translation RST tree to the reference RST tree is positively correlated with translation quality.</abstract>
      <pages>683–722</pages>
      <doi>10.1162/COLI_a_00298</doi>
      <url hash="18904d86">J17-4001</url>
      <bibkey>joty-etal-2017-discourse</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="2">
      <title>Adapting to Learner Errors with Minimal Supervision</title>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Mark</first><last>Sammons</last></author>
      <abstract>This article considers the problem of correcting errors made by English as a Second Language writers from a machine learning perspective, and addresses an important issue of developing an appropriate training paradigm for the task, one that accounts for error patterns of non-native writers using minimal supervision. Existing training approaches present a trade-off between large amounts of cheap data offered by the native-trained models and additional knowledge of learner error patterns provided by the more expensive method of training on annotated learner data. We propose a novel training approach that draws on the strengths offered by the two standard training paradigms—of training either on native or on annotated learner data—and that outperforms both of these standard methods. Using the key observation that parameters relating to error regularities exhibited by non-native writers are relatively simple, we develop models that can incorporate knowledge about error regularities based on a small annotated sample but that are otherwise trained on native English data. The key contribution of this article is the introduction and analysis of two methods for adapting the learned models to error patterns of non-native writers; one method that applies to generative classifiers and a second that applies to discriminative classifiers. Both methods demonstrated state-of-the-art performance in several text correction competitions. In particular, the Illinois system that implements these methods ranked at the top in two recent CoNLL shared tasks on error correction.1 We conduct further evaluation of the proposed approaches studying the effect of using error data from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.</abstract>
      <pages>723–760</pages>
      <doi>10.1162/COLI_a_00299</doi>
      <url hash="4373f1eb">J17-4002</url>
      <bibkey>rozovskaya-etal-2017-adapting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="3">
      <title>Representation of Linguistic Form and Function in Recurrent Neural Networks</title>
      <author><first>Ákos</first><last>Kádár</last></author>
      <author><first>Grzegorz</first><last>Chrupała</last></author>
      <author><first>Afra</first><last>Alishahi</last></author>
      <abstract>We present novel methods for analyzing the activation patterns of recurrent neural networks from a linguistic point of view and explore the types of linguistic structure they learn. As a case study, we use a standard standalone language model, and a multi-task gated recurrent network architecture consisting of two parallel pathways with shared word embeddings: The Visual pathway is trained on predicting the representations of the visual scene corresponding to an input sentence, and the Textual pathway is trained to predict the next word in the same sentence. We propose a method for estimating the amount of contribution of individual tokens in the input to the final prediction of the networks. Using this method, we show that the Visual pathway pays selective attention to lexical categories and grammatical functions that carry semantic information, and learns to treat word types differently depending on their grammatical function and their position in the sequential structure of the sentence. In contrast, the language models are comparatively more sensitive to words with a syntactic function. Further analysis of the most informative n-gram contexts for each model shows that in comparison with the Visual pathway, the language models react more strongly to abstract contexts that represent syntactic constructions.</abstract>
      <pages>761–780</pages>
      <doi>10.1162/COLI_a_00300</doi>
      <url hash="1627a1f1">J17-4003</url>
      <bibkey>kadar-etal-2017-representation</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="4">
      <title><fixed-case>H</fixed-case>yper<fixed-case>L</fixed-case>ex: A Large-Scale Evaluation of Graded Lexical Entailment</title>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Daniela</first><last>Gerz</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Felix</first><last>Hill</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <abstract>We introduce HyperLex—a data set and evaluation resource that quantifies the extent of the semantic category membership, that is, type-of relation, also known as hyponymy–hypernymy or lexical entailment (LE) relation between 2,616 concept pairs. Cognitive psychology research has established that typicality and category/class membership are computed in human semantic memory as a gradual rather than binary relation. Nevertheless, most NLP research and existing large-scale inventories of concept category membership (WordNet, DBPedia, etc.) treat category membership and LE as binary. To address this, we asked hundreds of native English speakers to indicate typicality and strength of category membership between a diverse range of concept pairs on a crowdsourcing platform. Our results confirm that category membership and LE are indeed more gradual than binary. We then compare these human judgments with the predictions of automatic systems, which reveals a huge gap between human performance and state-of-the-art LE, distributional and representation learning models, and substantial differences between the models themselves. We discuss a pathway for improving semantic models to overcome this discrepancy, and indicate future application areas for improved graded LE systems.</abstract>
      <pages>781–835</pages>
      <doi>10.1162/COLI_a_00301</doi>
      <url hash="099e7e5d">J17-4004</url>
      <bibkey>vulic-etal-2017-hyperlex</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hyperlex">HyperLex</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/yago">YAGO</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>S</fixed-case>urvey: Multiword Expression Processing: A <fixed-case>S</fixed-case>urvey</title>
      <author><first>Mathieu</first><last>Constant</last></author>
      <author><first>Gülşen</first><last>Eryiǧit</last></author>
      <author><first>Johanna</first><last>Monti</last></author>
      <author><first>Lonneke</first><last>van der Plas</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <author><first>Michael</first><last>Rosner</last></author>
      <author><first>Amalia</first><last>Todirascu</last></author>
      <abstract>Multiword expressions (MWEs) are a class of linguistic forms spanning conventional word boundaries that are both idiosyncratic and pervasive across different languages. The structure of linguistic processing that depends on the clear distinction between words and phrases has to be re-thought to accommodate MWEs. The issue of MWE handling is crucial for NLP applications, where it raises a number of challenges. The emergence of solutions in the absence of guiding principles motivates this survey, whose aim is not only to provide a focused review of MWE processing, but also to clarify the nature of interactions between MWE processing and downstream applications. We propose a conceptual framework within which challenges and research contributions can be positioned. It offers a shared understanding of what is meant by “MWE processing,” distinguishing the subtasks of MWE discovery and identification. It also elucidates the interactions between MWE processing and two use cases: Parsing and machine translation. Many of the approaches in the literature can be differentiated according to how MWE processing is timed with respect to underlying use cases. We discuss how such orchestration choices affect the scope of MWE-aware systems. For each of the two MWE processing subtasks and for each of the two use cases, we conclude on open issues and research perspectives.</abstract>
      <pages>837–892</pages>
      <doi>10.1162/COLI_a_00302</doi>
      <url hash="2feaa06e">J17-4005</url>
      <bibkey>constant-etal-2017-survey</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="6">
      <title>Book Review: Syntax-Based Statistical Machine Translation by Philip <fixed-case>W</fixed-case>illiams, Rico <fixed-case>S</fixed-case>ennrich, Matt Post and Philipp <fixed-case>K</fixed-case>oehn</title>
      <author><first>Christian</first><last>Hadiwinoto</last></author>
      <pages>893–896</pages>
      <doi>10.1162/COLI_r_00303</doi>
      <url hash="8eae9e03">J17-4006</url>
      <bibkey>hadiwinoto-2017-book</bibkey>
    </paper>
    <paper id="7">
      <title>Last Words: Sharing Is Caring: The Future of Shared Tasks</title>
      <author><first>Malvina</first><last>Nissim</last></author>
      <author><first>Lasha</first><last>Abzianidze</last></author>
      <author><first>Kilian</first><last>Evang</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Hessel</first><last>Haagsma</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Martijn</first><last>Wieling</last></author>
      <pages>897–904</pages>
      <doi>10.1162/COLI_a_00304</doi>
      <url hash="08f8fbcd">J17-4007</url>
      <bibkey>nissim-etal-2017-last</bibkey>
    </paper>
    <paper id="8">
      <title>Reviewers for Volume 43</title>
      <pages>905-905</pages>
      <doi>10.1162/COLI_x_00305</doi>
      <url hash="0dc2a35e">J17-4008</url>
      <bibkey>nn-2017-reviewers</bibkey>
    </paper>
  </volume>
</collection>
