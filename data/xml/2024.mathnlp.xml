<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.mathnlp">
  <volume id="1" ingest-date="2024-05-16" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Mathematical Natural Language Processing @ LREC-COLING 2024</booktitle>
      <editor><first>Marco</first><last>Valentino</last></editor>
      <editor><first>Deborah</first><last>Ferreira</last></editor>
      <editor><first>Mokanarangan</first><last>Thayaparan</last></editor>
      <editor><first>Andre</first><last>Freitas</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="719a2d72">2024.mathnlp-1</url>
      <venue>mathnlp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="a18e47bb">2024.mathnlp-1.0</url>
      <bibkey>mathnlp-2024-mathematical</bibkey>
    </frontmatter>
    <paper id="1">
      <title>An Approach to Co-reference Resolution and Formula Grounding for Mathematical Identifiers Using Large Language Models</title>
      <author><first>Aamin</first><last>Dev</last></author>
      <author><first>Takuto</first><last>Asakura</last></author>
      <author><first>Rune</first><last>Sætre</last></author>
      <pages>1–10</pages>
      <abstract>This paper outlines an automated approach to annotate mathematical identifiers in scientific papers — a process historically laborious and costly. We employ state-of-the-art LLMs, including GPT-3.5 and GPT-4, and open-source alternatives to generate a dictionary for annotating mathematical identifiers, linking each identifier to its conceivable descriptions and then assigning these definitions to the respective identifier in- stances based on context. Evaluation metrics include the CoNLL score for co-reference cluster quality and semantic correctness of the annotations.</abstract>
      <url hash="46dfe997">2024.mathnlp-1.1</url>
      <bibkey>dev-etal-2024-approach</bibkey>
    </paper>
    <paper id="2">
      <title>Fluid Dynamics-Inspired Emotional Analysis in <fixed-case>S</fixed-case>hakespearean Tragedies: A Novel Computational Linguistics Methodology</title>
      <author><first>Davide</first><last>Picca</last></author>
      <pages>11–18</pages>
      <abstract>This study introduces an innovative method for analyzing emotions in texts, drawing inspiration from the principles of fluid dynamics, particularly the Navier-Stokes equations. It applies this framework to analyze Shakespeare’s tragedies “Hamlet” and “Romeo and Juliet”, treating emotional expressions as entities akin to fluids. By mapping linguistic characteristics onto fluid dynamics components, this approach provides a dynamic perspective on how emotions are expressed and evolve in narrative texts. The results, when compared with conventional sentiment analysis methods, reveal a more detailed and subtle grasp of the emotional arcs within these works. This interdisciplinary strategy not only enriches emotion analysis in computational linguistics but also paves the way for potential integrations with machine learning in NLP.</abstract>
      <url hash="340464dc">2024.mathnlp-1.2</url>
      <bibkey>picca-2024-fluid</bibkey>
    </paper>
    <paper id="3">
      <title>Math Problem Solving: Enhancing Large Language Models with Semantically Rich Symbolic Variables</title>
      <author><first>Ali Emre</first><last>Narin</last></author>
      <pages>19–24</pages>
      <abstract>The advent of Large Language Models (LLMs) based on the Transformer architecture has led to remarkable advancements in various domains, including reasoning tasks. However, accurately assessing the performance of Large Language Models, particularly in the reasoning domain, remains a challenge. In this paper, we propose the Semantically Rich Variable Substitution Method (SemRiVas) as an enhancement to existing symbolic methodologies for evaluating LLMs on Mathematical Word Problems (MWPs). Unlike previous approaches that utilize generic symbols for variable substitution, SemRiVas employs descriptive variable names, aiming to improve the problem-solving abilities of LLMs. Our method aims to eliminate the need for LLMs to possess programming proficiency and perform arithmetic operations, to be universally applicable. Our experimental results demonstrate the superior accuracy of SemRiVas compared to prior symbolic methods, particularly in resolving longer and more complex MWP questions. However, LLMs’ performance with SemRiVas and symbolic methods that utilize one-character variables still falls short compared to notable techniques like CoT and PaL.</abstract>
      <url hash="e0823b59">2024.mathnlp-1.3</url>
      <bibkey>narin-2024-math</bibkey>
    </paper>
    <paper id="4">
      <title>Data Driven Approach for Mathematical Problem Solving</title>
      <author><first>Byungju</first><last>Kim</last></author>
      <author><first>Wonseok</first><last>Lee</last></author>
      <author><first>Jaehong</first><last>Kim</last></author>
      <author><first>Jungbin</first><last>Im</last></author>
      <pages>25–34</pages>
      <abstract>In this paper, we investigate and introduce a novel Llama-2 based model, fine-tuned with an original dataset designed to mirror real-world mathematical challenges. The dataset was collected through a question-answering platform, incorporating solutions generated by both rule-based solver and question answering, to cover a broad spectrum of mathematical concepts and problem-solving techniques. Experimental results demonstrate significant performance improvements when the models are fine-tuned with our dataset. The results suggest that the integration of contextually rich and diverse problem sets into the training substantially enhances the problem-solving capability of language models across various mathematical domains. This study showcases the critical role of curated educational content in advancing AI research.</abstract>
      <url hash="6b461bbd">2024.mathnlp-1.4</url>
      <bibkey>kim-etal-2024-data</bibkey>
    </paper>
    <paper id="5">
      <title>Exploring Internal Numeracy in Language Models: A Case Study on <fixed-case>ALBERT</fixed-case></title>
      <author><first>Ulme</first><last>Wennberg</last></author>
      <author><first>Gustav Eje</first><last>Henter</last></author>
      <pages>35–40</pages>
      <abstract>It has been found that Transformer-based language models have the ability to perform basic quantitative reasoning. In this paper, we propose a method for studying how these models internally represent numerical data, and use our proposal to analyze the ALBERT family of language models. Specifically, we extract the learned embeddings these models use to represent tokens that correspond to numbers and ordinals, and subject these embeddings to Principal Component Analysis (PCA). PCA results reveal that ALBERT models of different sizes, trained and initialized separately, consistently learn to use the axes of greatest variation to represent the approximate ordering of various numerical concepts. Numerals and their textual counterparts are represented in separate clusters, but increase along the same direction in 2D space. Our findings illustrate that language models, trained purely to model text, can intuit basic mathematical concepts, opening avenues for NLP applications that intersect with quantitative reasoning.</abstract>
      <url hash="5420fbb0">2024.mathnlp-1.5</url>
      <bibkey>wennberg-henter-2024-exploring</bibkey>
    </paper>
  </volume>
</collection>
