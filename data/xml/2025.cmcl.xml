<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.cmcl">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</booktitle>
      <editor><first>Tatsuki</first><last>Kuribayashi</last></editor>
      <editor><first>Giulia</first><last>Rambelli</last></editor>
      <editor><first>Ece</first><last>Takmaz</last></editor>
      <editor><first>Philipp</first><last>Wicke</last></editor>
      <editor><first>Jixing</first><last>Li</last></editor>
      <editor><first>Byung-Doh</first><last>Oh</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico, USA</address>
      <month>May</month>
      <year>2025</year>
      <url hash="ebaeecfb">2025.cmcl-1</url>
      <venue>cmcl</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-227-5</isbn>
    </meta>
    <frontmatter>
      <url hash="2fadbf17">2025.cmcl-1.0</url>
      <bibkey>cmcl-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Linguistic Blind Spots of Large Language Models</title>
      <author><first>Jiali</first><last>Cheng</last></author>
      <author><first>Hadi</first><last>Amiri</last><affiliation>University of Massachusetts Lowell</affiliation></author>
      <pages>1-17</pages>
      <abstract>Large language models (LLMs) serve as the foundation of numerous AI applications today. However, despite their remarkable proficiency in generating coherent text, questions linger regarding their ability in performing fine-grained linguistic annotation tasks, such as detecting nouns or verbs, or identifying more complex syntactic structures like clauses or T-units in input texts. These tasks require precise syntactic and semantic understanding of input text, and when LLMs underperform on specific linguistic structures, it raises concerns about their reliability for detailed linguistic analysis and whether their (even correct) outputs truly reflect an understanding of the inputs. In this paper, we empirically study recent LLMs performance across fine-grained linguistic annotation tasks. Through a series of experiments, we find that recent LLMs show limited efficacy in addressing linguistic queries and often struggle with linguistically complex inputs. We show that the most capable LLM (Llama3-70b) makes notable errors in detecting linguistic structures, such as misidentifying embedded clauses, failing to recognize verb phrases, and confusing complex nominals with clauses. Our study provides valuable insights to inform future endeavors in LLM design and development.</abstract>
      <url hash="9730476c">2025.cmcl-1.3</url>
      <bibkey>cheng-amiri-2025-linguistic</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>P</fixed-case>ara<fixed-case>BL</fixed-case>o<fixed-case>CC</fixed-case>: Parallel Basic Locative Constructions Corpus</title>
      <author><first>Peter</first><last>Viechnicki</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Anthony</first><last>Kostacos</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>18-23</pages>
      <abstract>We introduce ParaBLoCC, the Parallel Basic Locative Construction Corpus, the first multilingual compendium of this important grammatico-functional construction, and particularly the first such corpus containing semantically equivalent BLCs in source/target language pairs. The data – taken from bitext corpora in English paired with twenty-six typologically diverse languages – are likely to prove useful for studying questions of cognitive underpinnings and cross-linguistic usage patterns of spatial expressions, as well as for improving multilingual spatial relation extraction and related tasks. The data are being made available at https://github.com/pviechnicki/parablocc.</abstract>
      <url hash="7db4655b">2025.cmcl-1.4</url>
      <bibkey>viechnicki-kostacos-2025-parablocc</bibkey>
    </paper>
    <paper id="5">
      <title>Capturing Online <fixed-case>SRC</fixed-case>/<fixed-case>ORC</fixed-case> Effort with Memory Measures from a Minimalist Parser</title>
      <author><first>Aniello</first><last>De Santo</last><affiliation>University of Utah</affiliation></author>
      <pages>24-35</pages>
      <abstract>A parser for Minimalist grammars (Stabler, 2013) has been shown to successfully model sentence processing preferences across an array of languages and phenomena when combined with complexity metrics that relate parsing behavior to memory usage (Gerth, 2015; Graf et al., 2017; De Santo, 2020, a.o.). This model provides a quantifiable theory of the effects of fine-grained grammatical structure on cognitive cost, and can help strengthen the link between generative syntactic theory and sentence processing.However, work on it has focused on offline asymmetries.Here, we extend this approach by showing how memory-based measures of effort that explicitly consider minimalist-like structure-building operations improve our ability to account for word-by-word (online) behavioral data.</abstract>
      <url hash="bf0a03d4">2025.cmcl-1.5</url>
      <bibkey>de-santo-2025-capturing</bibkey>
    </paper>
    <paper id="6">
      <title>From Punchlines to Predictions: A Metric to Assess <fixed-case>LLM</fixed-case> Performance in Identifying Humor in Stand-Up Comedy</title>
      <author><first>Adrianna</first><last>Romanowski</last></author>
      <author><first>Pedro H. V.</first><last>Valois</last></author>
      <author><first>Kazuhiro</first><last>Fukui</last><affiliation>University of Tsukuba</affiliation></author>
      <pages>36-46</pages>
      <abstract>Comedy serves as a profound reflection of the times we live in and is a staple element of human interactions. In light of the widespread adoption of Large Language Models (LLMs), the intersection of humor and AI has become no laughing matter. Advancements in the naturalness of human-computer interaction correlates with improvements in AI systems’ abilities to understand humor. In this study, we assess the ability of models in accurately identifying humorous quotes from a stand-up comedy transcript. Stand-up comedy’s unique comedic narratives make it an ideal dataset to improve the overall naturalness of comedic understanding. We propose a novel humor detection metric designed to evaluate LLMs amongst various prompts on their capability to extract humorous punchlines. The metric has a modular structure that offers three different scoring methods - fuzzy string matching, sentence embedding, and subspace similarity - to provide an overarching assessment of a model’s performance. The model’s results are compared against those of human evaluators on the same task. Our metric reveals that regardless of prompt engineering, leading models, ChatGPT, Claude, and DeepSeek, achieve scores of at most 51% in humor detection. Notably, this performance surpasses that of humans who achieve a score of 41%. The analysis of human evaluators and LLMs reveals variability in agreement, highlighting the subjectivity inherent in humor and the complexities involved in extracting humorous quotes from live performance transcripts.</abstract>
      <url hash="f541ea31">2025.cmcl-1.6</url>
      <bibkey>romanowski-etal-2025-punchlines</bibkey>
    </paper>
    <paper id="7">
      <title>Profiling neural grammar induction on morphemically tokenised child-directed speech</title>
      <author><first>Mila</first><last>Marcheva</last></author>
      <author><first>Theresa</first><last>Biberauer</last><affiliation>University of the Western Cape, University of Stellenbosch and University of Cambridge</affiliation></author>
      <author><first>Weiwei</first><last>Sun</last><affiliation>University of Cambridge</affiliation></author>
      <pages>47-54</pages>
      <abstract>We investigate the performance of state-of-the-art (SotA) neural grammar induction (GI) models on a morphemically tokenised English dataset based on the CHILDES treebank (Pearl and Sprouse, 2013). Using implementations from Yang et al. (2021a), we train models and evaluate them with the standard F1 score. We introduce novel evaluation metrics—depth-of-morpheme and sibling-of-morpheme—which measure phenomena around bound morpheme attachment. Our results reveal that models with the highest F1 scores do not necessarily induce linguistically plausible structures for bound morpheme attachment, highlighting a key challenge for cognitively plausible GI.</abstract>
      <url hash="df2c343b">2025.cmcl-1.7</url>
      <bibkey>marcheva-etal-2025-profiling</bibkey>
    </paper>
    <paper id="9">
      <title>Exploring the Integration of Eye Movement Data on Word Embeddings</title>
      <author><first>Fermín</first><last>Travi</last><affiliation>Computer Science Department, University of Buenos Aires</affiliation></author>
      <author><first>Gabriel Aimé</first><last>Leclercq</last><affiliation>Universidad de Buenos Aires</affiliation></author>
      <author><first>Diego Fernandez</first><last>Slezak</last><affiliation>Computer Science Department, Universidad de Buenos Aires</affiliation></author>
      <author><first>Bruno</first><last>Bianchi</last></author>
      <author><first>Juan E</first><last>Kamienkowski</last></author>
      <pages>55-65</pages>
      <abstract>Reading, while structured, is a non-linear process. Readers may skip some words, linger on others, or revisit earlier text. Emerging work has started exploring the incorporation of reading behaviour through eye-tracking into the training of specific language tasks. In this work, we investigate the broader question of how gaze data can shape word embeddings by using text as read by human participants and predicting gaze measures from them. To that end, we conducted an eye-tracking experiment with 76 participants reading 20 short stories in Spanish and fine-tuned Word2Vec and LSTM models on the collected data. Evaluations with representational similarity analysis and word pair similarities showed a limited, but largely consistent, gain from gaze incorporation, suggesting future work should expand linguistic diversity and use cognitively aligned evaluations to better understand its role in bridging computational and human language representations.</abstract>
      <url hash="89e35920">2025.cmcl-1.9</url>
      <bibkey>travi-etal-2025-exploring</bibkey>
    </paper>
    <paper id="11">
      <title>Unzipping the Causality of <fixed-case>Z</fixed-case>ipf’s Law and Other Lexical Trade-offs</title>
      <author><first>Amanda</first><last>Doucette</last><affiliation>McGill University, McGill University</affiliation></author>
      <author><first>Timothy J.</first><last>O’Donnell</last><affiliation>McGill University, Mila and McGill University</affiliation></author>
      <author><first>Morgan</first><last>Sonderegger</last><affiliation>McGill University</affiliation></author>
      <pages>66-76</pages>
      <abstract>There are strong constraints on the structure of a possible lexicon. For example, the negative correlation between word frequency and length known as Zipf’s law, and a negative correlation between word length and phonotactic complexity appear to hold across languages. While lexical trade-offs like these have been examined individually, it is unclear how they interact as a system. In this paper, we propose causal discovery as a method for identifying lexical biases and their interactions in a set of variables. We represent the lexicon as a causal model, and apply the Fast Causal Discovery algorithm (Spirtes et al., 1995) to identify both causal relationships between measured variables and the existence of possible unmeasured confounding variables. We apply this method to lexical data including measures of word length, frequency, phonotactic complexity, and morphological irregularity for 25 languages and find evidence of universal associations involving word length with a high likelihood of involving an unmeasured confounder, suggesting that additional variables need to be measured to determine how they are related. We also find evidence of variation across languages in relationships between the remaining variables, and suggest that given a larger dataset, causal discovery algorithms can be a useful tool in assessing the universality of lexical biases.</abstract>
      <url hash="aefab50b">2025.cmcl-1.11</url>
      <bibkey>doucette-etal-2025-unzipping</bibkey>
    </paper>
    <paper id="12">
      <title>Quantifying Semantic Functional Specialization in the Brain Using Encoding Models of Natural Language</title>
      <author><first>Jiaqi</first><last>Chen</last></author>
      <author><first>Richard</first><last>Antonello</last><affiliation>Columbia University</affiliation></author>
      <author><first>Kaavya</first><last>Chaparala</last><affiliation>The MITRE Corporation</affiliation></author>
      <author><first>Coen</first><last>Arrow</last><affiliation>University of Western Australia</affiliation></author>
      <author><first>Nima</first><last>Mesgarani</last><affiliation>Columbia University</affiliation></author>
      <pages>77-90</pages>
      <abstract>Although functional specialization in the brain - a phenomenon where different regions process different types of information - is well documented, we still lack precise mathematical methods with which to measure it. This work proposes a technique to quantify how brain regions respond to distinct categories of information. Using a topic encoding model, we identify brain regions that respond strongly to specific semantic categories while responding minimally to all others. We then use a language model to characterize the common themes across each region’s preferred categories. Our technique successfully identifies previously known functionally selective regions and reveals consistent patterns across subjects while also highlighting new areas of high specialization worthy of further study.</abstract>
      <url hash="d485113c">2025.cmcl-1.12</url>
      <bibkey>chen-etal-2025-quantifying</bibkey>
    </paper>
    <paper id="15">
      <title>“Is There Anything Else?”: Examining Administrator Influence on Linguistic Features from the Cookie Theft Picture Description Cognitive Test</title>
      <author><first>Changye</first><last>Li</last><affiliation>University of Washington</affiliation></author>
      <author><first>Zhecheng</first><last>Sheng</last></author>
      <author><first>Trevor</first><last>Cohen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Serguei V. S.</first><last>Pakhomov</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <pages>91-103</pages>
      <abstract>Alzheimer’s Disease (AD) dementia is a progressive neurodegenerative disease that negatively impacts patients’ cognitive ability. Previous studies have demonstrated that changes in naturalistic language samples can be useful for early screening of AD dementia. However, the nature of language deficits often requires test administrators to use various speech elicitation techniques during spontaneous language assessments to obtain enough propositional utterances from dementia patients. This could lead to the “observer’s effect” on the downstream analysis that has not been fully investigated. Our study seeks to quantify the influence of test administrators on linguistic features in dementia assessment with two English corpora the “Cookie Theft” picture description datasets collected at different locations and test administrators show different levels of administrator involvement. Our results show that the level of test administrator involvement significantly impacts observed linguistic features in patient speech. These results suggest that many of significant linguistic features in the downstream classification task may be partially attributable to differences in the test administration practices rather than solely to participants’ cognitive status. The variations in test administrator behavior can lead to systematic biases in linguistic data, potentially confounding research outcomes and clinical assessments. Our study suggests that there is a need for a more standardized test administration protocol in the development of responsible clinical speech analytics frameworks.</abstract>
      <url hash="7c00206f">2025.cmcl-1.15</url>
      <bibkey>li-etal-2025-anything</bibkey>
    </paper>
    <paper id="16">
      <title>Cross-Framework Generalizable Discourse Relation Classification Through Cognitive Dimensions</title>
      <author><first>Yingxue</first><last>Fu</last><affiliation>Centre Inria d’Université Côte d’Azur</affiliation></author>
      <pages>104-134</pages>
      <abstract>Existing discourse corpora annotated under different frameworks adopt distinct but somewhat related taxonomies of relations. How to integrate discourse frameworks has been an open research question. Previous studies on this topic are mainly theoretical, although such research is typically performed with the hope of benefiting computational applications. In this paper, we show how the proposal by Sanders et al. (2018) based on the Cognitive approach to Coherence Relations (CCR) (Sanders et al.,1992, 1993) can be used effectively to facilitate cross-framework discourse relation (DR) classification. To address the challenges of using predicted UDims for DR classification, we adopt the Bayesian learning framework based on Monte Carlo dropout (Gal and Ghahramani, 2016) to obtain more robust predictions. Data augmentation enabled by our proposed method yields strong performance (55.75 for RST and 55.01 for PDTB implicit DR classification in macro-averaged F1). We compare four model designs and analyze the experimental results from different perspectives. Our study shows an effective and cross-framework generalizable approach for DR classification, filling a gap in existing studies.</abstract>
      <url hash="155bcbaf">2025.cmcl-1.16</url>
      <bibkey>fu-2025-cross</bibkey>
    </paper>
    <paper id="18">
      <title>Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment</title>
      <author><first>Hanlin</first><last>Wu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xufeng</first><last>Duan</last></author>
      <author><first>Zhenguang</first><last>Cai</last></author>
      <pages>135-143</pages>
      <abstract>Voice-based AI development faces unique challenges in processing both linguistic and paralinguistic information. This study compares how large audio-language models (LALMs) and humans integrate speaker characteristics during speech comprehension, asking whether LALMs process speaker-contextualized language in ways that parallel human cognitive mechanisms. We compared two LALMs’ (Qwen2-Audio and Ultravox 0.5) processing patterns with human EEG responses. Using surprisal and entropy metrics from the models, we analyzed their sensitivity to speaker-content incongruency across social stereotype violations (e.g., a man claiming to regularly get manicures) and biological knowledge violations (e.g., a man claiming to be pregnant). Results revealed that Qwen2-Audio exhibited increased surprisal for speaker-incongruent content and its surprisal values significantly predicted human N400 responses, while Ultravox 0.5 showed limited sensitivity to speaker characteristics. Importantly, neither model replicated the human-like processing distinction between social violations (eliciting N400 effects) and biological violations (eliciting P600 effects). These findings reveal both the potential and limitations of current LALMs in processing speaker-contextualized language, and suggest differences in social-linguistic processing mechanisms between humans and LALMs.</abstract>
      <url hash="2c9032c6">2025.cmcl-1.18</url>
      <bibkey>wu-etal-2025-distinct</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>SPACER</fixed-case>: A Parallel Dataset of Speech Production And Comprehension of Error Repairs</title>
      <author><first>Shiva</first><last>Upadhye</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Jiaxuan</first><last>Li</last></author>
      <author><first>Richard</first><last>Futrell</last><affiliation>University of California, Irvine</affiliation></author>
      <pages>144-154</pages>
      <abstract>Speech errors are a natural part of communication, yet they rarely lead to complete communicative failure because both speakers and comprehenders can detect and correct errors. Although prior research has examined error monitoring and correction in production and comprehension separately, integrated investigation of both systems has been impeded by the scarcity of parallel data. In this study, we present SPACER, a parallel dataset that captures how naturalistic speech errors are corrected by both speakers and comprehenders. We focus on single-word substitution errors extracted from the Switchboard speech corpus, accompanied by speaker’s self-repairs and comprehenders’ responses from an offline text-editing experiment. Our exploratory analysis suggests asymmetries in error correction strategies: speakers are more likely to repair errors that introduce greater semantic and phonemic deviations, whereas comprehenders tend to correct errors that are phonemically similar to more plausible alternatives or do not fit into prior contexts. Our dataset enables future research on the integrated approach of language production and comprehension.</abstract>
      <url hash="dd736673">2025.cmcl-1.19</url>
      <bibkey>upadhye-etal-2025-spacer</bibkey>
    </paper>
    <paper id="20">
      <title>Are Larger Language Models Better at Disambiguation?</title>
      <author><first>Ziyuan</first><last>Cao</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>William</first><last>Schuler</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>155-164</pages>
      <abstract>Humans deal with temporary syntactic ambiguity all the time in incremental sentence processing. Sentences with temporary ambiguity that causes processing difficulties, often reflected by increase in reading time, are referred to as garden-path sentences. Garden-path theories of sentence processing attribute the increases in reading time to the reanalysis of the previously ambiguous syntactic structure to make it consistent with the new disambiguating text. It is unknown whether transformer-based language models successfully resolve the temporary ambiguity after encountering the disambiguating text. We investigated this question by analyzing completions generated from language models for a type of garden-path sentence with ambiguity between a complement clause interpretation and a relative clause interpretation. We found that larger language models are worse at resolving such ambiguity.</abstract>
      <url hash="70b74b17">2025.cmcl-1.20</url>
      <bibkey>cao-schuler-2025-larger</bibkey>
    </paper>
    <paper id="21">
      <title>Towards a <fixed-case>B</fixed-case>ayesian hierarchical model of lexical processing</title>
      <author><first>Cassandra L</first><last>Jacobs</last><affiliation>State University of New York, Buffalo</affiliation></author>
      <author><first>Loïc</first><last>Grobol</last><affiliation>Université Paris Nanterre</affiliation></author>
      <pages>165-171</pages>
      <abstract>In cases of pervasive uncertainty, cognitive systems benefit from heuristics or committing to more general hypotheses. Here we have presented a hierarchical cognitive model of lexical processing that synthesizes advances in early rational cognitive models with modern-day neural architectures. Probabilities of higher-order categories derived from layers extracted from the middle layers of an encoder language model have predictive power in accounting for several reading measures for both predicted and unpredicted words and influence even early first fixation duration behavior. The results suggest that lexical processing can take place within a latent, but nevertheless discrete, space in cases of uncertainty.</abstract>
      <url hash="4cf3e670">2025.cmcl-1.21</url>
      <bibkey>jacobs-grobol-2025-towards</bibkey>
    </paper>
    <paper id="22">
      <title>Modeling <fixed-case>C</fixed-case>hinese <fixed-case>L</fixed-case>2 Writing Development: The <fixed-case>LLM</fixed-case>-Surprisal Perspective</title>
      <author><first>Jingying</first><last>Hu</last></author>
      <author><first>Yan</first><last>Cong</last><affiliation>Purdue University</affiliation></author>
      <pages>172-183</pages>
      <abstract>LLM-surprisal is a computational measure of how unexpected a word or character is given the preceding context, as estimated by large language models (LLMs). This study investigated the effectiveness of LLM-surprisal in modeling second language (L2) writing development, focusing on Chinese L2 writing as a case to test its cross-linguistical generalizability. We selected three types of LLMs with different pretraining settings: a multilingual model trained on various languages, a Chinese-general model trained on both Simplified and Traditional Chinese, and a Traditional-Chinese-specific model. This comparison allowed us to explore how model architecture and training data affect LLM-surprisal estimates of learners’ essays written in Traditional Chinese, which in turn influence the modeling of L2 proficiency and development. We also correlated LLM-surprisals with 16 classic linguistic complexity indices (e.g., character sophistication, lexical diversity, syntactic complexity, and discourse coherence) to evaluate its interpretability and validity as a measure of L2 writing assessment. Our findings demonstrate the potential of LLM-surprisal as a robust, interpretable, cross-linguistically applicable metric for automatic writing assessment and contribute to bridging computational and linguistic approaches in understanding and modeling L2 writing development. All analysis scripts are available at https://github.com/JingyingHu/ChineseL2Writing-Surprisals.</abstract>
      <url hash="7954d081">2025.cmcl-1.22</url>
      <bibkey>hu-cong-2025-modeling</bibkey>
    </paper>
    <paper id="23">
      <title>Beyond Binary <fixed-case>A</fixed-case>nimacy: A Multi-Method Investigation of <fixed-case>LM</fixed-case>s’ Sensitivity in <fixed-case>E</fixed-case>nglish Object Relative Clauses</title>
      <author><first>Yue</first><last>Li</last></author>
      <author><first>Yan</first><last>Cong</last><affiliation>Purdue University</affiliation></author>
      <author><first>Elaine J.</first><last>Francis</last><affiliation>Purdue University</affiliation></author>
      <pages>184-196</pages>
      <abstract>Animacy is a well-documented factor affecting language production, but its influence on Language Models (LMs) in complex structures like Object Relative Clauses (ORCs) remains underexplored. This study examines LMs’ sensitivity to animacy in English ORC structure choice (passive vs. active) using surprisal-based and prompting-based analyses, alongside human baselines. In surprisal-based analysis, DistilGPT-2 best mirrored human preferences, while GPT-Neo and BERT-base showed rigid biases, diverging from human patterns. Prompting-based analysis expanded testing to GPT-4o-mini, Gemini models, and DeepSeek-R1, revealing GPT-4o-mini’s stronger human alignment but limited animacy sensitivity in Gemini models and DeepSeek-R1. Some LMs exhibited inconsistencies between analyses, reinforcing that prompting alone is unreliable for assessing linguistic competence. Corpus analysis confirmed that training data alone cannot fully explain animacy sensitivity, suggesting emergent animacy-aware representations. These findings underscore the interaction between training data, model architecture, and linguistic generalization, highlighting the need for integrating structured linguistic knowledge into LMs to enhance their alignment with human sentence processing mechanisms.</abstract>
      <url hash="cf232ac3">2025.cmcl-1.23</url>
      <bibkey>li-etal-2025-beyond</bibkey>
    </paper>
    <paper id="24">
      <title>An Empirical Study of Language Syllabification using Syllabary and Lexical Networks</title>
      <author><first>Rusali</first><last>Saha</last></author>
      <author><first>Yannick</first><last>Marchand</last><affiliation>Dalhousie University</affiliation></author>
      <pages>197-206</pages>
      <abstract>Language syllabification is the separation of a word into written or spoken syllables. The study of syllabification plays a pivotal role in morphology and there have been previous attempts to study this phenomenon using graphs or networks. Previous approaches have claimed through visual estimation that the degree distribution of language networks follows the Power Law distribution, however, there have not been any empirically grounded metrics to determine the same. In our study, we implement two kinds of language networks, namely, syllabary and lexical networks, and investigate the syllabification of four European languages: English, French, German and Spanish using network analysis and examine their small-world, random and scale-free nature. We additionally empirically prove that contrary to claims in previous works, although the degree distribution of these networks appear to follow a power law distribution, they are actually more in agreement with a log-normal distribution, when a numerically grounded curve-fitting is applied. Finally, we explore how syllabary and lexical networks for the English language change over time using a database of age-of-acquisition rating words. Our analysis further shows that the preferential attachment mechanism appears to be a well-grounded explanation for the degree distribution of the syllabary network.</abstract>
      <url hash="cd5ebcf3">2025.cmcl-1.24</url>
      <bibkey>saha-marchand-2025-empirical</bibkey>
    </paper>
    <paper id="25">
      <title>Creolization versus code-switching: An agent-based cognitive model for bilingual strategies in language contact</title>
      <author><first>Charles John</first><last>Torres</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Weijie</first><last>Xu</last></author>
      <author><first>Yanting</first><last>Li</last></author>
      <author><first>Richard</first><last>Futrell</last><affiliation>University of California, Irvine</affiliation></author>
      <pages>207-218</pages>
      <abstract>Creolization and code-switching are closely related contact-induced linguistic phenomena, yet little attention has been paid to the connection between them. In this paper, we propose an agent-based cognitive model which provides a linkage between these two phenomena focusing on the statistical regularization of language use. That is, we identify that creolization as a conventionalization process and code-switching as flexible language choice can emerge from the same cognitive model in different social environments. Our model postulates a social structure of bilingual and monolingual populations, in which a set of agents seek for optimal communicative strategy shaped by multiple cognitive constraints. The simulation results show that our model successfully captures both phenomena as two ends of a continuum, characterized by varying degrees of regularization in the use of linguistic constructions from multiple source languages. The model also reveals a subtle dynamic between social structure and individual-level cognitive constraints.</abstract>
      <url hash="ae39ae41">2025.cmcl-1.25</url>
      <bibkey>torres-etal-2025-creolization</bibkey>
    </paper>
    <paper id="26">
      <title>When Men Bite Dogs: Testing Good-Enough Parsing in <fixed-case>T</fixed-case>urkish with Humans and Large Language Models</title>
      <author><first>Onur</first><last>Keleş</last><affiliation>Bogazici University</affiliation></author>
      <author><first>Nazik Dinctopal</first><last>Deniz</last><affiliation>Bogazici University</affiliation></author>
      <pages>219-231</pages>
      <abstract>This paper investigates good-enough parsing in Turkish by comparing human self-paced reading performance to the surprisal and attention patterns of three Turkish Large Language Models (LLMs), GPT-2-Base, GPT-2-Large, and LLaMA-3. The results show that Turkish speakers rely on good-enough parsing for implausible but grammatically permissible sentences (e.g., interpreting sentences such as ‘the man bit the dog’ as ‘the dog bit the man’). Although the smaller LLMs (e.g., GPT-2) were better predictors of human RTs, they seem to have relied more heavily on semantic plausibility than humans. Comparably, larger LLMs (e.g., LLaMA-3) tended to make more probabilistic parsing based on word order, exhibiting less <i>good-enough</i> parsing behavior. Therefore, we conclude that LLMs take syntactic and semantic constraints into account when processing thematic roles, but not to the same extent as human parsers.</abstract>
      <url hash="fc72f9f8">2025.cmcl-1.26</url>
      <bibkey>keles-deniz-2025-men</bibkey>
    </paper>
    <paper id="27">
      <title>Transformers Can Model Human Hyperprediction in Buzzer Quiz</title>
      <author><first>Yoichiro</first><last>Yamashita</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yuto</first><last>Harada</last><affiliation>Tokyo University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <pages>232-243</pages>
      <abstract>Humans tend to predict the next words during sentence comprehension, but under unique circumstances, they demonstrate an ability for longer coherent word sequence prediction. In this paper, we investigate whether Transformers can model such hyperprediction observed in humans during sentence processing, specifically in the context of Japanese buzzer quizzes. We conducted eye-tracking experiments where the participants read the first half of buzzer quiz questions and predicted the second half, while we modeled their reading time using the GPT-2. By modeling the reading times of each word in the first half of the question using GPT-2 surprisal, we examined under what conditions fine-tuned language models can better predict reading times. As a result, we found that GPT-2 surprisal effectively explains the reading times of quiz experts as they read the first half of the question while predicting the latter half. When the language model was fine-tuned with quiz questions, the perplexity value decreased. Lower perplexity corresponded to higher psychometric predictive power; however, excessive data for fine-tuning led to a decrease in perplexity and the fine-tuned model exhibited a low psychometric predictive power. Overall, our findings suggest that a moderate amount of data is required for fine-tuning in order to model human hyperprediction.</abstract>
      <url hash="c0bcf07a">2025.cmcl-1.27</url>
      <bibkey>yamashita-etal-2025-transformers</bibkey>
    </paper>
    <paper id="28">
      <title>What to Predict? Exploring How Sentence Structure Influences Contrast Predictions in Humans and Large Language Models</title>
      <author><first>Shuqi</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xufeng</first><last>Duan</last></author>
      <author><first>Zhenguang</first><last>Cai</last></author>
      <pages>244-252</pages>
      <abstract>This study examines how sentence structure shapes contrast predictions in both humans and large language models (LLMs). Using Mandarin ditransitive constructions — double object (DO, “She gave the girl the candy, but not...”) vs. prepositional object (PO, “She gave the candy to the girl, but not...”) as a testbed, we employed a sentence continuation task involving three human groups (written, spoken, and prosodically normalized spoken stimuli) and three LLMs (GPT-4o, LLaMA-3, and Qwen-2.5). Two principal findings emerged: (1) Although human participants predominantly focused on the theme (e.g., “the candy”), contrast predictions were significantly modulated by sentence structure—particularly in spoken contexts, where the sentence-final element drew more attention. (2) While LLMs showed a similar reliance on structure, they displayed a larger effect size and more closely resembled human spoken data than written data, indicating a stronger emphasis on linear order in generating contrast predictions. By adopting a unified psycholinguistic paradigm, this study advances our understanding of predictive language processing for both humans and LLMs and informs research on human–model alignment in linguistic tasks.</abstract>
      <url hash="56a3bbad">2025.cmcl-1.28</url>
      <bibkey>wang-etal-2025-predict</bibkey>
    </paper>
    <paper id="30">
      <title>Investigating noun-noun compound relation representations in autoregressive large language models</title>
      <author><first>Saffron</first><last>Kendrick</last><affiliation>The Queen’s University Belfast</affiliation></author>
      <author><first>Mark</first><last>Ormerod</last><affiliation>The Queen’s University Belfast</affiliation></author>
      <author><first>Hui</first><last>Wang</last><affiliation>The Queen’s University Belfast</affiliation></author>
      <author><first>Barry</first><last>Devereux</last><affiliation>Queen’s University Belfast</affiliation></author>
      <pages>253-263</pages>
      <abstract>This paper uses autoregressive large language models to explore at which points in a given input sentence the semantic information is decodable. Using representational similarity analysis and probing, the results show that autoregressive models are capable of extracting the semantic relation information from a dataset of noun-noun compounds. When considering the effect of processing the head and modifier nouns in context, the extracted representations show greater correlation after processing both constituent nouns in the same sentence. The linguistic properties of the head nouns may influence the ability of LLMs to extract relation information when the head and modifier words are processed separately. Probing suggests that Phi-1 and LLaMA-3.2 are exposed to relation information during training, as they are able to predict the relation vectors for compounds from separate word representations to a similar degree as using compositional compound representations. However, the difference in processing condition for GPT-2 and DeepSeek-R1 indicates that these models are actively processing the contextual semantic relation information of the compound.</abstract>
      <url hash="8ab2b25b">2025.cmcl-1.30</url>
      <bibkey>kendrick-etal-2025-investigating</bibkey>
    </paper>
  </volume>
</collection>
