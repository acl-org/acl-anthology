<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.insights">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the First Workshop on Insights from Negative Results in NLP</booktitle>
      <editor><first>Anna</first><last>Rogers</last></editor>
      <editor><first>João</first><last>Sedoc</last></editor>
      <editor><first>Anna</first><last>Rumshisky</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="6e7b4663">2020.insights-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Domain adaptation challenges of <fixed-case>BERT</fixed-case> in tokenization and sub-word representations of Out-of-Vocabulary words</title>
      <author><first>Anmol</first><last>Nayak</last></author>
      <author><first>Hariprasad</first><last>Timmapathini</last></author>
      <author><first>Karthikeyan</first><last>Ponnalagu</last></author>
      <author><first>Vijendran</first><last>Gopalan Venkoparao</last></author>
      <pages>1–5</pages>
      <abstract>BERT model (Devlin et al., 2019) has achieved significant progress in several Natural Language Processing (NLP) tasks by leveraging the multi-head self-attention mechanism (Vaswani et al., 2017) in its architecture. However, it still has several research challenges which are not tackled well for domain specific corpus found in industries. In this paper, we have highlighted these problems through detailed experiments involving analysis of the attention scores and dynamic word embeddings with the BERT-Base-Uncased model. Our experiments have lead to interesting findings that showed: 1) Largest substring from the left that is found in the vocabulary (in-vocab) is always chosen at every sub-word unit that can lead to suboptimal tokenization choices, 2) Semantic meaning of a vocabulary word deteriorates when found as a substring in an Out-Of-Vocabulary (OOV) word, and 3) Minor misspellings in words are inadequately handled. We believe that if these challenges are tackled, it will significantly help the domain adaptation aspect of BERT.</abstract>
      <url hash="d7913652">2020.insights-1.1</url>
      <doi>10.18653/v1/2020.insights-1.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>Q</fixed-case>. Can Knowledge Graphs be used to Answer <fixed-case>B</fixed-case>oolean Questions? <fixed-case>A</fixed-case>. It’s complicated!</title>
      <author><first>Daria</first><last>Dzendzik</last></author>
      <author><first>Carl</first><last>Vogel</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <pages>6–14</pages>
      <abstract>In this paper we explore the problem of machine reading comprehension, focusing on the BoolQ dataset of Yes/No questions. We carry out an error analysis of a BERT-based machine reading comprehension model on this dataset, revealing issues such as unstable model behaviour and some noise within the dataset itself. We then experiment with two approaches for integrating information from knowledge graphs: (i) concatenating knowledge graph triples to text passages and (ii) encoding knowledge with a Graph Neural Network. Neither of these approaches show a clear improvement and we hypothesize that this may be due to a combination of inaccuracies in the knowledge graph, imprecision in entity linking, and the models’ inability to capture additional information from knowledge graphs.</abstract>
      <url hash="02ff6ed9">2020.insights-1.2</url>
      <doi>10.18653/v1/2020.insights-1.2</doi>
    </paper>
    <paper id="3">
      <title>How Far Can We Go with Data Selection? A Case Study on Semantic Sequence Tagging Tasks</title>
      <author><first>Samuel</first><last>Louvan</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <pages>15–21</pages>
      <abstract>Although several works have addressed the role of data selection to improve transfer learning for various NLP tasks, there is no consensus about its real benefits and, more generally, there is a lack of shared practices on how it can be best applied. We propose a systematic approach aimed at evaluating data selection in scenarios of increasing complexity. Specifically, we compare the case in which source and target tasks are the same while source and target domains are different, against the more challenging scenario where both tasks and domains are different. We run a number of experiments on semantic sequence tagging tasks, which are relatively less investigated in data selection, and conclude that data selection has more benefit on the scenario when the tasks are the same, while in case of different (although related) tasks from distant domains, a combination of data selection and multi-task learning is ineffective for most cases.</abstract>
      <url hash="f42a715d">2020.insights-1.3</url>
      <attachment type="OptionalSupplementaryMaterial" hash="fcdcae12">2020.insights-1.3.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.insights-1.3</doi>
    </paper>
    <paper id="4">
      <title>Evaluating the Effectiveness of Efficient Neural Architecture Search for Sentence-Pair Tasks</title>
      <author><first>Ansel</first><last>MacLaughlin</last></author>
      <author><first>Jwala</first><last>Dhamala</last></author>
      <author><first>Anoop</first><last>Kumar</last></author>
      <author><first>Sriram</first><last>Venkatapathy</last></author>
      <author><first>Ragav</first><last>Venkatesan</last></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <pages>22–31</pages>
      <abstract>Neural Architecture Search (NAS) methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art (SOTA) performance on variety of natural language processing and computer vision tasks, including language modeling, natural language inference, and image classification. In this work, we explore the applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search (ENAS) (Pham et al., 2018) to two sentence pair tasks, paraphrase detection and semantic textual similarity. We use ENAS to perform a micro-level search and learn a task-optimized RNN cell architecture as a drop-in replacement for an LSTM. We explore the effectiveness of ENAS through experiments on three datasets (MRPC, SICK, STS-B), with two different models (ESIM, BiLSTM-Max), and two sets of embeddings (Glove, BERT). In contrast to prior work applying ENAS to NLP tasks, our results are mixed – we find that ENAS architectures sometimes, but not always, outperform LSTMs and perform similarly to random architecture search.</abstract>
      <url hash="a43e14e5">2020.insights-1.4</url>
      <doi>10.18653/v1/2020.insights-1.4</doi>
    </paper>
    <paper id="5">
      <title>Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models</title>
      <author><first>Silvia</first><last>Terragni</last></author>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <author><first>Messina</first><last>Enza</last></author>
      <pages>32–40</pages>
      <abstract>Topic models have been widely used to discover hidden topics in a collection of documents. In this paper, we propose to investigate the role of two different types of relational information, i.e. document relationships and concept relationships. While exploiting the document network significantly improves topic coherence, the introduction of concepts and their relationships does not influence the results both quantitatively and qualitatively.</abstract>
      <url hash="7068a284">2020.insights-1.5</url>
      <attachment type="OptionalSupplementaryMaterial" hash="97511234">2020.insights-1.5.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.insights-1.5</doi>
    </paper>
    <paper id="6">
      <title>On Task-Level Dialogue Composition of Generative Transformer Model</title>
      <author><first>Prasanna</first><last>Parthasarathi</last></author>
      <author><first>Sharan</first><last>Narang</last></author>
      <author><first>Arvind</first><last>Neelakantan</last></author>
      <pages>41–47</pages>
      <abstract>Task-oriented dialogue systems help users accomplish tasks such as booking a movie ticket and ordering food via conversation. Generative models parameterized by a deep neural network are widely used for next turn response generation in such systems. It is natural for users of the system to want to accomplish multiple tasks within the same conversation, but the ability of generative models to compose multiple tasks is not well studied. In this work, we begin by studying the effect of training human-human task-oriented dialogues towards improving the ability to compose multiple tasks on Transformer generative models. To that end, we propose and explore two solutions: (1) creating synthetic multiple task dialogue data for training from human-human single task dialogue and (2) forcing the encoder representation to be invariant to single and multiple task dialogues using an auxiliary loss. The results from our experiments highlight the difficulty of even the sophisticated variant of transformer model in learning to compose multiple tasks from single task dialogues.</abstract>
      <url hash="ec3767f7">2020.insights-1.6</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2a607f54">2020.insights-1.6.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.insights-1.6</doi>
    </paper>
    <paper id="7">
      <title>How Effectively Can Machines Defend Against Machine-Generated Fake News? An Empirical Study</title>
      <author><first>Meghana Moorthy</first><last>Bhat</last></author>
      <author><first>Srinivasan</first><last>Parthasarathy</last></author>
      <pages>48–53</pages>
      <abstract>We empirically study the effectiveness of machine-generated fake news detectors by understanding the model’s sensitivity to different synthetic perturbations during test time. The current machine-generated fake news detectors rely on provenance to determine the veracity of news. Our experiments find that the success of these detectors can be limited since they are rarely sensitive to semantic perturbations and are very sensitive to syntactic perturbations. Also, we would like to open-source our code and believe it could be a useful diagnostic tool for evaluating models aimed at fighting machine-generated fake news.</abstract>
      <url hash="26116b08">2020.insights-1.7</url>
      <doi>10.18653/v1/2020.insights-1.7</doi>
    </paper>
    <paper id="8">
      <title>Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification</title>
      <author><first>Ashwin Geet</first><last>D’Sa</last></author>
      <author><first>Irina</first><last>Illina</last></author>
      <author><first>Dominique</first><last>Fohr</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Dana</first><last>Ruiter</last></author>
      <pages>54–59</pages>
      <abstract>Research on hate speech classification has received increased attention. In real-life scenarios, a small amount of labeled hate speech data is available to train a reliable classifier. Semi-supervised learning takes advantage of a small amount of labeled data and a large amount of unlabeled data. In this paper, label propagation-based semi-supervised learning is explored for the task of hate speech classification. The quality of labeling the unlabeled set depends on the input representations. In this work, we show that pre-trained representations are label agnostic, and when used with label propagation yield poor results. Neural network-based fine-tuning can be adopted to learn task-specific representations using a small amount of labeled data. We show that fully fine-tuned representations may not always be the best representations for the label propagation and intermediate representations may perform better in a semi-supervised setup.</abstract>
      <url hash="469cea02">2020.insights-1.8</url>
      <doi>10.18653/v1/2020.insights-1.8</doi>
    </paper>
    <paper id="9">
      <title>Layout-Aware Text Representations Harm Clustering Documents by Type</title>
      <author><first>Catherine</first><last>Finegan-Dollak</last></author>
      <author><first>Ashish</first><last>Verma</last></author>
      <pages>60–65</pages>
      <abstract>Clustering documents by type—grouping invoices with invoices and articles with articles—is a desirable first step for organizing large collections of document scans. Humans approaching this task use both the semantics of the text and the document layout to assist in grouping like documents. LayoutLM (Xu et al., 2019), a layout-aware transformer built on top of BERT with state-of-the-art performance on document-type classification, could reasonably be expected to outperform regular BERT (Devlin et al., 2018) for document-type clustering. However, we find experimentally that BERT significantly outperforms LayoutLM on this task (p &lt;0.001). We analyze clusters to show where layout awareness is an asset and where it is a liability.</abstract>
      <url hash="7664a7ce">2020.insights-1.9</url>
      <doi>10.18653/v1/2020.insights-1.9</doi>
    </paper>
    <paper id="10">
      <title>An Analysis of Capsule Networks for Part of Speech Tagging in High- and Low-resource Scenarios</title>
      <author><first>Andrew</first><last>Zupon</last></author>
      <author><first>Faiz</first><last>Rafique</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>66–70</pages>
      <abstract>Neural networks are a common tool in NLP, but it is not always clear which architecture to use for a given task. Different tasks, different languages, and different training conditions can all affect how a neural network will perform. Capsule Networks (CapsNets) are a relatively new architecture in NLP. Due to their novelty, CapsNets are being used more and more in NLP tasks. However, their usefulness is still mostly untested.In this paper, we compare three neural network architectures—LSTM, CNN, and CapsNet—on a part of speech tagging task. We compare these architectures in both high- and low-resource training conditions and find that no architecture consistently performs the best. Our analysis shows that our CapsNet performs nearly as well as a more complex LSTM under certain training conditions, but not others, and that our CapsNet almost always outperforms our CNN. We also find that our CapsNet implementation shows faster prediction times than the LSTM for Scottish Gaelic but not for Spanish, highlighting the effect that the choice of languages can have on the models.</abstract>
      <url hash="9b48abaf">2020.insights-1.10</url>
      <doi>10.18653/v1/2020.insights-1.10</doi>
    </paper>
    <paper id="11">
      <title>Can Knowledge Graph Embeddings Tell Us What Fact-checked Claims Are About?</title>
      <author><first>Valentina</first><last>Beretta</last></author>
      <author><first>Sébastien</first><last>Harispe</last></author>
      <author><first>Katarina</first><last>Boland</last></author>
      <author><first>Luke</first><last>Lo Seen</last></author>
      <author><first>Konstantin</first><last>Todorov</last></author>
      <author><first>Andon</first><last>Tchechmedjiev</last></author>
      <pages>71–75</pages>
      <abstract>The web offers a wealth of discourse data that help researchers from various fields analyze debates about current societal issues and gauge the effects on society of important phenomena such as misinformation spread. Such analyses often revolve around claims made by people about a given topic of interest. Fact-checking portals offer partially structured information that can assist such analysis. However, exploiting the network structure of such online discourse data is as of yet under-explored. We study the effectiveness of using neural-graph embedding features for claim topic prediction and their complementarity with text embeddings. We show that graph embeddings are modestly complementary with text embeddings, but the low performance of graph embedding features alone indicate that the model fails to capture topological features pertinent of the topic prediction task.</abstract>
      <url hash="d508bfff">2020.insights-1.11</url>
      <doi>10.18653/v1/2020.insights-1.11</doi>
    </paper>
    <paper id="12">
      <title>Do Transformers Dream of Inference, or Can Pretrained Generative Models Learn Implicit Inferential Rules?</title>
      <author><first>Zhengzhong</first><last>Liang</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>76–81</pages>
      <abstract>Large pretrained language models (LM) have been used successfully for multi-hop question answering. However, most of these directions are not interpretable, as they do not make the inference hops necessary to explain a candidate answer explicitly. In this work, we investigate the capability of a state-of-the-art transformer LM to generate explicit inference hops, i.e., to infer a new statement necessary to answer a question given some premise input statements. Our analysis shows that such LMs can generate new statements for some simple inference types, but performance remains poor for complex, real-world inference types such as those that require monotonicity, composition, and commonsense knowledge.</abstract>
      <url hash="95f2216e">2020.insights-1.12</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5d14c4f6">2020.insights-1.12.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.insights-1.12</doi>
    </paper>
    <paper id="13">
      <title>Counterfactually-Augmented <fixed-case>SNLI</fixed-case> Training Data Does Not Yield Better Generalization Than Unaugmented Data</title>
      <author><first>William</first><last>Huang</last></author>
      <author><first>Haokun</first><last>Liu</last></author>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <pages>82–87</pages>
      <abstract>A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks—datasets collected from crowdworkers to create an evaluation task—while still failing on out-of-domain examples for the same task. Recent work has explored the use of counterfactually-augmented data—data built by minimally editing a set of seed examples to yield counterfactual labels—to augment training data associated with these benchmarks and build more robust classifiers that generalize better. However, Khashabi et al. (2020) find that this type of augmentation yields little benefit on reading comprehension tasks when controlling for dataset size and cost of collection. We build upon this work by using English natural language inference data to test model generalization and robustness and find that models trained on a counterfactually-augmented SNLI dataset do not generalize better than unaugmented datasets of similar size and that counterfactual augmentation can hurt performance, yielding models that are less robust to challenge examples. Counterfactual augmentation of natural language understanding data through standard crowdsourcing techniques does not appear to be an effective way of collecting training data and further innovation is required to make this general line of work viable.</abstract>
      <url hash="959b2e63">2020.insights-1.13</url>
      <doi>10.18653/v1/2020.insights-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>NMF</fixed-case> Ensembles? Not for Text Summarization!</title>
      <author><first>Alka</first><last>Khurana</last></author>
      <author><first>Vasudha</first><last>Bhatnagar</last></author>
      <pages>88–93</pages>
      <abstract>Non-negative Matrix Factorization (NMF) has been used for text analytics with promising results. Instability of results arising due to stochastic variations during initialization makes a case for use of ensemble technology. However, our extensive empirical investigation indicates otherwise. In this paper, we establish that ensemble summary for single document using NMF is no better than the best base model summary.</abstract>
      <url hash="aa09bc0b">2020.insights-1.14</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c85871da">2020.insights-1.14.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.insights-1.14</doi>
    </paper>
    <paper id="15">
      <title>If You Build Your Own <fixed-case>NER</fixed-case> Scorer, Non-replicable Results Will Come</title>
      <author><first>Constantine</first><last>Lignos</last></author>
      <author><first>Marjan</first><last>Kamyab</last></author>
      <pages>94–99</pages>
      <abstract>We attempt to replicate a named entity recognition (NER) model implemented in a popular toolkit and discover that a critical barrier to doing so is the inconsistent evaluation of improper label sequences. We define these sequences and examine how two scorers differ in their handling of them, finding that one approach produces F1 scores approximately 0.5 points higher on the CoNLL 2003 English development and test sets. We propose best practices to increase the replicability of NER evaluations by increasing transparency regarding the handling of improper label sequences.</abstract>
      <url hash="c4e4f96b">2020.insights-1.15</url>
      <doi>10.18653/v1/2020.insights-1.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>HINT</fixed-case>3: Raising the bar for Intent Detection in the Wild</title>
      <author><first>Gaurav</first><last>Arora</last></author>
      <author><first>Chirag</first><last>Jain</last></author>
      <author><first>Manas</first><last>Chaturvedi</last></author>
      <author><first>Krupal</first><last>Modi</last></author>
      <pages>100–105</pages>
      <abstract>Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domain-specific aberrations. To facilitate benchmarking which can reflect near real-world scenarios, we introduce 3 new datasets created from live chatbots in diverse domains. Unlike most existing datasets that are crowdsourced, our datasets contain real user queries received by the chatbots and facilitates penalising unwanted correlations grasped during the training process. We evaluate 4 NLU platforms and a BERT based classifier and find that performance saturates at inadequate levels on test sets because all systems latch on to unintended patterns in training data.</abstract>
      <url hash="8cf041fe">2020.insights-1.16</url>
      <doi>10.18653/v1/2020.insights-1.16</doi>
    </paper>
    <paper id="17">
      <title>The Extraordinary Failure of Complement Coercion Crowdsourcing</title>
      <author><first>Yanai</first><last>Elazar</last></author>
      <author><first>Victoria</first><last>Basmov</last></author>
      <author><first>Shauli</first><last>Ravfogel</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <pages>106–116</pages>
      <abstract>Crowdsourcing has eased and scaled up the collection of linguistic annotation in recent years. In this work, we follow known methodologies of collecting labeled data for the complement coercion phenomenon. These are constructions with an implied action — e.g., “I started a new book I bought last week”, where the implied action is reading. We aim to collect annotated data for this phenomenon by reducing it to either of two known tasks: Explicit Completion and Natural Language Inference. However, in both cases, crowdsourcing resulted in low agreement scores, even though we followed the same methodologies as in previous work. Why does the same process fail to yield high agreement scores? We specify our modeling schemes, highlight the differences with previous work and provide some insights about the task and possible explanations for the failure. We conclude that specific phenomena require tailored solutions, not only in specialized algorithms, but also in data collection methods.</abstract>
      <url hash="ec372c47">2020.insights-1.17</url>
      <doi>10.18653/v1/2020.insights-1.17</doi>
    </paper>
    <paper id="18">
      <title>Embedding Structured Dictionary Entries</title>
      <author><first>Steven</first><last>Wilson</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <author><first>Barbara</first><last>McGillivray</last></author>
      <author><first>Gareth</first><last>Tyson</last></author>
      <pages>117–125</pages>
      <abstract>Previous work has shown how to effectively use external resources such as dictionaries to improve English-language word embeddings, either by manipulating the training process or by applying post-hoc adjustments to the embedding space. We experiment with a multi-task learning approach for explicitly incorporating the structured elements of dictionary entries, such as user-assigned tags and usage examples, when learning embeddings for dictionary headwords. Our work generalizes several existing models for learning word embeddings from dictionaries. However, we find that the most effective representations overall are learned by simply training with a skip-gram objective over the concatenated text of all entries in the dictionary, giving no particular focus to the structure of the entries.</abstract>
      <url hash="94b46997">2020.insights-1.18</url>
      <doi>10.18653/v1/2020.insights-1.18</doi>
    </paper>
  </volume>
</collection>
