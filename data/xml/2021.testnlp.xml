<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.testnlp">
  <volume id="1" ingest-date="2021-03-09">
    <meta>
      <booktitle>Proceedings of the The 18th Annual Workshop of the Australasian Language Technology Association</booktitle>
      <editor><first>Maria</first><last>Kim</last></editor>
      <editor><first>Daniel</first><last>Beck</last></editor>
      <editor><first>Meladel</first><last>Mistica</last></editor>
      <publisher>Australasian Language Technology Association</publisher>
      <address>Virtual Workshop</address>
      <month>December</month>
      <year>2021</year>
      <url hash="5ebe2288">2021.testnlp-1</url>
    </meta>
    <frontmatter>
      <url hash="c94ee8a9">2021.testnlp-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Domain Adaptative Causality Encoder</title>
      <author><first>Farhad</first><last>Moghimifar</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Mahsa</first><last>Baktashmotlagh</last></author>
      <pages>1–10</pages>
      <abstract>Automated discovery of causal relationships from text is a challenging task. Current approaches which are mainly based on the extraction of low-level relations among individual events are limited by the shortage of publicly available labelled data. Therefore, the resulting models perform poorly when applied to a distributionally different domain for which labelled data did not exist at the time of training. To overcome this limitation, in this paper, we leverage the characteristics of dependency trees and adversarial learning to address the tasks of adaptive causality identification and localisation. The term adaptive is used since the training and test data come from two distributionally different datasets, which to the best of our knowledge, this work is the first to address. Moreover, we present a new causality dataset, namely MedCaus, which integrates all types of causality in the text. Our experiments on four different benchmark causality datasets demonstrate the superiority of our approach over the existing baselines, by up to 7% improvement, on the tasks of identification and localisation of the causal relations from the text.</abstract>
      <url hash="4489dddb">2021.testnlp-1.1</url>
    </paper>
    <paper id="2">
      <title>Automated Detection of Cyberbullying Against Women and Immigrants and Cross-domain Adaptability</title>
      <author><first>Thushari</first><last>Atapattu</last></author>
      <author><first>Mahen</first><last>Herath</last></author>
      <author><first>Georgia</first><last>Zhang</last></author>
      <author><first>Katrina</first><last>Falkner</last></author>
      <pages>11–20</pages>
      <abstract>Cyberbullying is a prevalent and growing social problem due to the surge of social media technology usage. Minorities, women, and adolescents are among the common victims of cyberbullying. Despite the advancement of NLP technologies, the automated cyberbullying detection remains challenging. This paper focuses on advancing the technology using state-of-the-art NLP techniques. We use a Twitter dataset from SemEval 2019 - Task 5 (HatEval) on hate speech against women and immigrants. Our best performing ensemble model based on DistiBERT has achieved 0.73 and 0.74 of F1 score in the task of classifying hate speech (Task A) and aggressiveness and target (Task B) respectively. We adapt the ensemble model developed for Task A to classify offensive language in external datasets and achieved ~0.7 of F1 score using three benchmark datasets, enabling promising results for cross-domain adaptability. We conduct a qualitative analysis of misclassified tweets to provide insightful recommendations for future cyberbullying research.</abstract>
      <url hash="a5215aa8">2021.testnlp-1.2</url>
    </paper>
    <paper id="3">
      <title>The Influence of Background Data Size on the Performance of a Score-based Likelihood Ratio System: A Case of Forensic Text Comparison</title>
      <author><first>Shunichi</first><last>Ishihara</last></author>
      <pages>21–31</pages>
      <abstract>This study investigates the robustness and stability of a likelihood ratio–based (LR-based) forensic text comparison (FTC) system against the size of background population data. Focus is centred on a score-based approach for estimating authorship LRs. Each document is represented with a bag-of-words model, and the Cosine distance is used as the score-generating function. A set of population data that differed in the number of scores was synthesised 20 times using the Monte-Carol simulation technique. The FTC system’s performance with different population sizes was evaluated by a gradient metric of the log–LR cost (Cllr). The experimental results revealed two outcomes: 1) that the score-based approach is rather robust against a small population size—in that, with the scores obtained from the 40~60 authors in the database, the stability and the performance of the system become fairly comparable to the system with a maximum number of authors (720); and 2) that poor performance in terms of Cllr, which occurred because of limited background population data, is largely due to poor calibration. The results also indicated that the score-based approach is more robust against data scarcity than the feature-based approach; however, this finding obliges further study.</abstract>
      <url hash="fed70c98">2021.testnlp-1.3</url>
    </paper>
  </volume>
</collection>
