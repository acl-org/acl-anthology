<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.pslt">
  <volume id="1" ingest-date="2025-08-07" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Eleventh Workshop on Patent and Scientific Literature Translation (PSLT 2025)</booktitle>
      <editor><first>Takashi</first><last>Tsunakawa</last></editor>
      <editor><first>Katsuhito</first><last>Sudoh</last></editor>
      <editor><first>Isao</first><last>Goto</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Geneva, Switzerland</address>
      <month>June</month>
      <year>2025</year>
      <url hash="a3adabc0">2025.pslt-1</url>
      <venue>pslt</venue>
      <isbn>978-2-9701897-2-5</isbn>
    </meta>
    <frontmatter>
      <url hash="09f052c7">2025.pslt-1.0</url>
      <bibkey>pslt-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>G</fixed-case>en<fixed-case>AI</fixed-case>ese - A Comprehensive Comparison of <fixed-case>GPT</fixed-case>-4o and <fixed-case>D</fixed-case>eep<fixed-case>S</fixed-case>eek-V3 for <fixed-case>E</fixed-case>nglish-to-<fixed-case>C</fixed-case>hinese Academic Translation</title>
      <author><first>Longhui</first><last>Zou</last></author>
      <author><first>Ke</first><last>Li</last></author>
      <author><first>Joshua</first><last>Lamerton</last></author>
      <author><first>Mehdi</first><last>Mirzapour</last></author>
      <pages>1–12</pages>
      <abstract>This study investigates the translation performance of two large language models–ChatGPT-4o and DeepSeek-V3–in translating English academic papers on on language, culture, and literature into Chinese at the discourse level. Using a corpus of 11 academic texts totaling 3,498 sentences, we evaluated translation quality through automatic metrics (COMET-KIWI), lexical diversity indicators, and syntactic complexity measures. Our findings reveal an interesting contrast<tex-math>\colon</tex-math> while DeepSeek-V3 achieves higher overall quality scores, GPT-4o produces translations with consistently greater lexical richness (higher type-token ratio, standardized TTR, average sentence length, and word entropy) and syntactic complexity across all five measured metrics, such as Incomplete Dependency Theory Metric (IDT), Dependency Locality Theory Metric (DLT), Combined IDT+DLT Metric (IDT+DLT), Left-Embeddedness (LE), and Nested Nouns Distance (NND). Particularly notable are GPT-4o’s higher scores in Left-Embeddedness and Nested Nouns Distance metrics, which are specifically relevant to Chinese linguistic patterns. The divergence between automatic quality estimation and linguistic complexity metrics highlights the multifaceted nature of translation quality assessment.</abstract>
      <url hash="50acd7b2">2025.pslt-1.1</url>
      <bibkey>zou-etal-2025-genaiese</bibkey>
    </paper>
    <paper id="2">
      <title>Tailoring Machine Translation for Scientific Literature through Topic Filtering and Fuzzy Match Augmentation</title>
      <author><first>Thomas</first><last>Moerman</last></author>
      <author><first>Tom</first><last>Vanallemeersch</last></author>
      <author><first>Sara</first><last>Szoc</last></author>
      <author><first>Arda</first><last>Tezcan</last></author>
      <pages>13–26</pages>
      <abstract>To enhance the accessibility of scientific literature in multiple languages and facilitate the exchange of information among scholars and a wider audience, there is a need for high-performing specialized machine translation (MT) engines. However, this requires efficient filtering and the use of domain-specific data. In this study, we investigate whether approaches for increasing training data using topic filtering and more efficient use of such data through exploiting fuzzy matches (i.e. similar translations to a given input; FMs) improve translation quality. We apply these techniques both to sequence-to-sequence MT models and off-the-shelf multilingual large language models (LLMs) in three scientific disciplines. Our results suggest that the combination of topic filtering and FM augmentation is an effective strategy for training neural machine translation (NMT) models from scratch, not only surpassing baseline NMT models but also delivering improved translation performance compared to smaller LLMs in terms of the number of parameters. Furthermore, we find that although FM augmentation through in-context learning generally improves LLM translation performance, limited domain-specific datasets can yield results comparable to those achieved with additional multi-domain datasets.</abstract>
      <url hash="c3449d49">2025.pslt-1.2</url>
      <bibkey>moerman-etal-2025-tailoring</bibkey>
    </paper>
  </volume>
</collection>
