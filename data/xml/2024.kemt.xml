<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.kemt">
  <volume id="1" ingest-date="2024-09-16" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First International Workshop on Knowledge-Enhanced Machine Translation</booktitle>
      <editor><first>Arda</first><last>Tezcan</last></editor>
      <editor><first>Víctor M.</first><last>Sánchez-Cartagena</last></editor>
      <editor><first>Miquel</first><last>Esplà-Gomis</last></editor>
      <publisher>European Association for Machine Translation (EAMT)</publisher>
      <address>Sheffield, United Kingdom</address>
      <month>June</month>
      <year>2024</year>
      <url hash="9d0c9df1">2024.kemt-1</url>
      <venue>kemt</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="a8233a23">2024.kemt-1.0</url>
      <bibkey>kemt-2024-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Incorporating Hypernym Features for Improving Low-resource Neural Machine Translation</title>
      <author><first>Abhisek</first><last>Chakrabarty</last></author>
      <author><first>Haiyue</first><last>Song</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Hideki</first><last>Tanaka</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <pages>1-6</pages>
      <abstract>Parallel data is difficult to obtain for low-resource languages in machine translation tasks, making it crucial to leverage monolingual linguistic features as auxiliary information. This article introduces a novel integration of hypernym features into the model by combining learnable hypernym embeddings with word embeddings, providing semantic information. Experimental results based on bilingual and multilingual models showed that: (1) incorporating hypernyms improves translation quality in low-resource settings, yielding +1.7 BLEU scores for bilingual models, (2) the hypernym feature demonstrates efficacy both in isolation and in conjunction with syntactic features, and (3) the performance is influenced by the choice of feature combination operators and hypernym-path hyperparameters.</abstract>
      <url hash="07f512b2">2024.kemt-1.1</url>
      <bibkey>chakrabarty-etal-2024-incorporating</bibkey>
    </paper>
    <paper id="2">
      <title>Exploring Inline Lexicon Injection for Cross-Domain Transfer in Neural Machine Translation</title>
      <author><first>Jesujoba O.</first><last>Alabi</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <pages>7-20</pages>
      <abstract>Domain transfer remains a challenge in machine translation (MT), particularly concerning rare or unseen words. Amongst the strategies proposed to address the issue, one of the simplest and most promising in terms of generalisation capacity is coupling the MT system with external resources such as bilingual lexicons and appending inline annotations within source sentences. This method has been shown to work well for controlled language settings, but its usability for general language (and ambiguous) MT is less certain. In this article we explore this question further, testing the strategy in a multi-domain transfer setting for German-to-English MT, using the mT5 language model fine-tuned on parallel data. We analyse the MT outputs and design evaluation strategies to understand the behaviour of such models. Our analysis using distractor annotations suggests that although improvements are not systematic according to automatic metrics, the model does learn to select appropriate translation candidates and ignore irrelevant ones, thereby exhibiting more than a systematic copying behaviour. However, we also find that the method is less successful in a higher-resource setting with a larger lexicon, suggesting that it is not a magic solution, especially when the baseline model is already exposed to a wide range of vocabulary.</abstract>
      <url hash="849e8d59">2024.kemt-1.2</url>
      <bibkey>alabi-bawden-2024-exploring</bibkey>
    </paper>
    <paper id="3">
      <title>Adding soft terminology constraints to pre-trained generic <fixed-case>MT</fixed-case> models by means of continued training</title>
      <author><first>Tommi</first><last>Nieminen</last></author>
      <pages>21-33</pages>
      <abstract>This article describes an efficient method of adding terminology support to existing machine translation models. The training of the pre-trained models is continued with parallel data where strings identified as terms in the source language data have been annotated with the lemmas of the corresponding target terms. Evaluation using standard test sets and methods confirms that continued training from generic base models can produce term models that are competitive with models specifically trained as term models.</abstract>
      <url hash="6ff152dd">2024.kemt-1.3</url>
      <bibkey>nieminen-2024-adding</bibkey>
    </paper>
    <paper id="4">
      <title>Leveraging Synthetic Monolingual Data for Fuzzy-Match Augmentation in Neural Machine Translation: A Preliminary Study</title>
      <author><first>Thomas</first><last>Moerman</last></author>
      <author><first>Arda</first><last>Tezcan</last></author>
      <pages>34-39</pages>
      <url hash="ec0e44b4">2024.kemt-1.4</url>
      <bibkey>moerman-tezcan-2024-leveraging</bibkey>
    </paper>
    <paper id="5">
      <title>Can True Zero-shot Methods with Large Language Models be Adopted for Sign Language Machine Translation?</title>
      <author><first>Euan</first><last>McGill</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>40-43</pages>
      <url hash="f691ba19">2024.kemt-1.5</url>
      <bibkey>mcgill-saggion-2024-true</bibkey>
    </paper>
  </volume>
</collection>
