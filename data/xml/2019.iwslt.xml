<?xml version='1.0' encoding='UTF-8'?>
<collection id="2019.iwslt">
  <volume id="1" ingest-date="2022-02-17">
    <meta>
      <booktitle>Proceedings of the 16th International Conference on Spoken Language Translation</booktitle>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong</address>
      <month>November 2-3</month>
      <year>2019</year>
      <editor><first>Jan</first><last>Niehues</last></editor>
      <editor><first>Rolando</first><last>Cattoni</last></editor>
      <editor><first>Sebastian</first><last>Stüker</last></editor>
      <editor><first>Matteo</first><last>Negri</last></editor>
      <editor><first>Marco</first><last>Turchi</last></editor>
      <editor><first>Thanh-Le</first><last>Ha</last></editor>
      <editor><first>Elizabeth</first><last>Salesky</last></editor>
      <editor><first>Ramon</first><last>Sanabria</last></editor>
      <editor><first>Loic</first><last>Barrault</last></editor>
      <editor><first>Lucia</first><last>Specia</last></editor>
      <editor><first>Marcello</first><last>Federico</last></editor>
      <venue>iwslt</venue>
    </meta>
    <paper id="1">
      <title>The <fixed-case>IWSLT</fixed-case> 2019 Evaluation Campaign</title>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Rolando</first><last>Cattoni</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Thanh-Le</first><last>Ha</last></author>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>Ramon</first><last>Sanabria</last></author>
      <author><first>Loic</first><last>Barrault</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <abstract>The IWSLT 2019 evaluation campaign featured three tasks: speech translation of (i) TED talks and (ii) How2 instructional videos from English into German and Portuguese, and (iii) text translation of TED talks from English into Czech. For the first two tasks we encouraged submissions of end- to-end speech-to-text systems, and for the second task participants could also use the video as additional input. We received submissions by 12 research teams. This overview provides detailed descriptions of the data and evaluation conditions of each task and reports results of the participating systems.</abstract>
      <url hash="96ae3b28">2019.iwslt-1.1</url>
      <bibkey>niehues-etal-2019-iwslt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="2">
      <title><fixed-case>OPPO</fixed-case> <fixed-case>NMT</fixed-case> System for <fixed-case>IWSLT</fixed-case> 2019</title>
      <author><first>Xiaopu</first><last>Li</last></author>
      <author><first>Zhengshan</first><last>Xue</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <abstract>This paper illustrates the OPPO's submission for IWSLT2019 text translation task Our system is based on Transformer architecture. Besides, we also study the effect of model ensembling. On the devsets of IWSLT 2019, the BLEU of our system reaches 19.94.</abstract>
      <url hash="e5a53769">2019.iwslt-1.2</url>
      <bibkey>li-etal-2019-oppo</bibkey>
    </paper>
    <paper id="3">
      <title>The <fixed-case>IWSLT</fixed-case> 2019 <fixed-case>KIT</fixed-case> Speech Translation System</title>
      <author><first>Ngoc-Quan</first><last>Pham</last></author>
      <author><first>Thai-Son</first><last>Nguyen</last></author>
      <author><first>Thanh-Le</first><last>Ha</last></author>
      <author><first>Juan</first><last>Hussain</last></author>
      <author><first>Felix</first><last>Schneider</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <abstract>This paper describes KIT’s submission to the IWSLT 2019 Speech Translation task on two sub-tasks corresponding to two different datasets. We investigate different end-to-end architectures for the speech recognition module, including our new transformer-based architectures. Overall, our modules in the pipe-line are based on the transformer architecture which has recently achieved great results in various fields. In our systems, using transformer is also advantageous compared to traditional hybrid systems in term of simplicity while still having competent results.</abstract>
      <url hash="208726cc">2019.iwslt-1.3</url>
      <bibkey>pham-etal-2019-iwslt</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>ESP</fixed-case>net How2 Speech Translation System for <fixed-case>IWSLT</fixed-case> 2019: Pre-training, Knowledge Distillation, and Going Deeper</title>
      <author><first>Hirofumi</first><last>Inaguma</last></author>
      <author><first>Shun</first><last>Kiyono</last></author>
      <author><first>Nelson Enrique Yalta</first><last>Soplin</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <abstract>This paper describes the ESPnet submissions to the How2 Speech Translation task at IWSLT2019. In this year, we mainly build our systems based on Transformer architectures in all tasks and focus on the end-to-end speech translation (E2E-ST). We first compare RNN-based models and Transformer, and then confirm Transformer models significantly and consistently outperform RNN models in all tasks and corpora. Next, we investigate pre-training of E2E-ST models with the ASR and MT tasks. On top of the pre-training, we further explore knowledge distillation from the NMT model and the deeper speech encoder, and confirm drastic improvements over the baseline model. All of our codes are publicly available in ESPnet.</abstract>
      <url hash="be527f4e">2019.iwslt-1.4</url>
      <bibkey>inaguma-etal-2019-espnet</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>ON</fixed-case>-<fixed-case>TRAC</fixed-case> Consortium End-to-End Speech Translation Systems for the <fixed-case>IWSLT</fixed-case> 2019 Shared Task</title>
      <author><first>Ha</first><last>Nguyen</last></author>
      <abstract>This paper describes the ON-TRAC Consortium translation systems developed for the end-to-end model task of IWSLT Evaluation 2019 for the English→ Portuguese language pair. ON-TRAC Consortium is composed of researchers from three French academic laboratories: LIA (Avignon Université), LIG (Université Grenoble Alpes), and LIUM (Le Mans Université). A single end-to-end model built as a neural encoder-decoder architecture with attention mechanism was used for two primary submissions corresponding to the two EN-PT evaluations sets: (1) TED (MuST-C) and (2) How2. In this paper, we notably investigate impact of pooling heterogeneous corpora for training, impact of target tokenization (characters or BPEs), impact of speech input segmentation and we also compare our best end-to-end model (BLEU of 26.91 on MuST-C and 43.82 on How2 validation sets) to a pipeline (ASR+MT) approach.</abstract>
      <url hash="4b4966fb">2019.iwslt-1.5</url>
      <bibkey>nguyen-2019-trac</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ted-lium-3">TED-LIUM 3</pwcdataset>
    </paper>
    <paper id="6">
      <title>Transformer-based Cascaded Multimodal Speech Translation</title>
      <author><first>Zixiu</first><last>Wu</last></author>
      <author><first>Ozan</first><last>Caglayan</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Josiah</first><last>Wang</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <abstract>This paper describes the cascaded multimodal speech translation systems developed by Imperial College London for the IWSLT 2019 evaluation campaign. The architecture consists of an automatic speech recognition (ASR) system followed by a Transformer-based multimodal machine translation (MMT) system. While the ASR component is identical across the experiments, the MMT model varies in terms of the way of integrating the visual context (simple conditioning vs. attention), the type of visual features exploited (pooled, convolutional, action categories) and the underlying architecture. For the latter, we explore both the canonical transformer and its deliberation version with additive and cascade variants which differ in how they integrate the textual attention. Upon conducting extensive experiments, we found that (i) the explored visual integration schemes often harm the translation performance for the transformer and additive deliberation, but considerably improve the cascade deliberation; (ii) the transformer and cascade deliberation integrate the visual modality better than the additive deliberation, as shown by the incongruence analysis.</abstract>
      <url hash="3a1fe36d">2019.iwslt-1.6</url>
      <bibkey>wu-etal-2019-transformer</bibkey>
    </paper>
    <paper id="7">
      <title>End-to-end Speech Translation System Description of <fixed-case>LIT</fixed-case> for <fixed-case>IWSLT</fixed-case> 2019</title>
      <author><first>Mei</first><last>Tu</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Lijie</first><last>Wang</last></author>
      <author><first>Xiao</first><last>Chen</last></author>
      <author><first>Xue</first><last>Wen</last></author>
      <abstract>This paper describes our end-to-end speech translation system for the speech translation task of lectures and TED talks from English to German for IWSLT Evaluation 2019. We propose layer-tied self-attention for end-to-end speech translation. Our method takes advantage of sharing weights of speech encoder and text decoder. The representation of source speech and the representation of target text are coordinated layer by layer, so that the speech and text can learn a better alignment during the training procedure. We also adopt data augmentation to enhance the parallel speech-text corpus. The En-De experimental results show that our best model achieves 17.68 on tst2015. Our ASR achieves WER of 6.6% on TED-LIUM test set. The En-Pt model can achieve about 11.83 on the MuST-C dev set.</abstract>
      <url hash="646debfe">2019.iwslt-1.7</url>
      <bibkey>tu-etal-2019-end</bibkey>
    </paper>
    <paper id="8">
      <title>Domain Adaptation of Document-Level <fixed-case>NMT</fixed-case> in <fixed-case>IWSLT</fixed-case>19</title>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <abstract>We describe our four NMT systems submitted to the IWSLT19 shared task in English→Czech text-to-text translation of TED talks. The goal of this study is to understand the interactions between document-level NMT and domain adaptation. All our systems are based on the Transformer model implemented in the Tensor2Tensor framework. Two of the systems serve as baselines, which are not adapted to the TED talks domain: SENTBASE is trained on single sen- tences, DOCBASE on multi-sentence (document-level) sequences. The other two submitted systems are adapted to TED talks: SENTFINE is fine-tuned on single sentences, DOCFINE is fine-tuned on multi-sentence sequences. We present both automatic-metrics evaluation and manual analysis of the translation quality, focusing on the differences between the four systems.</abstract>
      <url hash="49afa7a8">2019.iwslt-1.8</url>
      <bibkey>popel-federmann-2019-domain</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2018">WMT 2018</pwcdataset>
    </paper>
    <paper id="9">
      <title><fixed-case>S</fixed-case>amsung and <fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh’s System for the <fixed-case>IWSLT</fixed-case> 2019</title>
      <author><first>Joanna</first><last>Wetesko</last></author>
      <author><first>Marcin</first><last>Chochowski</last></author>
      <author><first>Pawel</first><last>Przybysz</last></author>
      <author><first>Philip</first><last>Williams</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first/><last>Barone</last></author>
      <author><first>Valerio</first><last>Miceli</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <abstract>This paper describes the joint submission to the IWSLT 2019 English to Czech task by Samsung RD Institute, Poland, and the University of Edinburgh. Our submission was ultimately produced by combining four Transformer systems through a mixture of ensembling and reranking.</abstract>
      <url hash="87f90f3a">2019.iwslt-1.9</url>
      <bibkey>wetesko-etal-2019-samsung</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>CMU</fixed-case>’s Machine Translation System for <fixed-case>IWSLT</fixed-case> 2019</title>
      <author><first>Tejas</first><last>Srinivasan</last></author>
      <author><first>Ramon</first><last>Sanabria</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <abstract>In Neural Machine Translation (NMT) the usage of sub-words and characters as source and target units offers a simple and flexible solution for translation of rare and unseen words. However, selecting the optimal subword segmentation involves a trade-off between expressiveness and flexibility, and is language and dataset-dependent. We present Block Multitask Learning (BMTL), a novel NMT architecture that predicts multiple targets of different granularities simulta- neously, removing the need to search for the optimal seg- mentation strategy. Our multi-task model exhibits improvements of up to 1.7 BLEU points on each decoder over single-task baseline models with the same number of parameters on datasets from two language pairs of IWSLT15 and one from IWSLT19. The multiple hypotheses generated at different granularities can also be combined as a post-processing step to give better translations.</abstract>
      <url hash="1d1b67f8">2019.iwslt-1.10</url>
      <bibkey>srinivasan-etal-2019-cmus</bibkey>
    </paper>
    <paper id="11">
      <title>The <fixed-case>LIG</fixed-case> system for the <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>zech Text Translation Task of <fixed-case>IWSLT</fixed-case> 2019</title>
      <author><first>Loïc</first><last>Vial</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Hang</first><last>Le</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <abstract>In this paper, we present our submission for the English to Czech Text Translation Task of IWSLT 2019. Our system aims to study how pre-trained language models, used as input embeddings, can improve a specialized machine translation system trained on few data. Therefore, we implemented a Transformer-based encoder-decoder neural system which is able to use the output of a pre-trained language model as input embeddings, and we compared its performance under three configurations: 1) without any pre-trained language model (constrained), 2) using a language model trained on the monolingual parts of the allowed English-Czech data (constrained), and 3) using a language model trained on a large quantity of external monolingual data (unconstrained). We used BERT as external pre-trained language model (configuration 3), and BERT architecture for training our own language model (configuration 2). Regarding the training data, we trained our MT system on a small quantity of parallel text: one set only consists of the provided MuST-C corpus, and the other set consists of the MuST-C corpus and the News Commentary corpus from WMT. We observed that using the external pre-trained BERT improves the scores of our system by +0.8 to +1.5 of BLEU on our development set, and +0.97 to +1.94 of BLEU on the test set. However, using our own language model trained only on the allowed parallel data seems to improve the machine translation performances only when the system is trained on the smallest dataset.</abstract>
      <url hash="61d6c2fb">2019.iwslt-1.11</url>
      <bibkey>vial-etal-2019-lig</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="12">
      <title><fixed-case>S</fixed-case>amsung’s System for the <fixed-case>IWSLT</fixed-case> 2019 End-to-End Speech Translation Task</title>
      <author><first>Tomasz</first><last>Potapczyk</last></author>
      <author><first>Pawel</first><last>Przybysz</last></author>
      <author><first>Marcin</first><last>Chochowski</last></author>
      <author><first>Artur</first><last>Szumaczuk</last></author>
      <abstract>This paper describes the submission to IWSLT 2019 End- to-End speech translation task by Samsung R&amp;D Institute, Poland. We decided to focus on end-to-end English to German TED lectures translation and did not provide any submission for other speech tasks. We used a slightly altered Transformer architecture with standard convolutional layer preparing the audio input to Transformer en- coder. Additionally, we propose an audio segmentation al- gorithm maximizing BLEU score on tst2015 test set.</abstract>
      <url hash="90e5d558">2019.iwslt-1.12</url>
      <bibkey>potapczyk-etal-2019-samsungs</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="13">
      <title><fixed-case>KIT</fixed-case>’s Submission to the <fixed-case>IWSLT</fixed-case> 2019 Shared Task on Text Translation</title>
      <author><first>Felix</first><last>Schneider</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <abstract>In this paper, we describe KIT’s submission for the IWSLT 2019 shared task on text translation. Our system is based on the transformer model [1] using our in-house implementation. We augment the available training data using back-translation and employ fine-tuning for the final model. For our best results, we used a 12-layer transformer-big config- uration, achieving state-of-the-art results on the WMT2018 test set. We also experiment with student-teacher models to improve performance of smaller models.</abstract>
      <url hash="a61bab92">2019.iwslt-1.13</url>
      <bibkey>schneider-waibel-2019-kits</bibkey>
    </paper>
    <paper id="14">
      <title>Data Augmentation for End-to-End Speech Translation: <fixed-case>FBK</fixed-case>@<fixed-case>IWSLT</fixed-case> ‘19</title>
      <author><first>Mattia A.</first><last>Di Gangi</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Viet Nhat</first><last>Nguyen</last></author>
      <author><first>Amirhossein</first><last>Tebbifakhr</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <abstract>This paper describes FBK’s submission to the end-to-end speech translation (ST) task at IWSLT 2019. The task consists in the “direct” translation (i.e. without intermediate discrete representation) of English speech data derived from TED Talks or lectures into German texts. Our participation had a twofold goal: i) testing our latest models, and ii) eval- uating the contribution to model training of different data augmentation techniques. On the model side, we deployed our recently proposed S-Transformer with logarithmic distance penalty, an ST-oriented adaptation of the Transformer architecture widely used in machine translation (MT). On the training side, we focused on data augmentation techniques recently proposed for ST and automatic speech recognition (ASR). In particular, we exploited augmented data in different ways and at different stages of the process. We first trained an end-to-end ASR system and used the weights of its encoder to initialize the decoder of our ST model (transfer learning). Then, we used an English-German MT system trained on large data to translate the English side of the English-French training set into German, and used this newly-created data as additional training material. Finally, we trained our models using SpecAugment, an augmentation technique that randomly masks portions of the spectrograms in order to make them different at every training epoch. Our synthetic corpus and SpecAugment resulted in an improvement of 5 BLEU points over our baseline model on the test set of MuST-C En-De, reaching the score of 22.3 with a single end-to-end system.</abstract>
      <url hash="9be8df11">2019.iwslt-1.14</url>
      <bibkey>di-gangi-etal-2019-data</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="15">
      <title>How Transformer Revitalizes Character-based Neural Machine Translation: An Investigation on <fixed-case>J</fixed-case>apanese-<fixed-case>V</fixed-case>ietnamese Translation Systems</title>
      <author><first>Thi-Vinh</first><last>Ngo</last></author>
      <author><first>Thanh-Le</first><last>Ha</last></author>
      <author><first>Phuong-Thai</first><last>Nguyen</last></author>
      <author><first>Le-Minh</first><last>Nguyen</last></author>
      <abstract>While translating between East Asian languages, many works have discovered clear advantages of using characters as the translation unit. Unfortunately, traditional recurrent neural machine translation systems hinder the practical usage of those character-based systems due to their architectural limitations. They are unfavorable in handling extremely long sequences as well as highly restricted in parallelizing the computations. In this paper, we demonstrate that the new transformer architecture can perform character-based trans- lation better than the recurrent one. We conduct experiments on a low-resource language pair: Japanese-Vietnamese. Our models considerably outperform the state-of-the-art systems which employ word-based recurrent architectures.</abstract>
      <url hash="b077720d">2019.iwslt-1.15</url>
      <bibkey>ngo-etal-2019-transformer</bibkey>
      <pwccode url="https://github.com/ngovinhtn/charTransform" additional="false">ngovinhtn/charTransform</pwccode>
    </paper>
    <paper id="16">
      <title>Adapting Multilingual Neural Machine Translation to Unseen Languages</title>
      <author><first>Surafel M.</first><last>Lakew</last></author>
      <author><first>Alina</first><last>Karakanta</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <abstract>Multilingual Neural Machine Translation (MNMT) for low- resource languages (LRL) can be enhanced by the presence of related high-resource languages (HRL), but the relatedness of HRL usually relies on predefined linguistic assumptions about language similarity. Recently, adapting MNMT to a LRL has shown to greatly improve performance. In this work, we explore the problem of adapting an MNMT model to an unseen LRL using data selection and model adapta- tion. In order to improve NMT for LRL, we employ perplexity to select HRL data that are most similar to the LRL on the basis of language distance. We extensively explore data selection in popular multilingual NMT settings, namely in (zero-shot) translation, and in adaptation from a multilingual pre-trained model, for both directions (LRL↔en). We further show that dynamic adaptation of the model’s vocabulary results in a more favourable segmentation for the LRL in comparison with direct adaptation. Experiments show re- ductions in training time and significant performance gains over LRL baselines, even with zero LRL data (+13.0 BLEU), up to +17.0 BLEU for pre-trained multilingual model dynamic adaptation with related data selection. Our method outperforms current approaches, such as massively multilingual models and data augmentation, on four LRL.</abstract>
      <url hash="1b241b83">2019.iwslt-1.16</url>
      <bibkey>lakew-etal-2019-adapting</bibkey>
      <pwccode url="https://github.com/surafelml/adapt-mnmt" additional="false">surafelml/adapt-mnmt</pwccode>
    </paper>
    <paper id="17">
      <title>Transformers without Tears: Improving the Normalization of Self-Attention</title>
      <author><first>Toan Q.</first><last>Nguyen</last></author>
      <author><first>Julian</first><last>Salazar</last></author>
      <abstract>We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PRENORM) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose l2 normalization with a single scale parameter (SCALENORM) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FIXNORM). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We ob- serve sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM remain competitive but PRENORM degrades performance.</abstract>
      <url hash="7cc09ad6">2019.iwslt-1.17</url>
      <bibkey>nguyen-salazar-2019-transformers</bibkey>
      <pwccode url="https://github.com/tnq177/transformers_without_tears" additional="true">tnq177/transformers_without_tears</pwccode>
    </paper>
    <paper id="18">
      <title>Harnessing Indirect Training Data for End-to-End Automatic Speech Translation: Tricks of the Trade</title>
      <author><first>Juan</first><last>Pino</last></author>
      <author><first>Liezl</first><last>Puzon</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Xutai</first><last>Ma</last></author>
      <author><first>Arya D.</first><last>McCarthy</last></author>
      <author><first>Deepak</first><last>Gopinath</last></author>
      <abstract>For automatic speech translation (AST), end-to-end approaches are outperformed by cascaded models that transcribe with automatic speech recognition (ASR), then trans- late with machine translation (MT). A major cause of the performance gap is that, while existing AST corpora are small, massive datasets exist for both the ASR and MT subsystems. In this work, we evaluate several data augmentation and pretraining approaches for AST, by comparing all on the same datasets. Simple data augmentation by translating ASR transcripts proves most effective on the English–French augmented LibriSpeech dataset, closing the performance gap from 8.2 to 1.4 BLEU, compared to a very strong cascade that could directly utilize copious ASR and MT data. The same end-to-end approach plus fine-tuning closes the gap on the English–Romanian MuST-C dataset from 6.7 to 3.7 BLEU. In addition to these results, we present practical rec- ommendations for augmentation and pretraining approaches. Finally, we decrease the performance gap to 0.01 BLEU us- ing a Transformer-based architecture.</abstract>
      <url hash="2c55855c">2019.iwslt-1.18</url>
      <bibkey>pino-etal-2019-harnessing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="19">
      <title>Neural Baselines for Word Alignment</title>
      <author><first>Anh Khoa Ngo</first><last>Ho</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <abstract>Word alignments identify translational correspondences between words in a parallel sentence pair and is used, for instance, to learn bilingual dictionaries, to train statistical machine translation systems, or to perform quality estimation. In most areas of natural lan- guage processing, neural network models nowadays constitute the preferred approach, a situation that might also apply to word align- ment models. In this work, we study and comprehensively evaluate neural models for unsupervised word alignment for four language pairs, contrasting several variants of neural models. We show that in most settings, neural versions of the IBM-1 and hidden Markov models vastly outperform their discrete counterparts. We also analyze typical alignment errors of the baselines that our models over- come to illustrate the benefits — and the limitations — of these new models for morphologically rich languages.</abstract>
      <url hash="f88d3093">2019.iwslt-1.19</url>
      <bibkey>ho-yvon-2019-neural</bibkey>
    </paper>
    <paper id="20">
      <title>Analysis of Positional Encodings for Neural Machine Translation</title>
      <author><first>Jan</first><last>Rosendahl</last></author>
      <author><first>Viet Anh Khoa</first><last>Tran</last></author>
      <author><first>Weiyue</first><last>Wang</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <abstract>In this work we analyze and compare the behavior of the Transformer architecture when using different positional encoding methods. While absolute and relative positional encoding perform equally strong overall, we show that relative positional encoding is vastly superior (4.4% to 11.9% BLEU) when translating a sentence that is longer than any observed training sentence. We further propose and analyze variations of relative positional encoding and observe that the number of trainable parameters can be reduced without a performance loss, by using fixed encoding vectors or by removing some of the positional encoding vectors.</abstract>
      <url hash="533f0046">2019.iwslt-1.20</url>
      <bibkey>rosendahl-etal-2019-analysis</bibkey>
    </paper>
    <paper id="21">
      <title>Using Whole Document Context in Neural Machine Translation</title>
      <author><first>Valentin</first><last>Macé</last></author>
      <author><first>Christophe</first><last>Servan</last></author>
      <abstract>In Machine Translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a simple yet promising approach to add contextual information in Neural Machine Translation. We present a method to add source context that capture the whole document with accurate boundaries, taking every word into account. We provide this additional information to a Transformer model and study the impact of our method on three language pairs. The proposed approach obtains promising results in the English-German, English-French and French-English document-level translation tasks. We observe interesting cross-sentential behaviors where the model learns to use document-level information to improve translation coherence.</abstract>
      <url hash="8fabd46c">2019.iwslt-1.21</url>
      <bibkey>mace-servan-2019-using</bibkey>
    </paper>
    <paper id="22">
      <title>On Using <fixed-case>S</fixed-case>pec<fixed-case>A</fixed-case>ugment for End-to-End Speech Translation</title>
      <author><first>Parnia</first><last>Bahar</last></author>
      <author><first>Albert</first><last>Zeyer</last></author>
      <author><first>Ralf</first><last>Schlüter</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <abstract>This work investigates a simple data augmentation technique, SpecAugment, for end-to-end speech translation. SpecAugment is a low-cost implementation method applied directly to the audio input features and it consists of masking blocks of frequency channels, and/or time steps. We apply SpecAugment on end-to-end speech translation tasks and achieve up to +2.2% BLEU on LibriSpeech Audiobooks En→Fr and +1.2% on IWSLT TED-talks En→De by alleviating overfitting to some extent. We also examine the effectiveness of the method in a variety of data scenarios and show that the method also leads to significant improvements in various data conditions irrespective of the amount of training data.</abstract>
      <url hash="e3ecaec4">2019.iwslt-1.22</url>
      <bibkey>bahar-etal-2019-using</bibkey>
    </paper>
    <paper id="23">
      <title>Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of <fixed-case>MT</fixed-case> quality</title>
      <author><first>Scarton</first><last>Scarton</last></author>
      <author><first>Mikel L.</first><last>Forcada</last></author>
      <author><first>Miquel</first><last>Esplà-Gomis</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <abstract>Devising metrics to assess translation quality has always been at the core of machine translation (MT) research. Traditional automatic reference-based metrics, such as BLEU, have shown correlations with human judgements of adequacy and fluency and have been paramount for the advancement of MT system development. Crowd-sourcing has popularised and enabled the scalability of metrics based on human judgments, such as subjective direct assessments (DA) of adequacy, that are believed to be more reliable than reference-based automatic metrics. Finally, task-based measurements, such as post-editing time, are expected to provide a more de- tailed evaluation of the usefulness of translations for a specific task. Therefore, while DA averages adequacy judgements to obtain an appraisal of (perceived) quality independently of the task, and reference-based automatic metrics try to objectively estimate quality also in a task-independent way, task-based metrics are measurements obtained either during or after performing a specific task. In this paper we argue that, although expensive, task-based measurements are the most reliable when estimating MT quality in a specific task; in our case, this task is post-editing. To that end, we report experiments on a dataset with newly-collected post-editing indicators and show their usefulness when estimating post-editing effort. Our results show that task-based metrics comparing machine-translated and post-edited versions are the best at tracking post-editing effort, as expected. These metrics are followed by DA, and then by metrics comparing the machine-translated version and independent references. We suggest that MT practitioners should be aware of these differences and acknowledge their implications when decid- ing how to evaluate MT for post-editing purposes.</abstract>
      <url hash="8a61b1b6">2019.iwslt-1.23</url>
      <bibkey>scarton-etal-2019-estimating</bibkey>
      <pwccode url="https://github.com/carolscarton/iwslt2019" additional="false">carolscarton/iwslt2019</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iwslt-2019">IWSLT 2019</pwcdataset>
    </paper>
    <paper id="24">
      <title>Exploring Kernel Functions in the Softmax Layer for Contextual Word Classification</title>
      <author><first>Yingbo</first><last>Gao</last></author>
      <author><first>Christian</first><last>Herold</last></author>
      <author><first>Weiyue</first><last>Wang</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <abstract>Prominently used in support vector machines and logistic re-gressions, kernel functions (kernels) can implicitly map data points into high dimensional spaces and make it easier to learn complex decision boundaries. In this work, by replacing the inner product function in the softmax layer, we explore the use of kernels for contextual word classification. In order to compare the individual kernels, experiments are conducted on standard language modeling and machine translation tasks. We observe a wide range of performances across different kernel settings. Extending the results, we look at the gradient properties, investigate various mixture strategies and examine the disambiguation abilities.</abstract>
      <url hash="f68150f6">2019.iwslt-1.24</url>
      <bibkey>gao-etal-2019-exploring</bibkey>
    </paper>
    <paper id="25">
      <title>Multitask Learning For Different Subword Segmentations In Neural Machine Translation</title>
      <author><first>Tejas</first><last>Srinivasan</last></author>
      <author><first>Ramon</first><last>Sanabria</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <abstract>In Neural Machine Translation (NMT) the usage of sub􏰃words and characters as source and target units offers a simple and flexible solution for translation of rare and unseen words. However, selecting the optimal subword segmentation involves a trade-off between expressiveness and flexibility, and is language and dataset-dependent. We present Block Multitask Learning (BMTL), a novel NMT architecture that predicts multiple targets of different granularities simultaneously, removing the need to search for the optimal segmentation strategy. Our multi-task model exhibits improvements of up to 1.7 BLEU points on each decoder over single-task baseline models with the same number of parameters on datasets from two language pairs of IWSLT15 and one from IWSLT19. The multiple hypotheses generated at different granularities can be combined as a post-processing step to give better translations, which improves over hypothesis combination from baseline models while using substantially fewer parameters.</abstract>
      <url hash="6fa8cead">2019.iwslt-1.25</url>
      <bibkey>srinivasan-etal-2019-multitask</bibkey>
    </paper>
    <paper id="26">
      <title>Generic and Specialized Word Embeddings for Multi-Domain Machine Translation</title>
      <author><first>MinhQuang</first><last>Pham</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <abstract>Supervised machine translation works well when the train and test data are sampled from the same distribution. When this is not the case, adaptation techniques help ensure that the knowledge learned from out-of-domain texts generalises to in-domain sentences. We study here a related setting, multi-domain adaptation, where the number of domains is potentially large and adapting separately to each domain would waste training resources. Our proposal transposes to neural machine translation the feature expansion technique of (Daumé III, 2007): it isolates domain-agnostic from domain-specific lexical representations, while sharing the most of the network across domains. Our experiments use two architectures and two language pairs: they show that our approach, while simple and computationally inexpensive, outperforms several strong baselines and delivers a multi-domain system that successfully translates texts from diverse sources.</abstract>
      <url hash="c5a76260">2019.iwslt-1.26</url>
      <bibkey>pham-etal-2019-generic</bibkey>
    </paper>
    <paper id="27">
      <title>Lexical Micro-adaptation for Neural Machine Translation</title>
      <author><first>Jitao</first><last>Xu</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <abstract>This work is inspired by a typical machine translation industry scenario in which translators make use of in-domain data for facilitating translation of similar or repeating sentences. We introduce a generic framework applied at inference in which a subset of segment pairs are first extracted from training data according to their similarity to the input sentences. These segments are then used to dynamically update the parameters of a generic NMT network, thus performing a lexical micro-adaptation. Our approach demonstrates strong adaptation performance to new and existing datasets including pseudo in-domain data. We evaluate our approach on a heterogeneous English-French training dataset showing accuracy gains on all evaluated domains when compared to strong adaptation baselines.</abstract>
      <url hash="74fc87c1">2019.iwslt-1.27</url>
      <bibkey>xu-etal-2019-lexical</bibkey>
    </paper>
    <paper id="28">
      <title>Efficient Bilingual Generalization from Neural Transduction Grammar Induction</title>
      <author><first>Yuchen</first><last>Yan</last></author>
      <author><first>Dekai</first><last>Wu</last></author>
      <author><first>Serkan</first><last>Kumyol</last></author>
      <abstract>We introduce (1) a novel neural network structure for bilingual modeling of sentence pairs that allows efficient capturing of bilingual relationship via biconstituent composition, (2) the concept of neural network biparsing, which applies to not only machine translation (MT) but also to a variety of other bilingual research areas, and (3) the concept of a biparsing-backpropagation training loop, which we hypothesize that can efficiently learn complex biparse tree patterns. Our work distinguishes from sequential attention-based models, which are more traditionally found in neural machine translation (NMT) in three aspects. First, our model enforces compositional constraints. Second, our model has a smaller search space in terms of discovering bilingual relationships from bilingual sentence pairs. Third, our model produces explicit biparse trees, which enable transparent error analysis during evaluation and external tree constraints during training.</abstract>
      <url hash="2cfa24d9">2019.iwslt-1.28</url>
      <bibkey>yan-etal-2019-efficient</bibkey>
    </paper>
    <paper id="29">
      <title>Breaking the Data Barrier: Towards Robust Speech Translation via Adversarial Stability Training</title>
      <author><first>Qiao</first><last>Cheng</last></author>
      <author><first>Meiyuan</first><last>Fan</last></author>
      <author><first>Yaqian</first><last>Han</last></author>
      <author><first>Jin</first><last>Huang</last></author>
      <author><first>Yitao</first><last>Duan</last></author>
      <abstract>In a pipeline speech translation system, automatic speech recognition (ASR) system will transmit errors in recognition to the downstream machine translation (MT) system. A standard machine translation system is usually trained on parallel corpus composed of clean text and will perform poorly on text with recognition noise, a gap well known in speech translation community. In this paper, we propose a training architecture which aims at making a neural machine translation model more robust against speech recognition errors. Our approach addresses the encoder and the decoder simultaneously using adversarial learning and data augmentation, respectively. Experimental results on IWSLT2018 speech translation task show that our approach can bridge the gap between the ASR output and the MT input, outperforms the baseline by up to 2.83 BLEU on noisy ASR output, while maintaining close performance on clean text.</abstract>
      <url hash="c688bc26">2019.iwslt-1.29</url>
      <bibkey>cheng-etal-2019-breaking</bibkey>
    </paper>
    <paper id="30">
      <title>Controlling Utterance Length in <fixed-case>NMT</fixed-case>-based Word Segmentation with Attention</title>
      <author><first>Pierre</first><last>Godard</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <abstract>One of the basic tasks of computational language documentation (CLD) is to identify word boundaries in an unsegmented phonemic stream. While several unsupervised monolingual word segmentation algorithms exist in the literature, they are challenged in real-world CLD settings by the small amount of available data. A possible remedy is to take advantage of glosses or translation in a foreign, well- resourced, language, which often exist for such data. In this paper, we explore and compare ways to exploit neural machine translation models to perform unsupervised boundary detection with bilingual information, notably introducing a new loss function for jointly learning alignment and segmentation. We experiment with an actual under-resourced language, Mboshi, and show that these techniques can effectively control the output segmentation length.</abstract>
      <url hash="3042bb6d">2019.iwslt-1.30</url>
      <bibkey>godard-etal-2019-controlling</bibkey>
    </paper>
    <paper id="31">
      <title>Controlling the Output Length of Neural Machine Translation</title>
      <author><first>Surafel Melaku</first><last>Lakew</last></author>
      <author><first>Mattia</first><last>Di Gangi</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <abstract>The recent advances introduced by neural machine translation (NMT) are rapidly expanding the application fields of machine translation, as well as reshaping the quality level to be targeted. In particular, if translations have to fit some given layout, quality should not only be measured in terms of adequacy and fluency, but also length. Exemplary cases are the translation of document files, subtitles, and scripts for dubbing, where the output length should ideally be as close as possible to the length of the input text. This pa-per addresses for the first time, to the best of our knowledge, the problem of controlling the output length in NMT. We investigate two methods for biasing the output length with a transformer architecture: i) conditioning the output to a given target-source length-ratio class and ii) enriching the transformer positional embedding with length information. Our experiments show that both methods can induce the network to generate shorter translations, as well as acquiring inter- pretable linguistic skills.</abstract>
      <url hash="e032ec61">2019.iwslt-1.31</url>
      <bibkey>lakew-etal-2019-controlling</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="32">
      <title>Robust Neural Machine Translation for Clean and Noisy Speech Transcripts</title>
      <author><first>Matti</first><last>Di Gangi</last></author>
      <author><first>Robert</first><last>Enyedi</last></author>
      <author><first>Alessandra</first><last>Brusadin</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <abstract>Neural machine translation models have shown to achieve high quality when trained and fed with well structured and punctuated input texts. Unfortunately, the latter condition is not met in spoken language translation, where the input is generated by an automatic speech recognition (ASR) system. In this paper, we study how to adapt a strong NMT system to make it robust to typical ASR errors. As in our application scenarios transcripts might be post-edited by human experts, we propose adaptation strategies to train a single system that can translate either clean or noisy input with no supervision on the input type. Our experimental results on a public speech translation data set show that adapting a model on a significant amount of parallel data including ASR transcripts is beneficial with test data of the same type, but produces a small degradation when translating clean text. Adapting on both clean and noisy variants of the same data leads to the best results on both input types.</abstract>
      <url hash="0a3a3300">2019.iwslt-1.32</url>
      <bibkey>di-gangi-etal-2019-robust</bibkey>
    </paper>
    <paper id="33">
      <title>Multi-Task Modeling of Phonographic Languages: Translating Middle <fixed-case>E</fixed-case>gyptian Hieroglyphs</title>
      <author><first>Philipp</first><last>Wiesenbach</last></author>
      <author><first>Stefan</first><last>Riezler</last></author>
      <abstract>Machine translation of ancient languages faces a low-resource problem, caused by the limited amount of available textual source data and their translations. We present a multi-task modeling approach to translating Middle Egyptian that is inspired by recent successful approaches to multi-task learning in end-to-end speech translation. We leverage the phonographic aspect of the hieroglyphic writing system, and show that similar to multi-task learning of speech recognition and translation, joint learning and sharing of structural information between hieroglyph transcriptions, translations, and POS tagging can improve direct translation of hieroglyphs by several BLEU points, using a minimal amount of manual transcriptions.</abstract>
      <url hash="4868994c">2019.iwslt-1.33</url>
      <bibkey>wiesenbach-riezler-2019-multi</bibkey>
    </paper>
  </volume>
</collection>
