<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.textgraphs">
  <volume id="1" ingest-date="2024-07-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing</booktitle>
      <editor><first>Dmitry</first><last>Ustalov</last></editor>
      <editor><first>Yanjun</first><last>Gao</last></editor>
      <editor><first>Alexander</first><last>Panchenko</last></editor>
      <editor><first>Elena</first><last>Tutubalina</last></editor>
      <editor><first>Irina</first><last>Nikishina</last></editor>
      <editor><first>Arti</first><last>Ramesh</last></editor>
      <editor><first>Andrey</first><last>Sakhovskiy</last></editor>
      <editor><first>Ricardo</first><last>Usbeck</last></editor>
      <editor><first>Gerald</first><last>Penn</last></editor>
      <editor><first>Marco</first><last>Valentino</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="56917caa">2024.textgraphs-1</url>
      <venue>textgraphs</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="5d32aa1c">2024.textgraphs-1.0</url>
      <bibkey>textgraphs-2024-textgraphs</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Learning Human Action Representations from Temporal Context in Lifestyle Vlogs</title>
      <author><first>Oana</first><last>Ignat</last></author>
      <author><first>Santiago</first><last>Castro</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Weiji</first><last>Li</last><affiliation>Tesla</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>1-18</pages>
      <abstract>We address the task of human action representation and show how the approach to generating word representations based on co-occurrence can be adapted to generate human action representations by analyzing their co-occurrence in videos. To this end, we formalize the new task of human action co-occurrence identification in online videos, i.e., determine whether two human actions are likely to co-occur in the same interval of time.We create and make publicly available the Co-Act (Action Co-occurrence) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring.We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains.</abstract>
      <url hash="9d7120c1">2024.textgraphs-1.1</url>
      <bibkey>ignat-etal-2024-learning</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>C</fixed-case>on<fixed-case>G</fixed-case>ra<fixed-case>T</fixed-case>: Self-Supervised Contrastive Pretraining for Joint Graph and Text Embeddings</title>
      <author><first>William</first><last>Brannon</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Wonjune</first><last>Kang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Suyash</first><last>Fulay</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Hang</first><last>Jiang</last></author>
      <author><first>Brandon</first><last>Roy</last><affiliation>Massachusetts Institute of Technology and Brown University</affiliation></author>
      <author><first>Deb</first><last>Roy</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jad</first><last>Kabbara</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>19-39</pages>
      <abstract>Learning on text-attributed graphs (TAGs), in which nodes are associated with one or more texts, has been the subject of much recent work. However, most approaches tend to make strong assumptions about the downstream task of interest, are reliant on hand-labeled data, or fail to equally balance the importance of both text and graph representations. In this work, we propose Contrastive Graph-Text pretraining (ConGraT), a general, self-supervised approach for jointly learning separate representations of texts and nodes in a TAG. Our method trains a language model (LM) and a graph neural network (GNN) to align their representations in a common latent space using a batch-wise contrastive learning objective inspired by CLIP. We further propose an extension to the CLIP objective that leverages graph structure to incorporate information about inter-node similarity. Extensive experiments demonstrate that ConGraT outperforms baselines on various downstream tasks, including node and text category classification, link prediction, and language modeling. Finally, we present an application of our method to community detection in social graphs, which enables finding more textually grounded communities, rather than purely graph-based ones.</abstract>
      <url hash="41af15d3">2024.textgraphs-1.2</url>
      <bibkey>brannon-etal-2024-congrat</bibkey>
    </paper>
    <paper id="3">
      <title>Uniform Meaning Representation Parsing as a Pipelined Approach</title>
      <author><first>Jayeol</first><last>Chun</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Nianwen</first><last>Xue</last><affiliation>Brandeis University</affiliation></author>
      <pages>40-52</pages>
      <abstract>Uniform Meaning Representation (UMR) is the next phase of semantic formalism following Abstract Meaning Representation (AMR), with added focus on inter-sentential relations allowing the representational scope of UMR to cover a full document. This, in turn, greatly increases the complexity of its parsing task with the additional requirement of capturing document-level linguistic phenomena such as coreference, modal and temporal dependencies. In order to establish a strong baseline despite the small size of recently released UMR v1.0 corpus, we introduce a pipeline model that does not require any training. At the core of our method is a two-track strategy of obtaining UMRâ€™s sentence and document graphs separately, with the document-level triples being compiled at the token level and the sentence graph being converted from AMR graphs. By leveraging alignment between AMR and its sentence, we are able to generate the first automatic English UMR parses.</abstract>
      <url hash="bdc62095">2024.textgraphs-1.3</url>
      <bibkey>chun-xue-2024-pipeline</bibkey>
    </paper>
    <paper id="4">
      <title>Financial Product Ontology Population with Large Language Models</title>
      <author><first>Chanatip</first><last>Saetia</last><affiliation>Kasikorn Business Technology Group</affiliation></author>
      <author><first>Jiratha</first><last>Phruetthiset</last></author>
      <author><first>Tawunrat</first><last>Chalothorn</last><affiliation>KASIKORN Business-Technology Group</affiliation></author>
      <author><first>Monchai</first><last>Lertsutthiwong</last></author>
      <author><first>Supawat</first><last>Taerungruang</last><affiliation>Chiang Mai University</affiliation></author>
      <author><first>Pakpoom</first><last>Buabthong</last><affiliation>Nakhon Ratchasima Rajabhat University</affiliation></author>
      <pages>53-60</pages>
      <abstract>Ontology population, which aims to extract structured data to enrich domain-specific ontologies from unstructured text, typically faces challenges in terms of data scarcity and linguistic complexity, particularly in specialized fields such as retail banking. In this study, we investigate the application of large language models (LLMs) to populate domain-specific ontologies of retail banking products from Thai corporate documents. We compare traditional span-based approaches to LLMs-based generative methods, with different prompting techniques. Our findings reveal that while span-based methods struggle with data scarcity and the complex linguistic structure, LLMs-based generative approaches substantially outperform, achieving a 61.05% F1 score, with the most improvement coming from providing examples in the prompts. This improvement highlights the potential of LLMs for ontology population tasks, offering a scalable and efficient solution for structured information extraction in especially in low-resource language settings.</abstract>
      <url hash="37fdee69">2024.textgraphs-1.4</url>
      <bibkey>saetia-etal-2024-financial</bibkey>
    </paper>
    <paper id="5">
      <title>Prompt Me One More Time: A Two-Step Knowledge Extraction Pipeline with Ontology-Based Verification</title>
      <author><first>Alla</first><last>Chepurova</last></author>
      <author><first>Yuri</first><last>Kuratov</last><affiliation>AIRI, Artificial Intelligence Research Institute and Moscow Institute of Physics and Technology</affiliation></author>
      <author><first>Aydar</first><last>Bulatov</last><affiliation>Moscow Institute of Physics and Technology</affiliation></author>
      <author><first>Mikhail</first><last>Burtsev</last><affiliation>London Institute for Mathematical Sciences</affiliation></author>
      <pages>61-77</pages>
      <abstract>This study explores a method for extending real-world knowledge graphs (specifically, Wikidata) by extracting triplets from texts with the aid of Large Language Models (LLMs). We propose a two-step pipeline that includes the initial extraction of entity candidates, followed by their refinement and linkage to the canonical entities and relations of the knowledge graph. Finally, we utilize Wikidata relation constraints to select only verified triplets. We compare our approach to a model that was fine-tuned on a machine-generated dataset and demonstrate that it performs better on natural data. Our results suggest that LLM-based triplet extraction from texts, with subsequent verification, is a viable method for real-world applications.</abstract>
      <url hash="563329d8">2024.textgraphs-1.5</url>
      <bibkey>chepurova-etal-2024-prompt</bibkey>
    </paper>
    <paper id="6">
      <title>Towards Understanding Attention-based Reasoning through Graph Structures in Medical Codes Classification</title>
      <author><first>Noon</first><last>Goldstein</last></author>
      <author><first>Saadullah</first><last>Amin</last></author>
      <author><first>GÃ¼nter</first><last>Neumann</last><affiliation>German Research Center for AI</affiliation></author>
      <pages>78-92</pages>
      <abstract>A common approach to automatically assigning diagnostic and procedural clinical codes to health records is to solve the task as a multi-label classification problem. Difficulties associated with this task stem from domain knowledge requirements, long document texts, large and imbalanced label space, reflecting the breadth and dependencies between medical diagnoses and procedures. Decisions in the healthcare domain also need to demonstrate sound reasoning, both when they are correct and when they are erroneous. Existing works address some of these challenges by incorporating external knowledge, which can be encoded into a graph-structured format. Incorporating graph structures on the output label space or between the input document and output label spaces have shown promising results in medical codes classification. Limited focus has been put on utilizing graph-based representation on the input document space. To partially bridge this gap, we represent clinical texts as graph-structured data through the UMLS Metathesaurus; we explore implicit graph representation through pre-trained knowledge graph embeddings and explicit domain-knowledge guided encoding of document concepts and relational information through graph neural networks. Our findings highlight the benefits of pre-trained knowledge graph embeddings in understanding modelâ€™s attention-based reasoning. In contrast, transparent domain knowledge guidance in graph encoder approaches is overshadowed by performance loss. Our qualitative analysis identifies limitations that contribute to prediction errors.</abstract>
      <url hash="acab3c4b">2024.textgraphs-1.6</url>
      <bibkey>goldstein-etal-2024-towards</bibkey>
    </paper>
    <paper id="7">
      <title>Leveraging Graph Structures to Detect Hallucinations in Large Language Models</title>
      <author><first>Noa</first><last>Nonkes</last></author>
      <author><first>Sergei</first><last>Agaronian</last></author>
      <author><first>Evangelos</first><last>Kanoulas</last><affiliation>University of Amsterdam and University of Amsterdam</affiliation></author>
      <author><first>Roxana</first><last>Petcu</last><affiliation>University of Amsterdam and University of Amsterdam</affiliation></author>
      <pages>93-104</pages>
      <abstract>Large language models are extensively applied across a wide range of tasks, such as customer support, content creation, educational tutoring, and providing financial guidance. However, a well-known drawback is their predisposition to generate hallucinations. This damages the trustworthiness of the information these models provide, impacting decision-making and user confidence. We propose a method to detect hallucinations by looking at the structure of the latent space and finding associations within hallucinated and non-hallucinated generations. We create a graph structure that connects generations that lie closely in the embedding space. Moreover, we employ a Graph Attention Network which utilizes message passing to aggregate information from neighboring nodes and assigns varying degrees of importance to each neighbor based on their relevance. Our findings show that 1) there exists a structure in the latent space that differentiates between hallucinated and non-hallucinated generations, 2) Graph Attention Networks can learn this structure and generalize it to unseen generations, and 3) the robustness of our method is enhanced when incorporating contrastive learning. When evaluated against evidence-based benchmarks, our model performs similarly without access to search-based methods.</abstract>
      <url hash="f436cee9">2024.textgraphs-1.7</url>
      <bibkey>nonkes-etal-2024-leveraging</bibkey>
    </paper>
    <paper id="8">
      <title>Semantic Graphs for Syntactic Simplification: A Revisit from the Age of <fixed-case>LLM</fixed-case></title>
      <author><first>Peiran</first><last>Yao</last></author>
      <author><first>Kostyantyn</first><last>Guzhva</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Denilson</first><last>Barbosa</last><affiliation>University of Alberta</affiliation></author>
      <pages>105-115</pages>
      <abstract>Symbolic sentence meaning representations, such as AMR (Abstract Meaning Representation) provide expressive and structured semantic graphs that act as intermediates that simplify downstream NLP tasks. However, the instruction-following capability of large language models (LLMs) offers a shortcut to effectively solve NLP tasks, questioning the utility of semantic graphs. Meanwhile, recent work has also shown the difficulty of using meaning representations merely as a helpful auxiliary for LLMs. We revisit the position of semantic graphs in syntactic simplification, the task of simplifying sentence structures while preserving their meaning, which requires semantic understanding, and evaluate it on a new complex and natural dataset. The AMR-based method that we propose, AMRS<tex-math>^3</tex-math>, demonstrates that state-of-the-art meaning representations can lead to easy-to-implement simplification methods with competitive performance and unique advantages in cost, interpretability, and generalization. With AMRS<tex-math>^3</tex-math> as an anchor, we discover that syntactic simplification is a task where semantic graphs are helpful in LLM prompting. We propose AMRCoC prompting that guides LLMs to emulate graph algorithms for explicit symbolic reasoning on AMR graphs, and show its potential for improving LLM on semantic-centered tasks like syntactic simplification.</abstract>
      <url hash="69a1217a">2024.textgraphs-1.8</url>
      <bibkey>yao-etal-2024-semantic</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs 2024 Shared Task on Text-Graph Representations for Knowledge Graph Question Answering</title>
      <author><first>Andrey</first><last>Sakhovskiy</last><affiliation>Kazan Federal University</affiliation></author>
      <author><first>Mikhail</first><last>Salnikov</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Irina</first><last>Nikishina</last></author>
      <author><first>Aida</first><last>Usmanova</last><affiliation>Leuphana UniversitÃ¤t LÃ¼neburg</affiliation></author>
      <author><first>Angelie</first><last>Kraft</last><affiliation>UniversitÃ¤t Hamburg</affiliation></author>
      <author><first>Cedric</first><last>MÃ¶ller</last><affiliation>UniversitÃ¤t Hamburg</affiliation></author>
      <author><first>Debayan</first><last>Banerjee</last><affiliation>UniversitÃ¤t Hamburg</affiliation></author>
      <author><first>Junbo</first><last>Huang</last><affiliation>UniversitÃ¤t Hamburg</affiliation></author>
      <author><first>Longquan</first><last>Jiang</last><affiliation>UniversitÃ¤t Hamburg</affiliation></author>
      <author><first>Rana</first><last>Abdullah</last><affiliation>UniversitÃ¤t Hamburg</affiliation></author>
      <author><first>Xi</first><last>Yan</last><affiliation>UniversitÃ¤t Hamburg</affiliation></author>
      <author><first>Dmitry</first><last>Ustalov</last><affiliation>JetBrains</affiliation></author>
      <author><first>Elena</first><last>Tutubalina</last><affiliation>Kazan Federal University</affiliation></author>
      <author><first>Ricardo</first><last>Usbeck</last><affiliation>Leuphana UniversitÃ¤t LÃ¼neburg</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <pages>116-125</pages>
      <abstract>This paper describes the results of the Knowledge Graph Question Answering (KGQA) shared task that was co-located with the TextGraphs 2024 workshop. In this task, given a textual question and a list of entities with the corresponding KG subgraphs, the participating system should choose the entity that correctly answers the question. Our competition attracted thirty teams, four of which outperformed our strong ChatGPT-based zero-shot baseline. In this paper, we overview the participating systems and analyze their performance according to a large-scale automatic evaluation. To the best of our knowledge, this is the first competition aimed at the KGQA problem using the interaction between large language models (LLMs) and knowledge graphs.</abstract>
      <url hash="ac49cbc6">2024.textgraphs-1.9</url>
      <bibkey>sakhovskiy-etal-2024-textgraphs</bibkey>
    </paper>
    <paper id="10">
      <title>nlp_enjoyers at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs-17 Shared Task: Text-Graph Representations for Knowledge Graph Question Answering using all-<fixed-case>MPN</fixed-case>et</title>
      <author><first>Nikita</first><last>Kurdiukov</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Viktoriia</first><last>Zinkovich</last></author>
      <author><first>Sergey</first><last>Karpukhin</last></author>
      <author><first>Pavel</first><last>Tikhomirov</last></author>
      <pages>126-130</pages>
      <abstract>This paper presents a model for solving the Multiple Choice Question Answering (MCQA) problem, focusing on the impact of subgraph extraction from a Knowledge Graph on model performance. The proposed method combines textual and graph information by adding linearized subgraphs directly into the main question prompt with separate tokens, enhancing the performance of models working with each modality separately. The study also includes an examination of Large Language Model (LLM) backbones and the benefits of linearized subgraphs and sequence length, with efficient training achieved through fine-tuning with LoRA. The top benchmark, using subgraphs and MPNet, achieved an F1 score of 0.3887. The main limitation of the experiments is the reliance on pre-generated subgraphs/triplets from the graph, and the lack of exploration of in-context learning and prompting strategies with decoder-based architectures.</abstract>
      <url hash="747647d8">2024.textgraphs-1.10</url>
      <bibkey>kurdiukov-etal-2024-nlp</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs-17 Shared Task: Enhancing Inference Capabilities of <fixed-case>LLM</fixed-case>s with Knowledge Graphs</title>
      <author><first>Wei</first><last>Tang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xiaosong</first><last>Qiao</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xiaofeng</first><last>Zhao</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Chang</first><last>Su</last></author>
      <author><first>Yuang</first><last>Li</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yinglu</first><last>Li</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yilun</first><last>Liu</last></author>
      <author><first>Feiyu</first><last>Yao</last></author>
      <author><first>Shimin</first><last>Tao</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>He</first><last>Xianghui</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>131-136</pages>
      <abstract>In this paper, we present an effective method for TextGraphs-17 Shared Task. This task requires selecting an entity from the candidate entities that is relevant to the given question and answer. The selection process is aided by utilizing the shortest path graph in the knowledge graph, connecting entities in the query to the candidate entity. This task aims to explore how to enhance LLMs output with KGs, although current LLMs have certain logical reasoning capabilities, they may not be certain about their own outputs, and the answers they produce may be correct by chance through incorrect paths. In this case, we have introduced a LLM prompt design strategy based on self-ranking and emotion. Specifically, we let the large model score its own answer choices to reflect its confidence in the answer. Additionally, we add emotional incentives to the prompts to encourage the model to carefully examine the questions. Our submissions was conducted under zero-resource setting, and we achieved the second place in the task with an F1-score of 0.8321.</abstract>
      <url hash="9e567241">2024.textgraphs-1.11</url>
      <bibkey>tang-etal-2024-hw</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>TIGFORMER</fixed-case> at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs-17 Shared Task: A Late Interaction Method for text and Graph Representations in <fixed-case>KBQA</fixed-case> Classification Task</title>
      <author><first>Mayank</first><last>Rakesh</last><affiliation>Statusneo Technology Consulting</affiliation></author>
      <author><first>Parikshit</first><last>Saikia</last><affiliation>GreyB</affiliation></author>
      <author><first>Saket</first><last>Shrivastava</last></author>
      <pages>137-141</pages>
      <abstract>This paper introduces a novel late interaction mechanism for knowledge base question answering (KBQA) systems, combining Graphormer and transformer representations. We conducted extensive experiments, comparing various pooling mechanisms and configurations. Our results demonstrate significant improvements in F1-score compared to traditional baselines. Specifically, we found that attention pooling, in conjunction with linearized graph and question features alongside sub-graph representations, yields the best performance. Our study highlights the importance of advanced interaction mechanisms and the integration of diverse modalities in KBQA systems.</abstract>
      <url hash="da1e5613">2024.textgraphs-1.12</url>
      <bibkey>rakesh-etal-2024-tigformer</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>NLP</fixed-case>eople at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs-17 Shared Task: Chain of Thought Questioning to Elicit Decompositional Reasoning</title>
      <author><first>Movina</first><last>Moses</last></author>
      <author><first>Vishnudev</first><last>Kuruvanthodi</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Mohab</first><last>Elkaref</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Shinnosuke</first><last>Tanaka</last><affiliation>International Business Machines</affiliation></author>
      <author><first>James</first><last>Barry</last></author>
      <author><first>Geeth</first><last>Mel</last></author>
      <author><first>Campbell</first><last>Watson</last><affiliation>International Business Machines</affiliation></author>
      <pages>142-148</pages>
      <abstract>This paper presents the approach of the NLPeople team for the Text-Graph Representations for KGQA Shared Task at TextGraphs-17. The task involved selecting an answer for a given question from a list of candidate entities. We show that prompting Large Language models (LLMs) to break down a natural language question into a series of sub-questions, allows models to understand complex questions. The LLMs arrive at the final answer by answering the intermediate questions using their internal knowledge and without needing additional context. Our approach to the task uses an ensemble of prompting strategies to guide how LLMs interpret various types of questions. Our submission achieves an F1 score of 85.90, ranking 1st among the other participants in the task.</abstract>
      <url hash="22f15f58">2024.textgraphs-1.13</url>
      <bibkey>moses-etal-2024-nlpeople</bibkey>
    </paper>
    <paper id="14">
      <title>Skoltech at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs-17 Shared Task: Finding <fixed-case>GPT</fixed-case>-4 Prompting Strategies for Multiple Choice Questions</title>
      <author><first>Maria</first><last>Lysyuk</last></author>
      <author><first>Pavel</first><last>Braslavski</last><affiliation>Nazarbayev University</affiliation></author>
      <pages>149-153</pages>
      <abstract>In this paper, we present our solution to the TextGraphs-17 Shared Task on Text-Graph Representations for KGQA. GPT-4 alone, with chain-of-thought reasoning and a given set of answers, achieves an F1 score of 0.78. By employing subgraph size as a feature, Wikidata answer description as an additional context, and question rephrasing technique, we further strengthen this result. These tricks help to answer questions that were not initially answered and to eliminate irrelevant, identical answers. We have managed to achieve an F1 score of 0.83 and took 2nd place, improving the score by 0.05 over the baseline. An open implementation of our method is available on GitHub.</abstract>
      <url hash="66199c15">2024.textgraphs-1.14</url>
      <bibkey>lysyuk-braslavski-2024-skoltech</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>J</fixed-case>elly<fixed-case>B</fixed-case>ell at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs-17 Shared Task: Fusing Large Language Models with External Knowledge for Enhanced Question Answering</title>
      <author><first>Julia</first><last>Belikova</last></author>
      <author><first>Evegeniy</first><last>Beliakin</last></author>
      <author><first>Vasily</first><last>Konovalov</last></author>
      <pages>154-160</pages>
      <abstract>This work describes an approach to develop Knowledge Graph Question Answering (KGQA) system for TextGraphs-17 shared task. The task focuses on the fusion of Large Language Models (LLMs) with Knowledge Graphs (KGs). The goal is to select a KG entity (out of several candidates) which corresponds to an answer given a textual question. Our approach applies LLM to identify the correct answer among the list of possible candidates. We confirm that integrating external information is particularly beneficial when the subject entities are not well-known, and using RAG can negatively impact the performance of LLM on questions related to popular entities, as the retrieved context might be misleading. With our result, we achieved 2nd place in the post-evaluation phase.</abstract>
      <url hash="598a3e88">2024.textgraphs-1.15</url>
      <bibkey>belikova-etal-2024-jellybell</bibkey>
    </paper>
  </volume>
</collection>
