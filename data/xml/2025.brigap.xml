<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.brigap">
  <volume id="1" ingest-date="2025-09-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second Workshop on the Bridges and Gaps between Formal and Computational Linguistics (BriGap-2)</booktitle>
      <editor><first>Timothée</first><last>Bernard</last></editor>
      <editor><first>Timothee</first><last>Mickus</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Düsseldorf, Germany</address>
      <month>September</month>
      <year>2025</year>
      <url hash="66f951e8">2025.brigap-1</url>
      <venue>brigap</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-317-3</isbn>
    </meta>
    <frontmatter>
      <url hash="a455d196">2025.brigap-1.0</url>
      <bibkey>brigap-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Natural Language Inference with <fixed-case>CCG</fixed-case> Parser and Automated Theorem Prover for <fixed-case>DTS</fixed-case></title>
      <author><first>Asa</first><last>Tomita</last></author>
      <author><first>Mai</first><last>Matsubara</last><affiliation>Ochanomizu University</affiliation></author>
      <author><first>Hinari</first><last>Daido</last><affiliation>Ochanomizu University</affiliation></author>
      <author orcid="0000-0002-9988-1260"><first>Daisuke</first><last>Bekki</last><affiliation>Ochanomizu University</affiliation></author>
      <pages>1-7</pages>
      <abstract>We propose a Natural Language Inference (NLI) system based on compositional semantics. The system combines lightblue, a syntactic and semantic parser grounded in Combinatory Categorial Grammar (CCG) and Dependent Type Semantics (DTS), with wani, an automated theorem prover for Dependent Type Theory (DTT). Because each computational step reflects a theoretical assumption, system evaluation serves as a form of hypothesis verification. We evaluate the inference system using the Japanese Semantic Test Suite JSeM, and demonstrate how error analysis provides feedback to improve both the system and the underlying linguistic theory.</abstract>
      <url hash="6737ee17">2025.brigap-1.1</url>
      <bibkey>tomita-etal-2025-natural</bibkey>
    </paper>
    <paper id="2">
      <title>Evaluating The Impact of Stimulus Quality in Investigations of <fixed-case>LLM</fixed-case> Language Performance</title>
      <author><first>Timothy</first><last>Pistotti</last></author>
      <author><first>Jason</first><last>Brown</last></author>
      <author orcid="0000-0002-7554-0971"><first>Michael J.</first><last>Witbrock</last><affiliation>University of Auckland</affiliation></author>
      <pages>8-14</pages>
      <abstract>Recent studies employing Large Language Models (LLMs) to test the Argument from the Poverty of the Stimulus (APS) have yielded contrasting results across syntactic phenomena. This paper investigates the hypothesis that characteristics of the stimuli used in recent studies, including lexical ambiguities and structural complexities, may confound model performance. A methodology is proposed for re-evaluating LLM competence on syntactic prediction, focusing on GPT-2. This involves: 1) establishing a baseline on previously used (both filtered and unfiltered) stimuli, and 2) generating a new, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5 Pro Preview) guided by linguistically-informed templates designed to mitigate identified confounds. Our preliminary findings indicate that GPT-2 demonstrates notably improved performance on these refined PG stimuli compared to baselines, suggesting that stimulus quality significantly influences outcomes in surprisal-based evaluations of LLM syntactic competency.</abstract>
      <url hash="9186b4fd">2025.brigap-1.2</url>
      <bibkey>pistotti-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="3">
      <title>Modal Subordination in Dependent Type Semantics</title>
      <author><first>Aoi</first><last>Iimura</last></author>
      <author><first>Teruyuki</first><last>Mizuno</last><affiliation>Ochanomizu University</affiliation></author>
      <author orcid="0000-0002-9988-1260"><first>Daisuke</first><last>Bekki</last><affiliation>Ochanomizu University</affiliation></author>
      <pages>15-19</pages>
      <abstract>In the field of natural language processing, the construction of “linguistic pipelines”, which draw on insights from theoretical linguistics, stands in a complementary relationship to the prevailing paradigm of large language models. The rapid development of these pipelines has been fueled by recent advancements, including the emergence of Dependent Type Semantics (DTS) — a type-theoretic framework for natural language semantics. While DTS has been successfully applied to analyze complex linguistic phenomena such as anaphora and presupposition, its capability to account for modal expressions remains an underexplored area. This study aims to address this gap by proposing a framework that extends DTS with modal types. This extension broadens the scope of linguistic phenomena that DTS can account for, including an analysis of modal subordination, where anaphora interacts with modal expressions.</abstract>
      <url hash="28e57443">2025.brigap-1.3</url>
      <bibkey>iimura-etal-2025-modal</bibkey>
    </paper>
    <paper id="4">
      <title>Exploring Gaps in the <fixed-case>APS</fixed-case>: Direct Minimal Pair Analysis in <fixed-case>LLM</fixed-case> Syntactic Assessments</title>
      <author><first>Timothy</first><last>Pistotti</last></author>
      <author><first>Jason</first><last>Brown</last></author>
      <author orcid="0000-0002-7554-0971"><first>Michael J.</first><last>Witbrock</last><affiliation>University of Auckland</affiliation></author>
      <pages>20-25</pages>
      <abstract>Recent studies probing the Argument from the Poverty of the Stimulus (APS) have applied Large Language Models (LLMs) to test the learnability of complex syntax through surprisal-based metrics. However, divergent conclusions raise questions concerning the insights these metrics offer. While Wilcox et al. (2024) used direct minimal pair comparisons (the “wh-effect”) to demonstrate that models successfully generalise knowledge of filler-gap dependencies, Lan et al. (2024) used a Difference-in-Differences (DiD) metric and found that models largely fail on parasitic gaps (PGs). This paper argues that the direct minimal pair approach offers greater diagnostic transparency. We demonstrate this by generating a full 8-permutation paradigm of refined PG stimuli and evaluating the GPT-2 model used in previous studies with a systematic Wilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across all four tested conditions, indicating robust knowledge of filler-gap licensing principles even in complex PG environments. This finding, which contrasts with the more ambiguous results from DiD-style metrics, suggests that the choice of evaluation metric is critical for assessing an LLM’s syntactic competence.</abstract>
      <url hash="59806411">2025.brigap-1.4</url>
      <bibkey>pistotti-etal-2025-exploring</bibkey>
    </paper>
    <paper id="5">
      <title>Coordination of Theoretical and Computational Linguistics</title>
      <author orcid="0000-0002-4398-2636"><first>Adam</first><last>Przepiórkowski</last><affiliation>University of Warsaw and Polish Academy of Sciences</affiliation></author>
      <author orcid="0000-0002-2367-9170"><first>Agnieszka</first><last>Patejuk</last><affiliation>Institute of Computer Science, Polish Academy of Sciences</affiliation></author>
      <pages>26-34</pages>
      <abstract>The aim of this paper is to present a case study of a fruitful and, hopefully, inspiring interaction between formal and computational linguistics. A variety of NLP tools and resources have been used in linguistic investigations of the symmetry of coordination, leading to novel theoretical arguments. The converse impact of theoretical results on NLP work has been successful only in some cases.</abstract>
      <url hash="6286e2a1">2025.brigap-1.5</url>
      <bibkey>przepiorkowski-patejuk-2025-coordination</bibkey>
    </paper>
    <paper id="6">
      <title>An instructive implementation of semantic parsing and reasoning using <fixed-case>L</fixed-case>exical <fixed-case>F</fixed-case>unctional <fixed-case>G</fixed-case>rammar</title>
      <author><first>Mark-Matthias</first><last>Zymla</last></author>
      <author><first>Kascha</first><last>Kruschwitz</last><affiliation>Universität Konstanz</affiliation></author>
      <author orcid="0009-0005-4815-1611"><first>Paul</first><last>Zodl</last></author>
      <pages>35-51</pages>
      <abstract>This paper presents a computational resource for exploring semantic parsing and reasoning through a strictly formal lense. Inspired by the framework of Lexical Functional Grammar, our system allows for modular exploration of different aspects of semantic parsing. It consists of a hand-coded formal grammar combining syntactic and semantic annotations, producing basic semantic representations. The system provides the option to extend these basic semantics via rewrite rules in a principled fashion to explore more complex reasoning. The result is a layered system enabling an incremental approach to semantic parsing. We illustrate this approach with examples from the Fracas testsuite demonstrating its overall functionality and viability.</abstract>
      <url hash="c7304b76">2025.brigap-1.6</url>
      <bibkey>zymla-etal-2025-instructive</bibkey>
    </paper>
    <paper id="7">
      <title>Modelling Expectation-based and Memory-based Predictors of Human Reading Times with Syntax-guided Attention</title>
      <author><first>Lukas</first><last>Mielczarek</last><affiliation>Heinrich-Heine Universität Düsseldorf and Heinrich-Heine Universität Düsseldorf</affiliation></author>
      <author orcid="0000-0003-4172-6986"><first>Timothée</first><last>Bernard</last><affiliation>Université Paris Cité</affiliation></author>
      <author orcid="0000-0001-9691-5990"><first>Laura</first><last>Kallmeyer</last><affiliation>Heinrich Heine University Düsseldorf, Germany</affiliation></author>
      <author orcid="0000-0001-7641-7310"><first>Katharina</first><last>Spalek</last></author>
      <author><first>Benoit</first><last>Crabbé</last><affiliation>Université de Paris</affiliation></author>
      <pages>52-71</pages>
      <abstract>The correlation between reading times and surprisal is well known in psycholinguistics and is easy to observe. There is also a correlation between reading times and structural integration, which is, however, harder to detect (Gibson, 2000). This correlation has been studied using parsing models whose outputs are linked to reading times. In this paper, we study the relevance of memory-based effects in reading times and how to predict them using neural language models. We find that integration costs significantly improve surprisal-based reading time prediction. Inspired by Timkey and Linzen (2023), we design a small-scale autoregressive transformer language model in which attention heads are supervised by dependency relations. We compare this model to a standard variant by checking how well each model’s outputs correlate with human reading times and find that predicted attention scores can be effectively used as proxies for syntactic integration costs to predict self-paced reading times.</abstract>
      <url hash="c0916ac3">2025.brigap-1.7</url>
      <bibkey>mielczarek-etal-2025-modelling</bibkey>
    </paper>
    <paper id="8">
      <title>Syntax-Guided Parameter Efficient Fine-Tuning of Large Language Models</title>
      <author><first/><last>Prasanth</last></author>
      <pages>72-78</pages>
      <abstract>Large language models (LLMs) demonstrate remarkable linguistic capabilities but lack explicit syntactic knowledge grounded in formal grammatical theory. This paper introduces a syntax-guided parameter-efficient fine-tuning approach that integrates formal syntactic constraints into transformer-based models using Low-Rank Adaptation (LoRA). We develop a hybrid training objective incorporating violations of syntactic well-formedness derived from dependency parsing and context-free grammar constraints. Our method is evaluated on established English syntactic benchmarks including BLiMP, CoLA, and SyntaxGym targeting specific grammatical phenomena. Results show modest but consistent improvements in syntactic competence: 1.6 percentage point average improvement on BLiMP overall, with gains of 1.7 percentage points on agreement phenomena and 1.6 percentage points on filler-gap dependencies, alongside 0.006 improvement in CoLA MCC scores, while maintaining stable performance on general natural language processing (NLP) tasks. The parameter-efficient approach reduces training time by 76% compared to full fine-tuning while achieving these incremental syntactic gains. This work demonstrates a practical pathway for incorporating linguistic theory into modern natural language processing (NLP) systems, though the improvements suggest that explicit syntactic supervision provides limited additional benefits over implicit learning from large-scale text.</abstract>
      <url hash="a0e67774">2025.brigap-1.8</url>
      <bibkey>prasanth-2025-syntax</bibkey>
    </paper>
    <paper id="9">
      <title>On the relative impact of categorical and semantic information on the induction of self-embedding structures</title>
      <author><first>Antoine</first><last>Venant</last><affiliation>Université de Montréal</affiliation></author>
      <author><first>Yutaka</first><last>Suzuki</last><affiliation>Université de Montréal</affiliation></author>
      <pages>79-96</pages>
      <abstract>We investigate the impact of center embedding and selectional restrictions on neural latent tree models’ tendency to induce self-embedding structures. To this aim we compare their behavior in different controlled artificial environments involving noun phrases modified by relative clauses, with different quantity of available training data. Our results provide evidence that the existence of multiple center self-embedding is a stronger incentive than selectional restrictions alone, but that the combination of both is the best incentive overall. We also show that different architectures benefit very differently from these incentives.</abstract>
      <url hash="34fa7438">2025.brigap-1.9</url>
      <bibkey>venant-suzuki-2025-relative</bibkey>
    </paper>
    <paper id="10">
      <title>Plural Interpretive Biases: A Comparison Between Human Language Processing and Language Models</title>
      <author><first>Jia</first><last>Ren</last></author>
      <pages>97-105</pages>
      <abstract>Human communication routinely relies on plural predication, and plural sentences are often ambiguous (see, e.g., Scha, 1984; Dalrymple et al., 1998a, to name a few). Building on extensive theoretical and experimental work in linguistics and philosophy, we ask whether large language models (LLMs) exhibit the same interpretive biases that humans show when resolving plural ambiguity. We focus on two lexical factors: (i) the collective bias of certain predicates (e.g., size/shape adjectives) and (ii) the symmetry bias of predicates. To probe these tendencies, we apply two complementary methods to premise–hypothesis pairs: an embedding-based heuristic using OpenAI’s text-embedding-3-large/small (OpenAI, 2024, 2025) with cosine similarity, and supervised NLI models (bart-large-mnli, roberta-large-mnli) (Lewis et al., 2020; Liu et al., 2019; Williams et al., 2018a; Facebook AI, 2024b,a) that yield asymmetric, calibrated entailment probabilities. Results show partial sensitivity to predicate-level distinctions, but neither method reproduces the robust human pattern, where neutral predicates favor entailment and strongly non-symmetric predicates disfavor it. These findings highlight both the potential and the limits of current LLMs: as cognitive models, they fall short of capturing human-like interpretive biases; as engineering systems, their representations of plural semantics remain unstable for tasks requiring precise entailment.</abstract>
      <url hash="983f9fc2">2025.brigap-1.10</url>
      <bibkey>ren-2025-plural</bibkey>
    </paper>
  </volume>
</collection>
