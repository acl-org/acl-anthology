<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.mwe">
  <volume id="1" ingest-date="2022-09-23">
    <meta>
      <booktitle>Proceedings of the 18th Workshop on Multiword Expressions @LREC2022</booktitle>
      <editor><first>Archna</first><last>Bhatia</last></editor>
      <editor><first>Paul</first><last>Cook</last></editor>
      <editor><first>Shiva</first><last>Taslimipoor</last></editor>
      <editor><first>Marcos</first><last>Garcia</last></editor>
      <editor><first>Carlos</first><last>Ramisch</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>June</month>
      <year>2022</year>
      <url hash="d96a1352">2022.mwe-1</url>
      <venue>mwe</venue>
    </meta>
    <frontmatter>
      <url hash="7f8b452a">2022.mwe-1.0</url>
      <bibkey>mwe-2022-multiword</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Figurative Language in Noun Compound Models across Target Properties, Domains and Time</title>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>1</pages>
      <abstract>A variety of distributional and multi-modal computational approaches has been suggested for modelling the degrees of compositionality across types of multiword expressions and languages. As the starting point of my talk, I will present standard variants of computational models that have been proven successful in predicting the compositionality of German and English noun compounds. The main part of the talk will then be concerned with investigating the general reliability of these standard models and discussing implications for gold-standard datasets: I will demonstrate how prediction results vary (i) across representations, (ii) across empirical target properties, (iii) across compound types, (iv) across levels of abstractness, and (v) for general- vs. domain-specific language. Finally, I will present a preliminary quantitative study on diachronic changes of noun compound meanings and compositionality over time.</abstract>
      <url hash="58c9ac36">2022.mwe-1.1</url>
      <bibkey>schulte-im-walde-2022-figurative</bibkey>
    </paper>
    <paper id="2">
      <title>Multiword Expressions and the Low-Resource Scenario from the Perspective of a Local Oral Culture</title>
      <author><first>Steven</first><last>Bird</last></author>
      <pages>2</pages>
      <abstract>Research on multiword expressions and on under-resourced languages often begins with problematisation. The existence of non-compositional meaning, or the paucity of conventional language resources, are treated as problems to be solved. This perspective is associated with the view of Language as a lexico-grammatical code, and of NLP as a conventional sequence of computational tasks. In this talk, I share from my experience in an Australian Aboriginal community, where people tend to see language as an expression of identity and of ‘connection to country’. Here, my early attempts to collect language data were thwarted. There was no obvious role for tasks like speech recognition, parsing, or translation. Instead, working under the authority of local elders, I pivoted to language processing tasks that were more in keeping with local interests and aspirations. I describe these tasks and suggest some new ways of framing the work of NLP, and I explore implications for work on multiword expressions and on under-resourced languages.</abstract>
      <url hash="99956519">2022.mwe-1.2</url>
      <bibkey>bird-2022-multiword</bibkey>
    </paper>
    <paper id="3">
      <title>A General Framework for Detecting Metaphorical Collocations</title>
      <author><first>Marija</first><last>Brkić Bakarić</last></author>
      <author><first>Lucia</first><last>Načinović Prskalo</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <pages>3–8</pages>
      <abstract>This paper aims at identifying a specific set of collocations known under the term metaphorical collocations. In this type of collocations, a semantic shift has taken place in one of the components. Since the appropriate gold standard needs to be compiled prior to any serious endeavour to extract metaphorical collocations automatically, this paper first presents the steps taken to compile it, and then establishes appropriate evaluation framework. The process of compiling the gold standard is illustrated on one of the most frequent Croatian nouns, which resulted in the preliminary relation significance set. With the aim to investigate the possibility of facilitating the process, frequency, logDice, relation, and pretrained word embeddings are used as features in the classification task conducted on the logDice-based word sketch relation lists. Preliminary results are presented.</abstract>
      <url hash="f854dc51">2022.mwe-1.3</url>
      <bibkey>brkic-bakaric-etal-2022-general</bibkey>
    </paper>
    <paper id="4">
      <title>Improving Grammatical Error Correction for Multiword Expressions</title>
      <author><first>Shiva</first><last>Taslimipoor</last></author>
      <author><first>Christopher</first><last>Bryant</last></author>
      <author><first>Zheng</first><last>Yuan</last></author>
      <pages>9–15</pages>
      <abstract>Grammatical error correction (GEC) is the task of automatically correcting errors in text. It has mainly been developed to assist language learning, but can also be applied to native text. This paper reports on preliminary work in improving GEC for multiword expression (MWE) error correction. We propose two systems which incorporate MWE information in two different ways: one is a multi-encoder decoder system which encodes MWE tags in a second encoder, and the other is a BART pre-trained transformer-based system that encodes MWE representations using special tokens. We show improvements in correcting specific types of verbal MWEs based on a modified version of a standard GEC evaluation approach.</abstract>
      <url hash="5bb142ff">2022.mwe-1.4</url>
      <bibkey>taslimipoor-etal-2022-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="5">
      <title>An Analysis of Attention in <fixed-case>G</fixed-case>erman Verbal Idiom Disambiguation</title>
      <author><first>Rafael</first><last>Ehren</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Timm</first><last>Lichte</last></author>
      <pages>16–25</pages>
      <abstract>In this paper we examine a BiLSTM architecture for disambiguating verbal potentially idiomatic expressions (PIEs) as to whether they are used in a literal or an idiomatic reading with respect to explainability of its decisions. Concretely, we extend the BiLSTM with an additional attention mechanism and track the elements that get the highest attention. The goal is to better understand which parts of an input sentence are particularly discriminative for the classifier’s decision, based on the assumption that these elements receive a higher attention than others. In particular, we investigate POS tags and dependency relations to PIE verbs for the tokens with the maximal attention. It turns out that the elements with maximal attention are oftentimes nouns that are the subjects of the PIE verb. For longer sentences however (i.e., sentences containing, among others, more modifiers), the highest attention word often stands in a modifying relation to the PIE components. This is particularly frequent for PIEs classified as literal. Our study shows that an attention mechanism can contribute to the explainability of classification decisions that depend on specific cues in the sentential context, as it is the case for PIE disambiguation.</abstract>
      <url hash="b0efe3d3">2022.mwe-1.5</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e4523e52">2022.mwe-1.5.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>ehren-etal-2022-analysis</bibkey>
      <pwccode url="https://github.com/rafehr/pie-attention" additional="false">rafehr/pie-attention</pwccode>
    </paper>
    <paper id="6">
      <title>Support Verb Constructions across the Ocean Sea</title>
      <author><first>Jorge</first><last>Baptista</last></author>
      <author><first>Nuno</first><last>Mamede</last></author>
      <author><first>Sónia</first><last>Reis</last></author>
      <pages>26–36</pages>
      <abstract>This paper analyses the support (or light) verb constructions (SVC) in a publicly available, manually annotated corpus of multiword expressions (MWE) in Brazilian Portuguese. The paper highlights several issues in the linguistic definitions therein adopted for these types of MWE, and reports the results from applying STRING, a rule-based parsing system, originally developed for European Portuguese, to this corpus from Brazilian Portuguese. The goal is two-fold: to improve the linguistic definition of SVC in the annotation task, as well as to gauge the major difficulties found when transposing linguistic resources between these two varieties of the same language.</abstract>
      <url hash="80eb8ac2">2022.mwe-1.6</url>
      <bibkey>baptista-etal-2022-support</bibkey>
    </paper>
    <paper id="7">
      <title>A Matrix-Based Heuristic Algorithm for Extracting Multiword Expressions from a Corpus</title>
      <author><first>Orhan</first><last>Bilgin</last></author>
      <pages>37–48</pages>
      <abstract>This paper describes an algorithm for automatically extracting multiword expressions (MWEs) from a corpus. The algorithm is node-based, i.e. extracts MWEs that contain the item specified by the user, using a fixed window-size around the node. The main idea is to detect the frequency anomalies that occur at the starting and ending points of an ngram that constitutes a MWE. This is achieved by locally comparing matrices of observed frequencies to matrices of expected frequencies, and determining, for each individual input, one or more sub-sequences that have the highest probability of being a MWE. Top-performing sub-sequences are then combined in a score-aggregation and ranking stage, thus producing a single list of score-ranked MWE candidates, without having to indiscriminately generate all possible sub-sequences of the input strings. The knowledge-poor and computationally efficient algorithm attempts to solve certain recurring problems in MWE extraction, such as the inability to deal with MWEs of arbitrary length, the repetitive counting of nested ngrams, and excessive sensitivity to frequency. Evaluation results show that the best-performing version generates top-50 precision values between 0.71 and 0.88 on Turkish and English data, and performs better than the baseline method even at n=1000.</abstract>
      <url hash="cdc727b9">2022.mwe-1.7</url>
      <attachment type="OptionalSupplementaryMaterial" hash="03a0fa1e">2022.mwe-1.7.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>bilgin-2022-matrix</bibkey>
      <pwccode url="https://github.com/melanuria/mwe_extractor" additional="false">melanuria/mwe_extractor</pwccode>
    </paper>
    <paper id="8">
      <title>Multi-word Lexical Units Recognition in <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Marek</first><last>Maziarz</last></author>
      <author><first>Ewa</first><last>Rudnicka</last></author>
      <author><first>Łukasz</first><last>Grabowski</last></author>
      <pages>49–54</pages>
      <abstract>WordNet is a state-of-the-art lexical resource used in many tasks in Natural Language Processing, also in multi-word expression (MWE) recognition. However, not all MWEs recorded in WordNet could be indisputably called lexicalised. Some of them are semantically compositional and show no signs of idiosyncrasy. This state of affairs affects all evaluation measures that use the list of all WordNet MWEs as a gold standard. We propose a method of distinguishing between lexicalised and non-lexicalised word combinations in WordNet, taking into account lexicality features, such as semantic compositionality, MWE length and translational criterion. Both a rule-based approach and a ridge logistic regression are applied, beating a random baseline in precision of singling out lexicalised MWEs, as well as in recall of ruling out cases of non-lexicalised MWEs.</abstract>
      <url hash="0a6174b2">2022.mwe-1.8</url>
      <bibkey>maziarz-etal-2022-multi</bibkey>
    </paper>
    <paper id="9">
      <title>Automatic Detection of Difficulty of <fixed-case>F</fixed-case>rench Medical Sequences in Context</title>
      <author><first>Anaïs</first><last>Koptient</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <pages>55–66</pages>
      <abstract>Medical documents use technical terms (single or multi-word expressions) with very specific semantics. Patients may find it difficult to understand these terms, which may lower their understanding of medical information. Before the simplification step of such terms, it is important to detect difficult to understand syntactic groups in medical documents as they may correspond to or contain technical terms. We address this question through categorization: we have to predict difficult to understand syntactic groups within syntactically analyzed medical documents. We use different models for this task: one built with only internal features (linguistic features), one built with only external features (contextual features), and one built with both sets of features. Our results show an f-measure over 0.8. Use of contextual (external) features and of annotations from all annotators impact the results positively. Ablation tests indicate that frequencies in large corpora and lexicon are relevant for this task.</abstract>
      <url hash="f4b1b281">2022.mwe-1.9</url>
      <bibkey>koptient-grabar-2022-automatic</bibkey>
    </paper>
    <paper id="10">
      <title>Annotating “Particles” in Multiword Expressions in te reo <fixed-case>M</fixed-case>āori for a Part-of-Speech Tagger</title>
      <author><first>Aoife</first><last>Finn</last></author>
      <author><first>Suzanne</first><last>Duncan</last></author>
      <author><first>Peter-Lucas</first><last>Jones</last></author>
      <author><first>Gianna</first><last>Leoni</last></author>
      <author><first>Keoni</first><last>Mahelona</last></author>
      <pages>67–74</pages>
      <abstract>This paper discusses the development of a Part-of-Speech tagger for te reo Māori, which is the Indigenous language of Aotearoa, also known as New Zealand. Te reo Māori is a particularly analytical and polysemic language. A word class called “particles” is introduced, they are small multi-functional words with many meanings, for example ē, ai, noa, rawa, mai, anō and koa. These “particles” are reflective of the analytical and polysemous nature of te reo Māori. They frequently occur both singularly and also in multiword expressions, including time adverbial phrases. The paper illustrates the challenges that they presented to part-of-speech tagging. It also discusses how we overcome these challenges in a way that is appropriate for te reo Māori, given its status an Indigenous language and history of colonisation. This includes a discussion of the importance of accurately reflecting the conceptualization of te reo Māori. And how this involved making no linguistic presumptions, and of eliciting faithful judgements from speakers, in a way that is uninfluenced by linguistic terminology.</abstract>
      <url hash="363dda0a">2022.mwe-1.10</url>
      <bibkey>finn-etal-2022-annotating</bibkey>
    </paper>
    <paper id="11">
      <title>Metaphor Detection for Low Resource Languages: From Zero-Shot to Few-Shot Learning in <fixed-case>M</fixed-case>iddle <fixed-case>H</fixed-case>igh <fixed-case>G</fixed-case>erman</title>
      <author><first>Felix</first><last>Schneider</last></author>
      <author><first>Sven</first><last>Sickert</last></author>
      <author><first>Phillip</first><last>Brandes</last></author>
      <author><first>Sophie</first><last>Marshall</last></author>
      <author><first>Joachim</first><last>Denzler</last></author>
      <pages>75–80</pages>
      <abstract>In this work, we present a novel unsupervised method for adjective-noun metaphor detection on low resource languages. We propose two new approaches: First, a way of artificially generating metaphor training examples and second, a novel way to find metaphors relying only on word embeddings. The latter enables application for low resource languages. Our method is based on a transformation of word embedding vectors into another vector space, in which the distance between the adjective word vector and the noun word vector represents the metaphoricity of the word pair. We train this method in a zero-shot pseudo-supervised manner by generating artificial metaphor examples and show that our approach can be used to generate a metaphor dataset with low annotation cost. It can then be used to finetune the system in a few-shot manner. In our experiments we show the capabilities of the method in its unsupervised and in its supervised version. Additionally, we test it against a comparable unsupervised baseline method and a supervised variation of it.</abstract>
      <url hash="0202346e">2022.mwe-1.11</url>
      <bibkey>schneider-etal-2022-metaphor</bibkey>
      <pwccode url="https://github.com/cvjena/metaphor-detector" additional="false">cvjena/metaphor-detector</pwccode>
    </paper>
    <paper id="12">
      <title>Automatic Bilingual Phrase Dictionary Construction from <fixed-case>GIZA</fixed-case>++ Output</title>
      <author><first>Albina</first><last>Khusainova</last></author>
      <author><first>Vitaly</first><last>Romanov</last></author>
      <author><first>Adil</first><last>Khan</last></author>
      <pages>81–88</pages>
      <abstract>Modern encoder-decoder based neural machine translation (NMT) models are normally trained on parallel sentences. Hence, they give best results when translating full sentences rather than sentence parts. Thereby, the task of translating commonly used phrases, which often arises for language learners, is not addressed by NMT models. While for high-resourced language pairs human-built phrase dictionaries exist, less-resourced pairs do not have them. We suggest an approach for building such dictionary automatically based on the GIZA++ output and show that it works significantly better than translating phrases with a sentences-trained NMT system.</abstract>
      <url hash="4498aa30">2022.mwe-1.12</url>
      <bibkey>khusainova-etal-2022-automatic</bibkey>
    </paper>
    <paper id="13">
      <title>A <fixed-case>BERT</fixed-case>’s Eye View: Identification of <fixed-case>I</fixed-case>rish Multiword Expressions Using Pre-trained Language Models</title>
      <author><first>Abigail</first><last>Walsh</last></author>
      <author><first>Teresa</first><last>Lynn</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <pages>89–99</pages>
      <abstract>This paper reports on the investigation of using pre-trained language models for the identification of Irish verbal multiword expressions (vMWEs), comparing the results with the systems submitted for the PARSEME shared task edition 1.2. We compare the use of a monolingual BERT model for Irish (gaBERT) with multilingual BERT (mBERT), fine-tuned to perform MWE identification, presenting a series of experiments to explore the impact of hyperparameter tuning and dataset optimisation steps on these models. We compare the results of our optimised systems to those achieved by other systems submitted to the shared task, and present some best practices for minority languages addressing this task.</abstract>
      <url hash="b7900baf">2022.mwe-1.13</url>
      <bibkey>walsh-etal-2022-berts</bibkey>
    </paper>
    <paper id="14">
      <title>Enhancing the <fixed-case>PARSEME</fixed-case> <fixed-case>T</fixed-case>urkish Corpus of Verbal Multiword Expressions</title>
      <author><first>Yagmur</first><last>Ozturk</last></author>
      <author><first>Najet</first><last>Hadj Mohamed</last></author>
      <author><first>Adam</first><last>Lion-Bouton</last></author>
      <author><first>Agata</first><last>Savary</last></author>
      <pages>100–104</pages>
      <abstract>The PARSEME (Parsing and Multiword Expressions) project proposes multilingual corpora annotated for multiword expressions (MWEs). In this case study, we focus on the Turkish corpus of PARSEME. Turkish is an agglutinative language and shows high inflection and derivation in word forms. This can cause some issues in terms of automatic morphosyntactic annotation. We provide an overview of the problems observed in the morphosyntactic annotation of the Turkish PARSEME corpus. These issues are mostly observed on the lemmas, which is important for the approximation of a type of an MWE. We propose modifications of the original corpus with some enhancements on the lemmas and parts of speech. The enhancements are then evaluated with an identification system from the PARSEME Shared Task 1.2 to detect MWEs, namely Seen2Seen. Results show increase in the F-measure for MWE identification, emphasizing the necessity of robust morphosyntactic annotation for MWE processing, especially for languages that show high surface variability.</abstract>
      <url hash="38b3b615">2022.mwe-1.14</url>
      <bibkey>ozturk-etal-2022-enhancing</bibkey>
    </paper>
    <paper id="15">
      <title>Sample Efficient Approaches for Idiomaticity Detection</title>
      <author><first>Dylan</first><last>Phelps</last></author>
      <author><first>Xuan-Rui</first><last>Fan</last></author>
      <author><first>Edward</first><last>Gow-Smith</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <pages>105–111</pages>
      <abstract>Deep neural models, in particular Transformer-based pre-trained language models, require a significant amount of data to train. This need for data tends to lead to problems when dealing with idiomatic multiword expressions (MWEs), which are inherently less frequent in natural text. As such, this work explores sample efficient methods of idiomaticity detection. In particular we study the impact of Pattern Exploit Training (PET), a few-shot method of classification, and BERTRAM, an efficient method of creating contextual embeddings, on the task of idiomaticity detection. In addition, to further explore generalisability, we focus on the identification of MWEs not present in the training data. Our experiments show that while these methods improve performance on English, they are much less effective on Portuguese and Galician, leading to an overall performance about on par with vanilla mBERT. Regardless, we believe sample efficient methods for both identifying and representing potentially idiomatic MWEs are very encouraging and hold significant potential for future exploration.</abstract>
      <url hash="a16b0722">2022.mwe-1.15</url>
      <bibkey>phelps-etal-2022-sample</bibkey>
    </paper>
    <paper id="16">
      <title>mwetoolkit-lib: Adaptation of the mwetoolkit as a Python Library and an Application to <fixed-case>MWE</fixed-case>-based Document Clustering</title>
      <author><first>Fernando</first><last>Zagatti</last></author>
      <author><first>Paulo Augusto de Lima</first><last>Medeiros</last></author>
      <author><first>Esther da Cunha</first><last>Soares</last></author>
      <author><first>Lucas Nildaimon dos Santos</first><last>Silva</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <author><first>Livy</first><last>Real</last></author>
      <pages>112–117</pages>
      <abstract>This paper introduces the mwetoolkit-lib, an adaptation of the mwetoolkit as a python library. The original toolkit performs the extraction and identification of multiword expressions (MWEs) in large text bases through the command line. One of the contributions of our work is the adaptation of the MWE extraction pipeline from the mwetoolkit, allowing its usage in python development environments and integration in larger pipelines. The other contribution is the execution of a pilot experiment aiming to show the impact of MWE discovery in data professionals’ work. This experiment found that the addition of MWE knowledge to the Term Frequency-Inverse Document Frequency (TF-IDF) vectorization altered the word relevance order, improving the linguistic quality of the clusters returned by k-means method.</abstract>
      <url hash="f1df9f8c">2022.mwe-1.16</url>
      <bibkey>zagatti-etal-2022-mwetoolkit</bibkey>
    </paper>
    <paper id="17">
      <title>Handling Idioms in Symbolic Multilingual Natural Language Generation</title>
      <author><first>Michaelle</first><last>Dubé</last></author>
      <author><first>François</first><last>Lareau</last></author>
      <pages>118–126</pages>
      <abstract>While idioms are usually very rigid in their expression, they sometimes allow a certain level of freedom in their usage, with modifiers or complements splitting them or being syntactically attached to internal nodes rather than to the root (e.g., “take something with a big grain of salt”). This means that they cannot always be handled as ready-made strings in rule-based natural language generation systems. Having access to the internal syntactic structure of an idiom allows for more subtle processing. We propose a way to enumerate all possible language-independent n-node trees and to map particular idioms of a language onto these generic syntactic patterns. Using this method, we integrate the idioms from the LN-fr into GenDR, a multilingual realizer. Our implementation covers nearly 98% of LN-fr’s idioms with high precision, and can easily be extended or ported to other languages.</abstract>
      <url hash="a21d2f93">2022.mwe-1.17</url>
      <bibkey>dube-lareau-2022-handling</bibkey>
    </paper>
  </volume>
</collection>
