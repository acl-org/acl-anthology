<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.splu">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the Third International Workshop on Spatial Language Understanding</booktitle>
      <editor><first>Parisa</first><last>Kordjamshidi</last></editor>
      <editor><first>Archna</first><last>Bhatia</last></editor>
      <editor><first>Malihe</first><last>Alikhani</last></editor>
      <editor><first>Jason</first><last>Baldridge</last></editor>
      <editor><first>Mohit</first><last>Bansal</last></editor>
      <editor><first>Marie-Francine</first><last>Moens</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
      <venue>splu</venue>
    </meta>
    <frontmatter>
      <url hash="a4356e56">2020.splu-1.0</url>
      <bibkey>splu-2020-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>An Element-wise Visual-enhanced <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case>-<fixed-case>CRF</fixed-case> Model for Location Name Recognition</title>
      <author><first>Takuya</first><last>Komada</last></author>
      <author><first>Takashi</first><last>Inui</last></author>
      <pages>1–9</pages>
      <abstract>In recent years, previous studies have used visual information in named entity recognition (NER) for social media posts with attached images. However, these methods can only be applied to documents with attached images. In this paper, we propose a NER method that can use element-wise visual information for any documents by using image data corresponding to each word in the document. The proposed method obtains element-wise image data using an image retrieval engine, to be used as extra features in the neural NER model. Experimental results on the standard Japanese NER dataset show that the proposed method achieves a higher F1 value (89.67%) than a baseline method, demonstrating the effectiveness of using element-wise visual information.</abstract>
      <url hash="b09ef8ce">2020.splu-1.1</url>
      <doi>10.18653/v1/2020.splu-1.1</doi>
      <video href="https://slideslive.com/38940081"/>
      <bibkey>komada-inui-2020-element</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>BERT</fixed-case>-based Spatial Information Extraction</title>
      <author><first>Hyeong Jin</first><last>Shin</last></author>
      <author><first>Jeong Yeon</first><last>Park</last></author>
      <author><first>Dae Bum</first><last>Yuk</last></author>
      <author><first>Jae Sung</first><last>Lee</last></author>
      <pages>10–17</pages>
      <abstract>Spatial information extraction is essential to understand geographical information in text. This task is largely divided to two subtasks: spatial element extraction and spatial relation extraction. In this paper, we utilize BERT (Devlin et al., 2018), which is very effective for many natural language processing applications. We propose a BERT-based spatial information extraction model, which uses BERT for spatial element extraction and R-BERT (Wu and He, 2019) for spatial relation extraction. The model was evaluated with the SemEval 2015 dataset. The result showed a 15.4% point increase in spatial element extraction and an 8.2% point increase in spatial relation extraction in comparison to the baseline model (Nichols and Botros, 2015).</abstract>
      <url hash="dec1234e">2020.splu-1.2</url>
      <doi>10.18653/v1/2020.splu-1.2</doi>
      <video href="https://slideslive.com/38940078"/>
      <bibkey>shin-etal-2020-bert</bibkey>
    </paper>
    <paper id="3">
      <title>A Cognitively Motivated Approach to Spatial Information Extraction</title>
      <author><first>Chao</first><last>Xu</last></author>
      <author><first>Emmanuelle-Anna</first><last>Dietz Saldanha</last></author>
      <author><first>Dagmar</first><last>Gromann</last></author>
      <author><first>Beihai</first><last>Zhou</last></author>
      <pages>18–28</pages>
      <abstract>Automatic extraction of spatial information from natural language can boost human-centered applications that rely on spatial dynamics. The field of cognitive linguistics has provided theories and cognitive models to address this task. Yet, existing solutions tend to focus on specific word classes, subject areas, or machine learning techniques that cannot provide cognitively plausible explanations for their decisions. We propose an automated spatial semantic analysis (ASSA) framework building on grammar and cognitive linguistic theories to identify spatial entities and relations, bringing together methods of spatial information extraction and cognitive frameworks on spatial language. The proposed rule-based and explainable approach contributes constructions and preposition schemas and outperforms previous solutions on the CLEF-2017 standard dataset.</abstract>
      <url hash="efd401e2">2020.splu-1.3</url>
      <doi>10.18653/v1/2020.splu-1.3</doi>
      <video href="https://slideslive.com/38940079"/>
      <bibkey>xu-etal-2020-cognitively</bibkey>
    </paper>
    <paper id="4">
      <title>They Are Not All Alike: Answering Different Spatial Questions Requires Different Grounding Strategies</title>
      <author><first>Alberto</first><last>Testoni</last></author>
      <author><first>Claudio</first><last>Greco</last></author>
      <author><first>Tobias</first><last>Bianchi</last></author>
      <author><first>Mauricio</first><last>Mazuecos</last></author>
      <author><first>Agata</first><last>Marcante</last></author>
      <author><first>Luciana</first><last>Benotti</last></author>
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <pages>29–38</pages>
      <abstract>In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed model outperforms the baseline by a large extent (9.70% on spatial questions and 6.27% overall). By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions.</abstract>
      <url hash="7075bf3b">2020.splu-1.4</url>
      <doi>10.18653/v1/2020.splu-1.4</doi>
      <video href="https://slideslive.com/38940076"/>
      <bibkey>testoni-etal-2020-alike</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/guesswhat">GuessWhat?!</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visdial">VisDial</pwcdataset>
    </paper>
    <paper id="5">
      <title>Categorisation, Typicality &amp; Object-Specific Features in Spatial Referring Expressions</title>
      <author><first>Adam</first><last>Richard-Bollans</last></author>
      <author><first>Anthony</first><last>Cohn</last></author>
      <author><first>Lucía</first><last>Gómez Álvarez</last></author>
      <pages>39–49</pages>
      <abstract>Various accounts of cognition and semantic representations have highlighted that, for some concepts, different factors may influence category and typicality judgements. In particular, some features may be more salient in categorisation tasks while other features are more salient when assessing typicality. In this paper we explore the extent to which this is the case for English spatial prepositions and discuss the implications for pragmatic strategies and semantic models. We hypothesise that object-specific features — related to object properties and affordances — are more salient in categorisation, while geometric and physical relationships between objects are more salient in typicality judgements. In order to test this hypothesis we conducted a study using virtual environments to collect both category and typicality judgements in 3D scenes. Based on the collected data we cannot verify the hypothesis and conclude that object-specific features appear to be salient in both category and typicality judgements, further evidencing the need to include these types of features in semantic models.</abstract>
      <url hash="2d47b1e5">2020.splu-1.5</url>
      <doi>10.18653/v1/2020.splu-1.5</doi>
      <video href="https://slideslive.com/38940077"/>
      <bibkey>richard-bollans-etal-2020-categorisation</bibkey>
      <pwccode url="https://github.com/alrichardbollans/spatial-preposition-annotation-tool-unity3d" additional="false">alrichardbollans/spatial-preposition-annotation-tool-unity3d</pwccode>
    </paper>
    <paper id="6">
      <title>A Hybrid Deep Learning Approach for Spatial Trigger Extraction from Radiology Reports</title>
      <author><first>Surabhi</first><last>Datta</last></author>
      <author><first>Kirk</first><last>Roberts</last></author>
      <pages>50–55</pages>
      <abstract>Radiology reports contain important clinical information about patients which are often tied through spatial expressions. Spatial expressions (or triggers) are mainly used to describe the positioning of radiographic findings or medical devices with respect to some anatomical structures. As the expressions result from the mental visualization of the radiologist’s interpretations, they are varied and complex. The focus of this work is to automatically identify the spatial expression terms from three different radiology sub-domains. We propose a hybrid deep learning-based NLP method that includes – 1) generating a set of candidate spatial triggers by exact match with the known trigger terms from the training data, 2) applying domain-specific constraints to filter the candidate triggers, and 3) utilizing a BERT-based classifier to predict whether a candidate trigger is a true spatial trigger or not. The results are promising, with an improvement of 24 points in the average F1 measure compared to a standard BERT-based sequence labeler.</abstract>
      <url hash="91dbc563">2020.splu-1.6</url>
      <doi>10.18653/v1/2020.splu-1.6</doi>
      <video href="https://slideslive.com/38940080"/>
      <bibkey>datta-roberts-2020-hybrid</bibkey>
    </paper>
    <paper id="7">
      <title>Retouchdown: Releasing Touchdown on <fixed-case>S</fixed-case>treet<fixed-case>L</fixed-case>earn as a Public Resource for Language Grounding Tasks in Street View</title>
      <author><first>Harsh</first><last>Mehta</last></author>
      <author><first>Yoav</first><last>Artzi</last></author>
      <author><first>Jason</first><last>Baldridge</last></author>
      <author><first>Eugene</first><last>Ie</last></author>
      <author><first>Piotr</first><last>Mirowski</last></author>
      <pages>56–62</pages>
      <abstract>The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both Touchdown tasks: vision and language navigation (VLN) and spatial description resolution (SDR). We compare our model results to those given in (Chen et al., 2019) and show that the panoramas we have added to StreetLearn support both Touchdown tasks and can be used effectively for further research and comparison.</abstract>
      <url hash="18261a58">2020.splu-1.7</url>
      <doi>10.18653/v1/2020.splu-1.7</doi>
      <video href="https://slideslive.com/38940082"/>
      <bibkey>mehta-etal-2020-retouchdown</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/streetlearn">StreetLearn</pwcdataset>
    </paper>
  </volume>
</collection>
