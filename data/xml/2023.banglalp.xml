<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.banglalp">
  <volume id="1" ingest-date="2023-12-04" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First Workshop on Bangla Language Processing (BLP-2023)</booktitle>
      <editor><first>Firoj</first><last>Alam</last></editor>
      <editor><first>Sudipta</first><last>Kar</last></editor>
      <editor><first>Shammur Absar</first><last>Chowdhury</last></editor>
      <editor><first>Farig</first><last>Sadeque</last></editor>
      <editor><first>Ruhul</first><last>Amin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore</address>
      <month>December</month>
      <year>2023</year>
      <url hash="2cd9403e">2023.banglalp-1</url>
      <venue>banglalp</venue>
    </meta>
    <frontmatter>
      <url hash="e884d46e">2023.banglalp-1.0</url>
      <bibkey>banglalp-2023-bangla</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Offensive Language Identification in Transliterated and Code-Mixed <fixed-case>B</fixed-case>angla</title>
      <author><first>Md Nishat</first><last>Raihan</last></author>
      <author><first>Umma</first><last>Tanmoy</last></author>
      <author><first>Anika Binte</first><last>Islam</last></author>
      <author><first>Kai</first><last>North</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last><affiliation>Aston University and University of Wolverhampton</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>George Mason University</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <pages>1-6</pages>
      <abstract>Identifying offensive content in social media is vital to create safe online communities. Several recent studies have addressed this problem by creating datasets for various languages. In this paper, we explore offensive language identification in texts with transliterations and code-mixing, linguistic phenomena common in multilingual societies, and a known challenge for NLP systems. We introduce TB-OLID, a transliterated Bangla offensive language dataset containing 5,000 manually annotated comments. We train and fine-tune machine learning models on TB-OLID, and we evaluate their results on this dataset. Our results show that English pre-trained transformer-based models, such as fBERT and HateBERT achieve the best performance on this dataset.</abstract>
      <url hash="01aed394">2023.banglalp-1.1</url>
      <bibkey>raihan-etal-2023-offensive</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.1</doi>
      <video href="2023.banglalp-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title><fixed-case>BS</fixed-case>pell: A <fixed-case>CNN</fixed-case>-Blended <fixed-case>BERT</fixed-case> Based <fixed-case>B</fixed-case>angla Spell Checker</title>
      <author><first>Chowdhury</first><last>Rahman</last></author>
      <author><first>MD.Hasibur</first><last>Rahman</last></author>
      <author><first>Samiha</first><last>Zakir</last></author>
      <author><first>Mohammad</first><last>Rafsan</last></author>
      <author><first>Mohammed Eunus</first><last>Ali</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <pages>7-17</pages>
      <abstract>Bangla typing is mostly performed using English keyboard and can be highly erroneous due to the presence of compound and similarly pronounced letters. Spelling correction of a misspelled word requires understanding of word typing pattern as well as the context of the word usage. A specialized BERT model named BSpell has been proposed in this paper targeted towards word for word correction in sentence level. BSpell contains an end-to-end trainable CNN sub-model named SemanticNet along with specialized auxiliary loss. This allows BSpell to specialize in highly inflected Bangla vocabulary in the presence of spelling errors. Furthermore, a hybrid pretraining scheme has been proposed for BSpell that combines word level and character level masking. Comparison on two Bangla and one Hindi spelling correction dataset shows the superiority of our proposed approach.</abstract>
      <url hash="d12a22f9">2023.banglalp-1.2</url>
      <bibkey>rahman-etal-2023-bspell</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.2</doi>
      <video href="2023.banglalp-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Advancing <fixed-case>B</fixed-case>angla Punctuation Restoration by a Monolingual Transformer-Based Method and a Large-Scale Corpus</title>
      <author><first>Mehedi Hasan</first><last>Bijoy</last><affiliation>Bangladesh University of Business &amp; Technology</affiliation></author>
      <author><first>Mir Fatema Afroz</first><last>Faria</last></author>
      <author><first>Mahbub</first><last>E Sobhani</last></author>
      <author><first>Tanzid</first><last>Ferdoush</last></author>
      <author><first>Swakkhar</first><last>Shatabda</last><affiliation>United International University</affiliation></author>
      <pages>18-25</pages>
      <abstract>Punctuation restoration is the endeavor of reinstating and rectifying missing or improper punctuation marks within a text, thereby eradicating ambiguity in written discourse. The Bangla punctuation restoration task has received little attention and exploration, despitethe rising popularity of textual communication in the language. The primary hindrances in the advancement of the task revolve aroundthe utilization of transformer-based methods and an openly accessible extensive corpus, challenges that we discovered remainedunresolved in earlier efforts. In this study, we propose a baseline by introducing a mono-lingual transformer-based method named Jatikarok, where the effectiveness of transfer learning has been meticulously scrutinized, and a large-scale corpus containing 1.48M source-target pairs to resolve the previous issues. The Jatikarok attains accuracy rates of 95.2%, 85.13%, and 91.36% on the BanglaPRCorpus, Prothom-Alo Balanced, and BanglaOPUS corpora, thereby establishing itself as the state-of-the-art method through its superior performance compared to BanglaT5 and T5-Small. Jatikarok and BanglaPRCorpus are publicly available at: https://github.com/mehedihasanbijoy/Jatikarok-and-BanglaPRCorpus</abstract>
      <url hash="dfcc967f">2023.banglalp-1.3</url>
      <bibkey>bijoy-etal-2023-advancing</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.3</doi>
      <video href="2023.banglalp-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Pipeline Enabling Zero-shot Classification for <fixed-case>B</fixed-case>angla Handwritten Grapheme</title>
      <author><first>Linsheng</first><last>Guo</last></author>
      <author><first>Md Habibur</first><last>Sifat</last></author>
      <author><first>Tashin</first><last>Ahmed</last></author>
      <pages>26-33</pages>
      <abstract>This research investigates Zero-Shot Learning (ZSL), and proposes CycleGAN-based image synthesis and accurate label mapping to build a strong association between labels and graphemes. The objective is to enhance model accuracy in detecting unseen classes by employing advanced font image categorization and a CycleGAN-based generator. The resulting representations of abstract character structures demonstrate a significant improvement in recognition, accommodating both seen and unseen classes. This investigation addresses the complex issue of Optical Character Recognition (OCR) in the specific context of the Bangla language. Bangla script is renowned for its intricate nature, consisting of a total of 49 letters, which include 11 vowels, 38 consonants, and 18 diacritics. The combination of letters in this complex arrangement provides the opportunity to create almost 13,000 unique variations of graphemes, which exceeds the number of graphemic units found in the English language. Our investigation presents a new strategy for ZSL in the context of Bangla OCR. This approach combines generative models with careful labeling techniques to enhance the progress of Bangla OCR, specifically focusing on grapheme categorization. Our goal is to make a substantial impact on the digitalization of educational resources in the Indian subcontinent.</abstract>
      <url hash="3864b7d4">2023.banglalp-1.4</url>
      <bibkey>guo-etal-2023-pipeline</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.4</doi>
      <video href="2023.banglalp-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Low-Resource Text Style Transfer for <fixed-case>B</fixed-case>angla: Data &amp; Models</title>
      <author><first>Sourabrata</first><last>Mukherjee</last></author>
      <author><first>Akanksha</first><last>Bansal</last></author>
      <author><first>Pritha</first><last>Majumdar</last></author>
      <author><first>Atul Kr.</first><last>Ojha</last><affiliation>University of Galway, Ireland, Insight SFI Research Centre for Data Analytics, DSI, University of Galway, Ireland and Panlingua Languague Processing LLP, India</affiliation></author>
      <author><first>Ondřej</first><last>Dušek</last><affiliation>Charles University, Prague</affiliation></author>
      <pages>34-47</pages>
      <abstract>Text style transfer (TST) involves modifying the linguistic style of a given text while retaining its core content. This paper addresses the challenging task of text style transfer in the Bangla language, which is low-resourced in this area. We present a novel Bangla dataset that facilitates text sentiment transfer, a subtask of TST, enabling the transformation of positive sentiment sentences to negative and vice versa. To establish a high-quality base for further research, we refined and corrected an existing English dataset of 1,000 sentences for sentiment transfer based on Yelp reviews, and we introduce a new human-translated Bangla dataset that parallels its English counterpart. Furthermore, we offer multiple benchmark models that serve as a validation of the dataset and baseline for further research.</abstract>
      <url hash="14b5c9c1">2023.banglalp-1.5</url>
      <bibkey>mukherjee-etal-2023-low</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for <fixed-case>B</fixed-case>angla and <fixed-case>S</fixed-case>ylheti</title>
      <author><first>Fardin Ahsan</first><last>Sakib</last></author>
      <author><first>A H M Rezaul</first><last>Karim</last></author>
      <author><first>Saadat Hasan</first><last>Khan</last></author>
      <author><first>Md Mushfiqur</first><last>Rahman</last></author>
      <pages>48-55</pages>
      <abstract>As voice assistants cement their place in our technologically advanced society, there remains a need to cater to the diverse linguistic landscape, including colloquial forms of low-resource languages. Our study introduces the first-ever comprehensive dataset for intent detection and slot filling in formal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples across 10 unique intents. Our analysis reveals the robustness of large language models for tackling downstream tasks with inadequate data. The GPT-3.5 model achieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot filling for colloquial Bangla.</abstract>
      <url hash="212d7d91">2023.banglalp-1.6</url>
      <bibkey>sakib-etal-2023-intent</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>BE</fixed-case>mo<fixed-case>L</fixed-case>ex<fixed-case>BERT</fixed-case>: A Hybrid Model for Multilabel Textual Emotion Classification in <fixed-case>B</fixed-case>angla by Combining Transformers with Lexicon Features</title>
      <author><first>Ahasan</first><last>Kabir</last></author>
      <author><first>Animesh</first><last>Roy</last></author>
      <author><first>Zaima</first><last>Taheri</last></author>
      <pages>56-61</pages>
      <abstract>Multilevel textual emotion classification involves the extraction of emotions from text data, a task that has seen significant progress in high resource languages. However, resource-constrained languages like Bangla have received comparatively less attention in the field of emotion classification. Furthermore, the availability of a comprehensive and accurate emotion lexiconspecifically designed for the Bangla language is limited. In this paper, we present a hybrid model that combines lexicon features with transformers for multilabel emotion classification in the Bangla language. We have developed a comprehensive Bangla emotion lexicon consisting of 5336 carefully curated lexicons across nine emotion categories. We experimented with pre-trained transformers including mBERT, XLM-R, BanglishBERT, and BanglaBERT on the EmoNaBa (Islam et al.,2022) dataset. By integrating lexicon features from our emotion lexicon, we evaluate the performance of these transformers in emotion detection tasks. The results demonstrate that incorporating lexicon features significantly improves the performance of transformers. Among the evaluated models, our hybrid approach achieves the highest performance using BanglaBERT(large) (Bhattacharjee et al., 2022) as the pre-trained transformer along with our emotion lexicon, achieving an impressive weighted F1 score of 82.73%. The emotion lexicon is publicly available at https://github.com/Ahasannn/BEmoLex-Bangla_Emotion_Lexicon</abstract>
      <url hash="2dc1ccde">2023.banglalp-1.7</url>
      <bibkey>kabir-etal-2023-bemolexbert</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.7</doi>
      <video href="2023.banglalp-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Assessing Political Inclination of <fixed-case>B</fixed-case>angla Language Models</title>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Ashwarya</first><last>Maratha</last></author>
      <author><first>Khan Md</first><last>Hasib</last></author>
      <author><first>Mehwish</first><last>Nasim</last><affiliation>University of Western Australia and Flinders University of South Australia</affiliation></author>
      <author><first>Usman</first><last>Naseem</last><affiliation>James Cook University of North Queensland</affiliation></author>
      <pages>62-71</pages>
      <abstract>Natural language processing has advanced with AI-driven language models (LMs), that are applied widely from text generation to question answering. These models are pre-trained on a wide spectrum of data sources, enhancing accuracy and responsiveness. However, this process inadvertently entails the absorption of a diverse spectrum of viewpoints inherent within the training data. Exploring political leaning within LMs due to such viewpoints remains a less-explored domain. In the context of a low-resource language like Bangla, this area of research is nearly non-existent. To bridge this gap, we comprehensively analyze biases present in Bangla language models, specifically focusing on social and economic dimensions. Our findings reveal the inclinations of various LMs, which will provide insights into ethical considerations and limitations associated with deploying Bangla LMs.</abstract>
      <url hash="9e20120d">2023.banglalp-1.8</url>
      <bibkey>thapa-etal-2023-assessing</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.8</doi>
      <video href="2023.banglalp-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Vio-Lens: A Novel Dataset of Annotated Social Network Posts Leading to Different Forms of Communal Violence and its Evaluation</title>
      <author><first>Sourav</first><last>Saha</last></author>
      <author><first>Jahedul Alam</first><last>Junaed</last></author>
      <author><first>Maryam</first><last>Saleki</last></author>
      <author><first>Arnab</first><last>Sen Sharma</last></author>
      <author><first>Mohammad Rashidujjaman</first><last>Rifat</last></author>
      <author><first>Mohamed</first><last>Rahouti</last><affiliation>Fordham University</affiliation></author>
      <author><first>Syed Ishtiaque</first><last>Ahmed</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Nabeel</first><last>Mohammed</last></author>
      <author><first>Mohammad Ruhul</first><last>Amin</last><affiliation>Fordham University</affiliation></author>
      <pages>72-84</pages>
      <abstract>This paper presents a computational approach for creating a dataset on communal violence in the context of Bangladesh and West Bengal of India and benchmark evaluation. In recent years, social media has been used as a weapon by factions of different religions and backgrounds to incite hatred, resulting in physical communal violence and causing death and destruction. To prevent such abusive use of online platforms, we propose a framework for classifying online posts using an adaptive question-based approach. We collected more than 168,000 YouTube comments from a set of manually selected videos known for inciting violence in Bangladesh and West Bengal. Using both unsupervised and later semi-supervised topic modeling methods on those unstructured data, we discovered the major word clusters to interpret the related topics of peace and violence. Topic words were later used to select 20,142 posts related to peace and violence of which we annotated a total of 6,046 posts. Finally, we applied different modeling techniques based on linguistic features, and sentence transformers to benchmark the labeled dataset with the best-performing model reaching ~71% macro F1 score.</abstract>
      <url hash="dbf6c4f4">2023.banglalp-1.9</url>
      <bibkey>saha-etal-2023-vio</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>B</fixed-case>angla<fixed-case>CHQ</fixed-case>-Summ: An Abstractive Summarization Dataset for Medical Queries in <fixed-case>B</fixed-case>angla Conversational Speech</title>
      <author><first>Alvi</first><last>Khan</last></author>
      <author><first>Fida</first><last>Kamal</last></author>
      <author><first>Mohammad Abrar</first><last>Chowdhury</last></author>
      <author><first>Tasnim</first><last>Ahmed</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Md Tahmid Rahman</first><last>Laskar</last><affiliation>Dialpad Inc.</affiliation></author>
      <author><first>Sabbir</first><last>Ahmed</last><affiliation>Islamic University of Technology</affiliation></author>
      <pages>85-93</pages>
      <abstract>Online health consultation is steadily gaining popularity as a platform for patients to discuss their medical health inquiries, known as Consumer Health Questions (CHQs). The emergence of the COVID-19 pandemic has also led to a surge in the use of such platforms, creating a significant burden for the limited number of healthcare professionals attempting to respond to the influx of questions. Abstractive text summarization is a promising solution to this challenge, since shortening CHQs to only the information essential to answering them reduces the amount of time spent parsing unnecessary information. The summarization process can also serve as an intermediate step towards the eventual development of an automated medical question-answering system. This paper presents ‘BanglaCHQ-Summ’, the first CHQ summarization dataset for the Bangla language, consisting of 2,350 question-summary pairs. It is benchmarked on state-of-the-art Bangla and multilingual text generation models, with the best-performing model, BanglaT5, achieving a ROUGE-L score of 48.35%. In addition, we address the limitations of existing automatic metrics for summarization by conducting a human evaluation. The dataset and all relevant code used in this work have been made publicly available.</abstract>
      <url hash="2b73bdf3">2023.banglalp-1.10</url>
      <bibkey>khan-etal-2023-banglachq</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.10</doi>
      <video href="2023.banglalp-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Contextual <fixed-case>B</fixed-case>angla Neural Stemmer: Finding Contextualized Root Word Representations for <fixed-case>B</fixed-case>angla Words</title>
      <author><first>Md</first><last>Fahim</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>Amin Ahsan</first><last>Ali</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>M Ashraful</first><last>Amin</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>Akmmahbubur</first><last>Rahman</last></author>
      <pages>94-103</pages>
      <abstract>Stemmers are commonly used in NLP to reduce words to their root form. However, this process may discard important information and yield incorrect root forms, affecting the accuracy of NLP tasks. To address these limitations, we propose a Contextual Bangla Neural Stemmer for Bangla language to enhance word representations. Our method involves splitting words into characters within the Neural Stemming Block, obtaining vector representations for both stem words and unknown vocabulary words. A loss function aligns these representations with Word2Vec representations, followed by contextual word representations from a Universal Transformer encoder. Mean Pooling generates sentence-level representations that are aligned with BanglaBERT’s representations using a MLP layer. The proposed model also tries to build good representations for out-of-vocabulary (OOV) words. Experiments with our model on five Bangla datasets shows around 5% average improvement over the vanilla approach. Notably, our method avoids BERT retraining, focusing on root word detection and addressing OOV and sub-word issues. By incorporating our approach into a large corpus-based Language Model, we expect further improvements in aspects like explainability.</abstract>
      <url hash="be9cbfd0">2023.banglalp-1.11</url>
      <bibkey>fahim-etal-2023-contextual</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Investigating the Effectiveness of Graph-based Algorithm for <fixed-case>B</fixed-case>angla Text Classification</title>
      <author><first>Farhan</first><last>Dehan</last></author>
      <author><first>Md</first><last>Fahim</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>Amin Ahsan</first><last>Ali</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>M Ashraful</first><last>Amin</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>Akmmahbubur</first><last>Rahman</last></author>
      <pages>104-116</pages>
      <abstract>In this study, we examine and analyze the behavior of several graph-based models for Bangla text classification tasks. Graph-based algorithms create heterogeneous graphs from text data. Each node represents either a word or a document, and each edge indicates relationship between any two words or word and document. We applied the BERT model and different graph-based models including TextGCN, GAT, BertGAT, and BertGCN on five different datasets including SentNoB, Sarcasm detection, BanFakeNews, Hate speech detection, and Emotion detection datasets for Bangla text. BERT’s model bested the TextGCN and the GAT models by a large difference in terms of accuracy, Macro F1 score, and weighted F1 score. BertGCN and BertGAT are shown to outperform standalone graph models and BERT model. BertGAT excelled in the Emotion detection dataset and achieved a 1%-2% performance boost in Sarcasm detection, Hate speech detection, and BanFakeNews datasets from BERT’s performance. Whereas, BertGCN outperformed BertGAT by 1% for SetNoB, and BanFakeNews datasets while beating BertGAT by 2% for Sarcasm detection, Hate Speech, and Emotion detection datasets. We also examined different variations in graph structure and analyzed their effects.</abstract>
      <url hash="c901c293">2023.banglalp-1.12</url>
      <bibkey>dehan-etal-2023-investigating</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>S</fixed-case>ynth<fixed-case>NID</fixed-case>: Synthetic Data to Improve End-to-end <fixed-case>B</fixed-case>angla Document Key Information Extraction</title>
      <author><first>Syed Mostofa</first><last>Monsur</last></author>
      <author><first>Shariar</first><last>Kabir</last></author>
      <author><first>Sakib</first><last>Chowdhury</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <pages>117-123</pages>
      <abstract>End-to-end Document Key Information Extraction models require a lot of compute and labeled data to perform well on real datasets. This is particularly challenging for low-resource languages like Bangla where domain-specific multimodal document datasets are scarcely available. In this paper, we have introduced SynthNID, a system to generate domain-specific document image data for training OCR-less end-to-end Key Information Extraction systems. We show the generated data improves the performance of the extraction model on real datasets and the system is easily extendable to generate other types of scanned documents for a wide range of document understanding tasks. The code for generating synthetic data is available at https://github.com/dv66/synthnid</abstract>
      <url hash="06136064">2023.banglalp-1.13</url>
      <bibkey>monsur-etal-2023-synthnid</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.13</doi>
      <video href="2023.banglalp-1.13.mp4"/>
    </paper>
    <paper id="14">
      <title><fixed-case>B</fixed-case>a<fixed-case>TEC</fixed-case>la<fixed-case>C</fixed-case>or: A Novel Dataset for <fixed-case>B</fixed-case>angla Text Error Classification and Correction</title>
      <author><first>Nabilah</first><last>Oshin</last></author>
      <author><first>Syed</first><last>Hoque</last><affiliation>Center for Computation and Data Sciences,IUB</affiliation></author>
      <author><first>Md</first><last>Fahim</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>Amin Ahsan</first><last>Ali</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>M Ashraful</first><last>Amin</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>Akmmahbubur</first><last>Rahman</last></author>
      <pages>124-135</pages>
      <abstract>In the context of the dynamic realm of Bangla communication, online users are often prone to bending the language or making errors due to various factors. We attempt to detect, categorize, and correct those errors by employing several machine learning and deep learning models. To contribute to the preservation and authenticity of the Bangla language, we introduce a meticulously categorized organic dataset encompassing 10,000 authentic Bangla comments from a commonly used social media platform. Through rigorous comparative analysis of distinct models, our study highlights BanglaBERT’s superiority in error-category classification and underscores the effectiveness of BanglaT5 for text correction. BanglaBERT achieves accuracy of 79.1% and 74.1% for binary and multiclass error-category classification while the BanglaBERT is fine-tuned and tested with our proposed dataset. Moreover, BanglaT5 achieves the best Rouge-L score (0.8459) when BanglaT5 is fine-tuned and tested with our corrected ground truths. Beyond algorithmic exploration, this endeavor represents a significant stride in enhancing the quality of digital discourse in the Bangla-speaking community, fostering linguistic precision and coherence in online interactions. The dataset and code is available at https://github.com/SyedT1/BaTEClaCor.</abstract>
      <url hash="c1b76df7">2023.banglalp-1.14</url>
      <bibkey>oshin-etal-2023-bateclacor</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.14</doi>
      <video href="2023.banglalp-1.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Crosslingual Retrieval Augmented In-context Learning for <fixed-case>B</fixed-case>angla</title>
      <author><first>Xiaoqian</first><last>Li</last></author>
      <author><first>Ercong</first><last>Nie</last></author>
      <author><first>Sheng</first><last>Liang</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>136-151</pages>
      <abstract>The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla. To address this, our paper presents a pioneering approach that utilizes cross-lingual retrieval augmented in-context learning. By strategically sourcing semantically similar prompts from high-resource language, we enable multilingual pretrained language models (MPLMs), especially the generative model BLOOMZ, to successfully boost performance on Bangla tasks. Our extensive evaluation highlights that the cross-lingual retrieval augmented prompts bring steady improvements to MPLMs over the zero-shot performance.</abstract>
      <url hash="10ed446d">2023.banglalp-1.15</url>
      <bibkey>li-etal-2023-crosslingual</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.15</doi>
      <video href="2023.banglalp-1.15.mp4"/>
    </paper>
    <paper id="16">
      <title>Pseudo-Labeling for Domain-Agnostic <fixed-case>B</fixed-case>angla Automatic Speech Recognition</title>
      <author><first>Rabindra Nath</first><last>Nandi</last><affiliation>Hishab Singapure Pte. Ltd</affiliation></author>
      <author><first>Mehadi</first><last>Menon</last></author>
      <author><first>Tareq</first><last>Muntasir</last></author>
      <author><first>Sagor</first><last>Sarker</last></author>
      <author><first>Quazi Sarwar</first><last>Muhtaseem</last></author>
      <author><first>Md. Tariqul</first><last>Islam</last></author>
      <author><first>Shammur</first><last>Chowdhury</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>152-162</pages>
      <abstract>One of the major challenges for developing automatic speech recognition (ASR) for low-resource languages is the limited access to labeled data with domain-specific variations. In this study, we propose a pseudo-labeling approach to develop a large-scale domain-agnostic ASR dataset. With the proposed methodology, we developed a 20k+ hours labeled Bangla speech dataset covering diverse topics, speaking styles, dialects, noisy environments, and conversational scenarios. We then exploited the developed corpus to design a conformer-based ASR system. We benchmarked the trained ASR with publicly available datasets and compared it with other available models. To investigate the efficacy, we designed and developed a human-annotated domain-agnostic test set composed of news, telephony, and conversational data among others. Our results demonstrate the efficacy of the model trained on psuedo-label data for the designed test-set along with publicly-available Bangla datasets. The experimental resources will be publicly available.https://github.com/hishab-nlp/Pseudo-Labeling-for-Domain-Agnostic-Bangla-ASR</abstract>
      <url hash="96339230">2023.banglalp-1.16</url>
      <bibkey>nandi-etal-2023-pseudo</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>B</fixed-case>angla<fixed-case>NLP</fixed-case> at <fixed-case>BLP</fixed-case>-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in <fixed-case>B</fixed-case>angla</title>
      <author><first>Saumajit</first><last>Saha</last></author>
      <author><first>Albert</first><last>Nanda</last></author>
      <pages>163-167</pages>
      <abstract>This paper presents the system that we have developed while solving this shared task on violence inciting text detection in Bangla. We explain both the traditional and the recent approaches that we have used to make our models learn. Our proposed system helps to classify if the given text contains any threat. We studied the impact of data augmentation when there is a limited dataset available. Our quantitative results show that finetuning a multilingual-e5-base model performed the best in our task compared to other transformer-based architectures. We obtained a macro F1 of 68.11% in the test set and our performance in this shared task is ranked at 23 in the leaderboard.</abstract>
      <url hash="4ee72fbe">2023.banglalp-1.17</url>
      <bibkey>saha-nanda-2023-banglanlp</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.17</doi>
    </paper>
    <paper id="18">
      <title>Team <fixed-case>C</fixed-case>entre<fixed-case>B</fixed-case>ack at <fixed-case>BLP</fixed-case>-2023 Task 1: Analyzing performance of different machine-learning based methods for detecting violence-inciting texts in <fixed-case>B</fixed-case>angla</title>
      <author><first>Refaat Mohammad</first><last>Alamgir</last></author>
      <author><first>Amira</first><last>Haque</last></author>
      <pages>168-173</pages>
      <abstract>Like all other things in the world, rapid growth of social media comes with its own merits and demerits. While it is providing a platform for the world to easily communicate with each other, on the other hand the room it has opened for hate speech has led to a significant impact on the well-being of the users. These types of texts have the potential to result in violence as people with similar sentiments may be inspired to commit violent acts after coming across such comments. Hence, the need for a system to detect and filter such texts is increasing drastically with time. This paper summarizes our experimental results and findings for the shared task on The First Bangla Language Processing Workshop at EMNLP 2023 - Singapore. We participated in the shared task 1 : Violence Inciting Text Detection (VITD). The objective was to build a system that classifies the given comments as either non-violence, passive violence or direct violence. We tried out different techniques, such as fine-tuning language models, few-shot learning with SBERT and a 2 stage training where we performed binary violence/non-violence classification first, then did a fine-grained classification of direct/passive violence. We found that the best macro-F1 score of 69.39 was yielded by fine-tuning the BanglaBERT language model and we attained a position of 21 among 27 teams in the final leaderboard. After the competition ended, we found that with some preprocessing of the dataset, we can get the score up to 71.68.</abstract>
      <url hash="1bf70641">2023.banglalp-1.18</url>
      <bibkey>alamgir-haque-2023-team</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>E</fixed-case>mpty<fixed-case>M</fixed-case>ind at <fixed-case>BLP</fixed-case>-2023 Task 1: A Transformer-based Hierarchical-<fixed-case>BERT</fixed-case> Model for <fixed-case>B</fixed-case>angla Violence-Inciting Text Detection</title>
      <author><first>Udoy</first><last>Das</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Karnis</first><last>Fatema</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Md Ayon</first><last>Mia</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Mahshar</first><last>Yahan</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Md Sajidul</first><last>Mowla</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Md Fayez</first><last>Ullah</last></author>
      <author><first>Arpita</first><last>Sarker</last></author>
      <author><first>Hasan</first><last>Murad</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <pages>174-178</pages>
      <abstract>The availability of the internet has made it easier for people to share information via social media. People with ill intent can use this widespread availability of the internet to share violent content easily. A significant portion of social media users prefer using their regional language which makes it quite difficult to detect violence-inciting text. The objective of our research work is to detect Bangla violence-inciting text from social media content. A shared task on Bangla violence-inciting text detection has been organized by the First Bangla Language Processing Workshop (BLP) co-located with EMNLP, where the organizer has provided a dataset named VITD with three categories: nonviolence, passive violence, and direct violence text. To accomplish this task, we have implemented three machine learning models (RF, SVM, XGBoost), two deep learning models (LSTM, BiLSTM), and two transformer-based models (BanglaBERT, Hierarchical-BERT). We have conducted a comparative study among different models by training and evaluating each model on the VITD dataset. We have found that Hierarchical-BERT has provided the best result with an F1 score of 0.73797 on the test set and ranked 9th position among all participants in the shared task 1 of the BLP Workshop co-located with EMNLP 2023.</abstract>
      <url hash="2f87b3c0">2023.banglalp-1.19</url>
      <bibkey>das-etal-2023-emptymind</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.19</doi>
      <video href="2023.banglalp-1.19.mp4"/>
    </paper>
    <paper id="20">
      <title>nlp<fixed-case>BD</fixed-case>patriots at <fixed-case>BLP</fixed-case>-2023 Task 1: Two-Step Classification for Violence Inciting Text Detection in <fixed-case>B</fixed-case>angla - Leveraging Back-Translation and Multilinguality</title>
      <author><first>Md Nishat</first><last>Raihan</last></author>
      <author><first>Dhiman</first><last>Goswami</last><affiliation>George Mason University</affiliation></author>
      <author><first>Sadiya Sayara Chowdhury</first><last>Puspo</last><affiliation>George Mason University</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <pages>179-184</pages>
      <abstract>In this paper, we discuss the nlpBDpatriots entry to the shared task on Violence Inciting Text Detection (VITD) organized as part of the first workshop on Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task is to identify and classify the violent threats, that provoke further unlawful violent acts. Our best-performing approach for the task is two-step classification using back translation and multilinguality which ranked <tex-math>6^{th}</tex-math> out of 27 teams with a macro F1 score of 0.74.</abstract>
      <url hash="3c20a606">2023.banglalp-1.20</url>
      <bibkey>raihan-etal-2023-nlpbdpatriots</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.20</doi>
      <video href="2023.banglalp-1.20.mp4"/>
    </paper>
    <paper id="21">
      <title><fixed-case>S</fixed-case>core_<fixed-case>I</fixed-case>s<fixed-case>A</fixed-case>ll_<fixed-case>Y</fixed-case>ou_<fixed-case>N</fixed-case>eed at <fixed-case>BLP</fixed-case>-2023 Task 1: A Hierarchical Classification Approach to Detect Violence Inciting Text using Transformers</title>
      <author><first>Kawsar</first><last>Ahmed</last></author>
      <author><first>Md</first><last>Osama</last></author>
      <author><first>Md. Sirajul</first><last>Islam</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Md Taosiful</first><last>Islam</last></author>
      <author><first>Avishek</first><last>Das</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <pages>185-189</pages>
      <abstract>Violence-inciting text detection has become critical due to its significance in social media monitoring, online security, and the prevention of violent content. Developing an automatic text classification model for identifying violence in languages with limited resources, like Bangla, poses significant challenges due to the scarcity of resources and complex morphological structures. This work presents a transformer-based method that can classify Bangla texts into three violence classes: direct, passive, and non-violence. We leveraged transformer models, including BanglaBERT, XLM-R, and m-BERT, to develop a hierarchical classification model for the downstream task. In the first step, the BanglaBERT is employed to identify the presence of violence in the text. In the next step, the model classifies stem texts that incite violence as either direct or passive. The developed system scored 72.37 and ranked 14th among the participants.</abstract>
      <url hash="18f70aba">2023.banglalp-1.21</url>
      <bibkey>ahmed-etal-2023-score</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.21</doi>
      <video href="2023.banglalp-1.21.mp4"/>
    </paper>
    <paper id="22">
      <title>Mavericks at <fixed-case>BLP</fixed-case>-2023 Task 1: Ensemble-based Approach Using Language Models for Violence Inciting Text Detection</title>
      <author><first>Saurabh</first><last>Page</last></author>
      <author><first>Sudeep</first><last>Mangalvedhekar</last></author>
      <author><first>Kshitij</first><last>Deshpande</last></author>
      <author><first>Tanmay</first><last>Chavan</last><affiliation>SCTR’s Pune Institute of Computer Technology</affiliation></author>
      <author><first>Sheetal</first><last>Sonawane</last></author>
      <pages>190-195</pages>
      <abstract>This paper presents our work for the Violence Inciting Text Detection shared task in the First Workshop on Bangla Language Processing. Social media has accelerated the propagation of hate and violence-inciting speech in society. It is essential to develop efficient mechanisms to detect and curb the propagation of such texts. The problem of detecting violence-inciting texts is further exacerbated in low-resource settings due to sparse research and less data. The data provided in the shared task consists of texts in the Bangla language, where each example is classified into one of the three categories defined based on the types of violence-inciting texts. We try and evaluate several BERT-based models, and then use an ensemble of the models as our final submission. Our submission is ranked 10th in the final leaderboard of the shared task with a macro F1 score of 0.737.</abstract>
      <url hash="a220935b">2023.banglalp-1.22</url>
      <bibkey>page-etal-2023-mavericks</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.22</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>V</fixed-case>ac<fixed-case>LM</fixed-case> at <fixed-case>BLP</fixed-case>-2023 Task 1: Leveraging <fixed-case>BERT</fixed-case> models for Violence detection in <fixed-case>B</fixed-case>angla</title>
      <author><first>Shilpa</first><last>Chatterjee</last></author>
      <author><first>P J Leo</first><last>Evenss</last></author>
      <author><first>Pramit</first><last>Bhattacharyya</last></author>
      <pages>196-200</pages>
      <abstract>This study introduces the system submitted to the BLP Shared Task 1: Violence Inciting Text Detection (VITD) by the VacLM team. In this work, we analyzed the impact of various transformer-based models for detecting violence in texts. BanglaBERT outperforms all the other competing models. We also observed that the transformer-based models are not adept at classifying Passive Violence and Direct Violence class but can better detect violence in texts, which was the task’s primary objective. On the shared task, we secured a rank of 12 with macro F1-score of 72.656%.</abstract>
      <url hash="f2d779f3">2023.banglalp-1.23</url>
      <bibkey>chatterjee-etal-2023-vaclm</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Aambela at <fixed-case>BLP</fixed-case>-2023 Task 1: Focus on <fixed-case>UNK</fixed-case> tokens: Analyzing Violence Inciting <fixed-case>B</fixed-case>angla Text with Adding Dataset Specific New Word Tokens</title>
      <author><first>Md</first><last>Fahim</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <pages>201-207</pages>
      <abstract>The BLP-2023 Task 1 aims to develop a Natural Language Inference system tailored for detecting and analyzing threats from Bangla YouTube comments. Bangla language models like BanglaBERT have demonstrated remarkable performance in various Bangla natural language processing tasks across different domains. We utilized BanglaBERT for the violence detection task, employing three different classification heads. As BanglaBERT’s vocabulary lacks certain crucial words, our model incorporates some of them as new special tokens, based on their frequency in the dataset, and their embeddings are learned during training. The model achieved the 2nd position on the leaderboard, boasting an impressive macro-F1 Score of 76.04% on the official test set. With the addition of new tokens, we achieved a 76.90% macro-F1 score, surpassing the top score (76.044%) on the test set.</abstract>
      <url hash="f11ea285">2023.banglalp-1.24</url>
      <bibkey>fahim-2023-aambela</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>SUST</fixed-case>_<fixed-case>B</fixed-case>lack Box at <fixed-case>BLP</fixed-case>-2023 Task 1: Detecting Communal Violence in Texts: An Exploration of <fixed-case>MLM</fixed-case> and Weighted Ensemble Techniques</title>
      <author><first>Hrithik</first><last>Shibu</last></author>
      <author><first>Shrestha</first><last>Datta</last></author>
      <author><first>Zhalok</first><last>Rahman</last></author>
      <author><first>Shahrab</first><last>Sami</last></author>
      <author><first>Md. Sumon</first><last>Miah</last></author>
      <author><first>Raisa</first><last>Fairooz</last></author>
      <author><first>Md</first><last>Mollah</last></author>
      <pages>208-213</pages>
      <abstract>In this study, we address the shared task of classifying violence-inciting texts from YouTube comments related to violent incidents in the Bengal region. We seamlessly integrated domain adaptation techniques by meticulously fine-tuning pre-existing Masked Language Models on a diverse array of informal texts. We employed a multifaceted approach, leveraging Transfer Learning, Stacking, and Ensemble techniques to enhance our model’s performance. Our integrated system, amalgamating the refined BanglaBERT model through MLM and our Weighted Ensemble approach, showcased superior efficacy, achieving macro F1 scores of 71% and 72%, respectively, while the MLM approach secured the 18th position among participants. This underscores the robustness and precision of our proposed paradigm in the nuanced detection and categorization of violent narratives within digital realms.</abstract>
      <url hash="19d8f16f">2023.banglalp-1.25</url>
      <bibkey>shibu-etal-2023-sust</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.25</doi>
    </paper>
    <paper id="26">
      <title>the_linguists at <fixed-case>BLP</fixed-case>-2023 Task 1: A Novel Informal <fixed-case>B</fixed-case>angla <fixed-case>F</fixed-case>asttext Embedding for Violence Inciting Text Detection</title>
      <author><first>Md.</first><last>Tariquzzaman</last></author>
      <author><first>Md Wasif</first><last>Kader</last></author>
      <author><first>Audwit</first><last>Anam</last></author>
      <author><first>Naimul</first><last>Haque</last></author>
      <author><first>Mohsinul</first><last>Kabir</last></author>
      <author><first>Hasan</first><last>Mahmud</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Md Kamrul</first><last>Hasan</last></author>
      <pages>214-219</pages>
      <abstract>This paper introduces a novel informal Bangla word embedding for designing a cost-efficient solution for the task “Violence Inciting Text Detection” which focuses on developing classification systems to categorize violence that can potentially incite further violent actions. We propose a semi-supervised learning approach by training an informal Bangla FastText embedding, which is further fine-tuned on lightweight models on task specific dataset and yielded competitive results to our initial method using BanglaBERT, which secured the 7th position with an f1-score of 73.98%. We conduct extensive experiments to assess the efficiency of the proposed embedding and how well it generalizes in terms of violence classification, along with it’s coverage on the task’s dataset. Our proposed Bangla IFT embedding achieved a competitive macro average F1 score of 70.45%. Additionally, we provide a detailed analysis of our findings, delving into potential causes of misclassification in the detection of violence-inciting text.</abstract>
      <url hash="cabbc5d5">2023.banglalp-1.26</url>
      <bibkey>tariquzzaman-etal-2023-linguists</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.26</doi>
      <video href="2023.banglalp-1.26.mp4"/>
    </paper>
    <paper id="27">
      <title><fixed-case>UFAL</fixed-case>-<fixed-case>ULD</fixed-case> at <fixed-case>BLP</fixed-case>-2023 Task 1: Violence Detection in <fixed-case>B</fixed-case>angla Text</title>
      <author><first>Sourabrata</first><last>Mukherjee</last></author>
      <author><first>Atul Kr.</first><last>Ojha</last><affiliation>University of Galway, Ireland, Insight SFI Research Centre for Data Analytics, DSI, University of Galway, Ireland and Panlingua Languague Processing LLP, India</affiliation></author>
      <author><first>Ondřej</first><last>Dušek</last><affiliation>Charles University, Prague</affiliation></author>
      <pages>220-224</pages>
      <abstract>In this paper, we present UFAL-ULD team’s system, desinged as a part of the BLP Shared Task 1: Violence Inciting Text Detection (VITD). This task aims to classify text, with a particular challenge of identifying incitement to violence into Direct, Indirect or Non-violence levels. We experimented with several pre-trained sequence classification models, including XLM-RoBERTa, BanglaBERT, Bangla BERT Base, and Multilingual BERT. Our best-performing model was based on the XLM-RoBERTa-base architecture, which outperformed the baseline models. Our system was ranked 20th among the 27 teams that participated in the task.</abstract>
      <url hash="ef906da5">2023.banglalp-1.27</url>
      <bibkey>mukherjee-etal-2023-ufal</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.27</doi>
    </paper>
    <paper id="28">
      <title>Semantics Squad at <fixed-case>BLP</fixed-case>-2023 Task 1: Violence Inciting <fixed-case>B</fixed-case>angla Text Detection with Fine-Tuned Transformer-Based Models</title>
      <author><first>Krishno</first><last>Dey</last></author>
      <author><first>Prerona</first><last>Tarannum</last></author>
      <author><first>Md. Arid</first><last>Hasan</last><affiliation>University of New Brunswick</affiliation></author>
      <author><first>Francis</first><last>Palma</last><affiliation>University of New Brunswick</affiliation></author>
      <pages>225-229</pages>
      <abstract>This study investigates the application of Transformer-based models for violence threat identification. We participated in the BLP-2023 Shared Task 1 and in our initial submission, BanglaBERT large achieved 5th position on the leader-board with a macro F1 score of 0.7441, approaching the highest baseline of 0.7879 established for this task. In contrast, the top-performing system on the leaderboard achieved an F1 score of 0.7604. Subsequent experiments involving m-BERT, XLM-RoBERTa base, XLM-RoBERTa large, BanglishBERT, BanglaBERT, and BanglaBERT large models revealed that BanglaBERT achieved an F1 score of 0.7441, which closely approximated the baseline. Remarkably, m-BERT and XLM-RoBERTa base also approximated the baseline with macro F1 scores of 0.6584 and 0.6968, respectively. A notable finding from our study is the under-performance by larger models for the shared task dataset, which requires further investigation. Our findings underscore the potential of transformer-based models in identifying violence threats, offering valuable insights to enhance safety measures on online platforms.</abstract>
      <url hash="7dc6432a">2023.banglalp-1.28</url>
      <bibkey>dey-etal-2023-semantics</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.28</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>L</fixed-case>ow<fixed-case>R</fixed-case>esource<fixed-case>NLU</fixed-case> at <fixed-case>BLP</fixed-case>-2023 Task 1 &amp; 2: Enhancing Sentiment Classification and Violence Incitement Detection in <fixed-case>B</fixed-case>angla Through Aggregated Language Models</title>
      <author><first>Hariram</first><last>Veeramani</last></author>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Usman</first><last>Naseem</last><affiliation>James Cook University of North Queensland</affiliation></author>
      <pages>230-235</pages>
      <abstract>Violence incitement detection and sentiment analysis hold significant importance in the field of natural language processing. However, in the case of the Bangla language, there are unique challenges due to its low-resource nature. In this paper, we address these challenges by presenting an innovative approach that leverages aggregated BERT models for two tasks at the BLP workshop in EMNLP 2023, specifically tailored for Bangla. Task 1 focuses on violence-inciting text detection, while task 2 centers on sentiment analysis. Our approach combines fine-tuning with textual entailment (utilizing BanglaBERT), Masked Language Model (MLM) training (making use of BanglaBERT), and the use of standalone Multilingual BERT. This comprehensive framework significantly enhances the accuracy of sentiment classification and violence incitement detection in Bangla text. Our method achieved the 11th rank in task 1 with an F1-score of 73.47 and the 4th rank in task 2 with an F1-score of 71.73. This paper provides a detailed system description along with an analysis of the impact of each component of our framework.</abstract>
      <url hash="85d645fb">2023.banglalp-1.29</url>
      <bibkey>veeramani-etal-2023-lowresourcenlu</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.29</doi>
    </paper>
    <paper id="30">
      <title>Team Error Point at <fixed-case>BLP</fixed-case>-2023 Task 1: A Comprehensive Approach for Violence Inciting Text Detection using Deep Learning and Traditional Machine Learning Algorithm</title>
      <author><first>Rajesh</first><last>Das</last></author>
      <author><first>Jannatul</first><last>Maowa</last></author>
      <author><first>Moshfiqur</first><last>Ajmain</last></author>
      <author><first>Kabid</first><last>Yeiad</last></author>
      <author><first>Mirajul</first><last>Islam</last></author>
      <author><first>Sharun</first><last>Khushbu</last></author>
      <pages>236-240</pages>
      <abstract>In the modern digital landscape, social media platforms have the dual role of fostering unprecedented connectivity and harboring a dark underbelly in the form of widespread violence-inciting content. Pioneering research in Bengali social media aims to provide a groundbreaking solution to this issue. This study thoroughly investigates violence-inciting text classification using a diverse range of machine learning and deep learning models, offering insights into content moderation and strategies for enhancing online safety. Situated at the intersection of technology and social responsibility, the aim is to empower platforms and communities to combat online violence. By providing insights into model selection and methodology, this work makes a significant contribution to the ongoing dialogue about the challenges posed by the darker aspects of the digital era. Our system scored 31.913 and ranked 26 among the participants.</abstract>
      <url hash="ade8ece3">2023.banglalp-1.30</url>
      <bibkey>das-etal-2023-team</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.30</doi>
    </paper>
    <paper id="31">
      <title><fixed-case>NLP</fixed-case>_<fixed-case>CUET</fixed-case> at <fixed-case>BLP</fixed-case>-2023 Task 1: Fine-grained Categorization of Violence Inciting Text using Transformer-based Approach</title>
      <author><first>Jawad</first><last>Hossain</last></author>
      <author><first>Hasan Mesbaul</first><last>Ali Taher</last></author>
      <author><first>Avishek</first><last>Das</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <pages>241-246</pages>
      <abstract>The amount of online textual content has increased significantly in recent years through social media posts, online chatting, web portals, and other digital platforms due to the significant increase in internet users and their unprompted access via digital devices. Unfortunately, the misappropriation of textual communication via the Internet has led to violence-inciting texts. Despite the availability of various forms of violence-inciting materials, text-based content is often used to carry out violent acts. Thus, developing a system to detect violence-inciting text has become vital. However, creating such a system in a low-resourced language like Bangla becomes challenging. Therefore, a shared task has been arranged to detect violence-inciting text in Bangla. This paper presents a hybrid approach (GAN+Bangla-ELECTRA) to classify violence-inciting text in Bangla into three classes: <i>direct</i>, <i>passive</i>, and <i>non-violence</i>. We investigated a variety of deep learning (CNN, BiLSTM, BiLSTM+Attention), machine learning (LR, DT, MNB, SVM, RF, SGD), transformers (BERT, ELECTRA), and GAN-based models to detect violence inciting text in Bangla. Evaluation results demonstrate that the GAN+Bangla-ELECTRA model gained the highest macro <tex-math>f_1</tex-math>-score (74.59), which obtained us a rank of 3rd position at the BLP-2023 Task 1.</abstract>
      <url hash="a266dc72">2023.banglalp-1.31</url>
      <bibkey>hossain-etal-2023-nlp</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.31</doi>
      <video href="2023.banglalp-1.31.mp4"/>
    </paper>
    <paper id="32">
      <title><fixed-case>T</fixed-case>eam_<fixed-case>S</fixed-case>yrax at <fixed-case>BLP</fixed-case>-2023 Task 1: Data Augmentation and Ensemble Based Approach for Violence Inciting Text Detection in <fixed-case>B</fixed-case>angla</title>
      <author><first>Omar</first><last>Riyad</last></author>
      <author><first>Trina</first><last>Chakraborty</last><affiliation>Shahjalal University of Science and Technology</affiliation></author>
      <author><first>Abhishek</first><last>Dey</last></author>
      <pages>247-254</pages>
      <abstract>This paper describes our participation in Task1 (VITD) of BLP Workshop 1 at EMNLP 2023,focused on the detection and categorizationof threats linked to violence, which could po-tentially encourage more violent actions. Ourapproach involves fine-tuning of pre-trainedtransformer models and employing techniqueslike self-training with external data, data aug-mentation through back-translation, and en-semble learning (bagging and majority voting).Notably, self-training improves performancewhen applied to data from external source butnot when applied to the test-set. Our anal-ysis highlights the effectiveness of ensemblemethods and data augmentation techniques inBangla Text Classification. Our system ini-tially scored 0.70450 and ranked 19th amongthe participants but post-competition experi-ments boosted our score to 0.72740.</abstract>
      <url hash="dd79cc41">2023.banglalp-1.32</url>
      <bibkey>riyad-etal-2023-team</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.32</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>BLP</fixed-case>-2023 Task 1: Violence Inciting Text Detection (<fixed-case>VITD</fixed-case>)</title>
      <author><first>Sourav</first><last>Saha</last></author>
      <author><first>Jahedul Alam</first><last>Junaed</last></author>
      <author><first>Maryam</first><last>Saleki</last></author>
      <author><first>Mohamed</first><last>Rahouti</last><affiliation>Fordham University</affiliation></author>
      <author><first>Nabeel</first><last>Mohammed</last></author>
      <author><first>Mohammad Ruhul</first><last>Amin</last><affiliation>Fordham University</affiliation></author>
      <pages>255-265</pages>
      <abstract>We present the comprehensive technical description of the outcome of the BLP shared task on Violence Inciting Text Detection (VITD).In recent years, social media has become a tool for groups of various religions and backgrounds to spread hatred, leading to physicalviolence with devastating consequences. To address this challenge, the VITD shared task was initiated, aiming to classify the level of violence incitement in various texts. The competition garnered significant interest with a total of 27 teams consisting of 88 participants successfully submitting their systems to the CodaLab leaderboard. During the post-workshop phase, we received 16 system papers on VITD from those participants. In this paper, we intend to discuss the VITD baseline performance, error analysis of the submitted models, and provide a comprehensive summary of the computational techniques applied by the participating teams</abstract>
      <url hash="ca54a0b0">2023.banglalp-1.33</url>
      <bibkey>saha-etal-2023-blp</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.33</doi>
    </paper>
    <paper id="34">
      <title><fixed-case>B</fixed-case>angla<fixed-case>NLP</fixed-case> at <fixed-case>BLP</fixed-case>-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of <fixed-case>B</fixed-case>angla Social Media Posts</title>
      <author><first>Saumajit</first><last>Saha</last></author>
      <author><first>Albert</first><last>Nanda</last></author>
      <pages>266-272</pages>
      <abstract>Bangla is the 7th most widely spoken language globally, with a staggering 234 million native speakers primarily hailing from India and Bangladesh. This morphologically rich language boasts a rich literary tradition, encompassing diverse dialects and language-specific challenges. Despite its linguistic richness and history, Bangla remains categorized as a low-resource language within the natural language processing (NLP) and speech community. This paper presents our submission to Task 2 (Sentiment Analysis of Bangla Social Media Posts) of the BLP Workshop. We experimented with various Transformer-based architectures to solve this task. Our quantitative results show that transfer learning really helps in better learning of the models in this low-resource language scenario. This becomes evident when we further finetuned a model that had already been finetuned on Twitter data for sentiment analysis task and that finetuned model performed the best among all other models. We also performed a detailed error analysis where we found some instances where ground truth labels need to be looked at. We obtained a micro-F1 of 67.02% on the test set and our performance in this shared task is ranked at 21 in the leaderboard.</abstract>
      <url hash="616c485e">2023.banglalp-1.34</url>
      <bibkey>saha-nanda-2023-banglanlp-blp</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.34</doi>
    </paper>
    <paper id="35">
      <title>Knowdee at <fixed-case>BLP</fixed-case>-2023 Task 2: Improving <fixed-case>B</fixed-case>angla Sentiment Analysis Using Ensembled Models with Pseudo-Labeling</title>
      <author><first>Xiaoyi</first><last>Liu</last><affiliation>Knowdee AI</affiliation></author>
      <author><first>Mao</first><last>Teng</last></author>
      <author><first>SHuangtao</first><last>Yang</last></author>
      <author><first>Bo</first><last>Fu</last></author>
      <pages>273-278</pages>
      <abstract>This paper outlines our submission to the Sentiment Analysis Shared Task at the Bangla Language Processing (BLP) Workshop at EMNLP2023 (Hasan et al., 2023a). The objective of this task is to detect sentiment in each text by classifying it as Positive, Negative, or Neutral. This shared task is based on the MUltiplatform BAngla SEntiment (MUBASE) (Hasan et al., 2023b) and SentNob (Islam et al., 2021) dataset, which consists of public comments from various social media platforms. Our proposed method for this task is based on the pre-trained Bangla language model BanglaBERT (Bhattacharjee et al., 2022). We trained an ensemble of BanglaBERT on the original dataset and used it to generate pseudo-labels for data augmentation. This expanded dataset was then used to train our final models. During the evaluation phase, 30 teams submitted their systems, and our system achieved the second highest performance with F1 score of 0.7267. The source code of the proposed approach is available at https://github.com/KnowdeeAI/blp_task2_knowdee.git.</abstract>
      <url hash="af8b1237">2023.banglalp-1.35</url>
      <bibkey>liu-etal-2023-knowdee</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.35</doi>
    </paper>
    <paper id="36">
      <title>M1437 at <fixed-case>BLP</fixed-case>-2023 Task 2: Harnessing <fixed-case>B</fixed-case>angla Text for Sentiment Analysis: A Transformer-based Approach</title>
      <author><first>Majidur</first><last>Rahman</last><affiliation>George Mason University</affiliation></author>
      <author><first>Ozlem</first><last>Uzuner</last><affiliation>George Mason University</affiliation></author>
      <pages>279-285</pages>
      <abstract>Analyzing public sentiment on social media is helpful in understanding the public’s emotions about any given topic. While numerous studies have been conducted in this field, there has been limited research on Bangla social media data. Team M1437 from George Mason University participated in the Sentiment Analysis shared task of the Bangla Language Processing (BLP) Workshop at EMNLP-2023. The team fine-tuned various BERT-based Transformer architectures to solve the task. This article shows that <tex-math>BanglaBERT_{large}</tex-math>, a language model pre-trained on Bangla text, outperformed other BERT-based models. This model achieved an F1 score of 73.15% and top position in the development phase, was further tuned with external training data, and achieved an F1 score of 70.36% in the evaluation phase, securing the fourteenth place on the leaderboard. The F1 score on the test set, when <tex-math>BanglaBERT_{large}</tex-math> was trained without external training data, was 71.54%.</abstract>
      <url hash="a16b82ca">2023.banglalp-1.36</url>
      <bibkey>rahman-uzuner-2023-m1437</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.36</doi>
      <video href="2023.banglalp-1.36.mp4"/>
    </paper>
    <paper id="37">
      <title>nlp<fixed-case>BD</fixed-case>patriots at <fixed-case>BLP</fixed-case>-2023 Task 2: A Transfer Learning Approach towards <fixed-case>B</fixed-case>angla Sentiment Analysis</title>
      <author><first>Dhiman</first><last>Goswami</last><affiliation>George Mason University</affiliation></author>
      <author><first>Md Nishat</first><last>Raihan</last></author>
      <author><first>Sadiya Sayara Chowdhury</first><last>Puspo</last><affiliation>George Mason University</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <pages>286-292</pages>
      <abstract>In this paper, we discuss the entry of nlpBDpatriots to some sophisticated approaches for classifying Bangla Sentiment Analysis. This is a shared task of the first workshop on Bangla Language Processing (BLP) organized under EMNLP. The main objective of this task is to identify the sentiment polarity of social media content. There are 30 groups of NLP enthusiasts who participate in this shared task and our best-performing approach for the task is transfer learning with data augmentation. Our group ranked <tex-math>12^{th}</tex-math> position in this competition with this methodology securing a micro F1 score of 0.71.</abstract>
      <url hash="23bc05b0">2023.banglalp-1.37</url>
      <bibkey>goswami-etal-2023-nlpbdpatriots</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.37</doi>
      <video href="2023.banglalp-1.37.mp4"/>
    </paper>
    <paper id="38">
      <title>Ushoshi2023 at <fixed-case>BLP</fixed-case>-2023 Task 2: A Comparison of Traditional to Advanced Linguistic Models to Analyze Sentiment in <fixed-case>B</fixed-case>angla Texts</title>
      <author><first>Sharun</first><last>Khushbu</last></author>
      <author><first>Nasheen</first><last>Nur</last><affiliation>Florida Institute of Technology</affiliation></author>
      <author><first>Mohiuddin</first><last>Ahmed</last></author>
      <author><first>Nashtarin</first><last>Nur</last></author>
      <pages>293-299</pages>
      <abstract>This article describes our analytical approach designed for BLP Workshop-2023 Task-2: in Sentiment Analysis. During actual task submission, we used DistilBERT. However, we later applied rigorous hyperparameter tuning and pre-processing, improving the result to 68% accuracy and a 68% F1 micro score with vanilla LSTM. Traditional machine learning models were applied to compare the result where 75% accuracy was achieved with traditional SVM. Our contributions are a) data augmentation using the oversampling method to remove data imbalance and b) attention masking for data encoding with masked language modeling to capture representations of language semantics effectively, by further demonstrating it with explainable AI. Originally, our system scored 0.26 micro-F1 in the competition and ranked 30th among the participants for a basic DistilBERT model, which we later improved to 0.68 and 0.65 with LSTM and XLM-RoBERTa-base models, respectively.</abstract>
      <url hash="25f4fb92">2023.banglalp-1.38</url>
      <bibkey>khushbu-etal-2023-ushoshi2023</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.38</doi>
    </paper>
    <paper id="39">
      <title><fixed-case>E</fixed-case>mpty<fixed-case>M</fixed-case>ind at <fixed-case>BLP</fixed-case>-2023 Task 2: Sentiment Analysis of <fixed-case>B</fixed-case>angla Social Media Posts using Transformer-Based Models</title>
      <author><first>Karnis</first><last>Fatema</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Udoy</first><last>Das</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Md Ayon</first><last>Mia</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Md Sajidul</first><last>Mowla</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Mahshar</first><last>Yahan</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Md Fayez</first><last>Ullah</last></author>
      <author><first>Arpita</first><last>Sarker</last></author>
      <author><first>Hasan</first><last>Murad</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <pages>300-304</pages>
      <abstract>With the popularity of social media platforms, people are sharing their individual thoughts by posting, commenting, and messaging with their friends, which generates a significant amount of digital text data every day. Conducting sentiment analysis of social media content is a vibrant research domain within the realm of Natural Language Processing (NLP), and it has practical, real-world uses. Numerous prior studies have focused on sentiment analysis for languages that have abundant linguistic resources, such as English. However, limited prior research works have been done for automatic sentiment analysis in low-resource languages like Bangla. In this research work, we are going to finetune different transformer-based models for Bangla sentiment analysis. To train and evaluate the model, we have utilized a dataset provided in a shared task organized by the BLP Workshop co-located with EMNLP-2023. Moreover, we have conducted a comparative study among different machine learning models, deep learning models, and transformer-based models for Bangla sentiment analysis. Our findings show that the BanglaBERT (Large) model has achieved the best result with a micro F1-Score of 0.7109 and secured 7th position in the shared task 2 leaderboard of the BLP Workshop in EMNLP 2023.</abstract>
      <url hash="71541a8f">2023.banglalp-1.39</url>
      <bibkey>fatema-etal-2023-emptymind</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.39</doi>
      <video href="2023.banglalp-1.39.mp4"/>
    </paper>
    <paper id="40">
      <title><fixed-case>RSM</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>BLP</fixed-case>-2023 Task 2: <fixed-case>B</fixed-case>angla Sentiment Analysis using Weighted and Majority Voted Fine-Tuned Transformers</title>
      <author><first>Pratinav</first><last>Seth</last><affiliation>Indian Institute of Technology Kharagpur, Manipal University and Manipal Institute of Technology</affiliation></author>
      <author><first>Rashi</first><last>Goel</last></author>
      <author><first>Komal</first><last>Mathur</last></author>
      <author><first>Swetha</first><last>Vemulapalli</last></author>
      <pages>305-311</pages>
      <abstract>This paper describes our approach to submissions made at Shared Task 2 at BLP Workshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis is an action research area in the digital age. With the rapid and constant growth of online social media sites and services and the increasing amount of textual data, the application of automatic Sentiment Analysis is on the rise. However, most of the research in this domain is based on the English language. Despite being the world’s sixth most widely spoken language, little work has been done in Bangla. This task aims to promote work on Bangla Sentiment Analysis while identifying the polarity of social media content by determining whether the sentiment expressed in the text is Positive, Negative, or Neutral. Our approach consists of experimenting and finetuning various multilingual and pre-trained BERT-based models on our downstream tasks and using a Majority Voting and Weighted ensemble model that outperforms individual baseline model scores. Our system scored 0.711 for the multiclass classification task and scored 10th place among the participants on the leaderboard for the shared task. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .</abstract>
      <url hash="e47996f3">2023.banglalp-1.40</url>
      <bibkey>seth-etal-2023-rsm</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.40</doi>
    </paper>
    <paper id="41">
      <title>Semantics Squad at <fixed-case>BLP</fixed-case>-2023 Task 2: Sentiment Analysis of <fixed-case>B</fixed-case>angla Text with Fine Tuned Transformer Based Models</title>
      <author><first>Krishno</first><last>Dey</last></author>
      <author><first>Md. Arid</first><last>Hasan</last><affiliation>University of New Brunswick</affiliation></author>
      <author><first>Prerona</first><last>Tarannum</last></author>
      <author><first>Francis</first><last>Palma</last><affiliation>University of New Brunswick</affiliation></author>
      <pages>312-316</pages>
      <abstract>Sentiment analysis (SA) is a crucial task in natural language processing, especially in contexts with a variety of linguistic features, like Bangla. We participated in BLP-2023 Shared Task 2 on SA of Bangla text. We investigated the performance of six transformer-based models for SA in Bangla on the shared task dataset. We fine-tuned these models and conducted a comprehensive performance evaluation. We ranked 20th on the leaderboard of the shared task with a blind submission that used BanglaBERT Small. BanglaBERT outperformed other models with 71.33% accuracy, and the closest model was BanglaBERT Large, with an accuracy of 70.90%. BanglaBERT consistently outperformed others, demonstrating the benefits of models developed using sizable datasets in Bangla.</abstract>
      <url hash="14c8db2c">2023.banglalp-1.41</url>
      <bibkey>dey-etal-2023-semantics-squad</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.41</doi>
    </paper>
    <paper id="42">
      <title>Aambela at <fixed-case>BLP</fixed-case>-2023 Task 2: Enhancing <fixed-case>B</fixed-case>angla<fixed-case>BERT</fixed-case> Performance for <fixed-case>B</fixed-case>angla Sentiment Analysis Task with In Task Pretraining and Adversarial Weight Perturbation</title>
      <author><first>Md</first><last>Fahim</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <pages>317-323</pages>
      <abstract>This paper introduces the top-performing approachof “Aambela” for the BLP-2023 Task2: “Sentiment Analysis of Bangla Social MediaPosts”. The objective of the task was tocreate systems capable of automatically detectingsentiment in Bangla text from diverse socialmedia posts. My approach comprised finetuninga Bangla Language Model with threedistinct classification heads. To enhance performance,we employed two robust text classificationtechniques. To arrive at a final prediction,we employed a mode-based ensemble approachof various predictions from different models,which ultimately resulted in the 1st place in thecompetition.</abstract>
      <url hash="27504987">2023.banglalp-1.42</url>
      <bibkey>fahim-2023-aambela-blp</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.42</doi>
    </paper>
    <paper id="43">
      <title><fixed-case>Z</fixed-case>-Index at <fixed-case>BLP</fixed-case>-2023 Task 2: A Comparative Study on Sentiment Analysis</title>
      <author><first>Prerona</first><last>Tarannum</last><affiliation>Daffodil International University</affiliation></author>
      <author><first>Md. Arid</first><last>Hasan</last><affiliation>University of New Brunswick</affiliation></author>
      <author><first>Krishno</first><last>Dey</last><affiliation>University of New Brunswick</affiliation></author>
      <author><first>Sheak Rashed Haider</first><last>Noori</last><affiliation>Daffodil International University</affiliation></author>
      <pages>324-330</pages>
      <abstract>In this study, we report our participation in Task 2 of the BLP-2023 shared task. The main objective of this task is to determine the sentiment (Positive, Neutral, or Negative) of a given text. We first removed the URLs, hashtags, and other noises and then applied traditional and pretrained language models. We submitted multiple systems in the leaderboard and BanglaBERT with tokenized data provided thebest result and we ranked 5th position in the competition with an F1-micro score of 71.64. Our study also reports that the importance of tokenization is lessening in the realm of pretrained language models. In further experiments, our evaluation shows that BanglaBERT outperforms, and predicting the neutral class is still challenging for all the models.</abstract>
      <url hash="a42cc6ce">2023.banglalp-1.43</url>
      <bibkey>tarannum-etal-2023-z</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.43</doi>
    </paper>
    <paper id="44">
      <title>Team Error Point at <fixed-case>BLP</fixed-case>-2023 Task 2: A Comparative Exploration of Hybrid Deep Learning and Machine Learning Approach for Advanced Sentiment Analysis Techniques.</title>
      <author><first>Rajesh</first><last>Das</last></author>
      <author><first>Kabid</first><last>Yeiad</last></author>
      <author><first>Moshfiqur</first><last>Ajmain</last></author>
      <author><first>Jannatul</first><last>Maowa</last></author>
      <author><first>Mirajul</first><last>Islam</last></author>
      <author><first>Sharun</first><last>Khushbu</last></author>
      <pages>331-335</pages>
      <abstract>This paper presents a thorough and extensive investigation into the diverse models and techniques utilized for sentiment analysis. What sets this research apart is the deliberate and purposeful incorporation of data augmentation techniques with the goal of improving the efficacy of sentiment analysis in the Bengali language. We systematically explore various approaches, including preprocessing techniques, advancedmodels like Long Short-Term Memory (LSTM) and LSTM-CNN (Convolutional Neural Network) Combine, and traditional machine learning models such as Logistic Regression, Decision Tree, Random Forest, Multi-Naive Bayes, Support Vector Machine, and Stochastic Gradient Descent. Our study highlights the substantial impact of data augmentation on enhancing model accuracy and understanding Bangla sentiment nuances. Additionally, we emphasize the LSTM model’s ability to capture long-range correlations in Bangla text. Our system scored 0.4129 and ranked 27th among the participants.</abstract>
      <url hash="a19cb423">2023.banglalp-1.44</url>
      <bibkey>das-etal-2023-team-error</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.44</doi>
    </paper>
    <paper id="45">
      <title><fixed-case>UFAL</fixed-case>-<fixed-case>ULD</fixed-case> at <fixed-case>BLP</fixed-case>-2023 Task 2 Sentiment Classification in <fixed-case>B</fixed-case>angla Text</title>
      <author><first>Sourabrata</first><last>Mukherjee</last></author>
      <author><first>Atul Kr.</first><last>Ojha</last><affiliation>University of Galway, Ireland, Insight SFI Research Centre for Data Analytics, DSI, University of Galway, Ireland and Panlingua Languague Processing LLP, India</affiliation></author>
      <author><first>Ondřej</first><last>Dušek</last><affiliation>Charles University, Prague</affiliation></author>
      <pages>336-339</pages>
      <abstract>In this paper, we present the UFAL-ULD team’s system for the BLP Shared Task 2: Sentiment Analysis of Bangla Social Media Posts. The Task 2 involves classifying text into Positive, Negative, or Neutral sentiments. As a part of this task, we conducted a series of experiments with several pre-trained sequence classification models – XLM-RoBERTa, BanglaBERT, Bangla BERT Base and Multilingual BERT. Among these, our best-performing model was based on the XLM-RoBERTa-base architecture, which outperforms baseline models. Our system was ranked 19th among the 30 teams that participated in the task.</abstract>
      <url hash="e867cb4b">2023.banglalp-1.45</url>
      <bibkey>mukherjee-etal-2023-ufal-uld</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.45</doi>
    </paper>
    <paper id="46">
      <title>Embeddings at <fixed-case>BLP</fixed-case>-2023 Task 2: Optimizing Fine-Tuned Transformers with Cost-Sensitive Learning for Multiclass Sentiment Analysis</title>
      <author><first>S.m Towhidul Islam</first><last>Tonmoy</last></author>
      <pages>340-346</pages>
      <abstract>In this study, we address the task of Sentiment Analysis for Bangla Social Media Posts, introduced in first Workshop on Bangla Language Processing (CITATION). Our research encountered two significant challenges in the context of sentiment analysis. The first challenge involved extensive training times and memory constraints when we chose to employ oversampling techniques for addressing class imbalance in an attempt to enhance model performance. Conversely, when opting for undersampling, the training time was optimal, but this approach resulted in poor model performance. These challenges highlight the complex trade-offs involved in selecting sampling methods to address class imbalances in sentiment analysis tasks. We tackle these challenges through cost-sensitive approaches aimed at enhancing model performance. In our initial submission during the evaluation phase, we ranked 9th out of 30 participants with an F1-micro score of 0.7088 . Subsequently, through additional experimentation, we managed to elevate our F1-micro score to 0.7186 by leveraging the BanglaBERT-Large model in combination with the Self-adjusting Dice loss function. Our experiments highlight the effect in performance of the models achieved by modifying the loss function. Our experimental data and source code can be found here.</abstract>
      <url hash="0c6c2b04">2023.banglalp-1.46</url>
      <bibkey>tonmoy-2023-embeddings</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.46</doi>
    </paper>
    <paper id="47">
      <title><fixed-case>L</fixed-case>ow<fixed-case>R</fixed-case>esource at <fixed-case>BLP</fixed-case>-2023 Task 2: Leveraging <fixed-case>B</fixed-case>angla<fixed-case>B</fixed-case>ert for Low Resource Sentiment Analysis of <fixed-case>B</fixed-case>angla Language</title>
      <author><first>Aunabil</first><last>Chakma</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Masum</first><last>Hasan</last></author>
      <pages>347-353</pages>
      <abstract>This paper describes the system of the LowResource Team for Task 2 of BLP-2023, which involves conducting sentiment analysis on a dataset composed of public posts and comments from diverse social media platforms. Our primary aim was to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus, using various strategies including fine-tuning, dropping random tokens, and using several external datasets. Our final model is an ensemble of the three best BanglaBert variations. Our system achieved overall 3rd in the Test Set among 30 participating teams with a score of 0.718. Additionally, we discuss the promising systems that didn’t perform well namely task-adaptive pertaining and paraphrasing using BanglaT5. Our training codes are publicly available at https://github.com/Aunabil4602/bnlp-workshop-task2-2023</abstract>
      <url hash="63f7fb20">2023.banglalp-1.47</url>
      <bibkey>chakma-hasan-2023-lowresource</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.47</doi>
    </paper>
    <paper id="48">
      <title><fixed-case>BLP</fixed-case>-2023 Task 2: Sentiment Analysis</title>
      <author><first>Md. Arid</first><last>Hasan</last><affiliation>University of New Brunswick</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Anika</first><last>Anjum</last></author>
      <author><first>Shudipta</first><last>Das</last></author>
      <author><first>Afiyat</first><last>Anjum</last></author>
      <pages>354-364</pages>
      <abstract>We present an overview of the BLP Sentiment Shared Task, organized as part of the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is defined as the detection of sentiment in a given piece of social media text. This task attracted interest from 71 participants, among whom 29 and 30 teams submitted systems during the development and evaluation phases, respectively. In total, participants submitted 597 runs. However, only 15 teams submitted system description papers. The range of approaches in the submitted systems spans from classical machine learning models, fine-tuning pre-trained models, to leveraging Large Language Model (LLMs) in zero- and few-shot settings. In this paper, we provide a detailed account of the task setup, including dataset development and evaluation setup. Additionally, we provide a succinct overview of the systems submitted by the participants. All datasets and evaluation scripts from the shared task have been made publicly available for the research community, to foster further research in this domain.</abstract>
      <url hash="686d0343">2023.banglalp-1.48</url>
      <bibkey>hasan-etal-2023-blp</bibkey>
      <doi>10.18653/v1/2023.banglalp-1.48</doi>
      <video href="2023.banglalp-1.48.mp4"/>
    </paper>
  </volume>
</collection>
