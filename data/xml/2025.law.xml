<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.law">
  <volume id="1" ingest-date="2025-07-22" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 19th Linguistic Annotation Workshop (LAW-XIX-2025)</booktitle>
      <editor><first>Siyao</first><last>Peng</last></editor>
      <editor><first>Ines</first><last>Rehbein</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>July</month>
      <year>2025</year>
      <url hash="a42f93c8">2025.law-1</url>
      <venue>law</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-262-6</isbn>
      <doi>10.18653/v1/2025.law-1</doi>
    </meta>
    <frontmatter>
      <url hash="947ca88e">2025.law-1.0</url>
      <bibkey>law-ws-2025-1</bibkey>
      <doi>10.18653/v1/2025.law-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title>Understanding Disagreement: An Annotation Study of Sentiment and Emotional Language in Environmental Communication</title>
      <author><first>Christina</first><last>Barz</last><affiliation>Darmstadt University of Applied Sciences</affiliation></author>
      <author><first>Melanie</first><last>Siegel</last><affiliation>Hochschule Darmstadt - University of Applied Sciences</affiliation></author>
      <author><first>Daniel</first><last>Hanss</last><affiliation>Darmstadt University of Applied Sciences</affiliation></author>
      <author><first>Michael</first><last>Wiegand</last><affiliation>University of Vienna</affiliation></author>
      <pages>1-20</pages>
      <abstract>Emotional language is central to how environmental issues are communicated and received by the public. To better understand how such language is interpreted, we conducted an annotation study on sentiment and emotional language in texts from the environmental activist group Extinction Rebellion. The annotation process revealed substantial disagreement among annotators, highlighting the complexity and subjectivity involved in interpreting emotional language. In this paper, we analyze the sources of these disagreements, offering insights into how individual perspectives shape annotation outcomes. Our work contributes to ongoing discussions on perspectivism in NLP and emphasizes the importance of human-centered approaches and citizen science in analyzing environmental communication.</abstract>
      <url hash="bef63014">2025.law-1.1</url>
      <attachment type="dataset" hash="79c9ca94">2025.law-1.1.dataset.zip</attachment>
      <bibkey>barz-etal-2025-understanding</bibkey>
      <doi>10.18653/v1/2025.law-1.1</doi>
    </paper>
    <paper id="2">
      <title>Measuring Label Ambiguity in Subjective Tasks using Predictive Uncertainty Estimation</title>
      <author><first>Richard</first><last>Alies</last><affiliation>Humboldt-Universität zu Berlin</affiliation></author>
      <author><first>Elena</first><last>Merdjanovska</last><affiliation>Humboldt University</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt-Universität zu Berlin</affiliation></author>
      <pages>21-34</pages>
      <abstract>Human annotations in natural language corpora vary due to differing human perspectives. This is especially prevalent in subjective tasks. In these datasets, certain data samples are more prone to label variation and can be indicated as ambiguous samples.</abstract>
      <url hash="82608446">2025.law-1.2</url>
      <bibkey>alies-etal-2025-measuring</bibkey>
      <doi>10.18653/v1/2025.law-1.2</doi>
    </paper>
    <paper id="3">
      <title>Disagreements in analyses of rhetorical text structure: A new dataset and first analyses</title>
      <author><first>Freya</first><last>Hewett</last><affiliation>University of Potsdam</affiliation></author>
      <author><first>Manfred</first><last>Stede</last><affiliation>University of Potsdam</affiliation></author>
      <pages>35-47</pages>
      <abstract>Discourse structure annotation is known to involve a high level of subjectivity, which often results in low inter-annotator agreement. In this paper, we focus on “legitimate disagreements”, by which we refer to multiple valid annotations for a text or text segment. We provide a new dataset of English and German texts, where each text comes with two parallel analyses (both done by well-trained annotators) in the framework of Rhetorical Structure Theory. Using the RST Tace tool, we build a list of all conflicting annotation decisions and present some statistics for the corpus. Thereafter, we undertake a qualitative analysis of the disagreements and propose a typology of underlying reasons. From this we derive the need to differentiate two kinds of ambiguities in RST annotation: those that result from inherent “everyday” linguistic ambiguity, and those that arise from specifications in the theory and/or the annotation schemes.</abstract>
      <url hash="cc4d1d20">2025.law-1.3</url>
      <bibkey>hewett-stede-2025-disagreements</bibkey>
      <doi>10.18653/v1/2025.law-1.3</doi>
    </paper>
    <paper id="4">
      <title>Subjectivity in the Annotation of Bridging Anaphora</title>
      <author><first>Lauren</first><last>Levine</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Amir</first><last>Zeldes</last><affiliation>Georgetown University</affiliation></author>
      <pages>48-59</pages>
      <abstract>Bridging refers to the associative relationship between inferable entities in a discourse and the antecedents which allow us to understand them, such as understanding what “the door” means with respect to an aforementioned “house”. As identifying associative relations between entities is an inherently subjective task, it is difficult to achieve consistent agreement in the annotation of bridging anaphora and their antecedents. In this paper, we explore the subjectivity involved in the annotation of bridging instances at three levels: anaphor recognition, antecedent resolution, and bridging subtype selection. To do this, we conduct an annotation pilot on the test set of the existing GUM corpus, and propose a newly developed classification system for bridging subtypes, which we compare to previously proposed schemes. Our results suggest that some previous resources are likely to be severely under-annotated. We also find that while agreement on the bridging subtype category was moderate, annotator overlap for exhaustively identifying instances of bridging is low, and that many disagreements resulted from subjective understanding of the entities involved.</abstract>
      <url hash="9a46d956">2025.law-1.4</url>
      <attachment type="attachment" hash="f0729330">2025.law-1.4.attachment.pdf</attachment>
      <bibkey>levine-zeldes-2025-subjectivity</bibkey>
      <doi>10.18653/v1/2025.law-1.4</doi>
    </paper>
    <paper id="5">
      <title>The revision of linguistic annotation in the <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies framework: a look at the annotators’ behavior</title>
      <author><first>Magali Sanches</first><last>Duran</last><affiliation>Universidade de São Paulo</affiliation></author>
      <author><first>Lucelene</first><last>Lopes</last><affiliation>USP - ICMC</affiliation></author>
      <author><first>Thiago Alexandre Salgueiro</first><last>Pardo</last><affiliation>University of São Paulo</affiliation></author>
      <pages>60-69</pages>
      <abstract>This paper presents strategies to revise an automatically annotated corpus according to the Universal Dependencies framework and discusses the learned lessons, mainly regarding the annotators’ behavior. The revision strategies are not relying on examples from any specific language and, because they are languageindependent, can be adopted in any language and corpus annotation initiative.</abstract>
      <url hash="7139f93c">2025.law-1.5</url>
      <bibkey>duran-etal-2025-revision</bibkey>
      <doi>10.18653/v1/2025.law-1.5</doi>
    </paper>
    <paper id="6">
      <title>Forbidden <fixed-case>FRUIT</fixed-case> is the Sweetest: An Annotated Tweets Corpus for <fixed-case>F</fixed-case>rench Unfrozen Idioms Identification</title>
      <author><first>Julien</first><last>Bezançon</last><affiliation>Sorbonne Université</affiliation></author>
      <author><first>Gaël</first><last>Lejeune</last><affiliation>STIH, Sorbonne Université</affiliation></author>
      <author><first>Antoine</first><last>Gautier</last><affiliation>Sorbonne Université</affiliation></author>
      <author><first>Marceau</first><last>Hernandez</last><affiliation>Sorbonne Université</affiliation></author>
      <author><first>Félix</first><last>Alié</last><affiliation>Sorbonne Université</affiliation></author>
      <pages>70-86</pages>
      <abstract>Multiword expressions (MWEs) are a key area of interest in NLP, studied across various languages and inspiring the creation of dedicated datasets and shared tasks such as PARSEME. Puns in multiword expressions (PMWEs) can be described as MWEs that have been “unfrozen” to acquire a new meaning or create a wordplay. Unlike MWEs, they have received little attention in NLP, mainly due to the lack of resources available for their study. In this context, we introduce the French Unfrozen Idioms in Tweets (FRUIT) corpus, a dataset of tweets spanning three years and comprising 60,617 tweets containing both MWEs and PMWE candidates. We first describe the process of constructing this corpus, followed by an overview of the manual annotation task performed by three experts on 600 tweets, achieving a maximum α score of 0.83. Insights from this manual annotation process were then used to develop a Game With A Purpose (GWAP) to annotate more tweets from the FRUIT corpus. This GWAP aims to enhance players’ understanding of MWEs and PMWEs. Currently, 13 players made 2,206 annotations on 931 tweets, reaching an α score of 0.70. In total, 1,531 tweets from the FRUIT corpus have been annotated.</abstract>
      <url hash="e7bcdf17">2025.law-1.6</url>
      <bibkey>bezancon-etal-2025-forbidden</bibkey>
      <doi>10.18653/v1/2025.law-1.6</doi>
    </paper>
    <paper id="7">
      <title>Another Approach to Agreement Measurement and Prediction with Emotion Annotations</title>
      <author><first>Quanqi</first><last>Du</last><affiliation>LT3, Ghent University</affiliation></author>
      <author><first>Veronique</first><last>Hoste</last><affiliation>LT3, Ghent University</affiliation></author>
      <pages>87-102</pages>
      <abstract>Emotion annotation, as an inherently subjective task, often suffers from significant inter-annotator disagreement when evaluated using traditional metrics like kappa or alpha. These metrics often fall short of capturing the nuanced nature of disagreement, especially in multimodal settings. This study introduces Absolute Annotation Difference (AAD), a novel metric offering a complementary perspective on inter- and intra-annotator agreement across different modalities. Our analysis reveals that AAD not only identifies overall agreement levels but also uncovers fine-grained disagreement patterns across modalities often overlooked by conventional metrics. Furthermore, we propose an AAD-based RMSE variant for predicting annotation disagreement. Through extensive experiments on the large-scale DynaSent corpus, we demonstrate that our approach significantly improves disagreement prediction accuracy, rising from 41.71% to 51.64% and outperforming existing methods. Cross-dataset prediction results suggest good generalization. These findings underscore AAD’s potential to enhance annotation agreement analysis and provide deeper insights into subjective NLP tasks. Future work will investigate its applicability to broader emotion-related tasks and other subjective annotation scenarios.</abstract>
      <url hash="ef51c19f">2025.law-1.7</url>
      <bibkey>du-hoste-2025-another</bibkey>
      <doi>10.18653/v1/2025.law-1.7</doi>
    </paper>
    <paper id="8">
      <title>Harmonizing Divergent Lemmatization and Part-of-Speech Tagging Practices for <fixed-case>L</fixed-case>atin Participles through the <fixed-case>L</fixed-case>i<fixed-case>L</fixed-case>a Knowledge Base</title>
      <author><first>Marco</first><last>Passarotti</last><affiliation>Università Cattolica del Sacro Cuore</affiliation></author>
      <author><first>Federica</first><last>Iurescia</last><affiliation>Catholic University of the Sacred Hearth</affiliation></author>
      <author><first>Paolo</first><last>Ruffolo</last><affiliation>Università Cattolica del Sacro Cuore</affiliation></author>
      <pages>103-114</pages>
      <abstract>This paper addresses the challenge of divergent lemmatization and part-of-speech (PoS) tagging practices for Latin participles in annotated corpora. We propose a solution through the LiLa Knowledge Base, a Linked Open Data framework designed to unify lexical and textual data for Latin. Using lemmas as the point of connection between distributed textual and lexical resources, LiLa introduces hypolemmas — secondary citation forms belonging to a word’s inflectional paradigm — as a means of reconciling divergent annotations for participles. Rather than advocating a single uniform annotation scheme, LiLa preserves each resource’s native guidelines while ensuring that users can retrieve and analyze participial data seamlessly. Via empirical assessments of multiple Latin corpora, we show how the LiLa’s integration of lemmas and hypolemmas enables consistent retrieval of participle forms regardless of whether they are categorized as verbal or adjectival.</abstract>
      <url hash="5a3ae696">2025.law-1.8</url>
      <bibkey>passarotti-etal-2025-harmonizing</bibkey>
      <doi>10.18653/v1/2025.law-1.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>UD</fixed-case>-<fixed-case>KSL</fixed-case> Treebank v1.3: A semi-automated framework for aligning <fixed-case>XPOS</fixed-case>-extracted units with <fixed-case>UPOS</fixed-case> tags</title>
      <author><first>Hakyung</first><last>Sung</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Gyu-Ho</first><last>Shin</last><affiliation>University of Illinois Chicago</affiliation></author>
      <author><first>Chanyoung</first><last>Lee</last><affiliation>Konkuk University</affiliation></author>
      <author><first>You Kyung</first><last>Sung</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Boo Kyung</first><last>Jung</last><affiliation>Yale University</affiliation></author>
      <pages>115-125</pages>
      <abstract>The present study extends recent work on Universal Dependencies annotations for second-language (L2) Korean by introducing a semi-automated framework that identifies morphosyntactic constructions from XPOS sequences and aligns those constructions with corresponding UPOS categories. We also broaden the existing L2-Korean corpus by annotating 2,998 new sentences from argumentative essays. To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean morphosyntactic analysis models on datasets both with and without these alignments, using two NLP toolkits. Our results indicate that the aligned dataset not only improves consistency across annotation layers but also enhances morphosyntactic tagging and dependency-parsing accuracy, particularly in cases of limited annotated data.</abstract>
      <url hash="9ec07420">2025.law-1.9</url>
      <bibkey>sung-etal-2025-ud</bibkey>
      <doi>10.18653/v1/2025.law-1.9</doi>
    </paper>
    <paper id="10">
      <title>Bootstrapping <fixed-case>UMR</fixed-case>s from <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies for Scalable Multilingual Annotation</title>
      <author><first>Federica</first><last>Gamba</last><affiliation>Charles University</affiliation></author>
      <author><first>Alexis</first><last>Palmer</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Daniel</first><last>Zeman</last><affiliation>Charles University, Faculty of Mathematics and Physics</affiliation></author>
      <pages>126-136</pages>
      <abstract>Uniform Meaning Representation (UMR) is a semantic annotation framework designed to be applicable across typologically diverse languages. However, UMR annotation is a labor-intensive task, requiring significant effort and time especially when no prior annotations are available. In this paper, we present a method for bootstrapping UMR graphs by leveraging Universal Dependencies (UD), one of the most comprehensive multilingual resources, encompassing languages across a wide range of language families. Given UMR’s strong typological and cross-linguistic orientation, UD serves as a particularly suitable starting point for the conversion. We describe and evaluate an approach that automatically derives partial UMR graphs from UD trees, providing annotators with an initial representation to build upon. While UD is not a semantic resource, our method extracts useful structural information that aligns with the UMR formalism, thereby facilitating the annotation process. By leveraging UD’s broad typological coverage, this approach offers a scalable way to support UMR annotation across different languages.</abstract>
      <url hash="2726ac01">2025.law-1.10</url>
      <bibkey>gamba-etal-2025-bootstrapping</bibkey>
      <doi>10.18653/v1/2025.law-1.10</doi>
    </paper>
    <paper id="11">
      <title>Classifying <fixed-case>TEI</fixed-case> Encoding for <fixed-case>D</fixed-case>utch<fixed-case>D</fixed-case>ra<fixed-case>C</fixed-case>or with Transformer Models</title>
      <author><first>Florian</first><last>Debaene</last><affiliation>Ghent University</affiliation></author>
      <author><first>Veronique</first><last>Hoste</last><affiliation>LT3, Ghent University</affiliation></author>
      <pages>137-141</pages>
      <abstract>Computational Drama Analysis relies on well-structured textual data, yet many dramatic works remain in need of encoding. The Dutch dramatic tradition is one such an example, with currently 180 plays available in the DraCor database, while many more plays await integration still. To facilitate this process, we propose a semi-automated TEI encoding annotation methodology using transformer encoder language models to classify structural elements in Dutch drama. We fine-tune 4 Dutch models on the DutchDraCor dataset to predict the 9 most relevant labels used in the DraCor TEI encoding, experimenting with 2 model input settings. Our results show that incorporating additional context through beginning-of-sequence (BOS) and end-of-sequence (EOS) tokens greatly improves performance, increasing the average macro F1 score across models from 0.717 to 0.923 (+0.206). Using the best-performing model, we generate silver-standard DraCor labels for EmDComF, an unstructured corpus of early modern Dutch comedies and farces, paving the way for its integration into DutchDraCor after validation.</abstract>
      <url hash="e7440b0b">2025.law-1.11</url>
      <bibkey>debaene-hoste-2025-classifying</bibkey>
      <doi>10.18653/v1/2025.law-1.11</doi>
    </paper>
    <paper id="12">
      <title>Label Bias in Symbolic Representation of Meaning</title>
      <author><first>Marie</first><last>Mikulová</last><affiliation>Charles University</affiliation></author>
      <author><first>Jan</first><last>Štěpánek</last><affiliation>Charles University in Prague, Faculty of Mathematics and Physics, UFAL</affiliation></author>
      <author><first>Jan</first><last>Hajič</last><affiliation>Charles University</affiliation></author>
      <pages>142-159</pages>
      <abstract>This paper contributes to the trend of building semantic representations and exploring the relations between a language and the world it represents. We analyse alternative approaches to semantic representation, focusing on methodology of determining meaning categories, their arrangement and granularity, and annotation consistency and reliability. Using the task of semantic classification of circumstantial meanings within the Prague Dependency Treebank framework, we present our principles for analyzing meaning categories. Compared with the discussed projects, the unique aspect of our approach is its focus on how a language, in its structure, reflects reality. We employ a two-level classification: a higher, coarse-grained set of general semantic concepts (defined by questions: where, how, why, etc.) and a fine-grained set of circumstantial meanings based on data-driven analysis, reflecting meanings fixed in the language. We highlight that the inherent vagueness of linguistic meaning is crucial for capturing the limitless variety of the world but it can lead to label biases in datasets. Therefore, besides semantically clear categories, we also use fuzzy meaning categories.</abstract>
      <url hash="1ca89b64">2025.law-1.12</url>
      <bibkey>mikulova-etal-2025-label</bibkey>
      <doi>10.18653/v1/2025.law-1.12</doi>
    </paper>
    <paper id="13">
      <title>An Annotation Protocol for Diachronic Evaluation of Semantic Drift in Disability Sources</title>
      <author><first>Nitisha</first><last>Jain</last><affiliation>King’s College London</affiliation></author>
      <author><first>Chiara</first><last>Di Bonaventura</last><affiliation>King’s College London</affiliation></author>
      <author><first>Albert</first><last>Merono Penuela</last><affiliation>King’s College London</affiliation></author>
      <author><first>Barbara</first><last>McGillivray</last><affiliation>King’s College London</affiliation></author>
      <pages>160-172</pages>
      <abstract>Annotating terms referring to aspects of disability in historical texts is crucial for understanding how societies in different periods conceptualized and treated disability. Such annotations help modern readers grasp the evolving language, cultural attitudes, and social structures surrounding disability, shedding light on both marginalization and inclusion throughout history. This is important as evolving societal attitudes can influence the perpetuation of harmful language that reinforces stereotypes and discrimination. However, this task presents significant challenges. Terminology often reflects outdated, offensive, or ambiguous concepts that require sensitive interpretation. Meaning of terms may have shifted over time, making it difficult to align historical terms with contemporary understandings of disability. Additionally, contextual nuances and the lack of standardized language in historical records demand careful scholarly judgment to avoid anachronism or misrepresentation.</abstract>
      <url hash="faa0d90b">2025.law-1.13</url>
      <bibkey>jain-etal-2025-annotation</bibkey>
      <doi>10.18653/v1/2025.law-1.13</doi>
    </paper>
    <paper id="14">
      <title>Pre-annotation Matters: A Comparative Study on <fixed-case>POS</fixed-case> and Dependency Annotation for an <fixed-case>A</fixed-case>lsatian Dialect</title>
      <author><first>Delphine</first><last>Bernhard</last><affiliation>Lilpa, Université de Strasbourg</affiliation></author>
      <author><first>Nathanaël</first><last>Beiner</last><affiliation>LiLPa, Université de Strasbourg</affiliation></author>
      <author><first>Barbara</first><last>Hoff</last><affiliation>LiLPa, Université de Strasbourg</affiliation></author>
      <pages>173-186</pages>
      <abstract>The annotation of corpora for lower-resource languages can benefit from automatic pre-annotation to increase the throughput of the annotation process in a a context where human resources are scarce. However, this can be hindered by the lack of available pre-annotation tools. In this work, we compare three pre-annotation methods in zero-shot or near-zero-shot contexts for part-of-speech (POS) and dependency annotation of an Alsatian Alemannic dialect. Our study shows that good levels of annotation quality can be achieved, with human annotators adapting their correction effort to the perceived quality of the pre-annotation. The pre-annotation tools also vary in efficiency depending on the task, with better global results for a system trained on closely related languages and dialects.</abstract>
      <url hash="2f72ba79">2025.law-1.14</url>
      <bibkey>bernhard-etal-2025-pre</bibkey>
      <doi>10.18653/v1/2025.law-1.14</doi>
    </paper>
    <paper id="15">
      <title>Where it’s at: Annotating Verb Placement Types in Learner Language</title>
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <author><first>Annette</first><last>Portmann</last></author>
      <author><first>Matthias</first><last>Schwendemann</last></author>
      <author><first>Christine</first><last>Renker</last></author>
      <author><first>Katrin</first><last>Wisniewski</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>187-200</pages>
      <abstract>The annotation of learner language is an often ambiguous and challenging task. It is therefore surprising that in Second Language Acquisition research, information on annotation quality is hardly ever published. This is also true for verb placement, a linguistic feature that has re- ceived much attention within SLA. This paper presents an annotation on verb placement in German learner texts at different proficiency levels. We argue that as part of the annotation process target hypotheses should be provided as ancillary annotations that make explicit each annotator’s interpretation of a learner sentence. Our study demonstrates that verb placement can be annotated with high agreement between multiple annotators, for texts at all proficiency levels and across sentences of varying complex- ity. We release our corpus with annotations by four annotators on more than 600 finite clauses sampled across 5 CEFR levels.</abstract>
      <url hash="d361babe">2025.law-1.15</url>
      <bibkey>ruppenhofer-etal-2025-annotating</bibkey>
      <doi>10.18653/v1/2025.law-1.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>ICLE</fixed-case>-<fixed-case>RC</fixed-case>: International Corpus of Learner <fixed-case>E</fixed-case>nglish for Relative Clauses</title>
      <author><first>Debopam</first><last>Das</last><affiliation>Abo Akademi University</affiliation></author>
      <author><first>Izabela</first><last>Czerniak</last><affiliation>Åbo Akademi University</affiliation></author>
      <author><first>Peter</first><last>Bourgonje</last><affiliation>Potsdam University</affiliation></author>
      <pages>201-215</pages>
      <abstract>We present the ICLE-RC, a corpus of learner English texts annotated for relative clauses and related phenomena. The corpus contains a collection of 144 academic essays from the International Corpus of Learner English (ICLE; Granger et al., 2002), representing six L1 backgrounds – Finnish, Italian, Polish, Swedish, Turkish, and Urdu. These texts are annotated for over 900 relative clauses, with respect to a wide array of lexical, syntactic, semantic, and discourse features. The corpus also provides annotation of over 400 related phenomena (it-clefts, pseudo-clefts, existential-relatives, etc.). Here, we describe the corpus annotation framework, report on the IAA study, discuss the prospects of (semi-)automating annotation, and present the first results from our corpus analysis. We envisage the ICLE-RC to be used as a valuable resource for research on relative clauses in SLA, language typology, World Englishes, and discourse analysis.</abstract>
      <url hash="79b49d69">2025.law-1.16</url>
      <bibkey>das-etal-2025-icle</bibkey>
      <doi>10.18653/v1/2025.law-1.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>E</fixed-case>xp<fixed-case>L</fixed-case>ay: A new Corpus Resource for the Research on Expertise as an Influential Factor on Language Production</title>
      <author><first>Carmen</first><last>Schacht</last><affiliation>Ruhr-Universität Bochum</affiliation></author>
      <author><first>Renate</first><last>Delucchi Danhier</last><affiliation>TU Dortmund</affiliation></author>
      <pages>216-227</pages>
      <abstract>This paper introduces the ExpLay-Pipeline, a novel semi-automated processing tool designed for the analysis of language production data from experts in comparison to the language production of a control group of laypeople. The pipeline combines manual annotation and curation with state-of-the-art machine learning and rule-based methods, following a silver standard approach. It integrates various analysis modules specifically for the syntactic and lexical evaluation of parsed linguistic data. While implemented initially for the creation of the ExpLay-Corpus, it is designed for the processing of linguistic data in general. The paper details the design and implementation of this pipeline.</abstract>
      <url hash="22bc4a5c">2025.law-1.17</url>
      <bibkey>schacht-delucchi-danhier-2025-explay</bibkey>
      <doi>10.18653/v1/2025.law-1.17</doi>
    </paper>
    <paper id="18">
      <title>Towards Resource-Rich Mizo and <fixed-case>K</fixed-case>hasi in <fixed-case>NLP</fixed-case>: Resource Development, Synthetic Data Generation and Model Building</title>
      <author><first>Soumyadip</first><last>Ghosh</last><affiliation>IIIT-Hyderabad</affiliation></author>
      <author><first>Henry</first><last>Lalsiam</last><affiliation>North Eastern Hill University</affiliation></author>
      <author><first>Dorothy</first><last>Marbaniang</last><affiliation>Assam University</affiliation></author>
      <author><first>Gracious Mary</first><last>Temsen</last><affiliation>University of Hyderabad</affiliation></author>
      <author><first>Rahul</first><last>Mishra</last><affiliation>IIIT-Hyderabad</affiliation></author>
      <author><first>Parameswari</first><last>Krishnamurthy</last><affiliation>Assistant Professor, IIIT Hyderabad</affiliation></author>
      <pages>228-239</pages>
      <abstract>In the rapidly evolving field of Natural Language Processing (NLP), Indian regional languages remain significantly underrepresented due to their limited digital presence and lack of annotated resources. This work presents the first comprehensive effort toward developing high quality linguistic datasets for two extremely low resource languages Mizo and Khasi. We introduce human annotated, gold standard datasets for three core NLP tasks: Part-of-Speech (POS) tagging, Named Entity Recognition (NER), and Keyword Identification. To overcome annotation bottlenecks in NER, we further explore a synthetic data generation pipeline involving translation from Hindi and cross lingual word alignment. For POS tagging, we adopt and subsequently modify the Universal Dependencies (UD) framework to better suit the linguistic characteristics of Mizo and Khasi, while custom annotation guidelines are developed for NER and Keyword Identification. The constructed datasets are evaluated using multilingual language models, demonstrating that structured resource development, coupled with gradual fine-tuning, yields significant improvements in performance. This work represents a critical step toward advancing linguistic resources and computational tools for Mizo and Khasi.</abstract>
      <url hash="af7f8d47">2025.law-1.18</url>
      <bibkey>ghosh-etal-2025-towards</bibkey>
      <doi>10.18653/v1/2025.law-1.18</doi>
    </paper>
    <paper id="19">
      <title>Creating Hierarchical Relations in a Multilingual Event-type Ontology</title>
      <author><first>Zdeňka</first><last>Urešová</last><affiliation>Charles University</affiliation></author>
      <author><first>Eva</first><last>Fučíková</last><affiliation>Charles University</affiliation></author>
      <author><first>Jan</first><last>Hajič</last><affiliation>Charles University</affiliation></author>
      <pages>240-249</pages>
      <abstract>This paper describes the work on hierarchization of the SynSemClass event-type ontology. The original resource has been extended by a hierarchical structure to model specialization and generalization relations between classes that are formally and technically unrelated in the original ontology. The goal is to enable one to use the ontology enriched by the hierarchical concepts for annotation of running texts in symbolic meaning representations, such as UMR or PDT. The hierarchy is in principle built bottom-up, based on existing SSC classes (concepts). This approach differs from other approaches to semantic classes, such as in WordNet or VerbNet. Although the hierarchical relations are similar, the underlying nodes in the hierarchy are not. In this paper, we describe the challenges related to the principles chosen: single-tree constraint and finding features for the definitions of specificity/generality. Also, a pilot inter-annotator experiment is described that shows the difficulty of the hierarchization task.</abstract>
      <url hash="a2c6a32d">2025.law-1.19</url>
      <bibkey>uresova-etal-2025-creating</bibkey>
      <doi>10.18653/v1/2025.law-1.19</doi>
    </paper>
    <paper id="20">
      <title>Visual Representations of Temporal Relations between Events and Time Expressions in News Stories</title>
      <author><first>Evelin</first><last>Amorim</last><affiliation>Porto University</affiliation></author>
      <author><first>António</first><last>Leal</last><affiliation>University of Porto/ Centre of Linguistics of the University of Porto</affiliation></author>
      <author><first>Nana</first><last>Yu</last><affiliation>Universidade do Porto</affiliation></author>
      <author><first>Purificação Moura</first><last>Silvano</last><affiliation>University of Porto/ Centre of Linguistics of the University of Porto</affiliation></author>
      <author><first>Alipio</first><last>Mario Jorge</last><affiliation>University of Porto</affiliation></author>
      <pages>250-263</pages>
      <abstract>High-quality annotation is paramount for effective predictions of machine learning models. When the annotation is dense, achieving superior human labeling can be challenging since the most used annotation tools present an overloaded visualization of labels. Thus, we present a tool for viewing annotations made in corpora, specifically for temporal relations between events and temporal expressions, filling a gap in this type of tool. We focus on narrative text, which is a rich source for these types of elements.</abstract>
      <url hash="3c4a37ba">2025.law-1.20</url>
      <bibkey>amorim-etal-2025-visual</bibkey>
      <doi>10.18653/v1/2025.law-1.20</doi>
    </paper>
    <paper id="21">
      <title>Annotating candy speech in <fixed-case>G</fixed-case>erman <fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ube comments</title>
      <author><first>Yulia</first><last>Clausen</last><affiliation>Ruhr-University of Bochum</affiliation></author>
      <author><first>Tatjana</first><last>Scheffler</last><affiliation>Ruhr University Bochum</affiliation></author>
      <pages>264-269</pages>
      <abstract>We describe the phenomenon of candy speech – positive emotional speech in online communication – and introduce a classification of its various types based on the theoretical framework of social interaction by Goffman (1967). We provide a dataset of 46,286 German YouTube comments manually annotated with candy speech</abstract>
      <url hash="25b05a64">2025.law-1.21</url>
      <bibkey>clausen-scheffler-2025-annotating</bibkey>
      <doi>10.18653/v1/2025.law-1.21</doi>
    </paper>
    <paper id="22">
      <title>Variety delights (sometimes) - Annotation differences in morphologically annotated corpora</title>
      <author><first>Andrea</first><last>Dömötör</last><affiliation>ELTE Department of Digital Humanities</affiliation></author>
      <author><first>Balázs</first><last>Indig</last><affiliation>ELTE Faculty of Informatics</affiliation></author>
      <author><first>Dávid Márk</first><last>Nemeskey</last><affiliation>ELTE DH</affiliation></author>
      <pages>270-278</pages>
      <abstract>The goal of annotation standards is to ensure consistency across different corpora and languages. But do they succeed? In our paper we experiment with morphologically annotated Hungarian corpora of different sizes (ELTE DH gold standard corpus, NYTK-NerKor, and Szeged Treebank) to assess their compatibility as a merged training corpus for morphological analysis and disambiguation. Our results show that combining any two corpora not only failed to improve the results of the trained tagger but even degraded them due the inconsistent annotations. Further analysis of the annotation differences among the corpora revealed inconsistencies of several sources: different theoretical approach, lack of consensus, and tagset conversion issues.</abstract>
      <url hash="59a66943">2025.law-1.22</url>
      <bibkey>domotor-etal-2025-variety</bibkey>
      <doi>10.18653/v1/2025.law-1.22</doi>
    </paper>
    <paper id="23">
      <title>Addressing Variability in Interlinear Glossed Texts with Linguistic Linked Data</title>
      <author><first>Maxim</first><last>Ionov</last><affiliation>University of Zaragoza</affiliation></author>
      <author><first>Natalia</first><last>Patiño Mazzotti</last><affiliation>Goethe University Frankfurt</affiliation></author>
      <pages>279-284</pages>
      <abstract>In this paper, we identify types of uncertainty in interlinear glossed text (IGT) annotation, a common notation for language data in linguistic research.</abstract>
      <url hash="aa7ba129">2025.law-1.23</url>
      <bibkey>ionov-patino-mazzotti-2025-addressing</bibkey>
      <doi>10.18653/v1/2025.law-1.23</doi>
    </paper>
    <paper id="24">
      <title>Illuminating Logical Fallacies with the <fixed-case>CAMPFIRE</fixed-case> Corpus</title>
      <author><first>Austin</first><last>Blodgett</last><affiliation>US Army Research Lab</affiliation></author>
      <author><first>Claire</first><last>Bonial</last><affiliation>US Army Research Lab</affiliation></author>
      <author><first>Taylor A.</first><last>Pellegrin</last><affiliation>ARL</affiliation></author>
      <author><first>Melissa</first><last>Torgbi</last><affiliation>University of Bath</affiliation></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last><affiliation>University of Bath</affiliation></author>
      <pages>285-296</pages>
      <abstract>Misinformation detection remains today a challenging task for both annotators and computer systems. While there are many known markers of misinformation—e.g., logical fallacies, propaganda techniques, and improper use of sources—labeling these markers in practice has been shown to produce low agreement as it requires annotators to make several subjective judgments and rely on their own knowledge, external to the text, which may vary between annotators. In this work, we address these challenges with a collection of linguistically-inspired litmus tests. We annotate a schema of 25 logical fallacies, each of which is defined with rigorous tests applied during annotation. Our annotation methodology results in a comparatively high IAA on this task: Cohen’s kappa in the range .69-.86. We release a corpus of 12 documents from various domains annotated with fallacy labels. Additionally, we experiment with a large language model baseline showing that the largest, most advanced models struggle on this challenging task, achieving an F1-score with our gold standard of .08 when excluding non-fallacious examples, compared to human performance of .59-.73. However, we find that prompting methodologies requiring the model to work through our litmus tests improves performance. Our work contributes a robust fallacy annotation schema and annotated corpus, which advance capabilities in this critical research area.</abstract>
      <url hash="d018a9f9">2025.law-1.24</url>
      <bibkey>blodgett-etal-2025-illuminating</bibkey>
      <doi>10.18653/v1/2025.law-1.24</doi>
    </paper>
    <paper id="25">
      <title>Cheap Annotation of Complex Information: A Study on the Annotation of Information Status in <fixed-case>G</fixed-case>erman <fixed-case>TED</fixed-case>x Talks</title>
      <author><first>Carmen</first><last>Schacht</last><affiliation>Ruhr-Universität Bochum</affiliation></author>
      <author><first>Tobias</first><last>Nischk</last><affiliation>Ruhr-Universität Bochum</affiliation></author>
      <author><first>Oleksandra</first><last>Yazdanfar</last><affiliation>Ruhr-Universität Bochum</affiliation></author>
      <author><first>Stefanie</first><last>Dipper</last><affiliation>Ruhr University Bochum</affiliation></author>
      <pages>297-307</pages>
      <abstract>We present an annotation experiment for the annotation of information status in German TEDx Talks with the main goal to reduce annotation costs in terms of time and personnel. We aim for maximizing efficiency while keeping annotation quality constant by testing various different annotation scenarios for an optimal ratio of annotation expenses to resulting quality of the annotations. We choose the RefLex scheme of Riester and Baumann (2017) as a basis for our annotations, refine their annotation guidelines for a more generalizable tagset and conduct the experiment on German Tedx talks, applying different constellations of annotators, curators and correctors to test for an optimal annotation scenario. Our results show that we can achieve equally good and possibly even better results with significantly less effort, by using correctors instead of additional annotators.</abstract>
      <url hash="441c62be">2025.law-1.25</url>
      <bibkey>schacht-etal-2025-cheap</bibkey>
      <doi>10.18653/v1/2025.law-1.25</doi>
    </paper>
    <paper id="26">
      <title>Annotating Spatial Descriptions in Literary and Non-Literary Text</title>
      <author><first>Emilie</first><last>Sitter</last><affiliation>University of Bielefeld</affiliation></author>
      <author><first>Omar</first><last>Momen</last><affiliation>University of Bielefeld</affiliation></author>
      <author><first>Florian</first><last>Steig</last><affiliation>University of Bielefeld</affiliation></author>
      <author><first>J. Berenike</first><last>Herrmann</last><affiliation>University of Bielefeld</affiliation></author>
      <author><first>Sina</first><last>Zarrieß</last><affiliation>University of Bielefeld</affiliation></author>
      <pages>308-325</pages>
      <abstract>Descriptions are a central component of literary texts, yet their systematic identification remains a challenge. This work suggests an approach to identifying sentences describing spatial conditions in literary text. It was developed iteratively on German literary text and extended to non-literary text to evaluate its applicability across textual domains. To assess the robustness of the method, we involved both humans and a selection of state-of-the-art Large Language Models (LLMs) in annotating a collection of sentences regarding their descriptiveness and spatiality. We compare the annotations across human annotators and between humans and LLMs. The main contributions of this paper are: (1) a set of annotation guidelines for identifying spatial descriptions in literary texts, (2) a curated dataset of almost 4,700 annotated sentences of which around 500 are spatial descriptions, produced through in-depth discussion and consensus among annotators, and (3) a pilot study of automating the task of spatial description annotation of German texts. We publish the codes and all human and LLM annotations for the public to be used for research purposes only.</abstract>
      <url hash="bbc89fd5">2025.law-1.26</url>
      <bibkey>sitter-etal-2025-annotating</bibkey>
      <doi>10.18653/v1/2025.law-1.26</doi>
    </paper>
    <paper id="27">
      <title>A <fixed-case>G</fixed-case>it<fixed-case>H</fixed-case>ub-based Workflow for Annotated Resource Development</title>
      <author><first>Brandon</first><last>Waldon</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Nathan</first><last>Schneider</last><affiliation>Georgetown University</affiliation></author>
      <pages>326-331</pages>
      <abstract>Computational linguists have long recognized the value of version control systems such as Git (and related platforms, e.g., GitHub) when it comes to managing and distributing computer code. However, the benefits of version control remain under-explored for a central activity within computational linguistics: the development of annotated natural language resources. We argue that researchers can employ version control practices to make development workflows more transparent, efficient, consistent, and participatory. We report a proof-of-concept, GitHub-based solution which facilitated the creation of a legal English treebank.</abstract>
      <url hash="cdff0767">2025.law-1.27</url>
      <bibkey>waldon-schneider-2025-github</bibkey>
      <doi>10.18653/v1/2025.law-1.27</doi>
    </paper>
    <paper id="28">
      <title>Enhancing an Annotation Scheme for Clinical Narratives in <fixed-case>P</fixed-case>ortuguese through Human Variation Analysis</title>
      <author><first>Ana Luisa</first><last>Fernandes</last><affiliation>FLUP, CLUP and INESC TEC</affiliation></author>
      <author><first>Purificação</first><last>Silvano</last><affiliation>FLUP, CLUP and INESC TEC</affiliation></author>
      <author><first>António</first><last>Leal</last><affiliation>FLUP, CLUP and University of Macau</affiliation></author>
      <author><first>Nuno</first><last>Guimarães</last><affiliation>FCUP and INESC TEC</affiliation></author>
      <author><first>Rita</first><last>Rb-Silva</last><affiliation>CI-IPOP and Rise-Health</affiliation></author>
      <author><first>Luís Filipe</first><last>Cunha</last><affiliation>University of Porto, University of Minho and INESC TEC</affiliation></author>
      <author><first>Alípio</first><last>Jorge</last><affiliation>FCUP and INESC TEC</affiliation></author>
      <pages>332-343</pages>
      <abstract>The development of a robust annotation scheme and corresponding guidelines is crucial for producing annotated datasets that advance both linguistic and computational research. This paper presents a case study that outlines a methodology for designing an annotation scheme and its guidelines, specifically aimed at representing morphosyntactic and semantic information regarding temporal features, as well as medical information in medical reports written in Portuguese. We detail a multi-step process that includes reviewing existing frameworks, conducting an annotation experiment to determine the optimal approach, and designing a model based on these findings. We validated the approach through a pilot experiment where we assessed the reliability and applicability of the annotation scheme and guidelines. In this experiment, two annotators independently annotated a patient’s medical report consisting of six documents using the proposed model, while a curator established the ground truth. The analysis of inter-annotator agreement and the annotation results enabled the identification of sources of human variation and provided insights for further refinement of the annotation scheme and guidelines.</abstract>
      <url hash="1c9172c4">2025.law-1.28</url>
      <bibkey>fernandes-etal-2025-enhancing</bibkey>
      <doi>10.18653/v1/2025.law-1.28</doi>
    </paper>
    <paper id="29">
      <title>Expanding the <fixed-case>UNSC</fixed-case> Conflicts Corpus by Incorporating Domain Expert Annotations and <fixed-case>LLM</fixed-case> Experiments</title>
      <author><first>Karolina</first><last>Zaczynska</last><affiliation>University of Potsdam</affiliation></author>
      <pages>344-358</pages>
      <abstract>In this work we expand the UN Security Council Conflicts corpus (UNSCon) (Zaczynska at al. 2024) on verbal disputes in diplomatic speeches in English.</abstract>
      <url hash="bad35e18">2025.law-1.29</url>
      <bibkey>zaczynska-2025-expanding</bibkey>
      <doi>10.18653/v1/2025.law-1.29</doi>
    </paper>
    <paper id="30">
      <title>Guidelines for Fine-grained Sentence-level <fixed-case>A</fixed-case>rabic Readability Annotation</title>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Hanada</first><last>Taha-Thomure</last></author>
      <author><first>Khalid N.</first><last>Elmadani</last></author>
      <author><first>Zeina</first><last>Zeino</last></author>
      <author><first>Abdallah</first><last>Abushmaes</last></author>
      <pages>359-376</pages>
      <abstract>This paper presents the annotation guidelines of the Balanced Arabic Readability Evaluation Corpus (BAREC), a large-scale resource for fine-grained sentence-level readability assessment in Arabic. BAREC includes 69,441 sentences (1M+ words) labeled across 19 levels, from kindergarten to postgraduate. Based on the Taha/Arabi21 framework, the guidelines were refined through iterative training with native Arabic-speaking educators. We highlight key linguistic, pedagogical, and cognitive factors in determining readability and report high inter-annotator agreement: Quadratic Weighted Kappa 81.8% (substantial/excellent agreement) in the last annotation phase. We also benchmark automatic readability models across multiple classification granularities (19-, 7-, 5-, and 3-level). The corpus and guidelines are publicly available: http://barec.camel-lab.com.</abstract>
      <url hash="54fddcc7">2025.law-1.30</url>
      <bibkey>habash-etal-2025-guidelines</bibkey>
      <doi>10.18653/v1/2025.law-1.30</doi>
    </paper>
  </volume>
</collection>
