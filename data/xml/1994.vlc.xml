<?xml version='1.0' encoding='UTF-8'?>
<collection id="1994.vlc">
  <volume id="1" type="proceedings" ingest-date="2024-07-13">
    <meta>
      <booktitle>Second Workshop on Very Large Corpora</booktitle>
      <venue>vlc</venue>
      <year>1994</year>
      <url hash="816a4e63">1994.vlc-1</url>
    </meta>
    <frontmatter>
      <pages>1-5</pages>
      <url hash="efcd1d9b">1994.vlc-1.0</url>
      <bibkey>vlc-1994-large</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>TEI</fixed-case>-Conformant Structural Markup of a Trilingual Parallel Corpus in the <fixed-case>ECI</fixed-case> Multilingual Corpus 1</title>
      <author><first>David</first><last>McKelvie</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Henry S.</first><last>Thompson</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>7-18</pages>
      <abstract>In this paper we provide an overview of the ACL European Corpus Initiative (ECI) Multilingual Corpus 1 (ECI/MC1). In particular, we look at one particular subcorpus in the ECI/MC1, the trilingual corpus of International Labour Organisation reports, and discuss the problems involved in TEI-compliant structural markup and preliminary alignment of this large corpus. We discuss gross structural alignment down to the level of text paragraphs. We see this as a necessary first step in corpus preparation before detailed (possibly automatic) alignment of text is possible. We try and generalise our experience with this corpus to illustrate the process of preliminary markup of large corpora which in their raw state can be in an arbitrary format (eg printers tapes, proprietary word-processor format); noisy (not fully parallel, with structure obscured by spelling mistakes); full of poorly documented formatting instructions; and whose structure is present but anything but explicit. We illustrate these points by reference to other parallel subcorpora of ECI/MC1. We attempt to define some guidelines for the development of corpus annotation toolkits which would aid this kind of structural preparation of large corpora.</abstract>
      <url hash="ff5e8fc3">1994.vlc-1.1</url>
      <bibkey>mckelvie-thompson-1994-tei</bibkey>
    </paper>
    <paper id="2">
      <title>A Comparison of Corpus-based Techniques for Restoring Accents in <fixed-case>S</fixed-case>panish and <fixed-case>F</fixed-case>rench Text</title>
      <author><first>David</first><last>Yarowsky</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>19-32</pages>
      <abstract>This paper will explore and compare three corpus-based techniques for lexical ambiguity resolution, focusing on the problem of restoring missing accents to Spanish and French text. Many of the ambiguities created by missing accents are differences in part of speech: hence one of the methods considered is an N-gram tagger using Viterbi decoding, such as is found in stochastic part-of-speech taggers. A second technique, Bayesian classification, has been successfully applied to word-sense disambiguation and is well suited for some of the semantic ambiguities which arise from missing accents. The third approach, based on decision lists, combines the strengths of the two other methods, incorporating both local syntactic patterns and more distant collocational evidence, and outperforms them both. The problem of accent restoration is particularly well suited for demonstrating and testing the capabilities of the given algorithms because it requires the resolution of both semantic and syntactic ambiguity, and offers an objective ground truth for automatic evaluation. It is also a practical problem with immediate application.</abstract>
      <url hash="e6881719">1994.vlc-1.2</url>
      <bibkey>yarowsky-1994-comparison</bibkey>
    </paper>
    <paper id="3">
      <title>Extracting a Disambiguated Thesaurus from Parallel Dictionary Definitions</title>
      <author><first>Naohiko</first><last>Uramoto</last><affiliation>IBM Research</affiliation></author>
      <pages>33-42</pages>
      <abstract>This paper describes a method for extracting disambiguated (bilingual) is-a relationships from parallel (English and Japanese) dictionary definitions by using word-level alignment. Definitions have a specific pattern, namely, a "genus term and differentia" structure; therefore, bilingual genus terms can be extracted by using bilingual pattern matching. For the alignment of words in the genus terms, a dynamic programming framework for sentence-level alignment proposed by Gale et al. [6] is used.</abstract>
      <url hash="d7783f5a">1994.vlc-1.3</url>
      <bibkey>uramoto-1994-extracting</bibkey>
    </paper>
    <paper id="4">
      <title>Application of Corpora in Second Language Learning: The Problem of Collocational Knowledge Acquisition</title>
      <author><first>Kenji</first><last>Kita</last><affiliation>Tokushima University</affiliation></author>
      <author><first>Takashi</first><last>Omoto</last><affiliation>Tokushima University</affiliation></author>
      <author><first>Yoneo</first><last>Yano</last><affiliation>Tokushima University</affiliation></author>
      <author><first>Yasuhiko</first><last>Kato</last><affiliation>Tokushima University</affiliation></author>
      <pages>43-56</pages>
      <abstract>While corpus-based studies are now becoming a new methodology in natural language processing, second language learning offers one interesting potential application. In this paper, we are primarily concerned with the acquisition of collocational knowledge from corpora for use in language learning. First we discuss the importance of collocational knowledge in second language learning, and then take up two measures, mutual information and cost criteria, for automatically identifying or extracting collocations from corpora. Comparitive experiments are made between the two measures using both Japanese and English corpora. In our experiments, the cost criteria measure proved more effective in extracting interesting collocations such as fundamental idiomatic expressions and phrases.</abstract>
      <url hash="929c492f">1994.vlc-1.4</url>
      <bibkey>kita-etal-1994-application</bibkey>
    </paper>
    <paper id="5">
      <title>Iterative Alignment of Syntactic Structures for a Bilingual Corpus</title>
      <author><first>Ralph</first><last>Grishman</last><affiliation>New York University</affiliation></author>
      <pages>57-68</pages>
      <abstract>Alignment of parallel bilingual corpora at the level of syntactic structure holds the promise of being able to discover detailed bilingual structural correspondences automatically. This paper describes a procedure for the alignment of regularized syntactic structures, proceeding bottom-up through the trees. It makes use of information about possible lexical correspondences, from a bilingual dictionary, to generate initial candidate alignments. We consider in particular how much dictionary coverage is needed for the alignment process, and how the alignment can be iteratively improved by having an initial alignment generate additional lexical correspondences for the dictionary, and then using this augmented dictionary for subsequent alignment passes.</abstract>
      <url hash="7880c317">1994.vlc-1.5</url>
      <bibkey>grishman-1994-iterative</bibkey>
    </paper>
    <paper id="6">
      <title>Statistical Augmentation of a <fixed-case>C</fixed-case>hinese Machine-Readable Dictionary</title>
      <author><first>Pascale</first><last>Fung</last><affiliation>Columbia University</affiliation></author>
      <author><first>Dekai</first><last>Wu</last><affiliation>HKUST</affiliation></author>
      <pages>69-86</pages>
      <abstract>We describe a method of using statistically-collected Chinese character groups from a corpus to augment a Chinese dictionary. The method is particularly useful for extracting domain-specific and regional words not readily available in machine-readable dictionaries. Output was evaluated both using human evaluators and against a previously available dictionary. We also evaluated performance improvement in automatic Chinese tokenization. Results show that our method outputs legitimate words, acronymic constructions, idioms, names and titles, as well as technical compounds, many of which were lacking from the original dictionary.</abstract>
      <url hash="6ed10083">1994.vlc-1.6</url>
      <bibkey>fung-wu-1994-statistical</bibkey>
    </paper>
    <paper id="7">
      <title>Comparing the Retrieval Performance of <fixed-case>E</fixed-case>nglish and <fixed-case>J</fixed-case>apanese Text Databases</title>
      <author><first>Hideo</first><last>Fuji</last><affiliation>University of Massachusetts</affiliation></author>
      <author><first>Bruce W.</first><last>Croft</last><affiliation>University of Massachusetts</affiliation></author>
      <pages>87-98</pages>
      <abstract>The retrieval effectiveness for English and Japanese full-text databases are studied using the INQUERY retrieval system. Two series of experiments - short queries and longer TIPSTER queries - were examined. For short queries, Japanese generally performed more effectively than English. For longer queries, relative effectiveness showed little correlation among various query strategies. This result suggests that the best Japanese query processing strategy may be quite different from the English one.</abstract>
      <url hash="9e257ff4">1994.vlc-1.7</url>
      <bibkey>fuji-croft-1994-comparing</bibkey>
    </paper>
    <paper id="8">
      <title>A Phrase-Retrieval System based on Recurrence</title>
      <author><first>Magnus</first><last>Merkel</last><affiliation>Linköping University</affiliation></author>
      <author><first>Bertn</first><last>Nilsson</last><affiliation>Linköping University</affiliation></author>
      <author><first>Lars</first><last>Ahrenberg</last><affiliation>Linköping University</affiliation></author>
      <pages>99-108</pages>
      <abstract>The paper describes a simple but useful phrase-retrieval system that primarily is intended as a support tool for computer-aided translation. Given no other input than a text (and a word list used for filtering purposes), the system retrieves recurrent sentences and phrases of the text and their positions. In addition the system provides information on internal and external recurrence rates.</abstract>
      <url hash="14a796a3">1994.vlc-1.8</url>
      <bibkey>merkel-etal-1994-phrase</bibkey>
    </paper>
    <paper id="9">
      <title>Automatic Sublanguage Identification for a New Text</title>
      <author><first>Satoshi</first><last>Sekine</last><affiliation>New York University</affiliation></author>
      <pages>109-120</pages>
      <abstract>A number of theoretical studies have been devoted to the notion of sublanguage, which mainly concerns linguistic phenomena restricted by the domain or context. Furthermore, there are some successful NLP systems which have explicitly or implicitly addressed the sublanguage restrictions (e.g. TAUM-METEO, ATR). This suggests the following two objectives for future NLP research: 1) automatic linguistic knowledge acquisition for sublanguage, and 2) automatic definition of sublanguage and identification of it for a new text. The two issues become realistic owing to the appearance of large corpora. Despite of the recent bloom of the research on the first objective, there are few on the second objective. If this objective is achieved, NLP systems will be able to optimize to the sublanguage before processing the text, and this will be a significant help in automatic processing. A preliminary experiment aiming at the second objective is addressed in this paper. It is conducted on about 3 MB of Wall Street Journal corpus. We made up article clusters (sublanguages) based on word appearance, and the closest article cluster among the set of clusters is chosen for each test article. The comparison between the new articles and the clusters shows the success of the sublanguage identification and also the promising ability of the method. Also the result of an experiment using the first two sentences in the articles indicates the feasibility of applying this method to speech recognition or other systems which can't access the whole article prior to the processing.</abstract>
      <url hash="9177b3ff">1994.vlc-1.9</url>
      <bibkey>sekine-1994-automatic</bibkey>
    </paper>
    <paper id="10">
      <title>String Comparison based on Substring Equations</title>
      <author><first>Kyoji</first><last>Umemura</last><affiliation>NTT</affiliation></author>
      <pages>121-128</pages>
      <abstract>This paper describes a practical method to compute whether two strings are equivalent under certain equations. This method uses a procedure called Critical-Pair/Completion. that generates rewriting rules from equations. Unlike other Critical-Pair/Completion procedures, the procedure described here always stops for all equations because it treats strings of bounded length. This paper also explains the importance of the string equivalence problem if international data handling is required.</abstract>
      <url hash="cc7f241c">1994.vlc-1.10</url>
      <bibkey>umemura-1994-string</bibkey>
    </paper>
    <paper id="11">
      <title>Bilingual Alignment and Tense</title>
      <author><first>Diana</first><last>Santos</last><affiliation>INESC</affiliation></author>
      <pages>129-142</pages>
      <abstract>In this paper, I describe one annotation of tense transfer in parallel English and Portuguese texts. Even though the primary aim of the study is to compare the tense and aspect systems of the two languages, it also raises some questions as far as bilingual alignment in general is concerned. First, I present a detailed list of clausal mismatches, which shows that intra-sentential alignment is not an easy task. Subsequently, I present a detailed quantitative description of the translation pairs found and discuss some possible conclusions for the translation of tense. Finally, I discuss some theoretical problems related to translation.</abstract>
      <url hash="50d9ea0d">1994.vlc-1.11</url>
      <bibkey>santos-1994-bilingual</bibkey>
    </paper>
    <paper id="12">
      <title>Comparative Discourse Analysis of Parallel Texts</title>
      <author><first>Pim</first><last>van der Eijk</last><affiliation>Digital Equipment Corporation</affiliation></author>
      <pages>143-160</pages>
      <abstract>A quantitative representation of discourse structure can be computed by measuring lexical cohesion relations among adjacent blocks of text. These representations have been proposed to deal with sub-topic text segmentation. In a parallel corpus, similar representations can be derived for versions of a text in various languages. These can be used for parallel segmentation and as an alternative measure of text-translation similarity.</abstract>
      <url hash="2b8457b8">1994.vlc-1.12</url>
      <bibkey>van-der-eijk-1994-comparative</bibkey>
    </paper>
  </volume>
</collection>
