<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.wat">
  <volume id="1" ingest-date="2023-10-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 10th Workshop on Asian Translation</booktitle>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Kazutaka</first><last>Kinugawa</last></editor>
      <editor><first>Hideya</first><last>Mino</last></editor>
      <editor><first>Isao</first><last>Goto</last></editor>
      <editor><first>Raj</first><last>Dabre</last></editor>
      <editor><first>Shohei</first><last>Higashiyama</last></editor>
      <editor><first>Shantipriya</first><last>Parida</last></editor>
      <editor><first>Makoto</first><last>Morishita</last></editor>
      <editor><first>Ondrej</first><last>Bojar</last></editor>
      <editor><first>Akiko</first><last>Eriguchi</last></editor>
      <editor><first>Yusuke</first><last>Oda</last></editor>
      <editor><first>Akiko</first><last>Eriguchi</last></editor>
      <editor><first>Chenhui</first><last>Chu</last></editor>
      <editor><first>Sadao</first><last>Kurohashi</last></editor>
      <publisher>Asia-Pacific Association for Machine Translation</publisher>
      <address>Macau SAR, China</address>
      <month>September</month>
      <year>2023</year>
      <url hash="c9767e32">2023.wat-1</url>
      <venue>wat</venue>
    </meta>
    <frontmatter>
      <url hash="e089603a">2023.wat-1.0</url>
      <bibkey>wat-2023-asian</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Overview of the 10th Workshop on <fixed-case>A</fixed-case>sian Translation</title>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Kazutaka</first><last>Kinugawa</last></author>
      <author><first>Hideya</first><last>Mino</last></author>
      <author><first>Isao</first><last>Goto</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Shohei</first><last>Higashiyama</last></author>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Akiko</first><last>Eriguchi</last></author>
      <author><first>Yusuke</first><last>Oda</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>1–28</pages>
      <abstract>This paper presents the results of the shared tasks from the 10th workshop on Asian translation (WAT2023). For the WAT2023, 2 teams submitted their translation results for the human evaluation. We also accepted 1 research paper. About 40 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated.</abstract>
      <url hash="83c1222a">2023.wat-1.1</url>
      <bibkey>nakazawa-etal-2023-overview</bibkey>
    </paper>
    <paper id="2">
      <title>Mitigating Domain Mismatch in Machine Translation via Paraphrasing</title>
      <author><first>Hyuga</first><last>Koretaka</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <pages>29–40</pages>
      <abstract>Quality of machine translation (MT) deteriorates significantly when translating texts having characteristics that differ from the training data, such as content domain. Although previous studies have focused on adapting MT models on a bilingual parallel corpus in the target domain, this approach is not applicable when no parallel data are available for the target domain or when utilizing black-box MT systems. To mitigate problems caused by such domain mismatch without relying on any corpus in the target domain, this study proposes a method to search for better translations by paraphrasing input texts of MT. To obtain better translations even for input texts from unforeknown domains, we generate their multiple paraphrases, translate each, and rerank the resulting translations to select the most likely one. Experimental results on Japanese-to-English translation reveal that the proposed method improves translation quality in terms of BLEU score for input texts from specific domains.</abstract>
      <url hash="95c558f4">2023.wat-1.2</url>
      <bibkey>koretaka-etal-2023-mitigating</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>BITS</fixed-case>-<fixed-case>P</fixed-case> at <fixed-case>WAT</fixed-case> 2023: Improving <fixed-case>I</fixed-case>ndic Language Multimodal Translation by Image Augmentation using Diffusion Models</title>
      <author><first>Amulya</first><last>Dash</last></author>
      <author><first>Hrithik Raj</first><last>Gupta</last></author>
      <author><first>Yashvardhan</first><last>Sharma</last></author>
      <pages>41–45</pages>
      <abstract>This paper describes the proposed system for mutlimodal machine translation. We have participated in multimodal translation tasks for English into three Indic languages: Hindi, Bengali, and Malayalam. We leverage the inherent richness of multimodal data to bridge the gap of ambiguity in translation. We fine-tuned the ‘No Language Left Behind’ (NLLB) machine translation model for multimodal translation, further enhancing the model accuracy by image data augmentation using latent diffusion. Our submission achieves the best BLEU score for English-Hindi, English-Bengali, and English-Malayalam language pairs for both Evaluation and Challenge test sets.</abstract>
      <url hash="1da3607a">2023.wat-1.3</url>
      <bibkey>dash-etal-2023-bits</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>O</fixed-case>dia<fixed-case>G</fixed-case>en<fixed-case>AI</fixed-case>’s Participation at <fixed-case>WAT</fixed-case>2023</title>
      <author><first>Sk</first><last>Shahid</last></author>
      <author><first>Guneet Singh</first><last>Kohli</last></author>
      <author><first>Sambit</first><last>Sekhar</last></author>
      <author><first>Debasish</first><last>Dhal</last></author>
      <author><first>Adit</first><last>Sharma</last></author>
      <author><first>Shubhendra</first><last>Kushwaha</last></author>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Stig-Arne</first><last>Grönroos</last></author>
      <author><first>Satya Ranjan</first><last>Dash</last></author>
      <pages>46–52</pages>
      <abstract>This paper offers an in-depth overview of the team “ODIAGEN’s” translation system submitted to the Workshop on Asian Translation (WAT2023). Our focus lies in the domain of Indic Multimodal tasks, specifically targeting English to Hindi, English to Malayalam, and English to Bengali translations. The system uses a state-of-the-art Transformer-based architecture, specifically the NLLB-200 model, fine-tuned with language-specific Visual Genome Datasets. With this robust system, we were able to manage both text-to-text and multimodal translations, demonstrating versatility in handling different translation modes. Our results showcase strong performance across the board, with particularly promising results in the Hindi and Bengali translation tasks. A noteworthy achievement of our system lies in its stellar performance across all text-to-text translation tasks. In the categories of English to Hindi, English to Bengali, and English to Malayalam translations, our system claimed the top positions for both the evaluation and challenge sets. This system not only advances our understanding of the challenges and nuances of Indic language translation but also opens avenues for future research to enhance translation accuracy and performance.</abstract>
      <url hash="20cd5eb5">2023.wat-1.4</url>
      <bibkey>shahid-etal-2023-odiagenais</bibkey>
    </paper>
  </volume>
</collection>
