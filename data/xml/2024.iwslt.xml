<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.iwslt">
  <volume id="1" ingest-date="2024-08-02" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)</booktitle>
      <editor><first>Elizabeth</first><last>Salesky</last></editor>
      <editor><first>Marcello</first><last>Federico</last></editor>
      <editor><first>Marine</first><last>Carpuat</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand (in-person and online)</address>
      <month>August</month>
      <year>2024</year>
      <url hash="b56115fe">2024.iwslt-1</url>
      <venue>iwslt</venue>
    </meta>
    <frontmatter>
      <url hash="7b51936e">2024.iwslt-1.0</url>
      <bibkey>iwslt-1-2024</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>FINDINGS</fixed-case> <fixed-case>OF</fixed-case> <fixed-case>THE</fixed-case> <fixed-case>IWSLT</fixed-case> 2024 <fixed-case>EVALUATION</fixed-case> <fixed-case>CAMPAIGN</fixed-case></title>
      <author><first>Ibrahim Said</first><last>Ahmad</last><affiliation>Northeastern U.</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>GMU</affiliation></author>
      <author><first>Ondřej</first><last>Bojar</last><affiliation>Charles U.</affiliation></author>
      <author><first>Claudia</first><last>Borg</last><affiliation>U. Malta</affiliation></author>
      <author><first>Marine</first><last>Carpuat</last><affiliation>UMD</affiliation></author>
      <author><first>Roldano</first><last>Cattoni</last><affiliation>FBK</affiliation></author>
      <author><first>Mauro</first><last>Cettolo</last><affiliation>FBK</affiliation></author>
      <author><first>William</first><last>Chen</last><affiliation>CMU</affiliation></author>
      <author><first>Qianqian</first><last>Dong</last><affiliation>Bytedance</affiliation></author>
      <author><first>Marcello</first><last>Federico</last><affiliation>Amazon</affiliation></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>U. Edinburgh</affiliation></author>
      <author><first>Dávid</first><last>Javorský</last><affiliation>Charles U.</affiliation></author>
      <author><first>Mateusz</first><last>Krubiński</last><affiliation>Charles U.</affiliation></author>
      <author><first>Tsz Kin</first><last>Lam</last><affiliation>U. Edinburgh</affiliation></author>
      <author><first>Xutai</first><last>Ma</last><affiliation>Meta</affiliation></author>
      <author><first>Prashant</first><last>Mathur</last><affiliation>Amazon</affiliation></author>
      <author><first>Evgeny</first><last>Matusov</last><affiliation>AppTek</affiliation></author>
      <author><first>Chandresh</first><last>Maurya</last><affiliation>IIT Indore</affiliation></author>
      <author><first>John</first><last>McCrae</last><affiliation>U. Galway</affiliation></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>JHU</affiliation></author>
      <author><first>Satoshi</first><last>Nakamura</last><affiliation>NAIST</affiliation></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>FBK</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>KIT</affiliation></author>
      <author><first>Xing</first><last>Niu</last><affiliation>Amazon</affiliation></author>
      <author><first>Atul Kr.</first><last>Ojha</last><affiliation>U. Galway</affiliation></author>
      <author><first>John</first><last>Ortega</last><affiliation>Northeastern</affiliation></author>
      <author><first>Sara</first><last>Papi</last><affiliation>FBK</affiliation></author>
      <author><first>Peter</first><last>Polák</last><affiliation>Charles U.</affiliation></author>
      <author><first>Adam</first><last>Pospíšil</last><affiliation>Charles U.</affiliation></author>
      <author><first>Pavel</first><last>Pecina</last><affiliation>Charles U.</affiliation></author>
      <author><first>Elizabeth</first><last>Salesky</last><affiliation>JHU</affiliation></author>
      <author><first>Nivedita</first><last>Sethiya</last><affiliation>IIT Indore</affiliation></author>
      <author><first>Balaram</first><last>Sarkar</last><affiliation>IIT Indore</affiliation></author>
      <author><first>Jiatong</first><last>Shi</last><affiliation>CMU</affiliation></author>
      <author><first>Claytone</first><last>Sikasote</last><affiliation>U. Zambia</affiliation></author>
      <author><first>Matthias</first><last>Sperber</last><affiliation>Apple</affiliation></author>
      <author><first>Sebastian</first><last>Stüker</last><affiliation>Zoom</affiliation></author>
      <author><first>Katsuhito</first><last>Sudoh</last><affiliation>NAIST</affiliation></author>
      <author><first>Brian</first><last>Thompson</last><affiliation>Amazon</affiliation></author>
      <author><first>Alex</first><last>Waibel</last><affiliation>CMU</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>CMU</affiliation></author>
      <author><first>Patrick</first><last>Wilken</last><affiliation>AppTek</affiliation></author>
      <author><first>Petr</first><last>Zemánek</last><affiliation>Charles U.</affiliation></author>
      <author><first>Rodolfo</first><last>Zevallos</last><affiliation>U. Pompeu Fabra</affiliation></author>
      <pages>1-11</pages>
      <abstract>This paper reports on the shared tasks organized by the 21st IWSLT Conference. The shared tasks address 7 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, dialect and low-resource speech translation, and Indic languages. The shared tasks attracted 17 teams whose submissions are documented in 27 system papers. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.</abstract>
      <url hash="02129690">2024.iwslt-1.1</url>
      <bibkey>ahmad-etal-2024-findings</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.1</doi>
    </paper>
    <paper id="2">
      <title>Pause-Aware Automatic Dubbing using <fixed-case>LLM</fixed-case> and Voice Cloning</title>
      <author><first>Yuang</first><last>Li</last><affiliation>Huawei</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Huawei</affiliation></author>
      <author><first>Ma</first><last>Miaomiao</last><affiliation>Huawei TSC</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Weidong</first><last>Zhang</last><affiliation>Huawei</affiliation></author>
      <author><first>Xianghui</first><last>He</last><affiliation>Huawei</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <pages>12-16</pages>
      <abstract>Automatic dubbing aims to translate the speech of a video into another language, ensuring the new speech naturally fits the original video. This paper details Huawei Translation Services Center’s (HW-TSC) submission for IWSLT 2024’s automatic dubbing task, under an unconstrained setting. Our system’s machine translation (MT) component utilizes a Transformer-based MT model and an LLM-based post-editor to produce translations of varying lengths. The text-to-speech (TTS) component employs a VITS-based TTS model and a voice cloning module to emulate the original speaker’s vocal timbre. For enhanced dubbing synchrony, we introduce a parsing-informed pause selector. Finally, we rerank multiple results based on lip-sync error distance (LSE-D) and character error rate (CER). Our system achieves LSE-D of 10.75 and 12.19 on subset1 and subset2 of DE-EN test sets respectively, superior to last year’s best system.</abstract>
      <url hash="c57c6450">2024.iwslt-1.2</url>
      <bibkey>li-etal-2024-pause</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>NICT</fixed-case>’s Cascaded and End-To-End Speech Translation Systems using Whisper and <fixed-case>I</fixed-case>ndic<fixed-case>T</fixed-case>rans2 for the <fixed-case>I</fixed-case>ndic Task</title>
      <author><first>Raj</first><last>Dabre</last><affiliation>NICT</affiliation></author>
      <author><first>Haiyue</first><last>Song</last><affiliation>Kyoto University</affiliation></author>
      <pages>17-22</pages>
      <abstract>This paper presents the NICT’s submission for the IWSLT 2024 Indic track, focusing on three speech-to-text (ST) translation directions: English to Hindi, Bengali, and Tamil. We aim to enhance translation quality in this low-resource scenario by integrating state-of-the-art pre-trained automated speech recognition (ASR) and text-to-text machine translation (MT) models. Our cascade system incorporates a Whisper model fine-tuned for ASR and an IndicTrans2 model fine-tuned for MT. Additionally, we propose an end-to-end system that combines a Whisper model for speech-to-text conversion with knowledge distilled from an IndicTrans2 MT model. We first fine-tune the IndicTrans2 model to generate pseudo data in Indic languages. This pseudo data, along with the original English speech data, is then used to fine-tune the Whisper model. Experimental results show that the cascaded system achieved a BLEU score of 51.0, outperforming the end-to-end model, which scored 19.1 BLEU. Moreover, the analysis indicates that applying knowledge distillation from the IndicTrans2 model to the end-to-end ST model improves the translation quality by about 0.7 BLEU.</abstract>
      <url hash="a57ece3c">2024.iwslt-1.3</url>
      <bibkey>dabre-song-2024-nicts</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.3</doi>
    </paper>
    <paper id="4">
      <title>Transforming <fixed-case>LLM</fixed-case>s into Cross-modal and Cross-lingual Retrieval Systems</title>
      <author><first>Frank</first><last>Palma Gomez</last><affiliation>Boston University</affiliation></author>
      <author><first>Ramon</first><last>Sanabria</last><affiliation>The University Of Edinburgh</affiliation></author>
      <author><first>Yun-hsuan</first><last>Sung</last><affiliation>Google</affiliation></author>
      <author><first>Daniel</first><last>Cer</last><affiliation>Google Research, University of California at Berkeley</affiliation></author>
      <author><first>Siddharth</first><last>Dalmia</last><affiliation>Google</affiliation></author>
      <author><first>Gustavo</first><last>Hernandez Abrego</last><affiliation>Google</affiliation></author>
      <pages>23-32</pages>
      <abstract>Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn’t require speech data during LLM pre-training and can exploit LLM’s multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.</abstract>
      <url hash="7874482b">2024.iwslt-1.4</url>
      <bibkey>palma-gomez-etal-2024-transforming</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.4</doi>
    </paper>
    <paper id="5">
      <title>Conditioning <fixed-case>LLM</fixed-case>s with Emotion in Neural Machine Translation</title>
      <author><first>Charles</first><last>Brazier</last><affiliation>Université de Bordeaux</affiliation></author>
      <author><first>Jean-Luc</first><last>Rouas</last><affiliation>LaBRI CNRS UMR 5800 Univ. Bordeaux</affiliation></author>
      <pages>33-38</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable performance in Natural Language Processing tasks, including Machine Translation (MT). In this work, we propose a novel MT pipeline that integrates emotion information extracted from a Speech Emotion Recognition (SER) model into LLMs to enhance translation quality. We first fine-tune five existing LLMs on the Libri-trans dataset and select the most performant model. Subsequently, we augment LLM prompts with different dimensional emotions and train the selected LLM under these different configurations. Our experiments reveal that integrating emotion information, especially arousal, into LLM prompts leads to notable improvements in translation quality.</abstract>
      <url hash="31e95ae7">2024.iwslt-1.5</url>
      <bibkey>brazier-rouas-2024-conditioning</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.5</doi>
    </paper>
    <paper id="6">
      <title>The <fixed-case>NYA</fixed-case>’s Offline Speech Translation System for <fixed-case>IWSLT</fixed-case> 2024</title>
      <author><first>Yingxin</first><last>Zhang</last><affiliation>NetEase YiDun AI Lab</affiliation></author>
      <author><first>Guodong</first><last>Ma</last><affiliation>NetEase YiDun AI Lab</affiliation></author>
      <author><first>Binbin</first><last>Du</last><affiliation>NetEase YiDun AI Lab</affiliation></author>
      <pages>39-45</pages>
      <abstract>This paper reports the NYA’s submissions to IWSLT 2024 Offline Speech Translation (ST) task on the sub-tasks including English to Chinese, Japanese, and German. In detail, we participate in the unconstrained training track using the cascaded ST structure. For the automatic speech recognition (ASR) model, we use the Whisper large-v3 model. For the neural machine translation (NMT) model, the wider and deeper Transformer is adapted as the backbone model. Furthermore, we use data augmentation technologies to augment training data and data filtering strategies to improve the quality of training data. In addition, we explore many MT technologies such as Back Translation, Forward Translation, R-Drop, and Domain Adaptation.</abstract>
      <url hash="6c53c08c">2024.iwslt-1.6</url>
      <bibkey>zhang-etal-2024-nyas</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.6</doi>
    </paper>
    <paper id="7">
      <title>Improving the Quality of <fixed-case>IWLST</fixed-case> 2024 Cascade Offline Speech Translation and Speech-to-Speech Translation via Translation Hypothesis Ensembling with <fixed-case>NMT</fixed-case> models and Large Language Models</title>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuanchang</first><last>Luo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>HuaweiTechnologiesCo.,Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <pages>46-52</pages>
      <abstract>This paper presents HW-TSC’s submission to the IWSLT 2024 Offline Speech Translation Task and Speech-to-Speech Translation Task. The former includes three translation directions: English to German, English to Chinese, and English to Japanese, while the latter only includes the translation direction of English to Chinese. We attend all three tracks (Constraint training, Constrained with Large Language Models training, and Unconstrained training) of offline speech translation task, using the cascade model architecture. Under the constrained training track, we train an ASR model from scratch, and then employ R-Drop and domain data selection to train the NMT model. In the constrained with Large Language Models training track, we use Wav2vec 2.0 and mBART50 for ASR model training initialization, and then train the LLama2-7B-based MT model using continuous training with sentence-aligned parallel data, supervised fine-tuning, and contrastive preference optimization. In the unconstrained training track, we fine-tune the whisper model for speech recognition, and then ensemble the translation results of NMT models and LLMs to produce superior translation output. For the speech-to-speech translation Task, we initially employ the offline speech translation system described above to generate the translated text. Then, we utilize the VITS model to generate the corresponding speech and employ the OpenVoice model for timbre cloning.</abstract>
      <url hash="806f94e5">2024.iwslt-1.7</url>
      <bibkey>wu-etal-2024-improving-quality</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Speech to Text Translation System for <fixed-case>IWSLT</fixed-case> 2024 in <fixed-case>I</fixed-case>ndic track</title>
      <author><first>Bin</first><last>Wei</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>HuaweiTechnologiesCo.,Ltd.</affiliation></author>
      <author><first>Yuanchang</first><last>Luo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yanfei</first><last>Jiang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <pages>53-56</pages>
      <abstract>This article introduces the process of HW-TSC and the results of IWSLT 2024 Indic Track Speech to Text Translation. We designed a cascade system consisting of an ASR model and a machine translation model to translate speech from one language to another. For the ASR part, we directly use whisper large v3 as our ASR model. Our main task is to optimize the machine translation model (en2ta, en2hi, en2bn). In the process of optimizing the translation model, we first use bilingual corpus to train the baseline model. Then we use monolingual data to construct pseudo-corpus data to further enhance the baseline model. Finally, we filter the parallel corpus data through the labse filtering method and finetune the model again, which can further improve the bleu value. We also selected domain data from bilingual corpus to finetune previous model to achieve the best results.</abstract>
      <url hash="b428d10c">2024.iwslt-1.8</url>
      <bibkey>wei-etal-2024-hw</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.8</doi>
    </paper>
    <paper id="9">
      <title>Multi-Model System for Effective Subtitling Compression</title>
      <author><first>Carol-Luca</first><last>Gasan</last><affiliation>Research Institute for Artificial Intelligence, Romanian Academy</affiliation></author>
      <author><first>Vasile</first><last>Păiș</last><affiliation>Research Institute for Artificial Intelligence, Romanian Academy</affiliation></author>
      <pages>57-64</pages>
      <abstract>This paper presents RACAI’s system used for the shared task of ‘Subtitling track: Subtitle Compression’ (the English to Spanish language direction), organized as part of ‘the 21st edition of The International Conference on Spoken Language Translation (IWSLT 2024)’. The proposed system consists of multiple models whose outputs are then ensembled using an algorithm, which has the purpose of maximizing the similarity of the initial and resulting text. We present the introduced datasets and the models’ training strategy, along with the reported results on the proposed test set.</abstract>
      <url hash="6c8523e7">2024.iwslt-1.9</url>
      <bibkey>gasan-pais-2024-multi</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>FBK</fixed-case>@<fixed-case>IWSLT</fixed-case> Test Suites Task: Gender Bias evaluation with <fixed-case>M</fixed-case>u<fixed-case>ST</fixed-case>-<fixed-case>SHE</fixed-case></title>
      <author><first>Beatrice</first><last>Savoldi</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Marco</first><last>Gaido</last><affiliation>Fondazione Bruno Kessler, University of Trento</affiliation></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Luisa</first><last>Bentivogli</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>65-71</pages>
      <abstract>This paper presents the FBK contribution to the IWSLT-2024 ‘Test suites’ shared subtask, part of the Offline Speech Translation Task. Our contribution consists of the MuST-SHE-IWSLT24 benchmark evaluation, designed to assess gender bias in speech translation. By focusing on the en-de language pair, we rely on a newly created test suite to investigate systems’ ability to correctly translate feminine and masculine gender. Our results indicate that – under realistic conditions – current ST systems achieve reasonable and comparable performance in correctly translating both feminine and masculine forms when contextual gender information is available. For ambiguous references to the speaker, however, we attest a consistent preference towards masculine gender, thus calling for future endeavours on the topic. Towards this goal we make MuST-SHE-IWSLT24 freely available at: https://mt.fbk.eu/must-she/</abstract>
      <url hash="3bbe0947">2024.iwslt-1.10</url>
      <bibkey>savoldi-etal-2024-fbk</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>S</fixed-case>imul<fixed-case>S</fixed-case>eamless: <fixed-case>FBK</fixed-case> at <fixed-case>IWSLT</fixed-case> 2024 Simultaneous Speech Translation</title>
      <author><first>Sara</first><last>Papi</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Marco</first><last>Gaido</last><affiliation>Fondazione Bruno Kessler, University of Trento</affiliation></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Luisa</first><last>Bentivogli</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>72-79</pages>
      <abstract>This paper describes the FBK’s participation in the Simultaneous Translation Evaluation Campaign at IWSLT 2024. For this year’s submission in the speech-to-text translation (ST) sub-track, we propose SimulSeamless, which is realized by combining AlignAtt and SeamlessM4T in its medium configuration. The SeamlessM4T model is used ‘off-the-shelf’ and its simultaneous inference is enabled through the adoption of AlignAtt, a SimulST policy based on cross-attention that can be applied without any retraining or adaptation of the underlying model for the simultaneous task. We participated in all the Shared Task languages (English-&gt;German, Japanese, Chinese, and Czech-&gt;English), achieving acceptable or even better results compared to last year’s submissions. SimulSeamless, covering more than 143 source languages and 200 target languages, is released at: https://github.com/hlt-mt/FBK-fairseq/.</abstract>
      <url hash="5f529912">2024.iwslt-1.11</url>
      <bibkey>papi-etal-2024-simulseamless</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.11</doi>
    </paper>
    <paper id="12">
      <title>The <fixed-case>SETU</fixed-case>-<fixed-case>DCU</fixed-case> Submissions to <fixed-case>IWSLT</fixed-case> 2024 Low-Resource Speech-to-Text Translation Tasks</title>
      <author><first>Maria</first><last>Zafar</last><affiliation>SETU Carlow</affiliation></author>
      <author><first>Antonio</first><last>Castaldo</last><affiliation>ADAPT DCU</affiliation></author>
      <author><first>Prashanth</first><last>Nayak</last><affiliation>DCU ADAPT</affiliation></author>
      <author><first>Rejwanul</first><last>Haque</last><affiliation>SETU Carlow</affiliation></author>
      <author><first>Neha</first><last>Gajakos</last><affiliation>DCU ADAPT</affiliation></author>
      <author><first>Andy</first><last>Way</last><affiliation>DCU ADAPT</affiliation></author>
      <pages>80-85</pages>
      <abstract>Natural Language Processing (NLP) research and development has experienced rapid progression in the recent times due to advances in deep learning. The introduction of pre-trained large language models (LLMs) is at the core of this transformation, significantly enhancing the performance of machine translation (MT) and speech technologies. This development has also led to fundamental changes in modern translation and speech tools and their methodologies. However, there remain challenges when extending this progress to underrepresented dialects and low-resource languages, primarily due to the need for more data. This paper details our submissions to the IWSLT speech translation (ST) tasks. We used the Whisper model for the automatic speech recognition (ASR) component. We then used mBART and NLLB as cascaded systems for utilising their MT capabilities. Our research primarily focused on exploring various dialects of low-resource languages and harnessing existing resources from linguistically related languages. We conducted our experiments for two morphologically diverse language pairs: Irish-to-English and Maltese-to-English. We used BLEU, chrF and COMET for evaluating our MT models.</abstract>
      <url hash="5f441bd3">2024.iwslt-1.12</url>
      <bibkey>zafar-etal-2024-setu</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.12</doi>
    </paper>
    <paper id="13">
      <title>Automatic Subtitling and Subtitle Compression: <fixed-case>FBK</fixed-case> at the <fixed-case>IWSLT</fixed-case> 2024 Subtitling track</title>
      <author><first>Marco</first><last>Gaido</last><affiliation>Fondazione Bruno Kessler, University of Trento</affiliation></author>
      <author><first>Sara</first><last>Papi</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Mauro</first><last>Cettolo</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Roldano</first><last>Cattoni</last><affiliation>FBK</affiliation></author>
      <author><first>Andrea</first><last>Piergentili</last><affiliation>Fondazione Bruno Kessler, University of Trento</affiliation></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Luisa</first><last>Bentivogli</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>86-96</pages>
      <abstract>The paper describes the FBK submissions to the Subtitling track of the 2024 IWSLT Evaluation Campaign, which covers both the Automatic Subtitling and the Subtitle Compression task for two language pairs: English to German (en-de) and English to Spanish (en-es). For the Automatic Subtitling task, we submitted two systems: i) a direct model, trained in constrained conditions, that produces the SRT files from the audio without intermediate outputs (e.g., transcripts), and ii) a cascade solution that integrates only free-to-use components, either taken off-the-shelf or developed in-house. Results show that, on both language pairs, our direct model outperforms both cascade and direct systems trained in constrained conditions in last year’s edition of the campaign, while our cascade solution is competitive with the best 2023 runs. For the Subtitle Compression task, our primary submission involved prompting a Large Language Model (LLM) in zero-shot mode to shorten subtitles that exceed the reading speed limit of 21 characters per second. Our results highlight the challenges inherent in shrinking out-of-context sentence fragments that are automatically generated and potentially error-prone, underscoring the need for future studies to develop targeted solutions.</abstract>
      <url hash="aff7193f">2024.iwslt-1.13</url>
      <bibkey>gaido-etal-2024-automatic</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>UM</fixed-case> <fixed-case>IWSLT</fixed-case> 2024 Low-Resource Speech Translation: Combining <fixed-case>M</fixed-case>altese and <fixed-case>N</fixed-case>orth <fixed-case>L</fixed-case>evantine <fixed-case>A</fixed-case>rabic</title>
      <author><first>Sara</first><last>Nabhani</last><affiliation>University of Malta</affiliation></author>
      <author><first>Aiden</first><last>Williams</last><affiliation>University of Malta</affiliation></author>
      <author><first>Miftahul</first><last>Jannat</last><affiliation>University of Malta</affiliation></author>
      <author><first>Kate</first><last>Rebecca Belcher</last><affiliation>University of Malta</affiliation></author>
      <author><first>Melanie</first><last>Galea</last><affiliation>University of Malta</affiliation></author>
      <author><first>Anna</first><last>Taylor</last><affiliation>University of Malta</affiliation></author>
      <author><first>Kurt</first><last>Micallef</last><affiliation>University of Malta</affiliation></author>
      <author><first>Claudia</first><last>Borg</last><affiliation>University of Malta</affiliation></author>
      <pages>97-107</pages>
      <abstract>The IWSLT low-resource track encourages innovation in the field of speech translation, particularly in data-scarce conditions. This paper details our submission for the IWSLT 2024 low-resource track shared task for Maltese-English and North Levantine Arabic-English spoken language translation using an unconstrained pipeline approach. Using language models, we improve ASR performance by correcting the produced output. We present a 2 step approach for MT using data from external sources showing improvements over baseline systems. We also explore transliteration as a means to further augment MT data and exploit the cross-lingual similarities between Maltese and Arabic.</abstract>
      <url hash="700cae17">2024.iwslt-1.14</url>
      <bibkey>nabhani-etal-2024-um</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>UOM</fixed-case>-Constrained <fixed-case>IWSLT</fixed-case> 2024 Shared Task Submission - <fixed-case>M</fixed-case>altese Speech Translation</title>
      <author><first>Kurt</first><last>Abela</last><affiliation>University of Malta</affiliation></author>
      <author><first>Md</first><last>Abdur Razzaq Riyadh</last><affiliation>University of Malta</affiliation></author>
      <author><first>Melanie</first><last>Galea</last><affiliation>University of Malta</affiliation></author>
      <author><first>Alana</first><last>Busuttil</last><affiliation>University of Malta</affiliation></author>
      <author><first>Roman</first><last>Kovalev</last><affiliation>University of Malta</affiliation></author>
      <author><first>Aiden</first><last>Williams</last><affiliation>University of Malta</affiliation></author>
      <author><first>Claudia</first><last>Borg</last><affiliation>University of Malta</affiliation></author>
      <pages>108-113</pages>
      <abstract>This paper presents our IWSLT-2024 shared task submission on the low-resource track. This submission forms part of the constrained setup; implying limited data for training. Following the introduction, this paper consists of a literature review defining previous approaches to speech translation, as well as their application to Maltese, followed by the defined methodology, evaluation and results, and the conclusion. A cascaded submission on the Maltese to English language pair is presented; consisting of a pipeline containing: a DeepSpeech 1 Automatic Speech Recognition (ASR) system, a KenLM model to optimise the transcriptions, and finally an LSTM machine translation model. The submission achieves a 0.5 BLEU score on the overall test set, and the ASR system achieves a word error rate of 97.15%. Our code is made publicly available.</abstract>
      <url hash="57acdd1c">2024.iwslt-1.15</url>
      <bibkey>abela-etal-2024-uom</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.15</doi>
    </paper>
    <paper id="16">
      <title>Compact Speech Translation Models via Discrete Speech Units Pretraining</title>
      <author><first>Tsz Kin</first><last>Lam</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>114-124</pages>
      <abstract>We propose a pretraining method to use Self-Supervised Speech (SSS) model to creating more compact Speech-to-text Translation. In contrast to using the SSS model for initialization, our method is more suitable to memory constrained scenario such as on-device deployment. Our method is based on Discrete Speech Units (DSU) extracted from the SSS model. In the first step, our method pretrains two smaller encoder-decoder models on 1) Filterbank-to-DSU (Fbk-to-DSU) and 2) DSU-to-Translation (DSU-to-Trl) data respectively. The DSU thus become the distillation inputs of the smaller models. Subsequently, the encoder from the Fbk-to-DSU model and the decoder from the DSU-to-Trl model are taken to initialise the compact model. Finally, the compact model is finetuned on the paired Fbk-Trl data. In addition to being compact, our method requires no transcripts, making it applicable to low-resource settings. It also avoids speech discretization in inference and is more robust to the DSU tokenization. Evaluation on CoVoST-2 (X-En) shows that our method has consistent improvement over the baseline in three metrics while being compact i.e., only half the SSS model size.</abstract>
      <url hash="9bd6cb84">2024.iwslt-1.16</url>
      <bibkey>kin-lam-etal-2024-compact</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>QUESPA</fixed-case> Submission for the <fixed-case>IWSLT</fixed-case> 2024 Dialectal and Low-resource Speech Translation Task</title>
      <author><first>John</first><last>E. Ortega</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Rodolfo</first><last>Joel Zevallos</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author><first>Ibrahim</first><last>Said Ahmad</last><affiliation>Northeastern University</affiliation></author>
      <author><first>William</first><last>Chen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>125-133</pages>
      <abstract>This article describes the QUESPA team speech translation (ST) submissions for the Quechua to Spanish (QUE–SPA) track featured in the Evaluation Campaign of IWSLT 2024: dialectal and low-resource speech translation. Two main submission types were supported in the campaign: constrained and unconstrained. This is our second year submitting our ST systems to the IWSLT shared task and we feel that we have achieved novel performance, surpassing last year’s submissions. Again, we were able to submit six total systems of which our best (primary) constrained system consisted of an ST model based on the Fairseq S2T framework where the audio representations were created using log mel-scale filter banks as features and the translations were performed using a transformer. The system was similar to last year’s submission with slight configuration changes, allowing us to achieve slightly higher performance (2 BLEU). Contrastingly, we were able to achieve much better performance than last year on the unconstrained task using a larger pre-trained language (PLM) model for ST (without cascading) and the inclusion of parallel QUE–SPA data found on the internet. The fine-tuning of Microsoft’s SpeechT5 model in a ST setting along with the addition of new data and a data augmentation technique allowed us to achieve 19.7 BLEU. Additionally, we present the other four submissions (2 constrained and 2 unconstrained) which are part of additional efforts of hyper-parameter and configuration tuning on existent models and the inclusion of Whisper for speech recognition</abstract>
      <url hash="1ece0584">2024.iwslt-1.17</url>
      <bibkey>e-ortega-etal-2024-quespa</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.17</doi>
    </paper>
    <paper id="18">
      <title>Speech Data from Radio Broadcasts for Low Resource Languages</title>
      <author><first>Bismarck</first><last>Bamfo Odoom</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Leibny</first><last>Paola Garcia Perera</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Prangthip</first><last>Hansanti</last><affiliation>Meta</affiliation></author>
      <author><first>Loic</first><last>Barrault</last><affiliation>Meta AI</affiliation></author>
      <author><first>Christophe</first><last>Ropers</last><affiliation>Meta</affiliation></author>
      <author><first>Matthew</first><last>Wiesner</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Alexandre</first><last>Mourachko</last><affiliation>Meta AI</affiliation></author>
      <author><first>Philipp</first><last>Koehn</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>134-139</pages>
      <abstract>We created a collection of speech data for 48 low resource languages. The corpus is extracted from radio broadcasts and processed with novel speech detection and language identification models based on a manually vetted subset of the audio for 10 languages. The data is made publicly available.</abstract>
      <url hash="a1ee108e">2024.iwslt-1.18</url>
      <bibkey>bamfo-odoom-etal-2024-speech</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>JHU</fixed-case> <fixed-case>IWSLT</fixed-case> 2024 Dialectal and Low-resource System Description</title>
      <author><first>Nathaniel</first><last>Romney Robinson</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Kaiser</first><last>Sun</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Cihan</first><last>Xiao</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Niyati</first><last>Bafna</last><affiliation>Center for Language and Speech Processing, Johns Hopkins University</affiliation></author>
      <author><first>Weiting</first><last>Tan</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Haoran</first><last>Xu</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Henry</first><last>Li Xinyuan</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Ankur</first><last>Kejriwal</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Sanjeev</first><last>Khudanpur</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Paul</first><last>McNamee</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>140-153</pages>
      <abstract>Johns Hopkins University (JHU) submitted systems for all eight language pairs in the 2024 Low-Resource Language Track. The main effort of this work revolves around fine-tuning large and publicly available models in three proposed systems: i) end-to-end speech translation (ST) fine-tuning of Seamless4MT v2; ii) ST fine-tuning of Whisper; iii) a cascaded system involving automatic speech recognition with fine-tuned Whisper and machine translation with NLLB. On top of systems above, we conduct a comparative analysis on different training paradigms, such as intra-distillation for NLLB as well as joint training and curriculum learning for SeamlessM4T v2. Our results show that the best-performing approach differs by language pairs, but that i) fine-tuned SeamlessM4T v2 tends to perform best for source languages on which it was pre-trained, ii) multi-task training helps Whisper fine-tuning, iii) cascaded systems with Whisper and NLLB tend to outperform Whisper alone, and iv) intra-distillation helps NLLB fine-tuning.</abstract>
      <url hash="e8bd625d">2024.iwslt-1.19</url>
      <bibkey>romney-robinson-etal-2024-jhu</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>CMU</fixed-case>’s <fixed-case>IWSLT</fixed-case> 2024 Simultaneous Speech Translation System</title>
      <author><first>Xi</first><last>Xu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Siqi</first><last>Ouyang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Brian</first><last>Yan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Patrick</first><last>Fernandes</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>William</first><last>Chen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>154-159</pages>
      <abstract>This paper describes CMU’s submission to the IWSLT 2024 Simultaneous Speech Translation (SST) task for translating English speech to German text in a streaming manner. Our end-to-end speech-to-text (ST) system integrates the WavLM speech encoder, a modality adapter, and the Llama2-7B-Base model as the decoder. We employ a two-stage training approach: initially, we align the representations of speech and text, followed by full fine-tuning. Both stages are trained on MuST-c v2 data with cross-entropy loss. We adapt our offline ST model for SST using a simple fixed hold-n policy. Experiments show that our model obtains an offline BLEU score of 31.1 and a BLEU score of 29.5 under 2 seconds latency on the MuST-C-v2 tst-COMMON.</abstract>
      <url hash="23c12ae1">2024.iwslt-1.20</url>
      <bibkey>xu-etal-2024-cmus</bibkey>
      <revision id="1" href="2024.iwslt-1.20v1" hash="fd2a633a"/>
      <revision id="2" href="2024.iwslt-1.20v2" hash="23c12ae1" date="2024-08-17">Author updates.</revision>
      <doi>10.18653/v1/2024.iwslt-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Submissions To the <fixed-case>IWSLT</fixed-case>2024 Low-resource Speech Translation Tasks</title>
      <author><first>Zheng</first><last>Jiawei</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>HuaweiTechnologiesCo.,Ltd.</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Bin</first><last>Wei</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Yuanchang</first><last>Luo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <pages>160-163</pages>
      <abstract>In this work, we submitted our systems to the low-resource track of the IWSLT 2024 Speech Translation Campaign. Our systems tackled the unconstrained condition of the Dialectal Arabic North Levantine (ISO-3 code: apc) to English language pair. We proposed a cascaded solution consisting of an automatic speech recognition (ASR) model and a machine translation (MT) model. It was noted that the ASR model employed the pre-trained Whisper-large-v3 model to process the speech data, while the MT model adopted the Transformer architecture. To improve the quality of the MT model, it was stated that our system utilized not only the data provided by the competition but also an additional 54 million parallel sentences. Ultimately, we reported that our final system achieved a BLEU score of 24.7 for apc-to-English translation.</abstract>
      <url hash="2832edfe">2024.iwslt-1.21</url>
      <bibkey>jiawei-etal-2024-hw</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.21</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>CMU</fixed-case>’s <fixed-case>IWSLT</fixed-case> 2024 Offline Speech Translation System: A Cascaded Approach For Long-Form Robustness</title>
      <author><first>Brian</first><last>Yan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Patrick</first><last>Fernandes</last><affiliation>Carnegie Mellon University, Instituto de Telecomunicações</affiliation></author>
      <author><first>Jinchuan</first><last>Tian</last><affiliation>Peking University</affiliation></author>
      <author><first>Siqi</first><last>Ouyang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>William</first><last>Chen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Karen</first><last>Livescu</last><affiliation>TTI-Chicago</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>164-169</pages>
      <abstract>This work describes CMU’s submission to the IWSLT 2024 Offline Speech Translation (ST) Shared Task for translating English speech to German, Chinese, and Japanese text. We are the first participants to employ a long-form strategy which directly processes unsegmented recordings without the need for a separate voice-activity detection stage (VAD). We show that the Whisper automatic speech recognition (ASR) model has a hallucination problem when applied out-of-the-box to recordings containing non-speech noises, but a simple noisy fine-tuning approach can greatly enhance Whisper’s long-form robustness across multiple domains. Then, we feed English ASR outputs into fine-tuned NLLB machine translation (MT) models which are decoded using COMET-based Minimum Bayes Risk. Our VAD-free ASR+MT cascade is tested on TED talks, TV series, and workout videos and shown to outperform prior winning IWSLT submissions and large open-source models.</abstract>
      <url hash="66c6e06d">2024.iwslt-1.22</url>
      <bibkey>yan-etal-2024-cmus</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.22</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>NAIST</fixed-case> Simultaneous Speech Translation System for <fixed-case>IWSLT</fixed-case> 2024</title>
      <author><first>Yuka</first><last>Ko</last><affiliation>NAIST</affiliation></author>
      <author><first>Ryo</first><last>Fukuda</last><affiliation>NAIST</affiliation></author>
      <author><first>Yuta</first><last>Nishikawa</last><affiliation>NAIST</affiliation></author>
      <author><first>Yasumasa</first><last>Kano</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Tomoya</first><last>Yanagita</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Kosuke</first><last>Doi</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Mana</first><last>Makinae</last><affiliation>NAIST</affiliation></author>
      <author><first>Haotian</first><last>Tan</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Makoto</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Sakriani</first><last>Sakti</last><affiliation>JAIST/NAIST</affiliation></author>
      <author><first>Katsuhito</first><last>Sudoh</last><affiliation>Nara Women’s University</affiliation></author>
      <author><first>Satoshi</first><last>Nakamura</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>170-182</pages>
      <abstract>This paper describes NAIST’s submission to the simultaneous track of the IWSLT 2024 Evaluation Campaign: English-to-German, Japanese, Chinese speech-to-text translation and English-to-Japanese speech-to-speech translation. We develop a multilingual end-to-end speech-to-text translation model combining two pre-trained language models, HuBERT and mBART. We trained this model with two decoding policies, Local Agreement (LA) and AlignAtt. The submitted models employ the LA policy because it outperformed the AlignAtt policy in previous models. Our speech-to-speech translation method is a cascade of the above speech-to-text model and an incremental text-to-speech (TTS) module that incorporates a phoneme estimation model, a parallel acoustic model, and a parallel WaveGAN vocoder. We improved our incremental TTS by applying the Transformer architecture with the AlignAtt policy for the estimation model. The results show that our upgraded TTS module contributed to improving the system performance.</abstract>
      <url hash="a7c924b7">2024.iwslt-1.23</url>
      <bibkey>ko-etal-2024-naist</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.23</doi>
    </paper>
    <paper id="24">
      <title>Blending <fixed-case>LLM</fixed-case>s into Cascaded Speech Translation: <fixed-case>KIT</fixed-case>’s Offline Speech Translation System for <fixed-case>IWSLT</fixed-case> 2024</title>
      <author><first>Sai</first><last>Koneru</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Thai</first><last>Binh Nguyen</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Ngoc-Quan</first><last>Pham</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Zhaolin</first><last>Li</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Alexander</first><last>Waibel</last><affiliation>Carnegie Mellon</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>Karlsruhe Institut of Technology</affiliation></author>
      <pages>183-191</pages>
      <abstract>Large Language Models (LLMs) are currently under exploration for various tasks, including Automatic Speech Recognition (ASR), Machine Translation (MT), and even End-to-End Speech Translation (ST). In this paper, we present KIT’s offline submission in the constrained + LLM track by incorporating recently proposed techniques that can be added to any cascaded speech translation. Specifically, we integrate Mistral-7B into our system to enhance it in two ways. Firstly, we refine the ASR outputs by utilizing the N-best lists generated by our system and fine-tuning the LLM to predict the transcript accurately. Secondly, we refine the MT outputs at the document level by fine-tuning the LLM, leveraging both ASR and MT predictions to improve translation quality. We find that integrating the LLM into the ASR and MT systems results in an absolute improvement of 0.3% in Word Error Rate and 0.65% in COMET for tst2019 test set. In challenging test sets with overlapping speakers and background noise, we find that integrating LLM is not beneficial due to poor ASR performance. Here, we use ASR with chunked long-form decoding to improve context usage that may be unavailable when transcribing with Voice Activity Detection segmentation alone.</abstract>
      <url hash="93354545">2024.iwslt-1.24</url>
      <bibkey>koneru-etal-2024-blending</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>ALADAN</fixed-case> at <fixed-case>IWSLT</fixed-case>24 Low-resource <fixed-case>A</fixed-case>rabic Dialectal Speech Translation Task</title>
      <author><first>Waad</first><last>Ben Kheder</last><affiliation>Vocapia Research</affiliation></author>
      <author><first>Josef</first><last>Jon</last><affiliation>Charles University</affiliation></author>
      <author><first>André</first><last>Beyer</last><affiliation>Crowdee</affiliation></author>
      <author><first>Abdel</first><last>Messaoudi</last><affiliation>Vocapia</affiliation></author>
      <author><first>Rabea</first><last>Affan</last><affiliation>Mr.</affiliation></author>
      <author><first>Claude</first><last>Barras</last><affiliation>Vocapia Research</affiliation></author>
      <author><first>Maxim</first><last>Tychonov</last><affiliation>Lingea ltd.</affiliation></author>
      <author><first>Jean-Luc</first><last>Gauvain</last><affiliation>CNRS/LIMSI</affiliation></author>
      <pages>192-202</pages>
      <abstract>This paper presents ALADAN’s approach to the IWSLT 2024 Dialectal and Low-resource shared task, focusing on Levantine Arabic (apc) and Tunisian Arabic (aeb) to English speech translation (ST). Addressing challenges such as the lack of standardized orthography and limited training data, we propose a solution for data normalization in Dialectal Arabic, employing a modified Levenshtein distance and Word2vec models to find orthographic variants of the same word. Our system consists of a cascade ST system integrating two ASR systems (TDNN-F and Zipformer) and two NMT modules derived from pre-trained models (NLLB-200 1.3B distilled model and CohereAI’s Command-R). Additionally, we explore the integration of unsupervised textual and audio data, highlighting the importance of multi-dialectal datasets for both ASR and NMT tasks. Our system achieves BLEU score of 31.5 for Levantine Arabic on the official validation set.</abstract>
      <url hash="774906f8">2024.iwslt-1.25</url>
      <bibkey>ben-kheder-etal-2024-aladan</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.25</doi>
    </paper>
    <paper id="26">
      <title>Enhancing Translation Accuracy of Large Language Models through Continual Pre-Training on Parallel Data</title>
      <author><first>Minato</first><last>Kondo</last><affiliation>University of Tsukuba</affiliation></author>
      <author><first>Takehito</first><last>Utsuro</last><affiliation>University of Tsukuba</affiliation></author>
      <author><first>Masaaki</first><last>Nagata</last><affiliation>NTT Corporation</affiliation></author>
      <pages>203-220</pages>
      <abstract>In this paper, we propose a two-phase training approach where pre-trained large language models are continually pre-trained on parallel data and then supervised fine-tuned with a small amount of high-quality parallel data. To investigate the effectiveness of our proposed approach, we conducted continual pre-training with a 3.8B-parameter model and parallel data across eight different formats. We evaluate these methods on thirteen test sets for Japanese-to-English and English-to-Japanese translation. The results demonstrate that when utilizing parallel data in continual pre-training, it is essential to alternate between source and target sentences. Additionally, we demonstrated that the translation accuracy improves only for translation directions where the order of source and target sentences aligns between continual pre-training data and inference. In addition, we demonstrate that the LLM-based translation model is more robust in translating spoken language and achieves higher accuracy with less training data compared to supervised encoder-decoder models. We also show that the highest accuracy is achieved when the data for continual pre-training consists of interleaved source and target sentences and when tags are added to the source sentences.</abstract>
      <url hash="069c10ac">2024.iwslt-1.26</url>
      <bibkey>kondo-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.26</doi>
    </paper>
    <paper id="27">
      <title>The <fixed-case>KIT</fixed-case> Speech Translation Systems for <fixed-case>IWSLT</fixed-case> 2024 Dialectal and Low-resource Track</title>
      <author><first>Zhaolin</first><last>Li</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Enes</first><last>Yavuz Ugan</last><affiliation>KIT</affiliation></author>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Carlos</first><last>Mullov</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Tu</first><last>Anh Dinh</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Sai</first><last>Koneru</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Alexander</first><last>Waibel</last><affiliation>Carnegie Mellon</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>Karlsruhe Institut of Technology</affiliation></author>
      <pages>221-228</pages>
      <abstract>This paper presents KIT’s submissions to the IWSLT 2024 dialectal and low-resource track. In this work, we build systems for translating into English from speech in Maltese, Bemba, and two Arabic dialects Tunisian and North Levantine. Under the unconstrained condition, we leverage the pre-trained multilingual models by fine-tuning them for the target language pairs to address data scarcity problems in this track. We build cascaded and end-to-end speech translation systems for different language pairs and show the cascaded system brings slightly better overall performance. Besides, we find utilizing additional data resources boosts speech recognition performance but slightly harms machine translation performance in cascaded systems. Lastly, we show that Minimum Bayes Risk is effective in improving speech translation performance by combining the cascaded and end-to-end systems, bringing a consistent improvement of around 1 BLUE point.</abstract>
      <url hash="5006a5d5">2024.iwslt-1.27</url>
      <bibkey>li-etal-2024-kit</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.27</doi>
    </paper>
    <paper id="28">
      <title>Empowering Low-Resource Language Translation: Methodologies for <fixed-case>B</fixed-case>hojpuri-<fixed-case>H</fixed-case>indi and <fixed-case>M</fixed-case>arathi-<fixed-case>H</fixed-case>indi <fixed-case>ASR</fixed-case> and <fixed-case>MT</fixed-case></title>
      <author><first>Harpreet</first><last>Singh Anand</last><affiliation>Birla Institute of Technology and Science, Pilani - Pilani Campus</affiliation></author>
      <author><first>Amulya</first><last>Ratna Dash</last><affiliation>Birla Institute of Technology and Science, Pilani - Pilani Campus</affiliation></author>
      <author><first>Yashvardhan</first><last>Sharma</last><affiliation>Birla Institute of Technology and Science, Pilani - Pilani Campus</affiliation></author>
      <pages>229-234</pages>
      <abstract>The paper describes our submission for the unconstrained track of ‘Dialectal and Low-Resource Task’ proposed in IWSLT-2024. We designed cascaded Speech Translation systems for the language pairs Marathi-Hindi and Bhojpuri-Hindi utilising and fine-tuning different pre-trained models for carrying out Automatic Speech Recognition (ASR) and Machine Translation (MT).</abstract>
      <url hash="28d5bc7e">2024.iwslt-1.28</url>
      <bibkey>singh-anand-etal-2024-empowering</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.28</doi>
    </paper>
    <paper id="29">
      <title>Recent Highlights in Multilingual and Multimodal Speech Translation</title>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>Karlsruhe Institut of Technology</affiliation></author>
      <pages>235-253</pages>
      <abstract>Speech translation has witnessed significant progress driven by advancements in modeling techniques and the growing availability of training data. In this paper, we highlight recent advances in two ongoing research directions in ST: scaling the models to 1) many translation directions (multilingual ST) and 2) beyond the text output modality (multimodal ST). We structure this review by examining the sequential stages of a model’s development lifecycle: determining training resources, selecting model architecture, training procedures, evaluation metrics, and deployment considerations. We aim to highlight recent developments in each stage, with a particular focus on model architectures (dedicated speech translation models and LLM-based general-purpose model) and training procedures (task-specific vs. task-invariant approaches). Based on the reviewed advancements, we identify and discuss ongoing challenges within the field of speech translation.</abstract>
      <url hash="586bde74">2024.iwslt-1.29</url>
      <bibkey>liu-niehues-2024-recent</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.29</doi>
    </paper>
    <paper id="30">
      <title>Word Order in <fixed-case>E</fixed-case>nglish-<fixed-case>J</fixed-case>apanese Simultaneous Interpretation: Analyses and Evaluation using Chunk-wise Monotonic Translation</title>
      <author><first>Kosuke</first><last>Doi</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Yuka</first><last>Ko</last><affiliation>NAIST</affiliation></author>
      <author><first>Mana</first><last>Makinae</last><affiliation>NAIST</affiliation></author>
      <author><first>Katsuhito</first><last>Sudoh</last><affiliation>Nara Women’s University</affiliation></author>
      <author><first>Satoshi</first><last>Nakamura</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>254-264</pages>
      <abstract>This paper analyzes the features of monotonic translations, which follow the word order of the source language, in simultaneous interpreting (SI). Word order differences are one of the biggest challenges in SI, especially for language pairs with significant structural differences like English and Japanese. We analyzed the characteristics of chunk-wise monotonic translation (CMT) sentences using the NAIST English-to-Japanese Chunk-wise Monotonic Translation Evaluation Dataset and identified some grammatical structures that make monotonic translation difficult in English-Japanese SI. We further investigated the features of CMT sentences by evaluating the output from the existing speech translation (ST) and simultaneous speech translation (simulST) models on the NAIST English-to-Japanese Chunk-wise Monotonic Translation Evaluation Dataset as well as on existing test sets. The results indicate the possibility that the existing SI-based test set underestimates the model performance. The results also suggest that using CMT sentences as references gives higher scores to simulST models than ST models, and that using an offline-based test set to evaluate the simulST models underestimates the model performance.</abstract>
      <url hash="73ecda09">2024.iwslt-1.30</url>
      <bibkey>doi-etal-2024-word</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.30</doi>
    </paper>
    <paper id="31">
      <title>Leveraging Synthetic Audio Data for End-to-End Low-Resource Speech Translation</title>
      <author><first>Yasmin</first><last>Moslem</last><affiliation>N/A</affiliation></author>
      <pages>265-273</pages>
      <abstract>This paper describes our system submission to the International Conference on Spoken Language Translation (IWSLT 2024) for Irish-to-English speech translation. We built end-to-end systems based on Whisper, and employed a number of data augmentation techniques, such as speech back-translation and noise augmentation. We investigate the effect of using synthetic audio data and discuss several methods for enriching signal diversity.</abstract>
      <url hash="fbf7b3b9">2024.iwslt-1.31</url>
      <bibkey>moslem-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.31</doi>
    </paper>
    <paper id="32">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Simultaneous Speech Translation System for <fixed-case>IWSLT</fixed-case> 2024</title>
      <author><first>Shaojun</first><last>Li</last><affiliation>HuaweiTechnologiesCo.,Ltd.</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Bin</first><last>Wei</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Yuanchang</first><last>Luo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <pages>274-279</pages>
      <abstract>This paper outlines our submission for the IWSLT 2024 Simultaneous Speech-to-Text (SimulS2T) and Speech-to-Speech (SimulS2S) Translation competition. We have engaged in all four language directions and both the SimulS2T and SimulS2S tracks: English-German (EN-DE), English-Chinese (EN-ZH), English-Japanese (EN-JA), and Czech-English (CS-EN). For the S2T track, we have built upon our previous year’s system and further honed the cascade system composed of ASR model and MT model. Concurrently, we have introduced an end-to-end system specifically for the CS-EN direction. This end-to-end (E2E) system primarily employs the pre-trained seamlessM4T model. In relation to the SimulS2S track, we have integrated a novel TTS model into our SimulS2T system. The final submission for the S2T directions of EN-DE, EN-ZH, and EN-JA has been refined over our championship system from last year. Building upon this foundation, the incorporation of the new TTS into our SimulS2S system has resulted in the ASR-BLEU surpassing last year’s best score.</abstract>
      <url hash="eb0c9671">2024.iwslt-1.32</url>
      <bibkey>li-etal-2024-hw-tscs</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.32</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>U</fixed-case>o<fixed-case>M</fixed-case>-<fixed-case>DFKI</fixed-case> submission to the low resource shared task</title>
      <author><first>Kumar</first><last>Rishu</last><affiliation>Charles University</affiliation></author>
      <author><first>Aiden</first><last>Williams</last><affiliation>University of Malta</affiliation></author>
      <author><first>Claudia</first><last>Borg</last><affiliation>University of Malta</affiliation></author>
      <author><first>Simon</first><last>Ostermann</last><affiliation>DFKI</affiliation></author>
      <pages>280-285</pages>
      <abstract>This system description paper presents the details of our primary and contrastive approaches to translating Maltese into English for IWSLT 24. The Maltese language shares a large vocabulary with Arabic and Italian languages, thus making it an ideal candidate to test the cross-lingual capabilities of recent state-of-the-art models. We experiment with two end-to-end approaches for our submissions: the Whisper and wav2vec 2.0 models. Our primary system gets a BLEU score of 35.1 on the combined data, whereas our contrastive approach gets 18.5. We also provide a manual analysis of our contrastive approach to identify some pitfalls that may have caused this difference.</abstract>
      <url hash="049eda3a">2024.iwslt-1.33</url>
      <bibkey>rishu-etal-2024-uom</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.33</doi>
    </paper>
    <paper id="34">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s submission to the <fixed-case>IWSLT</fixed-case> 2024 Subtitling track</title>
      <author><first>Yuhao</first><last>Xie</last><affiliation>HW-TSC</affiliation></author>
      <author><first>Yuanchang</first><last>Luo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>HuaweiTechnologiesCo.,Ltd.</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <pages>286-290</pages>
      <abstract>This paper introduces HW-TSC’s submission to the IWSLT 2024 Subtitling track. For the automatic subtitling track, we use an unconstrained cascaded strategy, with the main steps being: ASR with word-level timestamps, sentence segmentation based on punctuation restoration, further alignment using CTC or using machine translation with length penalty. For the subtitle compression track, we employ a subtitle compression strategy that integrates machine translation models and extensive rewriting models. We acquire the subtitle text requiring revision through the CPS index, then utilize a translation model to obtain the English version of this text. Following this, we extract the compressed-length subtitle text through controlled decoding. If this method fails to compress the text successfully, we resort to the Llama2 few-shot model for further compression.</abstract>
      <url hash="34f72283">2024.iwslt-1.34</url>
      <bibkey>xie-etal-2024-hw</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.34</doi>
    </paper>
    <paper id="35">
      <title><fixed-case>C</fixed-case>harles Locock, Lowcock or Lockhart? Offline Speech Translation: Test Suite for Named Entities</title>
      <author><first>Maximilian</first><last>Awiszus</last><affiliation>Zoom Video Communications</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Marco</first><last>Turchi</last><affiliation>Zoom Video Communications</affiliation></author>
      <author><first>Sebastian</first><last>Stüker</last><affiliation>Zoom Video Communications</affiliation></author>
      <author><first>Alex</first><last>Waibel</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>291-297</pages>
      <abstract>Generating rare words is a challenging task for natural language processing in general and in speech translation (ST) specifically. This paper introduces a test suite prepared for the Offline ST shared task at IWSLT. In the test suite, corresponding rare words (i.e. named entities) were annotated on TED-Talks for English and German and the English side was made available to the participants together with some distractors (irrelevant named entities). Our evaluation checks the capabilities of ST systems to leverage the information in the contextual list of named entities and improve translation quality. Systems are ranked based on the recall and precision of named entities (separately on person, location, and organization names) in the translated texts. Our evaluation shows that using contextual information improves translation quality as well as the recall and precision of NEs. The recall of organization names in all submissions is the lowest of all categories with a maximum of 87.5 % confirming the difficulties of ST systems in dealing with names.</abstract>
      <url hash="2492d16b">2024.iwslt-1.35</url>
      <bibkey>awiszus-etal-2024-charles</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.35</doi>
    </paper>
    <paper id="36">
      <title>Fixed and Adaptive Simultaneous Machine Translation Strategies Using Adapters</title>
      <author><first>Abderrahmane</first><last>Issam</last><affiliation>University of Maastricht</affiliation></author>
      <author><first>Yusuf</first><last>Can Semerci</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Jan</first><last>Scholtes</last><affiliation>Professor at Maastricht University</affiliation></author>
      <author><first>Gerasimos</first><last>Spanakis</last><affiliation>Maastricht University</affiliation></author>
      <pages>298-310</pages>
      <abstract>Simultaneous machine translation aims at solving the task of real-time translation by starting to translate before consuming the full input, which poses challenges in terms of balancing quality and latency of the translation. The wait-k policy offers a solution by starting to translate after consuming words, where the choice of the number k directly affects the latency and quality. In applications where we seek to keep the choice over latency and quality at inference, the wait-k policy obliges us to train more than one model. In this paper, we address the challenge of building one model that can fulfil multiple latency levels and we achieve this by introducing lightweight adapter modules into the decoder. The adapters are trained to be specialized for different wait-k values and compared to other techniques they offer more flexibility to allow for reaping the benefits of parameter sharing and minimizing interference. Additionally, we show that by combining with an adaptive strategy, we can further improve the results. Experiments on two language directions show that our method outperforms or competes with other strong baselines on most latency values.</abstract>
      <url hash="f4b87a31">2024.iwslt-1.36</url>
      <bibkey>issam-etal-2024-fixed</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.36</doi>
    </paper>
    <paper id="37">
      <title><fixed-case>IWSLT</fixed-case> 2024 <fixed-case>I</fixed-case>ndic Track system description paper: Speech-to-Text Translation from <fixed-case>E</fixed-case>nglish to multiple Low-Resource <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Deepanjali</first><last>Singh</last><affiliation>National Institute of Technology Kurukshetra</affiliation></author>
      <author><first>Ayush</first><last>Anand</last><affiliation>National Institute of Technology Kurukshetra</affiliation></author>
      <author><first>Abhyuday</first><last>Chaturvedi</last><affiliation>National Institute of Technology Kurukshetra</affiliation></author>
      <author><first>Niyati</first><last>Baliyan</last><affiliation>National Institute of Technology Kurukshetra</affiliation></author>
      <pages>311-316</pages>
      <abstract>Our Speech-to-Text (ST) translation system addresses low-resource Indian languages (Hindi, Bengali, Tamil) by combining advanced transcription and translation models for accurate and efficient translations. The key components of the system are: The Audio Processor and Transcription Module which utilizes ResembleAI for noise reduction and OpenAI’s Whisper model for transcription. The Input Module validates and preprocesses audio files. The Translation Modules integrate the Helsinki-NLP model for English to Hindi translation and Facebook’s MBart model for English to Tamil and Bengali translations, fine-tuned for better quality. The Output Module corrects syntax and removes hallucinations, delivering the final translated text. For performance evaluation purpose, SacreBLEU scores were used and attained the following values: English-to-Hindi: 24.21 (baseline: 5.23); English-to-Bengali: 16.18 (baseline: 5.86); English-to-Tamil: 10.79 (baseline: 1.9). The solution streamlines workflow from input validation to output delivery, significantly enhancing communication across different linguistic contexts and achieving substantial improvements in SacreBLEU scores. Through the creation of dedicated datasets and the development of robust models, our aim is to facilitate seamless communication and accessibility across diverse linguistic communities, ultimately promoting inclusivity and empowerment.</abstract>
      <url hash="a6a00fd7">2024.iwslt-1.37</url>
      <bibkey>singh-etal-2024-iwslt</bibkey>
      <doi>10.18653/v1/2024.iwslt-1.37</doi>
    </paper>
  </volume>
</collection>
