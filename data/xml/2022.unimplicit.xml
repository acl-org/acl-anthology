<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.unimplicit">
  <volume id="1" ingest-date="2022-06-29" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Understanding Implicit and Underspecified Language</booktitle>
      <editor><first>Valentina</first><last>Pyatkin</last></editor>
      <editor><first>Daniel</first><last>Fried</last></editor>
      <editor><first>Talita</first><last>Anthonio</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, USA</address>
      <month>July</month>
      <year>2022</year>
      <url hash="61c7f460">2022.unimplicit-1</url>
      <venue>unimplicit</venue>
    </meta>
    <frontmatter>
      <url hash="0bb4aa78">2022.unimplicit-1.0</url>
      <bibkey>unimplicit-2022-understanding</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Pre-trained Language Models’ Interpretation of Evaluativity Implicature: Evidence from Gradable Adjectives Usage in Context</title>
      <author><first>Yan</first><last>Cong</last></author>
      <pages>1-7</pages>
      <abstract>By saying Maria is tall, a human speaker typically implies that Maria is evaluatively tall from the speaker’s perspective. However, by using a different construction Maria is taller than Sophie, we cannot infer from Maria and Sophie’s relative heights that Maria is evaluatively tall because it is possible for Maria to be taller than Sophie in a context in which they both count as short. Can pre-trained language models (LMs) “understand” evaulativity (EVAL) inference? To what extent can they discern the EVAL salience of different constructions in a conversation? Will it help LMs’ implicitness performance if we give LMs a persona such as chill, social, and pragmatically skilled? Our study provides an approach to probing LMs’ interpretation of EVAL inference by incorporating insights from experimental pragmatics and sociolinguistics. We find that with the appropriate prompt, LMs can succeed in some pragmatic level language understanding tasks. Our study suggests that socio-pragmatics methodology can shed light on the challenging questions in NLP.</abstract>
      <url hash="903a66c1">2022.unimplicit-1.1</url>
      <bibkey>cong-2022-pre</bibkey>
      <doi>10.18653/v1/2022.unimplicit-1.1</doi>
      <video href="2022.unimplicit-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Pragmatic and Logical Inferences in <fixed-case>NLI</fixed-case> Systems: The Case of Conjunction Buttressing</title>
      <author><first>Paolo</first><last>Pedinotti</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <pages>8-16</pages>
      <abstract>An intelligent system is expected to perform reasonable inferences, accounting for both the literal meaning of a word and the meanings a word can acquire in different contexts. A specific kind of inference concerns the connective and, which in some cases gives rise to a temporal succession or causal interpretation in contrast with the logic, commutative one (Levinson, 2000). In this work, we investigate the phenomenon by creating a new dataset for evaluating the interpretation of and by NLI systems, which we use to test three Transformer-based models. Our results show that all systems generalize patterns that are consistent with both the logical and the pragmatic interpretation, perform inferences that are inconsistent with each other, and show clear divergences with both theoretical accounts and humans’ behavior.</abstract>
      <url hash="88ac3313">2022.unimplicit-1.2</url>
      <bibkey>pedinotti-etal-2022-pragmatic</bibkey>
      <doi>10.18653/v1/2022.unimplicit-1.2</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="3">
      <title>“Devils Are in the Details”: Annotating Specificity of Clinical Advice from Medical Literature</title>
      <author><first>Yingya</first><last>Li</last></author>
      <author><first>Bei</first><last>Yu</last></author>
      <pages>17-21</pages>
      <abstract>Prior studies have raised concerns over specificity issues in clinical advice. Lacking specificity — explicitly discussed detailed information — may affect the quality and implementation of clinical advice in medical practice. In this study, we developed and validated a fine-grained annotation schema to describe different aspects of specificity in clinical advice extracted from medical research literature. We also presented our initial annotation effort and discussed future directions towards an NLP-based specificity analysis tool for summarizing and verifying the details in clinical advice.</abstract>
      <url hash="85f14de4">2022.unimplicit-1.3</url>
      <bibkey>li-yu-2022-devils</bibkey>
      <doi>10.18653/v1/2022.unimplicit-1.3</doi>
    </paper>
    <paper id="4">
      <title>Searching for <fixed-case>PET</fixed-case>s: Using Distributional and Sentiment-Based Methods to Find Potentially Euphemistic Terms</title>
      <author><first>Patrick</first><last>Lee</last></author>
      <author><first>Martha</first><last>Gavidia</last></author>
      <author><first>Anna</first><last>Feldman</last></author>
      <author><first>Jing</first><last>Peng</last></author>
      <pages>22-32</pages>
      <abstract>This paper presents a linguistically driven proof of concept for finding potentially euphemistic terms, or PETs. Acknowledging that PETs tend to be commonly used expressions for a certain range of sensitive topics, we make use of distri- butional similarities to select and filter phrase candidates from a sentence and rank them using a set of simple sentiment-based metrics. We present the results of our approach tested on a corpus of sentences containing euphemisms, demonstrating its efficacy for detecting single and multi-word PETs from a broad range of topics. We also discuss future potential for sentiment-based methods on this task.</abstract>
      <url hash="5518e559">2022.unimplicit-1.4</url>
      <bibkey>lee-etal-2022-searching</bibkey>
      <doi>10.18653/v1/2022.unimplicit-1.4</doi>
      <pwccode url="https://github.com/marsgav/petdetection" additional="false">marsgav/petdetection</pwccode>
    </paper>
  </volume>
</collection>
