<?xml version='1.0' encoding='UTF-8'?>
<collection id="2015.jeptalnrecital">
  <volume id="long">
    <meta>
      <booktitle>Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles. Articles longs</booktitle>
      <editor><first>Jean-Marc</first><last>Lecarpentier</last></editor>
      <editor><first>Nadine</first><last>Lucas</last></editor>
      <publisher>ATALA</publisher>
      <address>Caen, France</address>
      <month>June</month>
      <year>2015</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="4d3d279f">2015.jeptalnrecital-long.0</url>
      <bibkey>jep-taln-recital-2015-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Apprentissage par imitation pour l’étiquetage de séquences : vers une formalisation des méthodes d’étiquetage easy-first</title>
      <author><first>Elena</first><last>Knyazeva</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>1–12</pages>
      <abstract>De nombreuses méthodes ont été proposées pour accélérer la prédiction d’objets structurés (tels que les arbres ou les séquences), ou pour permettre la prise en compte de dépendances plus riches afin d’améliorer les performances de la prédiction. Ces méthodes reposent généralement sur des techniques d’inférence approchée et ne bénéficient d’aucune garantie théorique aussi bien du point de vue de la qualité de la solution trouvée que du point de vue de leur critère d’apprentissage. Dans ce travail, nous étudions une nouvelle formulation de l’apprentissage structuré qui consiste à voir celui-ci comme un processus incrémental au cours duquel la sortie est construite de façon progressive. Ce cadre permet de formaliser plusieurs approches de prédiction structurée existantes. Grâce au lien que nous faisons entre apprentissage structuré et apprentissage par renforcement, nous sommes en mesure de proposer une méthode théoriquement bien justifiée pour apprendre des méthodes d’inférence approchée. Les expériences que nous réalisons sur quatre tâches de TAL valident l’approche proposée.</abstract>
      <url hash="7e464405">2015.jeptalnrecital-long.1</url>
      <bibkey>knyazeva-etal-2015-apprentissage</bibkey>
    </paper>
    <paper id="2">
      <title>Stratégies de sélection des exemples pour l’apprentissage actif avec des champs aléatoires conditionnels</title>
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Ewa</first><last>Kijak</last></author>
      <pages>13–24</pages>
      <abstract>Beaucoup de problèmes de TAL sont désormais modélisés comme des tâches d’apprentissage supervisé. De ce fait, le coût des annotations des exemples par l’expert représente un problème important. L’apprentissage actif (active learning) apporte un cadre à ce problème, permettant de contrôler le coût d’annotation tout en maximisant, on l’espère, la performance de la tâche visée, mais repose sur le choix difficile des exemples à soumettre à l’expert. Dans cet article, nous examinons et proposons des stratégies de sélection des exemples pour le cas spécifique des champs aléatoires conditionnels (Conditional Random Fields, CRF), outil largement utilisé en TAL. Nous proposons d’une part une méthode simple corrigeant un biais de certaines méthodes de l’état de l’art. D’autre part, nous détaillons une méthode originale de sélection s’appuyant sur un critère de respect des proportions dans les jeux de données manipulés. Le bien- fondé de ces propositions est vérifié au travers de plusieurs tâches et jeux de données, incluant reconnaissance d’entités nommées, chunking, phonétisation, désambiguïsation de sens.</abstract>
      <url hash="83d90bec">2015.jeptalnrecital-long.2</url>
      <bibkey>claveau-kijak-2015-strategies</bibkey>
    </paper>
    <paper id="3">
      <title>Identification de facteurs de risque pour des patients diabétiques à partir de comptes-rendus cliniques par des approches hybrides</title>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>25–36</pages>
      <abstract>Dans cet article, nous présentons les méthodes que nous avons développées pour analyser des comptes- rendus hospitaliers rédigés en anglais. L’objectif de cette étude consiste à identifier les facteurs de risque de décès pour des patients diabétiques et à positionner les événements médicaux décrits par rapport à la date de création de chaque document. Notre approche repose sur (i) HeidelTime pour identifier les expressions temporelles, (ii) des CRF complétés par des règles de post-traitement pour identifier les traitements, les maladies et facteurs de risque, et (iii) des règles pour positionner temporellement chaque événement médical. Sur un corpus de 514 documents, nous obtenons une F-mesure globale de 0,8451. Nous observons que l’identification des informations directement mentionnées dans les documents se révèle plus performante que l’inférence d’informations à partir de résultats de laboratoire.</abstract>
      <url hash="e577d861">2015.jeptalnrecital-long.3</url>
      <bibkey>grouin-etal-2015-identification</bibkey>
    </paper>
    <paper id="4">
      <title>Oublier ce qu’on sait, pour mieux apprendre ce qu’on ne sait pas : une étude sur les contraintes de type dans les modèles <fixed-case>CRF</fixed-case></title>
      <author><first>Nicolas</first><last>Pécheux</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>37–48</pages>
      <abstract>Quand on dispose de connaissances a priori sur les sorties possibles d’un problème d’étiquetage, il semble souhaitable d’inclure cette information lors de l’apprentissage pour simplifier la tâche de modélisation et accélérer les traitements. Pourtant, même lorsque ces contraintes sont correctes et utiles au décodage, leur utilisation lors de l’apprentissage peut dégrader sévèrement les performances. Dans cet article, nous étudions ce paradoxe et montrons que le manque de contraste induit par les connaissances entraîne une forme de sous-apprentissage qu’il est cependant possible de limiter.</abstract>
      <url hash="a4d3e4a6">2015.jeptalnrecital-long.4</url>
      <bibkey>pecheux-etal-2015-oublier</bibkey>
    </paper>
    <paper id="5">
      <title>Analyse d’expressions temporelles dans les dossiers électroniques patients</title>
      <author><first>Mike Donald Tapi</first><last>Nzali</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <pages>49–58</pages>
      <abstract>Les références à des phénomènes du monde réel et à leur caractérisation temporelle se retrouvent dans beaucoup de types de discours en langue naturelle. Ainsi, l’analyse temporelle apparaît comme un élément important en traitement automatique de la langue. Cet article présente une analyse de textes en domaine de spécialité du point de vue temporel. En s’appuyant sur un corpus de documents issus de plusieurs dossiers électroniques patient désidentifiés, nous décrivons la construction d’une ressource annotée en expressions temporelles selon la norme TimeML. Par suite, nous utilisons cette ressource pour évaluer plusieurs méthodes d’extraction automatique d’expressions temporelles adaptées au domaine médical. Notre meilleur système statistique offre une performance de 0,91 de F-mesure, surpassant pour l’identification le système état de l’art HeidelTime. La comparaison de notre corpus de travail avec le corpus journalistique FR-Timebank permet également de caractériser les différences d’utilisation des expressions temporelles dans deux domaines de spécialité.</abstract>
      <url hash="01280465">2015.jeptalnrecital-long.5</url>
      <bibkey>nzali-etal-2015-analyse</bibkey>
    </paper>
    <paper id="6">
      <title>Compréhension automatique de la parole sans données de référence</title>
      <author><first>Emmanuel</first><last>Ferreira</last></author>
      <author><first>Bassam</first><last>Jabaian</last></author>
      <author><first>Fabrice</first><last>Lefèvre</last></author>
      <pages>59–70</pages>
      <abstract>La majorité des méthodes état de l’art en compréhension automatique de la parole ont en commun de devoir être apprises sur une grande quantité de données annotées. Cette dépendance aux données constitue un réel obstacle lors du développement d’un système pour une nouvelle tâche/langue. Aussi, dans cette étude, nous présentons une méthode visant à limiter ce besoin par un mécanisme d’apprentissage sans données de référence (zero-shot learning). Cette méthode combine une description ontologique minimale de la tâche visée avec l’utilisation d’un espace sémantique continu appris par des approches à base de réseaux de neurones à partir de données génériques non-annotées. Nous montrons que le modèle simple et peu coûteux obtenu peut atteindre, dès le démarrage, des performances comparables à celles des systèmes état de l’art reposant sur des règles expertes ou sur des approches probabilistes sur des tâches de compréhension de la parole de référence (tests des Dialog State Tracking Challenges, DSTC2 et DSTC3). Nous proposons ensuite une stratégie d’adaptation en ligne permettant d’améliorer encore les performances de notre approche à l’aide d’une supervision faible et ajustable par l’utilisateur.</abstract>
      <url hash="c78462a4">2015.jeptalnrecital-long.6</url>
      <bibkey>ferreira-etal-2015-comprehension</bibkey>
    </paper>
    <paper id="7">
      <title>Désambiguïsation d’entités pour l’induction non supervisée de schémas événementiels</title>
      <author><first>Kiem-Hieu</first><last>Nguyen</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Romaric</first><last>Besançon</last></author>
      <pages>71–82</pages>
      <abstract>Cet article présente un modèle génératif pour l’induction non supervisée d’événements. Les précédentes méthodes de la littérature utilisent uniquement les têtes des syntagmes pour représenter les entités. Pourtant, le groupe complet (par exemple, ”un homme armé”) apporte une information plus discriminante (que ”homme”). Notre modèle tient compte de cette information et la représente dans la distribution des schémas d’événements. Nous montrons que ces relations jouent un rôle important dans l’estimation des paramètres, et qu’elles conduisent à des distributions plus cohérentes et plus discriminantes. Les résultats expérimentaux sur le corpus de MUC-4 confirment ces progrès.</abstract>
      <url hash="6ab1c0ba">2015.jeptalnrecital-long.7</url>
      <bibkey>nguyen-etal-2015-desambiguisation</bibkey>
    </paper>
    <paper id="8">
      <title>Création rapide et efficace d’un système de désambiguïsation lexicale pour une langue peu dotée</title>
      <author><first>Mohammad</first><last>Nasiruddin</last></author>
      <author><first>Andon</first><last>Tchechmedjiev</last></author>
      <author><first>Hervé</first><last>Blanchon</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <pages>83–94</pages>
      <abstract>Nous présentons une méthode pour créer rapidement un système de désambiguïsation lexicale (DL) pour une langue L peu dotée pourvu que l’on dispose d’un système de traduction automatique statistique (TAS) d’une langue riche en corpus annotés en sens (ici l’anglais) vers L. Il est, en effet, plus facile de disposer des ressources nécessaires à la création d’un système de TAS que des ressources dédiées nécessaires à la création d’un système de DL pour la langue L. Notre méthode consiste à traduire automatiquement un corpus annoté en sens vers la langue L, puis de créer le système de désambiguïsation pour L par des méthodes supervisées classiques. Nous montrons la faisabilité de la méthode et sa généricité en traduisant le SemCor, un corpus en anglais annoté grâce au Princeton WordNet, de l’anglais vers le bangla et de l’anglais vers le français. Nous montrons la validité de l’approche en évaluant les résultats sur la tâche de désambiguïsation lexicale multilingue de Semeval 2013.</abstract>
      <url hash="c3565fa8">2015.jeptalnrecital-long.8</url>
      <bibkey>nasiruddin-etal-2015-creation</bibkey>
    </paper>
    <paper id="9">
      <title>Méthode faiblement supervisée pour l’extraction d’opinion ciblée dans un domaine spécifique</title>
      <author><first>Romaric</first><last>Besançon</last></author>
      <pages>95–106</pages>
      <abstract>La détection d’opinion ciblée a pour but d’attribuer une opinion à une caractéristique particulière d’un produit donné. La plupart des méthodes existantes envisagent pour cela une approche non supervisée. Or, les utilisateurs ont souvent une idée a priori des caractéristiques sur lesquelles ils veulent découvrir l’opinion des gens. Nous proposons dans cet article une méthode pour une extraction d’opinion ciblée, qui exploite cette information minimale sur les caractéristiques d’intérêt. Ce modèle s’appuie sur une segmentation automatique des textes, un enrichissement des données disponibles par similarité sémantique, et une annotation de l’opinion par classification supervisée. Nous montrons l’intérêt de l’approche sur un cas d’étude dans le domaine des jeux vidéos.</abstract>
      <url hash="0531dfa9">2015.jeptalnrecital-long.9</url>
      <bibkey>besancon-2015-methode</bibkey>
    </paper>
    <paper id="10">
      <title>Une méthodologie de sémantique de corpus appliquée à des tâches de fouille d’opinion et d’analyse des sentiments : étude sur l’impact de marqueurs dialogiques et dialectiques dans l’expression de la subjectivité</title>
      <author><first>Egle</first><last>Eensoo</last></author>
      <author><first>Mathieu</first><last>Valette</last></author>
      <pages>107–118</pages>
      <abstract>Cet article entend dresser, dans un premier temps, un panorama critique des relations entre TAL et linguistique. Puis, il esquisse une discussion sur l’apport possible d’une sémantique de corpus dans un contexte applicatif en s’appuyant sur plusieurs expériences en fouille de textes subjectifs (analyse de sentiments et fouille d’opinions). Ces expériences se démarquent des approches traditionnelles fondées sur la recherche de marqueurs axiologiques explicites par l’utilisation de critères relevant des représentations des acteurs (composante dialogique) et des structures argumentatives et narratives des textes (composante dialectique). Nous souhaitons de cette façon mettre en lumière le bénéfice d’un dialogue méthodologique entre une théorie (la sémantique textuelle), des méthodes de linguistique de corpus orientées vers l’analyse du sens (la textométrie) et les usages actuels du TAL en termes d’algorithmiques (apprentissage automatique) mais aussi de méthodologie d’évaluation des résultats.</abstract>
      <url hash="a48580ae">2015.jeptalnrecital-long.10</url>
      <bibkey>eensoo-valette-2015-une</bibkey>
    </paper>
    <paper id="11">
      <title>Estimation de l’homogénéité sémantique pour les Questionnaires à Choix Multiples</title>
      <author><first>Van-Minh</first><last>Pho</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <pages>119–130</pages>
      <abstract>L’homogénéité sémantique stipule que des termes sont sémantiquement proches mais non similaires. Cette notion est au cœur de travaux relatifs à la génération automatique de questionnaires à choix multiples, et particulièrement à la sélection automatique de distracteurs. Dans cet article, nous présentons une méthode d’estimation de l’homogénéité sémantique dans un cadre de validation automatique de distracteurs. Cette méthode est fondée sur une combinaison de plusieurs critères de voisinage et de similarité sémantique entre termes, par apprentissage automatique. Nous montrerons que notre méthode permet d’obtenir une meilleure estimation de l’homogénéité sémantique que les méthodes proposées dans l’état de l’art.</abstract>
      <url hash="8ae39a4e">2015.jeptalnrecital-long.11</url>
      <bibkey>pho-etal-2015-estimation</bibkey>
    </paper>
    <paper id="12">
      <title>Extraction automatique de relations sémantiques dans les dé finitions : approche hybride, construction d’un corpus de relations sémantiques pour le français</title>
      <author><first>Emmanuel</first><last>Cartier</last></author>
      <pages>131–145</pages>
      <abstract>Cet article présente une expérimentation visant à construire une ressource sémantique pour le français contemporain à partir d’un corpus d’environ un million de définitions tirées de deux ressources lexicographiques (Trésor de la Langue Française, Wiktionary) et d’une ressource encyclopédique (Wikipedia). L’objectif est d’extraire automatiquement dans les définitions différentes relations sémantiques : hyperonymie, synonymie, méronymie, autres relations sémantiques. La méthode suivie combine la précision des patrons lexico-syntaxiques et le rappel des méthodes statistiques, ainsi qu’un traitement inédit de canonisation et de décomposition des énoncés. Après avoir présenté les différentes approches et réalisations existantes, nous détaillons l’architecture du système et présentons les résultats : environ 900 000 relations d’hyperonymie et près de 100 000 relations de synonymie, avec un taux de précision supérieur à 90

% sur un échantillon aléatoire de 500 relations. Plus de 2 millions de prédications définitoires ont également été extraites.</abstract>
      <url hash="72093af9">2015.jeptalnrecital-long.12</url>
      <bibkey>cartier-2015-extraction</bibkey>
    </paper>
    <paper id="13">
      <title>Déclasser les voisins non sémantiques pour améliorer les thésaurus distributionnels</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>146–157</pages>
      <abstract>La plupart des méthodes d’amélioration des thésaurus distributionnels se focalisent sur les moyens – représentations ou mesures de similarité – de mieux détecter la similarité sémantique entre les mots. Dans cet article, nous proposons un point de vue inverse : nous cherchons à détecter les voisins sémantiques associés à une entrée les moins susceptibles d’être liés sémantiquement à elle et nous utilisons cette information pour réordonner ces voisins. Pour détecter les faux voisins sémantiques d’une entrée, nous adoptons une approche s’inspirant de la désambiguïsation sémantique en construisant un classifieur permettant de différencier en contexte cette entrée des autres mots. Ce classifieur est ensuite appliqué à un échantillon des occurrences des voisins de l’entrée pour repérer ceux les plus éloignés de l’entrée. Nous évaluons cette méthode pour des thésaurus construits à partir de cooccurrents syntaxiques et nous montrons l’intérêt de la combiner avec les méthodes décrites dans (Ferret, 2013b) selon une stratégie de type vote.</abstract>
      <url hash="b669eaea">2015.jeptalnrecital-long.13</url>
      <bibkey>ferret-2015-declasser</bibkey>
    </paper>
    <paper id="14">
      <title>Grammaires phrastiques et discursives fondées sur les <fixed-case>TAG</fixed-case> : une approche de <fixed-case>D</fixed-case>-<fixed-case>STAG</fixed-case> avec les <fixed-case>ACG</fixed-case></title>
      <author><first>Laurence</first><last>Danlos</last></author>
      <author><first>Aleksandre</first><last>Maskharashvili</last></author>
      <author><first>Sylvain</first><last>Pogodalla</last></author>
      <pages>158–169</pages>
      <abstract>Nous présentons une méthode pour articuler grammaire de phrase et grammaire de discours qui évite de recourir à une étape de traitement intermédiaire. Cette méthode est suffisamment générale pour construire des structures discursives qui ne soient pas des arbres mais des graphes orientés acycliques (DAG). Notre analyse s’appuie sur une approche de l’analyse discursive, Discourse Synchronous TAG (D-STAG), qui utilise les Grammaires d’Arbres Adjoint (TAG). Nous utilisons pour ce faire un encodage des TAG dans les Grammaires Catégorielles Abstraites (ACG). Cet encodage permet d’une part d’utiliser l’ordre supérieur pour l’interprétation sémantique afin de construire des structures qui soient des DAG et non des arbres, et d’autre part d’utiliser les propriétés de composition d’ACG pour réaliser naturellement l’interface entre grammaire phrastique et grammaire discursive. Tous les exemples proposés pour illustrer la méthode ont été implantés et peuvent être testés avec le logiciel approprié.</abstract>
      <url hash="9b35a7ea">2015.jeptalnrecital-long.14</url>
      <bibkey>danlos-etal-2015-grammaires</bibkey>
    </paper>
    <paper id="15">
      <title>Noyaux de réécriture de phrases munis de types lexico-sémantiques</title>
      <author><first>Martin</first><last>Gleize</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <pages>170–181</pages>
      <abstract>De nombreux problèmes en traitement automatique des langues requièrent de déterminer si deux phrases sont des réécritures l’une de l’autre. Une solution efficace consiste à apprendre les réécritures en se fondant sur des méthodes à noyau qui mesurent la similarité entre deux réécritures de paires de phrases. Toutefois, ces méthodes ne permettent généralement pas de prendre en compte des variations sémantiques entre mots, qui permettraient de capturer un plus grand nombre de règles de réécriture. Dans cet article, nous proposons la définition et l’implémentation d’une nouvelle classe de fonction noyau, fondée sur la réécriture de phrases enrichie par un typage pour combler ce manque. Nous l’évaluons sur deux tâches, la reconnaissance de paraphrases et d’implications textuelles.</abstract>
      <url hash="907ad3a4">2015.jeptalnrecital-long.15</url>
      <bibkey>gleize-grau-2015-noyaux</bibkey>
    </paper>
    <paper id="16">
      <title>Extraction automatique de paraphrases grand public pour les termes médicaux</title>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Thierry</first><last>Hamon</last></author>
      <pages>182–195</pages>
      <abstract>Nous sommes tous concernés par notre état de santé et restons sensibles aux informations de santé disponibles dans la société moderne à travers par exemple les résultats des recherches scientifiques, les médias sociaux de santé, les documents cliniques, les émissions de télé et de radio ou les nouvelles. Cependant, il est commun de rencontrer dans le domaine médical des termes très spécifiques (e.g., blépharospasme, alexitymie, appendicectomie), qui restent difficiles à comprendre par les non spécialistes. Nous proposons une méthode automatique qui vise l’acquisition de paraphrases pour les termes médicaux, qui soient plus faciles à comprendre que les termes originaux. La méthode est basée sur l’analyse morphologique des termes, l’analyse syntaxique et la fouille de textes non spécialisés. L’analyse et l’évaluation des résultats indiquent que de telles paraphrases peuvent être trouvées dans les documents non spécialisés et présentent une compréhension plus facile. En fonction des paramètres de la méthode, la précision varie entre 86 et 55

%. Ce type de ressources est utile pour plusieurs applications de TAL (e.g., recherche d’information grand public, lisibilité et simplification de textes, systèmes de question-réponses).</abstract>
      <url hash="89a81cf6">2015.jeptalnrecital-long.16</url>
      <bibkey>grabar-hamon-2015-extraction</bibkey>
    </paper>
    <paper id="17">
      <title>Analyse syntaxique de l’ancien français : quelles propriétés de la langue influent le plus sur la qualité de l’apprentissage ?</title>
      <author><first>Gaël</first><last>Guibon</last></author>
      <author><first>Isabelle</first><last>Tellier</last></author>
      <author><first>Sophie</first><last>Prévost</last></author>
      <author><first>Matthieu</first><last>Constant</last></author>
      <author><first>Kim</first><last>Gerdes</last></author>
      <pages>196–207</pages>
      <abstract>L’article présente des résultats d’expériences d’apprentissage automatique pour l’étiquetage morpho-syntaxique et l’analyse syntaxique en dépendance de l’ancien français. Ces expériences ont pour objectif de servir une exploration de corpus pour laquelle le corpus arboré SRCMF sert de données de référence. La nature peu standardisée de la langue qui y est utilisée implique des données d’entraînement hétérogènes et quantitativement limitées. Nous explorons donc diverses stratégies, fondées sur différents critères (variabilité du lexique, forme Vers/Prose des textes, dates des textes), pour constituer des corpus d’entrainement menant aux meilleurs résultats possibles.</abstract>
      <url hash="9f4f3398">2015.jeptalnrecital-long.17</url>
      <bibkey>guibon-etal-2015-analyse</bibkey>
    </paper>
    <paper id="18">
      <title>Attribution d’Auteur : approche multilingue fondée sur les répétitions maximales</title>
      <author><first>Romain</first><last>Brixtel</last></author>
      <author><first>Charlotte</first><last>Lecluze</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <pages>208–219</pages>
      <abstract>Cet article s’attaque à la tâche d’Attribution d’Auteur en contexte multilingue. Nous proposons une alternative aux méthodes supervisées fondées sur les n-grammes de caractères de longueurs variables : les répétitions maximales. Pour un texte donné, la liste de ses n-grammes de caractères contient des informations redondantes. A contrario, les répétitions maximales représentent l’ensemble des répétitions de ce texte de manière condensée. Nos expériences montrent que la redondance des n-grammes contribue à l’efficacité des techniques d’Attribution d’Auteur exploitant des sous-chaînes de caractères. Ce constat posé, nous proposons une fonction de pondération sur les traits donnés en entrée aux classifieurs, en introduisant les répétitions maximales du nème ordre (c’est-à-dire des répétitions maximales détectées dans un ensemble de répétitions maximales). Les résultats expérimentaux montrent de meilleures performances avec des répétitions maximales, avec moins de données que pour les approches fondées sur les n-grammes.</abstract>
      <url hash="6c59651c">2015.jeptalnrecital-long.18</url>
      <bibkey>brixtel-etal-2015-attribution</bibkey>
    </paper>
    <paper id="19">
      <title>Mesurer la similarité entre phrases grâce à Wikipédia en utilisant une indexation aléatoire</title>
      <author><first>Hai Hieu</first><last>Vu</last></author>
      <author><first>Jeanne</first><last>Villaneau</last></author>
      <author><first>Farida</first><last>Saïd</last></author>
      <author><first>Pierre-François</first><last>Marteau</last></author>
      <pages>220–231</pages>
      <abstract>Cet article présente une méthode pour mesurer la similarité sémantique entre phrases qui utilise Wikipédia comme unique ressource linguistique et qui est, de ce fait, utilisable pour un grand nombre de langues. Basée sur une représentation vectorielle, elle utilise une indexation aléatoire pour réduire la dimension des espaces manipulés. En outre, elle inclut une technique de calcul des vecteurs de termes qui corrige les défauts engendrés par l’utilisation d’un corpus aussi général que Wikipédia. Le système a été évalué sur les données de SemEval 2014 en anglais avec des résultats très encourageants, au-dessus du niveau moyen des systèmes en compétition. Il a également été testé sur un ensemble de paires de phrases en français, à partir de ressources que nous avons construites et qui seront mises à la libre disposition de la communauté scientifique.</abstract>
      <url hash="6e664a27">2015.jeptalnrecital-long.19</url>
      <bibkey>vu-etal-2015-mesurer</bibkey>
    </paper>
    <paper id="20">
      <title>Typologie automatique des langues à partir de treebanks</title>
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Grégroie</first><last>de Montcheuil</last></author>
      <author><first>Stéphane</first><last>Rauzy</last></author>
      <pages>232–243</pages>
      <abstract>La typologie des langues repose sur l’étude de la réalisation de propriétés ou phénomènes linguistiques dans plusieurs langues ou familles de langues. Nous abordons dans cet article la question de la typologie syntaxique et proposons une méthode permettant d’extraire automatiquement ces propriétés à partir de treebanks, puis de les analyser en vue de dresser une telle typologie. Nous décrivons cette méthode ainsi que les outils développés pour la mettre en œuvre. Celle-ci a été appliquée à l’analyse de 10 langues décrites dans le Universal Dependencies Treebank. Nous validons ces résultats en montrant comment une technique de classification permet, sur la base des informations extraites, de reconstituer des familles de langues.</abstract>
      <url hash="3e2871ce">2015.jeptalnrecital-long.20</url>
      <bibkey>blache-etal-2015-typologie</bibkey>
    </paper>
    <paper id="21">
      <title>Utilisation de mesures de confiance pour améliorer le décodage en traduction de parole</title>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Luong Ngoc</first><last>Quang</last></author>
      <pages>244–254</pages>
      <abstract>Les mesures de confiance au niveau mot (Word Confidence Estimation - WCE) pour la traduction auto- matique (TA) ou pour la reconnaissance automatique de la parole (RAP) attribuent un score de confiance à chaque mot dans une hypothèse de transcription ou de traduction. Dans le passé, l’estimation de ces mesures a le plus souvent été traitée séparément dans des contextes RAP ou TA. Nous proposons ici une estimation conjointe de la confiance associée à un mot dans une hypothèse de traduction automatique de la parole (TAP). Cette estimation fait appel à des paramètres issus aussi bien des systèmes de transcription de la parole (RAP) que des systèmes de traduction automatique (TA). En plus de la construction de ces estimateurs de confiance robustes pour la TAP, nous utilisons les informations de confiance pour re-décoder nos graphes d’hypothèses de traduction. Les expérimentations réalisées montrent que l’utilisation de ces mesures de confiance au cours d’une seconde passe de décodage permettent d’obtenir une amélioration significative des performances de traduction (évaluées avec la métrique BLEU - gains de deux points par rapport à notre système de traduc- tion de parole de référence). Ces expériences sont faites pour une tâche de TAP (français-anglais) pour laquelle un corpus a été spécialement conçu (ce corpus, mis à la disposition de la communauté TALN, est aussi décrit en détail dans l’article).</abstract>
      <url hash="d5bc5315">2015.jeptalnrecital-long.21</url>
      <bibkey>besacier-etal-2015-utilisation</bibkey>
    </paper>
    <paper id="22">
      <title>Multialignement vs bialignement : à plusieurs, c’est mieux !</title>
      <author><first>Olivier</first><last>Kraif</last></author>
      <pages>255–266</pages>
      <abstract>Dans cet article, nous proposons une méthode originale destinée à effectuer l’alignement d’un corpus multiparallèle, i.e. comportant plus de deux langues, en prenant en compte toutes les langues simultanément (et non en composant une série de bialignements indépendants). Pour ce faire, nous nous appuyons sur les réseaux de correspondances lexicales constitués par les transfuges (chaînes identiques) et cognats (mots apparentés), et nous montrons comment divers tuilages des couples de langues permettent d’exploiter au mieux les ressemblances superficielles liées aux relations génétiques interlinguistiques. Nous évaluons notre méthode par rapport à une méthode de bialignement classique, et montrons en quoi le multialignement permet d’obtenir des résultats à la fois plus précis et plus robustes.</abstract>
      <url hash="fd379c17">2015.jeptalnrecital-long.22</url>
      <bibkey>kraif-2015-multialignement</bibkey>
    </paper>
    <paper id="23">
      <title>Apprentissage discriminant des modèles continus de traduction</title>
      <author><first>Quoc-Khanh</first><last>Do</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>267–278</pages>
      <abstract>Alors que les réseaux neuronaux occupent une place de plus en plus importante dans le traitement automatique des langues, les méthodes d’apprentissage actuelles utilisent pour la plupart des critères qui sont décorrélés de l’application. Cet article propose un nouveau cadre d’apprentissage discriminant pour l’estimation des modèles continus de traduction. Ce cadre s’appuie sur la définition d’un critère d’optimisation permettant de prendre en compte d’une part la métrique utilisée pour l’évaluation de la traduction et d’autre part l’intégration de ces modèles au sein des systèmes de traduction automatique. De plus, cette méthode d’apprentissage est comparée aux critères existants d’estimation que sont le maximum de vraisemblance et l’estimation contrastive bruitée. Les expériences menées sur la tâches de traduction des séminaires TED Talks de l’anglais vers le français montrent la pertinence d’un cadre discriminant d’apprentissage, dont les performances restent toutefois très dépendantes du choix d’une stratégie d’initialisation idoine. Nous montrons qu’avec une initialisation judicieuse des gains significatifs en termes de scores BLEU peuvent être obtenus.</abstract>
      <url hash="f8f05d3c">2015.jeptalnrecital-long.23</url>
      <bibkey>do-etal-2015-apprentissage</bibkey>
    </paper>
    <paper id="24">
      <title>Utiliser les interjections pour détecter les émotions</title>
      <author><first>Amel</first><last>Fraisse</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>279–290</pages>
      <abstract>Bien que les interjections soient un phénomène linguistique connu, elles ont été peu étudiées et cela continue d’être le cas pour les travaux sur les microblogs. Des travaux en analyse de sentiments ont montré l’intérêt des émoticônes et récemment des mots-dièses, qui s’avèrent être très utiles pour la classification en polarité. Mais malgré leur statut grammatical et leur richesse sémantique, les interjections sont restées marginalisées par les systèmes d’analyse de sentiments. Nous montrons dans cet article l’apport majeur des interjections pour la détection des émotions. Nous détaillons la production automatique, basée sur les interjections, d’un corpus étiqueté avec les émotions. Nous expliquons ensuite comment nous avons utilisé ce corpus pour en déduire, automatiquement, un lexique affectif pour le français. Ce lexique a été évalué sur une tâche de détection des émotions, qui a montré un gain en mesure F1 allant, selon les émotions, de +0,04 à +0,21.</abstract>
      <url hash="69d9974d">2015.jeptalnrecital-long.24</url>
      <bibkey>fraisse-paroubek-2015-utiliser</bibkey>
    </paper>
    <paper id="25">
      <title>Comparaison d’architectures neuronales pour l’analyse syntaxique en constituants</title>
      <author><first>Maximin</first><last>Coavoux</last></author>
      <author><first>Benoît</first><last>Crabbé</last></author>
      <pages>291–302</pages>
      <abstract>L’article traite de l’analyse syntaxique lexicalisée pour les grammaires de constituants. On se place dans le cadre de l’analyse par transitions. Les modèles statistiques généralement utilisés pour cette tâche s’appuient sur une représentation non structurée du lexique. Les mots du vocabulaire sont représentés par des symboles discrets sans liens entre eux. À la place, nous proposons d’utiliser des représentations denses du type plongements (embeddings) qui permettent de modéliser la similarité entre symboles, c’est-à-dire entre mots, entre parties du discours et entre catégories syntagmatiques. Nous proposons d’adapter le modèle statistique sous-jacent à ces nouvelles représentations. L’article propose une étude de 3 architectures neuronales de complexité croissante et montre que l’utilisation d’une couche cachée non-linéaire permet de tirer parti des informations données par les plongements.</abstract>
      <url hash="4ba08c76">2015.jeptalnrecital-long.25</url>
      <bibkey>coavoux-crabbe-2015-comparaison</bibkey>
    </paper>
    <paper id="26">
      <title>...des conférences enfin disons des causeries... Détection automatique de segments en relation de paraphrase dans les reformulations de corpus oraux</title>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Iris</first><last>Eshkol</last></author>
      <pages>303–316</pages>
      <abstract>Notre travail porte sur la détection automatique des segments en relation de reformulation paraphrastique dans les corpus oraux. L’approche proposée est une approche syntagmatique qui tient compte des marqueurs de reformulation paraphrastique et des spécificités de l’oral. Les données de référence sont consensuelles. Une méthode automatique fondée sur l’apprentissage avec les CRF est proposée afin de détecter les segments paraphrasés. Différents descripteurs sont exploités dans une fenêtre de taille variable. Les tests effectués montrent que les segments en relation de paraphrase sont assez difficiles à détecter, surtout avec leurs frontières correctes. Les meilleures moyennes atteignent 0,65 de F-mesure, 0,75 de précision et 0,63 de rappel. Nous avons plusieurs perspectives à ce travail pour améliorer la détection des segments en relation de paraphrase et pour étudier les données depuis d’autres points de vue.</abstract>
      <url hash="e7bf6847">2015.jeptalnrecital-long.26</url>
      <bibkey>grabar-eshkol-2015-des</bibkey>
    </paper>
  </volume>
  <volume id="court">
    <meta>
      <booktitle>Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles. Articles courts</booktitle>
      <editor><first>Jean-Marc</first><last>Lecarpentier</last></editor>
      <editor><first>Nadine</first><last>Lucas</last></editor>
      <publisher>ATALA</publisher>
      <address>Caen, France</address>
      <month>June</month>
      <year>2015</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="4d3d279f">2015.jeptalnrecital-court.0</url>
      <bibkey>jep-taln-recital-2015-actes-de</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Une méthode discriminant formation simple pour la traduction automatique avec Grands Caractéristiques</title>
      <author><first>Tian</first><last>Xia</last></author>
      <author><first>Shaodan</first><last>Zhai</last></author>
      <author><first>Zhongliang</first><last>Li</last></author>
      <author><first>Shaojun</first><last>Wang</last></author>
      <pages>1–6</pages>
      <abstract>Marge infusé algorithmes détendus (MIRAS) dominent modèle de tuning dans la traduction automatique statistique dans le cas des grandes caractéristiques de l’échelle, mais ils sont également célèbres pour la complexité de mise en œuvre. Nous introduisons une nouvelle méthode, qui concerne une liste des N meilleures comme une permutation et minimise la perte Plackett-Luce de permutations rez-de-vérité. Des expériences avec des caractéristiques à grande échelle démontrent que, la nouvelle méthode est plus robuste que MERT ; si ce est seulement à rattacher avec Miras, il a un avantage comparativement, plus facile à mettre en œuvre.</abstract>
      <url hash="1fd9decb">2015.jeptalnrecital-court.1</url>
      <bibkey>xia-etal-2015-une</bibkey>
    </paper>
    <paper id="2">
      <title>Natural Language Reasoning using Coq: Interaction and Automation</title>
      <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
      <pages>7–13</pages>
      <abstract>Dans cet article, nous présentons une utilisation des assistants des preuves pour traiter l’inférence en Language Naturel (NLI). D’ abord, nous proposons d’utiliser les theories des types modernes comme langue dans laquelle traduire la sémantique du langage naturel. Ensuite, nous implémentons cette sémantique dans l’assistant de preuve Coq pour raisonner sur ceux-ci. En particulier, nous évaluons notre proposition sur un sous-ensemble de la suite de tests FraCas, et nous montrons que 95.2

% des exemples peuvent être correctement prédits. Nous discutons ensuite la question de l’automatisation et il est démontré que le langage de tactiques de Coq permet de construire des tactiques qui peuvent automatiser entièrement les preuves, au moins pour les cas qui nous intéressent.</abstract>
      <url hash="cb9d0249">2015.jeptalnrecital-court.2</url>
      <bibkey>chatzikyriakidis-2015-natural</bibkey>
    </paper>
    <paper id="3">
      <title>Vous aimez ?...ou pas ? <fixed-case>L</fixed-case>ike<fixed-case>I</fixed-case>t, un jeu pour construire une ressource lexicale de polarité</title>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Nathalie Le</first><last>Brun</last></author>
      <author><first>Alain</first><last>Joubert</last></author>
      <pages>14–20</pages>
      <abstract>En analyse de discours ou d’opinion, savoir caractériser la connotation générale d’un texte, les sentiments qu’il véhicule, est une aptitude recherchée, qui suppose la constitution préalable d’une ressource lexicale de polarité. Au sein du réseau lexical JeuxDeMots, nous avons mis au point LikeIt, un jeu qui permet d’affecter une valeur positive, négative, ou neutre à un terme, et de constituer ainsi pour chaque terme, à partir des votes, une polarité résultante. Nous présentons ici l’analyse quantitative des données de polarité obtenues, ainsi que la méthode pour les valider qualitativement.</abstract>
      <url hash="9ec89534">2015.jeptalnrecital-court.3</url>
      <bibkey>lafourcade-etal-2015-vous</bibkey>
    </paper>
    <paper id="4">
      <title>Étude des verbes introducteurs de noms de médicaments dans les forums de santé</title>
      <author><first>François</first><last>Morlane-Hondère</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>21–27</pages>
      <abstract>Dans cet article, nous combinons annotations manuelle et automatique pour identifier les verbes utilisés pour introduire un médicament dans les messages sur les forums de santé. Cette information est notamment utile pour identifier la relation entre un médicament et un effet secondaire. La mention d’un médicament dans un message ne garantit pas que l’utilisateur a pris ce traitement mais qu’il effectue un retour. Nous montrons ensuite que ces verbes peuvent servir pour extraire automatiquement des variantes de noms de médicaments. Nous estimons que l’analyse de ces variantes pourrait permettre de modéliser les erreurs faites par les usagers des forums lorsqu’ils écrivent les noms de médicaments, et améliorer en conséquence les systèmes de recherche d’information.</abstract>
      <url hash="a4505c61">2015.jeptalnrecital-court.4</url>
      <bibkey>morlane-hondere-etal-2015-etude</bibkey>
    </paper>
    <paper id="5">
      <title>Initialisation de Réseaux de Neurones à l’aide d’un Espace Thématique</title>
      <author><first>Mohamed</first><last>Morchid</last></author>
      <author><first>Richard</first><last>Dufour</last></author>
      <author><first>Georges</first><last>Linarès</last></author>
      <pages>28–33</pages>
      <abstract>Ce papier présente une méthode de traitement de documents parlés intégrant une représentation fondée sur un espace thématique dans un réseau de neurones artificiels (ANN) employé comme classifieur de document. La méthode proposée consiste à configurer la topologie d’un ANN ainsi que d’initialiser les connexions de celui-ci à l’aide des espaces thématiques appris précédemment. Il est attendu que l’initialisation fondée sur les probabilités thématiques permette d’optimiser le processus d’optimisation des poids du réseau ainsi qu’à accélérer la phase d’apprentissage tout en amélioration la précision de la classification d’un document de test. Cette méthode est évaluée lors d’une tâche de catégorisation de dialogues parlés entre des utilisateurs et des agents du service d’appels de la Régie Autonome Des Transports Parisiens (RATP). Les résultats montrent l’intérêt de la méthode proposée d’initialisation d’un réseau, avec un gain observé de plus de 4 points en termes de bonne classification comparativement à l’initialisation aléatoire. De plus, les expérimentations soulignent que les performances sont faiblement dépendantes de la topologie du ANN lorsque les poids de la couche cachée sont initialisés au moyen des espaces de thèmes issus d’une allocation latente de Dirichlet ou latent Dirichlet Allocation (LDA) en comparaison à une initialisation empirique.</abstract>
      <url hash="046cc7b0">2015.jeptalnrecital-court.5</url>
      <bibkey>morchid-etal-2015-initialisation</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>FDTB</fixed-case>1: Repérage des connecteurs de discours en corpus</title>
      <author><first>Jacques</first><last>Steinlin</last></author>
      <author><first>Margot</first><last>Colinet</last></author>
      <author><first>Laurence</first><last>Danlos</last></author>
      <pages>34–40</pages>
      <abstract>Cet article présente le repérage manuel des connecteurs de discours dans le corpus FTB (French Treebank) déjà annoté pour la morpho-syntaxe. C’est la première étape de l’annotation discursive complète de ce corpus. Il s’agit de projeter sur le corpus les éléments répertoriés dans LexConn, lexique des connecteurs du français, et de filtrer les occurrences de ces éléments qui n’ont pas un emploi discursif mais par exemple un emploi d’adverbe de manière ou de préposition introduisant un complément sous-catégorisé. Plus de 10 000 connecteurs ont ainsi été repérés.</abstract>
      <url hash="22a75746">2015.jeptalnrecital-court.6</url>
      <bibkey>steinlin-etal-2015-fdtb1</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>ROBO</fixed-case>, an edit distance for sentence comparison Application to automatic summarization</title>
      <author><first>Aurélien</first><last>Bossard</last></author>
      <author><first>Christophe</first><last>Rodrigues</last></author>
      <pages>41–47</pages>
      <abstract>Dans cet article, nous proposons une mesure de distance entre phrases fondée sur la distance de Levenshtein, doublement pondérée par la fréquence des mots et par le type d’opération réalisée. Nous l’évaluons au sein d’un système de résumé automatique dont la méthode de calcul est volontairement limitée à une approche fondée sur la similarité entre phrases. Nous sommes donc ainsi en mesure d’évaluer indirectement la performance de cette nouvelle mesure de distance.</abstract>
      <url hash="89099862">2015.jeptalnrecital-court.7</url>
      <bibkey>bossard-rodrigues-2015-robo</bibkey>
    </paper>
    <paper id="8">
      <title>Classification d’entités nommées de type film</title>
      <author><first>Olivier</first><last>Collin</last></author>
      <author><first>Aleksandra</first><last>Guerraz</last></author>
      <pages>48–54</pages>
      <abstract>Dans cet article, nous nous intéressons à la classification contextuelle d’entités nommées de type film . Notre travail s’inscrit dans un cadre applicatif dont le but est de repérer, dans un texte, un titre de film contenu dans un catalogue (par exemple catalogue de films disponibles en VoD). Pour ce faire, nous combinons deux approches : nous partons d’un système à base de règles, qui présente une bonne précision, que nous couplons avec un modèle de langage permettant d’augmenter le rappel. La génération peu coûteuse de données d’apprentissage pour le modèle de langage à partir de Wikipedia est au coeur de ce travail. Nous montrons, à travers l’évaluation de notre système, la difficulté de classification des entités nommées de type film ainsi que la complémentarité des approches que nous utilisons pour cette tâche.</abstract>
      <url hash="989558b0">2015.jeptalnrecital-court.8</url>
      <bibkey>collin-guerraz-2015-classification</bibkey>
    </paper>
    <paper id="9">
      <title>A critical survey on measuring success in rank-based keyword assignment to documents</title>
      <author><first>Natalie</first><last>Schluter</last></author>
      <pages>55–60</pages>
      <abstract>Evaluation approaches for unsupervised rank-based keyword assignment are nearly as numerous as are the existing systems. The prolific production of each newly used metric (or metric twist) seems to stem from general dis-satisfaction with the previous one and the source of that dissatisfaction has not previously been discussed in the literature. The difficulty may stem from a poor specification of the keyword assignment task in view of the rank-based approach. With a more complete specification of this task, we aim to show why the previous evaluation metrics fail to satisfy researchers’ goals to distinguish and detect good rank-based keyword assignment systems. We put forward a characterisation of an ideal evaluation metric, and discuss the consistency of the evaluation metrics with this ideal, finding that the average standard normalised cumulative gain metric is most consistent with this ideal.</abstract>
      <url hash="88a0d255">2015.jeptalnrecital-court.9</url>
      <bibkey>schluter-2015-critical</bibkey>
    </paper>
    <paper id="10">
      <title>Effects of Graph Generation for Unsupervised Non-Contextual Single Document Keyword Extraction</title>
      <author><first>Natalie</first><last>Schluter</last></author>
      <pages>61–67</pages>
      <abstract>This paper presents an exhaustive study on the generation of graph input to unsupervised graph-based non-contextual single document keyword extraction systems. A concrete hypothesis on concept coordination for documents that are scientific articles is put forward, consistent with two separate graph models : one which is based on word adjacency in the linear text–an approach forming the foundation of all previous graph-based keyword extraction methods, and a novel one that is based on word adjacency modulo their modifiers. In doing so, we achieve a best reported NDCG score to date of 0.431 for any system on the same data. In terms of a best parameter f-score, we achieve the highest reported to date (0.714) at a reasonable ranked list cut-off of n = 6, which is also the best reported f-score for any keyword extraction or generation system in the literature on the same data. The best-parameter f-score corresponds to a reduction in error of 12.6

% conservatively.</abstract>
      <url hash="7765ac61">2015.jeptalnrecital-court.10</url>
      <bibkey>schluter-2015-effects</bibkey>
    </paper>
    <paper id="11">
      <title>Adaptation par enrichissement terminologique en traduction automatique statistique fondée sur la génération et le filtrage de bi-segments virtuels</title>
      <author><first>Christophe</first><last>Servan</last></author>
      <author><first>Marc</first><last>Dymetman</last></author>
      <pages>68–74</pages>
      <abstract>Nous présentons des travaux préliminaires sur une approche permettant d’ajouter des termes bilingues à un système de Traduction Automatique Statistique (TAS) à base de segments. Les termes sont non seulement inclus individuellement, mais aussi avec des contextes les englobant. Tout d’abord nous générons ces contextes en généralisant des motifs (ou patrons) observés pour des mots de même nature syntaxique dans un corpus bilingue. Enfin, nous filtrons les contextes qui n’atteignent pas un certain seuil de confiance, à l’aide d’une méthode de sélection de bi-segments inspirée d’une approche de sélection de données, précédemment appliquée à des textes bilingues alignés.</abstract>
      <url hash="6e56be0f">2015.jeptalnrecital-court.11</url>
      <bibkey>servan-dymetman-2015-adaptation</bibkey>
    </paper>
    <paper id="12">
      <title>Une mesure d’intérêt à base de surreprésentation pour l’extraction des motifs syntaxiques stylistiques</title>
      <author><first>Mohamed Amine</first><last>Boukhaled</last></author>
      <author><first>Francesca</first><last>Frontini</last></author>
      <author><first>Jean-Gabriel</first><last>Ganascia</last></author>
      <pages>75–80</pages>
      <abstract>Dans cette contribution, nous présentons une étude sur la stylistique computationnelle des textes de la littérature classiques française fondée sur une approche conduite par données, où la découverte des motifs linguistiques intéressants se fait sans aucune connaissance préalable. Nous proposons une mesure objective capable de capturer et d’extraire des motifs syntaxiques stylistiques significatifs à partir d’un œuvre d’un auteur donné. Notre hypothèse de travail est fondée sur le fait que les motifs syntaxiques les plus pertinents devraient refléter de manière significative le choix stylistique de l’auteur, et donc ils doivent présenter une sorte de comportement de surreprésentation contrôlé par les objectifs de l’auteur. Les résultats analysés montrent l’efficacité dans l’extraction de motifs syntaxiques intéressants dans le texte littéraire français classique, et semblent particulièrement prometteurs pour les analyses de ce type particulier de texte.</abstract>
      <url hash="892a4243">2015.jeptalnrecital-court.12</url>
      <bibkey>boukhaled-etal-2015-une</bibkey>
    </paper>
    <paper id="13">
      <title>Une Approche évolutionnaire pour le résumé automatique</title>
      <author><first>Aurélien</first><last>Bossard</last></author>
      <author><first>Christophe</first><last>Rodrigues</last></author>
      <pages>81–87</pages>
      <abstract>Dans cet article, nous proposons une méthode de résumé automatique fondée sur l’utilisation d’un algorithme génétique pour parcourir l’espace des résumés candidats couplé à un calcul de divergence de distribution de probabilités de n-grammes entre résumés candidats et documents source. Cette méthode permet de considérer un résumé non plus comme une accumulation de phrases indépendantes les unes des autres, mais comme un texte vu dans sa globalité. Nous la comparons à une des meilleures méthodes existantes fondée sur la programmation linéaire en nombre entier, et montrons son efficacité sur le corpus TAC 2009.</abstract>
      <url hash="b00b21c3">2015.jeptalnrecital-court.13</url>
      <bibkey>bossard-rodrigues-2015-une</bibkey>
    </paper>
    <paper id="14">
      <title>Identification des unités de mesure dans les textes scientifiques</title>
      <author><first>Soumia Lilia</first><last>Berrahou</last></author>
      <author><first>Patrice</first><last>Buche</last></author>
      <author><first>Juliette</first><last>Dibie-Barthélemy</last></author>
      <author><first>Mathieu</first><last>Roche</last></author>
      <pages>88–94</pages>
      <abstract>Le travail présenté dans cet article se situe dans le cadre de l’identification de termes spécialisés (unités de mesure) à partir de données textuelles pour enrichir une Ressource Termino-Ontologique (RTO). La première étape de notre méthode consiste à prédire la localisation des variants d’unités de mesure dans les documents. Nous avons utilisé une méthode reposant sur l’apprentissage supervisé. Cette méthode permet de réduire sensiblement l’espace de recherche des variants tout en restant dans un contexte optimal de recherche (réduction de 86

% de l’espace de recherché sur le corpus étudié). La deuxième étape du processus, une fois l’espace de recherche réduit aux variants d’unités, utilise une nouvelle mesure de similarité permettant d’identifier automatiquement les variants découverts par rapport à un terme d’unité déjà référencé dans la RTO avec un taux de précision de 82

% pour un seuil au dessus de 0.6 sur le corpus étudié.</abstract>
      <url hash="97e9e207">2015.jeptalnrecital-court.14</url>
      <bibkey>berrahou-etal-2015-identification</bibkey>
    </paper>
    <paper id="15">
      <title>Évaluation intrinsèque et extrinsèque du nettoyage de pages Web</title>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <author><first>Romain</first><last>Brixtel</last></author>
      <author><first>Charlotte</first><last>Lecluze</last></author>
      <pages>95–101</pages>
      <abstract>Le nettoyage de documents issus du web est une tâche importante pour le TAL en général et pour la constitution de corpus en particulier. Cette phase est peu traitée dans la littérature, pourtant elle n’est pas sans influence sur la qualité des informations extraites des corpus. Nous proposons deux types d’évaluation de cette tâche de détourage : (I) une évaluation intrinsèque fondée sur le contenu en mots, balises et caractères ; (II) une évaluation extrinsèque fondée sur la tâche, en examinant l’effet du détourage des documents sur le système placé en aval de la chaîne de traitement. Nous montrons que les résultats ne sont pas cohérents entre ces deux évaluations ainsi qu’entre les différentes langues. Ainsi, le choix d’un outil de détourage devrait être guidé par la tâche visée plutôt que par la simple évaluation intrinsèque.</abstract>
      <url hash="33672a75">2015.jeptalnrecital-court.15</url>
      <bibkey>lejeune-etal-2015-evaluation</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>CANÉPHORE</fixed-case> : un corpus français pour la fouille d’opinion ciblée</title>
      <author><first>Joseph</first><last>Lark</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Sebastián Peña</first><last>Saldarriaga</last></author>
      <pages>102–108</pages>
      <abstract>La fouille d’opinion ciblée (aspect-based sentiment analysis) fait l’objet ces dernières années d’un intérêt particulier, visible dans les sujets des récentes campagnes d’évaluation comme SemEval 2014 et 2015 ou bien DEFT 2015. Cependant les corpus annotés et publiquement disponibles permettant l’évaluation de cette tâche sont rares. Dans ce travail nous présentons en premier lieu un corpus français librement accessible de 10 000 tweets manuellement annotés. Nous accompagnons ce corpus de résultats de référence pour l’extraction de marqueurs d’opinion non supervisée. Nous présentons ensuite une méthode améliorant les résultats de cette extraction, en suivant une approche semi-supervisée.</abstract>
      <url hash="9c828bd6">2015.jeptalnrecital-court.16</url>
      <bibkey>lark-etal-2015-canephore</bibkey>
    </paper>
    <paper id="17">
      <title>Extraction de Contextes Riches en Connaissances en corpus spécialisés</title>
      <author><first>Firas</first><last>Hmida</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Béatrice</first><last>Daille</last></author>
      <pages>109–115</pages>
      <abstract>Les banques terminologiques et les dictionnaires sont des ressources précieuses qui facilitent l’accès aux connaissances des domaines spécialisés. Ces ressources sont souvent assez pauvres et ne proposent pas toujours pour un terme à illustrer des exemples permettant d’appréhender le sens et l’usage de ce terme. Dans ce contexte, nous proposons de mettre en œuvre la notion de Contextes Riches en Connaissances (CRC) pour extraire directement de corpus spécialisés des exemples de contextes illustrant son usage. Nous définissons un cadre unifié pour exploiter tout à la fois des patrons de connaissances et des collocations avec une qualité acceptable pour une révision humaine.</abstract>
      <url hash="acbfe0b1">2015.jeptalnrecital-court.17</url>
      <bibkey>hmida-etal-2015-extraction</bibkey>
    </paper>
    <paper id="18">
      <title>Traitement automatique des formes métriques des textes versifiés</title>
      <author><first>Eliane</first><last>Delente</last></author>
      <author><first>Richard</first><last>Renault</last></author>
      <pages>116–122</pages>
      <abstract>L’objectif de cet article est de présenter tout d’abord dans ses grandes lignes le projet Anamètre qui a pour objet le traitement automatique des formes métriques de la poésie et du théâtre français du début du XVIIe au début du XXe siècle. Nous présenterons ensuite un programme de calcul automatique des mètres appliqué à notre corpus dans le cadre d’une approche déterministe en nous appuyant sur la méthode métricométrique de B. de Cornulier ainsi que la procédure d’appariement des rimes et la détermination des schémas de strophes dans les suites périodiques et les formes fixes.</abstract>
      <url hash="05397ade">2015.jeptalnrecital-court.18</url>
      <bibkey>delente-renault-2015-traitement</bibkey>
    </paper>
    <paper id="19">
      <title>Apprentissage automatique d’un modèle de résolution de la coréférence à partir de données orales transcrites du français : le système <fixed-case>CROC</fixed-case></title>
      <author><first>Adèle</first><last>Désoyer</last></author>
      <author><first>Frédéric</first><last>Landragin</last></author>
      <author><first>Isabelle</first><last>Tellier</last></author>
      <pages>123–129</pages>
      <abstract>Cet article présente CROC 1 (Coreference Resolution for Oral Corpus), un premier système de résolution des coréférences en français reposant sur des techniques d’apprentissage automatique. Une des spécificités du système réside dans son apprentissage sur des données exclusivement orales, à savoir ANCOR (anaphore et coréférence dans les corpus oraux), le premier corpus de français oral transcrit annoté en relations anaphoriques. En l’état actuel, le système CROC nécessite un repérage préalable des mentions. Nous détaillons les choix des traits – issus du corpus ou calculés – utilisés par l’apprentissage, et nous présentons un ensemble d’expérimentations avec ces traits. Les scores obtenus sont très proches de ceux de l’état de l’art des systèmes conçus pour l’écrit. Nous concluons alors en donnant des perspectives sur la réalisation d’un système end-to-end valable à la fois pour l’oral transcrit et l’écrit.</abstract>
      <url hash="4daf0132">2015.jeptalnrecital-court.19</url>
      <bibkey>desoyer-etal-2015-apprentissage</bibkey>
    </paper>
    <paper id="20">
      <title>Vers un diagnostic d’ambiguïté des termes candidats d’un texte</title>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <author><first>Béatrice</first><last>Daille</last></author>
      <pages>130–136</pages>
      <abstract>Les recherches autour de la désambiguïsation sémantique traitent de la question du sens à accorder à différentes occurrences d’un mot ou plus largement d’une unité lexicale. Dans cet article, nous nous intéressons à l’ambiguïté d’un terme en domaine de spécialité. Nous posons les premiers jalons de nos recherches sur une question connexe que nous nommons le diagnostic d’ambiguïté. Cette tâche consiste à décider si une occurrence d’un terme est ou n’est pas ambiguë. Nous mettons en œuvre une approche d’apprentissage supervisée qui exploite un corpus d’articles de sciences humaines rédigés en français dans lequel les termes ambigus ont été détectés par des experts. Le diagnostic s’appuie sur deux types de traits : syntaxiques et positionnels. Nous montrons l’intérêt de la structuration du texte pour établir le diagnostic d’ambiguïté.</abstract>
      <url hash="958607b7">2015.jeptalnrecital-court.20</url>
      <bibkey>lejeune-daille-2015-vers</bibkey>
    </paper>
    <paper id="21">
      <title>Augmentation d’index par propagation sur un réseau lexical Application aux comptes rendus de radiologie</title>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Lionel</first><last>Ramadier</last></author>
      <pages>137–143</pages>
      <abstract>Les données médicales étant de plus en plus informatisées, le traitement sémantiquement efficace des rapports médicaux est devenu une nécessité. La recherche d’images radiologiques peut être grandement facilitée grâce à l’indexation textuelle des comptes rendus associés. Nous présentons un algorithme d’augmentation d’index de comptes rendus fondé sur la propagation d’activation sur un réseau lexico-sémantique généraliste.</abstract>
      <url hash="a04e16f0">2015.jeptalnrecital-court.21</url>
      <bibkey>lafourcade-ramadier-2015-augmentation</bibkey>
    </paper>
    <paper id="22">
      <title>Détection automatique de l’ironie dans les tweets en français</title>
      <author><first>Jihen</first><last>Karoui</last></author>
      <author><first>Farah Benamara</first><last>Zitoune</last></author>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Nathalie</first><last>Aussenac-Gilles</last></author>
      <author><first>Lamia Hadrich</first><last>Belguith</last></author>
      <pages>144–149</pages>
      <abstract>Cet article présente une méthode par apprentissage supervisé pour la détection de l’ironie dans les tweets en français. Un classifieur binaire utilise des traits de l’état de l’art dont les performances sont reconnues, ainsi que de nouveaux traits issus de notre étude de corpus. En particulier, nous nous sommes intéressés à la négation et aux oppositions explicites/implicites entre des expressions d’opinion ayant des polarités différentes. Les résultats obtenus sont encourageants.</abstract>
      <url hash="9c36f9bc">2015.jeptalnrecital-court.22</url>
      <bibkey>karoui-etal-2015-detection</bibkey>
    </paper>
    <paper id="23">
      <title>Dictionnaires morphologiques du français contemporain : présentation de Morfetik, éléments d’un modèle pour le <fixed-case>TAL</fixed-case></title>
      <author><first>Michel</first><last>Mathieu-Colas</last></author>
      <author><first>Emmanuel</first><last>Cartier</last></author>
      <author><first>Aude</first><last>Grezka</last></author>
      <pages>150–156</pages>
      <abstract>Dans cet article, nous présentons une ressource linguistique, Morfetik, développée au LDI. Après avoir présenté le modèle sous-jacent et spécifié les modalités de sa construction, nous comparons cette ressource avec d’autres ressources du français : le GLAFF, le LEFF, Morphalou et Dicolecte. Nous étudions ensuite la couverture lexicale de ces dictionnaires sur trois corpus, le Wikipedia français, la version française de Wacky et les dix ans du Monde. Nous concluons par un programme de travail permettant de mettre à jour de façon continue la ressource lexicographique du point de vue des formes linguistiques, en connectant la ressource à un corpus continu.</abstract>
      <url hash="44678625">2015.jeptalnrecital-court.23</url>
      <bibkey>mathieu-colas-etal-2015-dictionnaires</bibkey>
    </paper>
    <paper id="24">
      <title>Une métagrammaire de l’interface morpho-sémantique dans les verbes en arabe</title>
      <author><first>Simon</first><last>Petitjean</last></author>
      <author><first>Younes</first><last>Samih</last></author>
      <author><first>Timm</first><last>Lichte</last></author>
      <pages>157–163</pages>
      <abstract>Dans cet article, nous présentons une modélisation de la morphologie dérivationnelle de l’arabe utilisant le cadre métagrammatical offert par XMG. Nous démontrons que l’utilisation de racines et patrons abstraits comme morphèmes atomiques sous-spécifiés offre une manière élégante de traiter l’interaction entre morphologie et sémantique.</abstract>
      <url hash="94995566">2015.jeptalnrecital-court.24</url>
      <bibkey>petitjean-etal-2015-une</bibkey>
    </paper>
    <paper id="25">
      <title>Création d’un nouveau treebank à partir de quatrièmes de couverture</title>
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Grégoire</first><last>Moncheuil</last></author>
      <author><first>Stéphane</first><last>Rauzy</last></author>
      <author><first>Marie-Laure</first><last>Guénot</last></author>
      <pages>164–170</pages>
      <abstract>Nous présentons ici 4-couv, un nouveau corpus arboré d’environ 3 500 phrases, constitué d’un ensemble de quatrièmes de couverture, étiqueté et analysé automatiquement puis corrigé et validé à la main. Il répond à des besoins spécifiques pour des projets de linguistique expérimentale, et vise à rester compatible avec les autres treebanks existants pour le français. Nous présentons ici le corpus lui-même ainsi que les outils utilisés pour les différentes étapes de son élaboration : choix des textes, étiquetage, parsing, correction manuelle.</abstract>
      <url hash="714de24b">2015.jeptalnrecital-court.25</url>
      <bibkey>blache-etal-2015-creation</bibkey>
    </paper>
    <paper id="26">
      <title>Entre écrit et oral ? Analyse comparée de conversations de type tchat et de conversations téléphoniques dans un centre de contact client</title>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <author><first>Aleksandra</first><last>Guerraz</last></author>
      <author><first>Delphine</first><last>Charlet</last></author>
      <pages>171–177</pages>
      <abstract>Dans cet article nous proposons une première étude descriptive d’un corpus de conversations de type tchat issues d’un centre de contact d’assistance. Les dimensions lexicales, syntaxiques et interactionnelles sont analysées. L’étude parallèle de transcriptions de conversations téléphoniques issues d’un centre d’appel dans le même domaine de l’assistance permet d’établir des comparaisons entre ces deux modes d’interaction. L’analyse révèle des différences marquées en termes de déroulement de la conversation, avec une plus grande efficacité pour les conversations de type tchat malgré un plus grand étalement temporel. L’analyse lexicale et syntaxique révèle également des différences de niveaux de langage avec une plus grande proximité entre le client et le téléconseiller à l’oral que pour les tchats où le décalage entre le style adopté par le téléconseiller et l’expression du client est plus important.</abstract>
      <url hash="fb70a130">2015.jeptalnrecital-court.26</url>
      <bibkey>damnati-etal-2015-entre</bibkey>
    </paper>
    <paper id="27">
      <title>Construction et maintenance d’une ressource lexicale basées sur l’usage</title>
      <author><first>Laurie</first><last>Planes</last></author>
      <pages>178–184</pages>
      <abstract>Notre société développe un moteur de recherche (MR) sémantique basé sur la reformulation de requête. Notre MR s’appuie sur un lexique que nous avons construit en nous inspirant de la Théorie Sens-Texte (TST). Nous présentons ici notre ressource lexicale et indiquons comment nous l’enrichissons et la maintenons en fonction des besoins détectés à l’usage. Nous abordons également la question de l’adaptation de la TST à nos besoins.</abstract>
      <url hash="c890b3bb">2015.jeptalnrecital-court.27</url>
      <bibkey>planes-2015-construction</bibkey>
    </paper>
    <paper id="28">
      <title>Utilisation d’annotations sémantiques pour la validation automatique d’hypothèses dans des conversations téléphoniques</title>
      <author><first>Carole</first><last>Lailler</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <author><first>Renato</first><last>De Mori</last></author>
      <author><first>Mohamed</first><last>Bouallègue</last></author>
      <author><first>Mohamed</first><last>Morchid</last></author>
      <pages>185–191</pages>
      <abstract>Les travaux présentés portent sur l’extraction automatique d’unités sémantiques et l’évaluation de leur pertinence pour des conversations téléphoniques. Le corpus utilisé est le corpus français DECODA. L’objectif de la tâche est de permettre l’étiquetage automatique en thème de chaque conversation. Compte tenu du caractère spontané de ce type de conversations et de la taille du corpus, nous proposons de recourir à une stratégie semi-supervisée fondée sur la construction d’une ontologie et d’un apprentissage actif simple : un annotateur humain analyse non seulement les listes d’unités sémantiques candidates menant au thème mais étudie également une petite quantité de conversations. La pertinence de la relation unissant les unités sémantiques conservées, le sous-thème issu de l’ontologie et le thème annoté est évaluée par un DNN, prenant en compte une représentation vectorielle du document. L’intégration des unités sémantiques retenues dans le processus de classification en thème améliore les performances.</abstract>
      <url hash="f166b3a4">2015.jeptalnrecital-court.28</url>
      <bibkey>lailler-etal-2015-utilisation</bibkey>
    </paper>
    <paper id="29">
      <title>Etiquetage morpho-syntaxique en domaine de spécialité: le domaine médical</title>
      <author><first>Christelle</first><last>Rabary</last></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <pages>192–198</pages>
      <abstract>L’étiquetage morpho-syntaxique est une tâche fondamentale du Traitement Automatique de la Langue, sur laquelle reposent souvent des traitements plus complexes tels que l’extraction d’information ou la traduction automatique. L’étiquetage en domaine de spécialité est limité par la disponibilité d’outils et de corpus annotés spécifiques au domaine. Dans cet article, nous présentons le développement d’un corpus clinique du français annoté morpho-syntaxiquement à l’aide d’un jeu d’étiquettes issus des guides d’annotation French Treebank et Multitag. L’analyse de ce corpus nous permet de caractériser le domaine clinique et de dégager les points clés pour l’adaptation d’outils d’analyse morpho-syntaxique à ce domaine. Nous montrons également les limites d’un outil entraîné sur un corpus journalistique appliqué au domaine clinique. En perspective de ce travail, nous envisageons une application du corpus clinique annoté pour améliorer l’étiquetage morpho-syntaxique des documents cliniques en français.</abstract>
      <url hash="a1f18ddc">2015.jeptalnrecital-court.29</url>
      <bibkey>rabary-etal-2015-etiquetage</bibkey>
    </paper>
    <paper id="30">
      <title>Vers une typologie de liens entre contenus journalistiques</title>
      <author><first>Remi</first><last>Bois</last></author>
      <author><first>Guillaume</first><last>Gravier</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Pascale</first><last>Sébillot</last></author>
      <pages>199–205</pages>
      <abstract>Nous présentons une typologie de liens pour un corpus multimédia ancré dans le domaine journalistique. Bien que plusieurs typologies aient été créées et utilisées par la communauté, aucune ne permet de répondre aux enjeux de taille et de variété soulevés par l’utilisation d’un corpus large comprenant des textes, des vidéos, ou des émissions radiophoniques. Nous proposons donc une nouvelle typologie, première étape visant à la création et la catégorisation automatique de liens entre des fragments de documents afin de proposer de nouveaux modes de navigation au sein d’un grand corpus. Plusieurs exemples d’instanciation de la typologie sont présentés afin d’illustrer son intérêt.</abstract>
      <url hash="6f664886">2015.jeptalnrecital-court.30</url>
      <bibkey>bois-etal-2015-vers</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>CDGF</fixed-case>r, un corpus en dépendances non-projectives pour le français</title>
      <author><first>Denis</first><last>Béchet</last></author>
      <author><first>Ophélie</first><last>Lacroix</last></author>
      <pages>206–212</pages>
      <abstract>Dans le cadre de l’analyse en dépendances du français, le phénomène de la non-projectivité est peu pris en compte, en majeure partie car les donneés sur lesquelles sont entraînés les analyseurs représentent peu ou pas ces cas particuliers. Nous présentons, dans cet article, un nouveau corpus en dépendances pour le français, librement disponible, contenant un nombre substantiel de dépendances non-projectives. Ce corpus permettra d’étudier et de mieux prendre en compte les cas de non-projectivité dans l’analyse du français.</abstract>
      <url hash="d037f758">2015.jeptalnrecital-court.31</url>
      <bibkey>bechet-lacroix-2015-cdgfr</bibkey>
    </paper>
    <paper id="32">
      <title>Utilisation des réseaux de neurones récurrents pour la projection interlingue d’étiquettes morpho-syntaxiques à partir d’un corpus parallèle</title>
      <author><first>Othman</first><last>Zennaki</last></author>
      <author><first>Nasredine</first><last>Semmar</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>213–220</pages>
      <abstract>La construction d’outils d’analyse linguistique pour les langues faiblement dotées est limitée, entre autres, par le manque de corpus annotés. Dans cet article, nous proposons une méthode pour construire automatiquement des outils d’analyse via une projection interlingue d’annotations linguistiques en utilisant des corpus parallèles. Notre approche n’utilise pas d’autres sources d’information, ce qui la rend applicable à un large éventail de langues peu dotées. Nous proposons d’utiliser les réseaux de neurones récurrents pour projeter les annotations d’une langue à une autre (sans utiliser d’information d’alignement des mots). Dans un premier temps, nous explorons la tâche d’annotation morpho-syntaxique. Notre méthode combinée avec une méthode de projection d’annotation basique (utilisant l’alignement mot à mot), donne des résultats comparables à ceux de l’état de l’art sur une tâche similaire.</abstract>
      <url hash="dd136c9f">2015.jeptalnrecital-court.32</url>
      <bibkey>zennaki-etal-2015-utilisation</bibkey>
    </paper>
    <paper id="33">
      <title>Segmentation et Titrage Automatique de Journaux Télévisés</title>
      <author><first>Abdessalam</first><last>Bouchekif</last></author>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <author><first>Nathalie</first><last>Camelin</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <author><first>Delphine</first><last>Charlet</last></author>
      <pages>221–227</pages>
      <abstract>Dans cet article, nous nous intéressons au titrage automatique des segments issus de la segmentation thématique de journaux télévisés. Nous proposons d’associer un segment à un article de presse écrite collecté le jour même de la diffusion du journal. La tâche consiste à apparier un segment à un article de presse à l’aide d’une mesure de similarité. Cette approche soulève plusieurs problèmes, comme la sélection des articles candidats, une bonne représentation du segment et des articles, le choix d’une mesure de similarité robuste aux imprécisions de la segmentation. Des expériences sont menées sur un corpus varié de journaux télévisés français collectés pendant une semaine, conjointement avec des articles aspirés à partir de la page d’accueil de Google Actualités. Nous introduisons une métrique d’évaluation reflétant la qualité de la segmentation, du titrage ainsi que la qualité conjointe de la segmentation et du titrage. L’approche donne de bonnes performances et se révèle robuste à la segmentation thématique.</abstract>
      <url hash="74449f37">2015.jeptalnrecital-court.33</url>
      <bibkey>bouchekif-etal-2015-segmentation</bibkey>
    </paper>
    <paper id="34">
      <title>Un système hybride pour l’analyse de sentiments associés aux aspects</title>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>Diana Nicoleta</first><last>Popa</last></author>
      <author><first>Claude</first><last>Roux</last></author>
      <pages>228–234</pages>
      <abstract>Cet article présente en détails notre participation à la tâche 4 de SemEval2014 (Analyse de Sentiments associés aux Aspects). Nous présentons la tâche et décrivons précisément notre système qui consiste en une combinaison de composants linguistiques et de modules de classification. Nous exposons ensuite les résultats de son évaluation, ainsi que les résultats des meilleurs systèmes. Nous concluons par la présentation de quelques nouvelles expériences réalisées en vue de l’amélioration de ce système.</abstract>
      <url hash="910fc207">2015.jeptalnrecital-court.34</url>
      <bibkey>brun-etal-2015-un</bibkey>
    </paper>
    <paper id="35">
      <title>La ressource <fixed-case>EXPLICADIS</fixed-case>, un corpus annoté spécifiquement pour l’étude des relations de discours causales</title>
      <author><first>Caroline</first><last>Atallah</last></author>
      <pages>235–241</pages>
      <abstract>Dans le but de proposer une caractérisation des relations de discours liées à la causalité, nous avons été amenée à constituer et annoter notre propre corpus d’étude : la ressource EXPLICADIS (EXPlication et Argumentation en DIScours). Cette ressource a été construite dans la continuité d’une ressource déjà disponible, le corpus ANNODIS. Proposant une annotation plus précise des relations causales sur un ensemble de textes diversifiés en genres textuels, EXPLICADIS est le premier corpus de ce type constitué spécifiquement pour l’étude des relations de discours causales.</abstract>
      <url hash="89f562a4">2015.jeptalnrecital-court.35</url>
      <bibkey>atallah-2015-la</bibkey>
    </paper>
    <paper id="36">
      <title>La séparation des composantes lexicale et flexionnelle des vecteurs de mots</title>
      <author><first>François</first><last>Lareau</last></author>
      <author><first>Gabriel</first><last>Bernier-Colborne</last></author>
      <author><first>Patrick</first><last>Drouin</last></author>
      <pages>242–248</pages>
      <abstract>En sémantique distributionnelle, le sens des mots est modélisé par des vecteurs qui représentent leur distribution en corpus. Les modèles étant souvent calculés sur des corpus sans pré-traitement linguistique poussé, ils ne permettent pas de rendre bien compte de la compositionnalité morphologique des mots-formes. Nous proposons une méthode pour décomposer les vecteurs de mots en vecteurs lexicaux et flexionnels.</abstract>
      <url hash="d15af8fd">2015.jeptalnrecital-court.36</url>
      <bibkey>lareau-etal-2015-la</bibkey>
    </paper>
    <paper id="37">
      <title>Traitements pour l’analyse du français préclassique</title>
      <author><first>Sascha</first><last>Diwersy</last></author>
      <author><first>Achille</first><last>Falaise</last></author>
      <author><first>Marie-Hélène</first><last>Lay</last></author>
      <author><first>Gilles</first><last>Souvay</last></author>
      <pages>249–255</pages>
      <abstract>La période préclassique du français s’étend sur tout le XVIe siècle et la première moitié du XVIIe siècle. Cet état de langue écrite, qui accompagne les débuts de l’imprimerie, est relativement proche du français moderne, mais se caractérise par une grande variabilité graphique. Il s’agit de l’un des moins bien dotés en termes de ressources. Nous présentons ici la construction d’un lexique, d’un corpus d’apprentissage et d’un modèle de langage pour la période préclassique, à partir de ressources du français moderne.</abstract>
      <url hash="4a437294">2015.jeptalnrecital-court.37</url>
      <bibkey>diwersy-etal-2015-traitements</bibkey>
    </paper>
    <paper id="38">
      <title>Classification de texte enrichie à l’aide de motifs séquentiels</title>
      <author><first>Pierre</first><last>Holat</last></author>
      <author><first>Nadi</first><last>Tomeh</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <pages>256–262</pages>
      <abstract>En classification de textes, la plupart des méthodes fondées sur des classifieurs statistiques utilisent des mots, ou des combinaisons de mots contigus, comme descripteurs. Si l’on veut prendre en compte plus d’informations le nombre de descripteurs non contigus augmente exponentiellement. Pour pallier à cette croissance, la fouille de motifs séquentiels permet d’extraire, de façon efficace, un nombre réduit de descripteurs qui sont à la fois fréquents et pertinents grâce à l’utilisation de contraintes. Dans ce papier, nous comparons l’utilisation de motifs fréquents sous contraintes et l’utilisation de motifs -libres, comme descripteurs. Nous montrons les avantages et inconvénients de chaque type de motif.</abstract>
      <url hash="47966fef">2015.jeptalnrecital-court.38</url>
      <bibkey>holat-etal-2015-classification</bibkey>
    </paper>
    <paper id="39">
      <title>Le traitement des collocations en génération de texte multilingue</title>
      <author><first>Florie</first><last>Lambrey</last></author>
      <author><first>François</first><last>Lareau</last></author>
      <pages>263–269</pages>
      <abstract>Pour concevoir des générateurs automatiques de texte génériques qui soient facilement réutilisables d’une langue et d’une application à l’autre, il faut modéliser les principaux phénomènes linguistiques qu’on retrouve dans les langues en général. Un des phénomènes fondamentaux qui demeurent problématiques pour le TAL est celui des collocations, comme grippe carabinée, peur bleue ou désir ardent, où un sens (ici, l’intensité) ne s’exprime pas de la même façon selon l’unité lexicale qu’il modifie. Dans la lexicographie explicative et combinatoire, on modélise les collocations au moyen de fonctions lexicales qui correspondent à des patrons récurrents de collocations. Par exemple, les expressions mentionnées ici se décrivent au moyen de la fonction Magn : Magn(PEUR) = BLEUE, Magn(GRIPPE) = CARABINÉE, etc. Il existe des centaines de fonctions lexicales. Dans cet article, nous nous intéressons à l’implémentation d’un sous-ensemble de fonctions qui décrivent les verbes supports et certains types de modificateurs.</abstract>
      <url hash="881ee5cf">2015.jeptalnrecital-court.39</url>
      <bibkey>lambrey-lareau-2015-le</bibkey>
    </paper>
    <paper id="40">
      <title>Médicaments qui soignent, médicaments qui rendent malades : étude des relations causales pour identifier les effets secondaires</title>
      <author><first>François</first><last>Morlane-Hondère</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>270–276</pages>
      <abstract>Dans cet article, nous nous intéressons à la manière dont sont exprimés les liens qui existent entre un traitement médical et un effet secondaire. Parce que les patients se tournent en priorité vers internet, nous fondons cette étude sur un corpus annoté de messages issus de forums de santé en français. L’objectif de ce travail consiste à mettre en évidence des éléments linguistiques (connecteurs logiques et expressions temporelles) qui pourraient être utiles pour des systèmes automatiques de repérage des effets secondaires. Nous observons que les modalités d’écriture sur les forums ne permettent pas de se fonder sur les expressions temporelles. En revanche, les connecteurs logiques semblent utiles pour identifier les effets secondaires.</abstract>
      <url hash="ffdf2202">2015.jeptalnrecital-court.40</url>
      <bibkey>morlane-hondere-etal-2015-medicaments</bibkey>
    </paper>
    <paper id="41">
      <title>Exploration de modèles distributionnels au moyen de graphes 1-<fixed-case>PPV</fixed-case></title>
      <author><first>Gabriel</first><last>Bernier-Colborne</last></author>
      <pages>277–283</pages>
      <abstract>Dans cet article, nous montrons qu’un graphe à 1 plus proche voisin (graphe 1-PPV) offre différents moyens d’explorer les voisinages sémantiques captés par un modèle distributionnel. Nous vérifions si les composantes connexes de ce graphe, qui représentent des ensembles de mots apparaissant dans des contextes similaires, permettent d’identifier des ensembles d’unités lexicales qui évoquent un même cadre sémantique. Nous illustrons également différentes façons d’exploiter le graphe 1-PPV afin d’explorer un modèle ou de comparer différents modèles.</abstract>
      <url hash="202fb321">2015.jeptalnrecital-court.41</url>
      <bibkey>bernier-colborne-2015-exploration</bibkey>
    </paper>
    <paper id="42">
      <title>Apport de l’information temporelle des contextes pour la représentation vectorielle continue des mots</title>
      <author><first>Killian</first><last>Janod</last></author>
      <author><first>Mohamed</first><last>Morchid</last></author>
      <author><first>Richard</first><last>Dufour</last></author>
      <author><first>Georges</first><last>Linares</last></author>
      <pages>284–290</pages>
      <abstract>Les représentations vectorielles continues des mots sont en plein essor et ont déjà été appliquées avec succès à de nombreuses tâches en traitement automatique de la langue (TAL). Dans cet article, nous proposons d’intégrer l’information temporelle issue du contexte des mots au sein des architectures fondées sur les sacs-de-mots continus (continuous bag-of-words ou CBOW) ou sur les Skip-Grams. Ces approches sont manipulées au travers d’un réseau de neurones, l’architecture CBOW cherchant alors à prédire un mot sachant son contexte, alors que l’architecture Skip-Gram prédit un contexte sachant un mot. Cependant, ces modèles, au travers du réseau de neurones, s’appuient sur des représentations en sac-de-mots et ne tiennent pas compte, explicitement, de l’ordre des mots. En conséquence, chaque mot a potentiellement la même influence dans le réseau de neurones. Nous proposons alors une méthode originale qui intègre l’information temporelle des contextes des mots en utilisant leur position relative. Cette méthode s’inspire des modèles contextuels continus. L’information temporelle est traitée comme coefficient de pondération, en entrée du réseau de neurones par le CBOW et dans la couche de sortie par le Skip-Gram. Les premières expériences ont été réalisées en utilisant un corpus de test mesurant la qualité de la relation sémantique-syntactique des mots. Les résultats préliminaires obtenus montrent l’apport du contexte des mots, avec des gains de 7 et 7,7 points respectivement avec l’architecture Skip-Gram et l’architecture CBOW.</abstract>
      <url hash="4cd158c2">2015.jeptalnrecital-court.42</url>
      <bibkey>janod-etal-2015-apport</bibkey>
    </paper>
    <paper id="43">
      <title>Etiquetage morpho-syntaxique de tweets avec des <fixed-case>CRF</fixed-case></title>
      <author><first>Tian</first><last>Tian</last></author>
      <author><first>Dinarelli</first><last>Marco</last></author>
      <author><first>Tellier</first><last>Isabelle</last></author>
      <author><first>Cardoso</first><last>Pedro</last></author>
      <pages>291–297</pages>
      <abstract>Nous nous intéressons dans cet article à l’apprentissage automatique d’un étiqueteur mopho-syntaxique pour les tweets en anglais. Nous proposons tout d’abord un jeu d’étiquettes réduit avec 17 étiquettes différentes, qui permet d’obtenir de meilleures performances en exactitude par rapport au jeu d’étiquettes traditionnel qui contient 45 étiquettes. Comme nous disposons de peu de tweets étiquetés, nous essayons ensuite de compenser ce handicap en ajoutant dans l’ensemble d’apprentissage des données issues de textes bien formés. Les modèles mixtes obtenus permettent d’améliorer les résultats par rapport aux modèles appris avec un seul corpus, qu’il soit issu de Twitter ou de textes journalistiques.</abstract>
      <url hash="926a32a5">2015.jeptalnrecital-court.43</url>
      <bibkey>tian-etal-2015-etiquetage</bibkey>
    </paper>
    <paper id="44">
      <title>Caractériser les discours académiques et de vulgarisation : quelles propriétés ?</title>
      <author><first>Amalia</first><last>Todirascu</last></author>
      <author><first>Beatriz Sanchez</first><last>Cardenas</last></author>
      <pages>298–304</pages>
      <abstract>L’article présente une étude des propriétés linguistiques (lexicales, morpho-syntaxiques, syntaxiques) permettant la classification automatique de documents selon leur genre (articles scientifiques et articles de vulgarisation), dans deux domaines différentes (médecine et informatique). Notre analyse, effectuée sur des corpus comparables en genre et en thèmes disponibles en français, permet de valider certaines propriétés identifiées dans la littérature comme caractéristiques des discours académiques ou de vulgarisation scientifique. Les premières expériences de classification évaluent l’influence de ces propriétés pour l’identification automatique du genre pour le cas spécifique des textes scientifiques ou de vulgarisation.</abstract>
      <url hash="0f67e999">2015.jeptalnrecital-court.44</url>
      <bibkey>todirascu-cardenas-2015-caracteriser</bibkey>
    </paper>
    <paper id="45">
      <title>Extraction et analyse automatique des comparaisons et des pseudo-comparaisons pour la détection des comparaisons figuratives</title>
      <author><first>Suzanne</first><last>Mpouli</last></author>
      <author><first>Jean-Gabriel</first><last>Ganascia</last></author>
      <pages>305–311</pages>
      <abstract>Le présent article s’intéresse à la détection et à la désambiguïsation des comparaisons figuratives. Il décrit un algorithme qui utilise un analyseur syntaxique de surface (chunker) et des règles manuelles afin d’extraire et d’analyser les (pseudo-)comparaisons présentes dans un texte. Cet algorithme, évalué sur un corpus de textes littéraires, donne de meilleurs résultats qu’un système reposant sur une analyse syntaxique profonde.</abstract>
      <url hash="50ecf5b8">2015.jeptalnrecital-court.45</url>
      <bibkey>mpouli-ganascia-2015-extraction</bibkey>
    </paper>
    <paper id="46">
      <title>Proposition méthodologique pour la détection automatique de Community Manager. Étude multilingue sur un corpus relatif à la Junk Food</title>
      <author><first>Johan</first><last>Ferguth</last></author>
      <author><first>Aurélie</first><last>Jouannet</last></author>
      <author><first>Asma</first><last>Zamiti</last></author>
      <author><first>Yunhe</first><last>Wu</last></author>
      <author><first>Jia</first><last>Li</last></author>
      <author><first>Antonina</first><last>Bondarenko</last></author>
      <author><first>Damien</first><last>Nouvel</last></author>
      <author><first>Mathieu</first><last>Valette</last></author>
      <pages>312–318</pages>
      <abstract>Dans cet article, nous présentons une méthodologie pour l’identification de messages suspectés d’être produits par des Community Managers à des fins commerciales déguisées dans des documents du Web 2.0. Le champ d’application est la malbouffe (junkfood) et le corpus est multilingue (anglais, chinois, français). Nous exposons dans un premier temps la stratégie de constitution et d’annotation de nos corpus, en explicitant notamment notre guide d’annotation, puis nous développons la méthode adoptée, basée sur la combinaison d’une analyse textométrique et d’un apprentissage supervisé.</abstract>
      <url hash="56f42e51">2015.jeptalnrecital-court.46</url>
      <bibkey>ferguth-etal-2015-proposition</bibkey>
    </paper>
  </volume>
  <volume id="demonstration">
    <meta>
      <booktitle>Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles. Démonstrations</booktitle>
      <editor><first>Jean-Marc</first><last>Lecarpentier</last></editor>
      <editor><first>Nadine</first><last>Lucas</last></editor>
      <publisher>ATALA</publisher>
      <address>Caen, France</address>
      <month>June</month>
      <year>2015</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="4d3d279f">2015.jeptalnrecital-demonstration.0</url>
      <bibkey>jep-taln-recital-2015-actes-de-la</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>MEDITE</fixed-case> : logiciel d’alignement de textes pour l’étude de la génétique textuelle</title>
      <author><first>Zied</first><last>Sellami</last></author>
      <author><first>Jean-Gabriel</first><last>Ganascia</last></author>
      <author><first>Mohamed Amine</first><last>Boukhaled</last></author>
      <pages>1–2</pages>
      <abstract>MEDITE est un logiciel d’alignement de textes permettant l’identification de transformations entre une version et une autre d’un même texte. Dans ce papier nous présentons les aspects théoriques et techniques de MEDITE.</abstract>
      <url hash="076b98c2">2015.jeptalnrecital-demonstration.1</url>
      <bibkey>sellami-etal-2015-medite</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>P</fixed-case>hœbus : un Logiciel d’Extraction de Réutilisations dans des Textes Littéraires</title>
      <author><first>Mohamed Amine</first><last>Boukhaled</last></author>
      <author><first>Zied</first><last>Sellami</last></author>
      <author><first>Jean-Gabriel</first><last>Ganascia</last></author>
      <pages>3–5</pages>
      <abstract>Phœbus est un logiciel d’extraction de réutilisations dans des textes littéraires. Il a été développé comme un outil d’analyse littéraire assistée par ordinateur. Dans ce contexte, ce logiciel détecte automatiquement et explore des réseaux de réutilisation textuelle dans la littérature classique.</abstract>
      <url hash="1ea41e89">2015.jeptalnrecital-demonstration.2</url>
      <bibkey>boukhaled-etal-2015-phoebus</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>YADTK</fixed-case> : Une plateforme open-source à base de règles pour développer des systèmes de dialogue oral</title>
      <author><first>Jérôme</first><last>Lehuen</last></author>
      <author><first>Carole</first><last>Lailler</last></author>
      <author><first>Julien</first><last>Stenzhorn</last></author>
      <pages>6–7</pages>
      <abstract>YADTK est une plateforme open-source pour développer des systèmes de dialogue oral. De part son caractère déclaratif et unifié, le modèle de représentation des connaissances permet un développement rapide et facilité.</abstract>
      <url hash="aeac7e32">2015.jeptalnrecital-demonstration.3</url>
      <bibkey>lehuen-etal-2015-yadtk</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>T</fixed-case>erm<fixed-case>L</fixed-case>is : un contexte d’information logique pour des ressources terminologiques</title>
      <author><first>Annie</first><last>Foret</last></author>
      <pages>8–9</pages>
      <abstract>Nous présentons TermLis un contexte d’information logique construit à partir de ressources terminologiques disponibles en xml (FranceTerme), pour une utilisation flexible avec un logiciel de contexte logique (CAMELIS). Une vue en contexte logique permet d’explorer des informations de manière flexible, sans rédaction de requête a priori, et d’obtenir aussi des indications sur la qualité des données. Un tel contexte peut être enrichi par d’autres informations (de natures diverses), mais aussi en le reliant à d’autres applications (par des actions associées selon des arguments fournis par le contexte). Nous montrons comment utiliser TermLis et nous illustrons, à travers cette réalisation concrète sur des données de FranceTerme, les avantages d’une telle approche pour des données terminologiques.</abstract>
      <url hash="7cae9ee3">2015.jeptalnrecital-demonstration.4</url>
      <bibkey>foret-2015-termlis</bibkey>
    </paper>
    <paper id="5">
      <title>Etude de l’image de marque d’entités dans le cadre d’une plateforme de veille sur le Web social</title>
      <author><first>Leila</first><last>Khouas</last></author>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>Anne</first><last>Peradotto</last></author>
      <author><first>Jean-Valère</first><last>Cossu</last></author>
      <author><first>Julien</first><last>Boyadjian</last></author>
      <author><first>Julien</first><last>Velcin</last></author>
      <pages>10–11</pages>
      <abstract>Ce travail concerne l’intégration à une plateforme de veille sur internet d’outils permettant l’analyse des opinions émises par les internautes à propos d’une entité, ainsi que la manière dont elles évoluent dans le temps. Les entités considérées peuvent être des personnes, des entreprises, des marques, etc. Les outils implémentés sont le produit d’une collaboration impliquant plusieurs partenaires industriels et académiques dans le cadre du projet ANR ImagiWeb.</abstract>
      <url hash="e1b6b03a">2015.jeptalnrecital-demonstration.5</url>
      <bibkey>khouas-etal-2015-etude</bibkey>
    </paper>
    <paper id="6">
      <title>Building a Bilingual <fixed-case>V</fixed-case>ietnamese-<fixed-case>F</fixed-case>rench Named Entity Annotated Corpus through Cross-Linguistic Projection</title>
      <author><first>Ngoc Tan</first><last>Le</last></author>
      <author><first>Fatiha</first><last>Sadat</last></author>
      <pages>12–13</pages>
      <abstract>The creation of high-quality named entity annotated resources is time-consuming and an expensive process. Most of the gold standard corpora are available for English but not for less-resourced languages such as Vietnamese. In Asian languages, this task is remained problematic. This paper focuses on an automatic construction of named entity annotated corpora for Vietnamese-French, a less-resourced pair of languages. We incrementally apply different cross-projection methods using parallel corpora, such as perfect string matching and edit distance similarity. Evaluations on Vietnamese –French pair of languages show a good accuracy (F-score of 94.90

%) when identifying named entities pairs and building a named entity annotated parallel corpus.</abstract>
      <url hash="2b328306">2015.jeptalnrecital-demonstration.6</url>
      <bibkey>le-sadat-2015-building</bibkey>
    </paper>
    <paper id="7">
      <title>Recherche de motifs de graphe en ligne</title>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <pages>14–15</pages>
      <abstract>Nous présentons un outil en ligne de recherche de graphes dans des corpus annotés en syntaxe.</abstract>
      <url hash="3b7379a4">2015.jeptalnrecital-demonstration.7</url>
      <bibkey>guillaume-2015-recherche</bibkey>
    </paper>
    <paper id="8">
      <title>Un patient virtuel dialogant</title>
      <author><first>Leonardo</first><last>Campillos</last></author>
      <author><first>Dhouha</first><last>Bouamor</last></author>
      <author><first>Éric</first><last>Bilinski</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <pages>16–17</pages>
      <abstract>Le démonstrateur que nous décrivons ici est un prototype de système de dialogue dont l’objectif est de simuler un patient. Nous décrivons son fonctionnement général en insistant sur les aspects concernant la langue et surtout le rapport entre langue médicale de spécialité et langue générale.</abstract>
      <url hash="e4017619">2015.jeptalnrecital-demonstration.8</url>
      <bibkey>campillos-etal-2015-un</bibkey>
    </paper>
    <paper id="9">
      <title>Intégration du corpus des actes de <fixed-case>TALN</fixed-case> à la plateforme <fixed-case>S</fixed-case>cien<fixed-case>Q</fixed-case>uest</title>
      <author><first>Achille</first><last>Falaise</last></author>
      <pages>18–19</pages>
      <abstract>Cette démonstration présente l’intégration du corpus arboré des Actes de TALN à la plateforme ScienQuest. Cette plateforme fut initialement créée pour l’étude du corpus de textes scientifiques Scientext. Cette intégration tient compte des métadonnées propres au corpus TALN, et a été effectuée en s’efforçant de rapprocher les jeux d’étiquettes de ces deux corpus, et en convertissant pour le corpus TALN les requêtes prédéfinies conçues pour le corpus Scientext, de manière à permettre d’effectuer facilement des recherches similaires sur les deux corpus.</abstract>
      <url hash="e6b1b50b">2015.jeptalnrecital-demonstration.9</url>
      <bibkey>falaise-2015-integration</bibkey>
    </paper>
    <paper id="10">
      <title>Une aide à la communication par pictogrammes avec prédiction sémantique</title>
      <author><first>Aurélie</first><last>Merlo</last></author>
      <pages>20–22</pages>
      <abstract>Cette démonstration présente une application mobile (pour tablette et smartphone) pour des personnes souffrant de troubles du langage et/ou de la parole permettant de générer des phrases à partir de la combinaison de pictogrammes puis de verbaliser le texte généré en Text-To-Speech (TTS). La principale critique adressée par les patients utilisant les solutions existantes est le temps de composition trop long d’une phrase. Cette limite ne permet pas ou très difficilement d’utiliser les solutions actuelles en condition dialogique. Pour pallier cela, nous avons développé un moteur de génération de texte avec prédiction sémantique ne proposant à l’utilisateur que les pictogrammes pertinents au regard de la saisie en cours (e.g. après le pictogramme [manger], l’application propose les pictogrammes [pomme] ou encore [viande] correspondant à des concepts comestibles). Nous avons ainsi multiplié de 5 à 10 la vitesse de composition d’une phrase par rapport aux solutions existantes.</abstract>
      <url hash="5f30cb69">2015.jeptalnrecital-demonstration.10</url>
      <bibkey>merlo-2015-une</bibkey>
    </paper>
    <paper id="11">
      <title>Un système expert fondé sur une analyse sémantique pour l’identification de menaces d’ordre biologique</title>
      <author><first>Cédric</first><last>Lopez</last></author>
      <author><first>Aleksandra</first><last>Ponomareva</last></author>
      <author><first>Cécile</first><last>Robin</last></author>
      <author><first>André</first><last>Bittar</last></author>
      <author><first>Xabier</first><last>Larrucea</last></author>
      <author><first>Frédérique</first><last>Segond</last></author>
      <author><first>Marie-Hélène</first><last>Metzger</last></author>
      <pages>23–24</pages>
      <abstract>Le projet européen TIER (Integrated strategy for CBRN – Chemical, Biological, Radiological and Nuclear – Threat Identification and Emergency Response) vise à intégrer une stratégie complète et intégrée pour la réponse d’urgence dans un contexte de dangers biologiques, chimiques, radiologiques, nucléaires, ou liés aux explosifs, basée sur l’identification des menaces et d’évaluation des risques. Dans cet article, nous nous focalisons sur les risques biologiques. Nous présentons notre système expert fondé sur une analyse sémantique, permettant l’extraction de données structurées à partir de données non structurées dans le but de raisonner.</abstract>
      <url hash="629b4f63">2015.jeptalnrecital-demonstration.11</url>
      <bibkey>lopez-etal-2015-un</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>D</fixed-case>is<fixed-case>M</fixed-case>o : un annotateur multi-niveaux pour les corpus oraux</title>
      <author><first>George</first><last>Christodoulides</last></author>
      <author><first>Giulia</first><last>Barreca</last></author>
      <author><first>Mathieu</first><last>Avanzi</last></author>
      <pages>25–27</pages>
      <abstract>Dans cette démonstration, nous présentons l’annotateur multi-niveaux DisMo, un outil conçu pour faire face aux spécificités des corpus oraux. Il fournit une annotation morphosyntaxique, une lemmatisation, une détection des unités poly-lexicales, une détection des phénomènes de disfluence et des marqueurs de discours.</abstract>
      <url hash="cc7ee690">2015.jeptalnrecital-demonstration.12</url>
      <bibkey>christodoulides-etal-2015-dismo</bibkey>
    </paper>
  </volume>
  <volume id="invite">
    <meta>
      <booktitle>Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles. Conférences invitées</booktitle>
      <editor><first>Jean-Marc</first><last>Lecarpentier</last></editor>
      <editor><first>Nadine</first><last>Lucas</last></editor>
      <publisher>ATALA</publisher>
      <address>Caen, France</address>
      <month>June</month>
      <year>2015</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="4d3d279f">2015.jeptalnrecital-invite.0</url>
      <bibkey>jep-taln-recital-2015-actes-de-la-22e</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Multilinguality at Your Fingertips : <fixed-case>B</fixed-case>abel<fixed-case>N</fixed-case>et, Babelfy and Beyond !</title>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>1–1</pages>
      <abstract>Multilinguality is a key feature of today’s Web, and it is this feature that we leverage and exploit in our research work at the Sapienza University of Rome’s Linguistic Computing Laboratory, which I am going to overview and showcase in this talk. I will start by presenting BabelNet 3.0, available at http://babelnet.org, a very large multilingual encyclopedic dictionary and semantic network, which covers 271 languages and provides both lexicographic and encyclopedic knowledge for all the open-class parts of speech, thanks to the seamless integration of WordNet, Wikipedia, Wiktionary, OmegaWiki, Wikidata and the Open Multilingual WordNet. Next, I will present Babelfy, available at http://babelfy.org, a unified approach that leverages BabelNet to jointly perform word sense disambiguation and entity linking in arbitrary languages, with performance on both tasks on a par with, or surpassing, those of task-specific state-of-the-art supervised systems. Finally I will describe the Wikipedia Bitaxonomy, available at http://wibitaxonomy.org, a new approach to the construction of a Wikipedia bitaxonomy, that is, the largest and most accurate currently available taxonomy of Wikipedia pages and taxonomy of categories, aligned to each other. I will also give an outline of future work on multilingual resources and processing, including state-of-the-art semantic similarity with sense embeddings.</abstract>
      <url hash="a5c97766">2015.jeptalnrecital-invite.1</url>
      <bibkey>navigli-2015-multilinguality</bibkey>
    </paper>
    <paper id="2">
      <title>Pourquoi construire des ressources terminologiques et pourquoi le faire différemment ?</title>
      <author><first>Marie-Claude</first><last>L’Homme</last></author>
      <pages>2–2</pages>
      <abstract>Dans cette présentation, je défendrai l’idée selon laquelle des ressources terminologiques décrivant les propriétés lexico-sémantiques des termes constituent un complément nécessaire, voire indispensable, à d’autres types de ressources, À partir d’exemples anglais et français empruntés au domaine de l’environnement, je montrerai, d’une part, que les ressources lexicales générales (y compris celles qui ont une large couverture) n’offrent pas un portait complet du sens des termes ou de la structure lexicale observée du point de vue d’un domaine de spécialité. Je montrerai, d’autre part, que les ressources terminologiques (thésaurus, ontologies, banques de terminologie) souvent d’obédience conceptuelle, se concentrent sur le lien entre les termes et les connaissances dénotées par eux et s’attardent peu sur leur fonctionnement linguistique. Je présenterai un type de ressource décrivant les propriétés lexico-sémantiques des termes d’un domaine (structure actantielle, liens lexicaux, annotations contextuelles, etc.) et des éléments méthodologiques présidant à son élaboration.</abstract>
      <url hash="c50bf130">2015.jeptalnrecital-invite.2</url>
      <bibkey>lhomme-2015-pourquoi</bibkey>
    </paper>
  </volume>
  <volume id="recital">
    <meta>
      <booktitle>Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues</booktitle>
      <editor><first>Jean-Marc</first><last>Lecarpentier</last></editor>
      <editor><first>Nadine</first><last>Lucas</last></editor>
      <publisher>ATALA</publisher>
      <address>Caen, France</address>
      <month>June</month>
      <year>2015</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="fa488400">2015.jeptalnrecital-recital.0</url>
      <bibkey>jep-taln-recital-2015-actes-de-la-22e-sur</bibkey>
    </frontmatter>
    <paper id="1">
      <title>fr2sql : Interrogation de bases de données en français</title>
      <author><first>Benoît</first><last>Couderc</last></author>
      <author><first>Jérémy</first><last>Ferrero</last></author>
      <pages>1–12</pages>
      <abstract>Les bases de données sont de plus en plus courantes et prennent de plus en plus d’ampleur au sein des applications et sites Web actuels. Elles sont souvent amenées à être utilisées par des personnes n’ayant pas une grande compétence en la matière et ne connaissant pas rigoureusement leur structure. C’est pour cette raison que des traducteurs du langage naturel aux requêtes SQL sont développés. Malheureusement, la plupart de ces traducteurs se cantonnent à une seule base du fait de la spécificité de l’architecture de celle-ci. Dans cet article, nous proposons une méthode visant à pouvoir interroger n’importe quelle base de données à partir de questions en français. Nous évaluons notre application sur deux bases à la structure différente et nous montrons également qu’elle supporte plus d’opérations que la plupart des autres traducteurs.</abstract>
      <url hash="2b0d1a40">2015.jeptalnrecital-recital.1</url>
      <bibkey>couderc-ferrero-2015-fr2sql</bibkey>
    </paper>
    <paper id="2">
      <title>Désambiguïsation lexicale à base de connaissances par sélection distributionnelle et traits sémantiques</title>
      <author><first>Mokhtar Boumedyen</first><last>Billami</last></author>
      <pages>13–24</pages>
      <abstract>La désambiguïsation lexicale permet d’améliorer de nombreuses applications en traitement automatique des langues (TAL) comme la recherche d’information, l’extraction d’information, la traduction automatique, ou la simplification lexicale de textes. Schématiquement, il s’agit de choisir quel est le sens le plus approprié pour chaque mot d’un texte. Une des approches classiques consiste à estimer la similarité sémantique qui existe entre les sens de deux mots puis de l’étendre à l’ensemble des mots du texte. La méthode la plus directe donne un score de similarité à toutes les paires de sens de mots puis choisit la chaîne de sens qui retourne le meilleur score (on imagine la complexité exponentielle liée à cette approche exhaustive). Dans cet article, nous proposons d’utiliser une méta-heuristique d’optimisation combinatoire qui consiste à choisir les voisins les plus proches par sélection distributionnelle autour du mot à désambiguïser. Le test et l’évaluation de notre méthode portent sur un corpus écrit en langue française en se servant du réseau sémantique BabelNet. Le taux d’exactitude obtenu est de 78

% sur l’ensemble des noms et des verbes choisis pour l’évaluation.</abstract>
      <url hash="8a011a99">2015.jeptalnrecital-recital.2</url>
      <bibkey>billami-2015-desambiguisation</bibkey>
    </paper>
    <paper id="3">
      <title>Vers un modèle de détection des affects, appréciations et jugements dans le cadre d’interactions humain-agent</title>
      <author><first>Caroline</first><last>Langlet</last></author>
      <pages>25–37</pages>
      <abstract>Cet article aborde la question de la détection des expressions d’attitude – i.e affect, d’appréciation et de jugement (Martin

&amp; White, 2005) – dans le contenu verbal de l’utilisateur au cours d’interactions en face-à-face avec un agent conversationnel animé. Il propose un positionnement en termes de modèles et de méthodes pour le développement d’un système de détection adapté aux buts communicationnels de l’agent et à une parole conversationnelle. Après une description du modèle théorique de référence choisi, l’article propose un modèle d’annotation des attitudes dédié l’exploration de ce phénomène dans un corpus d’interaction humain-agent. Il présente ensuite une première version de notre système. Cette première version se concentre sur la détection des expressions d’attitudes pouvant référer à ce qu’aime ou n’aime pas l’utilisateur. Le système est conçu selon une approche symbolique fondée sur un ensemble de règles sémantiques et de représentations logico-sémantiques des énoncés.</abstract>
      <url hash="f4d06f33">2015.jeptalnrecital-recital.3</url>
      <bibkey>langlet-2015-vers</bibkey>
    </paper>
    <paper id="4">
      <title>Résumé Automatique Multi-Document Dynamique : État de l’Art</title>
      <author><first>Maâli</first><last>Mnasri</last></author>
      <pages>38–49</pages>
      <abstract>Les travaux menés dans le cadre du résumé automatique de texte ont montré des résultats à la fois très encourageants mais qui sont toujours à améliorer. La problématique du résumé automatique ne cesse d’évoluer avec les nouveaux champs d’application qui s’imposent, ce qui augmente les contraintes liées à cette tâche. Nous nous inté- ressons au résumé extractif multi-document dynamique. Pour cela, nous examinons les différentes approches existantes en mettant l’accent sur les travaux les plus récents. Nous montrons ensuite que la performance des systèmes de résumé multi-document et dynamique est encore modeste. Trois contraintes supplémentaires sont ajoutées : la redondance inter-document, la redondance à travers le temps et la grande taille des données à traiter. Nous essayons de déceler les insuffisances des systèmes existants afin de bien définir notre problématique et guider ainsi nos prochains travaux.</abstract>
      <url hash="be8afbf5">2015.jeptalnrecital-recital.4</url>
      <bibkey>mnasri-2015-resume</bibkey>
    </paper>
    <paper id="5">
      <title>Alignement multimodal de ressources éducatives et scientifiques</title>
      <author><first>Hugo</first><last>Mougard</last></author>
      <pages>50–59</pages>
      <abstract>Cet article présente certaines questions de recherche liées au projet COC O 1 . L’ambition de ce projet est de valoriser les ressources éducatives et académiques en exploitant au mieux les différents médias disponibles (vidéos de cours ou de présentations d’articles, manuels éducatifs, articles scientifiques, présentations, etc). Dans un premier temps, nous décrirons le problème d’utilisation jointe de ressources multimédias éducatives ou scientifiques pour ensuite introduire l’état de l’art dans les domaines concernés. Cela nous permettra de présenter quelques questions de recherche sur lesquelles porteront des études ultérieures. Enfin nous finirons en introduisant trois prototypes développés pour analyser ces questions.</abstract>
      <url hash="18721e1a">2015.jeptalnrecital-recital.5</url>
      <bibkey>mougard-2015-alignement</bibkey>
    </paper>
    <paper id="6">
      <title>État de l’art : analyse des conversations écrites en ligne porteuses de demandes d’assistance en termes d’actes de dialogue</title>
      <author><first>Soufian</first><last>Salim</last></author>
      <pages>60–71</pages>
      <abstract>Le développement du Web 2.0 et le processus de création et de consommation massive de contenus générés par les utilisateurs qu’elle a enclenché a permis le développement de nouveaux types d’interactions chez les internautes. En particulier, nous nous intéressons au développement du support en ligne et des plate-formes d’entraide. En effet, les archives de conversations en ligne porteuses de demandes d’assistance représentent une ressource inestimable, mais peu exploitée. L’exploitation de cette ressource permettrait non seulement d’améliorer les systèmes liés à la résolution collaborative des problèmes, mais également de perfectionner les canaux de support proposés par les entreprises opérant sur le web. Pour ce faire, il est cependant nécessaire de définir un cadre formel pour l’analyse discursive de ce type de conversations. Cet article a pour objectif de présenter l’état de la recherche en analyse des conversations écrites en ligne, sous différents médiums, et de montrer dans quelle mesure les différentes méthodes exposées dans la littérature peuvent être appliquées à des conversations fonctionnelles inscrites dans le cadre de la résolution collaborative des problèmes utilisateurs.</abstract>
      <url hash="dce41c74">2015.jeptalnrecital-recital.6</url>
      <bibkey>salim-2015-etat</bibkey>
    </paper>
  </volume>
</collection>
