<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.nlp4dh">
  <volume id="1" ingest-date="2022-10-03">
    <meta>
      <booktitle>Proceedings of the Workshop on Natural Language Processing for Digital Humanities</booktitle>
      <editor><first>Mika</first><last>Hämäläinen</last></editor>
      <editor><first>Khalid</first><last>Alnajjar</last></editor>
      <editor><first>Niko</first><last>Partanen</last></editor>
      <editor><first>Jack</first><last>Rueter</last></editor>
      <publisher>NLP Association of India (NLPAI)</publisher>
      <address>NIT Silchar, India</address>
      <month>December</month>
      <year>2021</year>
      <url hash="b65afb6c">2021.nlp4dh-1</url>
      <venue>nlp4dh</venue>
    </meta>
    <frontmatter>
      <url hash="7548d903">2021.nlp4dh-1.0</url>
      <bibkey>nlp4dh-2021-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Sentiment Dynamics of Success: Fractal Scaling of Story Arcs Predicts Reader Preferences</title>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <author><first>Telma</first><last>Peura</last></author>
      <author><first>Mads Rosendahl</first><last>Thomsen</last></author>
      <author><first>Kristoffer</first><last>Nielbo</last></author>
      <pages>1–6</pages>
      <abstract>e explore the correlation between the sentiment arcs of H. C. Andersen’s fairy tales and their popularity, measured as their average score on the platform GoodReads. Specifically, we do not conceive a story’s overall sentimental trend as predictive <i>per se</i>, but we focus on its coherence and predictability over time as represented by the arc’s Hurst exponent. We find that degrading Hurst values tend to imply degrading quality scores, while a Hurst exponent between .55 and .65 might indicate a “sweet spot” for literary appreciation.</abstract>
      <url hash="b8580de7">2021.nlp4dh-1.1</url>
      <bibkey>bizzoni-etal-2021-sentiment</bibkey>
    </paper>
    <paper id="2">
      <title>The Validity of Lexicon-based Sentiment Analysis in Interdisciplinary Research</title>
      <author><first>Emily</first><last>Öhman</last></author>
      <pages>7–12</pages>
      <abstract>Lexicon-based sentiment and emotion analysis methods are widely used particularly in applied Natural Language Processing (NLP) projects in fields such as computational social science and digital humanities. These lexicon-based methods have often been criticized for their lack of validation and accuracy – sometimes fairly. However, in this paper, we argue that lexicon-based methods work well particularly when moving up in granularity and show how useful lexicon-based methods can be for projects where neither qualitative analysis nor a machine learning-based approach is possible. Indeed, we argue that the measure of a lexicon’s accuracy should be grounded in its usefulness.</abstract>
      <url hash="879fae39">2021.nlp4dh-1.2</url>
      <bibkey>ohman-2021-validity</bibkey>
    </paper>
    <paper id="3">
      <title>How Does the Hate Speech Corpus Concern Sociolinguistic Discussions? A Case Study on <fixed-case>K</fixed-case>orean Online News Comments</title>
      <author><first>Won Ik</first><last>Cho</last></author>
      <author><first>Jihyung</first><last>Moon</last></author>
      <pages>13–22</pages>
      <abstract>Social consensus has been established on the severity of online hate speech since it not only causes mental harm to the target, but also gives displeasure to the people who read it. For Korean, the definition and scope of hate speech have been discussed widely in researches, but such considerations were hardly extended to the construction of hate speech corpus. Therefore, we create a Korean online hate speech dataset with concrete annotation guideline to see how real world toxic expressions concern sociolinguistic discussions. This inductive observation reveals that hate speech in online news comments is mainly composed of social bias and toxicity. Furthermore, we check how the final corpus corresponds with the definition and scope of hate speech, and confirm that the overall procedure and outcome is in concurrence with the sociolinguistic discussions.</abstract>
      <url hash="2a8c11ad">2021.nlp4dh-1.3</url>
      <bibkey>cho-moon-2021-hate</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>M</fixed-case>ac<fixed-case>BERT</fixed-case>h: Development and Evaluation of a Historically Pre-trained Language Model for <fixed-case>E</fixed-case>nglish (1450-1950)</title>
      <author><first>Enrique</first><last>Manjavacas Arevalo</last></author>
      <author><first>Lauren</first><last>Fonteyn</last></author>
      <pages>23–36</pages>
      <abstract>The new pre-train-then-fine-tune paradigm in Natural made important performance gains accessible to a wider audience. Once pre-trained, deploying a large language model presents comparatively small infrastructure requirements, and offers robust performance in many NLP tasks. The Digital Humanities community has been an early adapter of this paradigm. Yet, a large part of this community is concerned with the application of NLP algorithms to historical texts, for which large models pre-trained on contemporary text may not provide optimal results. In the present paper, we present “MacBERTh”—a transformer-based language model pre-trained on historical English—and exhaustively assess its benefits on a large set of relevant downstream tasks. Our experiments highlight that, despite some differences across target time periods, pre-training on historical language from scratch outperforms models pre-trained on present-day language and later adapted to historical language.</abstract>
      <url hash="2d1ab924">2021.nlp4dh-1.4</url>
      <bibkey>manjavacas-arevalo-fonteyn-2021-macberth</bibkey>
    </paper>
    <paper id="5">
      <title>Named Entity Recognition for <fixed-case>F</fixed-case>rench medieval charters</title>
      <author><first>Sergio</first><last>Torres Aguilar</last></author>
      <author><first>Dominique</first><last>Stutzmann</last></author>
      <pages>37–46</pages>
      <abstract>This paper presents the process of annotating and modelling a corpus to automatically detect named entities in medieval charters in French. It introduces a new annotated corpus and a new system which outperforms state-of-the art libraries. Charters are legal documents and among the most important historical sources for medieval studies as they reflect economic and social dynamics as well as the evolution of literacy and writing practices. Automatic detection of named entities greatly improves the access to these unstructured texts and facilitates historical research. The experiments described here are based on a corpus encompassing about 500k words (1200 charters) coming from three charter collections of the 13th and 14th centuries. We annotated the corpus and then trained two state-of-the art NLP libraries for Named Entity Recognition (Spacy and Flair) and a custom neural model (Bi-LSTM-CRF). The evaluation shows that all three models achieve a high performance rate on the test set and a high generalization capacity against two external corpora unseen during training. This paper describes the corpus and the annotation model, and discusses the issues related to the linguistic processing of medieval French and formulaic discourse, so as to interpret the results within a larger historical perspective.</abstract>
      <url hash="c33b4845">2021.nlp4dh-1.5</url>
      <bibkey>torres-aguilar-stutzmann-2021-named</bibkey>
    </paper>
    <paper id="6">
      <title>Processing <fixed-case>M</fixed-case>.<fixed-case>A</fixed-case>. Castrén’s Materials: Multilingual Historical Typed and Handwritten Manuscripts</title>
      <author><first>Niko</first><last>Partanen</last></author>
      <author><first>Jack</first><last>Rueter</last></author>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <pages>47–54</pages>
      <abstract>The study forms a technical report of various tasks that have been performed on the materials collected and published by Finnish ethnographer and linguist, Matthias Alexander Castrén (1813–1852). The Finno-Ugrian Society is publishing Castrén’s manuscripts as new critical and digital editions, and at the same time different research groups have also paid attention to these materials. We discuss the workflows and technical infrastructure used, and consider how datasets that benefit different computational tasks could be created to further improve the usability of these materials, and also to aid the further processing of similar archived collections. We specifically focus on the parts of the collections that are processed in a way that improves their usability in more technical applications, complementing the earlier work on the cultural and linguistic aspects of these materials. Most of these datasets are openly available in Zenodo. The study points to specific areas where further research is needed, and provides benchmarks for text recognition tasks.</abstract>
      <url hash="662d5e03">2021.nlp4dh-1.6</url>
      <bibkey>partanen-etal-2021-processing</bibkey>
    </paper>
    <paper id="7">
      <title>Lotte and Annette: A Framework for Finding and Exploring Key Passages in Literary Works</title>
      <author><first>Frederik</first><last>Arnold</last></author>
      <author><first>Robert</first><last>Jäschke</last></author>
      <pages>55–63</pages>
      <abstract>We present an approach that leverages expert knowledge contained in scholarly works to automatically identify key passages in literary works. Specifically, we extend a text reuse detection method for finding quotations, such that our system Lotte can deal with common properties of quotations, for example, ellipses or inaccurate quotations. An evaluation shows that Lotte outperforms four existing approaches. To generate key passages, we combine overlapping quotations from multiple scholarly texts. An interactive website, called Annette, for visualizing and exploring key passages makes the results accessible and explorable.</abstract>
      <url hash="2913ceb4">2021.nlp4dh-1.7</url>
      <bibkey>arnold-jaschke-2021-lotte</bibkey>
    </paper>
    <paper id="8">
      <title>Using Referring Expression Generation to Model Literary Style</title>
      <author><first>Nick</first><last>Montfort</last></author>
      <author><first>Ardalan</first><last>SadeghiKivi</last></author>
      <author><first>Joanne</first><last>Yuan</last></author>
      <author><first>Alan Y.</first><last>Zhu</last></author>
      <pages>64–74</pages>
      <abstract>Novels and short stories are not just remarkable because of what events they represent. The narrative style they employ is significant. To understand the specific contributions of different aspects of this style, it is possible to create limited symbolic models of narrating that hold almost all of the narrative discourse constant while varying a single aspect. In this paper we use a new implementation of a system for narrative discourse generation, Curveship, to change how existents at the story level are named. This by itself allows for the telling of the same underlying story in ways that evoke, for instance, a fabular or parable-like mode, the style of narrator Patrick Bateman in Brett Easton Ellis’s American Psycho, and the unusual dialect of Anthony Burgess’s A Clockwork Orange.</abstract>
      <url hash="7ef4c50c">2021.nlp4dh-1.8</url>
      <attachment type="OptionalSupplementaryMaterial" hash="f3e655ed">2021.nlp4dh-1.8.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>montfort-etal-2021-using</bibkey>
    </paper>
    <paper id="9">
      <title>The concept of nation in nineteenth-century <fixed-case>G</fixed-case>reek fiction through computational literary analysis</title>
      <author><first>Fotini</first><last>Koidaki</last></author>
      <author><first>Despina</first><last>Christou</last></author>
      <author><first>Katerina</first><last>Tiktopoulou</last></author>
      <author><first>Grigorios</first><last>Tsoumakas</last></author>
      <pages>75–84</pages>
      <abstract>How the construction of national consciousness may be captured in the literary production of a whole century? What can the macro-analysis of the 19th-century prose fiction reveal about the formation of the concept of the nation-state of Greece? How could the concept of nationality be detected in literary writing and then interpreted? These are the questions addressed by the research that is published in this paper and which focuses on exploring how the concept of the nation is figured and shaped in 19th-century Greek prose fiction. This paper proposes a methodological approach that combines well-known text mining techniques with computational close reading methods in order to retrieve the nation-related passages and to analyze them linguistically and semantically. The main objective of the paper at hand is to map the frequency and the phraseology of the nation-related references, as well as to explore the phrase patterns in relation to the topic modeling results.</abstract>
      <url hash="35db3c4c">2021.nlp4dh-1.9</url>
      <bibkey>koidaki-etal-2021-concept</bibkey>
    </paper>
    <paper id="10">
      <title>Logical Layout Analysis Applied to Historical Newspapers</title>
      <author><first>Nicolas</first><last>Gutehrlé</last></author>
      <author><first>Iana</first><last>Atanassova</last></author>
      <pages>85–94</pages>
      <abstract>In recent years, libraries and archives led important digitisation campaigns that opened the access to vast collections of historical documents. While such documents are often available as XML ALTO documents, they lack information about their logical structure. In this paper, we address the problem of logical layout analysis applied to historical documents. We propose a method which is based on the study of a dataset in order to identify rules that assign logical labels to both block and lines of text from XML ALTO documents. Our dataset contains newspapers in French, published in the first half of the 20th century. The evaluation shows that our methodology performs well for the identification of first lines of paragraphs and text lines, with F1 above 0.9. The identification of titles obtains an F1 of 0.64. This method can be applied to preprocess XML ALTO documents in preparation for downstream tasks, and also to annotate large-scale datasets to train machine learning and deep learning algorithms.</abstract>
      <url hash="70f4bc20">2021.nlp4dh-1.10</url>
      <bibkey>gutehrle-atanassova-2021-logical</bibkey>
    </paper>
    <paper id="11">
      <title>“Don’t worry, it’s just noise’”: quantifying the impact of files treated as single textual units when they are really collections</title>
      <author><first>Thibault</first><last>Clérice</last></author>
      <pages>95–105</pages>
      <abstract>Literature works may present many autonomous or semi-autonomous units, such as poems for the first or chapter for the second. We make the hypothesis that such cuts in the text’s flow, if not taken care of in the way we process text, have an impact on the application of the distributional hypothesis. We test this hypothesis with a large 20M tokens corpus of Latin works, by using text files as a single unit or multiple “autonomous” units for the analysis of selected words. For groups of rare words and words specific to heavily segmented works, the results show that their semantic space is mostly different between both versions of the corpus. For the 1000 most frequent words of the corpus, variations are important as soon as the window for defining neighborhood is larger or equal to 10 words.</abstract>
      <url hash="ea75a2eb">2021.nlp4dh-1.11</url>
      <bibkey>clerice-2021-dont</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>NLP</fixed-case> in the <fixed-case>DH</fixed-case> pipeline: Transfer-learning to a Chronolect</title>
      <author><first>Aynat</first><last>Rubinstein</last></author>
      <author><first>Avi</first><last>Shmidman</last></author>
      <pages>106–110</pages>
      <abstract>A big unknown in Digital Humanities (DH) projects that seek to analyze previously untouched corpora is the question of how to adapt existing Natural Language Processing (NLP) resources to the specific nature of the target corpus. In this paper, we study the case of Emergent Modern Hebrew (EMH), an under-resourced chronolect of the Hebrew language. The resource we seek to adapt, a diacritizer, exists for both earlier and later chronolects of the language. Given a small annotated corpus of our target chronolect, we demonstrate that applying transfer-learning from either of the chronolects is preferable to training a new model from scratch. Furthermore, we consider just how much annotated data is necessary. For our task, we find that even a minimal corpus of 50K tokens provides a noticeable gain in accuracy. At the same time, we also evaluate accuracy at three additional increments, in order to quantify the gains that can be expected by investing in a larger annotated corpus.</abstract>
      <url hash="85a64492">2021.nlp4dh-1.12</url>
      <bibkey>rubinstein-shmidman-2021-nlp</bibkey>
    </paper>
    <paper id="13">
      <title>Using Computational Grounded Theory to Understand Tutors’ Experiences in the Gig Economy</title>
      <author><first>Lama</first><last>Alqazlan</last></author>
      <author><first>Rob</first><last>Procter</last></author>
      <author><first>Michael</first><last>Castelle</last></author>
      <pages>111–120</pages>
      <abstract>The introduction of online marketplace platforms has led to the advent of new forms of flexible, on-demand (or ‘gig’) work. Yet, most prior research concerning the experience of gig workers examines delivery or crowdsourcing platforms, while the experience of the large numbers of workers who undertake educational labour in the form of tutoring gigs remains understudied. To address this, we use a computational grounded theory approach to analyse tutors’ discussions on Reddit. This approach consists of three phases including data exploration, modelling and human-centred interpretation. We use both validation and human evaluation to increase the trustworthiness and reliability of the computational methods. This paper is a work in progress and reports on the first of the three phases of this approach.</abstract>
      <url hash="3313044b">2021.nlp4dh-1.13</url>
      <bibkey>alqazlan-etal-2021-using</bibkey>
    </paper>
    <paper id="14">
      <title>Can Domain Pre-training Help Interdisciplinary Researchers from Data Annotation Poverty? A Case Study of Legal Argument Mining with <fixed-case>BERT</fixed-case>-based Transformers</title>
      <author><first>Gechuan</first><last>Zhang</last></author>
      <author><first>David</first><last>Lillis</last></author>
      <author><first>Paul</first><last>Nulty</last></author>
      <pages>121–130</pages>
      <abstract>Interdisciplinary Natural Language Processing (NLP) research traditionally suffers from the requirement for costly data annotation. However, transformer frameworks with pre-training have shown their ability on many downstream tasks including digital humanities tasks with limited small datasets. Considering the fact that many digital humanities fields (e.g. law) feature an abundance of non-annotated textual resources, and the recent achievements led by transformer models, we pay special attention to whether domain pre-training will enhance transformer’s performance on interdisciplinary tasks and how. In this work, we use legal argument mining as our case study. This aims to automatically identify text segments with particular linguistic structures (i.e., arguments) from legal documents and to predict the reasoning relations between marked arguments. Our work includes a broad survey of a wide range of BERT variants with different pre-training strategies. Our case study focuses on: the comparison of general pre-training and domain pre-training; the generalisability of different domain pre-trained transformers; and the potential of merging general pre-training with domain pre-training. We also achieve better results than the current transformer baseline in legal argument mining.</abstract>
      <url hash="0f40f1b2">2021.nlp4dh-1.14</url>
      <bibkey>zhang-etal-2021-domain</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/echr">ECHR</pwcdataset>
    </paper>
    <paper id="15">
      <title><fixed-case>J</fixed-case>apanese Beauty Marketing on Social Media: Critical Discourse Analysis Meets <fixed-case>NLP</fixed-case></title>
      <author><first>Emily</first><last>Öhman</last></author>
      <author><first>Amy Gracy</first><last>Metcalfe</last></author>
      <pages>131–137</pages>
      <abstract>This project is a pilot study intending to combine traditional corpus linguistics, Natural Language Processing, critical discourse analysis, and digital humanities to gain an up-to-date understanding of how beauty is being marketed on social media, specifically Instagram, to followers. We use topic modeling combined with critical discourse analysis and NLP tools for insights into the “Japanese Beauty Myth” and show an overview of the dataset that we make publicly available.</abstract>
      <url hash="5d90b664">2021.nlp4dh-1.15</url>
      <bibkey>ohman-metcalfe-2021-japanese</bibkey>
      <pwccode url="https://github.com/esohman/JapaneseBeauty" additional="false">esohman/JapaneseBeauty</pwccode>
    </paper>
    <paper id="16">
      <title>Text Zoning of Theater Reviews: How Different are Journalistic from Blogger Reviews?</title>
      <author><first>Mylene</first><last>Maignant</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <author><first>Gaëtan</first><last>Brison</last></author>
      <pages>138–143</pages>
      <abstract>This paper aims at modeling the structure of theater reviews based on contemporary London performances by using text zoning. Text zoning consists in tagging sentences so as to reveal text structure. More than 40 000 theater reviews going from 2010 to 2020 were collected to analyze two different types of reception (journalistic vs digital). We present our annotation scheme and the classifiers used to perform the text zoning task, aiming at tagging reviews at the sentence level. We obtain the best results using the random forest algorithm, and show that this approach makes it possible to give a first insight of the similarities and differences between our two subcorpora.</abstract>
      <url hash="0ea45b47">2021.nlp4dh-1.16</url>
      <bibkey>maignant-etal-2021-text</bibkey>
    </paper>
    <paper id="17">
      <title>Word Sense Induction with Attentive Context Clustering</title>
      <author><first>Moshe</first><last>Stekel</last></author>
      <author><first>Amos</first><last>Azaria</last></author>
      <author><first>Shai</first><last>Gordin</last></author>
      <pages>144–151</pages>
      <abstract>In this paper, we present ACCWSI (Attentive Context Clustering WSI), a method for Word Sense Induction, suitable for languages with limited resources. Pretrained on a small corpus and given an ambiguous word (query word) and a set of excerpts that contain it, ACCWSI uses an attention mechanism for generating context-aware embeddings, distinguishing between the different senses assigned to the query word. These embeddings are then clustered to provide groups of main common uses of the query word. This method demonstrates practical applicability for shedding light on the meanings of ambiguous words in ancient languages, such as Classical Hebrew.</abstract>
      <url hash="698ad641">2021.nlp4dh-1.17</url>
      <bibkey>stekel-etal-2021-word</bibkey>
    </paper>
    <paper id="18">
      <title>Transferring Modern Named Entity Recognition to the Historical Domain: How to Take the Step?</title>
      <author><first>Baptiste</first><last>Blouin</last></author>
      <author><first>Benoit</first><last>Favre</last></author>
      <author><first>Jeremy</first><last>Auguste</last></author>
      <author><first>Christian</first><last>Henriot</last></author>
      <pages>152–162</pages>
      <abstract>Named entity recognition is of high interest to digital humanities, in particular when mining historical documents. Although the task is mature in the field of NLP, results of contemporary models are not satisfactory on challenging documents corresponding to out-of-domain genres, noisy OCR output, or old-variants of the target language. In this paper we study how model transfer methods, in the context of the aforementioned challenges, can improve historical named entity recognition according to how much effort is allocated to describing the target data, manually annotating small amounts of texts, or matching pre-training resources. In particular, we explore the situation where the class labels, as well as the quality of the documents to be processed, are different in the source and target domains. We perform extensive experiments with the transformer architecture on the LitBank and HIPE historical datasets, with different annotation schemes and character-level noise. They show that annotating 250 sentences can recover 93% of the full-data performance when models are pre-trained, that the choice of self-supervised and target-task pre-training data is crucial in the zero-shot setting, and that OCR errors can be handled by simulating noise on pre-training data and resorting to recent character-aware transformers.</abstract>
      <url hash="11b63c8d">2021.nlp4dh-1.18</url>
      <bibkey>blouin-etal-2021-transferring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/litbank">LitBank</pwcdataset>
    </paper>
    <paper id="19">
      <title><fixed-case>TFW</fixed-case>2<fixed-case>V</fixed-case>: An Enhanced Document Similarity Method for the Morphologically Rich <fixed-case>F</fixed-case>innish Language</title>
      <author><first>Quan</first><last>Duong</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <pages>163–172</pages>
      <abstract>Measuring the semantic similarity of different texts has many important applications in Digital Humanities research such as information retrieval, document clustering and text summarization. The performance of different methods depends on the length of the text, the domain and the language. This study focuses on experimenting with some of the current approaches to Finnish, which is a morphologically rich language. At the same time, we propose a simple method, TFW2V, which shows high efficiency in handling both long text documents and limited amounts of data. Furthermore, we design an objective evaluation method which can be used as a framework for benchmarking text similarity approaches.</abstract>
      <url hash="f43cc4ce">2021.nlp4dh-1.19</url>
      <bibkey>duong-etal-2021-tfw2v</bibkey>
      <pwccode url="https://github.com/ruathudo/tfw2v" additional="false">ruathudo/tfw2v</pwccode>
    </paper>
    <paper id="20">
      <title>Did You Enjoy the Last Supper? An Experimental Study on Cross-Domain <fixed-case>NER</fixed-case> Models for the Art Domain</title>
      <author><first>Alejandro</first><last>Sierra-Múnera</last></author>
      <author><first>Ralf</first><last>Krestel</last></author>
      <pages>173–182</pages>
      <abstract>Named entity recognition (NER) is an important task that constitutes the basis for multiple downstream natural language processing tasks. Traditional machine learning approaches for NER rely on annotated corpora. However, these are only largely available for standard domains, e.g., news articles. Domain-specific NER often lacks annotated training data and therefore two options are of interest: expensive manual annotations or transfer learning. In this paper, we study a selection of cross-domain NER models and evaluate them for use in the art domain, particularly for recognizing artwork titles in digitized art-historic documents. For the evaluation of the models, we employ a variety of source domain datasets and analyze how each source domain dataset impacts the performance of the different models for our target domain. Additionally, we analyze the impact of the source domain’s entity types, looking for a better understanding of how the transfer learning models adapt different source entity types into our target entity types.</abstract>
      <url hash="9912feac">2021.nlp4dh-1.20</url>
      <bibkey>sierra-munera-krestel-2021-enjoy</bibkey>
      <pwccode url="https://github.com/hpi-information-systems/cross-domain-ner" additional="false">hpi-information-systems/cross-domain-ner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="21">
      <title>An Exploratory Study on Temporally Evolving Discussion around Covid-19 using Diachronic Word Embeddings</title>
      <author><first>Avinash</first><last>Tulasi</last></author>
      <author><first>Asanobu</first><last>Kitamoto</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <author><first>Arun Balaji</first><last>Buduru</last></author>
      <pages>183–190</pages>
      <abstract>Covid 19 has seen the world go into a lock down and unconventional social situations throughout. During this time, the world saw a surge in information sharing around the pandemic and the topics shared in the time were diverse. People’s sentiments have changed during this period. Given the wide spread usage of Online Social Networks (OSN) and support groups, the user sentiment is well reflected in online discussions. In this work, we aim to show the topics under discussion, evolution of discussions, change in user sentiment during the pandemic. Alongside which, we also demonstrate the possibility of exploratory analysis to find pressing topics, change in perception towards the topics and ways to use the knowledge extracted from online discussions. For our work we employ Diachronic Word embeddings which capture the change in word usage over time. With the help of analysis from temporal word usages, we show the change in people’s option on covid-19 from being a conspiracy, to the post-covid topics that surround vaccination.</abstract>
      <url hash="fe6f23b4">2021.nlp4dh-1.21</url>
      <bibkey>tulasi-etal-2021-exploratory</bibkey>
    </paper>
  </volume>
</collection>
