<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.rocling">
  <volume id="main" ingest-date="2026-01-04" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 37th Conference on Computational Linguistics and Speech Processing (ROCLING 2025)</booktitle>
      <editor><first>Kai-Wei</first><last>Chang</last></editor>
      <editor><first>Ke-Han</first><last>Lu</last></editor>
      <editor><first>Chih-Kai</first><last>Yang</last></editor>
      <editor><first>Zhi-Rui</first><last>Tam</last></editor>
      <editor><first>Wen-Yu</first><last>Chang</last></editor>
      <editor><first>Chung-Che</first><last>Wang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>National Taiwan University, Taipei City, Taiwan</address>
      <month>November</month>
      <year>2025</year>
      <url hash="48b709e0">2025.rocling-main</url>
      <venue>rocling</venue>
      <isbn>979-8-89176-379-1</isbn>
    </meta>
    <frontmatter>
      <url hash="c3125299">2025.rocling-main.0</url>
      <bibkey>rocling-2025-main</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Training a <fixed-case>C</fixed-case>hinese Listenability Model Using <fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec to Predict the Difficulty of Spoken Texts</title>
      <author><first>Yen-Hsiang</first><last>Chien</last></author>
      <author><first>Hou-Chiang</first><last>Tseng</last></author>
      <author><first>Kuan-Yu</first><last>Chen</last></author>
      <author><first>Yao-Ting</first><last>Sung</last></author>
      <pages>1-10</pages>
      <abstract>With the proliferation of digital learning, an increasing number of learners are engaging with audio-visual materials. For preschool and lower elementary students, whose literacy skills are still limited, knowledge acquisition relies more heavily on spoken and visual content. Traditional readability models were primarily developed for written texts, and their applicability to spoken materials remains uncertain. To address this issue, this study investigates the impact of different word segmentation tools and language models on the performance of automatic grade classification models for Chinese spoken materials. Support Vector Machines were employed for grade prediction, aiming to automatically determine the appropriate grade level of learning resources and assist learners in selecting suitable materials. The results show that language models with higher-dimensional word embeddings achieved better classification performance, with an accuracy of up to 61% and an adjacent accuracy of 76%. These findings may contribute to future digital learning platforms or educational resource recommendation systems by automatically providing students with appropriate listening materials to enhance learning outcomes.</abstract>
      <url hash="d0533bb0">2025.rocling-main.1</url>
      <bibkey>chien-etal-2025-training</bibkey>
    </paper>
    <paper id="2">
      <title>Cubicpower Agentic Mixture of Experts(<fixed-case>AM</fixed-case>o<fixed-case>E</fixed-case>) Framework for Fine-Tuning <fixed-case>NLP</fixed-case> Tasks Without <fixed-case>GPU</fixed-case>s</title>
      <author><first>Chao-Yih</first><last>Hsia</last></author>
      <pages>11-19</pages>
      <abstract>The rise of Green AI emphasizes minimizing the environmental footprint of AI systems. This paper explores a no-GPU agentic architecture for fine-tuning NLP tasks. It presents our initial experiments applying these no-GPU algorithms in pretraining and fine-tuning tasks on our CubicPower agentic mixture of experts (AMoE) framework, with the aim of contributing to more sustainable AI development. In contrast to the training procedures of neural networks, which consume significant power, the AMoE framework’s primary contribution toward power savings is that it requires no training process. We explore non-neural-network methods for solving NLP tasks and employ similarity measures to match predefined patterns for use in a RAG database.</abstract>
      <url hash="b8c0e971">2025.rocling-main.2</url>
      <bibkey>hsia-2025-cubicpower</bibkey>
    </paper>
    <paper id="3">
      <title>Design and Evaluation of a Courtroom Examination <fixed-case>AI</fixed-case> Simulation System with Behavioral Fidelity</title>
      <author><first>Hsien-Jyh</first><last>Liao</last></author>
      <pages>20-28</pages>
      <abstract>AI simulation system centered on Behavioral Fidelity, with speech interaction included as a design feature to enhance immersion. For standardization and reproducibility, the present pilot evaluation uses transcripts. The system integrates pragmatic–psychological rules with Taiwanese criminal case files to simulate witness behavior under cross-examination pressure. Using an optimized Expert Turing Test framework with four dimensions—professional accuracy, situational adaptability, human-likeness, and logical consistency—we conduct a pilot study. Under identical prompts and knowledge sources, the customized GPT condition received higher ratings than GPT-Vanilla on adaptability and human-likeness. Applying the same framework to another mainstream model (Gemini 2.5 Flash) yielded comparable performance, while differences remain inconclusive at this sample size. Overall, the results provide preliminary evidence that Behavioral Fidelity is a feasible evaluation target and indicate the scalability of generative AI for legal training; speech-condition evaluation and multi-case, multi-role extensions are left for future work.</abstract>
      <url hash="bd70a048">2025.rocling-main.3</url>
      <bibkey>liao-2025-design</bibkey>
    </paper>
    <paper id="4">
      <title>Multimodal Approaches for Stress Recognition: A Comparative Study Using the <fixed-case>S</fixed-case>tress<fixed-case>ID</fixed-case> Dataset</title>
      <author><first>Chia-Yun</first><last>Lee</last></author>
      <author><first>Matúš</first><last>Pleva</last></author>
      <author><first>Daniel</first><last>Hladek</last></author>
      <author><first>Ming-Hsiang</first><last>Su</last></author>
      <pages>29-34</pages>
      <abstract>Mental health concerns have garnered increasing attention, highlighting the importance of timely and accurate identification of individual stress states as a critical research domain. This study employs the multimodal StressID dataset to evaluate the contributions of three modalities—physiological signals, video, and audio—in stress recognition tasks. A set of machine learning models, including Random Forests (RF), Support Vector Machines (SVM), Multi-Layer Perceptrons (MLP), and K-Nearest Neighbors (KNN), were trained and tested with optimized parameters for each modality. In addition, the effectiveness of different multimodal fusion strategies was systematically examined. The unimodal experiments revealed that the physiological modality achieved the highest performance in the binary stress classification task (F1-score = 0.751), whereas the audio modality outperformed the others in the three-class classification task (F1-score = 0.625). In the multimodal setting, feature-level fusion yielded stable improvements in the binary classification task, while decision-level fusion achieved superior performance in the three-class classification task (F1-score = 0.65). These findings demonstrate that multimodal integration can substantially enhance the accuracy of stress recognition. Future research directions include incorporating temporal modeling and addressing data imbalance to further improve the robustness and applicability of stress recognition systems.</abstract>
      <url hash="fcc3a864">2025.rocling-main.4</url>
      <bibkey>lee-etal-2025-multimodal-approaches</bibkey>
    </paper>
    <paper id="5">
      <title>Beyond Binary: Enhancing Misinformation Detection with Nuance-Controlled Event Context</title>
      <author><first>Elijah Frederick</first><last>Albertson</last></author>
      <author><first>Retnani</first><last>Latifah</last></author>
      <author><first>Yi-Shin</first><last>Chen</last></author>
      <pages>35-44</pages>
      <abstract>Misinformation rarely presents itself as entirely true or entirely false. Instead, it often embeds partial truths within misleading contexts, creating narratives that blur the boundary between fact and falsehood. Traditional binary fact-checking frameworks fail to capture this nuance, forcing complex claims into oversimplified categories. To address this gap, we introduce MEGA, a multidimensional graph framework designed to classify ambiguous claims, with a particular focus on those labelled Somewhat True. MEGA integrates event evidence, spatio-temporal metadata, and a quantifiable nuance score. Its Event Candidate Extraction (ECE) module identifies supporting or contradicting evidence, while the Nuance Control Module (NCM) injects or removes nuance to assess its effect on classification. Experiments show that nuance is both detectable and learnable: adding nuance improves borderline discrimination, while stripping it leads the decisions toward false extremes and conceals partial truth. Our top model— nuance-injected without score weighting— improve accuracy and F1 score by 15 and 16 points over the claims-only baseline, and 6 and 9 points over the ECE-only variant. These results show that explicitly modeling nuance alongside context is crucial for classifying mixed-truth claims and advancing fact-checking beyond binary judgments.</abstract>
      <url hash="c52638ed">2025.rocling-main.5</url>
      <bibkey>albertson-etal-2025-beyond</bibkey>
    </paper>
    <paper id="6">
      <title>A Preliminary Study of <fixed-case>RAG</fixed-case> for <fixed-case>T</fixed-case>aiwanese Historical Archives</title>
      <author><first>Claire</first><last>Lin</last></author>
      <author><first>Bo-Han</first><last>Feng</last></author>
      <author><first>Xuanjun</first><last>Chen</last></author>
      <author><first>Te-Lun</first><last>Yang</last></author>
      <author><first>Hung-Yi</first><last>Lee</last></author>
      <author><first>Jyh-Shing Roger</first><last>Jang</last></author>
      <pages>45-62</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.</abstract>
      <url hash="62d459b2">2025.rocling-main.6</url>
      <bibkey>lin-etal-2025-preliminary</bibkey>
    </paper>
    <paper id="7">
      <title>Bridging Underspecified Queries and Multimodal Retrieval: A Two-Stage Query Rewriting Approach</title>
      <author><first>Szu-Ting</first><last>Liu</last></author>
      <author><first>Wen-Yu</first><last>Cho</last></author>
      <author><first>Hsin-Wei</first><last>Wang</last></author>
      <author><first>Berlin</first><last>Chen</last></author>
      <pages>63-70</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has proven effective for text-only question answering, yet expanding it to visually rich documents remains a challenge. Existing multimodal benchmarks, often derived from visual question answering (VQA) datasets, or large vision-language model (LVLM)-generated query-image pairs, which often contain underspecified questions that assume direct image access. To mitigate this issue, we propose a two-stage query rewriting framework that first generates OCR-based image descriptions and then reformulates queries into precise, retrieval-friendly forms under explicit constraints. Experiments show consistent improvements across dense, hybrid and multimodal retrieval paradigms, with the most pronounced gains in visual document retrieval – Hits@1 rises from 21.0% to 56.6% with VDocRetriever and further to 79.3% when OCR-based descriptions are incorporated. These results indicate that query rewriting, particularly when combined with multimodal fusion, provides a reliable and scalable solution to bridge underspecified queries and improve retrieval over visually rich documents.</abstract>
      <url hash="1c3d8072">2025.rocling-main.7</url>
      <bibkey>liu-etal-2025-bridging-underspecified</bibkey>
    </paper>
    <paper id="8">
      <title>The Study of a Traffic Accident Information Collection Agent System Based on Fine-tuned Open-Source Large Language Models</title>
      <author><first>Jo-Chi</first><last>Kung</last></author>
      <author><first>Chia-Hui</first><last>Chang</last></author>
      <pages>71-79</pages>
      <abstract>本研究提出了一套名為「交通事故資訊蒐集代理人」(Collision Care Guide, CCG)的系統架構,專注於事故初期階段的結構化資訊蒐集。CCG 整合三大模組:問題生成、資訊擷取及事故重建,透過多輪對話引導使用者敘述事故細節並轉換為結構化資料格式(TARF),同時生成可讀性敘述供核對。為滿足成本效益、隱私保護及部署彈性需求,本研究比較開源 Llama 模型(3B/8B 參數,完整微調及 4-bit PEFT 方法)與商業基準 GPT-4o-mini 的效能表現。結果顯示,資訊擷取模組欄位準確率高於 0.94,JSON 語義相似度達 0.995;問題生成模組語義相似度介於 0.85-0.88,問題表達更加精煉。微調模型在對話品質與資訊擷取的 LLM 評估中均獲得 4 分以上(滿分 5 分),與商業基準差距小於 0.5 分。研究證實開源模型經微調後能逼近商業模型效能,且量化版本在資源受限場景中具備高效能與部署潛力。CCG 的設計填補了事故初期互動式資訊蒐集的技術空白,為交通事故處理提供了高效且具成本優勢的解決方案。</abstract>
      <url hash="2b886db7">2025.rocling-main.8</url>
      <bibkey>kung-chang-2025-study</bibkey>
    </paper>
    <paper id="9">
      <title>Automatic Generation of Corpus-Based Exercises Using Generative <fixed-case>AI</fixed-case></title>
      <author><first>Adrian Jan</first><last>Zasina</last></author>
      <pages>80-86</pages>
      <abstract>This study explores the automatic generation of corpus-based language exercises using generative AI models. We focus on the interaction between language models and corpus data, detailing a workflow in which lexical and syntactic patterns are extracted from a tagged corpus and structured prompts are constructed to guide the model in producing sentence-level exercises. The generated exercises reveal both the potential of AI-driven approaches. However, observations highlight the necessity of careful design and critical evaluation when integrating generative models with corpus-based language materials. By analysing these processes from a computational linguistics perspective, this study contributes to understanding how generative AI can interact with structured linguistic data, informing future applications in automated language resources.</abstract>
      <url hash="e6b31099">2025.rocling-main.9</url>
      <bibkey>zasina-2025-automatic</bibkey>
    </paper>
    <paper id="10">
      <title>Diversity is the Key: Enhancing <fixed-case>LLM</fixed-case>-based Post-processing for Automated Audio Captioning</title>
      <author><first>Seyed Ali</first><last>Farokh</last></author>
      <author><first>Mohammad Mehdi</first><last>Homayounpour</last></author>
      <author><first>Ahmad</first><last>Nickabadi</last></author>
      <pages>87-94</pages>
      <abstract>Automated Audio Captioning (AAC) is a multimodal task aimed at generating natural language descriptions of audio content. Previous studies have shown that LLMs can improve AAC performance by summarizing audio events based on a list of candidate captions, which are selected by an external reranker from those generated using Nucleus Sampling. However, the reranking process often selects overly similar captions, disregarding the original diversity of the sampled captions. In this work, we show that this diversity reflects the AAC model’s level of certainty and propose a lightweight candidate selection approach that preserves the initial diversity of the generated captions. This, in turn, enables an LLM to summarize the captions while considering the AAC model’s certainty in a few-shot setting. Experimental results demonstrate that our method outperforms previous post-processing techniques while being significantly faster.</abstract>
      <url hash="85219eb4">2025.rocling-main.10</url>
      <bibkey>farokh-etal-2025-diversity</bibkey>
    </paper>
    <paper id="11">
      <title>Memory-Efficient Training for Text-Dependent <fixed-case>SV</fixed-case> with Independent Pre-trained Models</title>
      <author><first>Seyed Ali</first><last>Farokh</last></author>
      <author><first>Hossein</first><last>Zeinali</last></author>
      <pages>95-102</pages>
      <abstract>This paper presents our submission to the Iranian division of the Text-Dependent Speaker Verification Challenge (TdSV) 2024. Conventional TdSV approaches typically jointly model speaker and linguistic features, requiring unsegmented inputs during training and incurring high computational costs. Additionally, these methods often fine-tune large-scale pre-trained speaker embedding models on the target domain dataset, which may compromise the pre-trained models’ original ability to capture speaker-specific characteristics. To overcome these limitations, we employ a TdSV system that utilizes two pre-trained models independently and demonstrate that, by leveraging pre-trained models with targeted domain adaptation, competitive results can be achieved while avoiding the substantial computational costs associated with joint fine-tuning on unsegmented inputs in conventional approaches. Our best system reached a MinDCF of 0.0358 on the evaluation subset and secured first place in the challenge.</abstract>
      <url hash="a14b9f98">2025.rocling-main.11</url>
      <bibkey>farokh-zeinali-2025-memory</bibkey>
    </paper>
    <paper id="12">
      <title>Information-theoretic conditioning in terminological alternations in specialized domains: The cases of <fixed-case>T</fixed-case>aiwan <fixed-case>M</fixed-case>andarin legal language and <fixed-case>E</fixed-case>nglish biomedical language</title>
      <author><first>Po-Hsuan</first><last>Huang</last></author>
      <author><first>Hsuan-Lei</first><last>Shao</last></author>
      <pages>103-107</pages>
      <abstract>This study examines how information-theoretic correlates, specifically contextual surprisal, condition terminological alternations in specialized domains, where both domain-specific and general terms express similar concepts. Specifically, two competing theories exist. The Uniform Information Density (UID) theory proposes that the speaker would avoid abrupt information rate changes. This predicts the use of more specific variants when the surprisals are higher. Conversely, availability-based production suggests the use of more readily-accessible items with higher surprisals. This study examines the dynamics between these two potential mechanisms in the terminological use in specialized domains. Specifically, we argue that, in specialized language, due to the higher frequency of domain-specific terms, both accounts predict the use of specific items in higher-surprisal contexts. The cases of Taiwan Mandarin legal language and English biomedical language were, therefore, examined. Crucially, a current popular method for probability estimation is through large language models (LLMs). The linguistic distribution in specialized domains, however, may deviate from the general linguistic distribution on which the LLMs are trained. Thus, we propose a novel semantics-based method of estimating the token probability distribution in a given corpus that avoids the potentially different linguistic distribution and the issue of word segmentation. As expected, results indicated a positive correlation between a variable’s surprisal and the use of domain-specific variants in both cases. This supports UID-based production, and arguably also availability-based production, since more specific and frequent variants are preferred in high-surprisal contexts. Specifically, our semantics-based probability estimation outperformed LLM-based estimation and the baseline in both cases. This suggests the feasibility of semantics-based probability estimation in specialized domains.</abstract>
      <url hash="29e98eeb">2025.rocling-main.12</url>
      <bibkey>huang-shao-2025-information</bibkey>
    </paper>
    <paper id="13">
      <title>Voice Spoofing Detection via Speech Rule Generation Using wav2vec 2.0-Based Attention</title>
      <author><first>Qian-Bei</first><last>Hong</last></author>
      <author><first>Yu-Chen</first><last>Gao</last></author>
      <author><first>Yu-Ying</first><last>Xiao</last></author>
      <author><first>Yeou-Jiunn</first><last>Chen</last></author>
      <author><first>Kun-Yi</first><last>Huang</last></author>
      <pages>108-115</pages>
      <abstract>Recent advancements in AI-based voice cloning have led to increasingly convincing synthetic speech, posing significant threats to speaker verification systems. In this paper, we propose a novel voice spoofing detection method that integrates acoustic feature variations with attention mechanisms derived from wav2vec 2.0 representations. Unlike prior approaches that directly utilize wav2vec 2.0 features as model inputs, the proposed method leverages wav2vec 2.0 features to construct speech rules characteristic of bona-fide speech. Experimental results indicate that the proposed RULE-AASIST-L system significantly outperforms the baseline systems on the ASVspoof 2019 LA evaluation set, achieving a 24.6% relative reduction in equal error rate (EER) and an 10.8% reduction in minimum tandem detection cost function (min t-DCF). Ablation studies further confirm the importance of incorporating speech rules and selecting appropriate hidden layer representations. These findings highlight the potential of using self-supervised representations to guide rule-based modeling for robust spoofing detection.</abstract>
      <url hash="ba5f2a2c">2025.rocling-main.13</url>
      <bibkey>hong-etal-2025-voice</bibkey>
    </paper>
    <paper id="14">
      <title>Computational Approaches to Quantitative Analysis of Pause Duration in <fixed-case>T</fixed-case>aiwan <fixed-case>M</fixed-case>andarin</title>
      <author><first>I-Ping</first><last>Wan</last></author>
      <author><first>Yu-Ju</first><last>Lai</last></author>
      <author><first>Pu</first><last>Yu</last></author>
      <pages>116-123</pages>
      <abstract>This study presents a quantitative analysis of pause-duration patterns in a Mandarin spoken corpus to establish a baseline for prosodic and cognitive assessment. Drawing on cross-linguistic research, the distribution of pause patterns is viewed as reflecting multiple underlying factors. Longer pauses aligned with prosodic and syntactic boundaries indicate more deliberative and planned discourse rather than spontaneous speech. Such settings place higher demands on cognitive and articulatory planning, producing extended thinking time as speakers handle complex topics and specialized terminology. The spoken corpus was automatically processed and annotated using an in-house alignment and pause-tagging pipeline. Outlier detection with a 3.0×IQR threshold retained 35,474 tokens and removed extreme values exceeding 1,016 ms. Short and medium pauses remained stable across mean, median, and variability measures, while long pauses showed a moderate reduction (16,436 to 15,420 tokens), with mean duration decreasing from 535 to 426 ms and standard deviation sharply reduced from 786 to 169 ms, while the median stayed around 370–380 ms. These findings demonstrate that automatic cleaning primarily removed aberrant values while preserving linguistically meaningful long pauses. This baseline from non-impaired adult speakers underscores the need for corpus-specific frameworks and offers a reference point for cross-linguistic research on speech planning.</abstract>
      <url hash="25ddb61f">2025.rocling-main.14</url>
      <bibkey>wan-etal-2025-computational</bibkey>
    </paper>
    <paper id="15">
      <title>A Novel <fixed-case>C</fixed-case>hinese-Idiom Automatic Error Correction Method Based on the Hidden <fixed-case>M</fixed-case>arkov Model</title>
      <author><first>Rongbin</first><last>Zhang</last></author>
      <author><first>Anlu</first><last>Gui</last></author>
      <author><first>Peng</first><last>Cao</last></author>
      <author><first>Lingfeng</first><last>Wu</last></author>
      <author><first>Feng</first><last>Huang</last></author>
      <author><first>Jiahui</first><last>Li</last></author>
      <pages>124-132</pages>
      <abstract>Spelling errors in Chinese idioms frequently occur due to various types of misspellings and optical character recognition (OCR) errors in daily learning and usage. Achieving automatic error correction for Chinese idioms is one of the important natural language processing tasks, as it helps improve the quality of Chinese texts as well as language learning. Existing methods, such as edit distance and custom dictionary approaches, suffer from limited error correction capability, low computational efficiency, and weak flexibility. To address these limitations, this paper proposes a novel automatic error correction method for Chinese idioms based on the hidden Markov model (HMM). Specifically, the generation process of idiom spelling errors is modeled using an HMM, transforming the idiom correction problem into a matching task between erroneous idioms and legitimate idioms. By constructing a legitimate idiom table and a Chinese character confusion set, a prototype system for idiom correction was developed, and performance testing was completed. Experiment results demonstrate that the proposed model is simpler with fewer parameters and has lower computational complexity while exhibiting stronger error correction capability and parameter robustness as compared to existing methods. It can more flexibly correct diverse types of idiom errors, showing high potential application value.</abstract>
      <url hash="f2fe750c">2025.rocling-main.15</url>
      <bibkey>zhang-etal-2025-novel</bibkey>
    </paper>
    <paper id="16">
      <title>Toward Traditional <fixed-case>C</fixed-case>hinese <fixed-case>M</fixed-case>odern<fixed-case>BERT</fixed-case>: A Preliminary Study</title>
      <author><first>Yi-En</first><last>Chen</last></author>
      <author><first>Qiao-Ying</first><last>He</last></author>
      <author><first>Kuan-Yu</first><last>Chen</last></author>
      <pages>133-139</pages>
      <abstract>This study employs several state-of-the-art techniques, including RoPE and Flash Attention, and leverages large-scale Chinese web corpora and encyclopedic data to pre-train an encoder model specifically designed for long text in Traditional Chinese. We evaluate the model on tasks such as reading comprehension and text classification, and the results show that its overall performance lags behind existing Chinese benchmarks. Through pseudo-perplexity analysis, we infer that the pre-training phase did not sufficiently capture the data distribution, potentially due to factors such as hyperparameters, convergence, and data quality. Although the results are suboptimal, this study still offers valuable experimental insights and directions for improving Chinese language model development.</abstract>
      <url hash="1191c1a8">2025.rocling-main.16</url>
      <bibkey>chen-etal-2025-toward</bibkey>
    </paper>
    <paper id="17">
      <title>Effective Speaker Diarization Leveraging Multi-task Logarithmic Loss Objectives</title>
      <author><first>Jhih-Rong</first><last>Guo</last></author>
      <author><first>Tien-Hong</first><last>Lo</last></author>
      <author><first>Yu-Sheng</first><last>Tsao</last></author>
      <author><first>Pei-Ying</first><last>Lee</last></author>
      <author><first>Yung-Chang</first><last>Hsu</last></author>
      <author><first>Berlin</first><last>Chen</last></author>
      <pages>140-145</pages>
      <abstract>End-to-End Neural Diarization (EEND) has undergone substantial development, particularly with powerset classification methods that enhance performance but can exacerbate speaker confusion. To address this, we propose a novel training strategy that complements the standard cross entropy loss with an auxiliary ordinal log loss, guided by a distance matrix of speaker combinations. Our experiments reveal that while this approach yields significant relative improvements of 15.8% in false alarm rate and 10.0% in confusion error rate, it also uncovers a critical trade-off with an increased missed error rate. The primary contribution of this work is the identification and analysis of this trade-off, which stems from the model adopting a more conservative prediction strategy. This insight is crucial for designing more balanced and effective loss functions in speaker diarization.</abstract>
      <url hash="66198f34">2025.rocling-main.17</url>
      <bibkey>guo-etal-2025-effective</bibkey>
    </paper>
    <paper id="18">
      <title>Leveraging Weak Segment Labels for Robust Automated Speaking Assessment in Read-Aloud Tasks</title>
      <author><first>Yue-Yang</first><last>He</last></author>
      <author><first>Berlin</first><last>Chen</last></author>
      <pages>146-152</pages>
      <abstract>Automated speaking assessment (ASA) has become a crucial component in computer-assisted language learning, providing scalable, objective, and timely feedback to second-language learners. While early ASA systems relied on hand-crafted features and shallow classifiers, recent advances in self-supervised learning (SSL) have enabled richer representations for both text and speech, improving assessment accuracy. Despite these advances, challenges remain in evaluating long speech responses, due to limited labeled data, class imbalance, and the importance of pronunciation clarity and fluency, especially for read-aloud tasks. In this work, we propose a segment-based ASA framework leveraging WhisperX to split long responses into shorter fragments, generate weak labels from holistic scores, and aggregate segment-level predictions to obtain final proficiency scores. Experiments on the GEPT corpus demonstrate that our framework outperforms baseline holistic models, generalizes robustly to unseen prompts and speakers, and provides diagnostic insights at both segment and response levels.</abstract>
      <url hash="8d838b61">2025.rocling-main.18</url>
      <bibkey>he-chen-2025-leveraging</bibkey>
    </paper>
    <paper id="19">
      <title>Exploring the Feasibility of Large Language Model- and Rubric-Based Automatic Assessment of Elementary Students’ Book Summaries</title>
      <author><first>Qi-Zhen</first><last>Huang</last></author>
      <author><first>Hou-Chiang</first><last>Tseng</last></author>
      <author><first>Yao-Ting</first><last>Sung</last></author>
      <pages>153-166</pages>
      <abstract>摘要寫作為閱讀與寫作整合的高層次語文任務,不僅可評量學生的文本理解能力,也能促進語言表達與重述能力的培養。過去自動摘要批改系統多依賴關鍵詞比對或語義重疊等「由下而上」的方法,較難以全面評估學生的理解深度與文本重述能力,且中文摘要寫作批改研究雖有,但相較於英文仍相對不足,形成研究缺口。隨著大型語言模型(Large Language Models, LLMs)的發展,其在語意理解與生成能力上的突破,為自動摘要批改與回饋帶來新契機。有鑑於此,本研究旨以由上而下的方式探討結合LLMs與閱讀摘要評分規準(Rubrics)對學生閱讀摘要批改與回饋之應用潛力,進一步而言,在考量教學資料隱私的情況下,本研究採用Meta-Llama-3.1-70B生成電腦摘要,並依據專家所制定的摘要評分規準,其評分涵蓋:理解與準確性、組織結構、簡潔性、語言表達與文法及重述能力五大構面,對學生閱讀摘要進行自動評分與回饋。研究結果顯示,Meta-Llama-3.1-70B能提供具體、清晰的即時回饋,不僅能指出摘要中遺漏的關鍵概念,也能針對結構安排與語法錯誤提出修正建議,協助學生快速掌握摘要改進方向;然而回饋多偏向表面語言與結構調整,在語言表達、修辭多樣性及重述能力等高層次語文能力評估上仍存在限制。整體而言,LLMs可作為形成性評量與教學輔助工具,提升評分效率,但需結合教師專業判斷與回饋以補足深層概念與策略性寫作指導,促進學生摘要寫作能力的發展。</abstract>
      <url hash="f1f31d0a">2025.rocling-main.19</url>
      <bibkey>huang-etal-2025-exploring</bibkey>
    </paper>
    <paper id="20">
      <title>From Scarcity to Scalability: Lexicon and Grammar Enhanced <fixed-case>A</fixed-case>mis to <fixed-case>M</fixed-case>andarin Translation with <fixed-case>GPT</fixed-case> Models</title>
      <author><first>Joseph</first><last>Lin</last></author>
      <author><first>Kai-Ying</first><last>Lin</last></author>
      <author><first>Hung-Yu</first><last>Kao</last></author>
      <pages>167-175</pages>
      <abstract>Machine translation (MT) for low-resource languages remains constrained by extreme data scarcity, making traditional fine-tuning infeasible. This study examines Amis→Mandarin translation as a practical case, leveraging GPT-4o-mini and GPT-5-mini with dictionary integration and grammar-informed prompting. Experiments show that GPT-5-mini, supported by dictionary, achieves usable quality (BLEU-3 ∼31, COMET ∼78, BLEURT ∼71). To address the bottleneck of incomplete dictionaries, we propose Context-Driven Lexical Augmentation, which infers Mandarin equivalents for unseen Amis terms from corpus context, raising BLEU-3 to 34 and establishing a stronger basis for semi-automatic corpus generation. These results demonstrate that expanding and refining dictionary provides greater benefits than parameter-intensive fine-tuning in extremely low-resource settings. We also discuss the performance gap between Amis→Mandarin and Mandarin→Amis translation, attributing it to Amis’s morphological complexity and narrower semantic coverage. Overall, our resource-driven strategy offers a scalable pathway toward high-quality MT and corpus expansion, ultimately supporting both linguistic research and language revitalization.</abstract>
      <url hash="880ceb68">2025.rocling-main.20</url>
      <bibkey>lin-etal-2025-scarcity</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>CL</fixed-case>i<fixed-case>FT</fixed-case>-<fixed-case>ASR</fixed-case>: A Cross-Lingual Fine-Tuning Framework for Low-Resource <fixed-case>T</fixed-case>aiwanese Hokkien Speech Recognition</title>
      <author><first>Hung-Yang</first><last>Sung</last></author>
      <author><first>Chien-Chun</first><last>Wang</last></author>
      <author><first>Kuan-Tang</first><last>Huang</last></author>
      <author><first>Tien-Hong</first><last>Lo</last></author>
      <author><first>Yu-Sheng</first><last>Tsao</last></author>
      <author><first>Yung-Chang</first><last>Hsu</last></author>
      <author><first>Berlin</first><last>Chen</last></author>
      <pages>176-183</pages>
      <abstract>Automatic speech recognition (ASR) for low-resource languages such as Taiwanese Hokkien is difficult due to the scarcity of annotated data. However, direct fine-tuning on Han-character transcriptions often fails to capture detailed phonetic and tonal cues, while training only on romanization lacks lexical and syntactic coverage. In addition, prior studies have rarely explored staged strategies that integrate both annotation types. To address this gap, we present CLiFT-ASR, a cross-lingual fine-tuning framework that builds on Mandarin HuBERT models and progressively adapts them to Taiwanese Hokkien. The framework employs a two-stage process in which it first learns acoustic and tonal representations from phonetic Tai-lo annotations and then captures vocabulary and syntax from Han-character transcriptions. This progressive adaptation enables effective alignment between speech sounds and orthographic structures. Experiments on the TAT-MOE corpus demonstrate that CLiFT-ASR achieves a 24.88% relative reduction in character error rate (CER) compared with strong baselines. The results indicate that CLiFT-ASR provides an effective and parameter-efficient solution for Taiwanese Hokkien ASR and that it has potential to benefit other low-resource language scenarios.</abstract>
      <url hash="b11bcbcf">2025.rocling-main.21</url>
      <bibkey>sung-etal-2025-clift</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>MINAS</fixed-case>: <fixed-case>M</fixed-case>andarin Intelligent Narrative Assessment of Syntax for Children</title>
      <author><first>Ruei-Ru</first><last>Wang</last></author>
      <author><first>Ya-Sin</first><last>Li</last></author>
      <author><first>Yi-Shuo</first><last>Yin</last></author>
      <author><first>Tao-Yu</first><last>Chen</last></author>
      <author><first>Hint-Tat</first><last>Cheung</last></author>
      <author><first>Ching-Tai</first><last>Chen</last></author>
      <pages>184-192</pages>
      <abstract>Children’s narrative ability is an important indicator of language development and is commonly used in clinical diagnosis and linguistic research. However, the lack of large-scale, standardized, and accurately annotated Chinese child language corpora makes grammatical analysis both time-consuming and prone to subjectivity, while existing automated tools fall short of clinical and research needs. This study introduces MINAS (Mandarin Intelligent Narrative Assessment of Syntax for Children), which integrates the MAIN story framework with the MAPS-R syntactic framework to construct a Chinese narrative corpus encompassing four categories and 20 indicators. We evaluated commercial models (ChatGPT-4, Claude Sonnet 4, Gemini 2.5 Flash, DeepSeek) through prompt engineering, and fine-tuned open-source models (Chinese RoBERTa, OpenHermes-2.5) with LoRA. Experimental results show that few-shot prompting achieves high accuracy across most indicators, while fine-tuning with LoRA achieves better performance in noun and verb phrase identification but is not as good for complex sentence structures. This study validates the feasibility of applying large language models to syntactic classification of Chinese child narrative corpora, highlighting their potential in clinical applications and linguistic research.</abstract>
      <url hash="9db2ceeb">2025.rocling-main.22</url>
      <bibkey>wang-etal-2025-minas</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>LOBSTER</fixed-case>: Linguistics Olympiad Benchmark for Structured Evaluation on Reasoning</title>
      <author><first>Da-Chen</first><last>Lian</last></author>
      <author><first>Ri-Sheng</first><last>Huang</last></author>
      <author><first>Pin-Er</first><last>Chen</last></author>
      <author><first>Chunki</first><last>Lim</last></author>
      <author><first>You-Kuan</first><last>Lin</last></author>
      <author><first>Guan-Yu</first><last>Tseng</last></author>
      <author><first>Zhen-Yu</first><last>Lin</last></author>
      <author><first>Pin-Cheng</first><last>Chen</last></author>
      <author><first>Shu-Kai</first><last>Hsieh</last></author>
      <pages>193-229</pages>
      <abstract>We propose the Linguistics Olympiad Benchmark for Structured Evaluation on Reasoning, or LOBSTER, a linguistically-informed benchmark designed to evaluate large language models (LLMs) on complex linguistic puzzles of the International Linguistics Olympiad (IOL). Unlike prior benchmarks that focus solely on final answer accuracy, our benchmark provides concrete evaluation protocols and rich typological metadata across over 90 low-resource and cross-cultural languages alongside the puzzles. Through systematic evaluations of state-of-the-art models on multilingual abilities, we demonstrate that LLMs struggle with low-resource languages, underscoring the need for such a benchmark. Experiments with various models on our benchmark showed that IOL problems remain a challenging task for reasoning models, though there are ways to enhance the performance—for example, iterative reasoning outperforms single-pass approaches in both final answers and explanations. Our benchmark offers a comprehensive foundation for advancing linguistically grounded, culturally informed, and cognitively plausible reasoning in LLMs.</abstract>
      <url hash="3a9765d0">2025.rocling-main.23</url>
      <bibkey>lian-etal-2025-lobster</bibkey>
    </paper>
    <paper id="24">
      <title>Cross-user Collaborative and Sequential Modeling for Recommendation</title>
      <author><first>Qiao-Ying</first><last>He</last></author>
      <author><first>Yi-En</first><last>Chen</last></author>
      <author><first>Kuan-Yu</first><last>Chen</last></author>
      <pages>230-236</pages>
      <abstract>Multi-behavior recommendation leverages auxiliary behaviors to effectively alleviate the sparsity of target behaviors. Existing approaches can be broadly categorized into two paradigms: sequential models that capture individual temporal dynamics but often omit cross-user information, and graph-based models that mine collaborative patterns yet lack temporal dependency modeling. To address these limitations, this paper proposes an integrated approach that combines sequential and graph modeling: the former focuses on learning temporal dependencies within user behavior sequences, while the latter captures cross-user behavior paths. By fusing the predictions from both components, the method achieves more accurate recommendations. Experiments on two e-commerce datasets, Taobao and RetailRocket, show that the integrated model outperforms the strong baseline MB-STR by about 1% in both HR@10 and NDCG@10. These results indicate that incorporating cross-user collaborative information consistently improves performance, even on top of strong sequential models.</abstract>
      <url hash="14ac1d65">2025.rocling-main.24</url>
      <bibkey>he-etal-2025-cross</bibkey>
    </paper>
    <paper id="25">
      <title>Structured vs. Unstructured Inputs in <fixed-case>LLM</fixed-case>s: Evaluating the Semantic and Pragmatic Predictive Power in Abnormal Event Forecasting</title>
      <author><first>Jou-An</first><last>Chi</last></author>
      <author><first>Shu-Kai</first><last>Hsieh</last></author>
      <pages>237-248</pages>
      <abstract>Large Language Models (LLMs) are increasingly applied to temporally grounded reasoning tasks, yet the role of input representation remains unclear. This paper compares structured temporal inputs, represented as Temporal Knowledge Graphs (TKGs), with unstructured captions in two settings: forecasting future events and detecting anomalies in surveillance video descriptions. To enable direct comparison, we build a unified dataset by aligning anomaly labels from UCF-Crime with caption annotations from UCA. Experiments show that unstructured captions consistently yield slightly higher scores across both tasks, but the differences do not reach statistical significance. Their trade-offs, however, differ: captions provide richer semantic cues for generation, while TKGs reduce input length, suppress noise, and enhance interpretability. These findings suggest that action-centric corpora, such as surveillance or forensic narratives, naturally lend themselves to structured representations, which can provide temporal scaffolds for timeline reconstruction and more traceable reasoning. All code, data processing scripts, and experimental results are available at our GitHub repository.</abstract>
      <url hash="7bf82843">2025.rocling-main.25</url>
      <bibkey>chi-hsieh-2025-structured</bibkey>
    </paper>
    <paper id="26">
      <title>Embodiment in Multimodal Semantics: Comparing Sensory, Emotional, and Visual Features in <fixed-case>C</fixed-case>hinese Color Metaphors</title>
      <author><first>Yufeng</first><last>Wu</last></author>
      <author><first>Meichun</first><last>Liu</last></author>
      <pages>249-257</pages>
      <abstract>This study examines how sensory-motor experience, emotional valence and arousal, and visual image statistics contribute to multimodal alignment in Chinese color metaphors. Using 184 metaphorical lexemes from six basic color terms, we combined textual data from the Chinese Corpus Internet (CCI 3.0) with image sets from Baidu, embedding both with Chinese-CLIP and measuring alignment via cosine similarity. Sensory-motor ratings, particularly effector exclusivity and tactile strength, correlated negatively with alignment, while emotional valence showed strong positive correlations and visual features such as color variability and entropy contributed positively. Regression and importance analyses confirmed emotion as the most reliable predictor, with sensory ratings offering little explanatory power. The findings indicate that affective salience and perceptual richness, rather than generalized sensory norms, are central to the embodied grounding of metaphorical words in multimodal contexts.</abstract>
      <url hash="96f2c73a">2025.rocling-main.26</url>
      <bibkey>wu-liu-2025-embodiment</bibkey>
    </paper>
    <paper id="27">
      <title>Language Modeling Using Entanglement Enhanced Tensor Trains</title>
      <author><first>Ellis</first><last>Reyes</last></author>
      <author><first>Yi-Shin</first><last>Chen</last></author>
      <pages>258-265</pages>
      <abstract>Tensor Train Language Models (TTLMs) offer significant memory savings by representing text sequences as tensor networks, but naive implementations struggle with long-range dependencies and limited flexibility. We introduce a modular TTLM framework that combine local and non-local context modules to achieve scalable language modeling. Our non-local modules, inspired by entanglement in quantum information theory, enable efficient modeling of long-range interactions between hidden states. Experiments on Penn Treebank and Wikitext datasets show that our modular TTLM, including entanglement-augmented variants, outperform naive baselines. These results highlight TTLMs as a promising, memory-efficient alternatives for modern language modeling.</abstract>
      <url hash="c00f64df">2025.rocling-main.27</url>
      <bibkey>reyes-chen-2025-language</bibkey>
    </paper>
    <paper id="28">
      <title>Multimodal Fake News Detection Combining Social Network Features with Images and Text</title>
      <author><first>Lawrence Yung Hak</first><last>Low</last></author>
      <author><first>Yen-Tsang</first><last>Wu</last></author>
      <author><first>Yan-Hong</first><last>Liu</last></author>
      <author><first>Jenq-Haur</first><last>Wang</last></author>
      <pages>266-276</pages>
      <abstract>The rapid development of social networks, coupled with the prevalence of Generative AI (GAI) in our society today, has led to a sharp increase in fake tweets and fake news on social media platforms. These fake media led to more in-depth research on fake news detection. At present, there are two mainstream methods used in detecting fake news, namely content-based fake news detection and propagation / network-based fake news detection. Early content-based detection method inputs an article’s content and uses a similarity algorithm to identify fake news. This method improved by using single-modality features such as images and text as input features. However, existing research shows that single-modality features alone cannot identify fake news efficiently. The most recent method then fuses multimodal features such as images and text, as features to be input into the model for classification purposes. The second propagation / network-based fake news detection method creates graphs or decision trees through social networks, treating them as features to be input into the model for classification purposes. In this study, we propose a multimodal fake news detection framework that combines these two mainstream methods. This framework not only uses images and text as input features but also combines social metadata features such as comments. The framework extracts these comments and builds them into a tree structure to obtain its features. Furthermore, we also propose different feature fusion methods which can achieve better results compared with the existing methods. Finally, we conducted ablation experiments and proved that each module is required to contribute to the framework’s overall performance. This clearly demonstrated the effectiveness of our proposed approach.</abstract>
      <url hash="6ae72c5d">2025.rocling-main.28</url>
      <bibkey>low-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="29">
      <title>Speech-Driven Editing System for <fixed-case>C</fixed-case>hinese <fixed-case>ASR</fixed-case> Errors</title>
      <author><first>Sji-Jie</first><last>Ding</last></author>
      <author><first>Chia-Hui</first><last>Chang</last></author>
      <author><first>Zi-Xuan</first><last>Jian</last></author>
      <pages>277-285</pages>
      <abstract>Despite recent advances in AI, ASR systems still struggle with real-world errors from pronunciation and homophones. To solve this issue, we propose a verbal-command-based correction system that enables users to utter natural-language instructions to refine recognition outputs with minimal effort. The system consists of three modules: an input classifier, a command classifier, and a correction labeler. To support training and evaluation, we simulate ASR errors via TTS and ASR pipelines to simulate the potential errors, followed by verbal correction commands issued based on linguistic features or LLMs. Experiments show that the overall system achieves over 80% correction accuracy and delivers stable performance. Compared to manual correction, this system also demonstrates highly competitive correction speed, which sufficiently indicates its feasibility for practical deployment.</abstract>
      <url hash="929058e1">2025.rocling-main.29</url>
      <bibkey>ding-etal-2025-speech</bibkey>
    </paper>
    <paper id="30">
      <title>A Fake News Detection Model Utilizing Graph Neural Networks to Capture Writing Styles</title>
      <author><first>Yen-Tsang</first><last>Wu</last></author>
      <author><first>Lawrence Y. H</first><last>Low</last></author>
      <author><first>Jenq-Haur</first><last>Wang</last></author>
      <pages>286-295</pages>
      <abstract>本文提出 CWSMN(Capture Writing Style Multi-Graph Network),一個以圖神經網路為基礎的早期假新聞偵測方法,透過捕捉寫作風格克服傳統語意內容與傳播特徵方法在標註稀缺與跨域泛化不足下的限制。CWSMN 結合文體分析、語意嵌入與多圖融合:以 Bi-GRU 進行上下文初始化,採用 GAT 進行注意力導向的圖聚合,並以 LDA 建構主題圖,同時以輕量級前饋分類器輸出預測。於多個資料集之實驗顯示,CWSMN 對比 BERT、ALBERT 與 GraphSAINT 等強基準皆有穩定超越;在未知來源的 Source-CV 場景尤為顯著,證明其於低資源與跨領域環境之穩健泛化能力,並實現不依賴傳播的早期偵測,實驗結果證實本方法在樣本稀缺與未知來源條件下,仍能達成有效的早期偵測。</abstract>
      <url hash="8fa65392">2025.rocling-main.30</url>
      <bibkey>wu-etal-2025-fake</bibkey>
    </paper>
    <paper id="31">
      <title>Revisiting Pre-trained Language Models for Conversation Disentanglement</title>
      <author><first>Tung-Thien</first><last>Lam</last></author>
      <author><first>Cheng-Zen</first><last>Yang</last></author>
      <pages>296-302</pages>
      <abstract>Multi-party conversation is a popular form in online group chatting. However, the interweaving of utterance threads complicates the understanding of the dialogues for participants. Many conversation disentanglement models have been proposed using transformer-based pre-trained language models (PrLMs). However, advanced transformer-based PrLMs have not been extensively studied. This paper investigates the effectiveness of five advanced PrLMs: BERT, XLNet, ELECTRA, RoBERTa, and ModernBERT. The experimental results show that ELECTRA and RoBERTa are two PrLMs with outstanding performance than other PrLMs for the conversation disentanglement task.</abstract>
      <url hash="b5d11c68">2025.rocling-main.31</url>
      <bibkey>lam-yang-2025-revisiting</bibkey>
    </paper>
    <paper id="32">
      <title>Multilingual Promise Verification in <fixed-case>ESG</fixed-case> Reports with Large Language Model Performance Evaluation</title>
      <author><first>Wei-Chen</first><last>Huang</last></author>
      <author><first>Hsin-Ting</first><last>Lu</last></author>
      <author><first>Wen-Ze</first><last>Chen</last></author>
      <author><first>Min-Yuh</first><last>Day</last></author>
      <pages>303-313</pages>
      <abstract>Corporate ESG reports often contain statements that are vague or difficult to verify, creating room for potential greenwashing. Building automated systems to evaluate such claims is therefore a relevant research direction. Yet, existing analytical tools still show limited ability to verify sustainability promises in multiple languages, especially beyond English. This study examines how large language models (GPT-5) perform in verifying ESG-related promises across Chinese, Japanese, and English reports, aiming to provide a multilingual evaluation baseline. We assess four verification tasks using the PromiseEval datasets [1] in three languages, comparing five prompting strategies from zero-shot to five-shot learning, including Chain-of-Thought reasoning. The four subtasks are Promise Identification (PI), Evidence Status Assessment (ESA), Evidence Quality Evaluation (EQE), and Verification Timeline Prediction (VTP). The five-shot setting achieved the highest overall performance (71.12 % accuracy, 51.92 % Macro-F1). Although the accuracy results appear higher for Chinese (85.12 %) than for Japanese (68.94 %) and English (63.62 %), this mainly reflects class imbalance in the data. Hence, Macro-F1 provides a fairer comparison across languages. Among the four tasks, Evidence Quality Evaluation (EQE) remains the most difficult. While Chain-of-Thought prompting slightly lowers the overall average, it shows selective benefit on the more complex EQE task. Overall, this work offers a clearer multilingual baseline for ESG promise verification and supports the development of language-based tools that enhance the credibility and transparency of sustainability reporting.</abstract>
      <url hash="0832a37b">2025.rocling-main.32</url>
      <bibkey>huang-etal-2025-multilingual-promise</bibkey>
    </paper>
    <paper id="33">
      <title>Exploring Sentence Stress Detection using Whisper-based Speech Models</title>
      <author><first>Ting-An</first><last>Hung</last></author>
      <author><first>Yu-Hsuan</first><last>Hsieh</last></author>
      <author><first>Tien-Hong</first><last>Lo</last></author>
      <author><first>Yung-Chang</first><last>Hsu</last></author>
      <author><first>Berlin</first><last>Chen</last></author>
      <pages>314-319</pages>
      <abstract>Sentence stress reflects the relative prominence of words within a sentence. It is fundamental to speech intelligibility and naturalness, and is particularly important in second language (L2) learning. Accurate stress production facilitates effective communication and reduces misinterpretation. In this work, we investigate sentence stress detection (SSD) using Whisper-based transformer speech models under diverse settings, including model scaling, backbone–decoder interactions, architectural and regularization enhancements, and embedding visualization for interpretability. Results show that smaller Whisper variants achieve stronger performance under limited data, while architectural and regularization enhancements improves stability and generalization. Embedding analysis reveal clear separation between stressed and unstressed words. These findings offer practical insights into model selection, architecture design, and interpretability for SSD applications, with implications for L2 learning support tools.</abstract>
      <url hash="f42f46de">2025.rocling-main.33</url>
      <bibkey>hung-etal-2025-exploring</bibkey>
    </paper>
    <paper id="34">
      <title>Integrating Sequential Information and Graph Structures for Anti-Money Laundering Anomaly Detection</title>
      <author><first>Yin-Ju</first><last>Wu</last></author>
      <author><first>Gavin</first><last>Tseng</last></author>
      <author><first>Berlin</first><last>Chen</last></author>
      <pages>320-325</pages>
      <abstract>反洗錢(Anti-Money Laundering, AML)是金融科技領域的重要研究課題,其目標在於識別潛在的可疑帳戶與交易。然而隨著跨境支付與新型態交易的興起,洗錢行為往往具有高度隱匿性與複雜的網路結構,傳統規則式方法在偵測效能與泛化能力上皆表現不足。近年來,雖然有研究嘗試將機器學習或深度學習方法應用於 AML,但仍存在許多挑戰。為了解決這些問題,本研究提出一個基於序列圖融合的 AML 帳戶風險預測框架。該方法的核心在於同時建模帳戶的個體時序行為與其在交易網路中的結構特徵。首先,將每個帳戶的交易歷史分解為入邊和出邊序列,使用雙分支GRU架構分別編碼,捕捉帳戶的時序交易模式,接著使用雙向注意力圖卷積層,通過差異感知的消息傳遞機制同時處理正向和反向鄰居關係,學習帳戶間的行為差異,並通過注意力機制自適應融合節點自身特徵與雙向鄰居聚合特徵。此外,針對 AML 資料集的極度不平衡特性,引入類別重加權與平衡採樣策略。我們在公開的反洗錢資料集上驗證所提方法,實驗結果顯示該框架在極度不平衡的情境下能取得穩定的 F1 表現,相較於傳統基線方法具有顯著優勢。</abstract>
      <url hash="be8b3767">2025.rocling-main.34</url>
      <bibkey>wu-etal-2025-integrating</bibkey>
    </paper>
    <paper id="35">
      <title>A Multi-faceted Statistical Analysis for Logit-based Pronunciation Assessment</title>
      <author><first>Chieh-Ren</first><last>Liao</last></author>
      <author><first>Berlin</first><last>Chen</last></author>
      <pages>326-333</pages>
      <abstract>The Goodness of Pronunciation (GOP) score for pronunciation quality assessment is a key technology in computer-assisted language learning. Recent studies have shown that computing GOP scores directly from the acoustic model’s raw output logits outperforms traditional softmax-probability-based methods, because logits avoid probability saturation issues and retain richer discriminative information. However, existing logit-based methods mostly rely on basic statistics such as maxima, means, or variances, which neglect the more complex dynamic distributions and temporal characteristics of logit sequences over phoneme durations. To more comprehensively capture pronunciation details embedded in logit sequences, this study proposes a multi-faceted statistical analysis method. We explore five higher-order statistical indicators that describe different characteristics of logit sequences: (1) moment-generating functions to compute distribution skewness and kurtosis; (2) information theory, using entropy to quantify model uncertainty; (3) Gaussian mixture models (GMMs) to fit multimodal distributions of logits; (4) time-series analysis, computing autocorrelation coefficients to measure logit stability; and (5) extreme value theory, using top-k averaging to obtain more robust peak-confidence estimates. We conduct experiments on the public L2 English speech corpus SpeechOcean762, comparing these newly proposed statistical indicators with baseline methods from the literature (GOP_MaxLogit, GOP_margin). Preliminary results show that some higher-order statistical indicators—particularly those that describe logit-sequence stability and distribution shape—achieve higher accuracy on pronunciation-error detection classification tasks and exhibit stronger correlation with human expert ratings. This study demonstrates that deeper statistical modeling of logit sequences is an effective approach to improving the performance of automated pronunciation assessment systems.</abstract>
      <url hash="b7bf2b72">2025.rocling-main.35</url>
      <bibkey>liao-chen-2025-multi</bibkey>
    </paper>
    <paper id="36">
      <title>Learning User Common Interests for Unseen Group Recommendation</title>
      <author><first>Yu-Ting</first><last>Cheng</last></author>
      <author><first>Pin-Hsin</first><last>Hsiao</last></author>
      <author><first>Chiou-Shann</first><last>Fuh</last></author>
      <author><first>Pu-Jen</first><last>Cheng</last></author>
      <pages>334-341</pages>
      <abstract>Previous studies on recommender systems have primarily focused on learning implicit preferences from individual user behaviors or enhancing recommendation performance by identifying similar users. However, in real-life scenarios, group decision-making is often required, such as when a group of friends decides which movie to watch together. Thus, discovering common interests has become a key research issue in group recommendation. The most straightforward approach to group recommendation is to model the past joint behaviors of a user group. Nevertheless, this method fails to handle newly formed groups with no historical interactions. To address this limitation, we apply Graph Convolutional Networks to capture high-order structural features within the user–item interaction graph, thereby uncovering the potential common interests of unseen groups. Experimental evaluations on three real-world datasets demonstrate the feasibility and effectiveness of the proposed method.</abstract>
      <url hash="ac8a70a3">2025.rocling-main.36</url>
      <bibkey>cheng-etal-2025-learning</bibkey>
    </paper>
    <paper id="37">
      <title>Introduction: Persuasive Language in the Age of <fixed-case>AI</fixed-case></title>
      <author><first>Siaw-Fong</first><last>Chung</last></author>
      <pages>342-347</pages>
      <abstract>Persuasive language shapes communication across disciplines and everyday life. As large language models (LLMs) become increasingly integrated into these spheres, understanding persuasion now encompasses both human and machine discourse. This introduction examines how persuasive language operates across diverse contexts by analyzing the interactional frameworks of human and AI communication. It also explores how persuasion emerges in human-AI exchanges and how these insights can inform language education and communication practices. Drawing on perspectives from linguistics, computer science, journalism, and communication studies, it presents persuasion as both a rhetorical and interactional process shaped by technology. Ultimately, it aims to deepen understanding of how AI transforms persuasive practices and to promote greater awareness of persuasion in language learning.</abstract>
      <url hash="5ba297d1">2025.rocling-main.37</url>
      <bibkey>chung-2025-introduction</bibkey>
    </paper>
    <paper id="38">
      <title>Stance and Cohesion: The Use of However and While in <fixed-case>AI</fixed-case>-Human Argumentative Discourse</title>
      <author><first>Yu-Che</first><last>Yen</last></author>
      <author><first>Siaw-Fong</first><last>Chung</last></author>
      <pages>348-357</pages>
      <abstract>This study investigates how connectives However and While, signaling contrast/ concession to construct stances, are distributed by AI chatbots in task-based argumentations. The corpus, comprising 13,482 words of chatbot-produced discourse, was analyzed to examine the connectives’ sentence positions and their relation to content-, writer-, and reader-oriented propositions, based on an integrated framework of Hyland’s (2005) framework and Thetela’s (1997) evaluative-entity framework. A total of 124 tokens of However and While were extracted, excluding tokens whose stance and cohesive functions can’t be clearly interpreted. Results show sentence-initial However (N=40) and sentence-initial while (N=59) are the primary devices for asserting a writer-oriented stance, signaling evaluation, claim or counter-claim. Sentence-initial while are more frequently used to frame a factual premise before projecting writer orientation. As to sentence-medial while, both preceding and subsequent clauses are often presented content-oriented propositions, indicating achieving cohesion is prioritized over expressing an evaluative stance. This study concludes that the use of these connectives, strategically applied in AI-human argumentations, shows how connectives contribute to manage stance construction and discourse coherence.</abstract>
      <url hash="55c4f8a8">2025.rocling-main.38</url>
      <bibkey>yen-chung-2025-stance</bibkey>
    </paper>
    <paper id="39">
      <title>Quantum Perspectives on Persuasive Language in <fixed-case>AI</fixed-case>-Generated News: A <fixed-case>QNLP</fixed-case>-Based Analysis</title>
      <author><first>Jung-Hua</first><last>Liu</last></author>
      <pages>358-368</pages>
      <abstract>This study applies quantum natural language processing (QNLP) to 298 Chinese AI-generated YouTube news articles. Using IBM Qiskit, we reveal multi-reality narratives with high frame competition but low conflict. Headlines employ emotion, content stays neutral or positive, showing strategic ambiguity. QNLP metrics highlight persuasive tactics and implications for communication theory and AI ethics.</abstract>
      <url hash="ffbbb2e0">2025.rocling-main.39</url>
      <bibkey>liu-2025-quantum</bibkey>
    </paper>
    <paper id="40">
      <title>Interpretation of the level of <fixed-case>ANGER</fixed-case> in discussion forum</title>
      <author><first>Suet Ching</first><last>Soon</last></author>
      <pages>369-374</pages>
      <abstract>In this internet era, people have easy access of a vast many options of social media platforms for quick communicating or interacting. The ways how internet users conveyed their emotional expression attracted our interest. This present paper investigates the literal emotional expression of ANGER in Chinese online discussion forum, targeting the term nu4 ‘angry/anger’. We referred to a Bulletin Board System (BBS) in Taiwan which is a conversation-like platform with no emoji icon to convey emotion directly. A collection of 7,464 instances were retrieved from the platform. After deducting noisy data, we looked into the meanings and distribution of nu4 of the 7,285 instances. With nearly a quarter of the data instances belonged to the unconventional use of nu4 where the expression does not necessarily show the emotion of anger, we further analyzed the col-locations of these instances. In conclusions, from the collocates of these unconventional use of nu4, it showed a shift from the expression of emotion to aggressiveness, and to express the extent level of an action.</abstract>
      <url hash="2242d81a">2025.rocling-main.40</url>
      <bibkey>soon-2025-interpretation</bibkey>
    </paper>
    <paper id="41">
      <title><fixed-case>ROCLING</fixed-case>-2025 Shared Task: <fixed-case>C</fixed-case>hinese Dimensional Sentiment Analysis for Medical Self-Reflection Texts</title>
      <author><first>Lung-Hao</first><last>Lee</last></author>
      <author><first>Tzu-Mi</first><last>Lin</last></author>
      <author><first>Hsiu-Min</first><last>Shih</last></author>
      <author><first>Kuo-Kai</first><last>Shyu</last></author>
      <author><first>Anna S.</first><last>Hsu</last></author>
      <author><first>Peih-Ying</first><last>Lu</last></author>
      <pages>375-380</pages>
      <abstract>This paper describes the ROCLING-2025 shared task aimed at Chinese dimensional sentiment analysis for medical self-refection texts, including task organization, data preparation, performance metrics, and evaluation results. A total of six participating teams submitted results for techniques developed for valence-arousal intensity prediction. All datasets with gold standards and evaluation scripts used in this shared task are publicly available online for further research.</abstract>
      <url hash="70522894">2025.rocling-main.41</url>
      <bibkey>lee-etal-2025-rocling</bibkey>
    </paper>
    <paper id="42">
      <title><fixed-case>CYUT</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>ROCLING</fixed-case>-2025 Shared Task: Valence–Arousal Prediction in Physicians’ Texts Using <fixed-case>BERT</fixed-case>, <fixed-case>RAG</fixed-case>, and Multi-Teacher Pseudo-Labeling</title>
      <author><first>Yi-Min</first><last>Jian</last></author>
      <author><first>An Yu</first><last>Hsiao</last></author>
      <author><first>Shih-Hung</first><last>Wu</last></author>
      <pages>381-389</pages>
      <abstract>Accurately modeling physicians’ emotional states from self-reflection texts remains challenging due to the lowresource, domain-specific nature of medical corpora. The proposed workflow performs Retrieval-Augmented Generation (RAG) and multi-teacher pseudo-labeling to generate high-quality augmented data. This workflow enables effective crossdomain adaptation from general text corpora to professional medical texts. Evaluations on the ROCLING 2025 test set demonstrate substantial improvements over the best-performing baseline in Valence–Arousal prediction accuracy and model stability. Importantly, the workflow is domain-agnostic and provides a generalizable methodology for systematically transferring models to new, low-resource domains, making it applicable beyond medical text analysis.</abstract>
      <url hash="e36822e4">2025.rocling-main.42</url>
      <bibkey>jian-etal-2025-cyut</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>NTULAW</fixed-case> at <fixed-case>ROCLING</fixed-case>-2025 Shared Task: Domain-Adaptive Modeling of Implicit Emotions in Medical Reflections</title>
      <author><first>Sieh-Chuen</first><last>Huang</last></author>
      <author><first>Hsuan-Lei</first><last>Shao</last></author>
      <pages>390-398</pages>
      <abstract>This paper describes the NTULAW team’s participation in the ROCLING 2025 Dimensional Sentiment Analysis (DSA) shared task, which focuses on predicting valence and arousal ratings for Chinese doctors’ self-reflection texts. Unlike previous editions of the DSA task that targeted words, phrases, or educational comments, this year’s dataset consists of domain-specific multi-sentence medical narratives, posing challenges such as low-arousal writing styles, implicit emotion expressions, and discourse complexity. To address the domain shift between general affective resources (Chinese EmoBank) and medical reflections, we designed a multi-scale BERT-based architecture and explored different data selection strategies. Our final system adopted a hybrid submission: using a model trained solely on doctors’ annotations for arousal prediction, and a combined model with Chinese EmoBank for valence prediction. The system achieved stable performance, ranking third among six participating teams. Error analysis shows systematic overestimation of implicit or negated expressions for valence and regression toward mid-range predictions for arousal. We conclude with limitations of relying only on BERT and outline future work involving domain adaptation, discourse-aware modeling, and large language models (LLMs).</abstract>
      <url hash="f91b66e7">2025.rocling-main.43</url>
      <bibkey>huang-shao-2025-ntulaw</bibkey>
    </paper>
    <paper id="44">
      <title><fixed-case>TCU</fixed-case> at <fixed-case>ROCLING</fixed-case>-2025 Shared Task: Leveraging <fixed-case>LLM</fixed-case> Embeddings and Ensemble Regression for <fixed-case>C</fixed-case>hinese Dimensional Sentiment Analysis</title>
      <author><first>Hsin-Chieh</first><last>Li</last></author>
      <author><first>Wen-Cheng</first><last>Lin</last></author>
      <pages>399-406</pages>
      <abstract>This study participates in the ROCLING-2025 shared task on Chinese dimensional sentiment analysis for medical self-reflection texts. Dimensional Sentiment Analysis (DSA) represents emotions as continuous dimensions, such as valence (positive to negative) and arousal (calm to excited), providing finer-grained representations compared to traditional categorical approaches, which are suitable for applications in mental health monitoring and risk assessment. We use large language models (LLMs) to extract contextual embedding vectors, which are then fed into regression models, such as Support Vector Regression (SVR), for valence-arousal prediction. The training data consists of the Chinese EmoBank dataset (2,954 general-domain samples), the validation data is a Medical Self-Reflection Corpus Dataset (994 samples), and the test data is another Medical Self-Reflection Corpus Dataset (1,541 samples). Experimental results show that the SVR model with DeepSeek embeddings performs best. Multi-model ensemble learning further improves performance to 0.463 valence MAE, 0.759 arousal MAE, 0.805 valence PCC, and 0.608 arousal PCC. This approach shows the potential of multi-model fusion in DSA for biomedical applications, facilitating the development of non-intrusive mental health assessment tools.</abstract>
      <url hash="54c46f52">2025.rocling-main.44</url>
      <bibkey>li-lin-2025-tcu</bibkey>
    </paper>
    <paper id="45">
      <title>Hey Vergil at <fixed-case>ROCLING</fixed-case>-2025 Shared Task: Emotion-Space-Based System for Doctors’ Self-Reflection Sentiment Analysis</title>
      <author><first>Ting-Yi</first><last>Lin</last></author>
      <author><first>Cong-Ying</first><last>Lin</last></author>
      <author><first>Jui-Feng</first><last>Yeh</last></author>
      <pages>407-412</pages>
      <abstract>In the ROCLING 2025 dimensional sentiment analysis task, we present EmoTracer. It is an emotion-space-based system for analyzing doctors’ self-reflection texts. The system uses XLNet, BERT, and LSTM models. It is trained on the SLAKE medical dataset and Chinese datasets, such as Chinese EmoBank and NRC-VAD. This helps the system capture the possible emotional changes of doctors when they write patient-related reflections. EmoTracer converts texts into Valence and Arousal scores. The experiments show about 60% accuracy, a Pearson correlation coefficient (PCC) of 0.9, and a mean absolute error (MAE) of 0.3. These results can help support mental health management. The system also has a simple front-end UI. Users can enter texts and see the analysis results. This demonstrates the full functionality of the EmoTracer system.</abstract>
      <url hash="d70450b3">2025.rocling-main.45</url>
      <bibkey>lin-etal-2025-hey</bibkey>
    </paper>
    <paper id="46">
      <title><fixed-case>KOL</fixed-case>ab at <fixed-case>ROCLING</fixed-case>-2025 Shared Task: Research on Emotional Dimensions in <fixed-case>C</fixed-case>hinese Medical Self-Reflection Texts</title>
      <author><first>Chia-Yu</first><last>Chan</last></author>
      <author><first>Chia-Wen</first><last>Wang</last></author>
      <author><first>Jui-Feng</first><last>Yeh</last></author>
      <pages>413-417</pages>
      <abstract>Currently, most sentiment analysis techniques are primarily applied to general texts such as social media or news reports, and there is still a relative gap in emotion recognition within the medical field. Self reflection involves communication between individuals and their inner selves, which has a positive impact on people’s future lives. This article aims to design a classification model for reflective texts aimed at medical professionals to fill gaps in sentiment analysis within the medical field. This task used a BERT model, trained on a dataset from the Chinese EmoBank, and evaluated using the test set provided by the ROCLING 2025 Dimensional Sentiment Analysis – Shared Task. The assessment results show that Valence and Arousal’s PCC scores are 0.76 and 0.58 respectively, while the MAE scores are 0.53 and 0.82, respectively.</abstract>
      <url hash="ba0d5985">2025.rocling-main.46</url>
      <bibkey>chan-etal-2025-kolab</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>SCUNLP</fixed-case> at <fixed-case>ROCLING</fixed-case>-2025 Shared Task: Systematic Guideline Refinement for Continuous Value Prediction with Outlier-Driven <fixed-case>LLM</fixed-case> Feedback</title>
      <author><first>Hong Rui</first><last>Pan</last></author>
      <author><first>Jheng Long</first><last>Wu</last></author>
      <pages>418-426</pages>
      <abstract>Regression-based prediction is widely applied to continuous outputs, such as emotion dimension estimation. However, traditional methods struggle to handle unclear annotation standards and ambiguous cases. To address this challenge, we propose a dual-layer agent-executor framework, where the agent is responsible for constructing and refining guidelines, while the executor applies these guidelines to annotate large-scale data. Notably, we introduce a novel refinement mechanism that can detect outlier instances and provide feedback to the agent for guideline revision, thereby achieving iterative improvement. We applied this method to the ROCLING 2025 shared task for predicting valence-arousal (VA) values in medical self-reflection texts. Compared to the unmodified version, the outlier-driven configuration effectively reduced MAE for both V/A, with A-MAE significantly decreased by 7.7%. The final valence-MAE was 0.51 and arousal-MAE was 0.87, ranking fourth.</abstract>
      <url hash="dd76a9de">2025.rocling-main.47</url>
      <bibkey>pan-wu-2025-scunlp</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>T</fixed-case>aiwanese <fixed-case>H</fixed-case>akka Across <fixed-case>T</fixed-case>aiwan Corpus and <fixed-case>F</fixed-case>ormosa Speech Recognition Challenge 2025 – Dapu &amp; Zhao’an Accents</title>
      <author><first>Yuan-Fu</first><last>Liao</last></author>
      <author><first>Chih-Chung</first><last>Kuo</last></author>
      <author><first>Chao-Shih</first><last>Huang</last></author>
      <author><first>Yu-Siang</first><last>Lan</last></author>
      <author><first>Han-Chun</first><last>Lai</last></author>
      <author><first>Wen-Han</first><last>Hsu</last></author>
      <pages>427-434</pages>
      <abstract>To revive the endangered Hakka language in Taiwan, the first large-scale Hakka speech corpus covering all aspects of Taiwanese Hakka across Taiwan (HAT) was created. This paper introduces the second part of the HAT corpus: the Dapu and Zhao’an accents. Furthermore, to promote this newly constructed corpus and evaluate the performance of the most advanced Hakka ASR system, the 2025 Formosa Speech Recognition Challenge, FSR-2025–Hakka ASR II, was held. Sixteen teams participated on two tracks: speech-to-Hakka-Hanzi and speech-to Hakka-Pinyin. The best results were: Hanzi character error rate (CER) 7.50%; Pinyin syllable error rate (SER) 14.81%.</abstract>
      <url hash="c618b082">2025.rocling-main.48</url>
      <bibkey>liao-etal-2025-taiwanese</bibkey>
    </paper>
    <paper id="49">
      <title>Speech Recognition for Low-resource Languages: A Comparative Study on <fixed-case>H</fixed-case>akka <fixed-case>H</fixed-case>an Characters and <fixed-case>R</fixed-case>omanization</title>
      <author><first>Yu-Hsiang</first><last>Cheng</last></author>
      <author><first>Yi-Syuan</first><last>Wu</last></author>
      <pages>435-440</pages>
      <abstract>This study focuses on speech recognition for low-resource languages, with Hakka as the case study. Since there is currently a lack of dedicated speech models for Taiwanese Southern Min, Hakka, and indigenous languages, we adopt OpenAI Whisper-Medium as the base model and apply Low-Rank Adaptation (LoRA) for fine-tuning. Two models with different output forms were developed: a Hakka character-based model and a Hakka phonetic-based model. The experimental dataset contains approximately 80 hours of speech, covering the Dapu and Zhao’an dialects, and the models were evaluated using Character Error Rate (CER) and Word Error Rate (WER).</abstract>
      <url hash="b5a5f6e9">2025.rocling-main.49</url>
      <bibkey>cheng-wu-2025-speech</bibkey>
    </paper>
    <paper id="50">
      <title>Applying Whisper Fine-tuning and Branchformer to <fixed-case>H</fixed-case>akka Speech Recognition</title>
      <author><first>Yu-Sheng</first><last>Huang</last></author>
      <author><first>Wei-Cheng</first><last>Hong</last></author>
      <author><first>Xin-Yu</first><last>Chen</last></author>
      <author><first>Szu-Yin</first><last>Lin</last></author>
      <pages>441-445</pages>
      <abstract>This study addresses the FSR 2025 Hakka speech recognition task by comparing two strategies: fine-tuning large pre-trained models and training from scratch. For character (Hanzi) recognition, we fine-tuned five different scales of the Whisper model, with large-v3-turbo achieving a 7.55% CER on the test set. For Pinyin recognition, a Branchformer model was compared against a LoRA fine-tuned Whisper-small, yielding WERs of 4.7% and 6.5% on the test set, respectively. Speed perturbation was the primary method used for data augmentation in our pre-processing pipeline.</abstract>
      <url hash="0988c744">2025.rocling-main.50</url>
      <bibkey>huang-etal-2025-applying</bibkey>
    </paper>
    <paper id="51">
      <title>Improving Low-Resource Speech Recognition with Whisper-<fixed-case>M</fixed-case>o<fixed-case>E</fixed-case> and Synthetic Data Augmentation: A Case Study on <fixed-case>H</fixed-case>akka</title>
      <author><first>Yuan-Chi</first><last>Hsu</last></author>
      <author><first>Liang-Chun</first><last>Fang</last></author>
      <author><first>Hong-Jie</first><last>Dai</last></author>
      <pages>446-449</pages>
      <abstract>The objective of this study is to improve speech recognition performance for low-resource Hakka, a language spoken by a specific ethnic group. Our team conducted experiments by fine-tuning different base versions of Whisper (e.g., the original model and the Mandarin-focused Belle model). We found that fine-tuning on different bases yielded distinct advantages and varying results in Hakka character and phonetic recognition tasks. To further enhance model accuracy, we experimented with replacing the q, k, and v linear layers in the attention blocks of the Whisper encoder with a mixture-of-experts model combined with RoLA. In addition, we augmented the training data with synthesized speech generated with diverse voice styles and varying speaking rates. The results showed a 0.73% reduction in character error rate for Task 1 and a 0.2% reduction in word error rate for Task 2. These findings confirm that both architectural adjustments to the model and the strategic use of limited synthetic speech data in low-resource dialect corpora can effectively improve recognition performance.</abstract>
      <url hash="233c7da3">2025.rocling-main.51</url>
      <bibkey>hsu-etal-2025-improving</bibkey>
    </paper>
    <paper id="52">
      <title>Whisper Finetuning For <fixed-case>H</fixed-case>akka Recognition in Low Resource</title>
      <author><first>Min Han</first><last>Teng</last></author>
      <author><first>Ci Dao</first><last>Chen</last></author>
      <author><first>You Ting</first><last>Lin</last></author>
      <author><first>Bing Jhih</first><last>Huang</last></author>
      <pages>450-453</pages>
      <abstract>We study automatic speech recognition (ASR) for Hakka, a low-resource language with substantial dialectal variation. Focusing on Zhaoan and Dapu, we fine-tune Whisper using Low-Rank Adaptation (LoRA) and apply data augmentation to mitigate data scarcity. Experiments show that LoRA combined with augmentation substantially improves cross-dialect recognition while maintaining parameter efficiency. Our results demonstrate the potential of lightweight adaptation to extend large-scale ASR systems to underrepresented languages, supporting the preservation of Hakka speech and orthography.</abstract>
      <url hash="3c5678a1">2025.rocling-main.52</url>
      <bibkey>teng-etal-2025-whisper</bibkey>
    </paper>
    <paper id="53">
      <title><fixed-case>H</fixed-case>akka Speech Recognition with Whisper and <fixed-case>P</fixed-case>inyin Post-processing for <fixed-case>FSR</fixed-case>-2025</title>
      <author><first>Chia-Hsin</first><last>Lee</last></author>
      <author><first>Yung-Jun</first><last>Chang</last></author>
      <author><first>Jin-Yan</first><last>Wu</last></author>
      <author><first>Kuan-Yu</first><last>Chen</last></author>
      <pages>454-458</pages>
      <abstract>本研究為參加 FSR-2025 客語語音辨識挑戰賽(Hakka ASR II)的技術報告,旨在推進客語自動語音辨識技術的發展。由於客語屬於低資源語言,且存在多種腔調,語音辨識面臨高度挑戰。我們以 Whisperlarge-v2 為骨幹模型,設計兩階段訓練流程:首先利用「Hakka Across Taiwan(HAT)」語料庫進行模型調適,以捕捉客語的一般聲學特徵;其次在賽事方提供的60 小時腔調語料上進行微調,以增強對目標資料的適應性。實驗發現,直接輸出客語漢字可達到良好的字錯率(CER),但由 於腔調差異與拼音規則變化多,拼音任務表現顯著下降。為解決此問題,我們以漢字模型的編碼器初始化拼音模型,並提出結合 RoBERTa 漢字轉拼音、腔調判斷與字典修正的後處理模組,期望可以在比賽中提升辨識的成效。</abstract>
      <url hash="2997283d">2025.rocling-main.53</url>
      <bibkey>lee-etal-2025-hakka</bibkey>
    </paper>
    <paper id="54">
      <title>A Study on a Low-Resource Speech Recognition System for <fixed-case>T</fixed-case>aiwan <fixed-case>H</fixed-case>akka Based on Whisper and <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case></title>
      <author><first>Zheng-Ting</first><last>Liu</last></author>
      <author><first>Heng-You</first><last>Wang</last></author>
      <author><first>Yi-Xiang</first><last>Liao</last></author>
      <author><first>Zhong-Yuan</first><last>Qiu</last></author>
      <author><first>Zhao-Yi</first><last>Huang</last></author>
      <pages>459-466</pages>
      <abstract>This study presents the development of a high-performance automatic speech recognition (ASR) system for Taiwan Hakka, a low-resource language facing challenges in preservation and digitalization. We adopt OpenAI’s Whisper large-v3-taiwanese-hakka as the foundation, leveraging its advanced Transformer encoder–decoder architecture. To achieve parameter efficiency and adaptability to a new language, we employ the Low-Rank Adaptation (LoRA) fine-tuning strategy, targeting key modules including q_proj, k_proj, v_proj, out_proj, fc1, and fc2. Experimental results demonstrate that the fine-tuned model achieves strong performance on the FSR 2025 HAT-Vol2 test set, with an average character error rate (CER) of 7.07% and an average word error rate (WER) of 40.99%. Training analysis further indicates that both validation loss and error rates consistently decreased and converged, confirming that LoRA enables effective knowledge transfer to Hakka ASR without catastrophic forgetting. These findings provide an efficient and practical solution for speech recognition in low-resource languages.</abstract>
      <url hash="2f4ff26b">2025.rocling-main.54</url>
      <bibkey>liu-etal-2025-study</bibkey>
    </paper>
    <paper id="55">
      <title>A Compact <fixed-case>W</fixed-case>hisper+<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Baseline for <fixed-case>T</fixed-case>aiwanese <fixed-case>H</fixed-case>akka <fixed-case>ASR</fixed-case> in <fixed-case>FSR</fixed-case>-2025</title>
      <author><first>Hung-Ting</first><last>Hsieh</last></author>
      <pages>467-470</pages>
      <abstract>We present a compact baseline for the For- mosa Speech Recognition (FSR-2025) Tai- wanese Hakka ASR challenge. Our system fine-tunes Whisper-large-v2 (Track 1) and Whisper-large-v3-turbo (Track 2) (Radford et al., 2022) with LoRA (Hu et al., 2021), under a consistent normalization policy and balanced speaker-based dev splits. On the official warm-up set, we obtain 10.94% CER for Track 1 (Hanzi) and 28.48% SER for Track 2 (Pinyin). We provide simple, reproducible pipelines covering data prepa- ration, training, inference, and evaluation, without using external data or language models.</abstract>
      <url hash="bf1f4856">2025.rocling-main.55</url>
      <bibkey>hsieh-2025-compact</bibkey>
    </paper>
    <paper id="56">
      <title>Optimizing Whisper Parameters and Training Data Processing for <fixed-case>F</fixed-case>ormosa Speech Recognition Challenge 2025 - <fixed-case>H</fixed-case>akka <fixed-case>ASR</fixed-case> <fixed-case>II</fixed-case></title>
      <author><first>Jhen-Hao</first><last>Lee</last></author>
      <author><first>Sheng-Wei</first><last>Kuo</last></author>
      <author><first>An-Che</first><last>Cheng</last></author>
      <author><first>Bing-Hua</first><last>Chen</last></author>
      <author><first>Yi-An</first><last>Liu</last></author>
      <pages>471-475</pages>
      <abstract>This paper presents the development and experimental process of our system for the Formosa Speech Recognition Challenge 2025 (Hakka ASR). The proposed system is built upon the OpenAI Whisper model. We achieved significant performance improvements for the Sixian dialect of Hakka through dataset preprocessing and model fine-tuning. In the warm-up evaluation, our system achieved a Character Error Rate (CER) of 10.51% on the character recognition track and a Syllable Error Rate (SER) of 14.72% on the pinyin recognition track. In the final evaluation, our system achieved a Character Error Rate (CER) of 11.21% on the character recognition track and a Syllable Error Rate (SER) of 15.08% on the pinyin recognition track.</abstract>
      <url hash="a74f017c">2025.rocling-main.56</url>
      <bibkey>lee-etal-2025-optimizing</bibkey>
    </paper>
    <paper id="57">
      <title>The <fixed-case>EZ</fixed-case>-<fixed-case>AI</fixed-case> System for <fixed-case>F</fixed-case>ormosa Speech Recognition Challenge 2025</title>
      <author><first>Yu-Sheng</first><last>Tsao</last></author>
      <author><first>Hung-Yang</first><last>Sung</last></author>
      <author><first>An-Ci</first><last>Peng</last></author>
      <author><first>Jhih-Rong</first><last>Guo</last></author>
      <author><first>Tien-Hong</first><last>Lo</last></author>
      <pages>476-480</pages>
      <abstract>This study presents our system for Hakka Speech Recognition Challenge 2025. We designed and compared different systems for two low-resource dialects: Dapu and Zhaoan. On the Pinyin track, we gain boosts by leveraging cross-lingual transfer-learning from related languages and combining with self-supervised learning (SSL). For the Hanzi track, we employ pretrained Whisper with Low-Rank Adaptation (LoRA) fine-tuning. To alleviate the low-resource issue, two data augmentation methods are experimented with: simulating conversational speech to handle multi-speaker scenarios, and generating additional corpus via text-to-speech (TTS). Results from the pilot test showed that transfer learning significantly improved performance in the Pinyin track, achieving an average character error rate (CER) of 19.57%, ranking third among all teams. While in the Hanzi track, the Whisper + LoRA system achieved an average CER of 6.84%, earning first place among all. This study demonstrates that transfer learning and data augmentation can effectively improve recognition performance for low-resource languages. However, the domain mismatch seen in the media test set remains a challenge. We plan to explore in-context learning (ICL) and hotword modeling in the future to better address this issue.</abstract>
      <url hash="dd365b68">2025.rocling-main.57</url>
      <bibkey>tsao-etal-2025-ez</bibkey>
    </paper>
    <paper id="58">
      <title>A Multi-Module Error Detection and Correction System for <fixed-case>H</fixed-case>akka <fixed-case>ASR</fixed-case></title>
      <author><first>Min-Chun</first><last>Hu</last></author>
      <author><first>Yu-Lin</first><last>Xiao</last></author>
      <author><first>Wen-Hsiang</first><last>Lu</last></author>
      <pages>481-488</pages>
      <abstract>本研究提出一個針對客語(以大埔/詔安腔為主)的自動語音辨識(ASR)後矯正系統,旨在解決低資源語言辨識錯誤率偏高的問題。客語因受限於語料規模、異體字與腔調差異,在既有的通用 ASR 模型上表現往往不佳。為此,我們首先以 Whisper Large v3 Turbo 為基底辨識模型,使用約 60 小時的大埔與詔安語料進行微調,以提升對特定腔調的適應性。在獲取 ASR N-best 候選句後,系統進一步透過多模組錯誤偵測矯正流程進行修正,包含四個主要步驟: (1) 潛在錯誤偵測,用於鎖定候選間錯誤的候選詞彙;(2) 音素混淆集偵測(Phoneme Confusion Set): 依據音素相近關係提供可能替代詞;(3) 辭典(Lexicon)修正: 確保詞彙存在於語言使用的實際範疇中,(4) 搭配詞關聯度偵測: 利用收集之語料所建立的搭配詞關聯度來偵測錯誤詞彙。本研究所提出的矯正機制能有效補足 ASR 在低資源語言中的不足,實驗顯示經過多階段錯誤偵測矯正後,最終CER減少至 15.49%,減少 2.14 % ,證明該方法能有效提升語音辨識的準確率。</abstract>
      <url hash="e6b3f082">2025.rocling-main.58</url>
      <bibkey>hu-etal-2025-multi-module</bibkey>
    </paper>
    <paper id="59">
      <title>A Whisper-Based System with Multi-Faceted Data Augmentation for Low-Resource Language</title>
      <author><first>Pin-Cheng</first><last>Chen</last></author>
      <author><first>Yu-Chi</first><last>Chen</last></author>
      <author><first>Chia-Chun</first><last>Liang</last></author>
      <author><first>Cheng-Yu</first><last>Lin</last></author>
      <author><first>Ping-Juei</first><last>Tsai</last></author>
      <author><first>Wei-Yun</first><last>Ma</last></author>
      <pages>489-498</pages>
      <abstract>This paper presents a comprehensive approach for the Formosa Speech Recognition Challenge 2025 (FSR-2025), targeting automatic speech recognition (ASR) for the under-resourced Dapu and Zhao’an dialects of Taiwanese Hakka. Our method integrates data augmentation and robustness techniques, including SpecAugment, dialect-aware special tokens, text-to-speech (TTS) augmentation, noise/reverberation mixing, and speed perturbation, to mitigate data scarcity and domain mismatch. Experiments on the official FSR-2025 datasets show consistent improvements in both character error rate (CER) and word error rate (WER). Extensive ablation studies further confirm that each component contributes positively. These results offer a practical path toward robust ASR for under-resourced Hakka dialects and suggest broader applicability to other low-resource languages.</abstract>
      <url hash="de667f4e">2025.rocling-main.59</url>
      <bibkey>chen-etal-2025-whisper</bibkey>
    </paper>
    <paper id="60">
      <title>A Channel-Aware Anomaly-Guided Data Augmentation Framework for the <fixed-case>FSR</fixed-case>-2025 <fixed-case>H</fixed-case>akka Speech Recognition Challenge</title>
      <author><first>Siang-Ting</first><last>Lin</last></author>
      <author><first>Arthur</first><last>Hao</last></author>
      <author><first>Chiun-Yu</first><last>Hua</last></author>
      <author><first>Kuan-Tang</first><last>Huang</last></author>
      <author><first>Berlin</first><last>Chen</last></author>
      <pages>499-503</pages>
      <abstract>The Formosa Speech Recognition Challenge 2025 (FSR-2025) focuses on Taiwanese Hakka, a low-resource language with limited data diversity and channel coverage. To address this challenge, we propose a channel-aware, data-centric framework that leverages multilingual foundation models to mitigate mismatches between field recordings and training data. Our method integrates unsupervised anomaly detection and channel-conditioned augmentation to enhance data representativeness before ASR fine-tuning, aiming to explore the potential for improving robustness in low-resource Hakka speech recognition.</abstract>
      <url hash="173d2b7f">2025.rocling-main.60</url>
      <bibkey>lin-etal-2025-channel</bibkey>
    </paper>
    <paper id="61">
      <title>The <fixed-case>AS</fixed-case>-<fixed-case>SLAM</fixed-case> system for <fixed-case>F</fixed-case>ormosa Speech Recognition Challenge 2025</title>
      <author><first>Chih-Hsi</first><last>Chen</last></author>
      <author><first>Pei-Jun</first><last>Liao</last></author>
      <author><first>Chia-Hua</first><last>Wu</last></author>
      <author><first>Pang-Cheng</first><last>Wu</last></author>
      <author><first>Hsin-Min</first><last>Wang</last></author>
      <pages>504-511</pages>
      <abstract>In recent years, large-scale pre-trained speech models such as Whisper have been widely applied to speech recognition. While they achieve strong performance on high-resource languages such as English and Mandarin, dialects and other low-resource languages remain challenging due to limited data availability. The government-led “Formosa Speech in the Wild (FSW) project” is an important cultural preservation initiative for Hakka, a regional dialect, where the development of Hakka ASR systems represents a key technological milestone. Beyond model architecture, data processing and training strategies are also critical. In this paper, we explore data augmentation techniques for Hakka speech, including TTS and MUSAN-based approaches, and analyze different data combinations by fine-tuning the pre-trained Whisper model. We participated in the 2025 Hakka FSR ASR competition (student track) for the Dapu and Zhaoan varieties. In the pilot test, our system achieved 7th place in Hanzi recognition (CER: 15.92) and 3rd place in Pinyin recognition (SER: 20.49). In the official finals, our system ranked 6 in Hanzi recognition (CER: 15.73) and 4 in Pinyin recognition (SER: 20.68). We believe that such data augmentation strategies can advance research on Hakka ASR and support the long-term preservation of Hakka culture.</abstract>
      <url hash="cbfc2c0c">2025.rocling-main.61</url>
      <bibkey>chen-etal-2025-slam-system</bibkey>
    </paper>
    <paper id="62">
      <title>Challenges and Limitations of the Multilingual Pre-trained Model Whisper on Low-Resource Languages: A Case Study of <fixed-case>H</fixed-case>akka Speech Recognition</title>
      <author><first>Pei-Chi</first><last>Lan</last></author>
      <author><first>Hsin-Tien</first><last>Chiang</last></author>
      <author><first>Ting-Chun</first><last>Lin</last></author>
      <author><first>Ming-Hsiang</first><last>Su</last></author>
      <pages>512-517</pages>
      <abstract>This study investigates the practical performance and limitations of the multilingual pre-trained model Whisper in low-resource language settings, using a Hakka speech recognition challenge as a case study. In the preliminary phase, our team (Group G) achieved official scores of 75.58% in Character Error Rate (CER) and 100.97% in Syllable Error Rate (SER). However, in the final phase, both CER and Word Error Rate (WER) reached 100%. Through a retrospective analysis of system design and implementation, we identified three major sources of failure: (1) improper handling of long utterances, where only the first segment was decoded, causing content truncation; (2) inconsistent language prompting, fixed to “Chinese” instead of the Hakka target; and (3) lack of systematic verification in data alignment and submission generation, combined with inadequate evaluation setup.Based on these findings, we propose a set of practical guidelines covering long-utterance processing, language consistency checking, and data submission validation. The results highlight that in low-resource speech recognition tasks, poor data quality or flawed workflow design can cause severe degradation of model performance. This study underscores the importance of robust data and process management in ASR system development and provides concrete insights for future improvements and reproducibility.</abstract>
      <url hash="a09b8839">2025.rocling-main.62</url>
      <bibkey>lan-etal-2025-challenges</bibkey>
    </paper>
    <paper id="63">
      <title>The <fixed-case>NPTU</fixed-case> <fixed-case>ASR</fixed-case> System for <fixed-case>FSR</fixed-case>2025 <fixed-case>H</fixed-case>akka Character/<fixed-case>P</fixed-case>inyin Recognition: Whisper with m<fixed-case>BART</fixed-case> Post-Editing and <fixed-case>RNNLM</fixed-case> Rescoring</title>
      <author><first>Yi-Chin</first><last>Huang</last></author>
      <author><first>Yu-Heng</first><last>Chen</last></author>
      <author><first>Jian-Hua</first><last>Wang</last></author>
      <author><first>Hsiu-Chi</first><last>Wu</last></author>
      <author><first>Chih-Chung</first><last>Kuo</last></author>
      <author><first>Chao-Shih</first><last>Huang</last></author>
      <author><first>Yuan-Fu</first><last>Liao</last></author>
      <pages>518-522</pages>
      <abstract>This paper presents our system for the FSR-2025 Hakka Automatic Speech Recognition (ASR) Challenge, which consists of two sub-tasks: (i) Hakka Characters and (ii) Hakka Pinyin. We propose a unified architecture built upon Whisper [1], a large weakly supervised ASR model, as the acoustic backbone, with optional LoRA (Low-Rank Adaptation [2]) for parameter-efficient fine-tuning. Data augmentation techniques include the MUSAN [3] corpus (music/speech/noise) and tempo/speed perturbation [4]. For the character task, mBART-50 [5,6], a multilingual sequence-to-sequence model, is applied for text correction, while both tasks employ an RNNLM [7] for N-best rescoring. Under the final evaluation setting of the character task, mBART-driven 10-best text correction combined with RNNLM rescoring achieved a CER (Character Error Rate) of 6.26%, whereas the official leaderboard reported 22.5%. For the Pinyin task, the Medium model proved more suitable than the Large model given the dataset size and accent distribution. With 10-best RNNLM rescoring, it achieved a SER (Syllable Error Rate) of 4.65% on our internal warm-up test set, and the official final score (with tone information) was 14.81%. Additionally, we analyze the contribution of LID (Language Identification) for accent recognition across different recording and media sources.</abstract>
      <url hash="d0003af6">2025.rocling-main.63</url>
      <bibkey>huang-etal-2025-nptu</bibkey>
    </paper>
  </volume>
</collection>
