<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.nlp4dh">
  <volume id="1" ingest-date="2022-11-21">
    <meta>
      <booktitle>Proceedings of the 2nd International Workshop on Natural Language Processing for Digital Humanities</booktitle>
      <editor><first>Mika</first><last>Hämäläinen</last></editor>
      <editor><first>Khalid</first><last>Alnajjar</last></editor>
      <editor><first>Niko</first><last>Partanen</last></editor>
      <editor><first>Jack</first><last>Rueter</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Taipei, Taiwan</address>
      <month>November</month>
      <year>2022</year>
      <url hash="fa941558">2022.nlp4dh-1</url>
      <venue>nlp4dh</venue>
    </meta>
    <frontmatter>
      <url hash="441f7328">2022.nlp4dh-1.0</url>
      <bibkey>nlp4dh-2022-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Stylometric Analysis of Amadís de Gaula and Sergas de Esplandián</title>
      <author><first>Yoshifumi</first><last>Kawasaki</last></author>
      <pages>1–7</pages>
      <abstract>Amadís de Gaula (AG) and its sequel Sergas de Esplandián (SE) are masterpieces of medieval Spanish chivalric romances. Much debate has been devoted to the role played by their purported author Garci Rodríguez de Montalvo. According to the prologue of AG, which consists of four books, the author allegedly revised the first three books that were in circulation at that time and added the fourth book and SE. However, the extent to which Montalvo edited the materials at hand to compose the extant works has yet to be explored extensively. To address this question, we applied stylometric techniques for the first time. Specifically, we investigated the stylistic differences (if any) between the first three books of AG and his own extensions. Literary style is represented as usage of parts-of-speech n-grams. We performed principal component analysis and k-means to demonstrate that Montalvo’s retouching on the first book was minimal, while revising the second and third books in such a way that they came to moderately resemble his authentic creation, that is, the fourth book and SE. Our findings empirically corroborate suppositions formulated from philological viewpoints.</abstract>
      <url hash="1671cf5d">2022.nlp4dh-1.1</url>
      <bibkey>kawasaki-2022-stylometric</bibkey>
    </paper>
    <paper id="2">
      <title>Computational Exploration of the Origin of Mood in Literary Texts</title>
      <author><first>Emily</first><last>Öhman</last></author>
      <author><first>Riikka H.</first><last>Rossi</last></author>
      <pages>8–14</pages>
      <abstract>This paper is a methodological exploration of the origin of mood in early modern and modern Finnish literary texts using computational methods. We discuss the pre-processing steps as well as the various natural language processing tools used to try to pinpoint where mood can be best detected in text. We also share several tools and resources developed during this process. Our early attempts suggest that overall mood can be computationally detected in the first three paragraphs of a book.</abstract>
      <url hash="4a825783">2022.nlp4dh-1.2</url>
      <bibkey>ohman-rossi-2022-computational</bibkey>
    </paper>
    <paper id="3">
      <title>Sentiment is all you need to win <fixed-case>US</fixed-case> Presidential elections</title>
      <author><first>Sovesh</first><last>Mohapatra</last></author>
      <author><first>Somesh</first><last>Mohapatra</last></author>
      <pages>15–20</pages>
      <abstract>Election speeches play an integral role in communicating the vision and mission of the candidates. From lofty promises to mud-slinging, the electoral candidate accounts for all. However, there remains an open question about what exactly wins over the voters. In this work, we used state-of-the-art natural language processing methods to study the speeches and sentiments of the Republican candidates and Democratic candidates fighting for the 2020 US Presidential election. Comparing the racial dichotomy of the United States, we analyze what led to the victory and defeat of the different candidates. We believe this work will inform the election campaigning strategy and provide a basis for communicating to diverse crowds.</abstract>
      <url hash="b1f3d5ae">2022.nlp4dh-1.3</url>
      <bibkey>mohapatra-mohapatra-2022-sentiment</bibkey>
    </paper>
    <paper id="4">
      <title>Interactive Analysis and Visualisation of Annotated Collocations in <fixed-case>S</fixed-case>panish (<fixed-case>AVA</fixed-case>n<fixed-case>CES</fixed-case>)</title>
      <author><first>Simon</first><last>Gonzalez</last></author>
      <pages>21–30</pages>
      <abstract>Phraseology studies have been enhanced by Corpus Linguistics, which has become an interdisciplinary field where current technologies play an important role in its development. Computational tools have been implemented in the last decades with positive results on the identification of phrases in different languages. One specific technology that has impacted these studies is social media. As researchers, we have turned our attention to collecting data from these platforms, which comes with great advantages and its own challenges. One of the challenges is the way we design and build corpora relevant to the questions emerging in this type of language expression. This has been approached from different angles, but one that has given invaluable outputs is the building of linguistic corpora with the use of online web applications. In this paper, we take a multidimensional approach to the collection, design, and deployment of a phraseology corpus for Latin American Spanish from Twitter data, extracting features using NLP techniques, and presenting it in an interactive online web application. We expect to contribute to the methodologies used for Corpus Linguistics in the current technological age. Finally, we make this tool publicly available to be used by any researcher interested in the data itself and also on the technological tools developed here.</abstract>
      <url hash="2442b036">2022.nlp4dh-1.4</url>
      <bibkey>gonzalez-2022-interactive</bibkey>
    </paper>
    <paper id="5">
      <title>Fractality of sentiment arcs for literary quality assessment: The case of Nobel laureates</title>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <author><first>Kristoffer Laigaard</first><last>Nielbo</last></author>
      <author><first>Mads Rosendahl</first><last>Thomsen</last></author>
      <pages>31–41</pages>
      <abstract>In the few works that have used NLP to study literary quality, sentiment and emotion analysis have often been considered valuable sources of information. At the same time, the idea that the nature and polarity of the sentiments expressed by a novel might have something to do with its perceived quality seems limited at best. In this paper, we argue that the fractality of narratives, specifically the long-term memory of their sentiment arcs, rather than their simple shape or average valence, might play an important role in the perception of literary quality by a human audience. In particular, we argue that such measure can help distinguish Nobel-winning writers from control groups in a recent corpus of English language novels. To test this hypothesis, we present the results from two studies: (i) a probability distribution test, where we compute the probability of seeing a title from a Nobel laureate at different levels of arc fractality; (ii) a classification test, where we use several machine learning algorithms to measure the predictive power of both sentiment arcs and their fractality measure. Our findings seem to indicate that despite the competitive and complex nature of the task, the populations of Nobel and non-Nobel laureates seem to behave differently and can to some extent be told apart by a classifier.</abstract>
      <url hash="27cd3193">2022.nlp4dh-1.5</url>
      <bibkey>bizzoni-etal-2022-fractality</bibkey>
    </paper>
    <paper id="6">
      <title>Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material</title>
      <author><first>Solomon</first><last>Tannor</last></author>
      <author><first>Nachum</first><last>Dershowitz</last></author>
      <author><first>Moshe</first><last>Lavee</last></author>
      <pages>42–46</pages>
      <abstract>Midrash collections are complex rabbinic works that consist of text in multiple languages, that evolved through long processes of instable oral and written transmission. Determining the origin of a given passage in such a compilation is not always straightforward and is often a matter disputed by scholars, yet it is essential for scholars’ understanding of the passage and its relationship to other texts in the rabbinic corpus. To help solve this problem, we propose a system for classification of rabbinic literature based on its style, leveraging recently released pretrained Transformer models for Hebrew. Additionally, we demonstrate how our method can be applied to uncover lost material from the Midrash Tanhuma.</abstract>
      <url hash="f970d750">2022.nlp4dh-1.6</url>
      <bibkey>tannor-etal-2022-style</bibkey>
    </paper>
    <paper id="7">
      <title>Use the Metadata, Luke! – An Experimental Joint Metadata Search and N-gram Trend Viewer for Personal Web Archives</title>
      <author><first>Balázs</first><last>Indig</last></author>
      <author><first>Zsófia</first><last>Sárközi-Lindner</last></author>
      <author><first>Mihály</first><last>Nagy</last></author>
      <pages>47–52</pages>
      <abstract>Many digital humanists (philologists, historians, sociologists, librarians, the audience for web archives) design their research around metadata (publication date ranges, sources, authors, etc.). However, current major web archives are limited to technical metadata while lacking high quality, descriptive metadata allowing for faceted queries. As researchers often lack the technical skill necessary to enrich existing web archives with descriptive metadata, they increasingly turn to creating personal web archives that contain such metadata, tailored to their research requirements. Software that enable creating such archives without advanced technical skills have gained popularity, however, tools for examination and querying are currently the missing link. We showcase a solution designed to fill this gap.</abstract>
      <url hash="d07f3a9e">2022.nlp4dh-1.7</url>
      <bibkey>indig-etal-2022-use</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>MALM</fixed-case>: Mixing Augmented Language Modeling for Zero-Shot Machine Translation</title>
      <author><first>Kshitij</first><last>Gupta</last></author>
      <pages>53–58</pages>
      <abstract>Large pre-trained language models have brought remarkable progress in NLP. Pre-training and Fine-tuning have given state-of-art performance across tasks in text processing. Data Augmentation techniques have also helped build state-of-art models on low or zero resource tasks. Many works in the past have attempted at learning a single massively multilingual machine translation model for zero-shot translation. Although those translation models are producing correct translations, the main challenge is those models are producing the wrong languages for zero-shot translation. This work and its results indicate that prompt conditioned large models do not suffer from off-target language errors i.e. errors arising due to translation to wrong languages. We empirically demonstrate the effectiveness of self-supervised pre-training and data augmentation for zero-shot multi-lingual machine translation.</abstract>
      <url hash="a77aa2fa">2022.nlp4dh-1.8</url>
      <bibkey>gupta-2022-malm</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>P</fixed-case>ars<fixed-case>S</fixed-case>imple<fixed-case>QA</fixed-case>: The <fixed-case>P</fixed-case>ersian Simple Question Answering Dataset and System over Knowledge Graph</title>
      <author><first>Hamed</first><last>Babaei Giglou</last></author>
      <author><first>Niloufar</first><last>Beyranvand</last></author>
      <author><first>Reza</first><last>Moradi</last></author>
      <author><first>Amir Mohammad</first><last>Salehoof</last></author>
      <author><first>Saeed</first><last>Bibak</last></author>
      <pages>59–68</pages>
      <abstract>The simple question answering over the knowledge graph concerns answering single-relation questions by querying the facts in the knowledge graph. This task has drawn significant attention in recent years. However, there is a demand for a simple question dataset in the Persian language to study open-domain simple question answering. In this paper, we present the first Persian single-relation question answering dataset and a model that uses a knowledge graph as a source of knowledge to answer questions. We create the ParsSimpleQA dataset semi-automatically in two steps. First, we build single-relation question templates. Next, we automatically create simple questions and answers using templates, entities, and relations from Farsbase. To present the reliability of the presented dataset, we proposed a simple question-answering system that receives questions and uses deep learning and information retrieval techniques for answering questions. The experimental results presented in this paper show that the ParsSimpleQA dataset is very promising for the Persian simple question-answering task.</abstract>
      <url hash="8c08d748">2022.nlp4dh-1.9</url>
      <attachment type="Dataset" hash="3f1a5d8c">2022.nlp4dh-1.9.Dataset.zip</attachment>
      <bibkey>babaei-giglou-etal-2022-parssimpleqa</bibkey>
    </paper>
    <paper id="10">
      <title>Enhancing Digital History – Event discovery via Topic Modeling and Change Detection</title>
      <author><first>King Ip</first><last>Lin</last></author>
      <author><first>Sabrina</first><last>Peng</last></author>
      <pages>69–78</pages>
      <abstract>Digital history is the application of computer science techniques to historical data in order to uncover insights into events occurring during specific time periods from the past. This relatively new interdisciplinary field can help identify and record latent information about political, cultural, and economic trends that are not otherwise apparent from traditional historical analysis. This paper presents a method that uses topic modeling and breakpoint detection to observe how extracted topics come in and out of prominence over various time periods. We apply our techniques on British parliamentary speech data from the 19th century. Findings show that some of the events produced are cohesive in topic content (religion, transportation, economics, etc.) and time period (events are focused in the same year or month). Topic content identified should be further analyzed for specific events and undergo external validation to determine the quality and value of the findings to historians specializing in 19th century Britain.</abstract>
      <url hash="365b21f1">2022.nlp4dh-1.10</url>
      <bibkey>lin-peng-2022-enhancing</bibkey>
    </paper>
    <paper id="11">
      <title>A Parallel Corpus and Dictionary for <fixed-case>A</fixed-case>mis-<fixed-case>M</fixed-case>andarin Translation</title>
      <author><first>Francis</first><last>Zheng</last></author>
      <author><first>Edison</first><last>Marrese-Taylor</last></author>
      <author><first>Yutaka</first><last>Matsuo</last></author>
      <pages>79–84</pages>
      <abstract>Amis is an endangered language indigenous to Taiwan with limited data available for computational processing. We thus present an Amis-Mandarin dataset containing a parallel corpus of 5,751 Amis and Mandarin sentences and a dictionary of 7,800 Amis words and phrases with their definitions in Mandarin. Using our dataset, we also established a baseline for machine translation between Amis and Mandarin in both directions. Our dataset can be found at https://github.com/francisdzheng/amis-mandarin.</abstract>
      <url hash="a92f2c0d">2022.nlp4dh-1.11</url>
      <bibkey>zheng-etal-2022-parallel</bibkey>
    </paper>
    <paper id="12">
      <title>Machines in the media: semantic change in the lexicon of mechanization in 19th-century <fixed-case>B</fixed-case>ritish newspapers</title>
      <author><first>Nilo</first><last>Pedrazzini</last></author>
      <author><first>Barbara</first><last>McGillivray</last></author>
      <pages>85–95</pages>
      <abstract>The industrialization process associated with the so-called Industrial Revolution in 19th-century Great Britain was a time of profound changes, including in the English lexicon. An important yet understudied phenomenon is the semantic shift in the lexicon of mechanisation. In this paper we present the first large-scale analysis of terms related to mechanization over the course of the 19th-century in English. We draw on a corpus of historical British newspapers comprising 4.6 billion tokens and train historical word embedding models. We test existing semantic change detection techniques and analyse the results in light of previous historical linguistic scholarship.</abstract>
      <url hash="8254b613">2022.nlp4dh-1.12</url>
      <bibkey>pedrazzini-mcgillivray-2022-machines</bibkey>
    </paper>
    <paper id="13">
      <title>Optimizing the weighted sequence alignment algorithm for large-scale text similarity computation</title>
      <author><first>Maciej</first><last>Janicki</last></author>
      <pages>96–100</pages>
      <abstract>We present an optimized implementation of the weighted sequence alignment algorithm (a.k.a. weighted edit distance) in a scenario where the items to align are numeric vectors and the substitution weights are determined by their cosine similarity. The optimization relies on using vector and matrix operations provided by numeric computation libraries (including GPU acceleration) instead of loops. The resulting algorithm provides an efficient way of aligning large sets of texts represented as sequences of continuous-space numeric vectors (embeddings). The optimization made it possible to compute alignment-based similarity for all pairs of texts in a large corpus of Finnic oral folk poetry for the purpose of studying intertextuality in the oral tradition.</abstract>
      <url hash="cc64a6a8">2022.nlp4dh-1.13</url>
      <bibkey>janicki-2022-optimizing</bibkey>
    </paper>
    <paper id="14">
      <title>Domain-specific Evaluation of Word Embeddings for Philosophical Text using Direct Intrinsic Evaluation</title>
      <author><first>Goya</first><last>van Boven</last></author>
      <author><first>Jelke</first><last>Bloem</last></author>
      <pages>101–107</pages>
      <abstract>We perform a direct intrinsic evaluation of word embeddings trained on the works of a single philosopher. Six models are compared to human judgements elicited using two tasks: a synonym detection task and a coherence task. We apply a method that elicits judgements based on explicit knowledge from experts, as the linguistic intuition of non-expert participants might differ from that of the philosopher. We find that an in-domain SVD model has the best 1-nearest neighbours for target terms, while transfer learning-based Nonce2Vec performs better for low frequency target terms.</abstract>
      <url hash="246f0524">2022.nlp4dh-1.14</url>
      <bibkey>van-boven-bloem-2022-domain</bibkey>
    </paper>
    <paper id="15">
      <title>Towards Bootstrapping a Chatbot on Industrial Heritage through Term and Relation Extraction</title>
      <author><first>Mihael</first><last>Arcan</last></author>
      <author><first>Rory</first><last>O’Halloran</last></author>
      <author><first>Cécile</first><last>Robin</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <pages>108–122</pages>
      <abstract>We describe initial work in developing a methodology for the automatic generation of a conversational agent or ‘chatbot’ through term and relation extraction from a relevant corpus of language data. We develop our approach in the domain of industrial heritage in the 18th and 19th centuries, and more specifically on the industrial history of canals and mills in Ireland. We collected a corpus of relevant newspaper reports and Wikipedia articles, which we deemed representative of a layman’s understanding of this topic. We used the Saffron toolkit to extract relevant terms and relations between the terms from the corpus and leveraged the extracted knowledge to query the British Library Digital Collection and the Project Gutenberg library. We leveraged the extracted terms and relations in identifying possible answers for a constructed set of questions based on the extracted terms, by matching them with sentences in the British Library Digital Collection and the Project Gutenberg library. In a final step, we then took this data set of question-answer pairs to train a chatbot. We evaluate our approach by manually assessing the appropriateness of the generated answers for a random sample, each of which is judged by four annotators.</abstract>
      <url hash="e5cd0b41">2022.nlp4dh-1.15</url>
      <bibkey>arcan-etal-2022-towards</bibkey>
    </paper>
    <paper id="16">
      <title>Non-Parametric Word Sense Disambiguation for Historical Languages</title>
      <author><first>Enrique</first><last>Manjavacas Arevalo</last></author>
      <author><first>Lauren</first><last>Fonteyn</last></author>
      <pages>123–134</pages>
      <abstract>Recent approaches to Word Sense Disambiguation (WSD) have profited from the enhanced contextualized word representations coming from contemporary Large Language Models (LLMs). This advancement is accompanied by a renewed interest in WSD applications in Humanities research, where the lack of suitable, specific WSD-annotated resources is a hurdle in developing ad-hoc WSD systems. Because they can exploit sentential context, LLMs are particularly suited for disambiguation tasks. Still, the application of LLMs is often limited to linear classifiers trained on top of the LLM architecture. In this paper, we follow recent developments in non-parametric learning and show how LLMs can be efficiently fine-tuned to achieve strong few-shot performance on WSD for historical languages (English and Dutch, date range: 1450-1950). We test our hypothesis using (i) a large, general evaluation set taken from large lexical databases, and (ii) a small real-world scenario involving an ad-hoc WSD task. Moreover, this paper marks the release of GysBERT, a LLM for historical Dutch.</abstract>
      <url hash="c269c0f7">2022.nlp4dh-1.16</url>
      <bibkey>manjavacas-arevalo-fonteyn-2022-non</bibkey>
    </paper>
    <paper id="17">
      <title>Introducing a Large Corpus of Tokenized Classical <fixed-case>C</fixed-case>hinese Poems of Tang and Song Dynasties</title>
      <author><first>Chao-Lin</first><last>Liu</last></author>
      <author><first>Ti-Yong</first><last>Zheng</last></author>
      <author><first>Kuan-Chun</first><last>Chen</last></author>
      <author><first>Meng-Han</first><last>Chung</last></author>
      <pages>135–144</pages>
      <abstract>Classical Chinese poems of Tang and Song dynasties are an important part for the studies of Chinese literature. To thoroughly understand the poems, properly segmenting the verses is an important step for human readers and software agents. Yet, due to the availability of data and the costs of annotation, there are still no known large and useful sources that offer classical Chinese poems with annotated word boundaries. In this project, annotators with Chinese literature background labeled 32399 poems. We analyzed the annotated patterns and conducted inter-rater agreement studies about the annotations. The distributions of the annotated patterns for poem lines are very close to some well-known professional heuristics, i.e., that the 2-2-1, 2-1-2, 2-2-1-2, and 2-2-2-1 patterns are very frequent. The annotators agreed well at the line level, but agreed on the segmentations of a whole poem only 43% of the time. We applied a traditional machine-learning approach to segment the poems, and achieved promising results at the line level as well. Using the annotated data as the ground truth, these methods could segment only about 18% of the poems completely right under favorable conditions. Switching to deep-learning methods helped us achieved better than 30%.</abstract>
      <url hash="1f8b334f">2022.nlp4dh-1.17</url>
      <bibkey>liu-etal-2022-introducing</bibkey>
    </paper>
    <paper id="18">
      <title>Creative Text-to-Image Generation: Suggestions for a Benchmark</title>
      <author><first>Irene</first><last>Russo</last></author>
      <pages>145–154</pages>
      <abstract>Language models for text-to-image generation can output good quality images when referential aspects of pictures are evaluated. The generation of creative images is not under scrutiny at the moment, but it poses interesting challenges: should we expect more creative images using more creative prompts? What is the relationship between prompts and images in the global process of human evaluation? In this paper, we want to highlight several criteria that should be taken into account for building a creative text-to-image generation benchmark, collecting insights from multiple disciplines (e.g., linguistics, cognitive psychology, philosophy, psychology of art).</abstract>
      <url hash="50d6dc4b">2022.nlp4dh-1.18</url>
      <bibkey>russo-2022-creative</bibkey>
    </paper>
    <paper id="19">
      <title>The predictability of literary translation</title>
      <author><first>Andrew</first><last>Piper</last></author>
      <author><first>Matt</first><last>Erlin</last></author>
      <pages>155–160</pages>
      <abstract>Research has shown that the practice of translation exhibits predictable linguistic cues that make translated texts detectable from original-language texts (a phenomenon known as “translationese”). In this paper, we test the extent to which literary translations are subject to the same effects and whether they also exhibit meaningful differences at the level of content. Research into the function of translations within national literary markets using smaller case studies has suggested that translations play a cultural role that is distinct from that of original-language literature, i.e. their differences reside not only at the level of translationese but at the level of content. Using a dataset consisting of original-language fiction in English and translations into English from 120 languages (N=21,302), we find that one of the principal functions of literary translation is to convey predictable geographic identities to local readers that nevertheless extend well beyond the foreignness of persons and places.</abstract>
      <url hash="44781b15">2022.nlp4dh-1.19</url>
      <bibkey>piper-erlin-2022-predictability</bibkey>
    </paper>
    <paper id="20">
      <title>Emotion Conditioned Creative Dialog Generation</title>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <pages>161–166</pages>
      <abstract>We present a DialGPT based model for generating creative dialog responses that are conditioned based on one of the following emotions: anger, disgust, fear, happiness, pain, sadness and surprise. Our model is capable of producing a contextually apt response given an input sentence and a desired emotion label. Our model is capable of expressing the desired emotion with an accuracy of 0.6. The best performing emotions are neutral, fear and disgust. When measuring the strength of the expressed emotion, we find that anger, fear and disgust are expressed in the most strong fashion by the model.</abstract>
      <url hash="d76eef4a">2022.nlp4dh-1.20</url>
      <bibkey>alnajjar-hamalainen-2022-emotion</bibkey>
    </paper>
    <paper id="21">
      <title>Integration of Named Entity Recognition and Sentence Segmentation on <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese based on Siku-<fixed-case>BERT</fixed-case></title>
      <author><first>Sijia</first><last>Ge</last></author>
      <pages>167–173</pages>
      <abstract>Sentence segmentation and named entity recognition are two significant tasks in ancient Chinese processing since punctuation and named entity information are important for further research on ancient classics. These two are sequence labeling tasks in essence so we can tag the labels of these two tasks for each token simultaneously. Our work is to evaluate whether such a unified way would be better than tagging the label of each task separately with a BERT-based model. The paper adopts a BERT-based model that was pre-trained on ancient Chinese text to conduct experiments on Zuozhuan text. The results show there is no difference between these two tagging approaches without concerning the type of entities and punctuation. The ablation experiments show that the punctuation token in the text is useful for NER tasks, and finer tagging sets such as differentiating the tokens that locate at the end of an entity and those are in the middle of an entity could offer a useful feature for NER while impact negatively sentences segmentation with unified tagging.</abstract>
      <url hash="c277913b">2022.nlp4dh-1.21</url>
      <bibkey>ge-2022-integration</bibkey>
    </paper>
    <paper id="22">
      <title>(Re-)Digitizing 吳守禮 Ngôo Siú-lé’s <fixed-case>M</fixed-case>andarin – <fixed-case>T</fixed-case>aiwanese Dictionary</title>
      <author><first>Pierre</first><last>Magistry</last></author>
      <author><first>Afala</first><last>Phaxay</last></author>
      <pages>174–178</pages>
      <abstract>This paper presents the efforts conducted to obtain a usable and open digital version in XML-TEI of one of the major lexicographic work for bilingual Taiwanese dictionaries, namely the 《國臺對照活用辭典》(Practical Mandarin-Taiwanese Dictionary) The original dictionary was published in 2000, after decades of work by Prof. 吳守禮 (Ngôo Siu-le/Wu Shouli)</abstract>
      <url hash="05900ee0">2022.nlp4dh-1.22</url>
      <bibkey>magistry-phaxay-2022-digitizing</bibkey>
    </paper>
  </volume>
</collection>
