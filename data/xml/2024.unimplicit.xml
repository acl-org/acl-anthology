<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.unimplicit">
  <volume id="1" ingest-date="2024-03-04" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Understanding Implicit and Underspecified Language</booktitle>
      <editor><first>Valentina</first><last>Pyatkin</last></editor>
      <editor><first>Daniel</first><last>Fried</last></editor>
      <editor><first>Elias</first><last>Stengel-Eskin</last></editor>
      <editor><first>Elias</first><last>Stengel-Eskin</last></editor>
      <editor><first>Alisa</first><last>Liu</last></editor>
      <editor><first>Sandro</first><last>Pezzelle</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="ef93eddc">2024.unimplicit-1</url>
      <venue>unimplicit</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="e3f29aea">2024.unimplicit-1.0</url>
      <bibkey>unimplicit-ws-2024-understanding</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Taking Action Towards Graceful Interaction: The Effects of Performing Actions on Modelling Policies for Instruction Clarification Requests</title>
      <author><first>Brielen</first><last>Madureira</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>1-21</pages>
      <abstract>Clarification requests are a mechanism to help solve communication problems, e.g. due to ambiguity or underspecification, in instruction-following interactions. Despite their importance, even skilful models struggle with producing or interpreting such repair acts. In this work, we test three hypotheses concerning the effects of action taking as an auxiliary task in modelling iCR policies. Contrary to initial expectations, we conclude that its contribution to learning an iCR policy is limited, but some information can still be extracted from prediction uncertainty. We present further evidence that even well-motivated, Transformer-based models fail to learn good policies for when to ask Instruction CRs (iCRs), while the task of determining what to ask about can be more successfully modelled. Considering the implications of these findings, we further discuss the shortcomings of the data-driven paradigm for learning meta-communication acts.</abstract>
      <url hash="16ddc5ad">2024.unimplicit-1.1</url>
      <bibkey>madureira-schlangen-2024-taking</bibkey>
    </paper>
    <paper id="2">
      <title>More Labels or Cases? Assessing Label Variation in Natural Language Inference</title>
      <author><first>Cornelia</first><last>Gruber</last></author>
      <author><first>Katharina</first><last>Hechinger</last></author>
      <author><first>Matthias</first><last>Assenmacher</last></author>
      <author><first>Göran</first><last>Kauermann</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>22-32</pages>
      <abstract>In this work, we analyze the uncertainty that is inherently present in the labels used for supervised machine learning in natural language inference (NLI). In cases where multiple annotations per instance are available, neither the majority vote nor the frequency of individual class votes is a trustworthy representation of the labeling uncertainty. We propose modeling the votes via a Bayesian mixture model to recover the data-generating process, i.e., the “true” latent classes, and thus gain insight into the class variations. This will enable a better understanding of the confusion happening during the annotation process. We also assess the stability of the proposed estimation procedure by systematically varying the numbers of i) instances and ii) labels. Thereby, we observe that few instances with many labels can predict the latent class borders reasonably well, while the estimation fails for many instances with only a few labels. This leads us to conclude that multiple labels are a crucial building block for properly analyzing label uncertainty.</abstract>
      <url hash="1725722c">2024.unimplicit-1.2</url>
      <bibkey>gruber-etal-2024-labels</bibkey>
    </paper>
    <paper id="3">
      <title>Resolving Transcription Ambiguity in <fixed-case>S</fixed-case>panish: A Hybrid Acoustic-Lexical System for Punctuation Restoration</title>
      <author><first>Xiliang</first><last>Zhu</last></author>
      <author><first>Chia-Tien</first><last>Chang</last></author>
      <author><first>Shayna</first><last>Gardiner</last></author>
      <author><first>David</first><last>Rossouw</last></author>
      <author><first>Jonas</first><last>Robertson</last></author>
      <pages>33-41</pages>
      <abstract>Punctuation restoration is a crucial step after Automatic Speech Recognition (ASR) systems to enhance transcript readability and facilitate subsequent NLP tasks. Nevertheless, conventional lexical-based approaches are inadequate for solving the punctuation restoration task in Spanish, where ambiguity can be often found between unpunctuated declaratives and questions. In this study, we propose a novel hybrid acoustic-lexical punctuation restoration system for Spanish transcription, which consolidates acoustic and lexical signals through a modular process. Our experiment results show that the proposed system can effectively improve F1 score of question marks and overall punctuation restoration on both public and internal Spanish conversational datasets. Additionally, benchmark comparison against LLMs (Large Language Model) indicates the superiority of our approach in accuracy, reliability and latency. Furthermore, we demonstrate that the Word Error Rate (WER) of the ASR module also benefits from our proposed system.</abstract>
      <url hash="21a8b2c2">2024.unimplicit-1.3</url>
      <bibkey>zhu-etal-2024-resolving</bibkey>
    </paper>
    <paper id="4">
      <title>Assessing the Significance of Encoded Information in Contextualized Representations to Word Sense Disambiguation</title>
      <author><first>Deniz</first><last>Ekin Yavas</last></author>
      <pages>42-53</pages>
      <abstract>The similarity of representations is crucial for WSD. However, a lot of information is encoded in the contextualized representations, and it is not clear which sentence context features drive this similarity and whether these features are significant to WSD. In this study, we address these questions. First, we identify the sentence context features that are responsible for the similarity of the contextualized representations of different occurrences of words. For this purpose, we conduct an explainability experiment and identify the sentence context features that lead to the formation of the clusters in word sense clustering with CWEs. Then, we provide a qualitative evaluation for assessing the significance of these features to WSD. Our results show that features that lack significance to WSD determine the similarity of the representations even when different senses of a word occur in highly diverse contexts and sentence context provides clear clues for different senses.</abstract>
      <url hash="8282ca3d">2024.unimplicit-1.4</url>
      <bibkey>ekin-yavas-2024-assessing</bibkey>
    </paper>
    <paper id="5">
      <title>Below the Sea (with the Sharks): Probing Textual Features of Implicit Sentiment in a Literary Case-study</title>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <author><first>Pascale</first><last>Feldkamp</last></author>
      <pages>54-61</pages>
      <abstract>Literary language presents an ongoing challenge for Sentiment Analysis due to its complex, nuanced, and layered form of expression. It is often suggested that effective literary writing is evocative, operating beneath the surface and understating emotional expression. To explore features of implicitness in literary expression, this study takes Ernest Hemingway’s The Old Man and the Sea as a case for examining implicit sentiment expression. We examine sentences where automatic sentiment annotations show substantial divergences from human sentiment annotations, and probe these sentences for distinctive traits. We find that sentences where humans perceived a strong sentiment while models did not are significantly lower in arousal and higher in concreteness than sentences where humans and models were more aligned, suggesting the importance of simplicity and concreteness for implicit sentiment expression in literary prose.</abstract>
      <url hash="ea074fe5">2024.unimplicit-1.5</url>
      <bibkey>bizzoni-feldkamp-2024-sea</bibkey>
    </paper>
    <paper id="6">
      <title>Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification</title>
      <author><first>Géraud</first><last>Faye</last></author>
      <author><first>Benjamin</first><last>Icard</last></author>
      <author><first>Morgane</first><last>Casanova</last></author>
      <author><first>Julien</first><last>Chanson</last></author>
      <author><first>François</first><last>Maine</last></author>
      <author><first>François</first><last>Bancilhon</last></author>
      <author><first>Guillaume</first><last>Gadek</last></author>
      <author><first>Guillaume</first><last>Gravier</last></author>
      <author><first>Paul</first><last>Égré</last></author>
      <pages>62-72</pages>
      <abstract>This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We use different NLP techniques to identify the cues used by annotators, and to compare them with machine classification: first the analyzer VAGO to detect discourse vagueness and subjectivity, and then four different classifiers, two based on RoBERTa, one CATS using syntax, and one XGBoost combining syntactic and semantic features.</abstract>
      <url hash="85deb1e5">2024.unimplicit-1.6</url>
      <bibkey>faye-etal-2024-exposing</bibkey>
    </paper>
    <paper id="7">
      <title>Different Tastes of Entities: Investigating Human Label Variation in Named Entity Annotations</title>
      <author><first>Siyao</first><last>Peng</last></author>
      <author><first>Zihang</first><last>Sun</last></author>
      <author><first>Sebastian</first><last>Loftus</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>73-81</pages>
      <abstract>Named Entity Recognition (NER) is a key information extraction task with a long-standing tradition. While recent studies address and aim to correct annotation errors via re-labeling efforts, little is known about the sources of label variation, such as text ambiguity, annotation error, or guideline divergence. This is especially the case for high-quality datasets and beyond English CoNLL03. This paper studies disagreements in expert-annotated named entity datasets for three varieties: English, Danish, and DialectX. We show that text ambiguity and artificial guideline changes are dominant factors for diverse annotations among high-quality revisions. We survey student annotations on a subset of difficult entities and substantiate the feasibility and necessity of manifold annotations for understanding named entity ambiguities from a distributional perspective.</abstract>
      <url hash="b9eb0fb0">2024.unimplicit-1.7</url>
      <bibkey>peng-etal-2024-different</bibkey>
    </paper>
    <paper id="8">
      <title>Colour Me Uncertain: Representing Vagueness with Probabilistic Semantics</title>
      <author><first>Kin</first><last>Chun Cheung</last></author>
      <author><first>Guy</first><last>Emerson</last></author>
      <pages>82-89</pages>
      <abstract>People successfully communicate in everyday situations using vague language. In particular, colour terms have no clear boundaries as to the ranges of colours they describe. We model people’s reasoning process in a dyadic reference game using the Rational Speech Acts (RSA) framework and probabilistic semantics, and we find that the implementation of probabilistic semantics requires a modification from pure theory to perform well on real-world data. In addition, we explore approaches to handling target disagreements in reference games, an issue that is rarely discussed in the RSA literature.</abstract>
      <url hash="a2f71d6d">2024.unimplicit-1.8</url>
      <bibkey>chun-cheung-emerson-2024-colour</bibkey>
    </paper>
  </volume>
</collection>
