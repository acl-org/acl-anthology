<?xml version='1.0' encoding='UTF-8'?>
<collection id="S19">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*<fixed-case>SEM</fixed-case> 2019)</booktitle>
      <url hash="da651103">S19-1</url>
      <editor><first>Rada</first><last>Mihalcea</last></editor>
      <editor><first>Ekaterina</first><last>Shutova</last></editor>
      <editor><first>Lun-Wei</first><last>Ku</last></editor>
      <editor><first>Kilian</first><last>Evang</last></editor>
      <editor><first>Soujanya</first><last>Poria</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="5ccfc31b">S19-1000</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>SUR</fixed-case>el: A Gold Standard for Incorporating Meaning Shifts into Term Extraction</title>
      <author><first>Anna</first><last>Hätty</last></author>
      <author><first>Dominik</first><last>Schlechtweg</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>1–8</pages>
      <abstract>We introduce SURel, a novel dataset with human-annotated meaning shifts between general-language and domain-specific contexts. We show that meaning shifts of term candidates cause errors in term extraction, and demonstrate that the SURel annotation reflects these errors. Furthermore, we illustrate that SURel enables us to assess optimisations of term extraction techniques when incorporating meaning shifts.</abstract>
      <url hash="cd5e172d">S19-1001</url>
      <doi>10.18653/v1/S19-1001</doi>
    </paper>
    <paper id="2">
      <title>Word Usage Similarity Estimation with Sentence Representations and Automatic Substitutes</title>
      <author><first>Aina</first><last>Garí Soler</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <pages>9–21</pages>
      <abstract>Usage similarity estimation addresses the semantic proximity of word instances in different contexts. We apply contextualized (ELMo and BERT) word and sentence embeddings to this task, and propose supervised models that leverage these representations for prediction. Our models are further assisted by lexical substitute annotations automatically assigned to word instances by context2vec, a neural model that relies on a bidirectional LSTM. We perform an extensive comparison of existing word and sentence representations on benchmark datasets addressing both graded and binary similarity.The best performing models outperform previous methods in both settings.</abstract>
      <url hash="a74a1d8e">S19-1002</url>
      <doi>10.18653/v1/S19-1002</doi>
    </paper>
    <paper id="3">
      <title>Beyond Context: A New Perspective for Word Embeddings</title>
      <author><first>Yichu</first><last>Zhou</last></author>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <pages>22–32</pages>
      <abstract>Most word embeddings today are trained by optimizing a language modeling goal of scoring words in their context, modeled as a multi-class classification problem. In this paper, we argue that, despite the successes of this assumption, it is incomplete: in addition to its context, orthographical or morphological aspects of words can offer clues about their meaning. We define a new modeling framework for training word embeddings that captures this intuition. Our framework is based on the well-studied problem of multi-label classification and, consequently, exposes several design choices for featurizing words and contexts, loss functions for training and score normalization. Indeed, standard models such as CBOW and fasttext are specific choices along each of these axes. We show via experiments that by combining feature engineering with embedding learning, our method can outperform CBOW using only 10% of the training data in both the standard word embedding evaluations and also text classification experiments.</abstract>
      <url hash="c5d72f94">S19-1003</url>
      <doi>10.18653/v1/S19-1003</doi>
    </paper>
    <paper id="4">
      <title>Composition of Sentence Embeddings: Lessons from Statistical Relational Learning</title>
      <author><first>Damien</first><last>Sileo</last></author>
      <author><first>Tim</first><last>Van De Cruys</last></author>
      <author><first>Camille</first><last>Pradel</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <pages>33–43</pages>
      <abstract>Various NLP problems – such as the prediction of sentence similarity, entailment, and discourse relations – are all instances of the same general task: the modeling of semantic relations between a pair of textual elements. A popular model for such problems is to embed sentences into fixed size vectors, and use composition functions (e.g. concatenation or sum) of those vectors as features for the prediction. At the same time, composition of embeddings has been a main focus within the field of Statistical Relational Learning (SRL) whose goal is to predict relations between entities (typically from knowledge base triples). In this article, we show that previous work on relation prediction between texts implicitly uses compositions from baseline SRL models. We show that such compositions are not expressive enough for several tasks (e.g. natural language inference). We build on recent SRL models to address textual relational problems, showing that they are more expressive, and can alleviate issues from simpler compositions. The resulting models significantly improve the state of the art in both transferable sentence representation learning and relation prediction.</abstract>
      <url hash="d3e79a64">S19-1004</url>
      <doi>10.18653/v1/S19-1004</doi>
    </paper>
    <paper id="5">
      <title>Multi-Label Transfer Learning for Multi-Relational Semantic Similarity</title>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Steven</first><last>Wilson</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>44–50</pages>
      <abstract>Multi-relational semantic similarity datasets define the semantic relations between two short texts in multiple ways, e.g., similarity, relatedness, and so on. Yet, all the systems to date designed to capture such relations target one relation at a time. We propose a multi-label transfer learning approach based on LSTM to make predictions for several relations simultaneously and aggregate the losses to update the parameters. This multi-label regression approach jointly learns the information provided by the multiple relations, rather than treating them as separate tasks. Not only does this approach outperform the single-task approach and the traditional multi-task learning approach, but it also achieves state-of-the-art performance on all but one relation of the Human Activity Phrase dataset.</abstract>
      <url hash="b93eca8c">S19-1005</url>
      <attachment type="presentation" hash="6675b8ce">S19-1005.Presentation.pdf</attachment>
      <doi>10.18653/v1/S19-1005</doi>
    </paper>
    <paper id="6">
      <title>Scalable Cross-Lingual Transfer of Neural Sentence Embeddings</title>
      <author><first>Hanan</first><last>Aldarmaki</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>51–60</pages>
      <abstract>We develop and investigate several cross-lingual alignment approaches for neural sentence embedding models, such as the supervised inference classifier, InferSent, and sequential encoder-decoder models. We evaluate three alignment frameworks applied to these models: joint modeling, representation transfer learning, and sentence mapping, using parallel text to guide the alignment. Our results support representation transfer as a scalable approach for modular cross-lingual alignment of neural sentence embeddings, where we observe better performance compared to joint models in intrinsic and extrinsic evaluations, particularly with smaller sets of parallel data.</abstract>
      <url hash="affb7ac2">S19-1006</url>
      <doi>10.18653/v1/S19-1006</doi>
    </paper>
    <paper id="7">
      <title>Second-order contexts from lexical substitutes for few-shot learning of word representations</title>
      <author><first>Qianchu</first><last>Liu</last></author>
      <author><first>Diana</first><last>McCarthy</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>61–67</pages>
      <abstract>There is a growing awareness of the need to handle rare and unseen words in word representation modelling. In this paper, we focus on few-shot learning of emerging concepts that fully exploits only a few available contexts. We introduce a substitute-based context representation technique that can be applied on an existing word embedding space. Previous context-based approaches to modelling unseen words only consider bag-of-word first-order contexts, whereas our method aggregates contexts as second-order substitutes that are produced by a sequence-aware sentence completion model. We experimented with three tasks that aim to test the modelling of emerging concepts. We found that these tasks show different emphasis on first and second order contexts, and our substitute-based method achieves superior performance on naturally-occurring contexts from corpora.</abstract>
      <url hash="33028942">S19-1007</url>
      <doi>10.18653/v1/S19-1007</doi>
    </paper>
    <paper id="8">
      <title>Pre-trained Contextualized Character Embeddings Lead to Major Improvements in Time Normalization: a Detailed Analysis</title>
      <author><first>Dongfang</first><last>Xu</last></author>
      <author><first>Egoitz</first><last>Laparra</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <pages>68–74</pages>
      <abstract>Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.</abstract>
      <url hash="12f5adfe">S19-1008</url>
      <doi>10.18653/v1/S19-1008</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>B</fixed-case>ot2<fixed-case>V</fixed-case>ec: Learning Representations of Chatbots</title>
      <author><first>Jonathan</first><last>Herzig</last></author>
      <author><first>Tommy</first><last>Sandbank</last></author>
      <author><first>Michal</first><last>Shmueli-Scheuer</last></author>
      <author><first>David</first><last>Konopnicki</last></author>
      <pages>75–84</pages>
      <abstract>Chatbots (i.e., bots) are becoming widely used in multiple domains, along with supporting bot programming platforms. These platforms are equipped with novel testing tools aimed at improving the quality of individual chatbots. Doing so requires an understanding of what sort of bots are being built (captured by their underlying conversation graphs) and how well they perform (derived through analysis of conversation logs). In this paper, we propose a new model, Bot2Vec, that embeds bots to a compact representation based on their structure and usage logs. Then, we utilize Bot2Vec representations to improve the quality of two bot analysis tasks. Using conversation data and graphs of over than 90 bots, we show that Bot2Vec representations improve detection performance by more than 16% for both tasks.</abstract>
      <url hash="20055270">S19-1009</url>
      <doi>10.18653/v1/S19-1009</doi>
    </paper>
    <paper id="10">
      <title>Are We Consistently Biased? Multidimensional Analysis of Biases in Distributional Word Vectors</title>
      <author><first>Anne</first><last>Lauscher</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>85–91</pages>
      <abstract>Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.</abstract>
      <url hash="ab622f39">S19-1010</url>
      <doi>10.18653/v1/S19-1010</doi>
    </paper>
    <paper id="11">
      <title>A Semantic Cover Approach for Topic Modeling</title>
      <author><first>Rajagopal</first><last>Venkatesaramani</last></author>
      <author><first>Doug</first><last>Downey</last></author>
      <author><first>Bradley</first><last>Malin</last></author>
      <author><first>Yevgeniy</first><last>Vorobeychik</last></author>
      <pages>92–102</pages>
      <abstract>We introduce a novel topic modeling approach based on constructing a semantic set cover for clusters of similar documents. Specifically, our approach first clusters documents using their Tf-Idf representation, and then covers each cluster with a set of topic words based on semantic similarity, defined in terms of a word embedding. Computing a topic cover amounts to solving a minimum set cover problem. Our evaluation compares our topic modeling approach to Latent Dirichlet Allocation (LDA) on three metrics: 1) qualitative topic match, measured using evaluations by Amazon Mechanical Turk (MTurk) workers, 2) performance on classification tasks using each topic model as a sparse feature representation, and 3) topic coherence. We find that qualitative judgments significantly favor our approach, the method outperforms LDA on topic coherence, and is comparable to LDA on document classification tasks.</abstract>
      <url hash="27db580a">S19-1011</url>
      <doi>10.18653/v1/S19-1011</doi>
    </paper>
    <paper id="12">
      <title><fixed-case>MCS</fixed-case>cript2.0: A Machine Comprehension Corpus Focused on Script Events and Participants</title>
      <author><first>Simon</first><last>Ostermann</last></author>
      <author><first>Michael</first><last>Roth</last></author>
      <author><first>Manfred</first><last>Pinkal</last></author>
      <pages>103–117</pages>
      <abstract>We introduce MCScript2.0, a machine comprehension corpus for the end-to-end evaluation of script knowledge. MCScript2.0 contains approx. 20,000 questions on approx. 3,500 texts, crowdsourced based on a new collection process that results in challenging questions. Half of the questions cannot be answered from the reading texts, but require the use of commonsense and, in particular, script knowledge. We give a thorough analysis of our corpus and show that while the task is not challenging to humans, existing machine comprehension models fail to perform well on the data, even if they make use of a commonsense knowledge base. The dataset is available at http://www.sfb1102. uni-saarland.de/?page_id=2582</abstract>
      <url hash="7ca6a86e">S19-1012</url>
      <doi>10.18653/v1/S19-1012</doi>
    </paper>
    <paper id="13">
      <title>Deconstructing multimodality: visual properties and visual context in human semantic processing</title>
      <author><first>Christopher</first><last>Davis</last></author>
      <author><first>Luana</first><last>Bulat</last></author>
      <author><first>Anita Lilla</first><last>Vero</last></author>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <pages>118–124</pages>
      <abstract>Multimodal semantic models that extend linguistic representations with additional perceptual input have proved successful in a range of natural language processing (NLP) tasks. Recent research has successfully used neural methods to automatically create visual representations for words. However, these works have extracted visual features from complete images, and have not examined how different kinds of visual information impact performance. In contrast, we construct multimodal models that differentiate between internal visual properties of the objects and their external visual context. We evaluate the models on the task of decoding brain activity associated with the meanings of nouns, demonstrating their advantage over those based on complete images.</abstract>
      <url hash="196a7bc7">S19-1013</url>
      <doi>10.18653/v1/S19-1013</doi>
    </paper>
    <paper id="14">
      <title>Learning Graph Embeddings from <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et-based Similarity Measures</title>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Mohammad</first><last>Dorgham</last></author>
      <author><first>Oleksiy</first><last>Oliynyk</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>125–135</pages>
      <abstract>We present path2vec, a new approach for learning graph embeddings that relies on structural measures of pairwise node similarities. The model learns representations for nodes in a dense space that approximate a given user-defined graph distance measure, such as e.g. the shortest path distance or distance measures that take information beyond the graph structure into account. Evaluation of the proposed model on semantic similarity and word sense disambiguation tasks, using various WordNet-based similarity measures, show that our approach yields competitive results, outperforming strong graph embedding baselines. The model is computationally efficient, being orders of magnitude faster than the direct computation of graph-based distances.</abstract>
      <url hash="b93a5da0">S19-1014</url>
      <doi>10.18653/v1/S19-1014</doi>
    </paper>
    <paper id="15">
      <title>Neural User Factor Adaptation for Text Classification: Learning to Generalize Across Author Demographics</title>
      <author><first>Xiaolei</first><last>Huang</last></author>
      <author><first>Michael J.</first><last>Paul</last></author>
      <pages>136–146</pages>
      <abstract>Language use varies across different demographic factors, such as gender, age, and geographic location. However, most existing document classification methods ignore demographic variability. In this study, we examine empirically how text data can vary across four demographic factors: gender, age, country, and region. We propose a multitask neural model to account for demographic variations via adversarial training. In experiments on four English-language social media datasets, we find that classification performance improves when adapting for user factors.</abstract>
      <url hash="4eb7c418">S19-1015</url>
      <doi>10.18653/v1/S19-1015</doi>
    </paper>
    <paper id="16">
      <title>Abstract Graphs and Abstract Paths for Knowledge Graph Completion</title>
      <author><first>Vivi</first><last>Nastase</last></author>
      <author><first>Bhushan</first><last>Kotnis</last></author>
      <pages>147–157</pages>
      <abstract>Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.</abstract>
      <url hash="486c5ef4">S19-1016</url>
      <doi>10.18653/v1/S19-1016</doi>
    </paper>
    <paper id="17">
      <title>A Corpus of Negations and their Underlying Positive Interpretations</title>
      <author><first>Zahra</first><last>Sarabi</last></author>
      <author><first>Erin</first><last>Killian</last></author>
      <author><first>Eduardo</first><last>Blanco</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <pages>158–167</pages>
      <abstract>Negation often conveys implicit positive meaning. In this paper, we present a corpus of negations and their underlying positive interpretations. We work with negations from Simple Wikipedia, automatically generate potential positive interpretations, and then collect manual annotations that effectively rewrite the negation in positive terms. This procedure yields positive interpretations for approximately 77% of negations, and the final corpus includes over 5,700 negations and over 5,900 positive interpretations. We also present baseline results using seq2seq neural models.</abstract>
      <url hash="6bbeb7de">S19-1017</url>
      <doi>10.18653/v1/S19-1017</doi>
    </paper>
    <paper id="18">
      <title>Enthymemetic Conditionals</title>
      <author><first>Eimear</first><last>Maguire</last></author>
      <pages>168–177</pages>
      <abstract>To model conditionals in a way that reflects their acceptability, we must include some means of making judgements about whether antecedent and consequent are meaningfully related or not. Enthymemes are non-logical arguments which do not hold up by themselves, but are acceptable through their relation to a topos, an already-known general principle or pattern for reasoning. This paper uses enthymemes and topoi as a way to model the world-knowledge behind these judgements. In doing so, it provides a reformalisation (in TTR) of enthymemes and topoi as networks rather than functions, and information state update rules for conditionals.</abstract>
      <url hash="040ebad9">S19-1018</url>
      <doi>10.18653/v1/S19-1018</doi>
    </paper>
    <paper id="19">
      <title>Acquiring Structured Temporal Representation via Crowdsourcing: A Feasibility Study</title>
      <author><first>Yuchen</first><last>Zhang</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <pages>178–185</pages>
      <abstract>Temporal Dependency Trees are a structured temporal representation that represents temporal relations among time expressions and events in a text as a dependency tree structure. Compared to traditional pair-wise temporal relation representations, temporal dependency trees facilitate efficient annotations, higher inter-annotator agreement, and efficient computations. However, annotations on temporal dependency trees so far have only been done by expert annotators, which is costly and time-consuming. In this paper, we introduce a method to crowdsource temporal dependency tree annotations, and show that this representation is intuitive and can be collected with high accuracy and agreement through crowdsourcing. We produce a corpus of temporal dependency trees, and present a baseline temporal dependency parser, trained and evaluated on this new corpus.</abstract>
      <url hash="ffc41e32">S19-1019</url>
      <doi>10.18653/v1/S19-1019</doi>
    </paper>
    <paper id="20">
      <title>Exploration of Noise Strategies in Semi-supervised Named Entity Classification</title>
      <author><first>Pooja</first><last>Lakshmi Narayan</last></author>
      <author><first>Ajay</first><last>Nagesh</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>186–191</pages>
      <abstract>Noise is inherent in real world datasets and modeling noise is critical during training as it is effective in regularization. Recently, novel semi-supervised deep learning techniques have demonstrated tremendous potential when learning with very limited labeled training data in image processing tasks. A critical aspect of these semi-supervised learning techniques is augmenting the input or the network with noise to be able to learn robust models. While modeling noise is relatively straightforward in continuous domains such as image classification, it is not immediately apparent how noise can be modeled in discrete domains such as language. Our work aims to address this gap by exploring different noise strategies for the semi-supervised named entity classification task, including statistical methods such as adding Gaussian noise to input embeddings, and linguistically-inspired ones such as dropping words and replacing words with their synonyms. We compare their performance on two benchmark datasets (OntoNotes and CoNLL) for named entity classification. Our results indicate that noise strategies that are linguistically informed perform at least as well as statistical approaches, while being simpler and requiring minimal tuning.</abstract>
      <url hash="c3e711d0">S19-1020</url>
      <doi>10.18653/v1/S19-1020</doi>
    </paper>
    <paper id="21">
      <title>Improving Generalization in Coreference Resolution via Adversarial Training</title>
      <author><first>Sanjay</first><last>Subramanian</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>192–197</pages>
      <abstract>In order for coreference resolution systems to be useful in practice, they must be able to generalize to new text. In this work, we demonstrate that the performance of the state-of-the-art system decreases when the names of PER and GPE named entities in the CoNLL dataset are changed to names that do not occur in the training set. We use the technique of adversarial gradient-based training to retrain the state-of-the-art system and demonstrate that the retrained system achieves higher performance on the CoNLL dataset (both with and without the change of named entities) and the GAP dataset.</abstract>
      <url hash="ec76392c">S19-1021</url>
      <doi>10.18653/v1/S19-1021</doi>
    </paper>
    <paper id="22">
      <title>Improving Human Needs Categorization of Events with Semantic Classification</title>
      <author><first>Haibo</first><last>Ding</last></author>
      <author><first>Ellen</first><last>Riloff</last></author>
      <author><first>Zhe</first><last>Feng</last></author>
      <pages>198–204</pages>
      <abstract>Human Needs categories have been used to characterize the reason why an affective event is positive or negative. For example, “I got the flu” and “I got fired” are both negative (undesirable) events, but getting the flu is a Health problem while getting fired is a Financial problem. Previous work created learning models to assign events to Human Needs categories based on their words and contexts. In this paper, we introduce an intermediate step that assigns words to relevant semantic concepts. We create lightly supervised models that learn to label words with respect to 10 semantic concepts associated with Human Needs categories, and incorporate these labels as features for event categorization. Our results show that recognizing relevant semantic concepts improves both the recall and precision of Human Needs categorization for events.</abstract>
      <url hash="69a87a25">S19-1022</url>
      <doi>10.18653/v1/S19-1022</doi>
    </paper>
    <paper id="23">
      <title>Word Embeddings (Also) Encode Human Personality Stereotypes</title>
      <author><first>Oshin</first><last>Agarwal</last></author>
      <author><first>Funda</first><last>Durupınar</last></author>
      <author><first>Norman I.</first><last>Badler</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <pages>205–211</pages>
      <abstract>Word representations trained on text reproduce human implicit bias related to gender, race and age. Methods have been developed to remove such bias. Here, we present results that show that human stereotypes exist even for much more nuanced judgments such as personality, for a variety of person identities beyond the typically legally protected attributes and that these are similarly captured in word representations. Specifically, we collected human judgments about a person’s Big Five personality traits formed solely from information about the occupation, nationality or a common noun description of a hypothetical person. Analysis of the data reveals a large number of statistically significant stereotypes in people. We then demonstrate the bias captured in lexical representations is statistically significantly correlated with the documented human bias. Our results, showing bias for a large set of person descriptors for such nuanced traits put in doubt the feasibility of broadly and fairly applying debiasing methods and call for the development of new methods for auditing language technology systems and resources.</abstract>
      <url hash="fd9d6bf8">S19-1023</url>
      <doi>10.18653/v1/S19-1023</doi>
    </paper>
    <paper id="24">
      <title>Automatic Accuracy Prediction for <fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Juri</first><last>Opitz</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>212–223</pages>
      <abstract>Abstract Meaning Representation (AMR) represents sentences as directed, acyclic and rooted graphs, aiming at capturing their meaning in a machine readable format. AMR parsing converts natural language sentences into such graphs. However, evaluating a parser on new data by means of comparison to manually created AMR graphs is very costly. Also, we would like to be able to detect parses of questionable quality, or preferring results of alternative systems by selecting the ones for which we can assess good quality. We propose AMR accuracy prediction as the task of predicting several metrics of correctness for an automatically generated AMR parse – in absence of the corresponding gold parse. We develop a neural end-to-end multi-output regression model and perform three case studies: firstly, we evaluate the model’s capacity of predicting AMR parse accuracies and test whether it can reliably assign high scores to gold parses. Secondly, we perform parse selection based on predicted parse accuracies of candidate parses from alternative systems, with the aim of improving overall results. Finally, we predict system ranks for submissions from two AMR shared tasks on the basis of their predicted parse accuracy averages. All experiments are carried out across two different domains and show that our method is effective.</abstract>
      <url hash="899db651">S19-1024</url>
      <doi>10.18653/v1/S19-1024</doi>
    </paper>
    <paper id="25">
      <title>An Argument-Marker Model for Syntax-Agnostic Proto-Role Labeling</title>
      <author><first>Juri</first><last>Opitz</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>224–234</pages>
      <abstract>Semantic proto-role labeling (SPRL) is an alternative to semantic role labeling (SRL) that moves beyond a categorical definition of roles, following Dowty’s feature-based view of proto-roles. This theory determines agenthood vs. patienthood based on a participant’s instantiation of more or less typical agent vs. patient properties, such as, for example, volition in an event. To perform SPRL, we develop an ensemble of hierarchical models with self-attention and concurrently learned predicate-argument markers. Our method is competitive with the state-of-the art, overall outperforming previous work in two formulations of the task (multi-label and multi-variate Likert scale pre- diction). In contrast to previous work, our results do not depend on gold argument heads derived from supplementary gold tree banks.</abstract>
      <url hash="bb5ee966">S19-1025</url>
      <doi>10.18653/v1/S19-1025</doi>
    </paper>
    <paper id="26">
      <title>Probing What Different <fixed-case>NLP</fixed-case> Tasks Teach Machines about Function Word Comprehension</title>
      <author><first>Najoung</first><last>Kim</last></author>
      <author><first>Roma</first><last>Patel</last></author>
      <author><first>Adam</first><last>Poliak</last></author>
      <author><first>Patrick</first><last>Xia</last></author>
      <author><first>Alex</first><last>Wang</last></author>
      <author><first>Tom</first><last>McCoy</last></author>
      <author><first>Ian</first><last>Tenney</last></author>
      <author><first>Alexis</first><last>Ross</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <pages>235–249</pages>
      <abstract>We introduce a set of nine challenge tasks that test for the understanding of function words. These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of specific types of function words (e.g., prepositions, wh-words). Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, CCG supertagging and natural language inference (NLI)) on the learned representations. Our results show that pretraining on CCG—our most syntactic objective—performs the best on average across our probing tasks, suggesting that syntactic knowledge helps function word comprehension. Language modeling also shows strong performance, supporting its widespread use for pretraining state-of-the-art NLP models. Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives, e.g., that NLI helps the comprehension of negation.</abstract>
      <url hash="7a1d5071">S19-1026</url>
      <doi>10.18653/v1/S19-1026</doi>
      <revision id="1" href="S19-1026v1" hash="d4821787"/>
      <revision id="2" href="S19-1026v2" hash="7a1d5071">We found some mismappings of labels in some of the datasets we used in the paper, which affected some of the results reported. It does not significantly affect any of the bottom line conclusions of the work, but the difference is enough that we would like to submit a correction. I.e. our main takeaways from the paper were that no pretraining objective uniformly dominated across the datasets (still true) and that pretraining can sometimes hurt performance (still true). But some intermediate conclusions did change, e.g. rather than CCG being the best on average, we now see LM as the best on average (or rather LM and CCG perform statistically equivalently).</revision>
    </paper>
    <paper id="27">
      <title><fixed-case>HELP</fixed-case>: A Dataset for Identifying Shortcomings of Neural Models in Monotonicity Reasoning</title>
      <author><first>Hitomi</first><last>Yanaka</last></author>
      <author><first>Koji</first><last>Mineshima</last></author>
      <author><first>Daisuke</first><last>Bekki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <author><first>Satoshi</first><last>Sekine</last></author>
      <author><first>Lasha</first><last>Abzianidze</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <pages>250–255</pages>
      <abstract>Large crowdsourced datasets are widely used for training and evaluating neural models on natural language inference (NLI). Despite these efforts, neural models have a hard time capturing logical inferences, including those licensed by phrase replacements, so-called monotonicity reasoning. Since no large dataset has been developed for monotonicity reasoning, it is still unclear whether the main obstacle is the size of datasets or the model architectures themselves. To investigate this issue, we introduce a new dataset, called HELP, for handling entailments with lexical and logical phenomena. We add it to training data for the state-of-the-art neural models and evaluate them on test sets for monotonicity phenomena. The results showed that our data augmentation improved the overall accuracy. We also find that the improvement is better on monotonicity inferences with lexical replacements than on downward inferences with disjunction and modification. This suggests that some types of inferences can be improved by our data augmentation while others are immune to it.</abstract>
      <url hash="09921d03">S19-1027</url>
      <doi>10.18653/v1/S19-1027</doi>
    </paper>
    <paper id="28">
      <title>On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference</title>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <author><first>Adam</first><last>Poliak</last></author>
      <author><first>Stuart</first><last>Shieber</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>256–262</pages>
      <abstract>Popular Natural Language Inference (NLI) datasets have been shown to be tainted by hypothesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy.</abstract>
      <url hash="ffb6c4fc">S19-1028</url>
      <doi>10.18653/v1/S19-1028</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>B</fixed-case>ayesian Inference Semantics: A Modelling System and A Test Suite</title>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <author><first>Rasmus</first><last>Blanck</last></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
      <author><first>Shalom</first><last>Lappin</last></author>
      <author><first>Aleksandre</first><last>Maskharashvili</last></author>
      <pages>263–272</pages>
      <abstract>We present BIS, a Bayesian Inference Semantics, for probabilistic reasoning in natural language. The current system is based on the framework of Bernardy et al. (2018), but departs from it in important respects. BIS makes use of Bayesian learning for inferring a hypothesis from premises. This involves estimating the probability of the hypothesis, given the data supplied by the premises of an argument. It uses a syntactic parser to generate typed syntactic structures that serve as input to a model generation system. Sentences are interpreted compositionally to probabilistic programs, and the corresponding truth values are estimated using sampling methods. BIS successfully deals with various probabilistic semantic phenomena, including frequency adverbs, generalised quantifiers, generics, and vague predicates. It performs well on a number of interesting probabilistic reasoning tasks. It also sustains most classically valid inferences (instantiation, de Morgan’s laws, etc.). To test BIS we have built an experimental test suite with examples of a range of probabilistic and classical inference patterns.</abstract>
      <url hash="6d97b949">S19-1029</url>
      <doi>10.18653/v1/S19-1029</doi>
    </paper>
    <paper id="30">
      <title>Target Based Speech Act Classification in Political Campaign Text</title>
      <author><first>Shivashankar</first><last>Subramanian</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>273–282</pages>
      <abstract>We study pragmatics in political campaign text, through analysis of speech acts and the target of each utterance. We propose a new annotation schema incorporating domain-specific speech acts, such as commissive-action, and present a novel annotated corpus of media releases and speech transcripts from the 2016 Australian election cycle. We show how speech acts and target referents can be modeled as sequential classification, and evaluate several techniques, exploiting contextualized word representations, semi-supervised learning, task dependencies and speaker meta-data.</abstract>
      <url hash="a8dd8298">S19-1030</url>
      <doi>10.18653/v1/S19-1030</doi>
    </paper>
    <paper id="31">
      <title>Incivility Detection in Online Comments</title>
      <author><first>Farig</first><last>Sadeque</last></author>
      <author><first>Stephen</first><last>Rains</last></author>
      <author><first>Yotam</first><last>Shmargad</last></author>
      <author><first>Kate</first><last>Kenski</last></author>
      <author><first>Kevin</first><last>Coe</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <pages>283–291</pages>
      <abstract>Incivility in public discourse has been a major concern in recent times as it can affect the quality and tenacity of the discourse negatively. In this paper, we present neural models that can learn to detect name-calling and vulgarity from a newspaper comment section. We show that in contrast to prior work on detecting toxic language, fine-grained incivilities like namecalling cannot be accurately detected by simple models like logistic regression. We apply the models trained on the newspaper comments data to detect uncivil comments in a Russian troll dataset, and find that despite the change of domain, the model makes accurate predictions.</abstract>
      <url hash="1303b586">S19-1031</url>
      <doi>10.18653/v1/S19-1031</doi>
    </paper>
    <paper id="32">
      <title>Generating Animations from Screenplays</title>
      <author><first>Yeyao</first><last>Zhang</last></author>
      <author><first>Eleftheria</first><last>Tsipidi</last></author>
      <author><first>Sasha</first><last>Schriber</last></author>
      <author><first>Mubbasir</first><last>Kapadia</last></author>
      <author><first>Markus</first><last>Gross</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>292–307</pages>
      <abstract>Automatically generating animation from natural language text finds application in a number of areas e.g. movie script writing, instructional videos, and public safety. However, translating natural language text into animation is a challenging task. Existing text-to-animation systems can handle only very simple sentences, which limits their applications. In this paper, we develop a text-to-animation system which is capable of handling complex sentences. We achieve this by introducing a text simplification step into the process. Building on an existing animation generation system for screenwriting, we create a robust NLP pipeline to extract information from screenplays and map them to the system’s knowledge base. We develop a set of linguistic transformation rules that simplify complex sentences. Information extracted from the simplified sentences is used to generate a rough storyboard and video depicting the text. Our sentence simplification module outperforms existing systems in terms of BLEU and SARI metrics.We further evaluated our system via a user study: 68% participants believe that our system generates reasonable animation from input screenplays.</abstract>
      <url hash="d2d97949">S19-1032</url>
      <doi>10.18653/v1/S19-1032</doi>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Proceedings of the 13th International Workshop on Semantic Evaluation</booktitle>
      <url hash="c6884df0">S19-2</url>
      <editor><first>Jonathan</first><last>May</last></editor>
      <editor><first>Ekaterina</first><last>Shutova</last></editor>
      <editor><first>Aurelie</first><last>Herbelot</last></editor>
      <editor><first>Xiaodan</first><last>Zhu</last></editor>
      <editor><first>Marianna</first><last>Apidianaki</last></editor>
      <editor><first>Saif M.</first><last>Mohammad</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota, USA</address>
      <month>June</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="8fc0fb1a">S19-2000</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 1: Cross-lingual Semantic Parsing with <fixed-case>UCCA</fixed-case></title>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Zohar</first><last>Aizenbud</last></author>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Elior</first><last>Sulem</last></author>
      <author><first>Ari</first><last>Rappoport</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>1–10</pages>
      <abstract>We present the SemEval 2019 shared task on Universal Conceptual Cognitive Annotation (UCCA) parsing in English, German and French, and discuss the participating systems and results. UCCA is a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. The shared task has yielded improvements over the state-of-the-art baseline in all languages and settings. Full results can be found in the task’s website <url>https://competitions.codalab.org/competitions/19160</url>.</abstract>
      <url hash="73466e5e">S19-2001</url>
      <attachment type="presentation" hash="bcbfd565">S19-2001.Presentation.pdf</attachment>
      <doi>10.18653/v1/S19-2001</doi>
      <revision id="1" href="S19-2001v1" hash="800901b3"/>
      <revision id="2" href="S19-2001v2" hash="73466e5e" date="2020-07-07">Content update to reflect a disqualified participant.</revision>
    </paper>
    <paper id="2">
      <title><fixed-case>HLT</fixed-case>@<fixed-case>SUDA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 1: <fixed-case>UCCA</fixed-case> Graph Parsing as Constituent Tree Parsing</title>
      <author><first>Wei</first><last>Jiang</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>11–15</pages>
      <abstract>This paper describes a simple UCCA semantic graph parsing approach. The key idea is to convert a UCCA semantic graph into a constituent tree, in which extra labels are deliberately designed to mark remote edges and discontinuous nodes for future recovery. In this way, we can make use of existing syntactic parsing techniques. Based on the data statistics, we recover discontinuous nodes directly according to the output labels of the constituent parser and use a biaffine classification model to recover the more complex remote edges. The classification model and the constituent parser are simultaneously trained under the multi-task learning framework. We use the multilingual BERT as extra features in the open tracks. Our system ranks the first place in the six English/German closed/open tracks among seven participating systems. For the seventh cross-lingual track, where there is little training data for French, we propose a language embedding approach to utilize English and German training data, and our result ranks the second place.</abstract>
      <url hash="d998f3a1">S19-2002</url>
      <doi>10.18653/v1/S19-2002</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 2: Unsupervised Lexical Frame Induction</title>
      <author><first>Behrang</first><last>QasemiZadeh</last></author>
      <author><first>Miriam R. L.</first><last>Petruck</last></author>
      <author><first>Regina</first><last>Stodden</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Marie</first><last>Candito</last></author>
      <pages>16–30</pages>
      <abstract>This paper presents Unsupervised Lexical Frame Induction, Task 2 of the International Workshop on Semantic Evaluation in 2019. Given a set of prespecified syntactic forms in context, the task requires that verbs and their arguments be clustered to resemble semantic frame structures. Results are useful in identifying polysemous words, i.e., those whose frame structures are not easily distinguished, as well as discerning semantic relations of the arguments. Evaluation of unsupervised frame induction methods fell into two tracks: Task A) Verb Clustering based on FrameNet 1.7; and B) Argument Clustering, with B.1) based on FrameNet’s core frame elements, and B.2) on VerbNet 3.2 semantic roles. The shared task attracted nine teams, of whom three reported promising results. This paper describes the task and its data, reports on methods and resources that these systems used, and offers a comparison to human annotation.</abstract>
      <url hash="3a330801">S19-2003</url>
      <doi>10.18653/v1/S19-2003</doi>
    </paper>
    <paper id="4">
      <title>Neural <fixed-case>GRANN</fixed-case>y at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 2: A combined approach for better modeling of semantic relationships in semantic frame induction</title>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <author><first>Boris</first><last>Sheludko</last></author>
      <author><first>Adis</first><last>Davletov</last></author>
      <author><first>Dmitry</first><last>Kharchev</last></author>
      <author><first>Alex</first><last>Nevidomsky</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>31–38</pages>
      <abstract>We describe our solutions for semantic frame and role induction subtasks of SemEval 2019 Task 2. Our approaches got the highest scores, and the solution for the frame induction problem officially took the first place. The main contributions of this paper are related to the semantic frame induction problem. We propose a combined approach that employs two different types of vector representations: dense representations from hidden layers of a masked language model, and sparse representations based on substitutes for the target word in the context. The first one better groups synonyms, the second one is better at disambiguating homonyms. Extending the context to include nearby sentences improves the results in both cases. New Hearst-like patterns for verbs are introduced that prove to be effective for frame induction. Finally, we propose an approach to selecting the number of clusters in agglomerative clustering.</abstract>
      <url hash="293e1fca">S19-2004</url>
      <doi>10.18653/v1/S19-2004</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: <fixed-case>E</fixed-case>mo<fixed-case>C</fixed-case>ontext Contextual Emotion Detection in Text</title>
      <author><first>Ankush</first><last>Chatterjee</last></author>
      <author><first>Kedhar Nath</first><last>Narahari</last></author>
      <author><first>Meghana</first><last>Joshi</last></author>
      <author><first>Puneet</first><last>Agrawal</last></author>
      <pages>39–48</pages>
      <abstract>In this paper, we present the SemEval-2019 Task 3 - EmoContext: Contextual Emotion Detection in Text. Lack of facial expressions and voice modulations make detecting emotions in text a challenging problem. For instance, as humans, on reading “Why don’t you ever text me!” we can either interpret it as a sad or angry emotion and the same ambiguity exists for machines. However, the context of dialogue can prove helpful in detection of the emotion. In this task, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes - Happy, Sad, Angry and Others. To facilitate the participation in this task, textual dialogues from user interaction with a conversational agent were taken and annotated for emotion classes after several data processing steps. A training data set of 30160 dialogues, and two evaluation data sets, Test1 and Test2, containing 2755 and 5509 dialogues respectively were released to the participants. A total of 311 teams made submissions to this task. The final leader-board was evaluated on Test2 data set, and the highest ranked submission achieved 79.59 micro-averaged F1 score. Our analysis of systems submitted to the task indicate that Bi-directional LSTM was the most common choice of neural architecture used, and most of the systems had the best performance for the Sad emotion class, and the worst for the Happy emotion class.</abstract>
      <url hash="ca8b84c8">S19-2005</url>
      <doi>10.18653/v1/S19-2005</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>ANA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Contextual Emotion detection in Conversations through hierarchical <fixed-case>LSTM</fixed-case>s and <fixed-case>BERT</fixed-case></title>
      <author><first>Chenyang</first><last>Huang</last></author>
      <author><first>Amine</first><last>Trabelsi</last></author>
      <author><first>Osmar</first><last>Zaïane</last></author>
      <pages>49–53</pages>
      <abstract>This paper describes the system submitted by ANA Team for the SemEval-2019 Task 3: EmoContext. We propose a novel Hierarchi- cal LSTMs for Contextual Emotion Detection (HRLCE) model. It classifies the emotion of an utterance given its conversational con- text. The results show that, in this task, our HRCLE outperforms the most recent state-of- the-art text classification framework: BERT. We combine the results generated by BERT and HRCLE to achieve an overall score of 0.7709 which ranked 5th on the final leader board of the competition among 165 Teams.</abstract>
      <url hash="8c794dd4">S19-2006</url>
      <doi>10.18653/v1/S19-2006</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in <fixed-case>T</fixed-case>witter</title>
      <author><first>Valerio</first><last>Basile</last></author>
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Viviana</first><last>Patti</last></author>
      <author><first>Francisco Manuel</first><last>Rangel Pardo</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <author><first>Manuela</first><last>Sanguinetti</last></author>
      <pages>54–63</pages>
      <abstract>The paper describes the organization of the SemEval 2019 Task 5 about the detection of hate speech against immigrants and women in Spanish and English messages extracted from Twitter. The task is organized in two related classification subtasks: a main binary subtask for detecting the presence of hate speech, and a finer-grained one devoted to identifying further features in hateful contents such as the aggressive attitude and the target harassed, to distinguish if the incitement is against an individual rather than a group. HatEval has been one of the most popular tasks in SemEval-2019 with a total of 108 submitted runs for Subtask A and 70 runs for Subtask B, from a total of 74 different teams. Data provided for the task are described by showing how they have been collected and annotated. Moreover, the paper provides an analysis and discussion about the participant systems and the results they achieved in both subtasks.</abstract>
      <url hash="fb60c578">S19-2007</url>
      <doi>10.18653/v1/S19-2007</doi>
    </paper>
    <paper id="8">
      <title>Atalaya at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2019 Task 5: Robust Embeddings for Tweet Classification</title>
      <author><first>Juan Manuel</first><last>Pérez</last></author>
      <author><first>Franco M.</first><last>Luque</last></author>
      <pages>64–69</pages>
      <abstract>In this article, we describe our participation in HatEval, a shared task aimed at the detection of hate speech against immigrants and women. We focused on Spanish subtasks, building from our previous experiences on sentiment analysis in this language. We trained linear classifiers and Recurrent Neural Networks, using classic features, such as bag-of-words, bag-of-characters, and word embeddings, and also with recent techniques such as contextualized word representations. In particular, we trained robust task-oriented subword-aware embeddings and computed tweet representations using a weighted-averaging strategy. In the final evaluation, our systems showed competitive results for both Spanish subtasks ES-A and ES-B, achieving the first and fourth places respectively.</abstract>
      <url hash="4983d7b5">S19-2008</url>
      <doi>10.18653/v1/S19-2008</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>FERMI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in <fixed-case>T</fixed-case>witter</title>
      <author><first>Vijayasaradhi</first><last>Indurthi</last></author>
      <author><first>Bakhtiyar</first><last>Syed</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Nikhil</first><last>Chakravartula</last></author>
      <author><first>Manish</first><last>Gupta</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <pages>70–74</pages>
      <abstract>This paper describes our system (Fermi) for Task 5 of SemEval-2019: HatEval: Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter. We participated in the subtask A for English and ranked first in the evaluation on the test set. We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team - Fermi’s model achieved an accuracy of 65.00% for English language in task A. Our models, which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task.</abstract>
      <url hash="4b28d222">S19-2009</url>
      <doi>10.18653/v1/S19-2009</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (<fixed-case>O</fixed-case>ffens<fixed-case>E</fixed-case>val)</title>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Shervin</first><last>Malmasi</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Sara</first><last>Rosenthal</last></author>
      <author><first>Noura</first><last>Farra</last></author>
      <author><first>Ritesh</first><last>Kumar</last></author>
      <pages>75–86</pages>
      <abstract>We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.</abstract>
      <url hash="752ced44">S19-2010</url>
      <doi>10.18653/v1/S19-2010</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>NULI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Transfer Learning for Offensive Language Detection using Bidirectional Transformers</title>
      <author><first>Ping</first><last>Liu</last></author>
      <author><first>Wen</first><last>Li</last></author>
      <author><first>Liang</first><last>Zou</last></author>
      <pages>87–91</pages>
      <abstract>Transfer learning and domain adaptive learning have been applied to various fields including computer vision (e.g., image recognition) and natural language processing (e.g., text classification). One of the benefits of transfer learning is to learn effectively and efficiently from limited labeled data with a pre-trained model. In the shared task of identifying and categorizing offensive language in social media, we preprocess the dataset according to the language behaviors on social media, and then adapt and fine-tune the Bidirectional Encoder Representation from Transformer (BERT) pre-trained by Google AI Language team. Our team NULI wins the first place (1st) in Sub-task A - Offensive Language Identification and is ranked 4th and 18th in Sub-task B - Automatic Categorization of Offense Types and Sub-task C - Offense Target Identification respectively.</abstract>
      <url hash="50f91936">S19-2011</url>
      <doi>10.18653/v1/S19-2011</doi>
    </paper>
    <paper id="12">
      <title><fixed-case>CUNY</fixed-case>-<fixed-case>PKU</fixed-case> Parser at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 1: Cross-Lingual Semantic Parsing with <fixed-case>UCCA</fixed-case></title>
      <author><first>Weimin</first><last>Lyu</last></author>
      <author><first>Sheng</first><last>Huang</last></author>
      <author><first>Abdul Rafae</first><last>Khan</last></author>
      <author><first>Shengqiang</first><last>Zhang</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <author><first>Jia</first><last>Xu</last></author>
      <pages>92–96</pages>
      <abstract>This paper describes the systems of the CUNY-PKU team in SemEval 2019 Task 1: Cross-lingual Semantic Parsing with UCCA. We introduce a novel model by applying a cascaded MLP and BiLSTM model. Then, we ensemble multiple system-outputs by reparsing. In particular, we introduce a new decoding algorithm for building the UCCA representation. Our system won the first place in one track (French-20K-Open), second places in four tracks (English-Wiki-Open, English-20K-Open, German-20K-Open, and German-20K-Closed), and third place in one track (English-20K-Closed), among all seven tracks.</abstract>
      <url hash="2868bb32">S19-2012</url>
      <doi>10.18653/v1/S19-2012</doi>
      <revision id="1" href="S19-2012v1" hash="2bb85903"/>
      <revision id="2" href="S19-2012v2" hash="2868bb32" date="2020-07-07">Authors used illegal training data in evaluation</revision>
    </paper>
    <paper id="13">
      <title><fixed-case>DANGNT</fixed-case>@<fixed-case>UIT</fixed-case>.<fixed-case>VNU</fixed-case>-<fixed-case>HCM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2019 Task 1: Graph Transformation System from <fixed-case>S</fixed-case>tanford Basic Dependencies to <fixed-case>U</fixed-case>niversal <fixed-case>C</fixed-case>onceptual <fixed-case>C</fixed-case>ognitive <fixed-case>A</fixed-case>nnotation (<fixed-case>UCCA</fixed-case>)</title>
      <author><first>Dang</first><last>Tuan Nguyen</last></author>
      <author><first>Trung</first><last>Tran</last></author>
      <pages>97–101</pages>
      <abstract>This paper describes the graph transfor-mation system (GT System) for SemEval 2019 Task 1: Cross-lingual Semantic Parsing with Universal Conceptual Cognitive Annotation (UCCA)1. The input of GT System is a pair of text and its unannotated xml, which is a layer 0 part of UCCA form. The output of GT System is the corresponding full UCCA xml. Based on the idea of graph illustration and transformation, we perform four main tasks when building GT System. At the first task, we illustrate the graph form of stanford dependencies2 of input text. We then transform into an intermediate graph in the second task. At the third task, we continue to transform into ouput graph form. Finally, we create the output UCCA xml. The evaluation results show that our method generates good-quality UCCA xml and has a meaningful contribution to the semantic represetation sub-field in Natural Language Processing.</abstract>
      <url hash="c80dfdfd">S19-2013</url>
      <doi>10.18653/v1/S19-2013</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>GCN</fixed-case>-Sem at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 1: Semantic Parsing using Graph Convolutional and Recurrent Neural Networks</title>
      <author><first>Shiva</first><last>Taslimipoor</last></author>
      <author><first>Omid</first><last>Rohanian</last></author>
      <author><first>Sara</first><last>Može</last></author>
      <pages>102–106</pages>
      <abstract>This paper describes the system submitted to the SemEval 2019 shared task 1 ‘Cross-lingual Semantic Parsing with UCCA’. We rely on the semantic dependency parse trees provided in the shared task which are converted from the original UCCA files and model the task as tagging. The aim is to predict the graph structure of the output along with the types of relations among the nodes. Our proposed neural architecture is composed of Graph Convolution and BiLSTM components. The layers of the system share their weights while predicting dependency links and semantic labels. The system is applied to the CONLLU format of the input data and is best suited for semantic dependency parsing.</abstract>
      <url hash="d45be4b3">S19-2014</url>
      <doi>10.18653/v1/S19-2014</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>M</fixed-case>ask<fixed-case>P</fixed-case>arse@Deskin at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 1: Cross-lingual <fixed-case>UCCA</fixed-case> Semantic Parsing using Recursive Masked Sequence Tagging</title>
      <author><first>Gabriel</first><last>Marzinotto</last></author>
      <author><first>Johannes</first><last>Heinecke</last></author>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <pages>107–112</pages>
      <abstract>This paper describes our recursive system for SemEval-2019 Task 1: Cross-lingual Semantic Parsing with UCCA. Each recursive step consists of two parts. We first perform semantic parsing using a sequence tagger to estimate the probabilities of the UCCA categories in the sentence. Then, we apply a decoding policy which interprets these probabilities and builds the graph nodes. Parsing is done recursively, we perform a first inference on the sentence to extract the main scenes and links and then we recursively apply our model on the sentence using a masking features that reflects the decisions made in previous steps. Process continues until the terminal nodes are reached. We chose a standard neural tagger and we focus on our recursive parsing strategy and on the cross lingual transfer problem to develop a robust model for the French language, using only few training samples</abstract>
      <url hash="14b9d317">S19-2015</url>
      <doi>10.18653/v1/S19-2015</doi>
    </paper>
    <paper id="16">
      <title>Tüpa at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task1: (Almost) feature-free Semantic Parsing</title>
      <author><first>Tobias</first><last>Pütz</last></author>
      <author><first>Kevin</first><last>Glocker</last></author>
      <pages>113–118</pages>
      <abstract>Our submission for Task 1 ‘Cross-lingual Semantic Parsing with UCCA’ at SemEval-2018 is a feed-forward neural network that builds upon an existing state-of-the-art transition-based directed acyclic graph parser. We replace most of its features by deep contextualized word embeddings and introduce an approximation to represent non-terminal nodes in the graph as an aggregation of their terminal children. We further demonstrate how augmenting data using the baseline systems provides a consistent advantage in all open submission tracks. We submitted results to all open tracks (English, in- and out-of-domain, German in-domain and French in-domain, low-resource). Our system achieves competitive performance in all settings besides the French, where we did not augment the data. Post-evaluation experiments showed that data augmentation is especially crucial in this setting.</abstract>
      <url hash="cf856295">S19-2016</url>
      <attachment type="supplementary" hash="f05d05e2">S19-2016.Supplementary.pdf</attachment>
      <doi>10.18653/v1/S19-2016</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>UC</fixed-case> <fixed-case>D</fixed-case>avis at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 1: <fixed-case>DAG</fixed-case> Semantic Parsing with Attention-based Decoder</title>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Kenji</first><last>Sagae</last></author>
      <pages>119–124</pages>
      <abstract>We present an encoder-decoder model for semantic parsing with UCCA SemEval 2019 Task 1. The encoder is a Bi-LSTM and the decoder uses recursive self-attention. The proposed model alleviates challenges and feature engineering in traditional transition-based and graph-based parsers. The resulting parser is simple and proved to effective on the semantic parsing task.</abstract>
      <url hash="e84d7927">S19-2017</url>
      <doi>10.18653/v1/S19-2017</doi>
    </paper>
    <paper id="18">
      <title><fixed-case>HHMM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 2: Unsupervised Frame Induction using Contextualized Word Embeddings</title>
      <author><first>Saba</first><last>Anwar</last></author>
      <author><first>Dmitry</first><last>Ustalov</last></author>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>125–129</pages>
      <abstract>We present our system for semantic frame induction that showed the best performance in Subtask B.1 and finished as the runner-up in Subtask A of the SemEval 2019 Task 2 on unsupervised semantic frame induction (Qasem-iZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages.</abstract>
      <url hash="49ac0d0a">S19-2018</url>
      <doi>10.18653/v1/S19-2018</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>L</fixed-case>2<fixed-case>F</fixed-case>/<fixed-case>INESC</fixed-case>-<fixed-case>ID</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 2: Unsupervised Lexical Semantic Frame Induction using Contextualized Word Representations</title>
      <author><first>Eugénio</first><last>Ribeiro</last></author>
      <author><first>Vânia</first><last>Mendonça</last></author>
      <author><first>Ricardo</first><last>Ribeiro</last></author>
      <author><first>David</first><last>Martins de Matos</last></author>
      <author><first>Alberto</first><last>Sardinha</last></author>
      <author><first>Ana Lúcia</first><last>Santos</last></author>
      <author><first>Luísa</first><last>Coheur</last></author>
      <pages>130–136</pages>
      <abstract>Building large datasets annotated with semantic information, such as FrameNet, is an expensive process. Consequently, such resources are unavailable for many languages and specific domains. This problem can be alleviated by using unsupervised approaches to induce the frames evoked by a collection of documents. That is the objective of the second task of SemEval 2019, which comprises three subtasks: clustering of verbs that evoke the same frame and clustering of arguments into both frame-specific slots and semantic roles. We approach all the subtasks by applying a graph clustering algorithm on contextualized embedding representations of the verbs and arguments. Using such representations is appropriate in the context of this task, since they provide cues for word-sense disambiguation. Thus, they can be used to identify different frames evoked by the same words. Using this approach we were able to outperform all of the baselines reported for the task on the test set in terms of Purity F1, as well as in terms of BCubed F1 in most cases.</abstract>
      <url hash="7a9c26fe">S19-2019</url>
      <doi>10.18653/v1/S19-2019</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>B</fixed-case>rain<fixed-case>EE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Ensembling Linear Classifiers for Emotion Prediction</title>
      <author><first>Vachagan</first><last>Gratian</last></author>
      <pages>137–141</pages>
      <abstract>The paper describes an ensemble of linear perceptrons trained for emotion classification as part of the SemEval-2019 shared-task 3. The model uses a matrix of probabilities to weight the activations of the base-classifiers and makes a final prediction using the sum rule. The base-classifiers are multi-class perceptrons utilizing character and word n-grams, part-of-speech tags and sentiment polarity scores. The results of our experiments indicate that the ensemble outperforms the base-classifiers, but only marginally. In the best scenario our model attains an F-Micro score of 0.672, whereas the base-classifiers attained scores ranging from 0.636 to 0.666.</abstract>
      <url hash="e5618f72">S19-2020</url>
      <doi>10.18653/v1/S19-2020</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>CA</fixed-case>i<fixed-case>RE</fixed-case>_<fixed-case>HKUST</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Hierarchical Attention for Dialogue Emotion Classification</title>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Zhaojiang</first><last>Lin</last></author>
      <author><first>Jamin</first><last>Shin</last></author>
      <author><first>Yan</first><last>Xu</last></author>
      <author><first>Peng</first><last>Xu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>142–147</pages>
      <abstract>Detecting emotion from dialogue is a challenge that has not yet been extensively surveyed. One could consider the emotion of each dialogue turn to be independent, but in this paper, we introduce a hierarchical approach to classify emotion, hypothesizing that the current emotional state depends on previous latent emotions. We benchmark several feature-based classifiers using pre-trained word and emotion embeddings, state-of-the-art end-to-end neural network models, and Gaussian processes for automatic hyper-parameter search. In our experiments, hierarchical architectures consistently give significant improvements, and our best model achieves a 76.77% F1-score on the test set.</abstract>
      <url hash="e115bd37">S19-2021</url>
      <revision id="1" href="S19-2021v1" hash="5238764f"/>
      <revision id="2" href="S19-2021v2" hash="e115bd37">We corrected a reference by adding a booktitle to fix the bibliography.Before: Pascale Fung, Dario Bertero, Peng Xu, Ji Ho Park, Chien-Sheng Wu, and Andrea Madotto. 2018. Empathetic dialog systems.After: Pascale Fung, Dario Bertero, Peng Xu, Ji Ho Park, Chien-Sheng Wu, and Andrea Madotto. 2018. Empathetic dialog systems. In The International Conference on Language Resources and Evaluation. European Language Resources Association.</revision>
      <doi>10.18653/v1/S19-2021</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>CECL</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Using Surface Learning for Detecting Emotion in Textual Conversations</title>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>148–152</pages>
      <abstract>This paper describes the system developed by the Centre for English Corpus Linguistics for the SemEval-2019 Task 3: EmoContext. It aimed at classifying the emotion of a user utterance in a textual conversation as happy, sad, angry or other. It is based on a large number of feature types, mainly unigrams and bigrams, which were extracted by a SAS program. The usefulness of the different feature types was evaluated by means of Monte-Carlo resampling tests. As this system does not rest on any deep learning component, which is currently considered as the state-of-the-art approach, it can be seen as a possible point of comparison for such kind of systems.</abstract>
      <url hash="77c50ab1">S19-2022</url>
      <doi>10.18653/v1/S19-2022</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>CL</fixed-case>a<fixed-case>C</fixed-case> Lab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Contextual Emotion Detection Using a Combination of Neural Networks and <fixed-case>SVM</fixed-case></title>
      <author><first>Elham</first><last>Mohammadi</last></author>
      <author><first>Hessam</first><last>Amini</last></author>
      <author><first>Leila</first><last>Kosseim</last></author>
      <pages>153–158</pages>
      <abstract>This paper describes our system at SemEval 2019, Task 3 (EmoContext), which focused on the contextual detection of emotions in a dataset of 3-round dialogues. For our final system, we used a neural network with pretrained ELMo word embeddings and POS tags as input, GRUs as hidden units, an attention mechanism to capture representations of the dialogues, and an SVM classifier which used the learned network representations to perform the task of multi-class classification.This system yielded a micro-averaged F1 score of 0.7072 for the three emotion classes, improving the baseline by approximately 12%.</abstract>
      <url hash="0410d267">S19-2023</url>
      <doi>10.18653/v1/S19-2023</doi>
    </paper>
    <paper id="24">
      <title><fixed-case>CLARK</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Exploring the Role of Context to Identify Emotion in a Short Conversation</title>
      <author><first>Joseph</first><last>Cummings</last></author>
      <author><first>Jason</first><last>Wilson</last></author>
      <pages>159–163</pages>
      <abstract>With text lacking valuable information avail-able in other modalities, context may provide useful information to better detect emotions. In this paper, we do a systematic exploration of the role of context in recognizing emotion in a conversation. We use a Naive Bayes model to show that inferring the mood of the conversation before classifying individual utterances leads to better performance. Additionally, we find that using context while train-ing the model significantly decreases performance. Our approach has the additional bene-fit that its performance rivals a baseline LSTM model while requiring fewer resources.</abstract>
      <url hash="7793352c">S19-2024</url>
      <doi>10.18653/v1/S19-2024</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>CLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Multi-Encoder in Hierarchical Attention Networks for Contextual Emotion Detection</title>
      <author><first>Changjie</first><last>Li</last></author>
      <author><first>Yun</first><last>Xing</last></author>
      <pages>164–168</pages>
      <abstract>In this paper, we describe the participation of team ”CLP” in SemEval-2019 Task 3 “Con- textual Emotion Detection in Text” that aims to classify emotion of user utterance in tex- tual conversation. The submitted system is a deep learning architecture based on Hier- archical Attention Networks (HAN) and Em- bedding from Language Model (ELMo). The core of the architecture contains two represen- tation layers. The first one combines the out- puts of ELMo, hand-craft features and Bidi- rectional Long Short-Term Memory with At- tention (Bi-LSTM-Attention) to represent user utterance. The second layer use a Bi-LSTM- Attention encoder to represent the conversa- tion. Our system achieved F1 score of 0.7524 which outperformed the baseline model of the organizers by 0.1656.</abstract>
      <url hash="b51d38ba">S19-2025</url>
      <doi>10.18653/v1/S19-2025</doi>
    </paper>
    <paper id="26">
      <title><fixed-case>C</fixed-case>o<fixed-case>AS</fixed-case>ta<fixed-case>L</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Affect Classification in Dialogue using Attentive <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case>s</title>
      <author><first>Ana Valeria</first><last>González</last></author>
      <author><first>Victor</first><last>Petrén Bach Hansen</last></author>
      <author><first>Joachim</first><last>Bingel</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>169–174</pages>
      <abstract>This work describes the system presented by the CoAStaL Natural Language Processing group at University of Copenhagen. The main system we present uses the same attention mechanism presented in (Yang et al., 2016). Our overall model architecture is also inspired by their hierarchical classification model and adapted to deal with classification in dialogue by encoding information at the turn level. We use different encodings for each turn to create a more expressive representation of dialogue context which is then fed into our classifier.We also define a custom preprocessing step in order to deal with language commonly used in interactions across many social media outlets. Our proposed system achieves a micro F1 score of 0.7340 on the test set and shows significant gains in performance compared to a system using dialogue level encoding.</abstract>
      <url hash="a92b0b56">S19-2026</url>
      <revision id="1" href="S19-2026v1" hash="dcce302d"/>
      <revision id="2" href="S19-2026v2" hash="a92b0b56">No description of the changes were recorded.</revision>
      <doi>10.18653/v1/S19-2026</doi>
    </paper>
    <paper id="27">
      <title><fixed-case>C</fixed-case>on<fixed-case>SSED</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Configurable Semantic and Sentiment Emotion Detector</title>
      <author><first>Rafał</first><last>Poświata</last></author>
      <pages>175–179</pages>
      <abstract>This paper describes our system participating in the SemEval-2019 Task 3: EmoContext: Contextual Emotion Detection in Text. The goal was to for a given textual dialogue, i.e. a user utterance along with two turns of context, identify the emotion of user utterance as one of the emotion classes: Happy, Sad, Angry or Others. Our system: ConSSED is a configurable combination of semantic and sentiment neural models. The official task submission achieved a micro-average F1 score of 75.31 which placed us 16th out of 165 participating systems.</abstract>
      <url hash="798148c9">S19-2027</url>
      <doi>10.18653/v1/S19-2027</doi>
    </paper>
    <paper id="28">
      <title><fixed-case>CX</fixed-case>-<fixed-case>ST</fixed-case>-<fixed-case>RNM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Fusion of Recurrent Neural Networks Based on Contextualized and Static Word Representations for Contextual Emotion Detection</title>
      <author><first>Michał</first><last>Perełkiewicz</last></author>
      <pages>180–184</pages>
      <abstract>In this paper, I describe a fusion model combining contextualized and static word representations for approaching the EmoContext task in the SemEval 2019 competition. The model is based on two Recurrent Neural Networks, the first one is fed with a state-of-the-art ELMo deep contextualized word representation and the second one is fed with a static Word2Vec embedding augmented with 10-dimensional affective word feature vector. The proposed model is compared with two baseline models based on a static word representation and a contextualized word representation, separately. My approach achieved officially 0.7278 microaveraged F1 score on the test dataset, ranking 47th out of 165 participants.</abstract>
      <url hash="3b122af4">S19-2028</url>
      <doi>10.18653/v1/S19-2028</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>P</fixed-case>arallel<fixed-case>D</fixed-case>ots at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Domain Adaptation with feature embeddings for Contextual Emotion Analysis</title>
      <author><first>Akansha</first><last>Jain</last></author>
      <author><first>Ishita</first><last>Aggarwal</last></author>
      <author><first>Ankit</first><last>Singh</last></author>
      <pages>185–189</pages>
      <abstract>This paper describes our proposed system &amp; experiments performed to detect contextual emotion in texts for SemEval 2019 Task 3. We exploit sentiment information, syntactic patterns &amp; semantic relatedness to capture diverse aspects of the text. Word level embeddings such as Glove, FastText, Emoji along with sentence level embeddings like Skip-Thought, DeepMoji &amp; Unsupervised Sentiment Neuron were used as input features to our architecture. We democratize the learning using ensembling of models with different parameters to produce the final output. This paper discusses comparative analysis of the significance of these embeddings and our approach for the task.</abstract>
      <url hash="ff16b71a">S19-2029</url>
      <doi>10.18653/v1/S19-2029</doi>
    </paper>
    <paper id="30">
      <title><fixed-case>E</fixed-case>-<fixed-case>LSTM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Semantic and Sentimental Features Retention for Emotion Detection in Text</title>
      <author><first>Harsh</first><last>Patel</last></author>
      <pages>190–194</pages>
      <abstract>This paper discusses the solution to the problem statement of the SemEval19: EmoContext competition which is ”Contextual Emotion Detection in Texts”. The paper includes the explanation of an architecture that I created by exploiting the embedding layers of Word2Vec and GloVe using LSTMs as memory unit cells which detects approximate emotion of chats between two people in the English language provided in the textual form. The set of emotions on which the model was trained was Happy, Sad, Angry and Others. The paper also includes an analysis of different conventional machine learning algorithms in comparison to E-LSTM.</abstract>
      <url hash="2a9e70cf">S19-2030</url>
      <doi>10.18653/v1/S19-2030</doi>
    </paper>
    <paper id="31">
      <title><fixed-case>EL</fixed-case>i<fixed-case>RF</fixed-case>-<fixed-case>UPV</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Snapshot Ensemble of Hierarchical Convolutional Neural Networks for Contextual Emotion Detection</title>
      <author><first>José-Ángel</first><last>González</last></author>
      <author><first>Lluís-F.</first><last>Hurtado</last></author>
      <author><first>Ferran</first><last>Pla</last></author>
      <pages>195–199</pages>
      <abstract>This paper describes the approach developed by the ELiRF-UPV team at SemEval 2019 Task 3: Contextual Emotion Detection in Text. We have developed a Snapshot Ensemble of 1D Hierarchical Convolutional Neural Networks to extract features from 3-turn conversations in order to perform contextual emotion detection in text. This Snapshot Ensemble is obtained by averaging the models selected by a Genetic Algorithm that optimizes the evaluation measure. The proposed ensemble obtains better results than a single model and it obtains competitive and promising results on Contextual Emotion Detection in Text.</abstract>
      <url hash="bc7fa2a2">S19-2031</url>
      <doi>10.18653/v1/S19-2031</doi>
    </paper>
    <paper id="32">
      <title><fixed-case>E</fixed-case>mo<fixed-case>D</fixed-case>et at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Emotion Detection in Text using Deep Learning</title>
      <author><first>Hani</first><last>Al-Omari</last></author>
      <author><first>Malak</first><last>Abdullah</last></author>
      <author><first>Nabeel</first><last>Bassam</last></author>
      <pages>200–204</pages>
      <abstract>Task 3, EmoContext, in the International Workshop SemEval 2019 provides training and testing datasets for the participant teams to detect emotion classes (Happy, Sad, Angry, or Others). This paper proposes a participating system (EmoDet) to detect emotions using deep learning architecture. The main input to the system is a combination of Word2Vec word embeddings and a set of semantic features (e.g. from AffectiveTweets Weka-package). The proposed system (EmoDet) ensembles a fully connected neural network architecture and LSTM neural network to obtain performance results that show substantial improvements (F1-Score 0.67) over the baseline model provided by Task 3 organizers (F1-score 0.58).</abstract>
      <url hash="7aa80730">S19-2032</url>
      <doi>10.18653/v1/S19-2032</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>EMOMINER</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: A Stacked <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> Architecture for Contextual Emotion Detection in Text</title>
      <author><first>Nikhil</first><last>Chakravartula</last></author>
      <author><first>Vijayasaradhi</first><last>Indurthi</last></author>
      <pages>205–209</pages>
      <abstract>This paper describes our participation in the SemEval 2019 Task 3 - Contextual Emotion Detection in Text. This task aims to identify emotions, viz. happiness, anger, sadness in the context of a text conversation. Our system is a stacked Bidirectional LSTM, equipped with attention on top of word embeddings pre-trained on a large collection of Twitter data. In this paper, apart from describing our official submission, we elucidate how different deep learning models respond to this task.</abstract>
      <url hash="c1b89edb">S19-2033</url>
      <doi>10.18653/v1/S19-2033</doi>
    </paper>
    <paper id="34">
      <title><fixed-case>E</fixed-case>mo<fixed-case>S</fixed-case>ense at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Bidirectional <fixed-case>LSTM</fixed-case> Network for Contextual Emotion Detection in Textual Conversations</title>
      <author><first>Sergey</first><last>Smetanin</last></author>
      <pages>210–214</pages>
      <abstract>In this paper, we describe a deep-learning system for emotion detection in textual conversations that participated in SemEval-2019 Task 3 “EmoContext”. We designed a specific architecture of bidirectional LSTM which allows not only to learn semantic and sentiment feature representation, but also to capture user-specific conversation features. To fine-tune word embeddings using distant supervision we additionally collected a significant amount of emotional texts. The system achieved 72.59% micro-average F1 score for emotion classes on the test dataset, thereby significantly outperforming the officially-released baseline. Word embeddings and the source code were released for the research community.</abstract>
      <url hash="2a66dec5">S19-2034</url>
      <doi>10.18653/v1/S19-2034</doi>
    </paper>
    <paper id="35">
      <title><fixed-case>EPITA</fixed-case>-<fixed-case>ADAPT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Detecting emotions in textual conversations using deep learning models combination</title>
      <author><first>Abdessalam</first><last>Bouchekif</last></author>
      <author><first>Praveen</first><last>Joshi</last></author>
      <author><first>Latifa</first><last>Bouchekif</last></author>
      <author><first>Haithem</first><last>Afli</last></author>
      <pages>215–219</pages>
      <abstract>Messaging platforms like WhatsApp, Facebook Messenger and Twitter have gained recently much popularity owing to their ability in connecting users in real-time. The content of these textual messages can be a useful resource for text mining to discover and unhide various aspects, including emotions. In this paper we present our submission for SemEval 2019 task ‘EmoContext’. The task consists of classifying a given textual dialogue into one of four emotion classes: Angry, Happy, Sad and Others. Our proposed system is based on the combination of different deep neural networks techniques. In particular, we use Recurrent Neural Networks (LSTM, B-LSTM, GRU, B-GRU), Convolutional Neural Network (CNN) and Transfer Learning (TL) methodes. Our final system, achieves an F1 score of 74.51% on the subtask evaluation dataset.</abstract>
      <url hash="4b26a04a">S19-2035</url>
      <doi>10.18653/v1/S19-2035</doi>
    </paper>
    <paper id="36">
      <title>Figure Eight at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Ensemble of Transfer Learning Methods for Contextual Emotion Detection</title>
      <author><first>Joan</first><last>Xiao</last></author>
      <pages>220–224</pages>
      <abstract>This paper describes our transfer learning-based approach to contextual emotion detection as part of SemEval-2019 Task 3. We experiment with transfer learning using pre-trained language models (ULMFiT, OpenAI GPT, and BERT) and fine-tune them on this task. We also train a deep learning model from scratch using pre-trained word embeddings and BiLSTM architecture with attention mechanism. The ensembled model achieves competitive result, ranking ninth out of 165 teams. The result reveals that ULMFiT performs best due to its superior fine-tuning techniques. We propose improvements for future work.</abstract>
      <url hash="e67e7db3">S19-2036</url>
      <doi>10.18653/v1/S19-2036</doi>
    </paper>
    <paper id="37">
      <title><fixed-case>G</fixed-case>en<fixed-case>SMT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Contextual Emotion Detection in tweets using multi task generic approach</title>
      <author><first>Dumitru</first><last>Bogdan</last></author>
      <pages>225–229</pages>
      <abstract>In this paper, we describe our participation in SemEval-2019 Task 3: EmoContext - A Shared Task on Contextual Emotion Detection in Text. We propose a three layer model with a generic, multi-purpose approach that without any task specific optimizations achieve competitive results (f1 score of 0.7096) in the EmoContext task. We describe our development strategy in detail along with an exposition of our results.</abstract>
      <url hash="d28d0960">S19-2037</url>
      <doi>10.18653/v1/S19-2037</doi>
    </paper>
    <paper id="38">
      <title><fixed-case>GWU</fixed-case> <fixed-case>NLP</fixed-case> Lab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3 : <fixed-case>E</fixed-case>mo<fixed-case>C</fixed-case>ontext: Effectiveness of<fixed-case>C</fixed-case>ontextual Information in Models for Emotion Detection in<fixed-case>S</fixed-case>entence-level at Multi-genre Corpus</title>
      <author><first>Shabnam</first><last>Tafreshi</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>230–235</pages>
      <abstract>In this paper we present an emotion classifier models that submitted to the SemEval-2019 Task 3 : <i>EmoContext</i>. Our approach is a Gated Recurrent Neural Network (GRU) model with attention layer is bootstrapped with contextual information and trained with a multigenre corpus, which is combination of several popular emotional data sets. We utilize different word embeddings to empirically select the most suited embedding to represent our features. Our aim is to build a robust emotion classifier that can generalize emotion detection, which is to learn emotion cues in a noisy training environment. To fulfill this aim we train our model with a multigenre emotion corpus, this way we leverage from having more training set. We achieved overall %56.05 f1-score and placed 144. Given our aim and noisy training environment, the results are anticipated.</abstract>
      <url hash="f842f76c">S19-2038</url>
      <doi>10.18653/v1/S19-2038</doi>
    </paper>
    <paper id="39">
      <title><fixed-case>IIT</fixed-case> <fixed-case>G</fixed-case>andhinagar at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Contextual Emotion Detection Using Deep Learning</title>
      <author><first>Arik</first><last>Pamnani</last></author>
      <author><first>Rajat</first><last>Goel</last></author>
      <author><first>Jayesh</first><last>Choudhari</last></author>
      <author><first>Mayank</first><last>Singh</last></author>
      <pages>236–240</pages>
      <abstract>Recent advancements in Internet and Mobile infrastructure have resulted in the development of faster and efficient platforms of communication. These platforms include speech, facial and text-based conversational mediums. Majority of these are text-based messaging platforms. Development of Chatbots that automatically understand latent emotions in the textual message is a challenging task. In this paper, we present an automatic emotion detection system that aims to detect the emotion of a person textually conversing with a chatbot. We explore deep learning techniques such as CNN and LSTM based neural networks and outperformed the baseline score by 14%. The trained model and code are kept in public domain.</abstract>
      <url hash="6710cbb0">S19-2039</url>
      <doi>10.18653/v1/S19-2039</doi>
    </paper>
    <paper id="40">
      <title><fixed-case>KGPC</fixed-case>hamps at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: A deep learning approach to detect emotions in the dialog utterances.</title>
      <author><first>Jasabanta</first><last>Patro</last></author>
      <author><first>Nitin</first><last>Choudhary</last></author>
      <author><first>Kalpit</first><last>Chittora</last></author>
      <author><first>Animesh</first><last>Mukherjee</last></author>
      <pages>241–246</pages>
      <abstract>This paper describes our approach to solve <i>Semeval task 3: EmoContext</i>; where, given a textual dialogue i.e. a user utterance along with two turns of context, we have to classify the emotion associated with the utterance as one of the following emotion classes: <i>Happy, Sad, Angry</i> or <i>Others</i>. To solve this problem, we experiment with different deep learning models ranging from simple bidirectional LSTM (Long and short term memory) model to comparatively complex attention model. We also experiment with word embedding conceptnet along with word embedding generated from bi-directional LSTM taking input characters. We fine-tune different parameters and hyper-parameters associated with each of our models and report the value of evaluating measure i.e. micro precision along with class wise precision, recall and F1-score of each system. We report the bidirectional LSTM model, along with the input word embedding as the concatenation of word embedding generated from bidirectional LSTM for word characters and conceptnet embedding, as the best performing model with a highest micro-F1 score of 0.7261. We also report class wise precision, recall, and f1-score of best performing model along with other models that we have experimented with.</abstract>
      <url hash="6bd6e274">S19-2040</url>
      <doi>10.18653/v1/S19-2040</doi>
    </paper>
    <paper id="41">
      <title><fixed-case>KSU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Hybrid Features for Emotion Recognition in Textual Conversation</title>
      <author><first>Nourah</first><last>Alswaidan</last></author>
      <author><first>Mohamed El Bachir</first><last>Menai</last></author>
      <pages>247–250</pages>
      <abstract>We proposed a model to address emotion recognition in textual conversation based on using automatically extracted features and human engineered features. The proposed model utilizes a fast gated-recurrent-unit backed by CuDNN, and a convolutional neural network to automatically extract features. The human engineered features take the frequency-inverse document frequency of semantic meaning and mood tags extracted from SinticNet.</abstract>
      <url hash="f76a3d26">S19-2041</url>
      <doi>10.18653/v1/S19-2041</doi>
    </paper>
    <paper id="42">
      <title><fixed-case>LIRMM</fixed-case>-Advanse at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Attentive Conversation Modeling for Emotion Detection and Classification</title>
      <author><first>Waleed</first><last>Ragheb</last></author>
      <author><first>Jérôme</first><last>Azé</last></author>
      <author><first>Sandra</first><last>Bringay</last></author>
      <author><first>Maximilien</first><last>Servajean</last></author>
      <pages>251–255</pages>
      <abstract>This paper addresses the problem of modeling textual conversations and detecting emotions. Our proposed model makes use of 1) deep transfer learning rather than the classical shallow methods of word embedding; 2) self-attention mechanisms to focus on the most important parts of the texts and 3) turn-based conversational modeling for classifying the emotions. The approach does not rely on any hand-crafted features or lexicons. Our model was evaluated on the data provided by the SemEval-2019 shared task on contextual emotion detection in text. The model shows very competitive results.</abstract>
      <url hash="06ee0a13">S19-2042</url>
      <doi>10.18653/v1/S19-2042</doi>
    </paper>
    <paper id="43">
      <title><fixed-case>MILAB</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Multi-View Turn-by-Turn Model for Context-Aware Sentiment Analysis</title>
      <author><first>Yoonhyung</first><last>Lee</last></author>
      <author><first>Yanghoon</first><last>Kim</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>256–260</pages>
      <abstract>This paper describes our system for SemEval-2019 Task 3: EmoContext, which aims to predict the emotion of the third utterance considering two preceding utterances in a dialogue. To address this challenge of predicting the emotion considering its context, we propose a Multi-View Turn-by-Turn (MVTT) model. Firstly, MVTT model generates vectors from each utterance using two encoders: word-level Bi-GRU encoder (WLE) and character-level CNN encoder (CLE). Then, MVTT grasps contextual information by combining the vectors and predict the emotion with the contextual information. We conduct experiments on the effect of vector encoding and vector combination. Our final MVTT model achieved 0.7634 microaveraged F1 score.</abstract>
      <url hash="635b53ed">S19-2043</url>
      <doi>10.18653/v1/S19-2043</doi>
    </paper>
    <paper id="44">
      <title><fixed-case>M</fixed-case>oon<fixed-case>G</fixed-case>rad at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Ensemble <fixed-case>B</fixed-case>i<fixed-case>RNN</fixed-case>s for Contextual Emotion Detection in Dialogues</title>
      <author><first>Chandrakant</first><last>Bothe</last></author>
      <author><first>Stefan</first><last>Wermter</last></author>
      <pages>261–265</pages>
      <abstract>When reading “I don’t want to talk to you any more”, we might interpret this as either an angry or a sad emotion in the absence of context. Often, the utterances are shorter, and given a short utterance like “Me too!”, it is difficult to interpret the emotion without context. The lack of prosodic or visual information makes it a challenging problem to detect such emotions only with text. However, using contextual information in the dialogue is gaining importance to provide a context-aware recognition of linguistic features such as emotion, dialogue act, sentiment etc. The SemEval 2019 Task 3 EmoContext competition provides a dataset of three-turn dialogues labeled with the three emotion classes, i.e. Happy, Sad and Angry, and in addition with Others as none of the aforementioned emotion classes. We develop an ensemble of the recurrent neural model with character- and word-level features as an input to solve this problem. The system performs quite well, achieving a microaveraged F1 score (F1μ) of 0.7212 for the three emotion classes.</abstract>
      <url hash="4641733c">S19-2044</url>
      <doi>10.18653/v1/S19-2044</doi>
    </paper>
    <paper id="45">
      <title><fixed-case>NELEC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Think Twice Before Going Deep</title>
      <author><first>Parag</first><last>Agrawal</last></author>
      <author><first>Anshuman</first><last>Suri</last></author>
      <pages>266–271</pages>
      <abstract>Existing Machine Learning techniques yield close to human performance on text-based classification tasks. However, the presence of multi-modal noise in chat data such as emoticons, slang, spelling mistakes, code-mixed data, etc. makes existing deep-learning solutions perform poorly. The inability of deep-learning systems to robustly capture these covariates puts a cap on their performance. We propose NELEC: Neural and Lexical Combiner, a system which elegantly combines textual and deep-learning based methods for sentiment classification. We evaluate our system as part of the third task of ‘Contextual Emotion Detection in Text’ as part of SemEval-2019. Our system performs significantly better than the baseline, as well as our deep-learning model benchmarks. It achieved a micro-averaged F1 score of 0.7765, ranking 3rd on the test-set leader-board. Our code is available at https://github.com/iamgroot42/nelec</abstract>
      <url hash="218c89ca">S19-2045</url>
      <doi>10.18653/v1/S19-2045</doi>
    </paper>
    <paper id="46">
      <title><fixed-case>NL</fixed-case>-<fixed-case>FIIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Emotion Detection From Conversational Triplets Using Hierarchical Encoders</title>
      <author><first>Michal</first><last>Farkas</last></author>
      <author><first>Peter</first><last>Lacko</last></author>
      <pages>272–276</pages>
      <abstract>In this paper, we present our system submission for the EmoContext, the third task of the SemEval 2019 workshop. Our solution is a hierarchical recurrent neural network with ELMo embeddings and regularization through dropout and Gaussian noise. We have mainly experimented with two main model architectures: simple and hierarchical LSTM network. We have also examined ensembling of the models and various variants of an ensemble. We have achieved microF1 score of 0.7481, which is significantly higher than baseline and currently the 19th best submission.</abstract>
      <url hash="967b9e56">S19-2046</url>
      <doi>10.18653/v1/S19-2046</doi>
    </paper>
    <paper id="47">
      <title><fixed-case>NTUA</fixed-case>-<fixed-case>ISL</fixed-case>ab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Determining emotions in contextual conversations with deep learning</title>
      <author><first>Rolandos Alexandros</first><last>Potamias</last></author>
      <author><first>Georgios</first><last>Siolas</last></author>
      <pages>277–281</pages>
      <abstract>Sentiment analysis (SA) in texts is a well-studied Natural Language Processing task, which in nowadays gains popularity due to the explosion of social media, and the subsequent accumulation of huge amounts of related data. However, capturing emotional states and the sentiment polarity of written excerpts requires knowledge on the events triggering them. Towards this goal, we present a computational end-to-end context-aware SA methodology, which was competed in the context of the SemEval-2019 / EmoContext task (Task 3). The proposed system is founded on the combination of two neural architectures, a deep recurrent neural network, structured by an attentive Bidirectional LSTM, and a deep dense network (DNN). The system achieved 0.745 micro f1-score, and ranked 26/165 (top 20%) teams among the official task submissions.</abstract>
      <url hash="6fcf387c">S19-2047</url>
      <doi>10.18653/v1/S19-2047</doi>
    </paper>
    <paper id="48">
      <title>ntuer at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Emotion Classification with Word and Sentence Representations in <fixed-case>RCNN</fixed-case></title>
      <author><first>Peixiang</first><last>Zhong</last></author>
      <author><first>Chunyan</first><last>Miao</last></author>
      <pages>282–286</pages>
      <abstract>In this paper we present our model on the task of emotion detection in textual conversations in SemEval-2019. Our model extends the Recurrent Convolutional Neural Network (RCNN) by using external fine-tuned word representations and DeepMoji sentence representations. We also explored several other competitive pre-trained word and sentence representations including ELMo, BERT and InferSent but found inferior performance. In addition, we conducted extensive sensitivity analysis, which empirically shows that our model is relatively robust to hyper-parameters. Our model requires no handcrafted features or emotion lexicons but achieved good performance with a micro-F1 score of 0.7463.</abstract>
      <url hash="8e5757c8">S19-2048</url>
      <doi>10.18653/v1/S19-2048</doi>
    </paper>
    <paper id="49">
      <title><fixed-case>PKUSE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Emotion Detection with Emotion-Oriented Neural Attention Network</title>
      <author><first>Luyao</first><last>Ma</last></author>
      <author><first>Long</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Ye</last></author>
      <author><first>Wenhui</first><last>Hu</last></author>
      <pages>287–291</pages>
      <abstract>This paper presents the system in SemEval-2019 Task 3, “EmoContext: Contextual Emotion Detection in Text”. We propose a deep learning architecture with bidirectional LSTM networks, augmented with an emotion-oriented attention network that is capable of extracting emotion information from an utterance. Experimental results show that our model outperforms its variants and the baseline. Overall, this system has achieved 75.57% for the microaveraged F1 score.</abstract>
      <url hash="107cbe3c">S19-2049</url>
      <doi>10.18653/v1/S19-2049</doi>
    </paper>
    <paper id="50">
      <title>Podlab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: The Importance of Being Shallow</title>
      <author><first>Andrew</first><last>Nguyen</last></author>
      <author><first>Tobin</first><last>South</last></author>
      <author><first>Nigel</first><last>Bean</last></author>
      <author><first>Jonathan</first><last>Tuke</last></author>
      <author><first>Lewis</first><last>Mitchell</last></author>
      <pages>292–296</pages>
      <abstract>This paper describes our linear SVM system for emotion classification from conversational dialogue, entered in SemEval2019 Task 3. We used off-the-shelf tools coupled with feature engineering and parameter tuning to create a simple, interpretable, yet high-performing, classification model. Our system achieves a micro F1 score of 0.7357, which is 92% of the top score for the competition, demonstrating that “shallow” classification approaches can perform well when coupled with detailed fea- ture selection and statistical analysis.</abstract>
      <url hash="56101b03">S19-2050</url>
      <doi>10.18653/v1/S19-2050</doi>
    </paper>
    <paper id="51">
      <title><fixed-case>SCIA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Sentiment Analysis in Textual Conversations Using Deep Learning</title>
      <author><first>Zinedine</first><last>Rebiai</last></author>
      <author><first>Simon</first><last>Andersen</last></author>
      <author><first>Antoine</first><last>Debrenne</last></author>
      <author><first>Victor</first><last>Lafargue</last></author>
      <pages>297–301</pages>
      <abstract>In this paper we present our submission for SemEval-2019 Task 3: EmoContext. The task consisted of classifying a textual dialogue into one of four emotion classes: happy, sad, angry or others. Our approach tried to improve on multiple aspects, preprocessing with an emphasis on spell-checking and ensembling with four different models: Bi-directional contextual LSTM (BC-LSTM), categorical Bi-LSTM (CAT-LSTM), binary convolutional Bi-LSTM (BIN-LSTM) and Gated Recurrent Unit (GRU). On the leader-board, we submitted two systems that obtained a micro F1 score (F1μ) of 0.711 and 0.712. After the competition, we merged our two systems with ensembling, which achieved a F1μ of 0.7324 on the test dataset.</abstract>
      <url hash="e4fe256a">S19-2051</url>
      <doi>10.18653/v1/S19-2051</doi>
    </paper>
    <paper id="52">
      <title>Sentim at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Convolutional Neural Networks For Sentiment in Conversations</title>
      <author><first>Jacob</first><last>Anderson</last></author>
      <pages>302–306</pages>
      <abstract>In this work convolutional neural networks were used in order to determine the sentiment in a conversational setting. This paper’s contributions include a method for handling any sized input and a method for breaking down the conversation into separate parts for easier processing. Finally, clustering was shown to improve results and that such a model for handling sentiment in conversations is both fast and accurate.</abstract>
      <url hash="697b23d3">S19-2052</url>
      <doi>10.18653/v1/S19-2052</doi>
    </paper>
    <paper id="53">
      <title><fixed-case>SINAI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Using affective features for emotion classification in textual conversations</title>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last></author>
      <author><first>M. Dolores</first><last>Molina-González</last></author>
      <author><first>Maite</first><last>Martin</last></author>
      <author><first>L. Alfonso</first><last>Ureña-López</last></author>
      <pages>307–311</pages>
      <abstract>Detecting emotions in textual conversation is a challenging problem in absence of nonverbal cues typically associated with emotion, like fa- cial expression or voice modulations. How- ever, more and more users are using message platforms such as WhatsApp or Telegram. For this reason, it is important to develop systems capable of understanding human emotions in textual conversations. In this paper, we carried out different systems to analyze the emotions of textual dialogue from SemEval-2019 Task 3: EmoContext for English language. Our main contribution is the integration of emotional and sentimental features in the classification using the SVM algorithm.</abstract>
      <url hash="57c05cce">S19-2053</url>
      <doi>10.18653/v1/S19-2053</doi>
    </paper>
    <paper id="54">
      <title><fixed-case>SNU</fixed-case> <fixed-case>IDS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Addressing Training-Test Class Distribution Mismatch in Conversational Classification</title>
      <author><first>Sanghwan</first><last>Bae</last></author>
      <author><first>Jihun</first><last>Choi</last></author>
      <author><first>Sang-goo</first><last>Lee</last></author>
      <pages>312–317</pages>
      <abstract>We present several techniques to tackle the mismatch in class distributions between training and test data in the Contextual Emotion Detection task of SemEval 2019, by extending the existing methods for class imbalance problem. Reducing the distance between the distribution of prediction and ground truth, they consistently show positive effects on the performance. Also we propose a novel neural architecture which utilizes representation of overall context as well as of each utterance. The combination of the methods and the models achieved micro F1 score of about 0.766 on the final evaluation.</abstract>
      <url hash="9520e254">S19-2054</url>
      <doi>10.18653/v1/S19-2054</doi>
    </paper>
    <paper id="55">
      <title><fixed-case>SSN</fixed-case>_<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Contextual Emotion Identification from Textual Conversation using <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Deep Neural Network</title>
      <author><first>Senthil Kumar</first><last>B.</last></author>
      <author><first>Thenmozhi</first><last>D.</last></author>
      <author><first>Aravindan</first><last>Chandrabose</last></author>
      <author><first>Srinethe</first><last>Sharavanan</last></author>
      <pages>318–323</pages>
      <abstract>Emotion identification is a process of identifying the emotions automatically from text, speech or images. Emotion identification from textual conversations is a challenging problem due to absence of gestures, vocal intonation and facial expressions. It enables conversational agents, chat bots and messengers to detect and report the emotions to the user instantly for a healthy conversation by avoiding emotional cues and miscommunications. We have adopted a Seq2Seq deep neural network to identify the emotions present in the text sequences. Several layers namely embedding layer, encoding-decoding layer, softmax layer and a loss layer are used to map the sequences from textual conversations to the emotions namely Angry, Happy, Sad and Others. We have evaluated our approach on the EmoContext@SemEval2019 dataset and we have obtained the micro-averaged F1 scores as 0.595 and 0.6568 for the pre-evaluation dataset and final evaluation test set respectively. Our approach improved the base line score by 7% for final evaluation test set.</abstract>
      <url hash="01824399">S19-2055</url>
      <doi>10.18653/v1/S19-2055</doi>
    </paper>
    <paper id="56">
      <title><fixed-case>SWAP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Emotion detection in conversations through Tweets, <fixed-case>CNN</fixed-case> and <fixed-case>LSTM</fixed-case> deep neural networks</title>
      <author><first>Marco</first><last>Polignano</last></author>
      <author><first>Marco</first><last>de Gemmis</last></author>
      <author><first>Giovanni</first><last>Semeraro</last></author>
      <pages>324–329</pages>
      <abstract>Emotion detection from user-generated contents is growing in importance in the area of natural language processing. The approach we proposed for the EmoContext task is based on the combination of a CNN and an LSTM using a concatenation of word embeddings. A stack of convolutional neural networks (CNN) is used for capturing the hierarchical hidden relations among embedding features. Meanwhile, a long short-term memory network (LSTM) is used for capturing information shared among words of the sentence. Each conversation has been formalized as a list of word embeddings, in particular during experimental runs pre-trained Glove and Google word embeddings have been evaluated. Surface lexical features have been also considered, but they have been demonstrated to be not usefully for the classification in this specific task. The final system configuration achieved a micro F1 score of 0.7089. The python code of the system is fully available at https://github.com/marcopoli/EmoContext2019</abstract>
      <url hash="dc0c2807">S19-2056</url>
      <attachment type="software" hash="9c93ea0a">S19-2056.Software.zip</attachment>
      <doi>10.18653/v1/S19-2056</doi>
    </paper>
    <paper id="57">
      <title><fixed-case>S</fixed-case>ymanto<fixed-case>R</fixed-case>esearch at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Combined Neural Models for Emotion Classification in Human-Chatbot Conversations</title>
      <author><first>Angelo</first><last>Basile</last></author>
      <author><first>Marc</first><last>Franco-Salvador</last></author>
      <author><first>Neha</first><last>Pawar</last></author>
      <author><first>Sanja</first><last>Štajner</last></author>
      <author><first>Mara</first><last>Chinea Rios</last></author>
      <author><first>Yassine</first><last>Benajiba</last></author>
      <pages>330–334</pages>
      <abstract>In this paper, we present our participation to the EmoContext shared task on detecting emotions in English textual conversations between a human and a chatbot. We propose four neural systems and combine them to further improve the results. We show that our neural ensemble systems can successfully distinguish three emotions (SAD, HAPPY, and ANGRY) and separate them from the rest (OTHERS) in a highly-imbalanced scenario. Our best system achieved a 0.77 F1-score and was ranked fourth out of 165 submissions.</abstract>
      <url hash="268bb535">S19-2057</url>
      <doi>10.18653/v1/S19-2057</doi>
    </paper>
    <paper id="58">
      <title><fixed-case>TDB</fixed-case>ot at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Context Aware Emotion Detection Using A Conditioned Classification Approach</title>
      <author><first>Sourabh</first><last>Maity</last></author>
      <pages>335–339</pages>
      <abstract>With the system description it is shown how to use the context information while detecting the emotion in a dialogue. Some guidelines about how to handle emojis was also laid out. While developing this system I realized the importance of pre-processing in conversational text data, or in general NLP related tasks; it can not be over emphasized.</abstract>
      <url hash="a6c337ff">S19-2058</url>
      <doi>10.18653/v1/S19-2058</doi>
    </paper>
    <paper id="59">
      <title><fixed-case>THU</fixed-case>_<fixed-case>NGN</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Dialog Emotion Classification using Attentional <fixed-case>LSTM</fixed-case>-<fixed-case>CNN</fixed-case></title>
      <author><first>Suyu</first><last>Ge</last></author>
      <author><first>Tao</first><last>Qi</last></author>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>340–344</pages>
      <abstract>With the development of the Internet, dialog systems are widely used in online platforms to provide personalized services for their users. It is important to understand the emotions through conversations to improve the quality of dialog systems. To facilitate the researches on dialog emotion recognition, the SemEval-2019 Task 3 named EmoContext is proposed. This task aims to classify the emotions of user utterance along with two short turns of dialogues into four categories. In this paper, we propose an attentional LSTM-CNN model to participate in this shared task. We use a combination of convolutional neural networks and long-short term neural networks to capture both local and long-distance contextual information in conversations. In addition, we apply attention mechanism to recognize and attend to important words within conversations. Besides, we propose to use ensemble strategies by combing the variants of our model with different pre-trained word embeddings via weighted voting. Our model achieved 0.7542 micro-F1 score in the final test data, ranking 15ˆth out of 165 teams.</abstract>
      <url hash="89b63fab">S19-2059</url>
      <doi>10.18653/v1/S19-2059</doi>
    </paper>
    <paper id="60">
      <title><fixed-case>THU</fixed-case>-<fixed-case>HCSI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Hierarchical Ensemble Classification of Contextual Emotion in Conversation</title>
      <author><first>Xihao</first><last>Liang</last></author>
      <author><first>Ye</first><last>Ma</last></author>
      <author><first>Mingxing</first><last>Xu</last></author>
      <pages>345–349</pages>
      <abstract>In this paper, we describe our hierarchical ensemble system designed for the SemEval-2019 task3, EmoContext. In our system, three sets of classifiers are trained for different sub-targets and the predicted labels of these base classifiers are combined through three steps of voting to make the final prediction. Effective details for developing base classifiers are highlighted.</abstract>
      <url hash="9ac0f968">S19-2060</url>
      <doi>10.18653/v1/S19-2060</doi>
    </paper>
    <paper id="61">
      <title><fixed-case>T</fixed-case>okyo<fixed-case>T</fixed-case>ech_<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Emotion-related Symbols in Emotion Detection</title>
      <author><first>Zhishen</first><last>Yang</last></author>
      <author><first>Sam</first><last>Vijlbrief</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>350–354</pages>
      <abstract>This paper presents our contextual emotion detection system in approaching the SemEval2019 shared task 3: EmoContext: Contextual Emotion Detection in Text. This system cooperates with an emotion detection neural network method (Poria et al., 2017), emoji2vec (Eisner et al., 2016) embedding, word2vec embedding (Mikolov et al., 2013), and our proposed emoticon and emoji preprocessing method. The experimental results demonstrate the usefulness of our emoticon and emoji prepossessing method, and representations of emoticons and emoji contribute model’s emotion detection.</abstract>
      <url hash="4c3af2ba">S19-2061</url>
      <doi>10.18653/v1/S19-2061</doi>
    </paper>
    <paper id="62">
      <title><fixed-case>UAIC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Extracting Much from Little</title>
      <author><first>Cristian</first><last>Simionescu</last></author>
      <author><first>Ingrid</first><last>Stoleru</last></author>
      <author><first>Diana</first><last>Lucaci</last></author>
      <author><first>Gheorghe</first><last>Balan</last></author>
      <author><first>Iulian</first><last>Bute</last></author>
      <author><first>Adrian</first><last>Iftene</last></author>
      <pages>355–359</pages>
      <abstract>In this paper, we present a system description for implementing a sentiment analysis agent capable of interpreting the state of an interlocutor engaged in short three message conversations. We present the results and observations of our work and which parts could be further improved in the future.</abstract>
      <url hash="f6498324">S19-2062</url>
      <doi>10.18653/v1/S19-2062</doi>
    </paper>
    <paper id="63">
      <title><fixed-case>YUN</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 3: Multi-Step Ensemble Neural Network for Sentiment Analysis in Textual Conversation</title>
      <author><first>Dawei</first><last>Li</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>360–364</pages>
      <abstract>This paper describes our approach to the sentiment analysis of Twitter textual conversations based on deep learning. We analyze the syntax, abbreviations, and informal-writing of Twitter; and perform perfect data preprocessing on the data to convert them to normative text. We apply a multi-step ensemble strategy to solve the problem of extremely unbalanced data in the training set. This is achieved by taking the GloVe and Elmo word vectors as input into a combination model with four different deep neural networks. The experimental results from the development dataset demonstrate that the proposed model exhibits a strong generalization ability. For evaluation on the best dataset, we integrated the results using the stacking ensemble learning approach and achieved competitive results. According to the final official review, the results of our model ranked 10th out of 165 teams.</abstract>
      <url hash="41639443">S19-2063</url>
      <doi>10.18653/v1/S19-2063</doi>
    </paper>
    <paper id="64">
      <title><fixed-case>KDEH</fixed-case>at<fixed-case>E</fixed-case>val at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: A Neural Network Model for Detecting Hate Speech in <fixed-case>T</fixed-case>witter</title>
      <author><first>Umme Aymun</first><last>Siddiqua</last></author>
      <author><first>Abu Nowshed</first><last>Chy</last></author>
      <author><first>Masaki</first><last>Aono</last></author>
      <pages>365–370</pages>
      <abstract>In the age of emerging volume of microblog platforms, especially twitter, hate speech propagation is now of great concern. However, due to the brevity of tweets and informal user generated contents, detecting and analyzing hate speech on twitter is a formidable task. In this paper, we present our approach for detecting hate speech in tweets defined in the SemEval-2019 Task 5. Our team KDEHatEval employs different neural network models including multi-kernel convolution (MKC), nested LSTMs (NLSTMs), and multi-layer perceptron (MLP) in a unified architecture. Moreover, we utilize the state-of-the-art pre-trained sentence embedding models including DeepMoji, InferSent, and BERT for effective tweet representation. We analyze the performance of our method and demonstrate the contribution of each component of our architecture.</abstract>
      <url hash="e01731ce">S19-2064</url>
      <doi>10.18653/v1/S19-2064</doi>
    </paper>
    <paper id="65">
      <title><fixed-case>ABARUAH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5 : Bi-directional <fixed-case>LSTM</fixed-case> for Hate Speech Detection</title>
      <author><first>Arup</first><last>Baruah</last></author>
      <author><first>Ferdous</first><last>Barbhuiya</last></author>
      <author><first>Kuntal</first><last>Dey</last></author>
      <pages>371–376</pages>
      <abstract>In this paper, we present the results obtained using bi-directional long short-term memory (BiLSTM) with and without attention and Logistic Regression (LR) models for SemEval-2019 Task 5 titled ”HatEval: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter”. This paper presents the results obtained for Subtask A for English language. The results of the BiLSTM and LR models are compared for two different types of preprocessing. One with no stemming performed and no stopwords removed. The other with stemming performed and stopwords removed. The BiLSTM model without attention performed the best for the first test, while the LR model with character n-grams performed the best for the second test. The BiLSTM model obtained an F1 score of 0.51 on the test set and obtained an official ranking of 8/71.</abstract>
      <url hash="bf0ed437">S19-2065</url>
      <doi>10.18653/v1/S19-2065</doi>
    </paper>
    <paper id="66">
      <title><fixed-case>A</fixed-case>mobee at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Tasks 5 and 6: Multiple Choice <fixed-case>CNN</fixed-case> Over Contextual Embedding</title>
      <author><first>Alon</first><last>Rozental</last></author>
      <author><first>Dadi</first><last>Biton</last></author>
      <pages>377–381</pages>
      <abstract>This article describes Amobee’s participation in “HatEval: Multilingual detection of hate speech against immigrants and women in Twitter” (task 5) and “OffensEval: Identifying and Categorizing Offensive Language in Social Media” (task 6). The goal of task 5 was to detect hate speech targeted to women and immigrants. The goal of task 6 was to identify and categorized offensive language in social media, and identify offense target. We present a novel type of convolutional neural network called “Multiple Choice CNN” (MC- CNN) that we used over our newly developed contextual embedding, Rozental et al. (2019). For both tasks we used this architecture and achieved 4th place out of 69 participants with an F1 score of 0.53 in task 5, in task 6 achieved 2nd place (out of 75) in Sub-task B - automatic categorization of offense types (our model reached places 18/2/7 out of 103/75/65 for sub-tasks A, B and C respectively in task 6).</abstract>
      <url hash="b274e83b">S19-2066</url>
      <doi>10.18653/v1/S19-2066</doi>
    </paper>
    <paper id="67">
      <title><fixed-case>CIC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Simple Yet Very Efficient Approach to Hate Speech Detection, Aggressive Behavior Detection, and Target Classification in <fixed-case>T</fixed-case>witter</title>
      <author><first>Iqra</first><last>Ameer</last></author>
      <author><first>Muhammad Hammad Fahim</first><last>Siddiqui</last></author>
      <author><first>Grigori</first><last>Sidorov</last></author>
      <author><first>Alexander</first><last>Gelbukh</last></author>
      <pages>382–386</pages>
      <abstract>In recent years, the use of social media has in-creased incredibly. Social media permits Inter-net users a friendly platform to express their views and opinions. Along with these nice and distinct communication chances, it also allows bad things like usage of hate speech. Online automatic hate speech detection in various aspects is a significant scientific problem. This paper presents the Instituto Politécnico Nacional (Mexico) approach for the Semeval 2019 Task-5 [Hateval 2019] (Basile et al., 2019) competition for Multilingual Detection of Hate Speech on Twitter. The goal of this paper is to detect (A) Hate speech against immigrants and women, (B) Aggressive behavior and target classification, both for English and Spanish. In the proposed approach, we used a bag of words model with preprocessing (stem-ming and stop words removal). We submitted two different systems with names: (i) CIC-1 and (ii) CIC-2 for Hateval 2019 shared task. We used TF values in the first system and TF-IDF for the second system. The first system, CIC-1 got 2nd rank in subtask B for both English and Spanish languages with EMR score of 0.568 for English and 0.675 for Spanish. The second system, CIC-2 was ranked 4th in sub-task A and 1st in subtask B for Spanish language with a macro-F1 score of 0.727 and EMR score of 0.705 respectively.</abstract>
      <url hash="ebf50cc8">S19-2067</url>
      <doi>10.18653/v1/S19-2067</doi>
    </paper>
    <paper id="68">
      <title><fixed-case>C</fixed-case>i<fixed-case>TIUS</fixed-case>-<fixed-case>COLE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Combining Linguistic Features to Identify Hate Speech Against Immigrants and Women on Multilingual Tweets</title>
      <author><first>Sattam</first><last>Almatarneh</last></author>
      <author><first>Pablo</first><last>Gamallo</last></author>
      <author><first>Francisco J. Ribadas</first><last>Pena</last></author>
      <pages>387–390</pages>
      <abstract>This article describes the strategy submitted by the CiTIUS-COLE team to SemEval 2019 Task 5, a task which consists of binary classi- fication where the system predicting whether a tweet in English or in Spanish is hateful against women or immigrants or not. The proposed strategy relies on combining linguis- tic features to improve the classifier’s perfor- mance. More precisely, the method combines textual and lexical features, embedding words with the bag of words in Term Frequency- Inverse Document Frequency (TF-IDF) repre- sentation. The system performance reaches about 81% F1 when it is applied to the training dataset, but its F1 drops to 36% on the official test dataset for the English and 64% for the Spanish language concerning the hate speech class</abstract>
      <url hash="99d94ef8">S19-2068</url>
      <doi>10.18653/v1/S19-2068</doi>
    </paper>
    <paper id="69">
      <title>Grunn2019 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Shared Task on Multilingual Detection of Hate</title>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Roy</first><last>David</last></author>
      <author><first>Leon</first><last>Graumans</last></author>
      <author><first>Gerben</first><last>Timmerman</last></author>
      <pages>391–395</pages>
      <abstract>Hate speech occurs more often than ever and polarizes society. To help counter this polarization, SemEval 2019 organizes a shared task called the Multilingual Detection of Hate. The first task (A) is to decide whether a given tweet contains hate against immigrants or women, in a multilingual perspective, for English and Spanish. In the second task (B), the system is also asked to classify the following sub-tasks: hateful tweets as aggressive or not aggressive, and to identify the target harassed as individual or generic. We evaluate multiple models, and finally combine them in an ensemble setting. This ensemble setting is built of five and three submodels for the English and Spanish task respectively. In the current setup it shows that using a bigger ensemble for English tweets performs mediocre, while a slightly smaller ensemble does work well for detecting hate speech in Spanish tweets. Our results on the test set for English show 0.378 macro F1 on task A and 0.553 macro F1 on task B. For Spanish the results are significantly higher, 0.701 macro F1 on task A and 0.734 macro F1 for task B.</abstract>
      <url hash="9bd73a34">S19-2069</url>
      <doi>10.18653/v1/S19-2069</doi>
    </paper>
    <paper id="70">
      <title><fixed-case>GSI</fixed-case>-<fixed-case>UPM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Semantic Similarity and Word Embeddings for Multilingual Detection of Hate Speech Against Immigrants and Women on <fixed-case>T</fixed-case>witter</title>
      <author><first>Diego</first><last>Benito</last></author>
      <author><first>Oscar</first><last>Araque</last></author>
      <author><first>Carlos A.</first><last>Iglesias</last></author>
      <pages>396–403</pages>
      <abstract>This paper describes the GSI-UPM system for SemEval-2019 Task 5, which tackles multilingual detection of hate speech on Twitter. The main contribution of the paper is the use of a method based on word embeddings and semantic similarity combined with traditional paradigms, such as n-grams, TF-IDF and POS. This combination of several features is fine-tuned through ablation tests, demonstrating the usefulness of different features. While our approach outperforms baseline classifiers on different sub-tasks, the best of our submitted runs reached the 5th position on the Spanish sub-task A.</abstract>
      <url hash="0fbc1756">S19-2070</url>
      <doi>10.18653/v1/S19-2070</doi>
    </paper>
    <paper id="71">
      <title><fixed-case>HATEMINER</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Hate speech detection against Immigrants and Women in <fixed-case>T</fixed-case>witter using a Multinomial Naive <fixed-case>B</fixed-case>ayes Classifier</title>
      <author><first>Nikhil</first><last>Chakravartula</last></author>
      <pages>404–408</pages>
      <abstract>This paper describes our participation in the SemEval 2019 Task 5 - Multilingual Detection of Hate. This task aims to identify hate speech against two specific targets, immigrants and women. We compare and contrast the performance of different word and sentence level embeddings on the state-of-the-art classification algorithms. Our final submission is a Multinomial binarized Naive Bayes model for both the subtasks in the English version.</abstract>
      <url hash="39d07e83">S19-2071</url>
      <doi>10.18653/v1/S19-2071</doi>
    </paper>
    <paper id="72">
      <title><fixed-case>HATER</fixed-case>ecognizer at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Using Features and Neural Networks to Face Hate Recognition</title>
      <author><first>Victor</first><last>Nina-Alcocer</last></author>
      <pages>409–415</pages>
      <abstract>This paper presents a detailed description of our participation in task 5 on SemEval-2019. This task consists of classifying English and Spanish tweets that contain hate towards women or immigrants. We carried out several experiments; for a finer-grained study of the task, we analyzed different features and designing architectures of neural networks. Additionally, to face the lack of hate content in tweets, we include data augmentation as a technique to in- crease hate content in our datasets.</abstract>
      <url hash="a97a8493">S19-2072</url>
      <doi>10.18653/v1/S19-2072</doi>
    </paper>
    <paper id="73">
      <title><fixed-case>GL</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Identifying hateful tweets with a deep learning approach.</title>
      <author><first>Gretel Liz</first><last>De la Peña</last></author>
      <pages>416–419</pages>
      <abstract>This paper describes the system we developed for SemEval 2019 on Multilingual detection of hate speech against immigrants and women in Twitter (HatEval - Task 5). We use an approach based on an Attention-based Long Short-Term Memory Recurrent Neural Network. In particular, we build a Bidirectional LSTM to extract information from the word embeddings over the sentence, then apply attention over the hidden states to estimate the importance of each word and finally feed this context vector to another LSTM model to get a representation. Then, the output obtained with this model is used to get the prediction of each of the sub-tasks.</abstract>
      <url hash="4150b04f">S19-2073</url>
      <doi>10.18653/v1/S19-2073</doi>
    </paper>
    <paper id="74">
      <title><fixed-case>INF</fixed-case>-<fixed-case>H</fixed-case>at<fixed-case>E</fixed-case>val at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Convolutional Neural Networks for Hate Speech Detection Against Women and Immigrants on <fixed-case>T</fixed-case>witter</title>
      <author><first>Alison</first><last>Ribeiro</last></author>
      <author><first>Nádia</first><last>Silva</last></author>
      <pages>420–425</pages>
      <abstract>In this paper, we describe our approach to detect hate speech against women and immigrants on Twitter in a multilingual context, English and Spanish. This challenge was proposed by the SemEval-2019 Task 5, where participants should develop models for hate speech detection, a two-class classification where systems have to predict whether a tweet in English or in Spanish with a given target (women or immigrants) is hateful or not hateful (Task A), and whether the hate speech is directed at a specific person or a group of individuals (Task B). For this, we implemented a Convolutional Neural Networks (CNN) using pre-trained word embeddings (GloVe and FastText) with 300 dimensions. Our proposed model obtained in Task A 0.488 and 0.696 F1-score for English and Spanish, respectively. For Task B, the CNN obtained 0.297 and 0.430 EMR for English and Spanish, respectively.</abstract>
      <url hash="e34af83d">S19-2074</url>
      <doi>10.18653/v1/S19-2074</doi>
    </paper>
    <paper id="75">
      <title><fixed-case>JCTDHS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Detection of Hate Speech in Tweets using Deep Learning Methods, Character N-gram Features, and Preprocessing Methods</title>
      <author><first>Yaakov</first><last>HaCohen-Kerner</last></author>
      <author><first>Elyashiv</first><last>Shayovitz</last></author>
      <author><first>Shalom</first><last>Rochman</last></author>
      <author><first>Eli</first><last>Cahn</last></author>
      <author><first>Gal</first><last>Didi</last></author>
      <author><first>Ziv</first><last>Ben-David</last></author>
      <pages>426–430</pages>
      <abstract>In this paper, we describe our submissions to SemEval-2019 contest. We tackled subtask A - “a binary classification where systems have to predict whether a tweet with a given target (women or immigrants) is hateful or not hateful”, a part of task 5 “Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)”. Our system JCTDHS (Jerusalem College of Technology Detects Hate Speech) was developed for tweets written in English. We applied various supervised ML methods, various combinations of n-gram features using the TF-IDF scheme and. In addition, we applied various combinations of eight basic preprocessing methods. Our best submission was a special bidirectional RNN, which was ranked at the 11th position out of 68 submissions.</abstract>
      <url hash="41ee7f2d">S19-2075</url>
      <doi>10.18653/v1/S19-2075</doi>
    </paper>
    <paper id="76">
      <title>Know-Center at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Multilingual Hate Speech Detection on <fixed-case>T</fixed-case>witter using <fixed-case>CNN</fixed-case>s</title>
      <author><first>Kevin</first><last>Winter</last></author>
      <author><first>Roman</first><last>Kern</last></author>
      <pages>431–435</pages>
      <abstract>This paper presents the Know-Center system submitted for task 5 of the SemEval-2019 workshop. Given a Twitter message in either English or Spanish, the task is to first detect whether it contains hateful speech and second, to determine the target and level of aggression used. For this purpose our system utilizes word embeddings and a neural network architecture, consisting of both dilated and traditional convolution layers. We achieved average F1-scores of 0.57 and 0.74 for English and Spanish respectively.</abstract>
      <url hash="d49b134b">S19-2076</url>
      <doi>10.18653/v1/S19-2076</doi>
    </paper>
    <paper id="77">
      <title><fixed-case>LT</fixed-case>3 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in <fixed-case>T</fixed-case>witter (hat<fixed-case>E</fixed-case>val)</title>
      <author><first>Nina</first><last>Bauwelinck</last></author>
      <author><first>Gilles</first><last>Jacobs</last></author>
      <author><first>Véronique</first><last>Hoste</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <pages>436–440</pages>
      <abstract>This paper describes our contribution to the SemEval-2019 Task 5 on the detection of hate speech against immigrants and women in Twitter (hatEval). We considered a supervised classification-based approach to detect hate speech in English tweets, which combines a variety of standard lexical and syntactic features with specific features for capturing offensive language. Our experimental results show good classification performance on the training data, but a considerable drop in recall on the held-out test set.</abstract>
      <url hash="e950a437">S19-2077</url>
      <doi>10.18653/v1/S19-2077</doi>
    </paper>
    <paper id="78">
      <title>ltl.uni-due at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Simple but Effective Lexico-Semantic Features for Detecting Hate Speech in <fixed-case>T</fixed-case>witter</title>
      <author><first>Huangpan</first><last>Zhang</last></author>
      <author><first>Michael</first><last>Wojatzki</last></author>
      <author><first>Tobias</first><last>Horsmann</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>441–446</pages>
      <abstract>In this paper, we present our contribution to SemEval 2019 Task 5 Multilingual Detection of Hate, specifically in the Subtask A (English and Spanish). We compare different configurations of shallow and deep learning approaches on the English data and use the system that performs best in both sub-tasks. The resulting SVM-based system with lexicosemantic features (n-grams and embeddings) is ranked 23rd out of 69 on the English data and beats the baseline system. On the Spanish data our system is ranked 25th out of 39.</abstract>
      <url hash="051bae7f">S19-2078</url>
      <doi>10.18653/v1/S19-2078</doi>
    </paper>
    <paper id="79">
      <title><fixed-case>M</fixed-case>ineria<fixed-case>UNAM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Detecting Hate Speech in <fixed-case>T</fixed-case>witter using Multiple Features in a Combinatorial Framework</title>
      <author><first>Luis Enrique</first><last>Argota Vega</last></author>
      <author><first>Jorge Carlos</first><last>Reyes-Magaña</last></author>
      <author><first>Helena</first><last>Gómez-Adorno</last></author>
      <author><first>Gemma</first><last>Bel-Enguix</last></author>
      <pages>447–452</pages>
      <abstract>This paper presents our approach to the Task 5 of Semeval-2019, which aims at detecting hate speech against immigrants and women in Twitter. The task consists of two sub-tasks, in Spanish and English: (A) detection of hate speech and (B) classification of hateful tweets as aggressive or not, and identification of the target harassed as individual or group. We used linguistically motivated features and several types of n-grams (words, characters, functional words, punctuation symbols, POS, among others). For task A, we trained a Support Vector Machine using a combinatorial framework, whereas for task B we followed a multi-labeled approach using the Random Forest classifier. Our approach achieved the highest F1-score in sub-task A for the Spanish language.</abstract>
      <url hash="c9718937">S19-2079</url>
      <doi>10.18653/v1/S19-2079</doi>
    </paper>
    <paper id="80">
      <title><fixed-case>MITRE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Transfer Learning for Multilingual Hate Speech Detection</title>
      <author><first>Abigail</first><last>Gertner</last></author>
      <author><first>John</first><last>Henderson</last></author>
      <author><first>Elizabeth</first><last>Merkhofer</last></author>
      <author><first>Amy</first><last>Marsh</last></author>
      <author><first>Ben</first><last>Wellner</last></author>
      <author><first>Guido</first><last>Zarrella</last></author>
      <pages>453–459</pages>
      <abstract>This paper describes MITRE’s participation in SemEval-2019 Task 5, HatEval: Multilingual detection of hate speech against immigrants and women in Twitter. The techniques explored range from simple bag-of-ngrams classifiers to neural architectures with varied attention mechanisms. We describe several styles of transfer learning from auxiliary tasks, including a novel method for adapting pre-trained BERT models to Twitter data. Logistic regression ties the systems together into an ensemble submitted for evaluation. The resulting system was used to produce predictions for all four HatEval subtasks, achieving the best mean rank of all teams that participated in all four conditions.</abstract>
      <url hash="2e969d29">S19-2080</url>
      <doi>10.18653/v1/S19-2080</doi>
    </paper>
    <paper id="81">
      <title>Multilingual Detection of Hate Speech Against Immigrants and Women in <fixed-case>T</fixed-case>witter at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Frequency Analysis Interpolation for Hate in Speech Detection</title>
      <author><first>Òscar</first><last>Garibo i Orts</last></author>
      <pages>460–463</pages>
      <abstract>This document describes a text change of representation approach to the task of Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter, as part of SemEval-2019 1 . The task is divided in two sub-tasks. Sub-task A consists in classifying tweets as being hateful or not hateful, whereas sub-task B requires fine tuning the classification by classifying the hateful tweets as being directed to single individuals or generic, if the tweet is aggressive or not. Our approach consists of a change of the space of representation of text into statistical descriptors which characterize the text. In addition, dimensional reduction is performed to 6 characteristics per class in order to make the method suitable for a Big Data environment. Frequency Analysis Interpolation (FAI) is the approach we use to achieve rank 5th in Spanish language and 9th in English language in sub-task B in both cases.</abstract>
      <url hash="88e1c86f">S19-2081</url>
      <doi>10.18653/v1/S19-2081</doi>
    </paper>
    <paper id="82">
      <title><fixed-case>STUFIIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Multilingual Hate Speech Detection on <fixed-case>T</fixed-case>witter with <fixed-case>MUSE</fixed-case> and <fixed-case>ELM</fixed-case>o Embeddings</title>
      <author><first>Michal</first><last>Bojkovský</last></author>
      <author><first>Matúš</first><last>Pikuliak</last></author>
      <pages>464–468</pages>
      <abstract>We present a number of models used for hate speech detection for Semeval 2019 Task-5: Hateval. We evaluate the viability of multilingual learning for this task. We also experiment with adversarial learning as a means of creating a multilingual model. Ultimately our multilingual models have had worse results than their monolignual counterparts. We find that the choice of word representations (word embeddings) is very crucial for deep learning as a simple switch between MUSE and ELMo embeddings has shown a 3-4% increase in accuracy. This also shows the importance of context when dealing with online content.</abstract>
      <url hash="8b6f8cd8">S19-2082</url>
      <doi>10.18653/v1/S19-2082</doi>
    </paper>
    <paper id="83">
      <title>Saagie at <fixed-case>S</fixed-case>emeval-2019 Task 5: From Universal Text Embeddings and Classical Features to Domain-specific Text Classification</title>
      <author><first>Miriam</first><last>Benballa</last></author>
      <author><first>Sebastien</first><last>Collet</last></author>
      <author><first>Romain</first><last>Picot-Clemente</last></author>
      <pages>469–475</pages>
      <abstract>This paper describes our contribution to SemEval 2019 Task 5: Hateval. We propose to investigate how domain-specific text classification task can benefit from pretrained state of the art language models and how they can be combined with classical handcrafted features. For this purpose, we propose an approach based on a feature-level Meta-Embedding to let the model choose which features to keep and how to use them.</abstract>
      <url hash="72425a67">S19-2083</url>
      <doi>10.18653/v1/S19-2083</doi>
    </paper>
    <paper id="84">
      <title><fixed-case>SINAI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Ensemble learning to detect hate speech against inmigrants and women in <fixed-case>E</fixed-case>nglish and <fixed-case>S</fixed-case>panish tweets</title>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last></author>
      <author><first>M. Dolores</first><last>Molina-González</last></author>
      <author><first>Maite</first><last>Martin</last></author>
      <author><first>L. Alfonso</first><last>Ureña-López</last></author>
      <pages>476–479</pages>
      <abstract>Misogyny and xenophobia are some of the most important social problems. With the in- crease in the use of social media, this feeling ofhatred towards women and immigrants can be more easily expressed, therefore it can cause harmful effects on social media users. For this reason, it is important to develop systems ca- pable of detecting hateful comments automatically. In this paper, we describe our system to analyze the hate speech in English and Spanish tweets against Immigrants and Women as part of our participation in SemEval-2019 Task 5: hatEval. Our main contribution is the integration of three individual algorithms of predic- tion in a model based on Vote ensemble classifier.</abstract>
      <url hash="4f5fa328">S19-2084</url>
      <doi>10.18653/v1/S19-2084</doi>
    </paper>
    <paper id="85">
      <title><fixed-case>SINAI</fixed-case>-<fixed-case>DL</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Recurrent networks and data augmentation by paraphrasing</title>
      <author><first>Arturo</first><last>Montejo-Ráez</last></author>
      <author><first>Salud María</first><last>Jiménez-Zafra</last></author>
      <author><first>Miguel A.</first><last>García-Cumbreras</last></author>
      <author><first>Manuel Carlos</first><last>Díaz-Galiano</last></author>
      <pages>480–483</pages>
      <abstract>This paper describes the participation of the SINAI-DL team at Task 5 in SemEval 2019, called HatEval. We have applied some classic neural network layers, like word embeddings and LSTM, to build a neural classifier for both proposed tasks. Due to the small amount of training data provided compared to what is expected for an adequate learning stage in deep architectures, we explore the use of paraphrasing tools as source for data augmentation. Our results show that this method is promising, as some improvement has been found over non-augmented training sets.</abstract>
      <url hash="6297434a">S19-2085</url>
      <doi>10.18653/v1/S19-2085</doi>
    </paper>
    <paper id="86">
      <title>sthruggle at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: An Ensemble Approach to Hate Speech Detection</title>
      <author><first>Aria</first><last>Nourbakhsh</last></author>
      <author><first>Frida</first><last>Vermeer</last></author>
      <author><first>Gijs</first><last>Wiltvank</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <pages>484–488</pages>
      <abstract>In this paper, we present our approach to detection of hate speech against women and immigrants in tweets for our participation in the SemEval-2019 Task 5. We trained an SVM and an RF classifier using character bi- and trigram features and a BiLSTM pre-initialized with external word embeddings. We combined the predictions of the SVM, RF and BiLSTM in two different ensemble models. The first was a majority vote of the binary values, and the second used the average of the confidence scores. For development, we got the highest accuracy (75%) by the final ensemble model with majority voting. For testing, all models scored substantially lower and the scores between the classifiers varied more. We believe that these large differences between the higher accuracies in the development phase and the lower accuracies we obtained in the testing phase have partly to do with differences between the training, development and testing data.</abstract>
      <url hash="c157c51a">S19-2086</url>
      <doi>10.18653/v1/S19-2086</doi>
    </paper>
    <paper id="87">
      <title>The binary trio at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Multitarget Hate Speech Detection in Tweets</title>
      <author><first>Patricia</first><last>Chiril</last></author>
      <author><first>Farah</first><last>Benamara Zitoune</last></author>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Abhishek</first><last>Kumar</last></author>
      <pages>489–493</pages>
      <abstract>The massive growth of user-generated web content through blogs, online forums and most notably, social media networks, led to a large spreading of hatred or abusive messages which have to be moderated. This paper proposes a supervised approach to hate speech detection towards immigrants and women in English tweets. Several models have been developed ranging from feature-engineering approaches to neural ones.</abstract>
      <url hash="6350a720">S19-2087</url>
      <doi>10.18653/v1/S19-2087</doi>
    </paper>
    <paper id="88">
      <title>The Titans at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Detection of hate speech against immigrants and women in <fixed-case>T</fixed-case>witter</title>
      <author><first>Avishek</first><last>Garain</last></author>
      <author><first>Arpan</first><last>Basu</last></author>
      <pages>494–497</pages>
      <abstract>This system paper is a description of the system submitted to ”SemEval-2019 Task 5” Task B for the English language, where we had to primarily detect hate speech and then detect aggressive behaviour and its target audience in Twitter. There were two specific target audiences, immigrants and women. The language of the tweets was English. We were required to first detect whether a tweet is containing hate speech. Thereafter we were required to find whether the tweet was showing aggressive behaviour, and then we had to find whether the targeted audience was an individual or a group of people.</abstract>
      <url hash="28f7a6de">S19-2088</url>
      <doi>10.18653/v1/S19-2088</doi>
    </paper>
    <paper id="89">
      <title><fixed-case>T</fixed-case>u<fixed-case>E</fixed-case>val at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: <fixed-case>LSTM</fixed-case> Approach to Hate Speech Detection in <fixed-case>E</fixed-case>nglish and <fixed-case>S</fixed-case>panish</title>
      <author><first>Mihai</first><last>Manolescu</last></author>
      <author><first>Denise</first><last>Löfflad</last></author>
      <author><first>Adham Nasser</first><last>Mohamed Saber</last></author>
      <author><first>Masoumeh</first><last>Moradipour Tari</last></author>
      <pages>498–502</pages>
      <abstract>The detection of hate speech, especially in online platforms and forums, is quickly becoming a hot topic as anti-hate speech legislation begins to be applied to public discourse online. The HatEval shared task was created with this in mind; participants were expected to develop a model capable of determining whether or not input (in this case, Twitter datasets in English and Spanish) could be considered hate speech (designated as Task A), if they were aggressive, and whether the tweet was targeting an individual, or speaking generally (Task B). We approached this task by creating an LSTM model with an embedding layer. We found that our model performed considerably better on English language input when compared to Spanish language input. In English, we achieved an F1-Score of 0.466 for Task A and 0.462 for Task B; In Spanish, we achieved scores of 0.617 and 0.612 on Task A and Task B, respectively.</abstract>
      <url hash="df9516c2">S19-2089</url>
      <doi>10.18653/v1/S19-2089</doi>
    </paper>
    <paper id="90">
      <title>Tw-<fixed-case>S</fixed-case>t<fixed-case>AR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: N-gram embeddings for Hate Speech Detection in Multilingual Tweets</title>
      <author><first>Hala</first><last>Mulki</last></author>
      <author><first>Chedi</first><last>Bechikh Ali</last></author>
      <author><first>Hatem</first><last>Haddad</last></author>
      <author><first>Ismail</first><last>Babaoğlu</last></author>
      <pages>503–507</pages>
      <abstract>In this paper, we describe our contribution in SemEval-2019: subtask A of task 5 “Multilingual detection of hate speech against immigrants and women in Twitter (HatEval)”. We developed two hate speech detection model variants through Tw-StAR framework. While the first model adopted one-hot encoding ngrams to train an NB classifier, the second generated and learned n-gram embeddings within a feedforward neural network. For both models, specific terms, selected via MWT patterns, were tagged in the input data. With two feature types employed, we could investigate the ability of n-gram embeddings to rival one-hot n-grams. Our results showed that in English, n-gram embeddings outperformed one-hot ngrams. However, representing Spanish tweets by one-hot n-grams yielded a slightly better performance compared to that of n-gram embeddings. The official ranking indicated that Tw-StAR ranked 9th for English and 20th for Spanish.</abstract>
      <url hash="92985795">S19-2090</url>
      <doi>10.18653/v1/S19-2090</doi>
    </paper>
    <paper id="91">
      <title><fixed-case>UA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Setting A Strong Linear Baseline for Hate Speech Detection</title>
      <author><first>Carlos</first><last>Perelló</last></author>
      <author><first>David</first><last>Tomás</last></author>
      <author><first>Alberto</first><last>Garcia-Garcia</last></author>
      <author><first>Jose</first><last>Garcia-Rodriguez</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <pages>508–513</pages>
      <abstract>This paper describes the system developed at the University of Alicante (UA) for the SemEval 2019 Task 5: Shared Task on Multilingual Detection of Hate. The purpose of this work is to build a strong baseline for hate speech detection, using a traditional machine learning approach with standard textual features, which could serve in a near future as a reference to compare with deep learning systems. We participated in both task A (Hate Speech Detection against Immigrants and Women) and task B (Aggressive behavior and Target Classification). Despite its simplicity, our system obtained a remarkable F1-score of 72.5 (sixth highest) and an accuracy of 73.6 (second highest) in Spanish (task A), outperforming more complex neural models from a total of 40 participant systems.</abstract>
      <url hash="6564c845">S19-2091</url>
      <doi>10.18653/v1/S19-2091</doi>
    </paper>
    <paper id="92">
      <title><fixed-case>UNBNLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5 and 6: Using Language Models to Detect Hate Speech and Offensive Language</title>
      <author><first>Ali</first><last>Hakimi Parizi</last></author>
      <author><first>Milton</first><last>King</last></author>
      <author><first>Paul</first><last>Cook</last></author>
      <pages>514–518</pages>
      <abstract>In this paper we apply a range of approaches to language modeling – including word-level n-gram and neural language models, and character-level neural language models – to the problem of detecting hate speech and offensive language. Our findings indicate that language models are able to capture knowledge of whether text is hateful or offensive. However, our findings also indicate that more conventional approaches to text classification often perform similarly or better.</abstract>
      <url hash="313e3d79">S19-2092</url>
      <doi>10.18653/v1/S19-2092</doi>
    </paper>
    <paper id="93">
      <title><fixed-case>UTFPR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Hate Speech Identification with Recurrent Neural Networks</title>
      <author><first>Gustavo Henrique</first><last>Paetzold</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Shervin</first><last>Malmasi</last></author>
      <pages>519–523</pages>
      <abstract>In this paper we revisit the problem of automatically identifying hate speech in posts from social media. We approach the task using a system based on minimalistic compositional Recurrent Neural Networks (RNN). We tested our approach on the SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter (HatEval) shared task dataset. The dataset made available by the HatEval organizers contained English and Spanish posts retrieved from Twitter annotated with respect to the presence of hateful content and its target. In this paper we present the results obtained by our system in comparison to the other entries in the shared task. Our system achieved competitive performance ranking 7th in sub-task A out of 62 systems in the English track.</abstract>
      <url hash="a5998468">S19-2093</url>
      <doi>10.18653/v1/S19-2093</doi>
    </paper>
    <paper id="94">
      <title>Vista.ue at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Single Multilingual Hate Speech Detection Model</title>
      <author><first>Kashyap</first><last>Raiyani</last></author>
      <author><first>Teresa</first><last>Gonçalves</last></author>
      <author><first>Paulo</first><last>Quaresma</last></author>
      <author><first>Vitor</first><last>Nogueira</last></author>
      <pages>524–528</pages>
      <abstract>This paper shares insight from participating in SemEval-2019 Task 5. The main propose of this system-description paper is to facilitate the reader with replicability and to provide insightful analysis of the developed system. Here in Vista.ue, we proposed a single multilingual hate speech detection model. This model was ranked 46/70 for English Task A and 31/43 for English Task B. Vista.ue was able to rank 38/41 for Spanish Task A and 22/25 for Spanish Task B.</abstract>
      <url hash="c7b39ff1">S19-2094</url>
      <doi>10.18653/v1/S19-2094</doi>
    </paper>
    <paper id="95">
      <title><fixed-case>YNU</fixed-case> <fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: Attention and Capsule Ensemble for Identifying Hate Speech</title>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Haiyan</first><last>Ding</last></author>
      <pages>529–534</pages>
      <abstract>This paper describes the system submitted to SemEval 2019 Task 5: Multilingual detection of hate speech against immigrants and women in Twitter (hatEval). Its main purpose is to conduct hate speech detection on Twitter, which mainly includes two specific different targets, immigrants and women. We participate in both subtask A and subtask B for English. In order to address this task, we develope an ensemble of an attention-LSTM model based on HAN and an BiGRU-capsule model. Both models use fastText pre-trained embeddings, and we use this model in both subtasks. In comparison to other participating teams, our system is ranked 16th in the Sub-task A for English, and 12th in the Sub-task B for English.</abstract>
      <url hash="182cb66b">S19-2095</url>
      <doi>10.18653/v1/S19-2095</doi>
    </paper>
    <paper id="96">
      <title><fixed-case>YNU</fixed-case>_<fixed-case>DYX</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5: A Stacked <fixed-case>B</fixed-case>i<fixed-case>GRU</fixed-case> Model Based on Capsule Network in Detection of Hate</title>
      <author><first>Yunxia</first><last>Ding</last></author>
      <author><first>Xiaobing</first><last>Zhou</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>535–539</pages>
      <abstract>This paper describes our system designed for SemEval 2019 Task 5 “Shared Task on Multilingual Detection of Hate”.We only participate in subtask-A in English. To address this task, we present a stacked BiGRU model based on a capsule network system. In or- der to convert the tweets into corresponding vector representations and input them into the neural network, we use the fastText tools to get word representations. Then, the sentence representation is enriched by stacked Bidirectional Gated Recurrent Units (BiGRUs) and used as the input of capsule network. Our system achieves an average F1-score of 0.546 and ranks 3rd in the subtask-A in English.</abstract>
      <url hash="df706a61">S19-2096</url>
      <doi>10.18653/v1/S19-2096</doi>
    </paper>
    <paper id="97">
      <title>Amrita School of Engineering - <fixed-case>CSE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Manipulating Attention with Temporal Convolutional Neural Network for Offense Identification and Classification</title>
      <author><first>Murali</first><last>Sridharan</last></author>
      <author><first>Swapna</first><last>TR</last></author>
      <pages>540–546</pages>
      <abstract>With the proliferation and ubiquity of smart gadgets and smart devices, across the world, data generated by them has been growing at exponential rates; in particular social media platforms like Facebook, Twitter and Instagram have been generating voluminous data on a daily basis. According to Twitter’s usage statistics, about 500 million tweets are generated each day. While the tweets reflect the users’ opinions on several events across the world, there are tweets which are offensive in nature that need to be tagged under the hateful conduct policy of Twitter. Offensive tweets have to be identified, captured and processed further, for a variety of reasons, which include i) identifying offensive tweets in order to prevent violent/abusive behavior in Twitter (or any social media for that matter), ii) creating and maintaining a history of offensive tweets for individual users (would be helpful in creating meta-data for user profile), iii) inferring the sentiment of the users on particular event/issue/topic . We have employed neural network models which manipulate attention with Temporal Convolutional Neural Network for the three shared sub-tasks i) ATT-TCN (ATTention based Temporal Convolutional Neural Network) employed for shared sub-task A that yielded a best macro-F1 score of 0.46, ii) SAE-ATT-TCN(Self Attentive Embedding-ATTention based Temporal Convolutional Neural Network) employed for shared sub-task B and sub-task C that yielded best macro-F1 score of 0.61 and 0.51 respectively. Among the two variants ATT-TCN and SAE-ATT-TCN, the latter performed better.</abstract>
      <url hash="d9755c0b">S19-2097</url>
      <doi>10.18653/v1/S19-2097</doi>
    </paper>
    <paper id="98">
      <title>bhanodaig at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Categorizing Offensive Language in social media</title>
      <author><first>Ritesh</first><last>Kumar</last></author>
      <author><first>Guggilla</first><last>Bhanodai</last></author>
      <author><first>Rajendra</first><last>Pamula</last></author>
      <author><first>Maheswara Reddy</first><last>Chennuru</last></author>
      <pages>547–550</pages>
      <abstract>This paper describes the work that our team bhanodaig did at Indian Institute of Technology (ISM) towards OffensEval i.e. identifying and categorizing offensive language in social media. Out of three sub-tasks, we have participated in sub-task B: automatic categorization of offensive types. We perform the task of categorizing offensive language, whether the tweet is targeted insult or untargeted. We use Linear Support Vector Machine for classification. The official ranking metric is macro-averaged F1. Our system gets the score 0.5282 with accuracy 0.8792. However, as new entrant to the field, our scores are encouraging enough to work for better results in future.</abstract>
      <url hash="35a496ed">S19-2098</url>
      <doi>10.18653/v1/S19-2098</doi>
    </paper>
    <paper id="99">
      <title><fixed-case>BNU</fixed-case>-<fixed-case>HKBU</fixed-case> <fixed-case>UIC</fixed-case> <fixed-case>NLP</fixed-case> Team 2 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Detecting Offensive Language Using <fixed-case>BERT</fixed-case> model</title>
      <author><first>Zhenghao</first><last>Wu</last></author>
      <author><first>Hao</first><last>Zheng</last></author>
      <author><first>Jianming</first><last>Wang</last></author>
      <author><first>Weifeng</first><last>Su</last></author>
      <author><first>Jefferson</first><last>Fong</last></author>
      <pages>551–555</pages>
      <abstract>In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.</abstract>
      <url hash="92467cc0">S19-2099</url>
      <doi>10.18653/v1/S19-2099</doi>
    </paper>
    <paper id="100">
      <title><fixed-case>CAM</fixed-case>sterdam at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Neural and graph-based feature extraction for the identification of offensive tweets</title>
      <author><first>Guy</first><last>Aglionby</last></author>
      <author><first>Chris</first><last>Davis</last></author>
      <author><first>Pushkar</first><last>Mishra</last></author>
      <author><first>Andrew</first><last>Caines</last></author>
      <author><first>Helen</first><last>Yannakoudakis</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>556–563</pages>
      <abstract>We describe the CAMsterdam team entry to the SemEval-2019 Shared Task 6 on offensive language identification in Twitter data. Our proposed model learns to extract textual features using a multi-layer recurrent network, and then performs text classification using gradient-boosted decision trees (GBDT). A self-attention architecture enables the model to focus on the most relevant areas in the text. In order to enrich input representations, we use node2vec to learn globally optimised embeddings for hashtags, which are then given as additional features to the GBDT classifier. Our best model obtains 78.79% macro F1-score on detecting offensive language (subtask A), 66.32% on categorising offence types (targeted/untargeted; subtask B), and 55.36% on identifying the target of offence (subtask C).</abstract>
      <url hash="cd1f1ba1">S19-2100</url>
      <doi>10.18653/v1/S19-2100</doi>
    </paper>
    <paper id="101">
      <title><fixed-case>CN</fixed-case>-<fixed-case>HIT</fixed-case>-<fixed-case>MI</fixed-case>.<fixed-case>T</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Offensive Language Identification Based on <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> with Double Attention</title>
      <author><first>Yaojie</first><last>Zhang</last></author>
      <author><first>Bing</first><last>Xu</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <pages>564–570</pages>
      <abstract>Offensive language has become pervasive in social media. In Offensive Language Identification tasks, it may be difficult to predict accurately only according to the surface words. So we try to dig deeper semantic information of text. This paper presents use an attention-based two layers bidirectional longshort memory neural network (BiLSTM) for semantic feature extraction. Additionally, a residual connection mechanism is used to synthesize two different deep features, and an emoji attention mechanism is used to extract semantic information of emojis in text. We participated in three sub-tasks of SemEval 2019 Task 6 as CN-HIT-MI.T team. Our macro-averaged F1-score in sub-task A is 0.768, ranking 28/103. We got 0.638 in sub-task B, ranking 30/75. In sub-task C, we got 0.549, ranking 22/65. We also tried some other methods of not submitting results.</abstract>
      <url hash="eb9ee7db">S19-2101</url>
      <attachment type="supplementary" hash="0f4fc551">S19-2101.Supplementary.zip</attachment>
      <doi>10.18653/v1/S19-2101</doi>
    </paper>
    <paper id="102">
      <title><fixed-case>C</fixed-case>onv<fixed-case>AI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Offensive Language Identification and Categorization with Perspective and <fixed-case>BERT</fixed-case></title>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <author><first>Nithum</first><last>Thain</last></author>
      <author><first>Lucas</first><last>Dixon</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <pages>571–576</pages>
      <abstract>This paper presents the application of two strong baseline systems for toxicity detection and evaluates their performance in identifying and categorizing offensive language in social media. PERSPECTIVE is an API, that serves multiple machine learning models for the improvement of conversations online, as well as a toxicity detection system, trained on a wide variety of comments from platforms across the Internet. BERT is a recently popular language representation model, fine tuned per task and achieving state of the art performance in multiple NLP tasks. PERSPECTIVE performed better than BERT in detecting toxicity, but BERT was much better in categorizing the offensive type. Both baselines were ranked surprisingly high in the SEMEVAL-2019 OFFENSEVAL competition, PERSPECTIVE in detecting an offensive post (12th) and BERT in categorizing it (11th). The main contribution of this paper is the assessment of two strong baselines for the identification (PERSPECTIVE) and the categorization (BERT) of offensive language with little or no additional training data.</abstract>
      <url hash="7b91d480">S19-2102</url>
      <doi>10.18653/v1/S19-2102</doi>
    </paper>
    <paper id="103">
      <title><fixed-case>DA</fixed-case>-<fixed-case>LD</fixed-case>-<fixed-case>H</fixed-case>ildesheim at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Tracking Offensive Content with Deep Learning using Shallow Representation</title>
      <author><first>Sandip</first><last>Modha</last></author>
      <author><first>Prasenjit</first><last>Majumder</last></author>
      <author><first>Daksh</first><last>Patel</last></author>
      <pages>577–581</pages>
      <abstract>This paper presents the participation of team DA-LD-Hildesheim of Information Retrieval and Language Processing lab at DA-IICT, India in Semeval-19 OffenEval track. The aim of this shared task is to identify offensive content at fined-grained level granularity. The task is divided into three sub-tasks. The system is required to check whether social media posts contain any offensive or profane content or not, targeted or untargeted towards any entity and classifying targeted posts into the individual, group or other categories. Social media posts suffer from data sparsity problem, Therefore, the distributed word representation technique is chosen over the Bag-of-Words for the text representation. Since limited labeled data was available for the training, pre-trained word vectors are used and fine-tuned on this classification task. Various deep learning models based on LSTM, Bidirectional LSTM, CNN, and Stacked CNN are used for the classification. It has been observed that labeled data was highly affected with class imbalance and our technique to handle the class-balance was not effective, in fact performance was degraded in some of the runs. Macro F1 score is used as a primary evaluation metric for the performance. Our System achieves Macro F1 score = 0.7833 in sub-task A, 0.6456 in the sub-task B and 0.5533 in the sub-task C.</abstract>
      <url hash="caf65055">S19-2103</url>
      <doi>10.18653/v1/S19-2103</doi>
    </paper>
    <paper id="104">
      <title><fixed-case>D</fixed-case>eep<fixed-case>A</fixed-case>nalyzer at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: A deep learning-based ensemble method for identifying offensive tweets</title>
      <author><first>Gretel Liz</first><last>De la Peña</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>582–586</pages>
      <abstract>This paper describes the system we developed for SemEval 2019 on Identifying and Categorizing Offensive Language in Social Media (OffensEval - Task 6). The task focuses on offensive language in tweets. It is organized into three sub-tasks for offensive language identification; automatic categorization of offense types and offense target identification. The approach for the first subtask is a deep learning-based ensemble method which uses a Bidirectional LSTM Recurrent Neural Network and a Convolutional Neural Network. Additionally we use the information from part-of-speech tagging of tweets for target identification and combine previous results for categorization of offense types.</abstract>
      <url hash="52e79ec1">S19-2104</url>
      <doi>10.18653/v1/S19-2104</doi>
    </paper>
    <paper id="105">
      <title><fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Detecting Offensive language using Neural Networks</title>
      <author><first>Prashant</first><last>Kapil</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Dipankar</first><last>Das</last></author>
      <pages>587–592</pages>
      <abstract>In this paper we built several deep learning architectures to participate in shared task OffensEval: Identifying and categorizing Offensive language in Social media by semEval-2019. The dataset was annotated with three level annotation schemes and task was to detect between offensive and not offensive, categorization and target identification in offensive contents. Deep learning models with POS information as feature were also leveraged for classification. The three best models that performed best on individual sub tasks are stacking of CNN-Bi-LSTM with Attention, BiLSTM with POS information added with word features and Bi-LSTM for third task. Our models achieved a Macro F1 score of 0.7594, 0.5378 and 0.4588 in Task(A,B,C) respectively with rank of 33rd, 54th and 52nd out of 103, 75 and 65 submissions.The three best models that performed best on individual sub task are using Neural Networks.</abstract>
      <url hash="42b6cf6a">S19-2105</url>
      <doi>10.18653/v1/S19-2105</doi>
    </paper>
    <paper id="106">
      <title><fixed-case>D</fixed-case>uluth at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Lexical Approaches to Identify and Categorize Offensive Tweets</title>
      <author><first>Ted</first><last>Pedersen</last></author>
      <pages>593–599</pages>
      <abstract>This paper describes the Duluth systems that participated in SemEval–2019 Task 6, Identifying and Categorizing Offensive Language in Social Media (OffensEval). For the most part these systems took traditional Machine Learning approaches that built classifiers from lexical features found in manually labeled training data. However, our most successful system for classifying a tweet as offensive (or not) was a rule-based black–list approach, and we also experimented with combining the training data from two different but related SemEval tasks. Our best systems in each of the three OffensEval tasks placed in the middle of the comparative evaluation, ranking 57th of 103 in task A, 39th of 75 in task B, and 44th of 65 in task C.</abstract>
      <url hash="a8819816">S19-2106</url>
      <doi>10.18653/v1/S19-2106</doi>
    </paper>
    <paper id="107">
      <title>Emad at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Offensive Language Identification using Traditional Machine Learning and Deep Learning approaches</title>
      <author><first>Emad</first><last>Kebriaei</last></author>
      <author><first>Samaneh</first><last>Karimi</last></author>
      <author><first>Nazanin</first><last>Sabri</last></author>
      <author><first>Azadeh</first><last>Shakery</last></author>
      <pages>600–603</pages>
      <abstract>In this paper, the used methods and the results obtained by our team, entitled Emad, on the OffensEval 2019 shared task organized at SemEval 2019 are presented. The OffensEval shared task includes three sub-tasks namely Offensive language identification, Automatic categorization of offense types and Offense target identification. We participated in sub-task A and tried various methods including traditional machine learning methods, deep learning methods and also a combination of the first two sets of methods. We also proposed a data augmentation method using word embedding to improve the performance of our methods. The results show that the augmentation approach outperforms other methods in terms of macro-f1.</abstract>
      <url hash="f945dee0">S19-2107</url>
      <doi>10.18653/v1/S19-2107</doi>
    </paper>
    <paper id="108">
      <title>Embeddia at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Detecting Hate with Neural Network and Transfer Learning Approaches</title>
      <author><first>Andraž</first><last>Pelicon</last></author>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Petra</first><last>Kralj Novak</last></author>
      <pages>604–610</pages>
      <abstract>SemEval 2019 Task 6 was OffensEval: Identifying and Categorizing Offensive Language in Social Media. The task was further divided into three sub-tasks: offensive language identification, automatic categorization of offense types, and offense target identification. In this paper, we present the approaches used by the Embeddia team, who qualified as fourth, eighteenth and fifth on the tree sub-tasks. A different model was trained for each sub-task. For the first sub-task, we used a BERT model fine-tuned on the OLID dataset, while for the second and third tasks we developed a custom neural network architecture which combines bag-of-words features and automatically generated sequence-based features. Our results show that combining automatically and manually crafted features fed into a neural architecture outperform transfer learning approach on more unbalanced datasets.</abstract>
      <url hash="c9a3bd14">S19-2108</url>
      <doi>10.18653/v1/S19-2108</doi>
    </paper>
    <paper id="109">
      <title>Fermi at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media using Sentence Embeddings</title>
      <author><first>Vijayasaradhi</first><last>Indurthi</last></author>
      <author><first>Bakhtiyar</first><last>Syed</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Manish</first><last>Gupta</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <pages>611–616</pages>
      <abstract>This paper describes our system (Fermi) for Task 6: OffensEval: Identifying and Categorizing Offensive Language in Social Media of SemEval-2019. We participated in all the three sub-tasks within Task 6. We evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team Fermi’s model achieved an F1-score of 64.40%, 62.00% and 62.60% for sub-task A, B and C respectively on the official leaderboard. Our model for sub-task C which uses pre-trained ELMo embeddings for transforming the input and uses SVM (RBF kernel) for training, scored third position on the official leaderboard. Through the paper we provide a detailed description of the approach, as well as the results obtained for the task.</abstract>
      <url hash="398f419e">S19-2109</url>
      <doi>10.18653/v1/S19-2109</doi>
    </paper>
    <paper id="110">
      <title>Ghmerti at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: A Deep Word- and Character-based Approach to Offensive Language Identification</title>
      <author><first>Ehsan</first><last>Doostmohammadi</last></author>
      <author><first>Hossein</first><last>Sameti</last></author>
      <author><first>Ali</first><last>Saffar</last></author>
      <pages>617–621</pages>
      <abstract>This paper presents the models submitted by Ghmerti team for subtasks A and B of the OffensEval shared task at SemEval 2019. OffensEval addresses the problem of identifying and categorizing offensive language in social media in three subtasks; whether or not a content is offensive (subtask A), whether it is targeted (subtask B) towards an individual, a group, or other entities (subtask C). The proposed approach includes character-level Convolutional Neural Network, word-level Recurrent Neural Network, and some preprocessing. The performance achieved by the proposed model is 77.93% macro-averaged F1-score.</abstract>
      <url hash="d0d70f2e">S19-2110</url>
      <doi>10.18653/v1/S19-2110</doi>
    </paper>
    <paper id="111">
      <title><fixed-case>HAD</fixed-case>-<fixed-case>T</fixed-case>übingen at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Deep Learning Analysis of Offensive Language on <fixed-case>T</fixed-case>witter: Identification and Categorization</title>
      <author><first>Himanshu</first><last>Bansal</last></author>
      <author><first>Daniel</first><last>Nagel</last></author>
      <author><first>Anita</first><last>Soloveva</last></author>
      <pages>622–627</pages>
      <abstract>This paper describes the submissions of our team, HAD-Tübingen, for the SemEval 2019 - Task 6: “OffensEval: Identifying and Categorizing Offensive Language in Social Media”. We participated in all the three sub-tasks: Sub-task A - “Offensive language identification”, sub-task B - “Automatic categorization of offense types” and sub-task C - “Offense target identification”. As a baseline model we used a Long short-term memory recurrent neural network (LSTM) to identify and categorize offensive tweets. For all the tasks we experimented with external databases in a postprocessing step to enhance the results made by our model. The best macro-average F1 scores obtained for the sub-tasks A, B and C are 0.73, 0.52, and 0.37, respectively.</abstract>
      <url hash="4464b2a5">S19-2111</url>
      <doi>10.18653/v1/S19-2111</doi>
    </paper>
    <paper id="112">
      <title><fixed-case>HHU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Context Does Matter - Tackling Offensive Language Identification and Categorization with <fixed-case>ELM</fixed-case>o</title>
      <author><first>Alexander</first><last>Oberstrass</last></author>
      <author><first>Julia</first><last>Romberg</last></author>
      <author><first>Anke</first><last>Stoll</last></author>
      <author><first>Stefan</first><last>Conrad</last></author>
      <pages>628–634</pages>
      <abstract>We present our results for OffensEval: Identifying and Categorizing Offensive Language in Social Media (SemEval 2019 - Task 6). Our results show that context embeddings are important features for the three different sub-tasks in connection with classical machine and with deep learning. Our best model reached place 3 of 75 in sub-task B with a macro <tex-math>F_1</tex-math> of 0.719. Our approaches for sub-task A and C perform less well but could also deliver promising results.</abstract>
      <url hash="1e357b3c">S19-2112</url>
      <doi>10.18653/v1/S19-2112</doi>
    </paper>
    <paper id="113">
      <title>Hope at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Mining social media language to discover offensive language</title>
      <author><first>Gabriel Florentin</first><last>Patras</last></author>
      <author><first>Diana Florina</first><last>Lungu</last></author>
      <author><first>Daniela</first><last>Gifu</last></author>
      <author><first>Diana</first><last>Trandabat</last></author>
      <pages>635–638</pages>
      <abstract>User’s content share through social media has reached huge proportions nowadays. However, along with the free expression of thoughts on social media, people risk getting exposed to various aggressive statements. In this paper, we present a system able to identify and classify offensive user-generated content.</abstract>
      <url hash="effcb26b">S19-2113</url>
      <doi>10.18653/v1/S19-2113</doi>
    </paper>
    <paper id="114">
      <title><fixed-case>INGEOTEC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 5 and Task 6: A Genetic Programming Approach for Text Classification</title>
      <author><first>Mario</first><last>Graff</last></author>
      <author><first>Sabino</first><last>Miranda-Jiménez</last></author>
      <author><first>Eric</first><last>Tellez</last></author>
      <author><first>Daniela Alejandra</first><last>Ochoa</last></author>
      <pages>639–644</pages>
      <abstract>This paper describes our participation in HatEval and OffensEval challenges for English and Spanish languages. We used several approaches, B4MSA, FastText, and EvoMSA. Best results were achieved with EvoMSA, which is a multilingual and domain-independent architecture that combines the prediction of different knowledge sources to solve text classification problems.</abstract>
      <url hash="c14e019c">S19-2114</url>
      <doi>10.18653/v1/S19-2114</doi>
    </paper>
    <paper id="115">
      <title><fixed-case>JCTICOL</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Classifying Offensive Language in Social Media using Deep Learning Methods, Word/Character N-gram Features, and Preprocessing Methods</title>
      <author><first>Yaakov</first><last>HaCohen-Kerner</last></author>
      <author><first>Ziv</first><last>Ben-David</last></author>
      <author><first>Gal</first><last>Didi</last></author>
      <author><first>Eli</first><last>Cahn</last></author>
      <author><first>Shalom</first><last>Rochman</last></author>
      <author><first>Elyashiv</first><last>Shayovitz</last></author>
      <pages>645–651</pages>
      <abstract>In this paper, we describe our submissions to SemEval-2019 task 6 contest. We tackled all three sub-tasks in this task “OffensEval - Identifying and Categorizing Offensive Language in Social Media”. In our system called JCTICOL (Jerusalem College of Technology Identifies and Categorizes Offensive Language), we applied various supervised ML methods. We applied various combinations of word/character n-gram features using the TF-IDF scheme. In addition, we applied various combinations of seven basic preprocessing methods. Our best submission, an RNN model was ranked at the 25th position out of 65 submissions for the most complex sub-task (C).</abstract>
      <url hash="1ee02284">S19-2115</url>
      <doi>10.18653/v1/S19-2115</doi>
    </paper>
    <paper id="116">
      <title>jhan014 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media</title>
      <author><first>Jiahui</first><last>Han</last></author>
      <author><first>Shengtan</first><last>Wu</last></author>
      <author><first>Xinyu</first><last>Liu</last></author>
      <pages>652–656</pages>
      <abstract>In this paper, we present two methods to identify and categorize the offensive language in Twitter. In the first method, we establish a probabilistic model to evaluate the sentence offensiveness level and target level according to different sub-tasks. In the second method, we develop a deep neural network consisting of bidirectional recurrent layers with Gated Recurrent Unit (GRU) cells and fully connected layers. In the comparison of two methods, we find both method has its own advantages and drawbacks while they have similar accuracy.</abstract>
      <url hash="fe8b07a3">S19-2116</url>
      <doi>10.18653/v1/S19-2116</doi>
    </paper>
    <paper id="117">
      <title><fixed-case>JTML</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Offensive Tweets Identification using Convolutional Neural Networks</title>
      <author><first>Johnny</first><last>Torres</last></author>
      <author><first>Carmen</first><last>Vaca</last></author>
      <pages>657–661</pages>
      <abstract>In this paper, we propose the use of a Convolutional Neural Network (CNN) to identify offensive tweets, as well as the type and target of the offense. We use an end-to-end model (i.e., no preprocessing) and fine-tune pre-trained embeddings (FastText) during training for learning words’ representation. We compare the proposed CNN model to a baseline model, such as Linear Regression, and several neural models. The results show that CNN outperforms other models, and stands as a simple but strong baseline in comparison to other systems submitted to the Shared Task.</abstract>
      <url hash="25d2016f">S19-2117</url>
      <doi>10.18653/v1/S19-2117</doi>
    </paper>
    <paper id="118">
      <title><fixed-case>JU</fixed-case>_<fixed-case>ETCE</fixed-case>_17_21 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Efficient Machine Learning and Neural Network Approaches for Identifying and Categorizing Offensive Language in Tweets</title>
      <author><first>Preeti</first><last>Mukherjee</last></author>
      <author><first>Mainak</first><last>Pal</last></author>
      <author><first>Somnath</first><last>Banerjee</last></author>
      <author><first>Sudip Kumar</first><last>Naskar</last></author>
      <pages>662–667</pages>
      <abstract>This paper describes our system submissions as part of our participation (team name: JU_ETCE_17_21) in the SemEval 2019 shared task 6: “OffensEval: Identifying and Catego- rizing Offensive Language in Social Media”. We participated in all the three sub-tasks: i) Sub-task A: offensive language identification, ii) Sub-task B: automatic categorization of of- fense types, and iii) Sub-task C: offense target identification. We employed machine learn- ing as well as deep learning approaches for the sub-tasks. We employed Convolutional Neural Network (CNN) and Recursive Neu- ral Network (RNN) Long Short-Term Memory (LSTM) with pre-trained word embeddings. We used both word2vec and Glove pre-trained word embeddings. We obtained the best F1- score using CNN based model for sub-task A, LSTM based model for sub-task B and Lo- gistic Regression based model for sub-task C. Our best submissions achieved 0.7844, 0.5459 and 0.48 F1-scores for sub-task A, sub-task B and sub-task C respectively.</abstract>
      <url hash="deed58ed">S19-2118</url>
      <doi>10.18653/v1/S19-2118</doi>
    </paper>
    <paper id="119">
      <title><fixed-case>KMI</fixed-case>-Coling at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Exploring N-grams for Offensive Language detection</title>
      <author><first>Priya</first><last>Rani</last></author>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <pages>668–671</pages>
      <abstract>In this paper, we present the system description of Offensive language detection tool which is developed by the KMI_Coling under the OffensEval Shared task. The OffensEval Shared Task was conducted in SemEval 2019 workshop. To develop the system, we have explored n-grams up to 8-gram and trained three different namely A, B and C systems for three different subtasks within the OffensEval task which achieves 79.76%, 87.91% and 44.37% accuracy respectively. The task was completed using the dataset provided to us by the OffensEval organisers was the part of OLID dataset. It consists of 13,240 tweets extracted from twitter and were annotated at three levels using crowdsourcing.</abstract>
      <url hash="7074d3f7">S19-2119</url>
      <doi>10.18653/v1/S19-2119</doi>
    </paper>
    <paper id="120">
      <title><fixed-case>L</fixed-case>a<fixed-case>STUS</fixed-case>/<fixed-case>TALN</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Identification and Categorization of Offensive Language in Social Media with Attention-based <fixed-case>B</fixed-case>i-<fixed-case>LSTM</fixed-case> model</title>
      <author><first>Lutfiye Seda</first><last>Mut Altin</last></author>
      <author><first>Àlex</first><last>Bravo Serrano</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>672–677</pages>
      <abstract>We present a bidirectional Long-Short Term Memory network for identifying offensive language in Twitter. Our system has been developed in the context of the SemEval 2019 Task 6 which comprises three different sub-tasks, namely A: Offensive Language Detection, B: Categorization of Offensive Language, C: Offensive Language Target Identification. We used a pre-trained Word Embeddings in tweet data, including information about emojis and hashtags. Our approach achieves good performance in the three sub-tasks.</abstract>
      <url hash="54124408">S19-2120</url>
      <doi>10.18653/v1/S19-2120</doi>
    </paper>
    <paper id="121">
      <title><fixed-case>LTL</fixed-case>-<fixed-case>UDE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: <fixed-case>BERT</fixed-case> and Two-Vote Classification for Categorizing Offensiveness</title>
      <author><first>Piush</first><last>Aggarwal</last></author>
      <author><first>Tobias</first><last>Horsmann</last></author>
      <author><first>Michael</first><last>Wojatzki</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>678–682</pages>
      <abstract>We present results for Subtask A and C of SemEval 2019 Shared Task 6. In Subtask A, we experiment with an embedding representation of postings and use BERT to categorize postings. Our best result reaches the 10th place (out of 103). In Subtask C, we applied a two-vote classification approach with minority fallback, which is placed on the 19th rank (out of 65).</abstract>
      <url hash="f92564e7">S19-2121</url>
      <doi>10.18653/v1/S19-2121</doi>
    </paper>
    <paper id="122">
      <title><fixed-case>MIDAS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Identifying Offensive Posts and Targeted Offense from <fixed-case>T</fixed-case>witter</title>
      <author><first>Debanjan</first><last>Mahata</last></author>
      <author><first>Haimin</first><last>Zhang</last></author>
      <author><first>Karan</first><last>Uppal</last></author>
      <author><first>Yaman</first><last>Kumar</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <author><first>Simra</first><last>Shahid</last></author>
      <author><first>Laiba</first><last>Mehnaz</last></author>
      <author><first>Sarthak</first><last>Anand</last></author>
      <pages>683–690</pages>
      <abstract>In this paper we present our approach and the system description for Sub Task A and Sub Task B of SemEval 2019 Task 6: Identifying and Categorizing Offensive Language in Social Media. Sub Task A involves identifying if a given tweet is offensive and Sub Task B involves detecting if an offensive tweet is targeted towards someone (group or an individual). Our models for Sub Task A is based on an ensemble of Convolutional Neural Network and Bidirectional LSTM, whereas for Sub Task B, we rely on a set of heuristics derived from the training data. We provide detailed analysis of the results obtained using the trained models. Our team ranked 5th out of 103 participants in Sub Task A, achieving a macro F1 score of 0.807, and ranked 8th out of 75 participants achieving a macro F1 of 0.695.</abstract>
      <url hash="542419fa">S19-2122</url>
      <doi>10.18653/v1/S19-2122</doi>
    </paper>
    <paper id="123">
      <title>Nikolov-Radivchev at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Offensive Tweet Classification with <fixed-case>BERT</fixed-case> and Ensembles</title>
      <author><first>Alex</first><last>Nikolov</last></author>
      <author><first>Victor</first><last>Radivchev</last></author>
      <pages>691–695</pages>
      <abstract>This paper examines different approaches and models towards offensive tweet classification which were used as a part of the OffensEval 2019 competition. It reviews Tweet preprocessing, techniques for overcoming unbalanced class distribution in the provided test data, and comparison of multiple attempted machine learning models.</abstract>
      <url hash="5a46fd99">S19-2123</url>
      <doi>10.18653/v1/S19-2123</doi>
    </paper>
    <paper id="124">
      <title><fixed-case>NIT</fixed-case>_<fixed-case>A</fixed-case>gartala_<fixed-case>NLP</fixed-case>_<fixed-case>T</fixed-case>eam at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: An Ensemble Approach to Identifying and Categorizing Offensive Language in <fixed-case>T</fixed-case>witter Social Media Corpora</title>
      <author><first>Steve Durairaj</first><last>Swamy</last></author>
      <author><first>Anupam</first><last>Jamatia</last></author>
      <author><first>Björn</first><last>Gambäck</last></author>
      <author><first>Amitava</first><last>Das</last></author>
      <pages>696–703</pages>
      <abstract>The paper describes the systems submitted to OffensEval (SemEval 2019, Task 6) on ‘Identifying and Categorizing Offensive Language in Social Media’ by the ‘NIT_Agartala_NLP_Team’. A Twitter annotated dataset of 13,240 English tweets was provided by the task organizers to train the individual models, with the best results obtained using an ensemble model composed of six different classifiers. The ensemble model produced macro-averaged F1-scores of 0.7434, 0.7078 and 0.4853 on Subtasks A, B, and C, respectively. The paper highlights the overall low predictive nature of various linguistic features and surface level count features, as well as the limitations of a traditional machine learning approach when compared to a Deep Learning counterpart.</abstract>
      <url hash="f5a81cb9">S19-2124</url>
      <doi>10.18653/v1/S19-2124</doi>
    </paper>
    <paper id="125">
      <title><fixed-case>NLP</fixed-case>@<fixed-case>UIOWA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Classifying the Crass using Multi-windowed <fixed-case>CNN</fixed-case>s</title>
      <author><first>Jonathan</first><last>Rusert</last></author>
      <author><first>Padmini</first><last>Srinivasan</last></author>
      <pages>704–711</pages>
      <abstract>This paper proposes a system for OffensEval (SemEval 2019 Task 6), which calls for a system to classify offensive language into several categories. Our system is a text based CNN, which learns only from the provided training data. Our system achieves 80 - 90% accuracy for the binary classification problems (offensive vs not offensive and targeted vs untargeted) and 63% accuracy for trinary classification (group vs individual vs other).</abstract>
      <url hash="04380059">S19-2125</url>
      <doi>10.18653/v1/S19-2125</doi>
    </paper>
    <paper id="126">
      <title><fixed-case>NLPR</fixed-case>@<fixed-case>SRPOL</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6 and Task 5: Linguistically enhanced deep learning offensive sentence classifier</title>
      <author><first>Alessandro</first><last>Seganti</last></author>
      <author><first>Helena</first><last>Sobol</last></author>
      <author><first>Iryna</first><last>Orlova</last></author>
      <author><first>Hannam</first><last>Kim</last></author>
      <author><first>Jakub</first><last>Staniszewski</last></author>
      <author><first>Tymoteusz</first><last>Krumholc</last></author>
      <author><first>Krystian</first><last>Koziel</last></author>
      <pages>712–721</pages>
      <abstract>The paper presents a system developed for the SemEval-2019 competition Task 5 hat- Eval Basile et al. (2019) (team name: LU Team) and Task 6 OffensEval Zampieri et al. (2019b) (team name: NLPR@SRPOL), where we achieved 2nd position in Subtask C. The system combines in an ensemble several models (LSTM, Transformer, OpenAI’s GPT, Random forest, SVM) with various embeddings (custom, ELMo, fastText, Universal Encoder) together with additional linguistic features (number of blacklisted words, special characters, etc.). The system works with a multi-tier blacklist and a large corpus of crawled data, annotated for general offensiveness. In the paper we do an extensive analysis of our results and show how the combination of features and embedding affect the performance of the models.</abstract>
      <url hash="b5aacbad">S19-2126</url>
      <doi>10.18653/v1/S19-2126</doi>
    </paper>
    <paper id="127">
      <title>nlp<fixed-case>UP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: A Deep Neural Language Model for Offensive Language Detection</title>
      <author><first>Jelena</first><last>Mitrović</last></author>
      <author><first>Bastian</first><last>Birkeneder</last></author>
      <author><first>Michael</first><last>Granitzer</last></author>
      <pages>722–726</pages>
      <abstract>This paper presents our submission for the SemEval shared task 6, sub-task A on the identification of offensive language. Our proposed model, C-BiGRU, combines a Convolutional Neural Network (CNN) with a bidirectional Recurrent Neural Network (RNN). We utilize word2vec to capture the semantic similarities between words. This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non-offensive tweets. In addition, we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets. Our model achieved a macro F1-score of 79.40% on the SemEval dataset.</abstract>
      <url hash="7b778ccd">S19-2127</url>
      <doi>10.18653/v1/S19-2127</doi>
    </paper>
    <paper id="128">
      <title>Pardeep at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media using Deep Learning</title>
      <author><first>Pardeep</first><last>Singh</last></author>
      <author><first>Satish</first><last>Chand</last></author>
      <pages>727–734</pages>
      <abstract>The rise of social media has made information exchange faster and easier among the people. However, in recent times, the use of offensive language has seen an upsurge in social media. The main challenge for a service provider is to correctly identify such offensive posts and take necessary action to monitor and control their spread. In this work, we try to address this problem by using sophisticated deep learning techniques like LSTM, Bidirectional LSTM and Bidirectional GRU. Our proposed approach solves 3 different Sub-tasks provided in the SemEval-2019 task 6 which incorporates identification of offensive tweets as well as their categorization. We obtain significantly better results in the leader-board for Sub-task B and decent results for Sub-task A and Subtask C validating the fact that the proposed models can be used for automating the offensive post-detection task in social media.</abstract>
      <url hash="cb48e286">S19-2128</url>
      <doi>10.18653/v1/S19-2128</doi>
    </paper>
    <paper id="129">
      <title><fixed-case>SINAI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Incorporating lexicon knowledge into <fixed-case>SVM</fixed-case> learning to identify and categorize offensive language in social media</title>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last></author>
      <author><first>M. Dolores</first><last>Molina-González</last></author>
      <author><first>Maite</first><last>Martin</last></author>
      <author><first>L. Alfonso</first><last>Ureña-López</last></author>
      <pages>735–738</pages>
      <abstract>Offensive language has an impact across society. The use of social media has aggravated this issue among online users, causing suicides in the worst cases. For this reason, it is important to develop systems capable of identifying and detecting offensive language in text automatically. In this paper, we developed a system to classify offensive tweets as part of our participation in SemEval-2019 Task 6: OffensEval. Our main contribution is the integration of lexical features in the classification using the SVM algorithm.</abstract>
      <url hash="4929df78">S19-2129</url>
      <doi>10.18653/v1/S19-2129</doi>
    </paper>
    <paper id="130">
      <title><fixed-case>SSN</fixed-case>_<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Offensive Language Identification in Social Media using Traditional and Deep Machine Learning Approaches</title>
      <author><first>Thenmozhi</first><last>D.</last></author>
      <author><first>Senthil Kumar</first><last>B.</last></author>
      <author><first>Srinethe</first><last>Sharavanan</last></author>
      <author><first>Aravindan</first><last>Chandrabose</last></author>
      <pages>739–744</pages>
      <abstract>Offensive language identification (OLI) in user generated text is automatic detection of any profanity, insult, obscenity, racism or vulgarity that degrades an individual or a group. It is helpful for hate speech detection, flame detection and cyber bullying. Due to immense growth of accessibility to social media, OLI helps to avoid abuse and hurts. In this paper, we present deep and traditional machine learning approaches for OLI. In deep learning approach, we have used bi-directional LSTM with different attention mechanisms to build the models and in traditional machine learning, TF-IDF weighting schemes with classifiers namely Multinomial Naive Bayes and Support Vector Machines with Stochastic Gradient Descent optimizer are used for model building. The approaches are evaluated on the OffensEval@SemEval2019 dataset and our team SSN_NLP submitted runs for three tasks of OffensEval shared task. The best runs of SSN_NLP obtained the F1 scores as 0.53, 0.48, 0.3 and the accuracies as 0.63, 0.84 and 0.42 for the tasks A, B and C respectively. Our approaches improved the base line F1 scores by 12%, 26% and 14% for Task A, B and C respectively.</abstract>
      <url hash="9ed97642">S19-2130</url>
      <doi>10.18653/v1/S19-2130</doi>
    </paper>
    <paper id="131">
      <title>Stop <fixed-case>P</fixed-case>ropag<fixed-case>H</fixed-case>ate at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Tasks 5 and 6: Are abusive language classification results reproducible?</title>
      <author><first>Paula</first><last>Fortuna</last></author>
      <author><first>Juan</first><last>Soler-Company</last></author>
      <author><first>Sérgio</first><last>Nunes</last></author>
      <pages>745–752</pages>
      <abstract>This paper summarizes the participation of Stop PropagHate team at SemEval 2019. Our approach is based on replicating one of the most relevant works on the literature, using word embeddings and LSTM. After circumventing some of the problems of the original code, we found poor results when applying it to the HatEval contest (F1=0.45). We think this is due mainly to inconsistencies in the data of this contest. Finally, for the OffensEval the classifier performed well (F1=0.74), proving to have a better performance for offense detection than for hate speech.</abstract>
      <url hash="a470cfb6">S19-2131</url>
      <doi>10.18653/v1/S19-2131</doi>
    </paper>
    <paper id="132">
      <title><fixed-case>TECHSSN</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Identifying and Categorizing Offensive Language in Tweets using Deep Neural Networks</title>
      <author><first>Angel</first><last>Suseelan</last></author>
      <author><first>Rajalakshmi</first><last>S</last></author>
      <author><first>Logesh</first><last>B</last></author>
      <author><first>Harshini</first><last>S</last></author>
      <author><first>Geetika</first><last>B</last></author>
      <author><first>Dyaneswaran</first><last>S</last></author>
      <author><first>S Milton</first><last>Rajendram</last></author>
      <author><first>Mirnalinee</first><last>T T</last></author>
      <pages>753–758</pages>
      <abstract>Task 6 of SemEval 2019 involves identifying and categorizing offensive language in social media. The systems developed by TECHSSN team uses multi-level classification techniques. We have developed two systems. In the first system, the first level of classification is done by a multi-branch 2D CNN classifier with Google’s pre-trained Word2Vec embedding and the second level of classification by string matching technique supported by offensive and bad words dictionary. The second system uses a multi-branch 1D CNN classifier with Glove pre-trained embedding layer for the first level of classification and string matching for the second level of classification. Input data with a probability of less than 0.70 in the first level are passed on to the second level. The misclassified examples are classified correctly in the second level.</abstract>
      <url hash="52900fbd">S19-2132</url>
      <doi>10.18653/v1/S19-2132</doi>
    </paper>
    <paper id="133">
      <title>The Titans at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Offensive Language Identification, Categorization and Target Identification</title>
      <author><first>Avishek</first><last>Garain</last></author>
      <author><first>Arpan</first><last>Basu</last></author>
      <pages>759–762</pages>
      <abstract>This system paper is a description of the system submitted to “SemEval-2019 Task 6”, where we had to detect offensive language in Twitter. There were two specific target audiences, immigrants and women. The language of the tweets was English. We were required to first detect whether a tweet contains offensive content, and then we had to find out whether the tweet was targeted against some individual, group or other entity. Finally we were required to classify the targeted audience.</abstract>
      <url hash="5e5b23da">S19-2133</url>
      <doi>10.18653/v1/S19-2133</doi>
    </paper>
    <paper id="134">
      <title><fixed-case>T</fixed-case>ü<fixed-case>K</fixed-case>a<fixed-case>S</fixed-case>t at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Something Old, Something Neu(ral): Traditional and Neural Approaches to Offensive Text Classification</title>
      <author><first>Madeeswaran</first><last>Kannan</last></author>
      <author><first>Lukas</first><last>Stein</last></author>
      <pages>763–769</pages>
      <abstract>We describe our system (TüKaSt) submitted for Task 6: Offensive Language Classification, at SemEval 2019. We developed multiple SVM classifier models that used sentence-level dense vector representations of tweets enriched with sentiment information and term-weighting. Our best results achieved F1 scores of 0.734, 0.660 and 0.465 in the first, second and third sub-tasks respectively. We also describe a neural network model that was developed in parallel but not used during evaluation due to time constraints.</abstract>
      <url hash="248be486">S19-2134</url>
      <doi>10.18653/v1/S19-2134</doi>
    </paper>
    <paper id="135">
      <title><fixed-case>TUVD</fixed-case> team at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Offense Target Identification</title>
      <author><first>Elena</first><last>Shushkevich</last></author>
      <author><first>John</first><last>Cardiff</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>770–774</pages>
      <abstract>This article presents our approach for detecting a target of offensive messages in Twitter, including Individual, Group and Others classes. The model we have created is an ensemble of simpler models, including Logistic Regression, Naive Bayes, Support Vector Machine and the interpolation between Logistic Regression and Naive Bayes with 0.25 coefficient of interpolation. The model allows us to achieve 0.547 macro F1-score.</abstract>
      <url hash="bd19f2e5">S19-2135</url>
      <doi>10.18653/v1/S19-2135</doi>
    </paper>
    <paper id="136">
      <title><fixed-case>UBC</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Ensemble Learning of Offensive Content With Enhanced Training Data</title>
      <author><first>Arun</first><last>Rajendran</last></author>
      <author><first>Chiyu</first><last>Zhang</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>775–781</pages>
      <abstract>We examine learning offensive content on Twitter with limited, imbalanced data. For the purpose, we investigate the utility of using various data enhancement methods with a host of classical ensemble classifiers. Among the 75 participating teams in SemEval-2019 sub-task B, our system ranks 6th (with 0.706 macro F1-score). For sub-task C, among the 65 participating teams, our system ranks 9th (with 0.587 macro F1-score).</abstract>
      <url hash="608e3264">S19-2136</url>
      <doi>10.18653/v1/S19-2136</doi>
    </paper>
    <paper id="137">
      <title><fixed-case>UHH</fixed-case>-<fixed-case>LT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Supervised vs. Unsupervised Transfer Learning for Offensive Language Detection</title>
      <author><first>Gregor</first><last>Wiedemann</last></author>
      <author><first>Eugen</first><last>Ruppert</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>782–787</pages>
      <abstract>We present a neural network based approach of transfer learning for offensive language detection. For our system, we compare two types of knowledge transfer: supervised and unsupervised pre-training. Supervised pre-training of our bidirectional GRU-3-CNN architecture is performed as multi-task learning of parallel training of five different tasks. The selected tasks are supervised classification problems from public NLP resources with some overlap to offensive language such as sentiment detection, emoji classification, and aggressive language classification. Unsupervised transfer learning is performed with a thematic clustering of 40M unlabeled tweets via LDA. Based on this dataset, pre-training is performed by predicting the main topic of a tweet. Results indicate that unsupervised transfer from large datasets performs slightly better than supervised training on small ‘near target category’ datasets. In the SemEval Task, our system ranks 14 out of 103 participants.</abstract>
      <url hash="b137e040">S19-2137</url>
      <doi>10.18653/v1/S19-2137</doi>
    </paper>
    <paper id="138">
      <title><fixed-case>UM</fixed-case>-<fixed-case>IU</fixed-case>@<fixed-case>LING</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Identifying Offensive Tweets Using <fixed-case>BERT</fixed-case> and <fixed-case>SVM</fixed-case>s</title>
      <author><first>Jian</first><last>Zhu</last></author>
      <author><first>Zuoyu</first><last>Tian</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <pages>788–795</pages>
      <abstract>This paper describes the UM-IU@LING’s system for the SemEval 2019 Task 6: Offens-Eval. We take a mixed approach to identify and categorize hate speech in social media. In subtask A, we fine-tuned a BERT based classifier to detect abusive content in tweets, achieving a macro F1 score of 0.8136 on the test data, thus reaching the 3rd rank out of 103 submissions. In subtasks B and C, we used a linear SVM with selected character n-gram features. For subtask C, our system could identify the target of abuse with a macro F1 score of 0.5243, ranking it 27th out of 65 submissions.</abstract>
      <url hash="361e7717">S19-2138</url>
      <doi>10.18653/v1/S19-2138</doi>
    </paper>
    <paper id="139">
      <title><fixed-case>USF</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Offensive Language Detection Using <fixed-case>LSTM</fixed-case> With Word Embeddings</title>
      <author><first>Bharti</first><last>Goel</last></author>
      <author><first>Ravi</first><last>Sharma</last></author>
      <pages>796–800</pages>
      <abstract>In this paper, we present a system description for the SemEval-2019 Task 6 submitted by our team. For the task, our system takes tweet as an input and determine if the tweet is offensive or non-offensive (Sub-task A). In case a tweet is offensive, our system identifies if a tweet is targeted (insult or threat) or non-targeted like swearing (Sub-task B). In targeted tweets, our system identifies the target as an individual or group (Sub-task C). We used data pre-processing techniques like splitting hashtags into words, removing special characters, stop-word removal, stemming, lemmatization, capitalization, and offensive word dictionary. Later, we used keras tokenizer and word embeddings for feature extraction. For classification, we used the LSTM (Long short-term memory) model of keras framework. Our accuracy scores for Sub-task A, B and C are <i>0.8128</i>, <i>0.8167</i> and <i>0.3662</i> respectively. Our results indicate that fine-grained classification to identify offense target was difficult for the system. Lastly, in the future scope section, we will discuss the ways to improve system performance.</abstract>
      <url hash="8cdfcb75">S19-2139</url>
      <doi>10.18653/v1/S19-2139</doi>
    </paper>
    <paper id="140">
      <title><fixed-case>UTFPR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Relying on Compositionality to Find Offense</title>
      <author><first>Gustavo Henrique</first><last>Paetzold</last></author>
      <pages>801–805</pages>
      <abstract>We present the UTFPR system for the OffensEval shared task of SemEval 2019: A character-to-word-to-sentence compositional RNN model trained exclusively over the training data provided by the organizers. We find that, although not very competitive for the task at hand, it offers a robust solution to the orthographic irregularity inherent to tweets.</abstract>
      <url hash="e83ad5b4">S19-2140</url>
      <doi>10.18653/v1/S19-2140</doi>
    </paper>
    <paper id="141">
      <title><fixed-case>UVA</fixed-case> Wahoos at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Hate Speech Identification using Ensemble Machine Learning</title>
      <author><first>Murugesan</first><last>Ramakrishnan</last></author>
      <author><first>Wlodek</first><last>Zadrozny</last></author>
      <author><first>Narges</first><last>Tabari</last></author>
      <pages>806–811</pages>
      <abstract>With the growth in the usage of social media, it has become increasingly common for people to hide behind a mask and abuse others. We have attempted to detect such tweets and comments that are malicious in intent, which either targets an individual or a group. Our best classifier for identifying offensive tweets for SubTask A (Classifying offensive vs. nonoffensive) has an accuracy of 83.14% and a f1- score of 0.7565 on the actual test data. For SubTask B, to identify if an offensive tweet is targeted (If targeted towards an individual or a group), the classifier performs with an accuracy of 89.17% and f1-score of 0.5885. The paper talks about how we generated linguistic and semantic features to build an ensemble machine learning model. By training with more extracts from different sources (Facebook, and more tweets), the paper shows how the accuracy changes with additional training data.</abstract>
      <url hash="7dd931d7">S19-2141</url>
      <doi>10.18653/v1/S19-2141</doi>
    </paper>
    <paper id="142">
      <title><fixed-case>YNU</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: Identifying and Categorising Offensive Language on <fixed-case>T</fixed-case>witter</title>
      <author><first>Chengjin</first><last>Zhou</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>812–817</pages>
      <abstract>This document describes the submission of team YNU-HPCC to SemEval-2019 for three Sub-tasks of Task 6: Sub-task A, Sub-task B, and Sub-task C. We have submitted four systems to identify and categorise offensive language. The first subsystem is an attention-based 2-layer bidirectional long short-term memory (BiLSTM). The second subsystem is a voting ensemble of four different deep learning architectures. The third subsystem is a stacking ensemble of four different deep learning architectures. Finally, the fourth subsystem is a bidirectional encoder representations from transformers (BERT) model. Among our models, in Sub-task A, our first subsystem performed the best, ranking 16th among 103 teams; in Sub-task B, the second subsystem performed the best, ranking 12th among 75 teams; in Sub-task C, the fourth subsystem performed best, ranking 4th among 65 teams.</abstract>
      <url hash="7646e971">S19-2142</url>
      <doi>10.18653/v1/S19-2142</doi>
    </paper>
    <paper id="143">
      <title><fixed-case>YNUWB</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: K-max pooling <fixed-case>CNN</fixed-case> with average meta-embedding for identifying offensive language</title>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Xiaobing</first><last>Zhou</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>818–822</pages>
      <abstract>This paper describes the system submitted to SemEval 2019 Task 6: OffensEval 2019. The task aims to identify and categorize offensive language in social media, we only participate in Sub-task A, which aims to identify offensive language. In order to address this task, we propose a system based on a K-max pooling convolutional neural network model, and use an argument for averaging as a valid meta-embedding technique to get a metaembedding. Finally, we also use a cyclic learning rate policy to improve model performance. Our model achieves a Macro F1-score of 0.802 (ranked 9/103) in the Sub-task A.</abstract>
      <url hash="a4a5db4a">S19-2143</url>
      <doi>10.18653/v1/S19-2143</doi>
    </paper>
    <paper id="144">
      <title>Zeyad at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 6: That’s Offensive! An All-Out Search For An Ensemble To Identify And Categorize Offense in Tweets.</title>
      <author><first>Zeyad</first><last>El-Zanaty</last></author>
      <pages>823–828</pages>
      <abstract>The objective of this paper is to provide a description for a classification system built for SemEval-2019 Task 6: OffensEval. This system classifies a tweet as either offensive or not offensive (Sub-task A) and further classifies offensive tweets into categories (Sub-tasks B - C). The system consists of two phases; a brute-force grid search to find the best learners amongst a given set and an ensemble of a subset of these best learners. The system achieved an F1-score of 0.728, ranking in subtask A, an F1-score score of 0.616 in subtask B and an F1-score of 0.509 in subtask C.</abstract>
      <url hash="eae9a361">S19-2144</url>
      <doi>10.18653/v1/S19-2144</doi>
    </paper>
    <paper id="145">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisan News Detection</title>
      <author><first>Johannes</first><last>Kiesel</last></author>
      <author><first>Maria</first><last>Mestre</last></author>
      <author><first>Rishabh</first><last>Shukla</last></author>
      <author><first>Emmanuel</first><last>Vincent</last></author>
      <author><first>Payam</first><last>Adineh</last></author>
      <author><first>David</first><last>Corney</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <pages>829–839</pages>
      <abstract>Hyperpartisan news is news that takes an extreme left-wing or right-wing standpoint. If one is able to reliably compute this meta information, news articles may be automatically tagged, this way encouraging or discouraging readers to consume the text. It is an open question how successfully hyperpartisan news detection can be automated, and the goal of this SemEval task was to shed light on the state of the art. We developed new resources for this purpose, including a manually labeled dataset with 1,273 articles, and a second dataset with 754,000 articles, labeled via distant supervision. The interest of the research community in our task exceeded all our expectations: The datasets were downloaded about 1,000 times, 322 teams registered, of which 184 configured a virtual machine on our shared task cloud service TIRA, of which in turn 42 teams submitted a valid run. The best team achieved an accuracy of 0.822 on a balanced sample (yes : no hyperpartisan) drawn from the manually tagged corpus; an ensemble of the submitted systems increased the accuracy by 0.048.</abstract>
      <url hash="0505e9ce">S19-2145</url>
      <doi>10.18653/v1/S19-2145</doi>
    </paper>
    <paper id="146">
      <title>Team Bertha von Suttner at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisan News Detection using <fixed-case>ELM</fixed-case>o Sentence Representation Convolutional Network</title>
      <author><first>Ye</first><last>Jiang</last></author>
      <author><first>Johann</first><last>Petrak</last></author>
      <author><first>Xingyi</first><last>Song</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <author><first>Diana</first><last>Maynard</last></author>
      <pages>840–844</pages>
      <abstract>This paper describes the participation of team “bertha-von-suttner” in the SemEval2019 task 4 Hyperpartisan News Detection task. Our system uses sentence representations from averaged word embeddings generated from the pre-trained ELMo model with Convolutional Neural Networks and Batch Normalization for predicting hyperpartisan news. The final predictions were generated from the averaged predictions of an ensemble of models. With this architecture, our system ranked in first place, based on accuracy, the official scoring metric.</abstract>
      <url hash="c50fb833">S19-2146</url>
      <doi>10.18653/v1/S19-2146</doi>
    </paper>
    <paper id="147">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 7: <fixed-case>R</fixed-case>umour<fixed-case>E</fixed-case>val, Determining Rumour Veracity and Support for Rumours</title>
      <author><first>Genevieve</first><last>Gorrell</last></author>
      <author><first>Elena</first><last>Kochkina</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <author><first>Ahmet</first><last>Aker</last></author>
      <author><first>Arkaitz</first><last>Zubiaga</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <pages>845–854</pages>
      <abstract>Since the first RumourEval shared task in 2017, interest in automated claim validation has greatly increased, as the danger of “fake news” has become a mainstream concern. However automated support for rumour verification remains in its infancy. It is therefore important that a shared task in this area continues to provide a focus for effort, which is likely to increase. Rumour verification is characterised by the need to consider evolving conversations and news updates to reach a verdict on a rumour’s veracity. As in RumourEval 2017 we provided a dataset of dubious posts and ensuing conversations in social media, annotated both for stance and veracity. The social media rumours stem from a variety of breaking news stories and the dataset is expanded to include Reddit as well as new Twitter posts. There were two concrete tasks; rumour stance prediction and rumour verification, which we present in detail along with results achieved by participants. We received 22 system submissions (a 70% increase from RumourEval 2017) many of which used state-of-the-art methodology to tackle the challenges involved.</abstract>
      <url hash="40ecd9c8">S19-2147</url>
      <doi>10.18653/v1/S19-2147</doi>
    </paper>
    <paper id="148">
      <title>event<fixed-case>AI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 7: Rumor Detection on Social Media by Exploiting Content, User Credibility and Propagation Information</title>
      <author><first>Quanzhi</first><last>Li</last></author>
      <author><first>Qiong</first><last>Zhang</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>855–859</pages>
      <abstract>This paper describes our system for SemEval 2019 RumorEval: Determining rumor veracity and support for rumors (SemEval 2019 Task 7). This track has two tasks: Task A is to determine a user’s stance towards the source rumor, and Task B is to detect the veracity of the rumor: true, false or unverified. For stance classification, a neural network model with language features is utilized. For rumor verification, our approach exploits information from different dimensions: rumor content, source credibility, user credibility, user stance, event propagation path, etc. We use an ensemble approach in both tasks, which includes neural network models as well as the traditional classification algorithms. Our system is ranked 1st place in the rumor verification task by both the macro F1 measure and the RMSE measure.</abstract>
      <url hash="7509f0a1">S19-2148</url>
      <doi>10.18653/v1/S19-2148</doi>
    </paper>
    <paper id="149">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 8: Fact Checking in Community Question Answering Forums</title>
      <author><first>Tsvetomila</first><last>Mihaylova</last></author>
      <author><first>Georgi</first><last>Karadzhov</last></author>
      <author><first>Pepa</first><last>Atanasova</last></author>
      <author><first>Ramy</first><last>Baly</last></author>
      <author><first>Mitra</first><last>Mohtarami</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>860–869</pages>
      <abstract>We present SemEval-2019 Task 8 on Fact Checking in Community Question Answering Forums, which features two subtasks. Subtask A is about deciding whether a question asks for factual information vs. an opinion/advice vs. just socializing. Subtask B asks to predict whether an answer to a factual question is true, false or not a proper answer. We received 17 official submissions for subtask A and 11 official submissions for Subtask B. For subtask A, all systems improved over the majority class baseline. For Subtask B, all systems were below a majority class baseline, but several systems were very close to it. The leaderboard and the data from the competition can be found at http://competitions.codalab.org/competitions/20022.</abstract>
      <url hash="8dd6041d">S19-2149</url>
      <doi>10.18653/v1/S19-2149</doi>
    </paper>
    <paper id="150">
      <title><fixed-case>AUTOHOME</fixed-case>-<fixed-case>ORCA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 8: Application of <fixed-case>BERT</fixed-case> for Fact-Checking in Community Forums</title>
      <author><first>Zhengwei</first><last>Lv</last></author>
      <author><first>Duoxing</first><last>Liu</last></author>
      <author><first>Haifeng</first><last>Sun</last></author>
      <author><first>Xiao</first><last>Liang</last></author>
      <author><first>Tao</first><last>Lei</last></author>
      <author><first>Zhizhong</first><last>Shi</last></author>
      <author><first>Feng</first><last>Zhu</last></author>
      <author><first>Lei</first><last>Yang</last></author>
      <pages>870–876</pages>
      <abstract>Fact checking is an important task for maintaining high quality posts and improving user experience in Community Question Answering forums. Therefore, the SemEval-2019 task 8 is aimed to identify factual question (subtask A) and detect true factual information from corresponding answers (subtask B). In order to address this task, we propose a system based on the BERT model with meta information of questions. For the subtask A, the outputs of fine-tuned BERT classification model are combined with the feature of length of questions to boost the performance. For the subtask B, the predictions of several variants of BERT model encoding the meta information are combined to create an ensemble model. Our system achieved competitive results with an accuracy of 0.82 in the subtask A and 0.83 in the subtask B. The experimental results validate the effectiveness of our system.</abstract>
      <url hash="71bbcacf">S19-2150</url>
      <doi>10.18653/v1/S19-2150</doi>
    </paper>
    <paper id="151">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Suggestion Mining from Online Reviews and Forums</title>
      <author><first>Sapna</first><last>Negi</last></author>
      <author><first>Tobias</first><last>Daudert</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <pages>877–887</pages>
      <abstract>We present the pilot SemEval task on Suggestion Mining. The task consists of subtasks A and B, where we created labeled data from feedback forum and hotel reviews respectively. Subtask A provides training and test data from the same domain, while Subtask B evaluates the system on a test dataset from a different domain than the available training data. 33 teams participated in the shared task, with a total of 50 members. We summarize the problem definition, benchmark dataset preparation, and methods used by the participating teams, providing details of the methods used by the top ranked systems. The dataset is made freely available to help advance the research in suggestion mining, and reproduce the systems submitted under this task</abstract>
      <url hash="d010037d">S19-2151</url>
      <doi>10.18653/v1/S19-2151</doi>
    </paper>
    <paper id="152">
      <title>m_y at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Exploring <fixed-case>BERT</fixed-case> for Suggestion Mining</title>
      <author><first>Masahiro</first><last>Yamamoto</last></author>
      <author><first>Toshiyuki</first><last>Sekiya</last></author>
      <pages>888–892</pages>
      <abstract>This paper presents our system to the SemEval-2019 Task 9, Suggestion Mining from Online Reviews and Forums. The goal of this task is to extract suggestions such as the expressions of tips, advice, and recommendations. We explore Bidirectional Encoder Representations from Transformers (BERT) focusing on target domain pre-training in Subtask A which provides training and test datasets in the same domain. In Subtask B, the cross domain suggestion mining task, we apply the idea of distant supervision. Our system obtained the third place in Subtask A and the fifth place in Subtask B, which demonstrates its efficacy of our approaches.</abstract>
      <url hash="49077f6e">S19-2152</url>
      <doi>10.18653/v1/S19-2152</doi>
    </paper>
    <paper id="153">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 10: Math Question Answering</title>
      <author><first>Mark</first><last>Hopkins</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Cristian</first><last>Petrescu-Prahova</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Rik</first><last>Koncel-Kedziorski</last></author>
      <pages>893–899</pages>
      <abstract>We report on the SemEval 2019 task on math question answering. We provided a question set derived from Math SAT practice exams, including 2778 training questions and 1082 test questions. For a significant subset of these questions, we also provided SMT-LIB logical form annotations and an interpreter that could solve these logical forms. Systems were evaluated based on the percentage of correctly answered questions. The top system correctly answered 45% of the test questions, a considerable improvement over the 17% random guessing baseline.</abstract>
      <url hash="22765a46">S19-2153</url>
      <doi>10.18653/v1/S19-2153</doi>
    </paper>
    <paper id="154">
      <title><fixed-case>A</fixed-case>i<fixed-case>F</fixed-case>u at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 10: A Symbolic and Sub-symbolic Integrated System for <fixed-case>SAT</fixed-case> Math Question Answering</title>
      <author><first>Yifan</first><last>Liu</last></author>
      <author><first>Keyu</first><last>Ding</last></author>
      <author><first>Yi</first><last>Zhou</last></author>
      <pages>900–906</pages>
      <abstract>AiFu has won the first place in the SemEval-2019 Task 10 - ”Math Question Answering”competition. This paper is to describe how it works technically and to report and analyze some essential experimental results</abstract>
      <url hash="3d968da9">S19-2154</url>
      <doi>10.18653/v1/S19-2154</doi>
    </paper>
    <paper id="155">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 12: Toponym Resolution in Scientific Papers</title>
      <author><first>Davy</first><last>Weissenbacher</last></author>
      <author><first>Arjun</first><last>Magge</last></author>
      <author><first>Karen</first><last>O’Connor</last></author>
      <author><first>Matthew</first><last>Scotch</last></author>
      <author><first>Graciela</first><last>Gonzalez-Hernandez</last></author>
      <pages>907–916</pages>
      <abstract>We present the SemEval-2019 Task 12 which focuses on toponym resolution in scientific articles. Given an article from PubMed, the task consists of detecting mentions of names of places, or toponyms, and mapping the mentions to their corresponding entries in GeoNames.org, a database of geospatial locations. We proposed three subtasks. In Subtask 1, we asked participants to detect all toponyms in an article. In Subtask 2, given toponym mentions as input, we asked participants to disambiguate them by linking them to entries in GeoNames. In Subtask 3, we asked participants to perform both the detection and the disambiguation steps for all toponyms. A total of 29 teams registered, and 8 teams submitted a system run. We summarize the corpus and the tools created for the challenge. They are freely available at https://competitions.codalab.org/competitions/19948. We also analyze the methods, the results and the errors made by the competing systems with a focus on toponym disambiguation.</abstract>
      <url hash="2aac10ac">S19-2155</url>
      <doi>10.18653/v1/S19-2155</doi>
    </paper>
    <paper id="156">
      <title><fixed-case>DM</fixed-case>_<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2018 Task 12: A Pipeline System for Toponym Resolution</title>
      <author><first>Xiaobin</first><last>Wang</last></author>
      <author><first>Chunping</first><last>Ma</last></author>
      <author><first>Huafei</first><last>Zheng</last></author>
      <author><first>Chu</first><last>Liu</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Linlin</first><last>Li</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>917–923</pages>
      <abstract>This paper describes DM-NLP’s system for toponym resolution task at Semeval 2019. Our system was developed for toponym detection, disambiguation and end-to-end resolution which is a pipeline of the former two. For toponym detection, we utilized the state-of-the-art sequence labeling model, namely, BiLSTM-CRF model as backbone. A lot of strategies are adopted for further improvement, such as pre-training, model ensemble, model averaging and data augment. For toponym disambiguation, we adopted the widely used searching and ranking framework. For ranking, we proposed several effective features for measuring the consistency between the detected toponym and toponyms in GeoNames. Eventually, our system achieved the best performance among all the submitted results in each sub task.</abstract>
      <url hash="45f8cb7a">S19-2156</url>
      <doi>10.18653/v1/S19-2156</doi>
    </paper>
    <paper id="157">
      <title>Brenda Starr at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisan News Detection</title>
      <author><first>Olga</first><last>Papadopoulou</last></author>
      <author><first>Giorgos</first><last>Kordopatis-Zilos</last></author>
      <author><first>Markos</first><last>Zampoglou</last></author>
      <author><first>Symeon</first><last>Papadopoulos</last></author>
      <author><first>Yiannis</first><last>Kompatsiaris</last></author>
      <pages>924–928</pages>
      <abstract>In the effort to tackle the challenge of Hyperpartisan News Detection, i.e., the task of deciding whether a news article is biased towards one party, faction, cause, or person, we experimented with two systems: i) a standard supervised learning approach using superficial text and bag-of-words features from the article title and body, and ii) a deep learning system comprising a four-layer convolutional neural network and max-pooling layers after the embedding layer, feeding the consolidated features to a bi-directional recurrent neural network. We achieved an F-score of 0.712 with our best approach, which corresponds to the mid-range of performance levels in the leaderboard.</abstract>
      <url hash="bdfd340b">S19-2157</url>
      <doi>10.18653/v1/S19-2157</doi>
    </paper>
    <paper id="158">
      <title><fixed-case>C</fixed-case>ardiff <fixed-case>U</fixed-case>niversity at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Linguistic Features for Hyperpartisan News Detection</title>
      <author><first>Carla</first><last>Pérez-Almendros</last></author>
      <author><first>Luis</first><last>Espinosa-Anke</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>929–933</pages>
      <abstract>This paper summarizes our contribution to the Hyperpartisan News Detection task in SemEval 2019. We experiment with two different approaches: 1) an SVM classifier based on word vector averages and hand-crafted linguistic features, and 2) a BiLSTM-based neural text classifier trained on a filtered training set. Surprisingly, despite their different nature, both approaches achieve an accuracy of 0.74. The main focus of this paper is to further analyze the remarkable fact that a simple feature-based approach can perform on par with modern neural classifiers. We also highlight the effectiveness of our filtering strategy for training the neural network on a large but noisy training set.</abstract>
      <url hash="efcfb604">S19-2158</url>
      <doi>10.18653/v1/S19-2158</doi>
    </paper>
    <paper id="159">
      <title><fixed-case>C</fixed-case>lark <fixed-case>K</fixed-case>ent at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Stylometric Insights into Hyperpartisan News Detection</title>
      <author><first>Viresh</first><last>Gupta</last></author>
      <author><first>Baani Leen</first><last>Kaur Jolly</last></author>
      <author><first>Ramneek</first><last>Kaur</last></author>
      <author><first>Tanmoy</first><last>Chakraborty</last></author>
      <pages>934–938</pages>
      <abstract>In this paper, we present a news bias prediction system, which we developed as part of a SemEval 2019 task. We developed an XGBoost based system which uses character and word level n-gram features represented using TF-IDF, count vector based correlation matrix, and predicts if an input news article is a hyperpartisan news article. Our model was able to achieve a precision of 68.3% on the test set provided by the contest organizers. We also run our model on the BuzzFeed corpus and find XGBoost with simple character level N-Gram embeddings to be performing well with an accuracy of around 96%.</abstract>
      <url hash="0a86af79">S19-2159</url>
      <doi>10.18653/v1/S19-2159</doi>
    </paper>
    <paper id="160">
      <title>Dick-Preston and Morbo at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Transfer Learning for Hyperpartisan News Detection</title>
      <author><first>Tim</first><last>Isbister</last></author>
      <author><first>Fredrik</first><last>Johansson</last></author>
      <pages>939–943</pages>
      <abstract>In a world of information operations, influence campaigns, and fake news, classification of news articles as following hyperpartisan argumentation or not is becoming increasingly important. We present a deep learning-based approach in which a pre-trained language model has been fine-tuned on domain-specific data and used for classification of news articles, as part of the SemEval-2019 task on hyperpartisan news detection. The suggested approach yields accuracy and F1-scores around 0.8 which places the best performing classifier among the top-5 systems in the competition.</abstract>
      <url hash="2485d756">S19-2160</url>
      <doi>10.18653/v1/S19-2160</doi>
    </paper>
    <paper id="161">
      <title>Doris <fixed-case>M</fixed-case>artin at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisan News Detection with Generic Semi-supervised Features</title>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <pages>944–948</pages>
      <abstract>In this paper we describe our participation to the Hyperpartisan News Detection shared task at SemEval 2019. Motivated by the late arrival of Doris Martin, we test a previously developed document classification system which consists of a combination of clustering features implemented on top of some simple shallow local features. We show how leveraging distributional features obtained from large in-domain unlabeled data helps to easily and quickly develop a reasonably good performing system for detecting hyperpartisan news. The system and models generated for this task are publicly available.</abstract>
      <url hash="4a55107b">S19-2161</url>
      <doi>10.18653/v1/S19-2161</doi>
    </paper>
    <paper id="162">
      <title><fixed-case>D</fixed-case>uluth at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: The Pioquinto Manterola Hyperpartisan News Detector</title>
      <author><first>Saptarshi</first><last>Sengupta</last></author>
      <author><first>Ted</first><last>Pedersen</last></author>
      <pages>949–953</pages>
      <abstract>This paper describes the Pioquinto Manterola Hyperpartisan News Detector, which participated in SemEval-2019 Task 4. Hyperpartisan news is highly polarized and takes a very biased or one–sided view of a particular story. We developed two variants of our system, the more successful was a Logistic Regression classifier based on unigram features. This was our official entry in the task, and it placed 23rd of 42 participating teams. Our second variant was a Convolutional Neural Network that did not perform as well.</abstract>
      <url hash="0b6e0c1a">S19-2162</url>
      <doi>10.18653/v1/S19-2162</doi>
    </paper>
    <paper id="163">
      <title>Fermi at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: The sarah-jane-smith Hyperpartisan News Detector</title>
      <author><first>Nikhil</first><last>Chakravartula</last></author>
      <author><first>Vijayasaradhi</first><last>Indurthi</last></author>
      <author><first>Bakhtiyar</first><last>Syed</last></author>
      <pages>954–956</pages>
      <abstract>This paper describes our system (Fermi) for Task 4: Hyper-partisan News detection of SemEval-2019. We use simple text classification algorithms by transforming the input features to a reduced feature set. We aim to find the right number of features useful for efficient classification and explore multiple training models to evaluate the performance of these text classification algorithms. Our team - Fermi’s model achieved an accuracy of 59.10% and an F1 score of 69.5% on the official test data set. In this paper, we provide a detailed description of the approach as well as the results obtained in the task.</abstract>
      <url hash="d1dcf6f4">S19-2163</url>
      <doi>10.18653/v1/S19-2163</doi>
    </paper>
    <paper id="164">
      <title>Harvey Mudd College at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: The Carl Kolchak Hyperpartisan News Detector</title>
      <author><first>Celena</first><last>Chen</last></author>
      <author><first>Celine</first><last>Park</last></author>
      <author><first>Jason</first><last>Dwyer</last></author>
      <author><first>Julie</first><last>Medero</last></author>
      <pages>957–961</pages>
      <abstract>We use various natural processing and machine learning methods to perform the Hyperpartisan News Detection task. In particular, some of the features we look at are bag-of-words features, the title’s length, number of capitalized words in the title, and the sentiment of the sentences and the title. By adding these features, we see improvements in our evaluation metrics compared to the baseline values. We find that sentiment analysis helps improve our evaluation metrics. We do not see a benefit from feature selection. Overall, our system achieves an accuracy of 0.739, finishing 18th out of 42 submissions to the task. From our work, it is evident that both title features and sentiment of articles are meaningful to the hyperpartisanship of news articles.</abstract>
      <url hash="9d7b0c5a">S19-2164</url>
      <doi>10.18653/v1/S19-2164</doi>
    </paper>
    <paper id="165">
      <title>Harvey Mudd College at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: The Clint Buchanan Hyperpartisan News Detector</title>
      <author><first>Mehdi</first><last>Drissi</last></author>
      <author><first>Pedro</first><last>Sandoval Segura</last></author>
      <author><first>Vivaswat</first><last>Ojha</last></author>
      <author><first>Julie</first><last>Medero</last></author>
      <pages>962–966</pages>
      <abstract>We investigate the recently developed Bidi- rectional Encoder Representations from Transformers (BERT) model (Devlin et al. 2018) for the hyperpartisan news detection task. Using a subset of hand-labeled articles from SemEval as a validation set, we test the performance of different parameters for BERT models. We find that accuracy from two different BERT models using different proportions of the articles is consistently high, with our best-performing model on the validation set achieving 85% accuracy and the best-performing model on the test set achieving 77%. We further determined that our model exhibits strong consistency, labeling independent slices of the same article identically. Finally, we find that randomizing the order of word pieces dramatically reduces validation accuracy (to approximately 60%), but that shuffling groups of four or more word pieces maintains an accuracy of about 80%, indicating the model mainly gains value from local context.</abstract>
      <url hash="b3a582c6">S19-2165</url>
      <doi>10.18653/v1/S19-2165</doi>
    </paper>
    <paper id="166">
      <title>Harvey Mudd College at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: The <fixed-case>D</fixed-case>.<fixed-case>X</fixed-case>. Beaumont Hyperpartisan News Detector</title>
      <author><first>Evan</first><last>Amason</last></author>
      <author><first>Jake</first><last>Palanker</last></author>
      <author><first>Mary Clare</first><last>Shen</last></author>
      <author><first>Julie</first><last>Medero</last></author>
      <pages>967–970</pages>
      <abstract>We use the 600 hand-labelled articles from SemEval Task 4 to hand-tune a classifier with 3000 features for the Hyperpartisan News Detection task. Our final system uses features based on bag-of-words (BoW), analysis of the article title, language complexity, and simple sentiment analysis in a naive Bayes classifier. We trained our final system on the 600,000 articles labelled by publisher. Our final system has an accuracy of 0.653 on the hand-labeled test set. The most effective features are the Automated Readability Index and the presence of certain words in the title. This suggests that hyperpartisan writing uses a distinct writing style, especially in the title.</abstract>
      <url hash="e0760bc0">S19-2166</url>
      <doi>10.18653/v1/S19-2166</doi>
    </paper>
    <paper id="167">
      <title><fixed-case>NLP</fixed-case>@<fixed-case>UIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: The Paparazzo Hyperpartisan News Detector</title>
      <author><first>Duc-Vu</first><last>Nguyen</last></author>
      <author><first>Thin</first><last>Dang</last></author>
      <author><first>Ngan</first><last>Nguyen</last></author>
      <pages>971–975</pages>
      <abstract>This paper describes the system of NLP@UIT that participated in Task 4 of SemEval-2019. We developed a system that predicts whether an English news article follows a hyperpartisan argumentation. Paparazzo is the name of our system and is also the code name of our team in Task 4 of SemEval-2019. The Paparazzo system, in which we use tri-grams of words and hepta-grams of characters, officially ranks thirteen with an accuracy of 0.747. Another system of ours, which utilizes trigrams of words, tri-grams of characters, trigrams of part-of-speech, syntactic dependency sub-trees, and named-entity recognition tags, achieved an accuracy of 0.787 and is proposed after the deadline of Task 4.</abstract>
      <url hash="24213edb">S19-2167</url>
      <doi>10.18653/v1/S19-2167</doi>
    </paper>
    <paper id="168">
      <title>Orwellian-times at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: A Stylistic and Content-based Classifier</title>
      <author><first>Jürgen</first><last>Knauth</last></author>
      <pages>976–980</pages>
      <abstract>While fake news detection received quite a bit of attention in recent years, hyperpartisan news detection is still an underresearched topic. This paper presents our work towards building a classification system for hyperpartisan news detection in the context of the SemEval2019 shared task 4. We experiment with two different approaches - a more stylistic one, and a more content related one - achieving average results.</abstract>
      <url hash="ac6035e2">S19-2168</url>
      <doi>10.18653/v1/S19-2168</doi>
    </paper>
    <paper id="169">
      <title>Rouletabille at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Neural Network Baseline for Identification of Hyperpartisan Publishers</title>
      <author><first>Jose G.</first><last>Moreno</last></author>
      <author><first>Yoann</first><last>Pitarch</last></author>
      <author><first>Karen</first><last>Pinel-Sauvagnat</last></author>
      <author><first>Gilles</first><last>Hubert</last></author>
      <pages>981–984</pages>
      <abstract>This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in (Potthast et al., 2018) show that neural-based classification methods reach state-of-the art results. Our final submission is composed of a unique run that ranks among all runs at 3/49 position for the by-publisher test dataset and 43/96 for the by-article test dataset in terms of Accuracy.</abstract>
      <url hash="7ae8e8b9">S19-2169</url>
      <doi>10.18653/v1/S19-2169</doi>
    </paper>
    <paper id="170">
      <title>Spider-<fixed-case>J</fixed-case>erusalem at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisan News Detection</title>
      <author><first>Amal</first><last>Alabdulkarim</last></author>
      <author><first>Tariq</first><last>Alhindi</last></author>
      <pages>985–989</pages>
      <abstract>This paper describes our system for detecting hyperpartisan news articles, which was submitted for the shared task in SemEval 2019 on Hyperpartisan News Detection. We developed a Support Vector Machine (SVM) model that uses TF-IDF of tokens, Language Inquiry and Word Count (LIWC) features, and structural features such as number of paragraphs and hyperlink count in an article. The model was trained on 645 articles from two classes: mainstream and hyperpartisan. Our system was ranked seventeenth out of forty two participating teams in the binary classification task with an accuracy score of 0.742 on the blind test set (the accuracy of the top ranked system was 0.822). We provide a detailed description of our preprocessing steps, discussion of our experiments using different combinations of features, and analysis of our results and prediction errors.</abstract>
      <url hash="5fbe5431">S19-2170</url>
      <doi>10.18653/v1/S19-2170</doi>
    </paper>
    <paper id="171">
      <title>Steve <fixed-case>M</fixed-case>artin at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Ensemble Learning Model for Detecting Hyperpartisan News</title>
      <author><first>Youngjun</first><last>Joo</last></author>
      <author><first>Inchon</first><last>Hwang</last></author>
      <pages>990–994</pages>
      <abstract>This paper describes our submission to task 4 in SemEval 2019, i.e., hyperpartisan news detection. Our model aims at detecting hyperpartisan news by incorporating the style-based features and the content-based features. We extract a broad number of feature sets and use as our learning algorithms the GBDT and the n-gram CNN model. Finally, we apply the weighted average for effective learning between the two models. Our model achieves an accuracy of 0.745 on the test set in subtask A.</abstract>
      <url hash="4b5936a3">S19-2171</url>
      <attachment type="supplementary" hash="fcfbfd16">S19-2171.Supplementary.pdf</attachment>
      <doi>10.18653/v1/S19-2171</doi>
    </paper>
    <paper id="172">
      <title><fixed-case>T</fixed-case>ake<fixed-case>L</fixed-case>ab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisan News Detection</title>
      <author><first>Niko</first><last>Palić</last></author>
      <author><first>Juraj</first><last>Vladika</last></author>
      <author><first>Dominik</first><last>Čubelić</last></author>
      <author><first>Ivan</first><last>Lovrenčić</last></author>
      <author><first>Maja</first><last>Buljan</last></author>
      <author><first>Jan</first><last>Šnajder</last></author>
      <pages>995–998</pages>
      <abstract>In this paper, we demonstrate the system built to solve the SemEval-2019 task 4: Hyperpartisan News Detection (Kiesel et al., 2019), the task of automatically determining whether an article is heavily biased towards one side of the political spectrum. Our system receives an article in its raw, textual form, analyzes it, and predicts with moderate accuracy whether the article is hyperpartisan. The learning model used was primarily trained on a manually prelabeled dataset containing news articles. The system relies on the previously constructed SVM model, available in the Python Scikit-Learn library. We ranked 6th in the competition of 42 teams with an accuracy of 79.1% (the winning team had 82.2%).</abstract>
      <url hash="0702359e">S19-2172</url>
      <doi>10.18653/v1/S19-2172</doi>
    </paper>
    <paper id="173">
      <title>Team Fernando-Pessa at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Back to Basics in Hyperpartisan News Detection</title>
      <author><first>André</first><last>Cruz</last></author>
      <author><first>Gil</first><last>Rocha</last></author>
      <author><first>Rui</first><last>Sousa-Silva</last></author>
      <author><first>Henrique</first><last>Lopes Cardoso</last></author>
      <pages>999–1003</pages>
      <abstract>This paper describes our submission to the SemEval 2019 Hyperpartisan News Detection task. Our system aims for a linguistics-based document classification from a minimal set of interpretable features, while maintaining good performance. To this goal, we follow a feature-based approach and perform several experiments with different machine learning classifiers. Additionally, we explore feature importances and distributions among the two classes. On the main task, our model achieved an accuracy of 71.7%, which was improved after the task’s end to 72.9%. We also participate on the meta-learning sub-task, for classifying documents with the binary classifications of all submitted systems as input, achieving an accuracy of 89.9%.</abstract>
      <url hash="d541abb7">S19-2173</url>
      <doi>10.18653/v1/S19-2173</doi>
    </paper>
    <paper id="174">
      <title>Team Harry Friberg at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Identifying Hyperpartisan News through Editorially Defined Metatopics</title>
      <author><first>Nazanin</first><last>Afsarmanesh</last></author>
      <author><first>Jussi</first><last>Karlgren</last></author>
      <author><first>Peter</first><last>Sumbler</last></author>
      <author><first>Nina</first><last>Viereckel</last></author>
      <pages>1004–1006</pages>
      <abstract>This report describes the starting point for a simple rule based hypothesis testing excercise on identifying hyperpartisan news items carried out by the Harry Friberg team from Gavagai. We used manually crafted <i>metatopics</i>, topics which often appear in hyperpartisan texts as rant conduits, together with tonality analysis to identify general characteristics of hyperpartisan news items. While the precision of the resulting effort is less than stellar— our contribution ranked 37th of the 42 successfully submitted experiments with overly high recall (95%) and low precision (54%)—we believe we have a model which allows us to continue exploring the underlying features of what the subgenre of hyperpartisan news items is characterised by.</abstract>
      <url hash="3db4824b">S19-2174</url>
      <doi>10.18653/v1/S19-2174</doi>
    </paper>
    <paper id="175">
      <title>Team <fixed-case>H</fixed-case>oward <fixed-case>B</fixed-case>eale at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisan News Detection with <fixed-case>BERT</fixed-case></title>
      <author><first>Osman</first><last>Mutlu</last></author>
      <author><first>Ozan Arkan</first><last>Can</last></author>
      <author><first>Erenay</first><last>Dayanik</last></author>
      <pages>1007–1011</pages>
      <abstract>This paper describes our system for SemEval-2019 Task 4: Hyperpartisan News Detection (Kiesel et al., 2019). We use pretrained BERT (Devlin et al., 2018) architecture and investigate the effect of different fine tuning regimes on the final classification task. We show that additional pretraining on news domain improves the performance on the Hyperpartisan News Detection task. Our system ranked 8th out of 42 teams with 78.3% accuracy on the held-out test dataset.</abstract>
      <url hash="91168127">S19-2175</url>
      <doi>10.18653/v1/S19-2175</doi>
    </paper>
    <paper id="176">
      <title>Team Jack Ryder at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Using <fixed-case>BERT</fixed-case> Representations for Detecting Hyperpartisan News</title>
      <author><first>Daniel</first><last>Shaprin</last></author>
      <author><first>Giovanni</first><last>Da San Martino</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1012–1015</pages>
      <abstract>We describe the system submitted by the Jack Ryder team to SemEval-2019 Task 4 on Hyperpartisan News Detection. The task asked participants to predict whether a given article is hyperpartisan, i.e., extreme-left or extreme-right. We proposed an approach based on BERT with fine-tuning, which was ranked 7th out 28 teams on the distantly supervised dataset, where all articles from a hyperpartisan/non-hyperpartisan news outlet are considered to be hyperpartisan/non-hyperpartisan. On a manually annotated test dataset, where human annotators double-checked the labels, we were ranked 29th out of 42 teams.</abstract>
      <url hash="4146e431">S19-2176</url>
      <doi>10.18653/v1/S19-2176</doi>
    </paper>
    <paper id="177">
      <title>Team Kermit-the-frog at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Bias Detection Through Sentiment Analysis and Simple Linguistic Features</title>
      <author><first>Talita</first><last>Anthonio</last></author>
      <author><first>Lennart</first><last>Kloppenburg</last></author>
      <pages>1016–1020</pages>
      <abstract>In this paper we describe our participation in the SemEval 2019 shared task on hyperpartisan news detection. We present the system that we submitted for final evaluation and the three approaches that we used: sentiment, bias-laden words and filtered n-gram features. Our submitted model is a Linear SVM that solely relies on the negative sentiment of a document. We achieved an accuracy of 0.621 and a f1 score of 0.694 in the competition, revealing the predictive power of negative sentiment for this task. There was no major improvement by adding or substituting the features of the other two approaches that we tried.</abstract>
      <url hash="1082aa05">S19-2177</url>
      <doi>10.18653/v1/S19-2177</doi>
    </paper>
    <paper id="178">
      <title>Team Kit Kittredge at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: <fixed-case>LSTM</fixed-case> Voting System</title>
      <author><first>Rebekah</first><last>Cramerus</last></author>
      <author><first>Tatjana</first><last>Scheffler</last></author>
      <pages>1021–1025</pages>
      <abstract>This paper describes the approach of team Kit Kittredge to SemEval-2019 Task 4: Hyperpartisan News Detection. The goal was binary classification of news articles into the categories of “biased” or “unbiased”. We had two software submissions: one a simple bag-of-words model, and the second an LSTM (Long Short Term Memory) neural network, which was trained on a subset of the original dataset selected by a voting system of other LSTMs. This method did not prove much more successful than the baseline, however, due to the models’ tendency to learn publisher-specific traits instead of general bias.</abstract>
      <url hash="1aa432e3">S19-2178</url>
      <doi>10.18653/v1/S19-2178</doi>
    </paper>
    <paper id="179">
      <title>Team <fixed-case>N</fixed-case>ed <fixed-case>L</fixed-case>eeds at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Exploring Language Indicators of Hyperpartisan Reporting</title>
      <author><first>Bozhidar</first><last>Stevanoski</last></author>
      <author><first>Sonja</first><last>Gievska</last></author>
      <pages>1026–1031</pages>
      <abstract>This paper reports an experiment carried out to investigate the relevance of several syntactic, stylistic and pragmatic features on the task of distinguishing between mainstream and partisan news articles. The results of the evaluation of different feature sets and the extent to which various feature categories could affect the performance metrics are discussed and compared. Among different combinations of features and classifiers, Random Forest classifier using vector representations of the headline and the text of the report, with the inclusion of 8 readability scores and few stylistic features yielded best result, ranking our team at the 9th place at the SemEval 2019 Hyperpartisan News Detection challenge.</abstract>
      <url hash="63a19ce3">S19-2179</url>
      <doi>10.18653/v1/S19-2179</doi>
    </paper>
    <paper id="180">
      <title>Team Peter Brinkmann at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Detecting Biased News Articles Using Convolutional Neural Networks</title>
      <author><first>Michael</first><last>Färber</last></author>
      <author><first>Agon</first><last>Qurdina</last></author>
      <author><first>Lule</first><last>Ahmedi</last></author>
      <pages>1032–1036</pages>
      <abstract>In this paper, we present an approach for classifying news articles as biased (i.e., hyperpartisan) or unbiased, based on a convolutional neural network. We experiment with various embedding methods (pretrained and trained on the training dataset) and variations of the convolutional neural network architecture and compare the results. When evaluating our best performing approach on the actual test data set of the SemEval 2019 Task 4, we obtained relatively low precision and accuracy values, while gaining the highest recall rate among all 42 participating teams.</abstract>
      <url hash="3dfbdeff">S19-2180</url>
      <doi>10.18653/v1/S19-2180</doi>
    </paper>
    <paper id="181">
      <title>Team Peter-Parker at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: <fixed-case>BERT</fixed-case>-Based Method in Hyperpartisan News Detection</title>
      <author><first>Zhiyuan</first><last>Ning</last></author>
      <author><first>Yuanzhen</first><last>Lin</last></author>
      <author><first>Ruichao</first><last>Zhong</last></author>
      <pages>1037–1040</pages>
      <abstract>This paper describes the team peter-parker’s participation in Hyperpartisan News Detection task (SemEval-2019 Task 4), which requires to classify whether a given news article is bias or not. We decided to use JAVA to do the article parsing tool and the BERT-Based model to do the bias prediction. Furthermore, we will show experiment results with analysis.</abstract>
      <url hash="525caeb8">S19-2181</url>
      <doi>10.18653/v1/S19-2181</doi>
    </paper>
    <paper id="182">
      <title>Team <fixed-case>QCRI</fixed-case>-<fixed-case>MIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Propaganda Analysis Meets Hyperpartisan News Detection</title>
      <author><first>Abdelrhman</first><last>Saleh</last></author>
      <author><first>Ramy</first><last>Baly</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <author><first>Giovanni</first><last>Da San Martino</last></author>
      <author><first>Mitra</first><last>Mohtarami</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>1041–1046</pages>
      <abstract>We describe our submission to SemEval-2019 Task 4 on Hyperpartisan News Detection. We rely on a variety of engineered features originally used to detect propaganda. This is based on the assumption that biased messages are propagandistic and promote a particular political cause or viewpoint. In particular, we trained a logistic regression model with features ranging from simple bag of words to vocabulary richness and text readability. Our system achieved 72.9% accuracy on the manually annotated testset, and 60.8% on the test data that was obtained with distant supervision. Additional experiments showed that significant performance gains can be achieved with better feature pre-processing.</abstract>
      <url hash="c4ce3a88">S19-2182</url>
      <doi>10.18653/v1/S19-2182</doi>
    </paper>
    <paper id="183">
      <title>Team Xenophilius Lovegood at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisanship Classification using Convolutional Neural Networks</title>
      <author><first>Albin</first><last>Zehe</last></author>
      <author><first>Lena</first><last>Hettinger</last></author>
      <author><first>Stefan</first><last>Ernst</last></author>
      <author><first>Christian</first><last>Hauptmann</last></author>
      <author><first>Andreas</first><last>Hotho</last></author>
      <pages>1047–1051</pages>
      <abstract>This paper describes our system for the SemEval 2019 Task 4 on hyperpartisan news detection. We build on an existing deep learning approach for sentence classification based on a Convolutional Neural Network. Modifying the original model with additional layers to increase its expressiveness and finally building an ensemble of multiple versions of the model, we obtain an accuracy of 67.52% and an F1 score of 73.78% on the main test dataset. We also report on additional experiments incorporating handcrafted features into the CNN and using it as a feature extractor for a linear SVM.</abstract>
      <url hash="8a20b7c0">S19-2183</url>
      <doi>10.18653/v1/S19-2183</doi>
    </paper>
    <paper id="184">
      <title>Team yeon-zi at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisan News Detection by De-noising Weakly-labeled Data</title>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>1052–1056</pages>
      <abstract>This paper describes our system that has been submitted to SemEval-2019 Task 4: Hyperpartisan News Detection. We focus on removing the noise inherent in the hyperpartisanship dataset from both data-level and model-level by leveraging semi-supervised pseudo-labels and the state-of-the-art BERT model. Our model achieves 75.8% accuracy in the final by-article dataset without ensemble learning.</abstract>
      <url hash="b19ed0b2">S19-2184</url>
      <doi>10.18653/v1/S19-2184</doi>
    </paper>
    <paper id="185">
      <title>The Sally Smedley Hyperpartisan News Detector at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4</title>
      <author><first>Kazuaki</first><last>Hanawa</last></author>
      <author><first>Shota</first><last>Sasaki</last></author>
      <author><first>Hiroki</first><last>Ouchi</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>1057–1061</pages>
      <abstract>This paper describes our system submitted to the formal run of SemEval-2019 Task 4: Hyperpartisan news detection. Our system is based on a linear classifier using several features, i.e., 1) embedding features based on the pre-trained BERT embeddings, 2) article length features, and 3) embedding features of informative phrases extracted from by-publisher dataset. Our system achieved 80.9% accuracy on the test set for the formal run and got the 3rd place out of 42 teams.</abstract>
      <url hash="5070a922">S19-2185</url>
      <doi>10.18653/v1/S19-2185</doi>
    </paper>
    <paper id="186">
      <title>Tintin at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Detecting Hyperpartisan News Article with only Simple Tokens</title>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>1062–1066</pages>
      <abstract>Tintin, the system proposed by the CECL for the Hyperpartisan News Detection task of SemEval 2019, is exclusively based on the tokens that make up the documents and a standard supervised learning procedure. It obtained very contrasting results: poor on the main task, but much more effective at distinguishing documents published by hyperpartisan media outlets from unbiased ones, as it ranked first. An analysis of the most important features highlighted the positive aspects, but also some potential limitations of the approach.</abstract>
      <url hash="c68f0cc5">S19-2186</url>
      <doi>10.18653/v1/S19-2186</doi>
    </paper>
    <paper id="187">
      <title>Tom Jumbo-Grumbo at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisan News Detection with <fixed-case>G</fixed-case>lo<fixed-case>V</fixed-case>e vectors and <fixed-case>SVM</fixed-case></title>
      <author><first>Chia-Lun</first><last>Yeh</last></author>
      <author><first>Babak</first><last>Loni</last></author>
      <author><first>Anne</first><last>Schuth</last></author>
      <pages>1067–1071</pages>
      <abstract>In this paper, we describe our attempt to learn bias from news articles. From our experiments, it seems that although there is a correlation between publisher bias and article bias, it is challenging to learn bias directly from the publisher labels. On the other hand, using few manually-labeled samples can increase the accuracy metric from around 60% to near 80%. Our system is computationally inexpensive and uses several standard document representations in NLP to train an SVM or LR classifier. The system ranked 4th in the SemEval-2019 task. The code is released for reproducibility.</abstract>
      <url hash="bad09e0e">S19-2187</url>
      <doi>10.18653/v1/S19-2187</doi>
    </paper>
    <paper id="188">
      <title><fixed-case>UBC</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisan News Detection With Attention-Based <fixed-case>B</fixed-case>i-<fixed-case>LSTM</fixed-case>s</title>
      <author><first>Chiyu</first><last>Zhang</last></author>
      <author><first>Arun</first><last>Rajendran</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>1072–1077</pages>
      <abstract>We present our deep learning models submitted to the SemEval-2019 Task 4 competition focused at Hyperpartisan News Detection. We acquire best results with a Bi-LSTM network equipped with a self-attention mechanism. Among 33 participating teams, our submitted system ranks top 7 (65.3% accuracy) on the ‘labels-by-publisher’ sub-task and top 24 out of 44 teams (68.3% accuracy) on the ‘labels-by-article’ sub-task (65.3% accuracy). We also report a model that scores higher than the 8th ranking system (78.5% accuracy) on the ‘labels-by-article’ sub-task.</abstract>
      <url hash="b16688d9">S19-2188</url>
      <doi>10.18653/v1/S19-2188</doi>
    </paper>
    <paper id="189">
      <title>Vernon-fenwick at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 4: Hyperpartisan News Detection using Lexical and Semantic Features</title>
      <author><first>Vertika</first><last>Srivastava</last></author>
      <author><first>Ankita</first><last>Gupta</last></author>
      <author><first>Divya</first><last>Prakash</last></author>
      <author><first>Sudeep Kumar</first><last>Sahoo</last></author>
      <author><first>Rohit</first><last>R.R</last></author>
      <author><first>Yeon Hyang</first><last>Kim</last></author>
      <pages>1078–1082</pages>
      <abstract>In this paper, we present our submission for SemEval-2019 Task 4: Hyperpartisan News Detection. Hyperpartisan news articles are sharply polarized and extremely biased (onesided). It shows blind beliefs, opinions and unreasonable adherence to a party, idea, faction or a person. Through this task, we aim to develop an automated system that can be used to detect hyperpartisan news and serve as a prescreening technique for fake news detection. The proposed system jointly uses a rich set of handcrafted textual and semantic features. Our system achieved 2nd rank on the primary metric (82.0% accuracy) and 1st rank on the secondary metric (82.1% F1-score), among all participating teams. Comparison with the best performing system on the leaderboard shows that our system is behind by only 0.2% absolute difference in accuracy.</abstract>
      <url hash="70a20b90">S19-2189</url>
      <doi>10.18653/v1/S19-2189</doi>
    </paper>
    <paper id="190">
      <title><fixed-case>A</fixed-case>ndrej<fixed-case>J</fixed-case>an at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 7: A Fusion Approach for Exploring the Key Factors pertaining to Rumour Analysis</title>
      <author><first>Andrej</first><last>Janchevski</last></author>
      <author><first>Sonja</first><last>Gievska</last></author>
      <pages>1083–1089</pages>
      <abstract>The viral spread of false, unverified and misleading information on the Internet has attracted a heightened attention of an interdisciplinary research community on the phenomenon. This paper contributes to the research efforts of automatically determining the veracity of rumourous tweets and classifying their replies according to stance. Our research objective was to investigate the interplay between a number of phenomenological and contextual features of rumours, in particular, we explore the extent to which network structural characteristics, metadata and user profiles could complement the linguistic analysis of the written content for the task at hand. The current findings strongly demonstrate that supplementary sources of information play significant role in classifying the veracity and the stance of Twitter interactions deemed to be rumourous.</abstract>
      <url hash="d0fb951a">S19-2190</url>
      <doi>10.18653/v1/S19-2190</doi>
    </paper>
    <paper id="191">
      <title><fixed-case>BLCU</fixed-case>_<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 7: An Inference Chain-based <fixed-case>GPT</fixed-case> Model for Rumour Evaluation</title>
      <author><first>Ruoyao</first><last>Yang</last></author>
      <author><first>Wanying</first><last>Xie</last></author>
      <author><first>Chunhua</first><last>Liu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>1090–1096</pages>
      <abstract>Researchers have been paying increasing attention to rumour evaluation due to the rapid spread of unsubstantiated rumours on social media platforms, including SemEval 2019 task 7. However, labelled data for learning rumour veracity is scarce, and labels in rumour stance data are highly disproportionate, making it challenging for a model to perform supervised-learning adequately. We propose an inference chain-based system, which fully utilizes conversation structure-based knowledge in the limited data and expand the training data in minority categories to alleviate class imbalance. Our approach obtains 12.6% improvement upon the baseline system for subtask A, ranks 1st among 21 systems in subtask A, and ranks 4th among 12 systems in subtask B.</abstract>
      <url hash="f419313f">S19-2191</url>
      <doi>10.18653/v1/S19-2191</doi>
    </paper>
    <paper id="192">
      <title><fixed-case>BUT</fixed-case>-<fixed-case>FIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 7: Determining the Rumour Stance with Pre-Trained Deep Bidirectional Transformers</title>
      <author><first>Martin</first><last>Fajcik</last></author>
      <author><first>Pavel</first><last>Smrz</last></author>
      <author><first>Lukas</first><last>Burget</last></author>
      <pages>1097–1104</pages>
      <abstract>This paper describes our system submitted to SemEval 2019 Task 7: RumourEval 2019: Determining Rumour Veracity and Support for Rumours, Subtask A (Gorrell et al., 2019). The challenge focused on classifying whether posts from Twitter and Reddit support, deny, query, or comment a hidden rumour, truthfulness of which is the topic of an underlying discussion thread. We formulate the problem as a stance classification, determining the rumour stance of a post with respect to the previous thread post and the source thread post. The recent BERT architecture was employed to build an end-to-end system which has reached the F1 score of 61.67 % on the provided test data. Without any hand-crafted feature, the system finished at the 2nd place in the competition, only 0.2 % behind the winner.</abstract>
      <url hash="46cd48b2">S19-2192</url>
      <doi>10.18653/v1/S19-2192</doi>
    </paper>
    <paper id="193">
      <title><fixed-case>CLEAR</fixed-case>umor at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 7: <fixed-case>C</fixed-case>onvo<fixed-case>L</fixed-case>ving <fixed-case>ELM</fixed-case>o Against Rumors</title>
      <author><first>Ipek</first><last>Baris</last></author>
      <author><first>Lukas</first><last>Schmelzeisen</last></author>
      <author><first>Steffen</first><last>Staab</last></author>
      <pages>1105–1109</pages>
      <abstract>This paper describes our submission to SemEval-2019 Task 7: RumourEval: Determining Rumor Veracity and Support for Rumors. We participated in both subtasks. The goal of subtask A is to classify the type of interaction between a rumorous social media post and a reply post as support, query, deny, or comment. The goal of subtask B is to predict the veracity of a given rumor. For subtask A, we implement a CNN-based neural architecture using ELMo embeddings of post text combined with auxiliary features and achieve a F1-score of 44.6%. For subtask B, we employ a MLP neural network leveraging our estimates for subtask A and achieve a F1-score of 30.1% (second place in the competition). We provide results and analysis of our system performance and present ablation experiments.</abstract>
      <url hash="049a561a">S19-2193</url>
      <doi>10.18653/v1/S19-2193</doi>
    </paper>
    <paper id="194">
      <title><fixed-case>C</fixed-case>olumbia at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 7: Multi-task Learning for Stance Classification and Rumour Verification</title>
      <author><first>Zhuoran</first><last>Liu</last></author>
      <author><first>Shivali</first><last>Goel</last></author>
      <author><first>Mukund</first><last>Yelahanka Raghuprasad</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>1110–1114</pages>
      <abstract>The paper presents Columbia team’s participation in the SemEval 2019 Shared Task 7: RumourEval 2019. Detecting rumour on social networks has been a focus of research in recent years. Previous work suffered from data sparsity, which potentially limited the application of more sophisticated neural architecture to this task. We mitigate this problem by proposing a multi-task learning approach together with language model fine-tuning. Our attention-based model allows different tasks to leverage different level of information. Our system ranked 6th overall with an F1-score of 36.25 on stance classification and F1 of 22.44 on rumour verification.</abstract>
      <url hash="bfc07a87">S19-2194</url>
      <doi>10.18653/v1/S19-2194</doi>
    </paper>
    <paper id="195">
      <title><fixed-case>GWU</fixed-case> <fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 7: Hybrid Pipeline for Rumour Veracity and Stance Classification on Social Media</title>
      <author><first>Sardar</first><last>Hamidian</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>1115–1119</pages>
      <abstract>Social media plays a crucial role as the main resource news for information seekers online. However, the unmoderated feature of social media platforms lead to the emergence and spread of untrustworthy contents which harm individuals or even societies. Most of the current automated approaches for automatically determining the veracity of a rumor are not generalizable for novel emerging topics. This paper describes our hybrid system comprising rules and a machine learning model which makes use of replied tweets to identify the veracity of the source tweet. The proposed system in this paper achieved 0.435 F-Macro in stance classification, and 0.262 F-macro and 0.801 RMSE in rumor verification tasks in Task7 of SemEval 2019.</abstract>
      <url hash="bf5545ce">S19-2195</url>
      <doi>10.18653/v1/S19-2195</doi>
    </paper>
    <paper id="196">
      <title><fixed-case>SINAI</fixed-case>-<fixed-case>DL</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 7: Data Augmentation and Temporal Expressions</title>
      <author><first>Miguel A.</first><last>García-Cumbreras</last></author>
      <author><first>Salud María</first><last>Jiménez-Zafra</last></author>
      <author><first>Arturo</first><last>Montejo-Ráez</last></author>
      <author><first>Manuel Carlos</first><last>Díaz-Galiano</last></author>
      <author><first>Estela</first><last>Saquete</last></author>
      <pages>1120–1124</pages>
      <abstract>This paper describes the participation of the SINAI-DL team at RumourEval (Task 7 in SemEval 2019, subtask A: SDQC). SDQC addresses the challenge of rumour stance classification as an indirect way of identifying potential rumours. Given a tweet with several replies, our system classifies each reply into either supporting, denying, questioning or commenting on the underlying rumours. We have applied data augmentation, temporal expressions labelling and transfer learning with a four-layer neural classifier. We achieve an accuracy of 0.715 with the official run over reply tweets.</abstract>
      <url hash="c2dad255">S19-2196</url>
      <doi>10.18653/v1/S19-2196</doi>
    </paper>
    <paper id="197">
      <title><fixed-case>UPV</fixed-case>-28-<fixed-case>UNITO</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 7: Exploiting Post’s Nesting and Syntax Information for Rumor Stance Classification</title>
      <author><first>Bilal</first><last>Ghanem</last></author>
      <author><first>Alessandra Teresa</first><last>Cignarella</last></author>
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <author><first>Francisco Manuel</first><last>Rangel Pardo</last></author>
      <pages>1125–1131</pages>
      <abstract>In the present paper we describe the UPV-28-UNITO system’s submission to the RumorEval 2019 shared task. The approach we applied for addressing both the subtasks of the contest exploits both classical machine learning algorithms and word embeddings, and it is based on diverse groups of features: stylistic, lexical, emotional, sentiment, meta-structural and Twitter-based. A novel set of features that take advantage of the syntactic information in texts is moreover introduced in the paper.</abstract>
      <url hash="cfe2890a">S19-2197</url>
      <doi>10.18653/v1/S19-2197</doi>
    </paper>
    <paper id="198">
      <title><fixed-case>BLCU</fixed-case>_<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 8: A Contextual Knowledge-enhanced <fixed-case>GPT</fixed-case> Model for Fact Checking</title>
      <author><first>Wanying</first><last>Xie</last></author>
      <author><first>Mengxi</first><last>Que</last></author>
      <author><first>Ruoyao</first><last>Yang</last></author>
      <author><first>Chunhua</first><last>Liu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>1132–1137</pages>
      <abstract>Since the resources of Community Question Answering are abundant and information sharing becomes universal, it will be increasingly difficult to find factual information for questioners in massive messages. SemEval 2019 task 8 is focusing on these issues. We participate in the task and use Generative Pre-trained Transformer (OpenAI GPT) as our system. Our innovations are data extension, feature extraction, and input transformation. For contextual knowledge enhancement, we extend the training set of subtask A, use several features to improve the results of our system and adapt the input formats to be more suitable for this task. We demonstrate the effectiveness of our approaches, which achieves 81.95% of subtask A and 61.08% of subtask B in accuracy on the SemEval 2019 task 8.</abstract>
      <url hash="9d86adf3">S19-2198</url>
      <doi>10.18653/v1/S19-2198</doi>
    </paper>
    <paper id="199">
      <title><fixed-case>C</fixed-case>ode<fixed-case>F</fixed-case>or<fixed-case>T</fixed-case>he<fixed-case>C</fixed-case>hange at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 8: Skip-Thoughts for Fact Checking in Community Question Answering</title>
      <author><first>Adithya</first><last>Avvaru</last></author>
      <author><first>Anupam</first><last>Pandey</last></author>
      <pages>1138–1143</pages>
      <abstract>The strengths of the scalable gradient tree boosting algorithm, XGBoost and distributed sentence encoder, Skip-Thought Vectors are not explored yet by the cQA research community. We tried to apply and combine these two effective methods for finding factual nature of the questions and answers. The work also include experimentation with other popular classifier models like AdaBoost Classifier, DecisionTree Classifier, RandomForest Classifier, ExtraTrees Classifier, XGBoost Classifier and Multi-layer Neural Network. In this paper, we present the features used, approaches followed for feature engineering, models experimented with and finally the results.</abstract>
      <url hash="9e7904d2">S19-2199</url>
      <doi>10.18653/v1/S19-2199</doi>
    </paper>
    <paper id="200">
      <title><fixed-case>C</fixed-case>olumbia<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 8: The Answer is Language Model Fine-tuning</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>1144–1148</pages>
      <abstract>Community Question Answering forums are very popular nowadays, as they represent effective means for communities to share information around particular topics. But the information shared on these forums are often not authentic. This paper presents the ColumbiaNLP submission for the SemEval-2019 Task 8: Fact-Checking in Community Question Answering Forums. We show how fine-tuning a language model on a large unannotated corpus of old threads from Qatar Living forum helps us to classify question types (factual, opinion, socializing) and to judge the factuality of answers on the shared task labeled data from the same forum. Our system finished 4th and 2nd on Subtask A (question type classification) and B (answer factuality prediction), respectively, based on the official metric of accuracy.</abstract>
      <url hash="9c353085">S19-2200</url>
      <doi>10.18653/v1/S19-2200</doi>
    </paper>
    <paper id="201">
      <title><fixed-case>DOMLIN</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 8: Automated Fact Checking exploiting Ratings in Community Question Answering Forums</title>
      <author><first>Dominik</first><last>Stammbach</last></author>
      <author><first>Stalin</first><last>Varanasi</last></author>
      <author><first>Guenter</first><last>Neumann</last></author>
      <pages>1149–1154</pages>
      <abstract>In the following, we describe our system developed for the Semeval2019 Task 8. We fine-tuned a BERT checkpoint on the qatar living forum dump and used this checkpoint to train a number of models. Our hand-in for subtask A consists of a fine-tuned classifier from this BERT checkpoint. For subtask B, we first have a classifier deciding whether a comment is factual or non-factual. If it is factual, we retrieve intra-forum evidence and using this evidence, have a classifier deciding the comment’s veracity. We trained this classifier on ratings which we crawled from qatarliving.com</abstract>
      <url hash="84843a14">S19-2201</url>
      <doi>10.18653/v1/S19-2201</doi>
    </paper>
    <paper id="202">
      <title><fixed-case>DUTH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 8: Part-Of-Speech Features for Question Classification</title>
      <author><first>Anastasios</first><last>Bairaktaris</last></author>
      <author><first>Symeon</first><last>Symeonidis</last></author>
      <author><first>Avi</first><last>Arampatzis</last></author>
      <pages>1155–1159</pages>
      <abstract>This report describes the methods employed by the Democritus University of Thrace (DUTH) team for participating in SemEval-2019 Task 8: Fact Checking in Community Question Answering Forums. Our team dealt only with Subtask A: Question Classification. Our approach was based on shallow natural language processing (NLP) pre-processing techniques to reduce the noise in data, feature selection methods, and supervised machine learning algorithms such as NearestCentroid, Perceptron, and LinearSVC. To determine the essential features, we were aided by exploratory data analysis and visualizations. In order to improve classification accuracy, we developed a customized list of stopwords, retaining some opinion- and fact-denoting common function words which would have been removed by standard stoplisting. Furthermore, we examined the usefulness of part-of-speech (POS) categories for the task; by trying to remove nouns and adjectives, we found some evidence that verbs are a valuable POS category for the opinion question class.</abstract>
      <url hash="80a720b5">S19-2202</url>
      <doi>10.18653/v1/S19-2202</doi>
    </paper>
    <paper id="203">
      <title>Fermi at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 8: An elementary but effective approach to Question Discernment in Community <fixed-case>QA</fixed-case> Forums</title>
      <author><first>Bakhtiyar</first><last>Syed</last></author>
      <author><first>Vijayasaradhi</first><last>Indurthi</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Manish</first><last>Gupta</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <pages>1160–1164</pages>
      <abstract>Online Community Question Answering Forums (cQA) have gained massive popularity within recent years. The rise in users for such forums have led to the increase in the need for automated evaluation for question comprehension and fact evaluation of the answers provided by various participants in the forum. Our team, <b>Fermi</b>, participated in sub-task A of Task 8 at SemEval 2019 - which tackles the first problem in the pipeline of factual evaluation in cQA forums, i.e., deciding whether a posed question asks for a factual information, an opinion/advice or is just socializing. This information is highly useful in segregating factual questions from non-factual ones which highly helps in organizing the questions into useful categories and trims down the problem space for the next task in the pipeline for fact evaluation among the available answers. Our system uses the embeddings obtained from Universal Sentence Encoder combined with XGBoost for the classification sub-task A. We also evaluate other combinations of embeddings and off-the-shelf machine learning algorithms to demonstrate the efficacy of the various representations and their combinations. Our results across the evaluation test set gave an accuracy of 84% and received the first position in the final standings judged by the organizers.</abstract>
      <url hash="a2b7b8cd">S19-2203</url>
      <doi>10.18653/v1/S19-2203</doi>
    </paper>
    <paper id="204">
      <title><fixed-case>S</fixed-case>olomon<fixed-case>L</fixed-case>ab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 8: Question Factuality and Answer Veracity Prediction in Community Forums</title>
      <author><first>Ankita</first><last>Gupta</last></author>
      <author><first>Sudeep Kumar</first><last>Sahoo</last></author>
      <author><first>Divya</first><last>Prakash</last></author>
      <author><first>Rohit</first><last>R.R</last></author>
      <author><first>Vertika</first><last>Srivastava</last></author>
      <author><first>Yeon Hyang</first><last>Kim</last></author>
      <pages>1165–1171</pages>
      <abstract>We describe our system for SemEval-2019, Task 8 on “Fact-Checking in Community Question Answering Forums (cQA)”. cQA forums are very prevalent nowadays, as they provide an effective means for communities to share knowledge. Unfortunately, this shared information is not always factual and fact-verified. In this task, we aim to identify factual questions posted on cQA and verify the veracity of answers to these questions. Our approach relies on data augmentation and aggregates cues from several dimensions such as semantics, linguistics, syntax, writing style and evidence obtained from trusted external sources. In subtask A, our submission is ranked 3rd, with an accuracy of 83.14%. Our current best solution stands 1st on the leaderboard with 88% accuracy. In subtask B, our present solution is ranked 2nd, with 58.33% MAP score.</abstract>
      <url hash="6ee476ba">S19-2204</url>
      <doi>10.18653/v1/S19-2204</doi>
    </paper>
    <paper id="205">
      <title><fixed-case>TML</fixed-case>ab <fixed-case>SRPOL</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 8: Fact Checking in Community Question Answering Forums</title>
      <author><first>Piotr</first><last>Niewiński</last></author>
      <author><first>Aleksander</first><last>Wawer</last></author>
      <author><first>Maria</first><last>Pszona</last></author>
      <author><first>Maria</first><last>Janicka</last></author>
      <pages>1172–1175</pages>
      <abstract>The article describes our submission to SemEval 2019 Task 8 on Fact-Checking in Community Forums. The systems under discussion participated in Subtask A: decide whether a question asks for factual information, opinion/advice or is just socializing. Our primary submission was ranked as the second one among all participants in the official evaluation phase. The article presents our primary solution: Deeply Regularized Residual Neural Network (DRR NN) with Universal Sentence Encoder embeddings. This is followed by a description of two contrastive solutions based on ensemble methods.</abstract>
      <url hash="c22e2ecb">S19-2205</url>
      <doi>10.18653/v1/S19-2205</doi>
    </paper>
    <paper id="206">
      <title><fixed-case>T</fixed-case>ue<fixed-case>F</fixed-case>act at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2019 Task 8: Fact checking in community question answering forums: context matters</title>
      <author><first>Réka</first><last>Juhász</last></author>
      <author><first>Franziska Barbara</first><last>Linnenschmidt</last></author>
      <author><first>Teslin</first><last>Roys</last></author>
      <pages>1176–1179</pages>
      <abstract>The SemEval 2019 Task 8 on Fact-Checking in community question answering forums aimed to classify questions into categories and verify the correctness of answers given on the QatarLiving public forum. The task was divided into two subtasks: the first classifying the question, the second the answers. The TueFact system described in this paper used different approaches for the two subtasks. Subtask A makes use of word vectors based on a bag-of-word-ngram model using up to trigrams. Predictions are done using multi-class logistic regression. The official SemEval result lists an accuracy of 0.60. Subtask B uses vectorized character n-grams up to trigrams instead. Predictions are done using a LSTM model and achieved an accuracy of 0.53 on the final SemEval Task 8 evaluation set.</abstract>
      <url hash="d35e7342">S19-2206</url>
      <doi>10.18653/v1/S19-2206</doi>
    </paper>
    <paper id="207">
      <title><fixed-case>YNU</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 8: Using A <fixed-case>LSTM</fixed-case>-Attention Model for Fact-Checking in Community Forums</title>
      <author><first>Peng</first><last>Liu</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>1180–1184</pages>
      <abstract>We propose a system that uses a long short-term memory with attention mechanism (LSTM-Attention) model to complete the task. The LSTM-Attention model uses two LSTM to extract the features of the question and answer pair. Then, each of the features is sequentially composed using the attention mechanism, concatenating the two vectors into one. Finally, the concatenated vector is used as input for the MLP and the MLP’s output layer uses the softmax function to classify the provided answers into three categories. This model is capable of extracting the features of the question and answer pair well. The results show that the proposed system outperforms the baseline algorithm.</abstract>
      <url hash="1d3d6714">S19-2207</url>
      <doi>10.18653/v1/S19-2207</doi>
    </paper>
    <paper id="208">
      <title><fixed-case>DBMS</fixed-case>-<fixed-case>KU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Exploring Machine Learning Approaches in Classifying Text as Suggestion or Non-Suggestion</title>
      <author><first>Tirana</first><last>Fatyanosa</last></author>
      <author><first>Al Hafiz Akbar Maulana</first><last>Siagian</last></author>
      <author><first>Masayoshi</first><last>Aritsugi</last></author>
      <pages>1185–1191</pages>
      <abstract>This paper describes the participation of DBMS-KU team in the SemEval 2019 Task 9, that is, suggestion mining from online reviews and forums. To deal with this task, we explore several machine learning approaches, i.e., Random Forest (RF), Logistic Regression (LR), Multinomial Naive Bayes (MNB), Linear Support Vector Classification (LSVC), Sublinear Support Vector Classification (SSVC), Convolutional Neural Network (CNN), and Variable Length Chromosome Genetic Algorithm-Naive Bayes (VLCGA-NB). Our system obtains reasonable results of F1-Score 0.47 and 0.37 on the evaluation data in Subtask A and Subtask B, respectively. In particular, our obtained results outperform the baseline in Subtask A. Interestingly, the results seem to show that our system could perform well in classifying Non-suggestion class.</abstract>
      <url hash="6b4ff76d">S19-2208</url>
      <doi>10.18653/v1/S19-2208</doi>
    </paper>
    <paper id="209">
      <title><fixed-case>DS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: From Suggestion Mining with neural networks to adversarial cross-domain classification</title>
      <author><first>Tobias</first><last>Cabanski</last></author>
      <pages>1192–1198</pages>
      <abstract>Suggestion Mining is the task of classifying sentences into suggestions or non-suggestions. SemEval-2019 Task 9 sets the task to mine suggestions from online texts. For each of the two subtasks, the classification has to be applied on a different domain. Subtask A addresses the domain of posts in suggestion online forums and comes with a set of training examples, that is used for supervised training. A combination of LSTM and CNN networks is constructed to create a model which uses BERT word embeddings as input features. For subtask B, the domain of hotel reviews is regarded. In contrast to subtask A, no labeled data for supervised training is provided, so that additional unlabeled data is taken to apply a cross-domain classification. This is done by using adversarial training of the three model parts label classifier, domain classifier and the shared feature representation. For subtask A, the developed model archives a F1-score of 0.7273, which is in the top ten of the leader board. The F1-score for subtask B is 0.8187 and is ranked in the top five of the submissions for that task.</abstract>
      <url hash="818f0519">S19-2209</url>
      <doi>10.18653/v1/S19-2209</doi>
    </paper>
    <paper id="210">
      <title>Hybrid <fixed-case>RNN</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Blending Information Sources for Domain-Independent Suggestion Mining</title>
      <author><first>Aysu</first><last>Ezen-Can</last></author>
      <author><first>Ethem F.</first><last>Can</last></author>
      <pages>1199–1203</pages>
      <abstract>Social media has an increasing amount of information that both customers and companies can benefit from. These social media posts can include Tweets or be in the form of vocalization of complements and complaints (e.g., reviews) of a product or service. Researchers have been actively mining this invaluable information source to automatically generate insights. Mining sentiments of customer reviews is an example that has gained momentum due to its potential to gather information that customers are not happy about. Instead of reading millions of reviews, companies prefer sentiment analysis to obtain feedback and to improve their products or services. In this work, we aim to identify information that companies can act on, or other customers can utilize for making their own experience better. This is different from identifying if reviews of a product or service is negative, positive, or neutral. To that end, we classify sentences of a given review as suggestion or not suggestion so that readers of the reviews do not have to go through thousands of reviews but instead can focus on actionable items and applicable suggestions. To identify suggestions within reviews, we employ a hybrid approach that utilizes a recurrent neural network (RNN) along with rule-based features to build a domain-independent suggestion mining model. In this way, a model trained on electronics reviews is used to extract suggestions from hotel reviews.</abstract>
      <url hash="26e4aae8">S19-2210</url>
      <doi>10.18653/v1/S19-2210</doi>
    </paper>
    <paper id="211">
      <title><fixed-case>INRIA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Suggestion Mining Using <fixed-case>SVM</fixed-case> with Handcrafted Features</title>
      <author><first>Ilia</first><last>Markov</last></author>
      <author><first>Eric</first><last>Villemonte de la Clergerie</last></author>
      <pages>1204–1207</pages>
      <abstract>We present the INRIA approach to the suggestion mining task at SemEval 2019. The task consists of two subtasks: suggestion mining under single-domain (Subtask A) and cross-domain (Subtask B) settings. We used the Support Vector Machines algorithm trained on handcrafted features, function words, sentiment features, digits, and verbs for Subtask A, and handcrafted features for Subtask B. Our best run archived a F1-score of 51.18% on Subtask A, and ranked in the top ten of the submissions for Subtask B with 73.30% F1-score.</abstract>
      <url hash="2445c35d">S19-2211</url>
      <doi>10.18653/v1/S19-2211</doi>
    </paper>
    <paper id="212">
      <title>Lijunyi at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: An attention-based <fixed-case>LSTM</fixed-case> and ensemble of different models for suggestion mining from online reviews and forums</title>
      <author><first>Junyi</first><last>Li</last></author>
      <pages>1208–1212</pages>
      <abstract>In this paper, we describe a suggestion mining system that participated in SemEval 2019 Task 9, SubTask A - Suggestion Mining from Online Reviews and Forums. Given some suggestions from online reviews and forums that can be classified into suggestion and non-suggestion classes. In this task, we combine the attention mechanism with the LSTM model, which is the final system we submitted. The final submission achieves 14th place in Task 9, SubTask A with the accuracy of 0.6776. After the challenge, we train a series of neural network models such as convolutional neural network(CNN), TextCNN, long short-term memory(LSTM) and C-LSTM. Finally, we make an ensemble on the predictions of these models and get a better result.</abstract>
      <url hash="8fff6c80">S19-2212</url>
      <doi>10.18653/v1/S19-2212</doi>
    </paper>
    <paper id="213">
      <title><fixed-case>MIDAS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Suggestion Mining from Online Reviews using <fixed-case>ULMF</fixed-case>it</title>
      <author><first>Sarthak</first><last>Anand</last></author>
      <author><first>Debanjan</first><last>Mahata</last></author>
      <author><first>Kartik</first><last>Aggarwal</last></author>
      <author><first>Laiba</first><last>Mehnaz</last></author>
      <author><first>Simra</first><last>Shahid</last></author>
      <author><first>Haimin</first><last>Zhang</last></author>
      <author><first>Yaman</first><last>Kumar</last></author>
      <author><first>Rajiv</first><last>Shah</last></author>
      <author><first>Karan</first><last>Uppal</last></author>
      <pages>1213–1217</pages>
      <abstract>In this paper we present our approach to tackle the Suggestion Mining from Online Reviews and Forums Sub-Task A. Given a review, we are asked to predict whether the review consists of a suggestion or not. Our model is based on Universal Language Model Fine-tuning for Text Classification. We apply various pre-processing techniques before training the language and the classification model. We further provide analysis of the model. Our team ranked 10th out of 34 participants, achieving an F1 score of 0.7011.</abstract>
      <url hash="a42ef3c3">S19-2213</url>
      <doi>10.18653/v1/S19-2213</doi>
    </paper>
    <paper id="214">
      <title><fixed-case>NL</fixed-case>-<fixed-case>FIIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Neural Model Ensemble for Suggestion Mining</title>
      <author><first>Samuel</first><last>Pecar</last></author>
      <author><first>Marian</first><last>Simko</last></author>
      <author><first>Maria</first><last>Bielikova</last></author>
      <pages>1218–1223</pages>
      <abstract>In this paper, we present neural model architecture submitted to the SemEval-2019 Task 9 competition: “Suggestion Mining from Online Reviews and Forums”. We participated in both subtasks for domain specific and also cross-domain suggestion mining. We proposed a recurrent neural network architecture that employs Bi-LSTM layers and also self-attention mechanism. Our architecture tries to encode words via word representation using ELMo and ensembles multiple models to achieve better results. We highlight importance of pre-processing of user-generated samples and its contribution to overall results. We performed experiments with different setups of our proposed model involving weighting of prediction classes for loss function. Our best model achieved in official test evaluation score of 0.6816 for subtask A and 0.6850 for subtask B. In official results, we achieved 12th and 10th place in subtasks A and B, respectively.</abstract>
      <url hash="58fe6ec5">S19-2214</url>
      <doi>10.18653/v1/S19-2214</doi>
    </paper>
    <paper id="215">
      <title><fixed-case>NTUA</fixed-case>-<fixed-case>ISL</fixed-case>ab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Mining Suggestions in the wild</title>
      <author><first>Rolandos Alexandros</first><last>Potamias</last></author>
      <author><first>Alexandros</first><last>Neofytou</last></author>
      <author><first>Georgios</first><last>Siolas</last></author>
      <pages>1224–1230</pages>
      <abstract>As online customer forums and product comparison sites increase their societal influence, users are actively expressing their opinions and posting their recommendations on their fellow customers online. However, systems capable of recognizing suggestions still lack in stability. Suggestion Mining, a novel and challenging field of Natural Language Processing, is increasingly gaining attention, aiming to track user advice on online forums. In this paper, a carefully designed methodology to identify customer-to-company and customer-to-customer suggestions is presented. The methodology implements a rule-based classifier using heuristic, lexical and syntactic patterns. The approach ranked at 5th and 1st position, achieving an f1-score of 0.749 and 0.858 for SemEval-2019/Suggestion Mining sub-tasks A and B, respectively. In addition, we were able to improve performance results by combining the rule-based classifier with a recurrent convolutional neural network, that exhibits an f1-score of 0.79 for subtask A.</abstract>
      <url hash="ce3f5cce">S19-2215</url>
      <doi>10.18653/v1/S19-2215</doi>
    </paper>
    <paper id="216">
      <title><fixed-case>O</fixed-case>le<fixed-case>N</fixed-case>et at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: <fixed-case>BERT</fixed-case> based Multi-Perspective Models for Suggestion Mining</title>
      <author><first>Jiaxiang</first><last>Liu</last></author>
      <author><first>Shuohuan</first><last>Wang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <pages>1231–1236</pages>
      <abstract>This paper describes our system partici- pated in Task 9 of SemEval-2019: the task is focused on suggestion mining and it aims to classify given sentences into sug- gestion and non-suggestion classes in do- main specific and cross domain training setting respectively. We propose a multi- perspective architecture for learning rep- resentations by using different classical models including Convolutional Neural Networks (CNN), Gated Recurrent Units (GRU), Feed Forward Attention (FFA), etc. To leverage the semantics distributed in large amount of unsupervised data, we also have adopted the pre-trained Bidi- rectional Encoder Representations from Transformers (BERT) model as an en- coder to produce sentence and word rep- resentations. The proposed architecture is applied for both sub-tasks, and achieved f1-score of 0.7812 for subtask A, and 0.8579 for subtask B. We won the first and second place for the two tasks respec- tively in the final competition.</abstract>
      <url hash="2b5ea3df">S19-2216</url>
      <doi>10.18653/v1/S19-2216</doi>
    </paper>
    <paper id="217">
      <title><fixed-case>SSN</fixed-case>-<fixed-case>SPARKS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Mining Suggestions from Online Reviews using Deep Learning Techniques on Augmented Data</title>
      <author><first>Rajalakshmi</first><last>S</last></author>
      <author><first>Angel</first><last>Suseelan</last></author>
      <author><first>S Milton</first><last>Rajendram</last></author>
      <author><first>Mirnalinee</first><last>T T</last></author>
      <pages>1237–1241</pages>
      <abstract>This paper describes the work on mining the suggestions from online reviews and forums. Opinion mining detects whether the comments are positive, negative or neutral, while suggestion mining explores the review content for the possible tips or advice. The system developed by SSN-SPARKS team in SemEval-2019 for task 9 (suggestion mining) uses a rule-based approach for feature selection, SMOTE technique for data augmentation and deep learning technique (Convolutional Neural Network) for classification. We have compared the results with Random Forest classifier (RF) and MultiLayer Perceptron (MLP) model. Results show that the CNN model performs better than other models for both the subtasks.</abstract>
      <url hash="892abf9f">S19-2217</url>
      <doi>10.18653/v1/S19-2217</doi>
    </paper>
    <paper id="218">
      <title>Suggestion Miner at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Suggestion Detection in Online Forum using Word Graph</title>
      <author><first>Usman</first><last>Ahmed</last></author>
      <author><first>Humera</first><last>Liaquat</last></author>
      <author><first>Luqman</first><last>Ahmed</last></author>
      <author><first>Syed Jawad</first><last>Hussain</last></author>
      <pages>1242–1246</pages>
      <abstract>This paper describes the suggestion miner system that participates in SemEval 2019 Task 9 - SubTask A - Suggestion Mining from Online Reviews and Forums. The system participated in the subtasks A. This paper discusses the results of our system in the development, evaluation and post evaluation. Each class in the dataset is represented as directed unweighted graphs. Then, the comparison is carried out with each class graph which results in a vector. This vector is used as features by a machine learning algorithm. The model is evaluated on hold on strategy. The organizers randomly split (8500 instances) training set (provided to the participant in training their system) and testing set (833 instances). The test set is reserved to evaluate the performance of participants systems. During the evaluation, our system ranked 31 in the Coda Lab result of the subtask A (binary class problem). The binary class system achieves evaluation value 0.34, precision 0.87, recall 0.73 and F measure 0.78.</abstract>
      <url hash="6476fdf0">S19-2218</url>
      <doi>10.18653/v1/S19-2218</doi>
    </paper>
    <paper id="219">
      <title>Team <fixed-case>T</fixed-case>aurus at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Expert-informed pattern recognition for suggestion mining</title>
      <author><first>Nelleke</first><last>Oostdijk</last></author>
      <author><first>Hans</first><last>van Halteren</last></author>
      <pages>1247–1253</pages>
      <abstract>This paper presents our submissions to SemEval-2019 Task9, Suggestion Mining. Our system is one in a series of systems in which we compare an approach using expert-defined rules with a comparable one using machine learning. We target tasks with a syntactic or semantic component that might be better described by a human understanding the task than by a machine learner only able to count features. For Semeval-2019 Task 9, the expert rules clearly outperformed our machine learning model when training and testing on equally balanced testsets.</abstract>
      <url hash="f7a90d9a">S19-2219</url>
      <doi>10.18653/v1/S19-2219</doi>
    </paper>
    <paper id="220">
      <title><fixed-case>T</fixed-case>his<fixed-case>I</fixed-case>s<fixed-case>C</fixed-case>ompetition at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: <fixed-case>BERT</fixed-case> is unstable for out-of-domain samples</title>
      <author><first>Cheoneum</first><last>Park</last></author>
      <author><first>Juae</first><last>Kim</last></author>
      <author><first>Hyeon-gu</first><last>Lee</last></author>
      <author><first>Reinald Kim</first><last>Amplayo</last></author>
      <author><first>Harksoo</first><last>Kim</last></author>
      <author><first>Jungyun</first><last>Seo</last></author>
      <author><first>Changki</first><last>Lee</last></author>
      <pages>1254–1261</pages>
      <abstract>This paper describes our system, Joint Encoders for Stable Suggestion Inference (JESSI), for the SemEval 2019 Task 9: Suggestion Mining from Online Reviews and Forums. JESSI is a combination of two sentence encoders: (a) one using multiple pre-trained word embeddings learned from log-bilinear regression (GloVe) and translation (CoVe) models, and (b) one on top of word encodings from a pre-trained deep bidirectional transformer (BERT). We include a domain adversarial training module when training for out-of-domain samples. Our experiments show that while BERT performs exceptionally well for in-domain samples, several runs of the model show that it is unstable for out-of-domain samples. The problem is mitigated tremendously by (1) combining BERT with a non-BERT encoder, and (2) using an RNN-based classifier on top of BERT. Our final models obtained second place with 77.78% F-Score on Subtask A (i.e. in-domain) and achieved an F-Score of 79.59% on Subtask B (i.e. out-of-domain), even without using any additional external data.</abstract>
      <url hash="c9669a8f">S19-2220</url>
      <doi>10.18653/v1/S19-2220</doi>
    </paper>
    <paper id="221">
      <title><fixed-case>WUT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Domain-Adversarial Neural Networks for Domain Adaptation in Suggestion Mining</title>
      <author><first>Mateusz</first><last>Klimaszewski</last></author>
      <author><first>Piotr</first><last>Andruszkiewicz</last></author>
      <pages>1262–1266</pages>
      <abstract>We present a system for cross-domain suggestion mining, prepared for the SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums (Subtask B). Our submitted solution for this text classification problem explores the idea of treating different suggestions’ sources as one of the settings of Transfer Learning - Domain Adaptation. Our experiments show that without any labeled target domain examples during training time, we are capable of proposing a system, reaching up to 0.778 in terms of F1 score on test dataset, based on Target Preserving Domain Adversarial Neural Networks.</abstract>
      <url hash="4ac0a851">S19-2221</url>
      <doi>10.18653/v1/S19-2221</doi>
    </paper>
    <paper id="222">
      <title>Yimmon at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Suggestion Mining with Hybrid Augmented Approaches</title>
      <author><first>Yimeng</first><last>Zhuang</last></author>
      <pages>1267–1271</pages>
      <abstract>Suggestion mining task aims to extract tips, advice, and recommendations from unstructured text. The task includes many challenges, such as class imbalance, figurative expressions, context dependency, and long and complex sentences. This paper gives a detailed system description of our submission in SemEval 2019 Task 9 Subtask A. We transfer Self-Attention Network (SAN), a successful model in machine reading comprehension field, into this task. Our model concentrates on modeling long-term dependency which is indispensable to parse long and complex sentences. Besides, we also adopt techniques, such as contextualized embedding, back-translation, and auxiliary loss, to augment the system. Our model achieves a performance of F1=76.3, and rank 4th among 34 participating systems. Further ablation study shows that the techniques used in our system are beneficial to the performance.</abstract>
      <url hash="613c2b7e">S19-2222</url>
      <doi>10.18653/v1/S19-2222</doi>
    </paper>
    <paper id="223">
      <title><fixed-case>YNU</fixed-case>_<fixed-case>DYX</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: A Stacked <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> for Suggestion Mining Classification</title>
      <author><first>Yunxia</first><last>Ding</last></author>
      <author><first>Xiaobing</first><last>Zhou</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>1272–1276</pages>
      <abstract>In this paper we describe a deep-learning system that competed as SemEval 2019 Task 9-SubTask A: Suggestion Mining from Online Reviews and Forums. We use Word2Vec to learn the distributed representations from sentences. This system is composed of a Stacked Bidirectional Long-Short Memory Network (SBiLSTM) for enriching word representations before and after the sequence relationship with context. We perform an ensemble to improve the effectiveness of our model. Our official submission results achieve an F1-score 0.5659.</abstract>
      <url hash="a808aaff">S19-2223</url>
      <doi>10.18653/v1/S19-2223</doi>
    </paper>
    <paper id="224">
      <title><fixed-case>YNU</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Using a <fixed-case>BERT</fixed-case> and <fixed-case>CNN</fixed-case>-<fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case>-<fixed-case>GRU</fixed-case> Model for Suggestion Mining</title>
      <author><first>Ping</first><last>Yue</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>1277–1281</pages>
      <abstract>Consumer opinions towards commercial entities are generally expressed through online reviews, blogs, and discussion forums. These opinions largely express positive and negative sentiments towards a given entity,but also tend to contain suggestions for improving the entity. In this task, we extract suggestions from given the unstructured text, compared to the traditional opinion mining systems. Such suggestion mining is more applicability and extends capabilities.</abstract>
      <url hash="90e51fd5">S19-2224</url>
      <doi>10.18653/v1/S19-2224</doi>
    </paper>
    <paper id="225">
      <title>Zoho at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 9: Semi-supervised Domain Adaptation using Tri-training for Suggestion Mining</title>
      <author><first>Sai</first><last>Prasanna</last></author>
      <author><first>Sri Ananda</first><last>Seelan</last></author>
      <pages>1282–1286</pages>
      <abstract>This paper describes our submission for the SemEval-2019 Suggestion Mining task. A simple Convolutional Neural Network (CNN) classifier with contextual word representations from a pre-trained language model was used for sentence classification. The model is trained using tri-training, a semi-supervised bootstrapping mechanism for labelling unseen data. Tri-training proved to be an effective technique to accommodate domain shift for cross-domain suggestion mining (Subtask B) where there is no hand labelled training data. For in-domain evaluation (Subtask A), we use the same technique to augment the training set. Our system ranks thirteenth in Subtask A with an F1-score of 68.07 and third in Subtask B with an F1-score of 81.94.</abstract>
      <url hash="1b986c2e">S19-2225</url>
      <attachment type="software" hash="af26406c">S19-2225.Software.zip</attachment>
      <attachment type="poster" hash="2c18a839">S19-2225.Poster.pdf</attachment>
      <doi>10.18653/v1/S19-2225</doi>
    </paper>
    <paper id="226">
      <title><fixed-case>ZQM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task9: A Single Layer <fixed-case>CNN</fixed-case> Based on Pre-trained Model for Suggestion Mining</title>
      <author><first>Qimin</first><last>Zhou</last></author>
      <author><first>Zhengxin</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Wu</last></author>
      <author><first>Linmao</first><last>Wang</last></author>
      <pages>1287–1291</pages>
      <abstract>This paper describes our system that competed at SemEval 2019 Task 9 - SubTask A: ”Sug- gestion Mining from Online Reviews and Forums”. Our system fuses the convolutional neural network and the latest BERT model to conduct suggestion mining. In our system, the input of convolutional neural network is the embedding vectors which are drawn from the pre-trained BERT model. And to enhance the effectiveness of the whole system, the pre-trained BERT model is fine-tuned by provided datasets before the procedure of embedding vectors extraction. Empirical results show the effectiveness of our model which obtained 9th position out of 34 teams with F1 score equals to 0.715.</abstract>
      <url hash="c06b582a">S19-2226</url>
      <doi>10.18653/v1/S19-2226</doi>
    </paper>
    <paper id="227">
      <title><fixed-case>P</fixed-case>roblem<fixed-case>S</fixed-case>olver at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 10: Sequence-to-Sequence Learning and Expression Trees</title>
      <author><first>Xuefeng</first><last>Luo</last></author>
      <author><first>Alina</first><last>Baranova</last></author>
      <author><first>Jonas</first><last>Biegert</last></author>
      <pages>1292–1296</pages>
      <abstract>This paper describes our participation in SemEval-2019 shared task “Math Question Answering”, where the aim is to create a program that could solve the Math SAT questions automatically as accurately as possible. We went with a dual-pronged approach, building a Sequence-to-Sequence Neural Network pre-trained with augmented data that could answer all categories of questions and a Tree system, which can only answer a certain type of questions. The systems did not perform well on the entire test data given in the task, but did decently on the questions they were actually capable of answering. The Sequence-to-Sequence Neural Network model managed to get slightly better than our baseline of guessing “A” for every question, while the Tree system additionally improved the results.</abstract>
      <url hash="89daab66">S19-2227</url>
      <doi>10.18653/v1/S19-2227</doi>
    </paper>
    <paper id="228">
      <title><fixed-case>RGCL</fixed-case>-<fixed-case>WLV</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 12: Toponym Detection</title>
      <author><first>Alistair</first><last>Plum</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Pablo</first><last>Calleja</last></author>
      <author><first>Constantin</first><last>Orăsan</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>1297–1301</pages>
      <abstract>This article describes the system submitted by the RGCL-WLV team to the SemEval 2019 Task 12: Toponym resolution in scientific papers. The system detects toponyms using a bootstrapped machine learning (ML) approach which classifies names identified using gazetteers extracted from the GeoNames geographical database. The paper evaluates the performance of several ML classifiers, as well as how the gazetteers influence the accuracy of the system. Several runs were submitted. The highest precision achieved for one of the submissions was 89%, albeit it at a relatively low recall of 49%.</abstract>
      <url hash="fb896867">S19-2228</url>
      <doi>10.18653/v1/S19-2228</doi>
    </paper>
    <paper id="229">
      <title><fixed-case>THU</fixed-case>_<fixed-case>NGN</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 12: Toponym Detection and Disambiguation on Scientific Papers</title>
      <author><first>Tao</first><last>Qi</last></author>
      <author><first>Suyu</first><last>Ge</last></author>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>1302–1307</pages>
      <abstract>First name: Tao Last name: Qi Email: taoqi.qt@gmail.com Affiliation: Department of Electronic Engineering, Tsinghua University First name: Suyu Last name: Ge Email: gesy17@mails.tsinghua.edu.cn Affiliation: Department of Electronic Engineering, Tsinghua University First name: Chuhan Last name: Wu Email: wuch15@mails.tsinghua.edu.cn Affiliation: Department of Electronic Engineering, Tsinghua University First name: Yubo Last name: Chen Email: chen-yb18@mails.tsinghua.edu.cn Affiliation: Department of Electronic Engineering, Tsinghua University First name: Yongfeng Last name: Huang Email: yfhuang@mail.tsinghua.edu.cn Affiliation: Department of Electronic Engineering, Tsinghua University Toponym resolution is an important and challenging task in the neural language processing field, and has wide applications such as emergency response and social media geographical event analysis. Toponym resolution can be roughly divided into two independent steps, i.e., toponym detection and toponym disambiguation. In order to facilitate the study on toponym resolution, the SemEval 2019 task 12 is proposed, which contains three subtasks, i.e., toponym detection, toponym disambiguation and toponym resolution. In this paper, we introduce our system that participated in the SemEval 2019 task 12. For toponym detection, in our approach we use TagLM as the basic model, and explore the use of various features in this task, such as word embeddings extracted from pre-trained language models, POS tags and lexical features extracted from dictionaries. For toponym disambiguation, we propose a heuristics rule-based method using toponym frequency and population. Our systems achieved 83.03% strict macro F1, 74.50 strict micro F1, 85.92 overlap macro F1 and 78.47 overlap micro F1 in toponym detection subtask.</abstract>
      <url hash="bc83adec">S19-2229</url>
      <doi>10.18653/v1/S19-2229</doi>
    </paper>
    <paper id="230">
      <title><fixed-case>UNH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 12: Toponym Resolution in Scientific Papers</title>
      <author><first>Matthew</first><last>Magnusson</last></author>
      <author><first>Laura</first><last>Dietz</last></author>
      <pages>1308–1312</pages>
      <abstract>The SemEval-2019 Task 12 is toponym resolution in scientific papers. We focus on Subtask 1: Toponym Detection which is the identification of spans of text for place names mentioned in a document. We propose two methods: 1) sliding window convolutional neural network using ELMo embeddings (cnn-elmo), and 2) sliding window multi-Layer perceptron using ELMo embeddings (mlp-elmo). We also submit Bi-lateral LSTM with Conditional Random Fields (bi-LSTM) as a strong baseline given its state-of-art performance in Named Entity Recognition (NER) task. Our best performing model is cnn-elmo with a F1 of 0.844 which was below bi-LSTM F1 of 0.862 when evaluated on overlap macro detection. Eight teams participated in this subtask with a total of 21 submissions.</abstract>
      <url hash="240c1320">S19-2230</url>
      <doi>10.18653/v1/S19-2230</doi>
    </paper>
    <paper id="231">
      <title><fixed-case>U</fixed-case>ni<fixed-case>M</fixed-case>elb at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 12: Multi-model combination for toponym resolution</title>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Martin</first><last>Tomko</last></author>
      <author><first>Maria</first><last>Vasardani</last></author>
      <pages>1313–1318</pages>
      <abstract>This paper describes our submission to SemEval-2019 Task 12 on toponym resolution over scientific articles. We train separate NER models for toponym detection over text extracted from tables vs. text from the body of the paper, and train another auxiliary model to eliminate misdetected toponyms. For toponym disambiguation, we use an SVM classifier with hand-engineered features. The best setting achieved a strict micro-F1 score of 80.92% and overlap micro-F1 score of 86.88% in the toponym detection subtask, ranking 2nd out of 8 teams on F1 score. For toponym disambiguation and end-to-end resolution, we officially ranked 2nd and 3rd, respectively.</abstract>
      <url hash="0a5963db">S19-2231</url>
      <doi>10.18653/v1/S19-2231</doi>
    </paper>
    <paper id="232">
      <title><fixed-case>U</fixed-case>niversity of <fixed-case>A</fixed-case>rizona at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2019 Task 12: Deep-Affix Named Entity Recognition of Geolocation Entities</title>
      <author><first>Vikas</first><last>Yadav</last></author>
      <author><first>Egoitz</first><last>Laparra</last></author>
      <author><first>Ti-Tai</first><last>Wang</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <pages>1319–1323</pages>
      <abstract>We present the Named Entity Recognition (NER) and disambiguation model used by the University of Arizona team (UArizona) for the SemEval 2019 task 12. We achieved fourth place on tasks 1 and 3. We implemented a deep-affix based LSTM-CRF NER model for task 1, which utilizes only character, word, pre- fix and suffix information for the identification of geolocation entities. Despite using just the training data provided by task organizers and not using any lexicon features, we achieved 78.85% strict micro F-score on task 1. We used the unsupervised population heuristics for task 3 and achieved 52.99% strict micro-F1 score in this task.</abstract>
      <url hash="e63fb91c">S19-2232</url>
      <doi>10.18653/v1/S19-2232</doi>
    </paper>
  </volume>
</collection>
