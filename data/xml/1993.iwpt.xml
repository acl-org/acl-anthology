<?xml version='1.0' encoding='UTF-8'?>
<collection id="1993.iwpt">
  <volume id="1" ingest-date="2020-05-11">
    <meta>
      <booktitle>Proceedings of the Third International Workshop on Parsing Technologies</booktitle>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Tilburg, Netherlands and Durbuy, Belgium</address>
      <month>August 10-13</month>
      <year>1993</year>
      <editor><first>Harry</first><last>Bunt</last></editor>
      <editor><first>Robert</first><last>Berwick</last></editor>
      <editor><first>Ken</first><last>Church</last></editor>
      <editor><first>Aravind</first><last>Joshi</last></editor>
      <editor><first>Ronald</first><last>Kaplan</last></editor>
      <editor><first>Martin</first><last>Kay</last></editor>
      <editor><first>Bernard</first><last>Lang</last></editor>
      <editor><first>Makoto</first><last>Nagao</last></editor>
      <editor><first>Anton</first><last>Nijholt</last></editor>
      <editor><first>Mark</first><last>Steedman</last></editor>
      <editor><first>Henry</first><last>Thompson</last></editor>
      <editor><first>Masaru</first><last>Tomita</last></editor>
      <editor><first>K.</first><last>Vijay-Shanker</last></editor>
      <editor><first>Yorick</first><last>Wilks</last></editor>
      <editor><first>Kent</first><last>Wittenburg</last></editor>
      <url hash="1691fbfb">1993.iwpt-1</url>
      <venue>iwpt</venue>
    </meta>
    <paper id="1">
      <title>Proceedings of the Third International Workshop on Parsing Technologies (<fixed-case>IWPT</fixed-case> ’93)</title>
      <author><first>Harry</first><last>Bunt</last></author>
      <pages>i-vii</pages>
      <url hash="257716fe">1993.iwpt-1.1</url>
      <abstract/>
      <bibkey>bunt-1993-proceedings</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Parsing</title>
      <author><first>Rens</first><last>Bod</last></author>
      <pages>1-12</pages>
      <url hash="8e843ed3">1993.iwpt-1.2</url>
      <abstract>In stochastic language processing, we are often interested in the most probable parse of an input string. Since there can be exponentially many parses, comparing all of them is not efficient. The Viterbi algorithm (Viterbi, 1967; Fujisaki et al., 1989) provides a tool to calculate in cubic time the most probable derivation of a string generated by a stochastic context free grammar. However, in stochastic language models that allow a parse tree to be generated by different derivations – like Data Oriented Parsing (DOP) or Stochastic Lexicalized Tree-Adjoining Grammar (SLTAG) – the most probable derivation does not necessarily produce the most probable parse. In such cases, a Viterbi-style optimisation does not seem feasible to calculate the most probable parse. In the present article we show that by incorporating Monte Carlo techniques into a polynomial time parsing algorithm, the maximum probability parse can be estimated as accurately as desired in polynomial time. Monte Carlo parsing is not only relevant to DOP or SLTAG, but also provides for stochastic CFGs an interesting alternative to Viterbi. Unlike the current versions of Viterbi style optimisation (Fujisaki et al., 1989; Jelinek et al., 1990; Wright et al., 1991), Monte Carlo parsing is not restricted to CFGs in Chomsky Normal Form. For stochastic grammars that are parsable in cubic time, the time complexity of estimating the most probable parse with Monte Carlo turns out to be <tex-math>O(n^2\varepsilon^{-2})</tex-math>, where <tex-math>n</tex-math> is the length of the input string and <tex-math>\varepsilon</tex-math> the estimation error. In this paper we will treat Monte Carlo parsing first of all in the context of the DOP model, since it is especially here that the number of derivations generating a single tree becomes dramatically large. Finally, a Monte Carlo Chart parser is used to test the DOP model on a set of hand-parsed strings from the Air Travel Information System (ATIS) spoken language corpus. Preliminary experiments indicate 96% test set parsing accuracy. </abstract>
      <bibkey>bod-1993-monte</bibkey>
    </paper>
    <paper id="3">
      <title>Transformation-Based Error-Driven Parsing</title>
      <author><first>Eric</first><last>Brill</last></author>
      <pages>13-26</pages>
      <url hash="38a7ef07">1993.iwpt-1.3</url>
      <abstract>In this paper we describe a new technique for parsing free text: a transformational grammar is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce the number of errors. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.</abstract>
      <bibkey>brill-1993-transformation</bibkey>
    </paper>
    <paper id="4">
      <title>Parsing as Dynamic Interpretation</title>
      <author><first>Harry</first><last>Bunt</last></author>
      <author><first>Ko</first><last>van der Sloat</last></author>
      <pages>27-38</pages>
      <url hash="7d5d9484">1993.iwpt-1.4</url>
      <abstract>In this paper we consider the merging of the language of feature structures with a formal logical language, and how the semantic definition of the resulting language can be used in parsing. For the logical language we use the language EL, defined and implemented earlier for computational semantic purposes. To this language we add the basic constructions and operations of feature structures. The extended language we refer to as ‘Generalized EL’, or ‘GEL’. The semantics of EL, and that of its extension GEL, is defined model-theoretically: for each construction of the language, a recursive rule describes how its value can be computed from the values of its constituents. Since GEL talks not only about semantic objects and their relations but also about syntactic concepts, GEL models are nonstandard in containing both kinds of entities. Whereas phrase-structure rules are traditionally viewed procedurally, as recipes for building phrases, and a rule in the parsing-as-deduction is viewed declaratively, as a proposition which is true when the conditions for building the phrase are satisfied, a rule in GEL is best viewed as a proposition in Dynamic Semantics: it can be evaluated recursively, and evaluates not to true or false, but to the minimal change in the model, needed to make the proposition true. The viability of this idea has been demonstrated by a proof-of-concept implementation for DPSG chart parsing and an emulation of HPSG parsing in the STUF environment.</abstract>
      <bibkey>bunt-van-der-sloat-1993-parsing</bibkey>
    </paper>
    <paper id="5">
      <title>Compiling Typed Attribute-Value Logic Grammars</title>
      <author><first>Bob</first><last>Carpenter</last></author>
      <pages>39-48</pages>
      <url hash="b82ef672">1993.iwpt-1.5</url>
      <abstract>The unification-based approach to processing attribute-value logic grammars, similar to Prolog <i>interpretation</i>, has become the standard. We propose an alternative, embodied in the Attribute-Logic Engine (ALE) (Carpenter 1993) , based on the Warren Abstract Machine (WAM) approach to compiling Prolog (Aït-Kaci 1991). Phrase structure grammars with procedural attachments, similar to Definite Clause Grammars (DCG) (Pereira — Warren 1980), are specified using a typed version of Rounds-Kasper logic (Carpenter 1992). We argue for the benefits of a strong and total version of typing in terms of both clarity and efficiency. Finally, we discuss the compilation of grammars into a few efficient low-level instructions for the basic feature structure operations. </abstract>
      <bibkey>carpenter-1993-compiling</bibkey>
    </paper>
    <paper id="6">
      <title>(Pictorial) <fixed-case>LR</fixed-case> Parsing from an Arbitrary Starting Point</title>
      <author><first>Gennaro</first><last>Costagliola</last></author>
      <pages>49-60</pages>
      <url hash="ddc5a86c">1993.iwpt-1.6</url>
      <abstract>In pictorial LR parsing it is always difficult to establish from which point of a picture the parsing process has to start. This paper introduces an algorithm that allows any element of the input to be considered as the starting one and, at the same time, assures that the parsing process is not compromised. The algorithm is first described on string grammars seen as a subclass of pictorial grammars and then adapted to the two-dimensional case. The extensions to generalized LR parsing and pictorial generalized LR parsing are immediate.</abstract>
      <bibkey>costagliola-1993-pictorial</bibkey>
    </paper>
    <paper id="7">
      <title>A New Transformation into Deterministically Parsable Form for Natural Language Grammars</title>
      <author><first>Nigel R.</first><last>Ellis</last></author>
      <author><first>Roberto</first><last>Garigliano</last></author>
      <author><first>Richard G.</first><last>Morgan</last></author>
      <pages>61-72</pages>
      <url hash="94fb29eb">1993.iwpt-1.7</url>
      <abstract>Marcus demonstrated that it was possible to construct a deterministic grammar/interpreter for a subset of natural language [Marcus, 1980]. Although his work with PARSIFAL pioneered the field of deterministic natural language parsing, his method has several drawbacks: • The rules and actions in the grammar / interpreter are so embedded that it is difficult to distinguish between them. • The grammar / interpreter is very difficult to construct (the small grammar shown in [Marcus, 1980] took about four months to construct). • The grammar is very difficult to maintain, as a small change may have several side effects. This paper outlines a set of structure transformations for converting a non-deterministic grammar into deterministic form. The original grammar is written in a context free form; this is then transformed to resolve ambiguities.</abstract>
      <bibkey>ellis-etal-1993-new</bibkey>
    </paper>
    <paper id="8">
      <title>A Principle-based Parser for Foreign Language Training in <fixed-case>G</fixed-case>erman and <fixed-case>A</fixed-case>rabic</title>
      <author><first>Joe</first><last>Garman</last></author>
      <author><first>Jeffery</first><last>Martin</last></author>
      <author><first>Paola</first><last>Merlo</last></author>
      <author><first>Amy</first><last>Weinberg</last></author>
      <pages>73-88</pages>
      <url hash="49efe7c1">1993.iwpt-1.8</url>
      <abstract>In this paper we discuss the design and implementation of a parser for German and Arabic, which is currently being used in a tutoring system for foreign language training. Computer-aided language tutoring is a good application for testing the robustness and flexibility of a parsing system, since the input is usually ungrammatical in some way. Efficiency is also a concern, as tutoring applications typically run on personal computers, with the parser sharing memory with other components of the system. Our system is principle-based, which ensures a compact representation, and improves portability, needed in order to extend the initial design from German to Arabic and (eventually) Spanish. Currently, the parser diagnoses agreement errors, case errors, selection errors, and some word order errors. The parser can handle simple and complex declaratives and questions, topicalisations, verb movement, relative clauses — broad enough coverage to be useful in the design of real exercises and dialogues.</abstract>
      <bibkey>garman-etal-1993-principle</bibkey>
    </paper>
    <paper id="9">
      <title>An Algorithm for the Construction of Dependency Trees</title>
      <author><first>Gerrit F.</first><last>van der Hoeven</last></author>
      <pages>89-100</pages>
      <url hash="ac71a100">1993.iwpt-1.9</url>
      <abstract>A casting system is a dictionary which contains information about words, and relations that can exist between words in sentences. A casting system allows the construction of dependency trees for sentences. They are trees which have words in roles at their nodes, and arcs which correspond to dependency relations. The trees are related to dependency trees in classical dependency syntax, but they are not the same. Formally, casting systems define a family of languages which is a proper subset of the contextfree languages. It is richer than the family of regular languages however. The interest in casting systems arose from an experiment in which it was investigated whether a dictionary of words and word-relations created by a group of experts on the basis of the analysis of a corpus of titles of scientific publications, would suffice to automatically produce reasonable but maybe superficial syntactical analyses of such titles. The results of the experiment were encouraging, but not clear enough to draw firm conclusions. A technical question which arose during the experiment, concerns the choice of a proper algorithm to construct the forest of dependency trees for a given sentence. It turns out that Earley’s well-known algorithm for the parsing of contextfree languages can be adapted to construct dependency trees on the basis of a casting system. The adaptation is of cubic complexity. In fact one can show that contextfree grammars and dictionaries of words and word-relations like casting systems, both belong to a more general family of systems, which associate trees with sequences of tokens. Earley’s algorithm cannot just be adapted to work for casting systems, but it can be generalized to work for the entire large family.</abstract>
      <bibkey>van-der-hoeven-1993-algorithm</bibkey>
    </paper>
    <paper id="10">
      <title>Integration of Morphological and Syntactic Analysis Based on <fixed-case>LR</fixed-case> Parsing Algorithm</title>
      <author><first>Tanaka</first><last>Hozumi</last></author>
      <author><first>Tokunaga</first><last>Takenobu</last></author>
      <author><first>Aizawa</first><last>Michio</last></author>
      <pages>101-110</pages>
      <url hash="a042d551">1993.iwpt-1.10</url>
      <abstract>Morphological analysis of Japanese is very different from that of English, because no spaces are placed between words. The analysis includes segmentation of words. However, ambiguities in segmentation is not always resolved only with morphological information. This paper proposes a method to integrate the morphological and syntactic analysis based on LR parsing algorithm. An LR table derived from grammar rules is modified on the basis of connectabilities between two adjacent words. The modified LR table reflects both the morphological and syntactic constraints. Using the LR table, efficient morphological and syntactic analysis is available.</abstract>
      <bibkey>hozumi-etal-1993-integration</bibkey>
    </paper>
    <paper id="11">
      <title>Structural Disambiguation in <fixed-case>J</fixed-case>apanese by Evaluating Case Structures based on Examples in a Case Frame Dictionary</title>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <author><first>Makoto</first><last>Nagao</last></author>
      <pages>111-122</pages>
      <url hash="031e44d6">1993.iwpt-1.11</url>
      <abstract>A case structure expression is one of the most important forms to represent the <i>meaning</i> of a sentence. Case structure analysis is usually performed by consulting <i>case frame information</i> in verb dictionaries and by selecting a <i>proper case frame</i> for an input sentence. However, this analysis is very difficult because of <i>word sense ambiguity</i> and <i>structural ambiguity</i>. A conventional method for solving these problems is to use the method of <i>selectional restriction</i>, but this method has a drawback in the semantic marker (SM) system – the trade-off between descriptive power and construction cost. This paper describes a method of case structure analysis of Japanese sentences which overcomes the drawback in the SM system, concentrating on the structural disambiguation. This method selects a proper case frame for an input by the similarity measure between the input and typical example sentences of each case frame. When there are two or more possible readings for an input because of structural ambiguity, the best reading will be selected by evaluating case structures in each possible reading by the similarity measure with typical example sentences of case frames. </abstract>
      <bibkey>kurohashi-nagao-1993-structural</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>GLR</fixed-case>* – An Efficient Noise-skipping Parsing Algorithm For Context Free Grammars</title>
      <author><first>Alon</first><last>Lavie</last></author>
      <author><first>Masaru</first><last>Tomita</last></author>
      <pages>123-134</pages>
      <url hash="4dcdd9d6">1993.iwpt-1.12</url>
      <abstract>This paper describes GLR*, a parser that can parse any input sentence by ignoring unrecognizable parts of the sentence. In case the standard parsing procedure fails to parse an input sentence, the parser nondeterministically skips some word(s) in the sentence, and returns the parse with fewest skipped words. Therefore, the parser will return some parse(s) with any input sentence, unless no part of the sentence can be recognized at all. The problem can be defined in the following way: Given a context-free grammar <tex-math>G</tex-math> and a sentence <tex-math>S</tex-math>, find and parse <tex-math>S'</tex-math> – the largest subset of words of <tex-math>S</tex-math>, such that <tex-math>S' \in L(G)</tex-math>. The algorithm described in this paper is a modification of the Generalized LR (Tomita) parsing algorithm [Tomita, 1986] . The parser accommodates the skipping of words by allowing shift operations to be performed from inactive state nodes of the Graph Structured Stack. A heuristic similar to beam search makes the algorithm computationally tractable. There have been several other approaches to the problem of robust parsing, most of which are special purpose algorithms [Carbonell and Hayes, 1984] , [Ward, 1991] and others. Because our approach is a modification to a standard context-free parsing algorithm, all the techniques and grammars developed for the standard parser can be applied as they are. Also, in case the input sentence is by itself grammatical, our parser behaves exactly as the standard GLR parser. The modified parser, GLR*, has been implemented and integrated with the latest version of the Generalized LR Parser/Compiler [Tomita et al , 1988], [Tomita, 1990]. We discuss an application of the GLR* parser to spontaneous speech understanding and present some preliminary tests on the utility of the GLR* parser in such settings. </abstract>
      <bibkey>lavie-tomita-1993-glr</bibkey>
    </paper>
    <paper id="13">
      <title>The Use of Bunch Notation in Parsing Theory</title>
      <author><first>René</first><last>Leermakers</last></author>
      <pages>135-144</pages>
      <url hash="8732621c">1993.iwpt-1.13</url>
      <abstract>Much of mathematics, and therefore much of computer science, is built on the notion of sets. In this paper it is argued that in parsing theory it is sometimes convenient to replace sets by a related notion, <i>bunches</i>. The replacement is not so much a matter of principle, but helps to create a more concise theory. Advantages of the bunch concept are illustrated by using it in descriptions of a formal semantics for context-free grammars and of functional parsing algorithms.</abstract>
      <bibkey>leermakers-1993-use</bibkey>
    </paper>
    <paper id="14">
      <title>Chart Parsing of Attributed Structure-Sharing Flowgraphs with Tie-Point Relationships</title>
      <author><first>Rudi</first><last>Lutz</last></author>
      <pages>145-170</pages>
      <url hash="d6521e08">1993.iwpt-1.14</url>
      <abstract>Many applications make use of diagrams to represent complex objects. In such applications it is often necessary to recognise how some diagram has been pieced together from other diagrams. Examples are electrical circuit analysis, and program understanding in the plan calculus (Rich, 1981). In these applications the recognition process can be formalised as flowgraph parsing, where a flowgraph is a special case of a plex (Feder 1971) . Nodes in a flowgraph are connected to each other via intermediate points known as tie-points. Lutz (1986, 1989) generalised chart parsing of context-free string languages (Thompson – Ritchie, 1984) to context-free flowgraph languages, enabling bottom-up and top-down recognition of flowgraphs. However, there are various features of the plan calculus that complicate this - in particular attributes, structure sharing, and relationships between tie-points. This paper will present a chart parsing algorithm for analysing graphs with all these features, suitable for both program understanding and digital circuit analysis. For a fixed grammar, this algorithm runs in time polynomial in the number of tie-points in the input graph.</abstract>
      <bibkey>lutz-1993-chart</bibkey>
    </paper>
    <paper id="15">
      <title>The Interplay of Syntactic and Semantic Node Labels in Partial Parsing</title>
      <author><first>David D.</first><last>McDonald</last></author>
      <pages>171-186</pages>
      <url hash="686ddb02">1993.iwpt-1.15</url>
      <abstract>Our natural language comprehension system, “Sparser” , uses a semantic grammar in conjunction with a domain model that defines the categories and already-known individuals that can be expected in the sublanguages we are studying, the most significant of which to date has been articles from the Wall Street Journal’s “Who’s News” column. In this paper we describe the systematic use of default syntactic rules in this grammar: an alternative set of labels on consitutents that are used to capture generalities in the semantic interpretation of constructions like the verbal auxiliaries or many adverbials. Syntactic rules form the basis of a set of schemas in a Tree Adjoining Grammar that are used as templates from which to create the primary, semantically labeled rules of the grammar as part of defining the categories in the domain models. This design permits the semantic grammar to be developed on a linguistically principled basis since all the rules must conform to syntactically sound patterns.</abstract>
      <bibkey>mcdonald-1993-interplay</bibkey>
    </paper>
    <paper id="16">
      <title>Increasing the Applicability of <fixed-case>LR</fixed-case> Parsing</title>
      <author><first>Mark-Jan</first><last>Nederhof</last></author>
      <author><first>Janos J.</first><last>Sarbo</last></author>
      <pages>187-202</pages>
      <url hash="96077b28">1993.iwpt-1.16</url>
      <abstract>In this paper we describe a phenomenon present in some context-free grammars, called <i>hidden left recursion</i>. We show that ordinary LR parsing according to hidden left-recursive grammars is not possible and we indicate a range of solutions to this problem. One of these solutions is a new parsing technique, which is a variant of traditional LR parsing. This new parsing technique can be used both with and without lookahead and the nondeterminism can be realized using backtracking or using a graph-structured stack.</abstract>
      <bibkey>nederhof-sarbo-1993-increasing</bibkey>
    </paper>
    <paper id="17">
      <title>Reducing Complexity in A Systemic Parser</title>
      <author><first>Michael</first><last>O’Donnell</last></author>
      <pages>203-218</pages>
      <url hash="9f455493">1993.iwpt-1.17</url>
      <abstract>Parsing with a large systemic grammar brings one face-to-face with the problem of unification with disjunctive descriptions. This paper outlines some techniques which we employed in a systemic parser to reduce the average-case complexity of such unification.</abstract>
      <bibkey>odonnell-1993-reducing</bibkey>
    </paper>
    <paper id="18">
      <title>Generalized <fixed-case>LR</fixed-case> parsing and attribute evaluation</title>
      <author><first>Paul Oude</first><last>Luttighuis</last></author>
      <author><first>Klaas</first><last>Sikkel</last></author>
      <pages>219-234</pages>
      <url hash="d1362d61">1993.iwpt-1.18</url>
      <abstract>This paper presents a thorough discussion of generalized <i>LR</i> parsing with simultaneous attribute evaluation. Nondeterministic parsers and combined parser/evaluators are presented for the <i>LL</i>(0) , <i>LR</i>(0) , and <i>SKLR</i>(0) strategies. <i>SKLR</i>(0) parsing occurs as an intermediate strategy between the first two. Particularly in the context of simultaneous attribute evaluation, generalized <i>SKLR</i>(0) parsing is a sensible alternative for generalized <i>LR</i>(0) parsing. </abstract>
      <bibkey>luttighuis-sikkel-1993-generalized</bibkey>
    </paper>
    <paper id="19">
      <title>A Proof-Theoretic Reconstruction of <fixed-case>HPSG</fixed-case></title>
      <author><first>Stephan</first><last>Raaijmakers</last></author>
      <pages>235-256</pages>
      <url hash="3d1009bf">1993.iwpt-1.19</url>
      <abstract>A reinterpretation of Head-Driven Phrase Structure Grammar (HPSG) in a proof-theoretic context is presented. This approach yields a <i>decision procedure</i> which can be used to establish whether certain strings are generated by a given HPSG grammar. It is possible to view HPSG as a fragment of linear logic (Girard, 1987), subject to partiality and side conditions on inference rules. This relates HPSG to several categorial logics (Morrill, 1990) . Specifically, HPSG signs are mapped onto quantified formulae, which can be interpreted as <i>second-order</i> types given the Curry-Howard isomorphism. The logic behind type inference will, aside from the usual quantifier introduction and elimination rules, consist of a partial logic for the undirected implication connective. It will be shown how this logical perspective can be turned into a parsing perspective. The enterprise takes the standard HPSG of Pollard – Sag (1987) as a starting point, since this version of HPSG is well-documented and has been around long enough to have displayed both merits and shortcomings; the approach is directly applicable to more recent versions of HPSG, however. In order to make the proof-theoretic recasting smooth, standard HPSG is reformulated in a binary format. </abstract>
      <bibkey>raaijmakers-1993-proof</bibkey>
    </paper>
    <paper id="20">
      <title>Stochastic Lexicalized Context-Free Grammar</title>
      <author><first>Yves</first><last>Schabes</last></author>
      <author><first>Richard C.</first><last>Waters</last></author>
      <pages>257-266</pages>
      <url hash="dc1c4fcc">1993.iwpt-1.20</url>
      <abstract>Stochastic lexicalized context-free grammar (SLCFG) is an attractive compromise between the parsing efficiency of stochastic context-free grammar (SCFG) and the lexical sensitivity of stochastic lexicalized tree-adjoining grammar (SLTAG) . SLCFG is a restricted form of SLTAG that can only generate context-free languages and can be parsed in cubic time. However, SLCFG retains the lexical sensitivity of SLTAG and is therefore a much better basis for capturing distributional information about words than SCFG.</abstract>
      <bibkey>schabes-waters-1993-stochastic</bibkey>
    </paper>
    <paper id="21">
      <title>Predictive Head-Corner Chart Parsing</title>
      <author><first>Klaas</first><last>Sikkel</last></author>
      <author><first>Rieks</first><last>op den Akker</last></author>
      <pages>267-276</pages>
      <url hash="b3d86b43">1993.iwpt-1.21</url>
      <abstract>Head-Corner (HC) parsing has come up in computational linguistics a few years ago, motivated by linguistic arguments. This idea is a heuristic, rather than a fail-safe principle, hence it is relevant indeed to consider the worst-case behaviour of the HC parser. We define a novel predictive head-corner chart parser of cubic time complexity. We start with a left-corner (LC) chart parser, which is easier to understand. Subsequently, the LC chart parser is generalized to an HC chart parser. It is briefly sketched how the parser can be enhanced with feature structures.</abstract>
      <bibkey>sikkel-op-den-akker-1993-predictive</bibkey>
    </paper>
    <paper id="22">
      <title>Parsing <fixed-case>E</fixed-case>nglish with a Link Grammar</title>
      <author><first>Daniel D.</first><last>Sleator</last></author>
      <author><first>Davy</first><last>Temperley</last></author>
      <pages>277-292</pages>
      <url hash="03121058">1993.iwpt-1.22</url>
      <abstract>We define a new formal grammatical system called a <i>link grammar</i>. A sequence of words is in the language of a link grammar if there is a way to draw <i>links</i> between words in such a way that (1) the local requirements of each word are satisfied, (2) the links do not cross, and (3) the words form a connected graph. We have encoded English grammar into such a system, and written a program (based on new algorithms) for efficiently parsing with a link grammar. The formalism is lexical and makes no explicit use of constituents and categories. The breadth of English phenomena that our system handles is quite large. A number of sophisticated and new techniques were used to allow efficient parsing of this very complex grammar. Our program is written in C, and the entire system may be obtained via anonymous ftp. Several other researchers have begun to use link grammars in their own research. </abstract>
      <bibkey>sleator-temperley-1993-parsing</bibkey>
    </paper>
    <paper id="23">
      <title>Evaluation of <fixed-case>TTP</fixed-case> Parser: A Preliminary Report</title>
      <author><first>Tomek</first><last>Strzalkowski</last></author>
      <author><first>Peter G. N.</first><last>Scheyen</last></author>
      <pages>293-308</pages>
      <url hash="26fd3cf9">1993.iwpt-1.23</url>
      <abstract>TTP (Tagged Text Parser) is a fast and robust natural language parser specifically designed to process vast quantities of unrestricted text. TTP can analyze written text at the speed of approximately 0.3 sec/sentence, or 73 words per second. An important novel feature of TTP parser is that it is equipped with a skip-and-fit recovery mechanism that allows for fast closing of more difficult sub-constituents after a preset amount of time has elapsed without producing a parse. Although a complete analysis is attempted for each sentence, the parser may occasionally ignore fragments of input to resume “normal” processing after skipping a few words. These fragments are later analyzed separately and attached as incomplete constituents to the main parse tree. TTP has recently been evaluated against several leading parsers. While no formal numbers were released (a formal evaluation is planned later this year), TTP has performed surprisingly well. The main argument of this paper is that TTP can provide a substantial gain in parsing speed giving up relatively little in terms of the quality of output it produces. This property allows TTP to be used effectively in parsing large volumes of text.</abstract>
      <bibkey>strzalkowski-scheyen-1993-evaluation</bibkey>
    </paper>
    <paper id="24">
      <title>Frequency Estimation of Verb Subcategorization Frames Based on Syntactic and Multidimensional Statistical Analysis</title>
      <author><first>Akira</first><last>Ushioda</last></author>
      <author><first>David A.</first><last>Evans</last></author>
      <author><first>Ted</first><last>Gibson</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <pages>309-318</pages>
      <url hash="aebf342e">1993.iwpt-1.24</url>
      <abstract>We describe a mechanism for automatically estimating frequencies of verb subcategorization frames in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a regular grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical method enables the system to learn patterns of errors based on a set of training samples and substantially improves the accuracy of the frequency estimation.</abstract>
      <bibkey>ushioda-etal-1993-frequency</bibkey>
    </paper>
    <paper id="25">
      <title>Handling Syntactic Extra-Grammaticality</title>
      <author><first>Fuliang</first><last>Weng</last></author>
      <pages>319-332</pages>
      <url hash="ead53bc4">1993.iwpt-1.25</url>
      <abstract>This paper reviews and summarizes six different types of extra-grammatical phenomena and their corresponding recovery principles at the syntactic level, and describes some techniques used to deal with four of them completely within an Extended GLR parser (EGLR). Partial solutions to the remaining two by the EGLR parser are also discussed. The EGLR has been implemented.</abstract>
      <bibkey>weng-1993-handling</bibkey>
    </paper>
    <paper id="26">
      <title>Adventures in Multi-dimensional Parsing: Cycles and Disorders</title>
      <author><first>Kent</first><last>Wittenburg</last></author>
      <pages>333-348</pages>
      <url hash="acde380b">1993.iwpt-1.26</url>
      <abstract>Among the proposals for multidimensional grammars is a family of constraint-based grammatical frameworks, including Relational Grammars. In Relational languages, expressions are formally defined as a set of relations whose tuples are taken from an indexed set of symbols. Both bottom-up parsing and Earley-style parsing algorithms have previously been proposed for different classes of Relational languages. The Relational language class for Earley style parsing in Wittenburg (1992a) requires that each relation be a partial order. However, in some real-world domains, the relations do not naturally conform to these restrictions. In this paper I discuss motivations and methods for predictive, Earley-style parsing of multidimensional languages when the relations involved do not necessarily yield an ordering, e.g., when the relations are symmetric and/or nontransitive. The solution involves guaranteeing that a single initial start position for parsing can be associated with any member of the input set. The domains in which these issues are discussed involve incremental parsing in interfaces and off-line verification of multidimensional data.</abstract>
      <bibkey>wittenburg-1993-adventures</bibkey>
    </paper>
    <paper id="27">
      <title>Probabilistic Incremental Parsing in Systemic Functional Grammar</title>
      <author><first>A. Ruvan</first><last>Weerasinghe</last></author>
      <author><first>Robin P.</first><last>Fawcett</last></author>
      <pages>351-367</pages>
      <url hash="ed119b78">1993.iwpt-1.27</url>
      <abstract>In this paper we suggest that a key feature to look for in a successful parser is its ability to lend itself naturally to semantic interpretation. We therefore argue in favour of a parser based on a semantically oriented model of grammar, demonstrating some of the benefits that such a model offers to the parsing process. In particular we adopt a systemic functional syntax as the basis for implementing a chart based probabilistic incremental parser for a non-trivial subset of English.</abstract>
      <bibkey>weerasinghe-fawcett-1993-probabilistic</bibkey>
    </paper>
  </volume>
</collection>
