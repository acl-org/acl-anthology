<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.cogalex">
  <volume id="1" ingest-date="2022-11-21" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop on Cognitive Aspects of the Lexicon</booktitle>
      <editor><first>Michael</first><last>Zock</last></editor>
      <editor><first>Emmanuele</first><last>Chersoni</last></editor>
      <editor><first>Yu-Yin</first><last>Hsu</last></editor>
      <editor><first>Enrico</first><last>Santus</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Taipei, Taiwan</address>
      <month>November</month>
      <year>2022</year>
      <url hash="91074d87">2022.cogalex-1</url>
      <venue>cogalex</venue>
    </meta>
    <frontmatter>
      <url hash="45e0e402">2022.cogalex-1.0</url>
      <bibkey>cogalex-2022-cognitive</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Patterns of Text Readability in Human and Predicted Eye Movements</title>
      <author><first>Nora</first><last>Hollenstein</last></author>
      <author><first>Itziar</first><last>Gonzalez-Dios</last></author>
      <author><first>Lisa</first><last>Beinborn</last></author>
      <author><first>Lena</first><last>Jäger</last></author>
      <pages>1–15</pages>
      <abstract>It has been shown that multilingual transformer models are able to predict human reading behavior when fine-tuned on small amounts of eye tracking data. As the cumulated prediction results do not provide insights into the linguistic cues that the model acquires to predict reading behavior, we conduct a deeper analysis of the predictions from the perspective of readability. We try to disentangle the three-fold relationship between human eye movements, the capability of language models to predict these eye movement patterns, and sentence-level readability measures for English. We compare a range of model configurations to multiple baselines. We show that the models exhibit difficulties with function words and that pre-training only provides limited advantages for linguistic generalization.</abstract>
      <url hash="4ef84556">2022.cogalex-1.1</url>
      <bibkey>hollenstein-etal-2022-patterns</bibkey>
    </paper>
    <paper id="2">
      <title>(In)Alienable Possession in <fixed-case>M</fixed-case>andarin Relative Clauses</title>
      <author><first>Deran</first><last>Kong</last></author>
      <author><first>Yu-Yin</first><last>Hsu</last></author>
      <pages>16–24</pages>
      <abstract>Inalienable possession differs from alienable possession in that, in the former – e.g., kinships and part-whole relations – there is an intrinsic semantic dependency between the possessor and possessum. This paper reports two studies that used acceptability-judgment tasks to investigate whether native Mandarin speakers experienced different levels of interpretational costs while resolving different types of possessive relations, i.e., inalienable possessions (kinship terms and body parts) and alienable ones, expressed within relative clauses. The results show that sentences received higher acceptability ratings when body parts were the possessum as compared to sentences with alienable possessum, indicating that the inherent semantic dependency facilitates the resolution. However, inalienable kinship terms received the lowest acceptability ratings. We argue that this was because the kinship terms, which had the [+human] feature and appeared at the beginning of the experimental sentences, tended to be interpreted as the subject in shallow processing; these features contradicted the semantic-syntactic requirements of the experimental sentences.</abstract>
      <url hash="172effd7">2022.cogalex-1.2</url>
      <bibkey>kong-hsu-2022-alienable</bibkey>
    </paper>
    <paper id="3">
      <title>Do Age of Acquisition and Orthographic Transparency Have the Same Effects in Different Modalities?</title>
      <author><first>Mohammad</first><last>Momenian</last></author>
      <pages>25–30</pages>
      <abstract>This paper is intended to study the effects of age of acquisition (AoA) and orthographic transparency on word retrieval in Persian, which is an understudied language. A naming task (both pictures and words) and a recall task (both pictures and words) were used to explore how lexical retrieval and verbal memory are affected by AoA and transparency. Seventy two native speakers of Persian were recruited to participate in two experiments. The results showed that early acquired words are processed faster than late acquired words only when pictures were used as stimuli. Transparency of the words was not an influential factor. However, in the recall experiment a three-way interaction was observed: early acquired pictures and words were processed faster than late acquired stimuli except the words in the transparent condition. The findings speak to the fact that language-specific properties of languages are very important.</abstract>
      <url hash="67a1b308">2022.cogalex-1.3</url>
      <bibkey>momenian-2022-age</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>CAT</fixed-case> <fixed-case>M</fixed-case>any<fixed-case>N</fixed-case>ames: A New Dataset for Object Naming in <fixed-case>C</fixed-case>atalan</title>
      <author><first>Mar</first><last>Domínguez Orfila</last></author>
      <author><first>Maite</first><last>Melero Nogués</last></author>
      <author><first>Gemma</first><last>Boleda Torrent</last></author>
      <pages>31–36</pages>
      <abstract>Object Naming is an important task within the field of Language and Vision that consists of generating a correct and appropriate name for an object given an image. The ManyNames dataset uses real-world human annotated images with multiple labels, instead of just one. In this work, we describe the adaptation of this dataset (originally in English) to Catalan, by (i) machine-translating the English labels and (ii) collecting human annotations for a subset of the original corpus and comparing both resources. Analyses reveal divergences in the lexical variation of the two sets showing potential problems of directly translated resources, particularly when there is no resource to a proper context, which in this case is conveyed by the image. The analysis also points to the impact of cultural factors in the naming task, which should be accounted for in future cross-lingual naming tasks.</abstract>
      <url hash="02402ad8">2022.cogalex-1.4</url>
      <attachment type="Dataset" hash="8dbd434c">2022.cogalex-1.4.Dataset.tsv</attachment>
      <bibkey>dominguez-orfila-etal-2022-cat</bibkey>
    </paper>
    <paper id="5">
      <title>Finetuning <fixed-case>L</fixed-case>atin <fixed-case>BERT</fixed-case> for Word Sense Disambiguation on the Thesaurus Linguae Latinae</title>
      <author><first>Piroska</first><last>Lendvai</last></author>
      <author><first>Claudia</first><last>Wick</last></author>
      <pages>37–41</pages>
      <abstract>The Thesaurus Linguae Latinae (TLL) is a comprehensive monolingual dictionary that records contextualized meanings and usages of Latin words in antique sources at an unprecedented scale. We created a new dataset based on a subset of sense representations in the TLL, with which we finetuned the Latin-BERT neural language model (Bamman and Burns, 2020) on a supervised Word Sense Disambiguation task. We observe that the contextualized BERT representations finetuned on TLL data score better than static embeddings used in a bidirectional LSTM classifier on the same dataset, and that our per-lemma BERT models achieve higher and more robust performance than reported by Bamman and Burns (2020) based on data from a bilingual Latin dictionary. We demonstrate the differences in sense organizational principles between these two lexical resources, and report about our dataset construction and improved evaluation methodology.</abstract>
      <url hash="41351a8f">2022.cogalex-1.5</url>
      <bibkey>lendvai-wick-2022-finetuning</bibkey>
    </paper>
    <paper id="6">
      <title>Putting <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et’s Dictionary Examples in the Context of Definition Modelling: An Empirical Analysis</title>
      <author><first>Fatemah</first><last>Almeman</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <pages>42–48</pages>
      <abstract>Definition modeling is the task to generate a valid definition for a given input term. This relatively novel task has been approached either with no context (i.e., given a word embedding alone) and, more recently, as word-in-context modeling. Despite their success, most works make little to no distinction between resources and their specific features (e.g., type and style of definitions, or quality of examples) when used for training. Given the high diversity lexicographic resources exhibit in terms of topic coverage, style and formal structure, it is desirable for downstream definition modeling to better understand which of them are better suited for the task. In this paper, we propose an empirical evaluation of the well-known lexical database WordNet, and specifically, its dictionary examples. We evaluate them both directly, by matching them against criteria for good dictionary writing, and indirectly, in the task of definition modeling. Our results suggest that WordNet’s dictionary examples could be improved by extending them in length, and incorporating prototypicality.</abstract>
      <url hash="2ab3bc41">2022.cogalex-1.6</url>
      <bibkey>almeman-espinosa-anke-2022-putting</bibkey>
    </paper>
    <paper id="7">
      <title>Exploring Nominal Coercion in Semantic Spaces with Static and Contextualized Word Embeddings</title>
      <author><first>Chenxin</first><last>Liu</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <pages>49–57</pages>
      <abstract>The distinction between mass nouns and count nouns has a long history in formal semantics, and linguists have been trying to identify the semantic properties defining the two classes. However, they also recognized that both can undergo meaning shifts and be used in contexts of a different type, via nominal coercion. In this paper, we present an approach to measure the meaning shift in count-mass coercion in English that makes use of static and contextualized word embedding distance. Our results show that the coercion shifts are detected only by a small subset of the traditional word embedding models, and that the shifts detected by the contextualized embedding of BERT are more pronounced for mass nouns.</abstract>
      <url hash="fe013845">2022.cogalex-1.7</url>
      <bibkey>liu-chersoni-2022-exploring</bibkey>
    </paper>
    <paper id="8">
      <title>A Frame-Based Model of Inherent Polysemy, Copredication and Argument Coercion</title>
      <author><first>Chen</first><last>Long</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Rainer</first><last>Osswald</last></author>
      <pages>58–67</pages>
      <abstract>The paper presents a frame-based model of inherently polysemous nouns (such as ‘book’, which denotes both a physical object and an informational content) in which the meaning facets are directly accessible via attributes and which also takes into account the semantic relations between the facets. Predication over meaning facets (as in ‘memorize the book’) is then modeled as targeting the value of the corresponding facet attribute while coercion (as in ‘finish the book’) is modeled via specific patterns that enrich the predication. We use a compositional framework whose basic components are lexicalized syntactic trees paired with semantic frames and in which frame unification is triggered by tree composition. The approach is applied to a variety of combinations of predications over meaning facets and coercions.</abstract>
      <url hash="d84b1380">2022.cogalex-1.8</url>
      <bibkey>long-etal-2022-frame</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>VISCOSE</fixed-case> - a Kanji Dictionary Enriched with <fixed-case>VIS</fixed-case>ual, <fixed-case>CO</fixed-case>mpositional, and <fixed-case>SE</fixed-case>mantic Information</title>
      <author><first>Werner</first><last>Winiwarter</last></author>
      <author><first>Bartholomäus</first><last>Wloka</last></author>
      <pages>68–77</pages>
      <abstract>In this paper, we present a novel approach for building kanji dictionaries by enriching the lexical data of 3,500 kanji with images, structural decompositions, and semantically based cross-media mappings from the textual to the visual dimension. Our kanji dictionary is part of a Web-based contextual language learning environment based on augmented browsing technology. We display our multimodal kanji information as kanji cards in the Web browser, offering a versatile representation that can be integrated into other advanced creative language learning applications, such as memorization puzzles, creative storytelling assignments, or educational games.</abstract>
      <url hash="6263ebb7">2022.cogalex-1.9</url>
      <bibkey>winiwarter-wloka-2022-viscose</bibkey>
    </paper>
    <paper id="10">
      <title>Compositionality as an Analogical Process: Introducing <fixed-case>ANNE</fixed-case></title>
      <author><first>Giulia</first><last>Rambelli</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <pages>78–96</pages>
      <abstract>Usage-based constructionist approaches consider language a structured inventory of constructions, form-meaning pairings of different schematicity and complexity, and claim that the more a linguistic pattern is encountered, the more it becomes accessible to speakers. However, when an expression is unavailable, what processes underlie the interpretation? While traditional answers rely on the principle of compositionality, for which the meaning is built word-by-word and incrementally, usage-based theories argue that novel utterances are created based on previously experienced ones through analogy, mapping an existing structural pattern onto a novel instance. Starting from this theoretical perspective, we propose here a computational implementation of these assumptions. As the principle of compositionality has been used to generate distributional representations of phrases, we propose a neural network simulating the construction of phrasal embedding as an analogical process. Our framework, inspired by word2vec and computer vision techniques, was evaluated on tasks of generalization from existing vectors.</abstract>
      <url hash="d8628290">2022.cogalex-1.10</url>
      <bibkey>rambelli-etal-2022-compositionality</bibkey>
    </paper>
  </volume>
</collection>
