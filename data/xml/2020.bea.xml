<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.bea">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</booktitle>
      <editor><first>Jill</first><last>Burstein</last></editor>
      <editor><first>Ekaterina</first><last>Kochmar</last></editor>
      <editor><first>Claudia</first><last>Leacock</last></editor>
      <editor><first>Nitin</first><last>Madnani</last></editor>
      <editor><first>Ildikó</first><last>Pilán</last></editor>
      <editor><first>Helen</first><last>Yannakoudakis</last></editor>
      <editor><first>Torsten</first><last>Zesch</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, WA, USA → Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="24dd231d">2020.bea-1</url>
    </meta>
    <frontmatter>
      <url hash="d9b0a62a">2020.bea-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Linguistic Features for Readability Assessment</title>
      <author><first>Tovly</first><last>Deutsch</last></author>
      <author><first>Masoud</first><last>Jasbi</last></author>
      <author><first>Stuart</first><last>Shieber</last></author>
      <pages>1–17</pages>
      <abstract>Readability assessment aims to automatically classify text by the level appropriate for learning readers. Traditional approaches to this task utilize a variety of linguistically motivated features paired with simple machine learning models. More recent methods have improved performance by discarding these features and utilizing deep learning models. However, it is unknown whether augmenting deep learning models with linguistically motivated features would improve performance further. This paper combines these two approaches with the goal of improving overall model performance and addressing this question. Evaluating on two large readability corpora, we find that, given sufficient training data, augmenting deep learning models with linguistically motivated features does not improve state-of-the-art performance. Our results provide preliminary evidence for the hypothesis that the state-of-the-art deep learning models represent linguistic features of the text related to readability. Future research on the nature of representations formed in these models can shed light on the learned features and their relations to linguistically motivated ones hypothesized in traditional approaches.</abstract>
      <url hash="a5ef0d94">2020.bea-1.1</url>
      <doi>10.18653/v1/2020.bea-1.1</doi>
      <video tag="video" href="http://slideslive.com/38929846"/>
    </paper>
    <paper id="2">
      <title>Using <fixed-case>PRMSE</fixed-case> to evaluate automated scoring systems in the presence of label noise</title>
      <author><first>Anastassia</first><last>Loukina</last></author>
      <author><first>Nitin</first><last>Madnani</last></author>
      <author><first>Aoife</first><last>Cahill</last></author>
      <author><first>Lili</first><last>Yao</last></author>
      <author><first>Matthew S.</first><last>Johnson</last></author>
      <author><first>Brian</first><last>Riordan</last></author>
      <author><first>Daniel F.</first><last>McCaffrey</last></author>
      <pages>18–29</pages>
      <abstract>The effect of noisy labels on the performance of NLP systems has been studied extensively for system training. In this paper, we focus on the effect that noisy labels have on system evaluation. Using automated scoring as an example, we demonstrate that the quality of human ratings used for system evaluation have a substantial impact on traditional performance metrics, making it impossible to compare system evaluations on labels with different quality. We propose that a new metric, PRMSE, developed within the educational measurement community, can help address this issue, and provide practical guidelines on using PRMSE.</abstract>
      <url hash="5713f2d0">2020.bea-1.2</url>
      <doi>10.18653/v1/2020.bea-1.2</doi>
    </paper>
    <paper id="3">
      <title>Multiple Instance Learning for Content Feedback Localization without Annotation</title>
      <author><first>Scott</first><last>Hellman</last></author>
      <author><first>William</first><last>Murray</last></author>
      <author><first>Adam</first><last>Wiemerslage</last></author>
      <author><first>Mark</first><last>Rosenstein</last></author>
      <author><first>Peter</first><last>Foltz</last></author>
      <author><first>Lee</first><last>Becker</last></author>
      <author><first>Marcia</first><last>Derr</last></author>
      <pages>30–40</pages>
      <abstract>Automated Essay Scoring (AES) can be used to automatically generate holistic scores with reliability comparable to human scoring. In addition, AES systems can provide formative feedback to learners, typically at the essay level. In contrast, we are interested in providing feedback specialized to the content of the essay, and specifically for the content areas required by the rubric. A key objective is that the feedback should be localized alongside the relevant essay text. An important step in this process is determining where in the essay the rubric designated points and topics are discussed. A natural approach to this task is to train a classifier using manually annotated data; however, collecting such data is extremely resource intensive. Instead, we propose a method to predict these annotation spans without requiring any labeled annotation data. Our approach is to consider AES as a Multiple Instance Learning (MIL) task. We show that such models can both predict content scores and localize content by leveraging their sentence-level score predictions. This capability arises despite never having access to annotation training data. Implications are discussed for improving formative feedback and explainable AES models.</abstract>
      <url hash="73e198c0">2020.bea-1.3</url>
      <doi>10.18653/v1/2020.bea-1.3</doi>
    </paper>
    <paper id="4">
      <title>Complementary Systems for Off-Topic Spoken Response Detection</title>
      <author><first>Vatsal</first><last>Raina</last></author>
      <author><first>Mark</first><last>Gales</last></author>
      <author><first>Kate</first><last>Knill</last></author>
      <pages>41–51</pages>
      <abstract>Increased demand to learn English for business and education has led to growing interest in automatic spoken language assessment and teaching systems. With this shift to automated approaches it is important that systems reliably assess all aspects of a candidate’s responses. This paper examines one form of spoken language assessment; whether the response from the candidate is relevant to the prompt provided. This will be referred to as off-topic spoken response detection. Two forms of previously proposed approaches are examined in this work: the hierarchical attention-based topic model (HATM); and the similarity grid model (SGM). The work focuses on the scenario when the prompt, and associated responses, have not been seen in the training data, enabling the system to be applied to new test scripts without the need to collect data or retrain the model. To improve the performance of the systems for unseen prompts, data augmentation based on easy data augmentation (EDA) and translation based approaches are applied. Additionally for the HATM, a form of prompt dropout is described. The systems were evaluated on both seen and unseen prompts from Linguaskill Business and General English tests. For unseen data the performance of the HATM was improved using data augmentation, in contrast to the SGM where no gains were obtained. The two approaches were found to be complementary to one another, yielding a combined F0.5 score of 0.814 for off-topic response detection where the prompts have not been seen in training.</abstract>
      <url hash="a7612ffd">2020.bea-1.4</url>
      <doi>10.18653/v1/2020.bea-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>CIMA</fixed-case>: A Large Open Access Dialogue Dataset for Tutoring</title>
      <author><first>Katherine</first><last>Stasaski</last></author>
      <author><first>Kimberly</first><last>Kao</last></author>
      <author><first>Marti A.</first><last>Hearst</last></author>
      <pages>52–64</pages>
      <abstract>One-to-one tutoring is often an effective means to help students learn, and recent experiments with neural conversation systems are promising. However, large open datasets of tutoring conversations are lacking. To remedy this, we propose a novel asynchronous method for collecting tutoring dialogue via crowdworkers that is both amenable to the needs of deep learning algorithms and reflective of pedagogical concerns. In this approach, extended conversations are obtained between crowdworkers role-playing as both students and tutors. The CIMA collection, which we make publicly available, is novel in that students are exposed to overlapping grounded concepts between exercises and multiple relevant tutoring responses are collected for the same input. CIMA contains several compelling properties from an educational perspective: student role-players complete exercises in fewer turns during the course of the conversation and tutor players adopt strategies that conform with some educational conversational norms, such as providing hints versus asking questions in appropriate contexts. The dataset enables a model to be trained to generate the next tutoring utterance in a conversation, conditioned on a provided action strategy.</abstract>
      <url hash="01bc5caa">2020.bea-1.5</url>
      <doi>10.18653/v1/2020.bea-1.5</doi>
    </paper>
    <paper id="6">
      <title>Becoming Linguistically Mature: Modeling <fixed-case>E</fixed-case>nglish and <fixed-case>G</fixed-case>erman Children’s Writing Development Across School Grades</title>
      <author><first>Elma</first><last>Kerz</last></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Daniel</first><last>Wiechmann</last></author>
      <author><first>Marcus</first><last>Ströbel</last></author>
      <pages>65–74</pages>
      <abstract>In this paper we employ a novel approach to advancing our understanding of the development of writing in English and German children across school grades using classification tasks. The data used come from two recently compiled corpora: The English data come from the the GiC corpus (983 school children in second-, sixth-, ninth- and eleventh-grade) and the German data are from the FD-LEX corpus (930 school children in fifth- and ninth-grade). The key to this paper is the combined use of what we refer to as ‘complexity contours’, i.e. series of measurements that capture the progression of linguistic complexity within a text, and Recurrent Neural Network (RNN) classifiers that adequately capture the sequential information in those contours. Our experiments demonstrate that RNN classifiers trained on complexity contours achieve higher classification accuracy than one trained on text-average complexity scores. In a second step, we determine the relative importance of the features from four distinct categories through a Sensitivity-Based Pruning approach.</abstract>
      <url hash="b544b2f5">2020.bea-1.6</url>
      <doi>10.18653/v1/2020.bea-1.6</doi>
      <attachment type="Dataset" hash="6275ac98">2020.bea-1.6.Dataset.pdf</attachment>
    </paper>
    <paper id="7">
      <title>Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing</title>
      <author><first>Tazin</first><last>Afrin</last></author>
      <author><first>Elaine Lin</first><last>Wang</last></author>
      <author><first>Diane</first><last>Litman</last></author>
      <author><first>Lindsay Clare</first><last>Matsumura</last></author>
      <author><first>Richard</first><last>Correnti</last></author>
      <pages>75–84</pages>
      <abstract>Automated writing evaluation systems can improve students’ writing insofar as students attend to the feedback provided and revise their essay drafts in ways aligned with such feedback. Existing research on revision of argumentative writing in such systems, however, has focused on the types of revisions students make (e.g., surface vs. content) rather than the extent to which revisions actually respond to the feedback provided and improve the essay. We introduce an annotation scheme to capture the nature of sentence-level revisions of evidence use and reasoning (the ‘RER’ scheme) and apply it to 5th- and 6th-grade students’ argumentative essays. We show that reliable manual annotation can be achieved and that revision annotations correlate with a holistic assessment of essay improvement in line with the feedback provided. Furthermore, we explore the feasibility of automatically classifying revisions according to our scheme.</abstract>
      <url hash="46115fdb">2020.bea-1.7</url>
      <doi>10.18653/v1/2020.bea-1.7</doi>
      <video tag="video" href="http://slideslive.com/38929856"/>
    </paper>
    <paper id="8">
      <title>Can Neural Networks Automatically Score Essay Traits?</title>
      <author><first>Sandeep</first><last>Mathias</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>85–91</pages>
      <abstract>Essay traits are attributes of an essay that can help explain how well written (or badly written) the essay is. Examples of traits include Content, Organization, Language, Sentence Fluency, Word Choice, etc. A lot of research in the last decade has dealt with automatic holistic essay scoring - where a machine rates an essay and gives a score for the essay. However, writers need feedback, especially if they want to improve their writing - which is why trait-scoring is important. In this paper, we show how a deep-learning based system can outperform feature-based machine learning systems, as well as a string kernel system in scoring essay traits.</abstract>
      <url hash="7665989d">2020.bea-1.8</url>
      <doi>10.18653/v1/2020.bea-1.8</doi>
    </paper>
    <paper id="9">
      <title>Tracking the Evolution of Written Language Competence in <fixed-case>L</fixed-case>2 <fixed-case>S</fixed-case>panish Learners</title>
      <author><first>Alessio</first><last>Miaschi</last></author>
      <author><first>Sam</first><last>Davidson</last></author>
      <author><first>Dominique</first><last>Brunato</last></author>
      <author><first>Felice</first><last>Dell’Orletta</last></author>
      <author><first>Kenji</first><last>Sagae</last></author>
      <author><first>Claudia Helena</first><last>Sanchez-Gutierrez</last></author>
      <author><first>Giulia</first><last>Venturi</last></author>
      <pages>92–101</pages>
      <abstract>In this paper we present an NLP-based approach for tracking the evolution of written language competence in L2 Spanish learners using a wide range of linguistic features automatically extracted from students’ written productions. Beyond reporting classification results for different scenarios, we explore the connection between the most predictive features and the teaching curriculum, finding that our set of linguistic features often reflect the explicit instructions that students receive during each course.</abstract>
      <url hash="814d4afc">2020.bea-1.9</url>
      <doi>10.18653/v1/2020.bea-1.9</doi>
    </paper>
    <paper id="10">
      <title>Distractor Analysis and Selection for Multiple-Choice Cloze Questions for Second-Language Learners</title>
      <author><first>Lingyu</first><last>Gao</last></author>
      <author><first>Kevin</first><last>Gimpel</last></author>
      <author><first>Arnar</first><last>Jensson</last></author>
      <pages>102–114</pages>
      <abstract>We consider the problem of automatically suggesting distractors for multiple-choice cloze questions designed for second-language learners. We describe the creation of a dataset including collecting manual annotations for distractor selection. We assess the relationship between the choices of the annotators and features based on distractors and the correct answers, both with and without the surrounding passage context in the cloze questions. Simple features of the distractor and correct answer correlate with the annotations, though we find substantial benefit to additionally using large-scale pretrained models to measure the fit of the distractor in the context. Based on these analyses, we propose and train models to automatically select distractors, and measure the importance of model components quantitatively.</abstract>
      <url hash="a05889be">2020.bea-1.10</url>
      <doi>10.18653/v1/2020.bea-1.10</doi>
    </paper>
    <paper id="11">
      <title>Assisting Undergraduate Students in Writing <fixed-case>S</fixed-case>panish Methodology Sections</title>
      <author><first>Samuel</first><last>González-López</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <author><first>Aurelio</first><last>Lopez-Lopez</last></author>
      <pages>115–123</pages>
      <abstract>In undergraduate theses, a good methodology section should describe the series of steps that were followed in performing the research. To assist students in this task, we develop machine-learning models and an app that uses them to provide feedback while students write. We construct an annotated corpus that identifies sentences representing methodological steps and labels when a methodology contains a logical sequence of such steps. We train machine-learning models based on language modeling and lexical features that can identify sentences representing methodological steps with 0.939 f-measure, and identify methodology sections containing a logical sequence of steps with an accuracy of 87%. We incorporate these models into a Microsoft Office Add-in, and show that students who improved their methodologies according to the model feedback received better grades on their methodologies.</abstract>
      <url hash="4bdd4f55">2020.bea-1.11</url>
      <doi>10.18653/v1/2020.bea-1.11</doi>
      <video tag="video" href="http://slideslive.com/38929853"/>
    </paper>
    <paper id="12">
      <title>Applications of Natural Language Processing in Bilingual Language Teaching: An <fixed-case>I</fixed-case>ndonesian-<fixed-case>E</fixed-case>nglish Case Study</title>
      <author><first>Zara</first><last>Maxwelll-Smith</last></author>
      <author><first>Simón</first><last>González Ochoa</last></author>
      <author><first>Ben</first><last>Foley</last></author>
      <author><first>Hanna</first><last>Suominen</last></author>
      <pages>124–134</pages>
      <abstract>Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this data so rich and problematic to classify. In this paper, we set out methodological considerations of using automated speech recognition to build a corpus of teacher speech in an Indonesian language classroom. Our preliminary results (64% word error rate) suggest these tools have the potential to speed data collection in this context. We provide practical examples of our data structure, details of our piloted computer-assisted processes, and fine-grained error analysis. Our study is informed and directed by genuine research questions and discussion in both the education and computational linguistics fields. We highlight some of the benefits and risks of using these emerging technologies to analyze the complex work of language teachers and in education more generally.</abstract>
      <url hash="dd31f883">2020.bea-1.12</url>
      <doi>10.18653/v1/2020.bea-1.12</doi>
    </paper>
    <paper id="13">
      <title>An empirical investigation of neural methods for content scoring of science explanations</title>
      <author><first>Brian</first><last>Riordan</last></author>
      <author><first>Sarah</first><last>Bichler</last></author>
      <author><first>Allison</first><last>Bradford</last></author>
      <author><first>Jennifer</first><last>King Chen</last></author>
      <author><first>Korah</first><last>Wiley</last></author>
      <author><first>Libby</first><last>Gerard</last></author>
      <author><first>Marcia</first><last>C. Linn</last></author>
      <pages>135–144</pages>
      <abstract>With the widespread adoption of the Next Generation Science Standards (NGSS), science teachers and online learning environments face the challenge of evaluating students’ integration of different dimensions of science learning. Recent advances in representation learning in natural language processing have proven effective across many natural language processing tasks, but a rigorous evaluation of the relative merits of these methods for scoring complex constructed response formative assessments has not previously been carried out. We present a detailed empirical investigation of feature-based, recurrent neural network, and pre-trained transformer models on scoring content in real-world formative assessment data. We demonstrate that recent neural methods can rival or exceed the performance of feature-based methods. We also provide evidence that different classes of neural models take advantage of different learning cues, and pre-trained transformer models may be more robust to spurious, dataset-specific learning cues, better reflecting scoring rubrics.</abstract>
      <url hash="c81cf5e9">2020.bea-1.13</url>
      <doi>10.18653/v1/2020.bea-1.13</doi>
    </paper>
    <paper id="14">
      <title>An Exploratory Study of Argumentative Writing by Young Students: A transformer-based Approach</title>
      <author><first>Debanjan</first><last>Ghosh</last></author>
      <author><first>Beata</first><last>Beigman Klebanov</last></author>
      <author><first>Yi</first><last>Song</last></author>
      <pages>145–150</pages>
      <abstract>We present a computational exploration of argument critique writing by young students. Middle school students were asked to criticize an argument presented in the prompt, focusing on identifying and explaining the reasoning flaws. This task resembles an established college-level argument critique task. Lexical and discourse features that utilize detailed domain knowledge to identify critiques exist for the college task but do not perform well on the young students’ data. Instead, transformer-based architecture (e.g., BERT) fine-tuned on a large corpus of critique essays from the college task performs much better (over 20% improvement in F1 score). Analysis of the performance of various configurations of the system suggests that while children’s writing does not exhibit the standard discourse structure of an argumentative essay, it does share basic local sequential structures with the more mature writers.</abstract>
      <url hash="eb5f70a9">2020.bea-1.14</url>
      <doi>10.18653/v1/2020.bea-1.14</doi>
    </paper>
    <paper id="15">
      <title>Should You Fine-Tune <fixed-case>BERT</fixed-case> for Automated Essay Scoring?</title>
      <author><first>Elijah</first><last>Mayfield</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>151–162</pages>
      <abstract>Most natural language processing research now recommends large Transformer-based models with fine-tuning for supervised classification tasks; older strategies like bag-of-words features and linear models have fallen out of favor. Here we investigate whether, in automated essay scoring (AES) research, deep neural models are an appropriate technological choice. We find that fine-tuning BERT produces similar performance to classical models at significant additional cost. We argue that while state-of-the-art strategies do match existing best results, they come with opportunity costs in computational resources. We conclude with a review of promising areas for research on student essays where the unique characteristics of Transformers may provide benefits over classical methods to justify the costs.</abstract>
      <url hash="e55e6ec6">2020.bea-1.15</url>
      <doi>10.18653/v1/2020.bea-1.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>GECT</fixed-case>o<fixed-case>R</fixed-case> – Grammatical Error Correction: Tag, Not Rewrite</title>
      <author><first>Kostiantyn</first><last>Omelianchuk</last></author>
      <author><first>Vitaliy</first><last>Atrasevych</last></author>
      <author><first>Artem</first><last>Chernodub</last></author>
      <author><first>Oleksandr</first><last>Skurzhanskyi</last></author>
      <pages>163–170</pages>
      <abstract>In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model/ensemble GEC tagger achieves an F_0.5 of 65.3/66.5 on CONLL-2014 (test) and F_0.5 of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.</abstract>
      <url hash="7ed8a32f">2020.bea-1.16</url>
      <doi>10.18653/v1/2020.bea-1.16</doi>
    </paper>
    <paper id="17">
      <title>Interpreting Neural <fixed-case>CWI</fixed-case> Classifiers’ Weights as Vocabulary Size</title>
      <author><first>Yo</first><last>Ehara</last></author>
      <pages>171–176</pages>
      <abstract>Complex Word Identification (CWI) is a task for the identification of words that are challenging for second-language learners to read. Even though the use of neural classifiers is now common in CWI, the interpretation of their parameters remains difficult. This paper analyzes neural CWI classifiers and shows that some of their parameters can be interpreted as vocabulary size. We present a novel formalization of vocabulary size measurement methods that are practiced in the applied linguistics field as a kind of neural classifier. We also contribute to building a novel dataset for validating vocabulary testing and readability via crowdsourcing.</abstract>
      <url hash="f7390621">2020.bea-1.17</url>
      <doi>10.18653/v1/2020.bea-1.17</doi>
      <attachment type="Dataset" hash="de00ecff">2020.bea-1.17.Dataset.zip</attachment>
    </paper>
    <paper id="18">
      <title>Automated Scoring of Clinical Expressive Language Evaluation Tasks</title>
      <author><first>Yiyi</first><last>Wang</last></author>
      <author><first>Emily</first><last>Prud’hommeaux</last></author>
      <author><first>Meysam</first><last>Asgari</last></author>
      <author><first>Jill</first><last>Dolata</last></author>
      <pages>177–185</pages>
      <abstract>Many clinical assessment instruments used to diagnose language impairments in children include a task in which the subject must formulate a sentence to describe an image using a specific target word. Because producing sentences in this way requires the speaker to integrate syntactic and semantic knowledge in a complex manner, responses are typically evaluated on several different dimensions of appropriateness yielding a single composite score for each response. In this paper, we present a dataset consisting of non-clinically elicited responses for three related sentence formulation tasks, and we propose an approach for automatically evaluating their appropriateness. We use neural machine translation to generate correct-incorrect sentence pairs in order to create synthetic data to increase the amount and diversity of training data for our scoring model. Our scoring model uses transfer learning to facilitate automatic sentence appropriateness evaluation. We further compare custom word embeddings with pre-trained contextualized embeddings serving as features for our scoring model. We find that transfer learning improves scoring accuracy, particularly when using pretrained contextualized embeddings.</abstract>
      <url hash="cd5965f2">2020.bea-1.18</url>
      <doi>10.18653/v1/2020.bea-1.18</doi>
    </paper>
    <paper id="19">
      <title>Context-based Automated Scoring of Complex Mathematical Responses</title>
      <author><first>Aoife</first><last>Cahill</last></author>
      <author><first>James H</first><last>Fife</last></author>
      <author><first>Brian</first><last>Riordan</last></author>
      <author><first>Avijit</first><last>Vajpayee</last></author>
      <author><first>Dmytro</first><last>Galochkin</last></author>
      <pages>186–192</pages>
      <abstract>The tasks of automatically scoring either textual or algebraic responses to mathematical questions have both been well-studied, albeit separately. In this paper we propose a method for automatically scoring responses that contain both text and algebraic expressions. Our method not only achieves high agreement with human raters, but also links explicitly to the scoring rubric – essentially providing explainable models and a way to potentially provide feedback to students in the future.</abstract>
      <url hash="99cdaf94">2020.bea-1.19</url>
      <doi>10.18653/v1/2020.bea-1.19</doi>
    </paper>
    <paper id="20">
      <title>Predicting the Difficulty and Response Time of Multiple Choice Questions Using Transfer Learning</title>
      <author><first>Kang</first><last>Xue</last></author>
      <author><first>Victoria</first><last>Yaneva</last></author>
      <author><first>Christopher</first><last>Runyon</last></author>
      <author><first>Peter</first><last>Baldwin</last></author>
      <pages>193–197</pages>
      <abstract>This paper investigates whether transfer learning can improve the prediction of the difficulty and response time parameters for 18,000 multiple-choice questions from a high-stakes medical exam. The type the signal that best predicts difficulty and response time is also explored, both in terms of representation abstraction and item component used as input (e.g., whole item, answer options only, etc.). The results indicate that, for our sample, transfer learning can improve the prediction of item difficulty when response time is used as an auxiliary task but not the other way around. In addition, difficulty was best predicted using signal from the item stem (the description of the clinical case), while all parts of the item were important for predicting the response time.</abstract>
      <url hash="1f9e95f0">2020.bea-1.20</url>
      <doi>10.18653/v1/2020.bea-1.20</doi>
    </paper>
    <paper id="21">
      <title>A Comparative Study of Synthetic Data Generation Methods for Grammatical Error Correction</title>
      <author><first>Max</first><last>White</last></author>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <pages>198–208</pages>
      <abstract>Grammatical Error Correction (GEC) is concerned with correcting grammatical errors in written text. Current GEC systems, namely those leveraging statistical and neural machine translation, require large quantities of annotated training data, which can be expensive or impractical to obtain. This research compares techniques for generating synthetic data utilized by the two highest scoring submissions to the restricted and low-resource tracks in the BEA-2019 Shared Task on Grammatical Error Correction.</abstract>
      <url hash="6dfffa87">2020.bea-1.21</url>
      <doi>10.18653/v1/2020.bea-1.21</doi>
    </paper>
  </volume>
</collection>
