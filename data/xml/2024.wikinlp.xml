<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.wikinlp">
  <volume id="1" ingest-date="2024-11-05" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First Workshop on Advancing Natural Language Processing for Wikipedia</booktitle>
      <editor><first>Lucie</first><last>Lucie-Aimée</last></editor>
      <editor><first>Angela</first><last>Fan</last></editor>
      <editor><first>Tajuddeen</first><last>Gwadabe</last></editor>
      <editor><first>Isaac</first><last>Johnson</last></editor>
      <editor><first>Fabio</first><last>Petroni</last></editor>
      <editor><first>Daniel</first><last>van Strien</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Miami, Florida, USA</address>
      <month>November</month>
      <year>2024</year>
      <url hash="a0cca7de">2024.wikinlp-1</url>
      <venue>wikinlp</venue>
    </meta>
    <frontmatter>
      <url hash="818a06f1">2024.wikinlp-1.0</url>
      <bibkey>wikinlp-2024-1</bibkey>
    </frontmatter>
    <paper id="3">
      <title><fixed-case>B</fixed-case>ord<fixed-case>IR</fixed-case>lines: A Dataset for Evaluating Cross-lingual Retrieval Augmented Generation</title>
      <author><first>Bryan</first><last>Li</last></author>
      <author><first>Samar</first><last>Haider</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Fiona</first><last>Luo</last></author>
      <author><first>Adwait</first><last>Agashe</last></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>Allen Institute for Artificial Intelligence and University of Pennsylvania</affiliation></author>
      <pages>1-13</pages>
      <abstract>Large language models excel at creative generation but continue to struggle with the issues of hallucination and bias. While retrieval-augmented generation (RAG) provides a framework for grounding LLMs’ responses in accurate and up-to-date information, it still raises the question of bias: which sources should be selected for inclusion in the context? And how should their importance be weighted? In this paper, we study the challenge of cross-lingual RAG and present a dataset to investigate the robustness of existing systems at answering queries about geopolitical disputes, which exist at the intersection of linguistic, cultural, and political boundaries. Our dataset is sourced from Wikipedia pages containing information relevant to the given queries and we investigate the impact of including additional context, as well as the composition of this context in terms of language and source, on an LLM’s response. Our results show that existing RAG systems continue to be challenged by cross-lingual use cases and suffer from a lack of consistency when they are provided with competing information in multiple languages. We present case studies to illustrate these issues and outline steps for future research to address these challenges.</abstract>
      <url hash="8bfe1404">2024.wikinlp-1.3</url>
      <bibkey>li-etal-2024-bordirlines</bibkey>
    </paper>
    <paper id="7">
      <title>Multi-Label Field Classification for Scientific Documents using Expert and Crowd-sourced Knowledge</title>
      <author><first>Rebecca</first><last>Gelles</last><affiliation>Georgetown University</affiliation></author>
      <author><first>James</first><last>Dunham</last><affiliation>Georgetown University</affiliation></author>
      <pages>14-20</pages>
      <abstract>Taxonomies of scientific research seek to describe complex domains of activity that are overlapping and dynamic. We address this challenge by combining knowledge curated by the Wikipedia community with the input of subject-matter experts to identify, define, and validate a system of 1,110 granular fields of study for use in multi-label classification of scientific publications. The result is capable of categorizing research across subfields of artificial intelligence, computer security, semiconductors, genetics, virology, immunology, neuroscience, biotechnology, and bioinformatics. We then develop and evaluate a solution for zero-shot classification of publications in terms of these fields.</abstract>
      <url hash="10fadbcd">2024.wikinlp-1.7</url>
      <bibkey>gelles-dunham-2024-multi</bibkey>
    </paper>
    <paper id="8">
      <title>Uncovering Differences in Persuasive Language in <fixed-case>R</fixed-case>ussian versus <fixed-case>E</fixed-case>nglish <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Bryan</first><last>Li</last></author>
      <author><first>Aleksey</first><last>Panasyuk</last><affiliation>Air Force Research Laboratory</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>Allen Institute for Artificial Intelligence and University of Pennsylvania</affiliation></author>
      <pages>21-35</pages>
      <abstract>We study how differences in persuasive language across Wikipedia articles, written in either English and Russian, can uncover each culture’s distinct perspective on different subjects. We develop a large language model (LLM) powered system to identify instances of persuasive language in multilingual texts. Instead of directly prompting LLMs to detect persuasion, which is subjective and difficult, we propose to reframe the task to instead ask high-level questions (HLQs) which capture different persuasive aspects. Importantly, these HLQs are authored by LLMs themselves. LLMs over-generate a large set of HLQs, which are subsequently filtered to a small set aligned with human labels for the original task. We then apply our approach to a large-scale, bilingual dataset of Wikipedia articles (88K total), using a two-stage identify-then-extract prompting strategy to find instances of persuasion. We quantify the amount of persuasion per article, and explore the differences in persuasion through several experiments on the paired articles. Notably, we generate rankings of articles by persuasion in both languages. These rankings match our intuitions on the culturally-salient subjects; Russian Wikipedia highlights subjects on Ukraine, while English Wikipedia highlights the Middle East. Grouping subjects into larger topics, we find politically-related events contain more persuasion than others. We further demonstrate that HLQs obtain similar performance when posed in either English or Russian. Our methodology enables cross-lingual, cross-cultural understanding at scale, and we release our code, prompts, and data.</abstract>
      <url hash="4125e112">2024.wikinlp-1.8</url>
      <bibkey>li-etal-2024-uncovering</bibkey>
    </paper>
    <paper id="9">
      <title>Retrieval Evaluation for Long-Form and Knowledge-Intensive Image–Text Article Composition</title>
      <author><first>Jheng-Hong</first><last>Yang</last></author>
      <author><first>Carlos</first><last>Lassance</last><affiliation>NA</affiliation></author>
      <author><first>Rafael S.</first><last>Rezende</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>Krishna</first><last>Srinivasan</last><affiliation>Research, Google</affiliation></author>
      <author><first>Stéphane</first><last>Clinchant</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>36-45</pages>
      <abstract>This paper examines the integration of images into Wikipedia articles by evaluating image–text retrieval tasks in multimedia content creation, focusing on developing retrieval-augmented tools to enhance the creation of high-quality multimedia articles. Despite ongoing research, the interplay between text and visuals, such as photos and diagrams, remains underexplored, limiting support for real-world applications. We introduce AToMiC, a dataset for long-form, knowledge-intensive image–text retrieval, detailing its task design, evaluation protocols, and relevance criteria.Our findings show that a hybrid approach combining a sparse retriever with a dense retriever achieves satisfactory effectiveness, with nDCG@10 scores around 0.4 for Image Suggestion and Image Promotion tasks, providing insights into the challenges of retrieval evaluation in an image–text interleaved article composition context.The AToMiC dataset is available at https://github.com/TREC-AToMiC/AToMiC.</abstract>
      <url hash="1c677e04">2024.wikinlp-1.9</url>
      <bibkey>yang-etal-2024-retrieval</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>W</fixed-case>iki<fixed-case>B</fixed-case>ias as an Extrapolation Corpus for Bias Detection</title>
      <author><first>K.</first><last>Salas-Jimenez</last><affiliation>Universidad Nacional Autónoma de México</affiliation></author>
      <author><first>Francisco Fernando</first><last>Lopez-Ponce</last></author>
      <author><first>Sergio-Luis</first><last>Ojeda-Trueba</last></author>
      <author><first>Gemma</first><last>Bel-Enguix</last><affiliation>Universidad Nacional Autónoma de México</affiliation></author>
      <pages>46-52</pages>
      <abstract>This paper explores whether it is possible to train a machine learning model using Wikipedia data to detect subjectivity in sentences and generalize effectively to other domains. To achieve this, we performed experiments with the WikiBias corpus, the BABE corpus, and the CheckThat! Dataset. Various classical models for ML were tested, including Logistic Regression, SVC, and SVR, including characteristics such as Sentence Transformers similarity, probabilistic sentiment measures, and biased lexicons. Pre-trained models like DistilRoBERTa, as well as large language models like Gemma and GPT-4, were also tested for the same classification task.</abstract>
      <url hash="a824a4e0">2024.wikinlp-1.10</url>
      <bibkey>salas-jimenez-etal-2024-wikibias</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>HOAXPEDIA</fixed-case>: A Unified <fixed-case>W</fixed-case>ikipedia Hoax Articles Dataset</title>
      <author><first>Hsuvas</first><last>Borkakoty</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Luis</first><last>Espinosa-Anke</last><affiliation>Cardiff University and AMPLYFI</affiliation></author>
      <pages>53-66</pages>
      <abstract>Hoaxes are a recognised form of disinformation created deliberately, with potential serious implications in the credibility of reference knowledge resources such as Wikipedia. What makes detecting Wikipedia hoaxes hard is that they often are written according to the official style guidelines. In this work, we first provide a systematic analysis of similarities and discrepancies between legitimate and hoax Wikipedia articles, and introduce HOAXPEDIA, a collection of 311 hoax articles (from existing literature and official Wikipedia lists), together with semantically similar legitimate articles, which together form a binary text classification dataset aimed at fostering research in automated hoax detection. In this paper, We report results after analyzing several language models, hoax-to-legit ratios, and the amount of text classifiers are exposed to (full article vs the article’s definition alone). Our results suggest that detecting deceitful content in Wikipedia based on content alone is hard but feasible, and complement our analysis with a study on the differences in distributions in edit histories, and find that looking at this feature yields better classification results than context.</abstract>
      <url hash="bc54668c">2024.wikinlp-1.11</url>
      <bibkey>borkakoty-espinosa-anke-2024-hoaxpedia</bibkey>
    </paper>
    <paper id="12">
      <title>The Rise of <fixed-case>AI</fixed-case>-Generated Content in <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Creston</first><last>Brooks</last></author>
      <author><first>Samuel</first><last>Eggert</last></author>
      <author><first>Denis</first><last>Peskoff</last></author>
      <pages>67-79</pages>
      <abstract>The rise of AI-generated content in popular information sources raises significant concerns about accountability, accuracy, and bias amplification. Beyond directly impacting consumers, the widespread presence of this content poses questions for the long-term viability of training language models on vast internet sweeps. We use GPTZero, a proprietary AI detector, and Binoculars, an open-source alternative, to establish lower bounds on the presence of AI-generated content in recently created Wikipedia pages. Both detectors reveal a marked increase in AI-generated content in recent pages compared to those from before the release of GPT-3.5. With thresholds calibrated to achieve a 1% false positive rate on pre-GPT-3.5 articles, detectors flag over 5% of newly created English Wikipedia articles as AI-generated, with lower percentages for German, French, and Italian articles. Flagged Wikipedia articles are typically of lower quality and are often self-promotional or partial towards a specific viewpoint on controversial topics.</abstract>
      <url hash="9024d660">2024.wikinlp-1.12</url>
      <bibkey>brooks-etal-2024-rise</bibkey>
    </paper>
    <paper id="13">
      <title>Embedded Topic Models Enhanced by Wikification</title>
      <author><first>Takashi</first><last>Shibuya</last><affiliation>University of Tsukuba and Sony AI</affiliation></author>
      <author><first>Takehito</first><last>Utsuro</last><affiliation>University of Tsukuba</affiliation></author>
      <pages>80-90</pages>
      <abstract>Topic modeling analyzes a collection of documents to learn meaningful patterns of words.However, previous topic models consider only the spelling of words and do not take into consideration the polysemy of words.In this study, we incorporate the Wikipedia knowledge into a neural topic model to make it aware of named entities.We evaluate our method on two datasets, 1) news articles of New York Times and 2) the AIDA-CoNLL dataset.Our experiments show that our method improves the performance of neural topic models in generalizability.Moreover, we analyze frequent words in each topic and the temporal dependencies between topics to demonstrate that our entity-aware topic models can capture the time-series development of topics well.</abstract>
      <url hash="24c8dea4">2024.wikinlp-1.13</url>
      <bibkey>shibuya-utsuro-2024-embedded</bibkey>
    </paper>
    <paper id="14">
      <title>Wikimedia data for <fixed-case>AI</fixed-case>: a review of Wikimedia datasets for <fixed-case>NLP</fixed-case> tasks and <fixed-case>AI</fixed-case>-assisted editing</title>
      <author><first>Isaac</first><last>Johnson</last><affiliation>Wikimedia</affiliation></author>
      <author><first>Lucie-Aimée</first><last>Kaffee</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Miriam</first><last>Redi</last><affiliation>Wikimedia Foundation</affiliation></author>
      <pages>91-101</pages>
      <abstract>Wikimedia content is used extensively by the AI community and within the language modeling community in particular. In this paper, we provide a review of the different ways in which Wikimedia data is curated to use in NLP tasks across pre-training, post-training, and model evaluations. We point to opportunities for greater use of Wikimedia content but also identify ways in which the language modeling community could better center the needs of Wikimedia editors. In particular, we call for incorporating additional sources of Wikimedia data, a greater focus on benchmarks for LLMs that encode Wikimedia principles, and greater multilingualism in Wikimedia-derived datasets.</abstract>
      <url hash="417e2650">2024.wikinlp-1.14</url>
      <bibkey>johnson-etal-2024-wikimedia</bibkey>
    </paper>
    <paper id="16">
      <title>Blocks Architecture (<fixed-case>B</fixed-case>lo<fixed-case>A</fixed-case>rk): Efficient, Cost-Effective, and Incremental Dataset Architecture for <fixed-case>W</fixed-case>ikipedia Revision History</title>
      <author><first>Lingxi</first><last>Li</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Zonghai</first><last>Yao</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Sunjae</first><last>Kwon</last></author>
      <author><first>Hong</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>102-111</pages>
      <abstract>Wikipedia (Wiki) is one of the most widely used and publicly available resources for natural language processing (NLP) applications. Wikipedia Revision History (WikiRevHist) shows the order in which edits were made to any Wiki page since its first modification. While the most up-to-date Wiki has been widely used as a training source, WikiRevHist can also be valuable resources for NLP applications. However, there are insufficient tools available to process WikiRevHist without having substantial computing resources, making additional customization, and spending extra time adapting others’ works. Therefore, we report Blocks Architecture (BloArk), an efficiency-focused data processing architecture that reduces running time, computing resource requirements, and repeated works in processing WikiRevHist dataset. BloArk consists of three parts in its infrastructure: blocks, segments, and warehouses. On top of that, we build the core data processing pipeline: builder and modifier. The BloArk builder transforms the original WikiRevHist dataset from XML syntax into JSON Lines (JSONL) format for improving the concurrent and storage efficiency. The BloArk modifier takes previously-built warehouses to operate incremental modifications for improving the utilization of existing databases and reducing the cost of reusing others’ works. In the end, BloArk can scale up easily in both processing Wikipedia Revision History and incrementally modifying existing dataset for downstream NLP use cases. The source code, documentations, and example usages are publicly available online and open-sourced under GPL-2.0 license.</abstract>
      <url hash="f64310eb">2024.wikinlp-1.16</url>
      <bibkey>li-etal-2024-blocks</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>ARMADA</fixed-case>: Attribute-Based Multimodal Data Augmentation</title>
      <author><first>Xiaomeng</first><last>Jin</last></author>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Yu</first><last>Zhou</last><affiliation>Stanford University</affiliation></author>
      <author><first>Kuan-Hao</first><last>Huang</last><affiliation>Texas A&amp;M University</affiliation></author>
      <author><first>Te-Lin</first><last>Wu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>112-125</pages>
      <abstract>In Multimodal Language Models (MLMs), the cost of manually annotating high-quality image-text pair data for fine-tuning and alignment is extremely high. While existing multimodal data augmentation frameworks propose ways to augment image-text pairs, they either suffer from semantic inconsistency between texts and images, or generate unrealistic images, causing knowledge gap with real world examples. To address these issues, we propose Attribute-based Multimodal Data Augmentation (ARMADA), a novel multimodal data augmentation method via knowledge-guided manipulation of visual attributes of the mentioned entities. Specifically, we extract entities and their visual attributes from the original text data, then search for alternative values for the visual attributes under the guidance of knowledge bases (KBs) and large language models (LLMs). We then utilize an image-editing model to edit the images with the extracted attributes. ARMADA is a novel multimodal data generation framework that: (i) extracts knowledge-grounded attributes from symbolic KBs for semantically consistent yet distinctive image-text pair generation, (ii) generates visually similar images of disparate categories using neighboring entities in the KB hierarchy, and (iii) uses the commonsense knowledge of LLMs to modulate auxiliary visual attributes such as backgrounds for more robust representation of original entities. Our empirical results over four downstream tasks demonstrate the efficacy of our framework to produce high-quality data and enhance the model performance. This also highlights the need to leverage external knowledge proxies for enhanced interpretability and real-world grounding.</abstract>
      <url hash="761a42e8">2024.wikinlp-1.17</url>
      <bibkey>jin-etal-2024-armada</bibkey>
    </paper>
    <paper id="18">
      <title>Summarization-Based Document <fixed-case>ID</fixed-case>s for Generative Retrieval with Language Models</title>
      <author><first>Alan</first><last>Li</last><affiliation>Yale University</affiliation></author>
      <author><first>Daniel</first><last>Cheng</last></author>
      <author><first>Phillip</first><last>Keung</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jungo</first><last>Kasai</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Noah A.</first><last>Smith</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>126-135</pages>
      <abstract>Generative retrieval (Wang et al., 2022; Tay et al., 2022) is a popular approach for end-to-end document retrieval that directly generates document identifiers given an input query. We introduce summarization-based document IDs, in which each document’s ID is composed of an extractive summary or abstractive keyphrases generated by a language model, rather than an integer ID sequence or bags of n-grams as proposed in past work. We find that abstractive, content-based IDs (ACID) and an ID based on the first 30 tokens are very effective in direct comparisons with previous approaches to ID creation. We show that using ACID improves top-10 and top-20 recall by 15.6% and 14.4% (relative) respectively versus the cluster-based integer ID baseline on the MSMARCO 100k retrieval task, and 9.8% and 9.9% respectively on the Wikipedia-based NQ 100k retrieval task. Our results demonstrate the effectiveness of human-readable, natural-language IDs created through summarization for generative retrieval. We also observed that extractive IDs outperformed abstractive IDs on Wikipedia articles in NQ but not the snippets in MSMARCO, which suggests that document characteristics affect generative retrieval performance.</abstract>
      <url hash="54becc2f">2024.wikinlp-1.18</url>
      <bibkey>li-etal-2024-summarization</bibkey>
    </paper>
  </volume>
</collection>
