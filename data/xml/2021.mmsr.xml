<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.mmsr">
  <volume id="1" ingest-date="2021-10-27" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR)</booktitle>
      <editor><first>Lucia</first><last>Donatelli</last></editor>
      <editor><first>Nikhil</first><last>Krishnaswamy</last></editor>
      <editor><first>Kenneth</first><last>Lai</last></editor>
      <editor><first>James</first><last>Pustejovsky</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Groningen, Netherlands (Online)</address>
      <month>June</month>
      <year>2021</year>
      <url hash="d8c17df0">2021.mmsr-1</url>
      <venue>mmsr</venue>
    </meta>
    <frontmatter>
      <url hash="a804a9ef">2021.mmsr-1.0</url>
      <bibkey>mmsr-2021-multimodal</bibkey>
    </frontmatter>
    <paper id="1">
      <title>What is Multimodality?</title>
      <author><first>Letitia</first><last>Parcalabescu</last></author>
      <author><first>Nils</first><last>Trost</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>1–10</pages>
      <abstract>The last years have shown rapid developments in the field of multimodal machine learning, combining e.g., vision, text or speech. In this position paper we explain how the field uses outdated definitions of multimodality that prove unfit for the machine learning era. We propose a new task-relative definition of (multi)modality in the context of multimodal machine learning that focuses on representations and information that are relevant for a given machine learning task. With our new definition of multimodality we aim to provide a missing foundation for multimodal research, an important component of language grounding and a crucial milestone towards NLU.</abstract>
      <url hash="0b1961ab">2021.mmsr-1.1</url>
      <bibkey>parcalabescu-etal-2021-multimodality</bibkey>
    </paper>
    <paper id="2">
      <title>Are Gestures Worth a Thousand Words? An Analysis of Interviews in the Political Domain</title>
      <author><first>Daniela</first><last>Trotta</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <pages>11–20</pages>
      <abstract>Speaker gestures are semantically co-expressive with speech and serve different pragmatic functions to accompany oral modality. Therefore, gestures are an inseparable part of the language system: they may add clarity to discourse, can be employed to facilitate lexical retrieval and retain a turn in conversations, assist in verbalizing semantic content and facilitate speakers in coming up with the words they intend to say. This aspect is particularly relevant in political discourse, where speakers try to apply communication strategies that are both clear and persuasive using verbal and non-verbal cues. In this paper we investigate the co-speech gestures of several Italian politicians during face-to-face interviews using a multimodal linguistic approach. We first enrich an existing corpus with a novel annotation layer capturing the function of hand movements. Then, we perform an analysis of the corpus, focusing in particular on the relationship between hand movements and other information layers such as the political party or non-lexical and semi-lexical tags. We observe that the recorded differences pertain more to single politicians than to the party they belong to, and that hand movements tend to occur frequently with semi-lexical phenomena, supporting the lexical retrieval hypothesis.</abstract>
      <url hash="6a9d4062">2021.mmsr-1.2</url>
      <bibkey>trotta-tonelli-2021-gestures</bibkey>
    </paper>
    <paper id="3">
      <title>Requesting clarifications with speech and gestures</title>
      <author><first>Jonathan</first><last>Ginzburg</last></author>
      <author><first>Andy</first><last>Luecking</last></author>
      <pages>21–31</pages>
      <abstract>In multimodal natural language interaction both speech and non-speech gestures are involved in the basic mechanism of grounding and repair. We discuss a couple of multimodal clarifica- tion requests and argue that gestures, as well as speech expressions, underlie comparable paral- lelism constraints. In order to make this precise, we slightly extend the formal dialogue frame- work KoS to cover also gestural counterparts of verbal locutionary propositions.</abstract>
      <url hash="eb227cd6">2021.mmsr-1.3</url>
      <bibkey>ginzburg-luecking-2021-requesting</bibkey>
    </paper>
    <paper id="4">
      <title>Seeing past words: Testing the cross-modal capabilities of pretrained <fixed-case>V</fixed-case>&amp;<fixed-case>L</fixed-case> models on counting tasks</title>
      <author><first>Letitia</first><last>Parcalabescu</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <author><first>Iacer</first><last>Calixto</last></author>
      <pages>32–44</pages>
      <abstract>We investigate the reasoning ability of pretrained vision and language (V&amp;L) models in two tasks that require multimodal integration: (1) discriminating a correct image-sentence pair from an incorrect one, and (2) counting entities in an image. We evaluate three pretrained V&amp;L models on these tasks: ViLBERT, ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results show that models solve task (1) very well, as expected, since all models are pretrained on task (1). However, none of the pretrained V&amp;L models is able to adequately solve task (2), our counting probe, and they cannot generalise to out-of-distribution quantities. We propose a number of explanations for these findings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of catastrophic forgetting on task (1). Concerning our results on the counting probe, we find evidence that all models are impacted by dataset bias, and also fail to individuate entities in the visual input. While a selling point of pretrained V&amp;L models is their ability to solve complex tasks, our findings suggest that understanding their reasoning and grounding capabilities requires more targeted investigations on specific phenomena.</abstract>
      <url hash="f4fa7033">2021.mmsr-1.4</url>
      <bibkey>parcalabescu-etal-2021-seeing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/counting-probe">Counting Probe</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual7w">Visual7W</pwcdataset>
    </paper>
    <paper id="5">
      <title>How Vision Affects Language: Comparing Masked Self-Attention in Uni-Modal and Multi-Modal Transformer</title>
      <author><first>Nikolai</first><last>Ilinykh</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>45–55</pages>
      <abstract>The problem of interpretation of knowledge learned by multi-head self-attention in transformers has been one of the central questions in NLP. However, a lot of work mainly focused on models trained for uni-modal tasks, e.g. machine translation. In this paper, we examine masked self-attention in a multi-modal transformer trained for the task of image captioning. In particular, we test whether the multi-modality of the task objective affects the learned attention patterns. Our visualisations of masked self-attention demonstrate that (i) it can learn general linguistic knowledge of the textual input, and (ii) its attention patterns incorporate artefacts from visual modality even though it has never accessed it directly. We compare our transformer’s attention patterns with masked attention in distilgpt-2 tested for uni-modal text generation of image captions. Based on the maps of extracted attention weights, we argue that masked self-attention in image captioning transformer seems to be enhanced with semantic knowledge from images, exemplifying joint language-and-vision information in its attention patterns.</abstract>
      <url hash="5e433943">2021.mmsr-1.5</url>
      <bibkey>ilinykh-dobnik-2021-vision</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>EMISSOR</fixed-case>: A platform for capturing multimodal interactions as Episodic Memories and Interpretations with Situated Scenario-based Ontological References</title>
      <author><first>Selene</first><last>Baez Santamaria</last></author>
      <author><first>Thomas</first><last>Baier</last></author>
      <author><first>Taewoon</first><last>Kim</last></author>
      <author><first>Lea</first><last>Krause</last></author>
      <author><first>Jaap</first><last>Kruijt</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <pages>56–77</pages>
      <abstract>We present EMISSOR: a platform to capture multimodal interactions as recordings of episodic experiences with explicit referential interpretations that also yield an episodic Knowledge Graph (eKG). The platform stores streams of multiple modalities as parallel signals. Each signal is segmented and annotated independently with interpretation. Annotations are eventually mapped to explicit identities and relations in the eKG. As we ground signal segments from different modalities to the same instance representations, we also ground different modalities across each other. Unique to our eKG is that it accepts different interpretations across modalities, sources and experiences and supports reasoning over conflicting information and uncertainties that may result from multimodal experiences. EMISSOR can record and annotate experiments in virtual and real-world, combine data, evaluate system behavior and their performance for preset goals but also model the accumulation of knowledge and interpretations in the Knowledge Graph as a result of these episodic experiences.</abstract>
      <url hash="6bb1d51b">2021.mmsr-1.6</url>
      <bibkey>baez-santamaria-etal-2021-emissor</bibkey>
      <pwccode url="https://github.com/cltl/EMISSOR" additional="false">cltl/EMISSOR</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/alfred">ALFRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reccon">RECCON</pwcdataset>
    </paper>
    <paper id="7">
      <title>Annotating anaphoric phenomena in situated dialogue</title>
      <author><first>Sharid</first><last>Loáiciga</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>78–88</pages>
      <abstract>In recent years several corpora have been developed for vision and language tasks. With this paper, we intend to start a discussion on the annotation of referential phenomena in situated dialogue. We argue that there is still significant room for corpora that increase the complexity of both visual and linguistic domains and which capture different varieties of perceptual and conversational contexts. In addition, a rich annotation scheme covering a broad range of referential phenomena and compatible with the textual task of coreference resolution is necessary in order to take the most advantage of these corpora. Consequently, there are several open questions regarding the semantics of reference and annotation, and the extent to which standard textual coreference accounts for the situated dialogue genre. Working with two corpora on situated dialogue, we present our extension to the ARRAU (Uryupina et al., 2020) annotation scheme in order to start this discussion.</abstract>
      <url hash="8ffcf838">2021.mmsr-1.7</url>
      <bibkey>loaiciga-etal-2021-annotating</bibkey>
    </paper>
    <paper id="8">
      <title>Incremental Unit Networks for Multimodal, Fine-grained Information State Representation</title>
      <author><first>Casey</first><last>Kennington</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>89–94</pages>
      <abstract>We offer a fine-grained information state annotation scheme that follows directly from the Incremental Unit abstract model of dialogue processing when used within a multimodal, co-located, interactive setting. We explain the Incremental Unit model and give an example application using the Localized Narratives dataset, then offer avenues for future research.</abstract>
      <url hash="7a8151bd">2021.mmsr-1.8</url>
      <bibkey>kennington-schlangen-2021-incremental</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/localized-narratives">Localized Narratives</pwcdataset>
    </paper>
    <paper id="9">
      <title>Teaching Arm and Head Gestures to a Humanoid Robot through Interactive Demonstration and Spoken Instruction</title>
      <author><first>Michael</first><last>Brady</last></author>
      <author><first>Han</first><last>Du</last></author>
      <pages>95–101</pages>
      <abstract>We describe work in progress for training a humanoid robot to produce iconic arm and head gestures as part of task-oriented dialogue interaction. This involves the development and use of a multimodal dialog manager for non-experts to quickly ‘program’ the robot through speech and vision. Using this dialog manager, videos of gesture demonstrations are collected. Motor positions are extracted from these videos to specify motor trajectories where collections of motor trajectories are used to produce robot gestures following a Gaussian mixtures approach. Concluding discussion considers how learned representations may be used for gesture recognition by the robot, and how the framework may mature into a system to address language grounding and semantic representation.</abstract>
      <url hash="6e2b7173">2021.mmsr-1.9</url>
      <bibkey>brady-du-2021-teaching</bibkey>
    </paper>
    <paper id="10">
      <title>Building a Video-and-Language Dataset with Human Actions for Multimodal Logical Inference</title>
      <author><first>Riko</first><last>Suzuki</last></author>
      <author><first>Hitomi</first><last>Yanaka</last></author>
      <author><first>Koji</first><last>Mineshima</last></author>
      <author><first>Daisuke</first><last>Bekki</last></author>
      <pages>102–107</pages>
      <abstract>This paper introduces a new video-and-language dataset with human actions for multimodal logical inference, which focuses on intentional and aspectual expressions that describe dynamic human actions. The dataset consists of 200 videos, 5,554 action labels, and 1,942 action triplets of the form (subject, predicate, object) that can be easily translated into logical semantic representations. The dataset is expected to be useful for evaluating multimodal inference systems between videos and semantically complicated sentences including negation and quantification.</abstract>
      <url hash="b068a49d">2021.mmsr-1.10</url>
      <bibkey>suzuki-etal-2021-building</bibkey>
      <pwccode url="https://github.com/rikos3/HumanActions" additional="false">rikos3/HumanActions</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/charades">Charades</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/violin">Violin</pwcdataset>
    </paper>
  </volume>
</collection>
