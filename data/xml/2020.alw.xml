<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.alw">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Online Abuse and Harms</booktitle>
      <editor><first>Seyi</first><last>Akiwowo</last></editor>
      <editor><first>Bertie</first><last>Vidgen</last></editor>
      <editor><first>Vinodkumar</first><last>Prabhakaran</last></editor>
      <editor><first>Zeerak</first><last>Waseem</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="4daefd0c">2020.alw-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Online Abuse and Human Rights: <fixed-case>WOAH</fixed-case> Satellite Session at <fixed-case>R</fixed-case>ights<fixed-case>C</fixed-case>on 2020</title>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <author><first>Zeerak</first><last>Waseem</last></author>
      <author><first>Seyi</first><last>Akiwowo</last></author>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <pages>1–6</pages>
      <abstract>In 2020 The Workshop on Online Abuse and Harms (WOAH) held a satellite panel at RightsCons 2020, an international human rights conference. Our aim was to bridge the gap between human rights scholarship and Natural Language Processing (NLP) research communities in tackling online abuse. We report on the discussions that took place, and present an analysis of four key issues which emerged: Problems in tackling online abuse, Solutions, Meta concerns and the Ecosystem of content moderation and research. We argue there is a pressing need for NLP research communities to engage with human rights perspectives, and identify four key ways in which NLP research into online abuse could immediately be enhanced to create better and more ethical solutions.</abstract>
      <url hash="171d945e">2020.alw-1.1</url>
      <doi>10.18653/v1/2020.alw-1.1</doi>
    </paper>
    <paper id="2">
      <title>A Novel Methodology for Developing Automatic Harassment Classifiers for <fixed-case>T</fixed-case>witter</title>
      <author><first>Ishaan</first><last>Arora</last></author>
      <author><first>Julia</first><last>Guo</last></author>
      <author><first>Sarah Ita</first><last>Levitan</last></author>
      <author><first>Susan</first><last>McGregor</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <pages>7–15</pages>
      <abstract>Most efforts at identifying abusive speech online rely on public corpora that have been scraped from websites using keyword-based queries or released by site or platform owners for research purposes. These are typically labeled by crowd-sourced annotators – not the targets of the abuse themselves. While this method of data collection supports fast development of machine learning classifiers, the models built on them often fail in the context of real-world harassment and abuse, which contain nuances less easily identified by non-targets. Here, we present a mixed-methods approach to create classifiers for abuse and harassment which leverages direct engagement with the target group in order to achieve high quality and ecological validity of data sets and labels, and to generate deeper insights into the key tactics of bad actors. We use women journalists’ experience on Twitter as an initial community of focus. We identify several structural mechanisms of abuse that we believe will generalize to other target communities.</abstract>
      <url hash="bda1d9f5">2020.alw-1.2</url>
      <doi>10.18653/v1/2020.alw-1.2</doi>
    </paper>
    <paper id="3">
      <title>Using Transfer-based Language Models to Detect Hateful and Offensive Language Online</title>
      <author><first>Vebjørn</first><last>Isaksen</last></author>
      <author><first>Björn</first><last>Gambäck</last></author>
      <pages>16–27</pages>
      <abstract>Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations from Transformers (BERT), with either general or domain-specific language models, were tested against two datasets containing tweets labelled as either ‘Hateful’, ‘Normal’ or ‘Offensive’. The results indicate that the attention-based models profoundly confuse hate speech with offensive and normal language. However, the pre-trained models outperform state-of-the-art results in terms of accurately predicting the hateful instances.</abstract>
      <url hash="b6670fac">2020.alw-1.3</url>
      <doi>10.18653/v1/2020.alw-1.3</doi>
    </paper>
    <paper id="4">
      <title>Fine-tuning for multi-domain and multi-label uncivil language detection</title>
      <author><first>Kadir Bulut</first><last>Ozler</last></author>
      <author><first>Kate</first><last>Kenski</last></author>
      <author><first>Steve</first><last>Rains</last></author>
      <author><first>Yotam</first><last>Shmargad</last></author>
      <author><first>Kevin</first><last>Coe</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <pages>28–33</pages>
      <abstract>Incivility is a problem on social media, and it comes in many forms (name-calling, vulgarity, threats, etc.) and domains (microblog posts, online news comments, Wikipedia edits, etc.). Training machine learning models to detect such incivility must handle the multi-label and multi-domain nature of the problem. We present a BERT-based model for incivility detection and propose several approaches for training it for multi-label and multi-domain datasets. We find that individual binary classifiers outperform a joint multi-label classifier, and that simply combining multiple domains of training data outperforms other recently-proposed fine tuning strategies. We also establish new state-of-the-art performance on several incivility detection datasets.</abstract>
      <url hash="3f75913b">2020.alw-1.4</url>
      <doi>10.18653/v1/2020.alw-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>H</fixed-case>urt<fixed-case>BERT</fixed-case>: Incorporating Lexical Features with <fixed-case>BERT</fixed-case> for the Detection of Abusive Language</title>
      <author><first>Anna</first><last>Koufakou</last></author>
      <author><first>Endang Wahyu</first><last>Pamungkas</last></author>
      <author><first>Valerio</first><last>Basile</last></author>
      <author><first>Viviana</first><last>Patti</last></author>
      <pages>34–43</pages>
      <abstract>The detection of abusive or offensive remarks in social texts has received significant attention in research. In several related shared tasks, BERT has been shown to be the state-of-the-art. In this paper, we propose to utilize lexical features derived from a hate lexicon towards improving the performance of BERT in such tasks. We explore different ways to utilize the lexical features in the form of lexicon-based encodings at the sentence level or embeddings at the word level. We provide an extensive dataset evaluation that addresses in-domain as well as cross-domain detection of abusive content to render a complete picture. Our results indicate that our proposed models combining BERT with lexical features help improve over a baseline BERT model in many of our in-domain and cross-domain experiments.</abstract>
      <url hash="b29ab768">2020.alw-1.5</url>
      <doi>10.18653/v1/2020.alw-1.5</doi>
    </paper>
    <paper id="6">
      <title>Abusive Language Detection using Syntactic Dependency Graphs</title>
      <author><first>Kanika</first><last>Narang</last></author>
      <author><first>Chris</first><last>Brew</last></author>
      <pages>44–53</pages>
      <abstract>Automated detection of abusive language online has become imperative. Current sequential models (LSTM) do not work well for long and complex sentences while bi-transformer models (BERT) are not computationally efficient for the task. We show that classifiers based on syntactic structure of the text, dependency graphical convolutional networks (DepGCNs) can achieve state-of-the-art performance on abusive language datasets. The overall performance is at par with of strong baselines such as fine-tuned BERT. Further, our GCN-based approach is much more efficient than BERT at inference time making it suitable for real-time detection.</abstract>
      <url hash="317e918d">2020.alw-1.6</url>
      <doi>10.18653/v1/2020.alw-1.6</doi>
    </paper>
    <paper id="7">
      <title>Impact of Politically Biased Data on Hate Speech Classification</title>
      <author><first>Maximilian</first><last>Wich</last></author>
      <author><first>Jan</first><last>Bauer</last></author>
      <author><first>Georg</first><last>Groh</last></author>
      <pages>54–64</pages>
      <abstract>One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years - in particular with the rise of deep learning. A problem of these models is their vulnerability to undesirable bias in training data. We investigate the impact of political bias on hate speech classification by constructing three politically-biased data sets (left-wing, right-wing, politically neutral) and compare the performance of classifiers trained on them. We show that (1) political bias negatively impairs the performance of hate speech classifiers and (2) an explainable machine learning model can help to visualize such bias within the training data. The results show that political bias in training data has an impact on hate speech classification and can become a serious issue.</abstract>
      <url hash="450be842">2020.alw-1.7</url>
      <doi>10.18653/v1/2020.alw-1.7</doi>
    </paper>
    <paper id="8">
      <title>Reducing Unintended Identity Bias in <fixed-case>R</fixed-case>ussian Hate Speech Detection</title>
      <author><first>Nadezhda</first><last>Zueva</last></author>
      <author><first>Madina</first><last>Kabirova</last></author>
      <author><first>Pavel</first><last>Kalaidin</last></author>
      <pages>65–69</pages>
      <abstract>Toxicity has become a grave problem for many online communities, and has been growing across many languages, including Russian. Hate speech creates an environment of intimidation, discrimination, and may even incite some real-world violence. Both researchers and social platforms have been focused on developing models to detect toxicity in online communication for a while now. A common problem of these models is the presence of bias towards some words (e.g. woman, black, jew or женщина, черный, еврей) that are not toxic, but serve as triggers for the classifier due to model caveats. In this paper, we describe our efforts towards classifying hate speech in Russian, and propose simple techniques of reducing unintended bias, such as generating training data with language models using terms and words related to protected identities as context and applying word dropout to such words.</abstract>
      <url hash="e67c17ce">2020.alw-1.8</url>
      <doi>10.18653/v1/2020.alw-1.8</doi>
    </paper>
    <paper id="9">
      <title>Investigating Sampling Bias in Abusive Language Detection</title>
      <author><first>Dante</first><last>Razo</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <pages>70–78</pages>
      <abstract>Abusive language detection is becoming increasingly important, but we still understand little about the biases in our datasets for abusive language detection, and how these biases affect the quality of abusive language detection. In the work reported here, we reproduce the investigation of Wiegand et al. (2019) to determine differences between different sampling strategies. They compared boosted random sampling, where abusive posts are upsampled, and biased topic sampling, which focuses on topics that are known to cause abusive language. Instead of comparing individual datasets created using these sampling strategies, we use the sampling strategies on a single, large dataset, thus eliminating the textual source of the dataset as a potential confounding factor. We show that differences in the textual source can have more effect than the chosen sampling strategy.</abstract>
      <url hash="77fb934f">2020.alw-1.9</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e4d73a60">2020.alw-1.9.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.alw-1.9</doi>
    </paper>
    <paper id="10">
      <title>Attending the Emotions to Detect Online Abusive Language</title>
      <author><first>Niloofar</first><last>Safi Samghabadi</last></author>
      <author><first>Afsheen</first><last>Hatami</last></author>
      <author><first>Mahsa</first><last>Shafaei</last></author>
      <author><first>Sudipta</first><last>Kar</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>79–88</pages>
      <abstract>In recent years, abusive behavior has become a serious issue in online social networks. In this paper, we present a new corpus for the task of abusive language detection that is collected from a semi-anonymous online platform, and unlike the majority of other available resources, is not created based on a specific list of bad words. We also develop computational models to incorporate emotions into textual cues to improve aggression identification. We evaluate our proposed methods on a set of corpora related to the task and show promising results with respect to abusive language detection.</abstract>
      <url hash="2aacb0ad">2020.alw-1.10</url>
      <doi>10.18653/v1/2020.alw-1.10</doi>
    </paper>
    <paper id="11">
      <title>Enhancing the Identification of Cyberbullying through Participant Roles</title>
      <author><first>Gathika</first><last>Rathnayake</last></author>
      <author><first>Thushari</first><last>Atapattu</last></author>
      <author><first>Mahen</first><last>Herath</last></author>
      <author><first>Georgia</first><last>Zhang</last></author>
      <author><first>Katrina</first><last>Falkner</last></author>
      <pages>89–94</pages>
      <abstract>Cyberbullying is a prevalent social problem that inflicts detrimental consequences to the health and safety of victims such as psychological distress, anti-social behaviour, and suicide. The automation of cyberbullying detection is a recent but widely researched problem, with current research having a strong focus on a binary classification of bullying versus non-bullying. This paper proposes a novel approach to enhancing cyberbullying detection through role modeling. We utilise a dataset from ASKfm to perform multi-class classification to detect participant roles (e.g. victim, harasser). Our preliminary results demonstrate promising performance including 0.83 and 0.76 of F1-score for cyberbullying and role classification respectively, outperforming baselines.</abstract>
      <url hash="f83805eb">2020.alw-1.11</url>
      <doi>10.18653/v1/2020.alw-1.11</doi>
    </paper>
    <paper id="12">
      <title>Developing a New Classifier for Automated Identification of Incivility in Social Media</title>
      <author><first>Sam</first><last>Davidson</last></author>
      <author><first>Qiusi</first><last>Sun</last></author>
      <author><first>Magdalena</first><last>Wojcieszak</last></author>
      <pages>95–101</pages>
      <abstract>Incivility is not only prevalent on online social media platforms, but also has concrete effects on individual users, online groups, and the platforms themselves. Given the prevalence and effects of online incivility, and the challenges involved in human-based incivility detection, it is urgent to develop validated and versatile automatic approaches to identifying uncivil posts and comments. This project advances both a neural, BERT-based classifier as well as a logistic regression classifier to identify uncivil comments. The classifier is trained on a dataset of Reddit posts, which are annotated for incivility, and further expanded using a combination of labeled data from Reddit and Twitter. Our best performing model achieves an F1 of 0.802 on our Reddit test set. The final model is not only applicable across social media platforms and their distinct data structures, but also computationally versatile, and - as such - ready to be used on vast volumes of online data. All trained models and annotated data are made available to the research community.</abstract>
      <url hash="30665449">2020.alw-1.12</url>
      <doi>10.18653/v1/2020.alw-1.12</doi>
    </paper>
    <paper id="13">
      <title>Countering hate on social media: Large scale classification of hate and counter speech</title>
      <author><first>Joshua</first><last>Garland</last></author>
      <author><first>Keyan</first><last>Ghazi-Zahedi</last></author>
      <author><first>Jean-Gabriel</first><last>Young</last></author>
      <author><first>Laurent</first><last>Hébert-Dufresne</last></author>
      <author><first>Mirta</first><last>Galesic</last></author>
      <pages>102–112</pages>
      <abstract>Hateful rhetoric is plaguing online discourse, fostering extreme societal movements and possibly giving rise to real-world violence. A potential solution to this growing global problem is citizen-generated counter speech where citizens actively engage with hate speech to restore civil non-polarized discourse. However, its actual effectiveness in curbing the spread of hatred is unknown and hard to quantify. One major obstacle to researching this question is a lack of large labeled data sets for training automated classifiers to identify counter speech. Here we use a unique situation in Germany where self-labeling groups engaged in organized online hate and counter speech. We use an ensemble learning algorithm which pairs a variety of paragraph embeddings with regularized logistic regression functions to classify both hate and counter speech in a corpus of millions of relevant tweets from these two groups. Our pipeline achieves macro F1 scores on out of sample balanced test sets ranging from 0.76 to 0.97—accuracy in line and even exceeding the state of the art. We then use the classifier to discover hate and counter speech in more than 135,000 fully-resolved Twitter conversations occurring from 2013 to 2018 and study their frequency and interaction. Altogether, our results highlight the potential of automated methods to evaluate the impact of coordinated counter speech in stabilizing conversations on social media.</abstract>
      <url hash="799042bc">2020.alw-1.13</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b2840f42">2020.alw-1.13.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.alw-1.13</doi>
    </paper>
    <paper id="14">
      <title>Moderating Our (Dis)Content: Renewing the Regulatory Approach</title>
      <author><first>Claire</first><last>Pershan</last></author>
      <pages>113</pages>
      <abstract>As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and interconnectedness of content moderation across a fragmented online landscape. This report urges regulators and legislators to consider a range of platforms and moderation approaches in the regulation. In particular, it calls for a holistic, process-oriented regulatory approach that accounts for actors beyond the handful of dominant platforms that currently shape public debate.</abstract>
      <url hash="dc387124">2020.alw-1.14</url>
      <doi>10.18653/v1/2020.alw-1.14</doi>
    </paper>
    <paper id="15">
      <title>Six Attributes of Unhealthy Conversations</title>
      <author><first>Ilan</first><last>Price</last></author>
      <author><first>Jordan</first><last>Gifford-Moore</last></author>
      <author><first>Jory</first><last>Flemming</last></author>
      <author><first>Saul</first><last>Musker</last></author>
      <author><first>Maayan</first><last>Roichman</last></author>
      <author><first>Guillaume</first><last>Sylvain</last></author>
      <author><first>Nithum</first><last>Thain</last></author>
      <author><first>Lucas</first><last>Dixon</last></author>
      <author><first>Jeffrey</first><last>Sorensen</last></author>
      <pages>114–124</pages>
      <abstract>We present a new dataset of approximately 44000 comments labeled by crowdworkers. Each comment is labelled as either ‘healthy’ or ‘unhealthy’, in addition to binary labels for the presence of six potentially ‘unhealthy’ sub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic; and/or (6) an unfair generalisation. Each label also has an associated confidence score. We argue that there is a need for datasets which enable research based on a broad notion of ‘unhealthy online conversation’. We build this typology to encompass a substantial proportion of the individual comments which contribute to unhealthy online conversation. For some of these attributes, this is the first publicly available dataset of this scale. We explore the quality of the dataset, present some summary statistics and initial models to illustrate the utility of this data, and highlight limitations and directions for further research.</abstract>
      <url hash="f542eb80">2020.alw-1.15</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8b683bf6">2020.alw-1.15.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.alw-1.15</doi>
    </paper>
    <paper id="16">
      <title>A Unified Taxonomy of Harmful Content</title>
      <author><first>Michele</first><last>Banko</last></author>
      <author><first>Brendon</first><last>MacKeen</last></author>
      <author><first>Laurie</first><last>Ray</last></author>
      <pages>125–137</pages>
      <abstract>The ability to recognize harmful content within online communities has come into focus for researchers, engineers and policy makers seeking to protect users from abuse. While the number of datasets aiming to capture forms of abuse has grown in recent years, the community has not standardized around how various harmful behaviors are defined, creating challenges for reliable moderation, modeling and evaluation. As a step towards attaining shared understanding of how online abuse may be modeled, we synthesize the most common types of abuse described by industry, policy, community and health experts into a unified typology of harmful content, with detailed criteria and exceptions for each type of abuse.</abstract>
      <url hash="e1867f87">2020.alw-1.16</url>
      <doi>10.18653/v1/2020.alw-1.16</doi>
    </paper>
    <paper id="17">
      <title>Towards a Comprehensive Taxonomy and Large-Scale Annotated Corpus for Online Slur Usage</title>
      <author><first>Jana</first><last>Kurrek</last></author>
      <author><first>Haji Mohammad</first><last>Saleem</last></author>
      <author><first>Derek</first><last>Ruths</last></author>
      <pages>138–149</pages>
      <abstract>Abusive language classifiers have been shown to exhibit bias against women and racial minorities. Since these models are trained on data that is collected using keywords, they tend to exhibit a high sensitivity towards pejoratives. As a result, comments written by victims of abuse are frequently labelled as hateful, even if they discuss or reclaim slurs. Any attempt to address bias in keyword-based corpora requires a better understanding of pejorative language, as well as an equitable representation of targeted users in data collection. We make two main contributions to this end. First, we provide an annotation guide that outlines 4 main categories of online slur usage, which we further divide into a total of 12 sub-categories. Second, we present a publicly available corpus based on our taxonomy, with 39.8k human annotated comments extracted from Reddit. This corpus was annotated by a diverse cohort of coders, with Shannon equitability indices of 0.90, 0.92, and 0.87 across sexuality, ethnicity, and gender. Taken together, our taxonomy and corpus allow researchers to evaluate classifiers on a wider range of speech containing slurs.</abstract>
      <url hash="b1160a5d">2020.alw-1.17</url>
      <attachment type="OptionalSupplementaryMaterial" hash="a04ebea8">2020.alw-1.17.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.alw-1.17</doi>
    </paper>
    <paper id="18">
      <title>In Data We Trust: A Critical Analysis of Hate Speech Detection Datasets</title>
      <author><first>Kosisochukwu</first><last>Madukwe</last></author>
      <author><first>Xiaoying</first><last>Gao</last></author>
      <author><first>Bing</first><last>Xue</last></author>
      <pages>150–161</pages>
      <abstract>Recently, a few studies have discussed the limitations of datasets collected for the task of detecting hate speech from different viewpoints. We intend to contribute to the conversation by providing a consolidated overview of these issues pertaining to the data that debilitate research in this area. Specifically, we discuss how the varying pre-processing steps and the format for making data publicly available result in highly varying datasets that make an objective comparison between studies difficult and unfair. There is currently no study (to the best of our knowledge) focused on comparing the attributes of existing datasets for hate speech detection, outlining their limitations and recommending approaches for future research. This work intends to fill that gap and become the one-stop shop for information regarding hate speech datasets.</abstract>
      <url hash="20d9a8ce">2020.alw-1.18</url>
      <doi>10.18653/v1/2020.alw-1.18</doi>
    </paper>
    <paper id="19">
      <title>Detecting <fixed-case>E</fixed-case>ast <fixed-case>A</fixed-case>sian Prejudice on Social Media</title>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <author><first>Scott</first><last>Hale</last></author>
      <author><first>Ella</first><last>Guest</last></author>
      <author><first>Helen</first><last>Margetts</last></author>
      <author><first>David</first><last>Broniatowski</last></author>
      <author><first>Zeerak</first><last>Waseem</last></author>
      <author><first>Austin</first><last>Botelho</last></author>
      <author><first>Matthew</first><last>Hall</last></author>
      <author><first>Rebekah</first><last>Tromble</last></author>
      <pages>162–172</pages>
      <abstract>During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.</abstract>
      <url hash="439658bd">2020.alw-1.19</url>
      <attachment type="OptionalSupplementaryMaterial" hash="53df7520">2020.alw-1.19.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.alw-1.19</doi>
    </paper>
    <paper id="20">
      <title>On Cross-Dataset Generalization in Automatic Detection of Online Abuse</title>
      <author><first>Isar</first><last>Nejadgholi</last></author>
      <author><first>Svetlana</first><last>Kiritchenko</last></author>
      <pages>173–183</pages>
      <abstract>NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, training and test datasets are usually obtained from similar data samples, in practice systems are often applied on data that are different from the training set in topic and class distributions. Also, the ambiguity in class definitions inherited in this task aggravates the discrepancies between source and target datasets. We explore the topic bias and the task formulation bias in cross-dataset generalization. We show that the benign examples in the Wikipedia Detox dataset are biased towards platform-specific topics. We identify these examples using unsupervised topic modeling and manual inspection of topics’ keywords. Removing these topics increases cross-dataset generalization, without reducing in-domain classification performance. For a robust dataset design, we suggest applying inexpensive unsupervised methods to inspect the collected data and downsize the non-generalizable content before manually annotating for class labels.</abstract>
      <url hash="91b1f621">2020.alw-1.20</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8dbcbfc5">2020.alw-1.20.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.alw-1.20</doi>
    </paper>
    <paper id="21">
      <title>Identifying and Measuring Annotator Bias Based on Annotators’ Demographic Characteristics</title>
      <author><first>Hala</first><last>Al Kuwatly</last></author>
      <author><first>Maximilian</first><last>Wich</last></author>
      <author><first>Georg</first><last>Groh</last></author>
      <pages>184–190</pages>
      <abstract>Machine learning is recently used to detect hate speech and other forms of abusive language in online platforms. However, a notable weakness of machine learning models is their vulnerability to bias, which can impair their performance and fairness. One type is annotator bias caused by the subjective perception of the annotators. In this work, we investigate annotator bias using classification models trained on data from demographically distinct annotator groups. To do so, we sample balanced subsets of data that are labeled by demographically distinct annotators. We then train classifiers on these subsets, analyze their performances on similarly grouped test sets, and compare them statistically. Our findings show that the proposed approach successfully identifies bias and that demographic features, such as first language, age, and education, correlate with significant performance differences.</abstract>
      <url hash="67ec9a1c">2020.alw-1.21</url>
      <attachment type="OptionalSupplementaryMaterial" hash="d3a20444">2020.alw-1.21.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.alw-1.21</doi>
    </paper>
    <paper id="22">
      <title>Investigating Annotator Bias with a Graph-Based Approach</title>
      <author><first>Maximilian</first><last>Wich</last></author>
      <author><first>Hala</first><last>Al Kuwatly</last></author>
      <author><first>Georg</first><last>Groh</last></author>
      <pages>191–199</pages>
      <abstract>A challenge that many online platforms face is hate speech or any other form of online abuse. To cope with this, hate speech detection systems are developed based on machine learning to reduce manual work for monitoring these platforms. Unfortunately, machine learning is vulnerable to unintended bias in training data, which could have severe consequences, such as a decrease in classification performance or unfair behavior (e.g., discriminating minorities). In the scope of this study, we want to investigate annotator bias — a form of bias that annotators cause due to different knowledge in regards to the task and their subjective perception. Our goal is to identify annotation bias based on similarities in the annotation behavior from annotators. To do so, we build a graph based on the annotations from the different annotators, apply a community detection algorithm to group the annotators, and train for each group classifiers whose performances we compare. By doing so, we are able to identify annotator bias within a data set. The proposed method and collected insights can contribute to developing fairer and more reliable hate speech classification models.</abstract>
      <url hash="a7eb4a69">2020.alw-1.22</url>
      <attachment type="OptionalSupplementaryMaterial" hash="f968f6da">2020.alw-1.22.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.alw-1.22</doi>
    </paper>
  </volume>
</collection>
