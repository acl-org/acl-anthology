<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.figlang">
  <volume id="1" ingest-date="2024-06-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th Workshop on Figurative Language Processing (FigLang 2024)</booktitle>
      <editor><first>Debanjan</first><last>Ghosh</last></editor>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Anna</first><last>Feldman</last></editor>
      <editor><first>Tuhin</first><last>Chakrabarty</last></editor>
      <editor><first>Emmy</first><last>Liu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico (Hybrid)</address>
      <month>June</month>
      <year>2024</year>
      <url hash="a5eae616">2024.figlang-1</url>
      <venue>figlang</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="65b7207c">2024.figlang-1.0</url>
      <bibkey>fig-lang-2024-figurative</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Context vs. Human Disagreement in Sarcasm Detection</title>
      <author><first>Hyewon</first><last>Jang</last><affiliation>Universität Konstanz</affiliation></author>
      <author><first>Moritz</first><last>Jakob</last></author>
      <author><first>Diego</first><last>Frassinelli</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>1-7</pages>
      <abstract>Prior work has highlighted the importance of context in the identification of sarcasm by humans and language models. This work examines how much context is required for a better identification of sarcasm by both parties. We collect textual responses to dialogical prompts and sarcasm judgment to the responses placed after long contexts, short contexts, and no contexts. We find that both for humans and language models, the presence of context is generally important in identifying sarcasm in the response. But increasing the amount of context provides no added benefit to humans (long = short &gt; none). This is the same for language models, but only on easily agreed-upon sentences; for sentences with disagreement among human evaluators, different models show different behavior. We also show how sarcasm detection patterns stay consistent as the amount of context is manipulated despite the low agreement in human evaluation.</abstract>
      <url hash="64064f24">2024.figlang-1.1</url>
      <bibkey>jang-etal-2024-context</bibkey>
      <doi>10.18653/v1/2024.figlang-1.1</doi>
    </paper>
    <paper id="2">
      <title>Optimizing Multilingual Euphemism Detection using Low-Rank Adaption Within and Across Languages</title>
      <author><first>Nicholas</first><last>Hankins</last></author>
      <pages>8-14</pages>
      <abstract>This short paper presents an investigation into the effectiveness of various classification methods as a submission in the Multilingual Euphemism Detection Shared Task for the Fourth Workshop on Figurative Language Processing co-located with NAACL 2024. The process used by the participant utilizes pre-trained large language models combined with parameter efficient fine-tuning methods, specifically Low-Rank Adaptation (LoRA), in classifying euphemisms across four different languages - Mandarin Chinese, American English, Spanish, and Yorùbá. The study is comprised of three main components that aim to explore heuristic methods to navigate how base models can most efficiently be fine-tuned into classifiers to learn figurative language. Multilingual labeled training data was utilized to fine-tune classifiers for each language, and later combined for one large classifier, while unseen test data was finally used to evaluate the accuracy of the best performing classifiers. In addition, cross-lingual tests were conducted by applying each language’s data on each of the other language’s classifiers. All of the results provide insights into the potential of pre-trained base models combined with LoRA fine-tuning methods in accurately classifying euphemisms across and within different languages.</abstract>
      <url hash="2b5477c5">2024.figlang-1.2</url>
      <bibkey>hankins-2024-optimizing</bibkey>
      <doi>10.18653/v1/2024.figlang-1.2</doi>
    </paper>
    <paper id="3">
      <title>Comparison of Image Generation Models for Abstract and Concrete Event Descriptions</title>
      <author><first>Mohammed</first><last>Khaliq</last></author>
      <author><first>Diego</first><last>Frassinelli</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Sabine</first><last>Schulte Im Walde</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>15-21</pages>
      <abstract>With the advent of diffusion-based image generation models such as DALL-E, Stable Diffusion and Midjourney, high quality images can be easily generated using textual inputs. It is unclear, however, to what extent the generated images resemble human mental representations, especially regarding abstract event knowledge. We analyse the capability of four state-of-the-art models in generating images of verb-object event pairs when we systematically manipulate the degrees of abstractness of both the verbs and the object nouns. Human judgements assess the generated images and demonstrate that DALL-E is strongest for event pairs with concrete nouns (e.g., “pour water”; “believe person”), while Midjourney is preferred for event pairs with abstract nouns (e.g., “raise awareness”; “remain mystery”), irrespective of the concreteness of the verb. Across models, humans were most unsatisfied with images of events pairs that combined concrete verbs with abstract direct-object nouns (e.g., “speak truth”), and an additional ad-hoc annotation contributes this to its potential for figurative language.</abstract>
      <url hash="9292c8c2">2024.figlang-1.3</url>
      <bibkey>khaliq-etal-2024-comparison</bibkey>
      <doi>10.18653/v1/2024.figlang-1.3</doi>
    </paper>
    <paper id="4">
      <title>Cross-Lingual Metaphor Detection for Low-Resource Languages</title>
      <author><first>Anna</first><last>Hülsing</last><affiliation>Universität Hildesheim</affiliation></author>
      <author><first>Sabine</first><last>Schulte Im Walde</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>22-34</pages>
      <abstract>Research on metaphor detection (MD) in a multilingual setup has recently gained momentum. As for many tasks, it is however unclear how the amount of data used to pretrain large language models affects the performance, and whether non-neural models might provide a reasonable alternative, especially for MD in low-resource languages. This paper compares neural and non-neural cross-lingual models for English as the source language and Russian, German and Latin as target languages. In a series of experiments we show that the neural cross-lingual adapter architecture MAD-X performs best across target languages. Zero-shot classification with mBERT achieves decent results above the majority baseline, while few-shot classification with mBERT heavily depends on shot-selection, which is inconvenient in a cross-lingual setup where no validation data for the target language exists. The non-neural model, a random forest classifier with conceptual features, is outperformed by the neural models. Overall, we recommend MAD-X for metaphor detection not only in high-resource but also in low-resource scenarios regarding the amounts of pretraining data for mBERT.</abstract>
      <url hash="b895331f">2024.figlang-1.4</url>
      <bibkey>hulsing-schulte-im-walde-2024-cross</bibkey>
      <doi>10.18653/v1/2024.figlang-1.4</doi>
    </paper>
    <paper id="5">
      <title>A Hard Nut to Crack: Idiom Detection with Conversational Large Language Models</title>
      <author><first>Francesca</first><last>De Luca Fornaciari</last><affiliation>Barcelona Supercomputing Center and Universidad del País Vasco</affiliation></author>
      <author><first>Begoña</first><last>Altuna</last></author>
      <author><first>Itziar</first><last>Gonzalez-Dios</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Maite</first><last>Melero</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <pages>35-44</pages>
      <abstract>In this work, we explore idiomatic language processing with Large Language Models (LLMs). We introduce the Idiomatic language Test Suite IdioTS, a dataset of difficult examples specifically designed by language experts to assess the capabilities of LLMs to process figurative language at sentence level. We propose a comprehensive evaluation methodology based on an idiom detection task, where LLMs are prompted with detecting an idiomatic expression in a given English sentence. We present a thorough automatic and manual evaluation of the results and a comprehensive error analysis.</abstract>
      <url hash="4d1dec66">2024.figlang-1.5</url>
      <bibkey>de-luca-fornaciari-etal-2024-hard</bibkey>
      <doi>10.18653/v1/2024.figlang-1.5</doi>
    </paper>
    <paper id="6">
      <title>The Elephant in the Room: Ten Challenges of Computational Detection of Rhetorical Figures</title>
      <author><first>Ramona</first><last>Kühn</last><affiliation>Universität Passau</affiliation></author>
      <author><first>Jelena</first><last>Mitrović</last><affiliation>University of Passau</affiliation></author>
      <pages>45-52</pages>
      <abstract>Computational detection of rhetorical figures focuses mostly on figures such as metaphor, irony, or analogy. However, there exist many more figures that are neither less important nor less prevalent. We wanted to pinpoint the reasons why researchers often avoid other figures and to shed light on the challenges they struggle with when investigating those figures. In this comprehensive survey, we analyzed over 40 papers dealing with the computational detection of rhetorical figures other than metaphor, simile, sarcasm, and irony. We encountered recurrent challenges from which we compiled a ten point list. Furthermore, we suggest solutions for each challenge to encourage researchers to investigate a greater variety of rhetorical figures.</abstract>
      <url hash="4c3b19f5">2024.figlang-1.6</url>
      <bibkey>kuhn-mitrovic-2024-elephant</bibkey>
      <doi>10.18653/v1/2024.figlang-1.6</doi>
    </paper>
    <paper id="7">
      <title>Guidelines for the Annotation of Intentional Linguistic Metaphor</title>
      <author><first>Stefanie</first><last>Dipper</last><affiliation>Ruhr-Universtät Bochum</affiliation></author>
      <author><first>Adam</first><last>Roussel</last><affiliation>Ruhr-Universtät Bochum</affiliation></author>
      <author><first>Alexandra</first><last>Wiemann</last><affiliation>NA</affiliation></author>
      <author><first>Won</first><last>Kim</last><affiliation>NA</affiliation></author>
      <author><first>Tra-my</first><last>Nguyen</last><affiliation>NA</affiliation></author>
      <pages>53-58</pages>
      <abstract>This paper presents guidelines for the annotation of intentional (i.e. non-conventionalized) linguistic metaphors. Expressions that contribute to the same metaphorical image are annotated as a chain, additionally a semantically contrasting expression of the target domain is marked as an anchor. So far, a corpus of ten TEDx talks with a total of 20k tokens has been annotated according to these guidelines. 1.25% of the tokens are intentional metaphorical expressions.</abstract>
      <url hash="714a87e5">2024.figlang-1.7</url>
      <bibkey>dipper-etal-2024-guidelines</bibkey>
      <doi>10.18653/v1/2024.figlang-1.7</doi>
    </paper>
    <paper id="8">
      <title>Evaluating the Development of Linguistic Metaphor Annotation in <fixed-case>M</fixed-case>exican <fixed-case>S</fixed-case>panish Popular Science Tweets</title>
      <author><first>Alec</first><last>Montero</last></author>
      <author><first>Gemma</first><last>Bel-Enguix</last><affiliation>Universidad Nacional Autónoma de México</affiliation></author>
      <author><first>Sergio-Luis</first><last>Ojeda-Trueba</last><affiliation>NA</affiliation></author>
      <author><first>Marisela</first><last>Colín Rodea</last><affiliation>NA</affiliation></author>
      <pages>59-64</pages>
      <abstract>Following previous work on metaphor annotation and automatic metaphor processing, this study presents the evaluation of an initial phase in the novel area of linguistic metaphor detection in Mexican Spanish popular science tweets. Specifically, we examine the challenges posed by the annotation process stemming from disagreement among annotators. During this phase of our work, we conducted the annotation of a corpus comprising 3733 Mexican Spanish popular science tweets. This corpus was divided into two halves and each half was then assigned to two different pairs of native Mexican Spanish-speaking annotators. Despite rigorous methodology and continuous training, inter-annotator agreement as measured by Cohen’s kappa was found to be low, slightly above chance levels, although the concordance percentage exceeded 60%. By elucidating the inherent complexity of metaphor annotation tasks, our evaluation emphasizes the implications of these findings and offers insights for future research in this field, with the aim of creating a robust dataset for machine learning.</abstract>
      <url hash="fd854fb2">2024.figlang-1.8</url>
      <bibkey>montero-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.figlang-1.8</doi>
    </paper>
    <paper id="9">
      <title>Can <fixed-case>GPT</fixed-case>4 Detect Euphemisms across Multiple Languages?</title>
      <author><first>Todd</first><last>Firsich</last></author>
      <author><first>Anthony</first><last>Rios</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <pages>65-72</pages>
      <abstract>A euphemism is a word or phrase used in place of another word or phrase that might be considered harsh, blunt, unpleasant, or offensive. Euphemisms generally soften the impact of what is being said, making it more palatable or appropriate for the context or audience. Euphemisms can vary significantly between languages, reflecting cultural sensitivities and taboos, and what might be a mild expression in one language could carry a stronger connotation or be completely misunderstood in another. This paper uses prompting techniques to evaluate OpenAI’s GPT4 for detecting euphemisms across multiple languages as part of the 2024 FigLang shared task. We evaluate both zero-shot and few-shot approaches. Our method achieved an average macro F1 of .732, ranking first in the competition. Moreover, we found that GPT4 does not perform uniformly across all languages, with a difference of .233 between the best (English .831) and the worst (Spanish .598) languages.</abstract>
      <url hash="e485ad9c">2024.figlang-1.9</url>
      <bibkey>firsich-rios-2024-gpt4</bibkey>
      <doi>10.18653/v1/2024.figlang-1.9</doi>
    </paper>
    <paper id="10">
      <title>Ensemble-based Multilingual Euphemism Detection: a Behavior-Guided Approach</title>
      <author><first>Fedor</first><last>Vitiugin</last><affiliation>Aalto University</affiliation></author>
      <author><first>Henna</first><last>Paakki</last><affiliation>Aalto University</affiliation></author>
      <pages>73-78</pages>
      <abstract>This paper describes the system submitted by our team to the Multilingual Euphemism Detection Shared Task for the Fourth Workshop on Figurative Language Processing (FigLang 2024). We propose a novel model for multilingual euphemism detection, combining contextual and behavior-related features. The system classifies texts that potentially contain euphemistic terms with an ensemble classifier based on outputs from behavior-related fine-tuned models. Our results show that, for this kind of task, our model outperforms baselines and state-of-the-art euphemism detection methods. As for the leader-board, our classification model achieved a macro averaged F1 score of [anonymized], reaching the [anonymized] place.</abstract>
      <url hash="6309e4db">2024.figlang-1.10</url>
      <bibkey>vitiugin-paakki-2024-ensemble</bibkey>
      <doi>10.18653/v1/2024.figlang-1.10</doi>
    </paper>
    <paper id="11">
      <title>An Expectation-Realization Model for Metaphor Detection: Within Distribution, Out of Distribution, and Out of Pretraining</title>
      <author><first>Oseremen</first><last>Uduehi</last><affiliation>Ohio University</affiliation></author>
      <author><first>Razvan</first><last>Bunescu</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <pages>79-84</pages>
      <abstract>We propose a new model for metaphor detection in which an expectation component estimates representations of expected word meanings in a given context, whereas a realization component computes representations of target word meanings in context. We also introduce a systematic evaluation methodology that estimates generalization performance in three settings: within distribution, a new strong out of distribution setting, and a novel out-of-pretraining setting. Across all settings, the expectation-realization model obtains results that are competitive with or better than previous metaphor detection models.</abstract>
      <url hash="67229d07">2024.figlang-1.11</url>
      <bibkey>uduehi-bunescu-2024-expectation</bibkey>
      <doi>10.18653/v1/2024.figlang-1.11</doi>
    </paper>
    <paper id="12">
      <title>A Textual Modal Supplement Framework for Understanding Multi-Modal Figurative Language</title>
      <author><first>Jiale</first><last>Chen</last></author>
      <author><first>Qihao</first><last>Yang</last><affiliation>South China Normal University</affiliation></author>
      <author><first>Xuelian</first><last>Dong</last><affiliation>NA</affiliation></author>
      <author><first>Xiaoling</first><last>Mao</last><affiliation>NA</affiliation></author>
      <author><first>Tianyong</first><last>Hao</last></author>
      <pages>85-91</pages>
      <abstract>Figurative language in media such as memes, art, or comics has gained dramatic interest recently. However, the challenge remains in accurately justifying and explaining whether an image caption complements or contradicts the image it accompanies. To tackle this problem, we design a modal-supplement framework MAPPER consisting of a describer and thinker. The describer based on a frozen large vision model is designed to describe an image in detail to capture entailed semantic information. The thinker based on a finetuned large multi-modal model is designed to utilize description, claim and image to make prediction and explanation. Experiment results on a publicly available benchmark dataset from FigLang2024 Task 2 show that our method ranks at top 1 in overall evaluation, the performance exceeds the second place by 28.57%. This indicates that MAPPER is highly effective in understanding, judging and explaining of the figurative language. The source code is available at https://github.com/Libv-Team/figlang2024.</abstract>
      <url hash="4eb04a61">2024.figlang-1.12</url>
      <bibkey>chen-etal-2024-textual</bibkey>
      <doi>10.18653/v1/2024.figlang-1.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>F</fixed-case>ig<fixed-case>CLIP</fixed-case>: A Generative Multimodal Model with Bidirectional Cross-attention for Understanding Figurative Language via Visual Entailment</title>
      <author><first>Qihao</first><last>Yang</last><affiliation>South China Normal University</affiliation></author>
      <author><first>Xuelin</first><last>Wang</last><affiliation>Jinan University</affiliation></author>
      <pages>92-98</pages>
      <abstract>This is a system paper for the FigLang-2024 Multimodal Figurative Language Shared Task. Figurative language is generally represented through multiple modalities, facilitating the expression of complex and abstract ideas. With the popularity of various text-to-image tools, a large number of images containing metaphors or ironies are created. Traditional recognizing textual entailment has been extended to the task of understanding figurative language via visual entailment. However, existing pre-trained multimodal models in open domains often struggle with this task due to the intertwining of counterfactuals, human culture, and imagination. To bridge this gap, we propose FigCLIP, an end-to-end model based on CLIP and GPT-2, to identify multimodal figurative semantics and generate explanations. It employs a bidirectional fusion module with cross-attention and leverages explanations to promote the alignment of figurative image-text representations. Experimental results on the benchmark demonstrate the effectiveness of our method, achieving 70% F1-score, 67% F1@50-score and 50% F1@60-score. It outperforms GPT-4V, which has robust visual reasoning capabilities.</abstract>
      <url hash="f74ed1d1">2024.figlang-1.13</url>
      <bibkey>yang-wang-2024-figclip</bibkey>
      <doi>10.18653/v1/2024.figlang-1.13</doi>
    </paper>
    <paper id="14">
      <title>The Register-specific Distribution of Personification in <fixed-case>H</fixed-case>ungarian: A Corpus-driven Analysis</title>
      <author><first>Gabor</first><last>Simon</last><affiliation>Eötvös Lorand University</affiliation></author>
      <pages>99-109</pages>
      <abstract>The aim of the paper is twofold: (i) to present an extended version of the PerSE corpus, the language resource for investigating personification in Hungarian; (ii) to explore the semantic and lexicogrammatical patterns of Hungarian personification in a corpus-driven analysis, based on the current version of the research corpus. PerSE corpus is compiled from online available Hungarian texts in different registers including journalistic (car reviews and reports on interstate relations) and academic discourse (original research papers from different fields). The paper provides the reader with the infrastructure and the protocol of the semi-automatic and manual annotation in the corpus. Then it gives an overview of the register-specific distribution of personifications and focuses on some of its lexicogrammatical patterns.</abstract>
      <url hash="4309a11a">2024.figlang-1.14</url>
      <bibkey>simon-2024-register</bibkey>
      <doi>10.18653/v1/2024.figlang-1.14</doi>
    </paper>
    <paper id="15">
      <title>Report on the Multilingual Euphemism Detection Task</title>
      <author><first>Patrick</first><last>Lee</last></author>
      <author><first>Anna</first><last>Feldman</last><affiliation>Montclair State University and Montclair State University</affiliation></author>
      <pages>110-114</pages>
      <abstract>This paper presents the Multilingual Euphemism Detection Shared Task for the Fourth Workshop on Figurative Language Processing (FigLang 2024) held in conjunction with NAACL 2024. Participants were invited to attempt the euphemism detection task on four different languages (American English, global Spanish, Yorùbá, and Mandarin Chinese): given input text containing a potentially euphemistic term (PET), determine if its use is euphemistic or not. We present the expanded datasets used for the shared task, summarize each team’s methods and findings, and analyze potential implications for future research.</abstract>
      <url hash="d9df10ab">2024.figlang-1.15</url>
      <bibkey>lee-feldman-2024-report</bibkey>
      <doi>10.18653/v1/2024.figlang-1.15</doi>
    </paper>
    <paper id="16">
      <title>A Report on the <fixed-case>F</fixed-case>ig<fixed-case>L</fixed-case>ang 2024 Shared Task on Multimodal Figurative Language</title>
      <author><first>Shreyas</first><last>Kulkarni</last></author>
      <author><first>Arkadiy</first><last>Saakyan</last><affiliation>Columbia University</affiliation></author>
      <author><first>Tuhin</first><last>Chakrabarty</last><affiliation>Columbia University</affiliation></author>
      <author><first>Smaranda</first><last>Muresan</last><affiliation>Columbia University</affiliation></author>
      <pages>115-119</pages>
      <abstract>We present the outcomes of the Multimodal Figurative Language Shared Task held at the 4th Workshop on Figurative Language Processing (FigLang 2024) co-located at NAACL 2024. The task utilized the V-FLUTE dataset which is comprised of <tex-math>&lt;</tex-math>image, text<tex-math>&gt;</tex-math> pairs that use figurative language and includes detailed textual explanations for the entailment or contradiction relationship of each pair. The challenge for participants was to develop models capable of accurately identifying the visual entailment relationship in these multimodal instances and generating persuasive free-text explanations. The results showed that the participants’ models significantly outperformed the initial baselines in both automated and human evaluations. We also provide an overview of the systems submitted and analyze the results of the evaluations. All participating systems outperformed the LLaVA-ZS baseline, provided by us in F1-score.</abstract>
      <url hash="53498f00">2024.figlang-1.16</url>
      <bibkey>kulkarni-etal-2024-report</bibkey>
      <doi>10.18653/v1/2024.figlang-1.16</doi>
    </paper>
  </volume>
</collection>
