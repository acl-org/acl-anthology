<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.genaik">
  <volume id="1" ingest-date="2025-01-24" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)</booktitle>
      <editor><first>Genet Asefa</first><last>Gesese</last></editor>
      <editor><first>Harald</first><last>Sack</last></editor>
      <editor><first>Heiko</first><last>Paulheim</last></editor>
      <editor><first>Albert</first><last>Merono-Penuela</last></editor>
      <editor><first>Lihu</first><last>Chen</last></editor>
      <publisher>International Committee on Computational Linguistics</publisher>
      <address>Abu Dhabi, UAE</address>
      <month>January</month>
      <year>2025</year>
      <url hash="802dd17e">2025.genaik-1</url>
      <venue>genaik</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="94f7cda8">2025.genaik-1.0</url>
      <bibkey>genaik-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Effective Modeling of Generative Framework for Document-level Relational Triple Extraction</title>
      <author><first>Pratik</first><last>Saini</last></author>
      <author><first>Tapas</first><last>Nayak</last></author>
      <pages>1–12</pages>
      <abstract>Document-level relation triple extraction (DocRTE) is a complex task that involves three key sub-tasks: entity mention extraction, entity clustering, and relation triple extraction. Past work has applied discriminative models to address these three sub-tasks, either by training them sequentially in a pipeline fashion or jointly training them. However, while end-to-end discriminative or generative models have proven effective for sentence-level relation triple extraction, they cannot be trivially extended to the document level, as they only handle relation extraction without addressing the remaining two sub-tasks, entity mention extraction or clustering. In this paper, we propose a three-stage generative framework leveraging a pre-trained BART model to address all three tasks required for document-level relation triple extraction. Tested on the widely used DocRED dataset, our approach outperforms previous generative methods and achieves competitive performance against discriminative models.</abstract>
      <url hash="8c3f1730">2025.genaik-1.1</url>
      <bibkey>saini-nayak-2025-effective</bibkey>
    </paper>
    <paper id="2">
      <title>Learn Together: Joint Multitask Finetuning of Pretrained <fixed-case>KG</fixed-case>-enhanced <fixed-case>LLM</fixed-case> for Downstream Tasks</title>
      <author><first>Anastasia</first><last>Martynova</last></author>
      <author><first>Vladislav</first><last>Tishin</last></author>
      <author><first>Natalia</first><last>Semenova</last></author>
      <pages>13–19</pages>
      <abstract>Recent studies have shown that a knowledge graph (KG) can enhance text data by providing structured background knowledge, which can significantly improve the language understanding skills of the LLM. Besides, finetuning of such models shows solid results on commonsense reasoning benchmarks. In this work, we introduce expandable Joint Multitask Finetuning on Pretrained KG-enchanced LLM approach for Question Answering (QA), Machine Reading Comprehension (MRC) and Knowledge Graph Question Answering (KGQA) tasks. Extensive experiments show competitive performance of joint finetuning QA+MRC+KGQA over single task approach with a maximum gain of 30% accuracy.</abstract>
      <url hash="d5f3e28d">2025.genaik-1.2</url>
      <bibkey>martynova-etal-2025-learn</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>GNET</fixed-case>-<fixed-case>QG</fixed-case>: Graph Network for Multi-hop Question Generation</title>
      <author><first>Samin</first><last>Jamshidi</last></author>
      <author><first>Yllias</first><last>Chali</last></author>
      <pages>20–26</pages>
      <abstract>Multi-hop question generation is a challenging task in natural language processing (NLP) that requires synthesizing information from multiple sources. We propose GNET-QG, a novel approach that integrates Graph Attention Networks (GAT) with sequence-to-sequence models, enabling structured reasoning over multiple information sources to generate complex questions. Our experiments demonstrate that GNET-QG outperforms previous state-of-the-art models across several evaluation metrics, particularly excelling in METEOR, showing its effectiveness in enhancing machine reasoning capabilities.</abstract>
      <url hash="8a4ab76c">2025.genaik-1.3</url>
      <bibkey>jamshidi-chali-2025-gnet</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>SKETCH</fixed-case>: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval</title>
      <author><first>Aakash</first><last>Mahalingam</last></author>
      <author><first>Vinesh Kumar</first><last>Gande</last></author>
      <author><first>Aman</first><last>Chadha</last></author>
      <author><first>Vinija</first><last>Jain</last></author>
      <author><first>Divya</first><last>Chaudhary</last></author>
      <pages>27–42</pages>
      <abstract>This paper discusses about the SKETCH approach which enhances text retrieval and context relevancy on large corpuses compared to the traditional baseline methods. The abstract attached below discusses this further. Abstract: Retrieval-Augmented Generation (RAG) systems have become pivotal in leveraging vast corpora to generate informed and contextually relevant responses, notably reducing hallucinations in Large Language Models. Despite significant advancements, these systems struggle to efficiently process and retrieve information from large datasets while maintaining a comprehensive understanding of the context. This paper introduces SKETCH, a novel methodology that enhances the RAG retrieval process by integrating semantic text retrieval with knowledge graphs, thereby merging structured and unstructured data for a more holistic comprehension. SKETCH, demonstrates substantial improvements in retrieval performance and maintains superior context integrity compared to traditional methods. Evaluated across four diverse datasets: QuALITY, QASPER, NarrativeQA, and Italian Cuisine—SKETCH consistently outperforms baseline approaches on key RAGAS metrics such as answer relevancy, faithfulness, context precision and context recall. Notably, on the Italian Cuisine dataset, SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99, representing the highest performance across all evaluated metrics. These results highlight SKETCH’s capability in delivering more accurate and contextually relevant responses, setting new benchmarks for future retrieval systems.</abstract>
      <url hash="585d9a36">2025.genaik-1.4</url>
      <bibkey>mahalingam-etal-2025-sketch</bibkey>
    </paper>
    <paper id="5">
      <title>On Reducing Factual Hallucinations in Graph-to-Text Generation Using Large Language Models</title>
      <author><first>Dmitrii</first><last>Iarosh</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Mikhail</first><last>Salnikov</last></author>
      <pages>43–53</pages>
      <abstract>Recent work in Graph-to-Text generation has achieved impressive results, but it still suffers from hallucinations in some cases, despite extensive pretraining stages and various methods for working with graph data. While the commonly used metrics for evaluating the quality of Graph-to-Text models show almost perfect results, it makes it challenging to compare different approaches. This paper demonstrates the challenges of recent Graph-to-Text systems in terms of hallucinations and proposes a simple yet effective approach to using a general LLM, which has shown state-of-the-art results and reduced the number of factual hallucinations. We provide step-by-step instructions on how to develop prompts for language models and a detailed analysis of potential factual errors in the generated text.</abstract>
      <url hash="849221e1">2025.genaik-1.5</url>
      <bibkey>iarosh-etal-2025-reducing</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>G</fixed-case>raph<fixed-case>RAG</fixed-case>: Leveraging Graph-Based Efficiency to Minimize Hallucinations in <fixed-case>LLM</fixed-case>-Driven <fixed-case>RAG</fixed-case> for Finance Data</title>
      <author><first>Mariam</first><last>Barry</last></author>
      <author><first>Gaetan</first><last>Caillaut</last></author>
      <author><first>Pierre</first><last>Halftermeyer</last></author>
      <author><first>Raheel</first><last>Qader</last></author>
      <author><first>Mehdi</first><last>Mouayad</last></author>
      <author><first>Fabrice</first><last>Le Deit</last></author>
      <author><first>Dimitri</first><last>Cariolaro</last></author>
      <author><first>Joseph</first><last>Gesnouin</last></author>
      <pages>54–65</pages>
      <abstract>This study explores the integration of graph-based methods into Retrieval-Augmented Generation (RAG) systems to enhance efficiency, reduce hallucinations, and improve explainability, with a particular focus on financial and regulatory document retrieval. We propose two strategies—FactRAG and HybridRAG—which leverage knowledge graphs to improve RAG performance. Experiments conducted using Finance Bench, a benchmark for AI in finance, demonstrate that these approaches achieve a 6% reduction in hallucinations and an 80% decrease in token usage compared to conventional RAG methods. Furthermore, we evaluate HybridRAG by comparing the Digital Operational Resilience Act (DORA) from the European Union with the Federal Financial Institutions Examination Council (FFIEC) guidelines from the United States. The results reveal a significant improvement in computational efficiency, reducing contradiction detection complexity from <tex-math>O(n^2)</tex-math> to <tex-math>O(k \cdot n)</tex-math>—where <tex-math>n</tex-math> is the number of chunks—and a remarkable 734-fold decrease in token consumption. Graph-based retrieval methods can improve the efficiency and cost-effectiveness of large language model (LLM) applications, though their performance and token usage depend on the dataset, knowledge graph design, and retrieval task.</abstract>
      <url hash="b81c985f">2025.genaik-1.6</url>
      <bibkey>barry-etal-2025-graphrag</bibkey>
    </paper>
    <paper id="7">
      <title>Structured Knowledge meets <fixed-case>G</fixed-case>en<fixed-case>AI</fixed-case>: A Framework for Logic-Driven Language Models</title>
      <author><first>Farida Helmy</first><last>Eldessouky</last></author>
      <author><first>Nourhan</first><last>Ehab</last></author>
      <author><first>Carolin</first><last>Schindler</last></author>
      <author><first>Mervat</first><last>Abuelkheir</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <pages>66–68</pages>
      <abstract>Large Language Models (LLMs) excel at generating fluent text but struggle with context sensitivity, logical reasoning, and personalization without extensive fine-tuning. This paper presents a logical modulator: an adaptable communication layer between Knowledge Graphs (KGs) and LLMs as a way to address these limitations. Unlike direct KG-LLM integrations, our modulator is domain-agnostic and incorporates logical dependencies and commonsense reasoning in order to achieve contextual personalization. By enhancing KG interaction, this method will produce linguistically coherent and logically sound outputs, increasing interpretability and reliability in generative AI.</abstract>
      <url hash="cf957733">2025.genaik-1.7</url>
      <bibkey>eldessouky-etal-2025-structured</bibkey>
    </paper>
    <paper id="8">
      <title>Performance and Limitations of Fine-Tuned <fixed-case>LLM</fixed-case>s in <fixed-case>SPARQL</fixed-case> Query Generation</title>
      <author><first>Thamer</first><last>Mecharnia</last></author>
      <author><first>Mathieu</first><last>d’Aquin</last></author>
      <pages>69–77</pages>
      <abstract>Generative AI has simplified information access by enabling natural language-driven interactions between users and automated systems. In particular, Question Answering (QA) has emerged as a key application of AI, facilitating efficient access to complex information through dialogue systems and virtual assistants. The Large Language Models (LLMs) combined with Knowledge Graphs (KGs) have further enhanced QA systems, allowing them to not only correctly interpret natural language but also retrieve precise answers from structured data sources such as Wikidata and DBpedia. However, enabling LLMs to generate machine-readable SPARQL queries from natural language questions (NLQs) remains challenging, particularly for complex questions. In this study, we present experiments in fine-tuning LLMs for the task of NLQ-to-SPARQL transformation. We rely on benchmark datasets for training and testing the fine-tuned models, generating queries directly from questions written in English (without further processing of the input or output). By conducting an analytical study, we examine the effectiveness of each model, as well as the limitations associated with using fine-tuned LLMs to generate SPARQL.</abstract>
      <url hash="79223336">2025.genaik-1.8</url>
      <bibkey>mecharnia-daquin-2025-performance</bibkey>
    </paper>
    <paper id="9">
      <title>Refining Noisy Knowledge Graph with Large Language Models</title>
      <author><first>Na</first><last>Dong</last></author>
      <author><first>Natthawut</first><last>Kertkeidkachorn</last></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Kiyoaki</first><last>Shirai</last></author>
      <pages>78–86</pages>
      <abstract>Knowledge graphs (KGs) represent structured real-world information composed by triplets of head entity, relation, and tail entity. These graphs can be constructed automatically from text or manually curated. However, regardless of the construction method, KGs often suffer from misinformation, incompleteness, and noise, which hinder their reliability and utility. This study addresses the challenge of noisy KGs, where incorrect or misaligned entities and relations degrade graph quality. Leveraging recent advancements in large language models (LLMs) with strong capabilities across diverse tasks, we explore their potential to detect and refine noise in KGs. Specifically, we propose a novel method, LLM_sim, to enhance the detection and refinement of noisy triples. Our results confirm the effectiveness of this approach in elevating KG quality in noisy environments. Additionally, we apply our proposed method to Knowledge Graph Completion (KGC), a downstream KG task that aims to predict missing links and improve graph completeness. Traditional KGC methods assume that KGs are noise-free, which is unrealistic in practical scenarios. Our experiments analyze the impact of varying noise levels on KGC performance, revealing that LLMs can mitigate noise by identifying and refining incorrect entries, thus enhancing KG quality.</abstract>
      <url hash="a446a71b">2025.genaik-1.9</url>
      <bibkey>dong-etal-2025-refining</bibkey>
    </paper>
    <paper id="10">
      <title>Can <fixed-case>LLM</fixed-case>s be Knowledge Graph Curators for Validating Triple Insertions?</title>
      <author><first>André Gomes</first><last>Regino</last></author>
      <author><first>Julio Cesar</first><last>dos Reis</last></author>
      <pages>87–99</pages>
      <abstract>As Knowledge Graphs (KGs) become central to modern applications, automated methods for validating RDF triples before insertion into these graphs are essential. The complexity and scalability challenges in manual validation processes have led researchers to explore Large Language Models (LLMs) as potential automated validators. This study investigates the feasibility of using LLMs to validate RDF triples by focusing on four distinct and complementary validation tasks: class and property alignment, URI standardization, semantic consistency, and syntactic correctness. We propose a systematic validation method that uses prompts to guide LLMs through each stage of the triple evaluation of the RDF. In our experiments, four models are evaluated across these tasks. Our results reveal that more advanced models like Llama-3-70B-Instruct offer superior accuracy and consistency. Our findings emphasize the practical open challenges of deploying LLMs in real-world RDF validation scenarios, including domain generalization, semantic drift, and the need for human-in-the-loop interventions. This investigation advances the research on the refinement and integration of LLM-based RDF validation techniques into KG management workflows.</abstract>
      <url hash="8d099abb">2025.genaik-1.10</url>
      <bibkey>regino-dos-reis-2025-llms</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>T</fixed-case>ext2<fixed-case>C</fixed-case>ypher: Bridging Natural Language and Graph Databases</title>
      <author><first>Makbule Gulcin</first><last>Ozsoy</last></author>
      <author><first>Leila</first><last>Messallem</last></author>
      <author><first>Jon</first><last>Besga</last></author>
      <author><first>Gianandrea</first><last>Minneci</last></author>
      <pages>100–108</pages>
      <abstract>Knowledge graphs use nodes, relationships, and properties to represent arbitrarily complex data. When stored in a graph database, the Cypher query language enables efficient modeling and querying of knowledge graphs. However, using Cypher requires specialized knowledge, which can present a challenge for non-expert users. Our work Text2Cypher aims to bridge this gap by translating natural language queries into Cypher query language and extending the utility of knowledge graphs to non-technical expert users. While large language models (LLMs) can be used for this purpose, they often struggle to capture complex nuances, resulting in incomplete or incorrect outputs. Fine-tuning LLMs on domain-specific datasets has proven to be a more promising approach, but the limited availability of high-quality, publicly available Text2Cypher datasets makes this challenging. In this work, we show how we combined, cleaned and organized several publicly available datasets into a total of 44,387 instances, enabling effective fine-tuning and evaluation. Models fine-tuned on this dataset showed significant performance gains, with improvements in Google-BLEU and Exact Match scores over baseline models, highlighting the importance of high-quality datasets and fine-tuning in improving Text2Cypher performance.</abstract>
      <url hash="f86037d4">2025.genaik-1.11</url>
      <bibkey>ozsoy-etal-2025-text2cypher</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>KGF</fixed-case>ake<fixed-case>N</fixed-case>et: A Knowledge Graph-Enhanced Model for Fake News Detection</title>
      <author><first>Anuj</first><last>Kumar</last></author>
      <author><first>Pardeep</first><last>Kumar</last></author>
      <author><first>Abhishek</first><last>Yadav</last></author>
      <author><first>Satyadev</first><last>Ahlawat</last></author>
      <author><first>Yamuna</first><last>Prasad</last></author>
      <pages>109–122</pages>
      <abstract>The proliferation of fake news on social media has intensified the spread of misinformation, promoting societal biases, hate, and violence. While recent advancements in Generative AI (GenAI), particularly large language models (LLMs), have shown promise, these models often need more structured representation for accurate verification, as they rely on pre-trained data patterns without access to real-time or validated information. This study presents a framework that utilizes Open Information Extractor 6 (OpenIE6) to extract triplet relationships (subject-predicate-object) from statements and justifications to compute the cosine similarity between the Knowledge Graphs (KGs) of the statements and their supporting justification to precisely measure the relevance and alignment between them. This similarity feature is integrated with an attention mechanism over GenAI-generated embeddings to enhance the model’s ability to capture semantic features accurately. In addition, a Multi-Layer Perceptron (MLP) classifier is employed to integrate all features, resulting in a 4% improvement in accuracy and a 5% increase in F1-score over state-of-the-art LLM-based approaches.</abstract>
      <url hash="18d2dac9">2025.genaik-1.12</url>
      <bibkey>kumar-etal-2025-kgfakenet</bibkey>
    </paper>
    <paper id="13">
      <title>Style Knowledge Graph: Augmenting Text Style Transfer with Knowledge Graphs</title>
      <author><first>Martina</first><last>Toshevska</last></author>
      <author><first>Slobodan</first><last>Kalajdziski</last></author>
      <author><first>Sonja</first><last>Gievska</last></author>
      <pages>123–135</pages>
      <abstract>Text style transfer is the task of modifying the stylistic attributes of a given text while preserving its original meaning. This task has also gained interest with the advent of large language models. Although knowledge graph augmentation has been explored in various tasks, its potential for enhancing text style transfer has received limited attention. This paper proposes a method to create a Style Knowledge Graph (SKG) to facilitate and improve text style transfer. The SKG captures words, their attributes, and relations in a particular style, that serves as a knowledge resource to augment text style transfer. We conduct baseline experiments to evaluate the effectiveness of the SKG for augmenting text style transfer by incorporating relevant parts from the SKG in the prompt. The preliminary results demonstrate its potential for enhancing content preservation and style transfer strength in text style transfer tasks, while the results on fluency indicate promising outcomes with some room for improvement. We hope that the proposed SKG and the initial experiments will inspire further research in the field.</abstract>
      <url hash="7178bd87">2025.genaik-1.13</url>
      <bibkey>toshevska-etal-2025-style</bibkey>
    </paper>
    <paper id="14">
      <title>Entity Quality Enhancement in Knowledge Graphs through <fixed-case>LLM</fixed-case>-based Question Answering</title>
      <author><first>Morteza</first><last>Kamaladdini Ezzabady</last></author>
      <author><first>Farah</first><last>Benamara</last></author>
      <pages>136–145</pages>
      <abstract>Most models for triple extraction from texts primarily focus on named entities. However, real-world applications often comprise non-named entities that pose serious challenges for entity linking and disambiguation. We focus on these entities and propose the first LLM-based entity revision framework to improve the quality of extracted triples via a multi-choice question-answering mechanism. When evaluated on two benchmark datasets, our results show a significant improvement, thereby generating more reliable triples for knowledge graphs.</abstract>
      <url hash="3c7ac461">2025.genaik-1.14</url>
      <bibkey>kamaladdini-ezzabady-benamara-2025-entity</bibkey>
    </paper>
    <paper id="15">
      <title>Multilingual Skill Extraction for Job Vacancy–Job Seeker Matching in Knowledge Graphs</title>
      <author><first>Hamit</first><last>Kavas</last></author>
      <author><first>Marc</first><last>Serra-Vidal</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <pages>146–155</pages>
      <abstract>In the modern labor market, accurate matching of job vacancies with suitable candidate CVs is critical. We present a novel multilingual knowledge graph-based framework designed to enhance the matching by accurately extracting the skills requested by a job and provided by a job seeker in a multilingual setting and aligning them via the standardized skill labels of the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy. The proposed framework employs a combination of state-of-the-art techniques to extract relevant skills from job postings and candidate experiences. These extracted skills are then filtered and mapped to the ESCO taxonomy and integrated into a multilingual knowledge graph that incorporates hierarchical relationships and cross-linguistic variations through embeddings. Our experiments demonstrate a significant improvement of the matching quality compared to the state of the art.</abstract>
      <url hash="338d41eb">2025.genaik-1.15</url>
      <bibkey>kavas-etal-2025-multilingual</bibkey>
    </paper>
  </volume>
</collection>
