<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.bucc">
  <volume id="1" ingest-date="2021-11-09">
    <meta>
      <booktitle>Proceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021)</booktitle>
      <editor><first>Reinhard</first><last>Rapp</last></editor>
      <editor><first>Serge</first><last>Sharoff</last></editor>
      <editor><first>Pierre</first><last>Zweigenbaum</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Online (Virtual Mode)</address>
      <month>September</month>
      <year>2021</year>
      <venue>bucc</venue>
    </meta>
    <frontmatter>
      <url hash="f525bf72">2021.bucc-1.0</url>
      <bibkey>bucc-2021-building</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Invited Presentation</title>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>1</pages>
      <abstract>AI now and in future will have to grapple continuously with the problem of low resource. AI will increasingly be ML intensive. But ML needs data often with annotation. However, annotation is costly. Over the years, through work on multiple problems, we have developed insight into how to do language processing in low resource setting. Following 6 methods—individually and in combination—seem to be the way forward: 1) Artificially augment resource (e.g. subwords) 2) Cooperative NLP (e.g., pivot in MT) 3) Linguistic embellishment (e.g. factor based MT, source reordering) 4) Joint Modeling (e.g., Coref and NER, Sentiment and Emotion: each task helping the other to either boost accuracy or reduce resource requirement) 5) Multimodality (e.g., eye tracking based NLP, also picture+text+speech based Sentiment Analysis) 6)Cross Lingual Embedding (e.g., embedding from multiple languages helping MT, close to 2 above) The present talk will focus on low resource machine translation. We describe the use of techniques from the above list and bring home the seriousness and methodology of doing Machine Translation in low resource settings.</abstract>
      <url hash="60f94858">2021.bucc-1.1</url>
      <bibkey>bhattacharyya-2021-invited</bibkey>
    </paper>
    <paper id="2">
      <title>Mining Bilingual Word Pairs from Comparable Corpus using <fixed-case>A</fixed-case>pache Spark Framework</title>
      <author><first>Sanjanasri</first><last>Jp</last></author>
      <author><first>Vijay Krishna</first><last>Menon</last></author>
      <author><first>Soman</first><last>Kp</last></author>
      <author><first>Krzysztof</first><last>Wolk</last></author>
      <pages>2–7</pages>
      <abstract>Bilingual dictionaries are essential resources in many areas of natural language processing tasks, but resource-scarce and less popular language pairs rarely have such. Efficient automatic methods for inducting bilingual dictionaries are needed as manual resources and efforts are scarce for low-resourced languages. In this paper, we induce word translations using bilingual embedding. We use the Apache Spark framework for parallel computation. Further, to validate the quality of the generated bilingual dictionary, we use it in a phrase-table aided Neural Machine Translation (NMT) system. The system can perform moderately well with a manual bilingual dictionary; we change this into our inducted dictionary. The corresponding translated outputs are compared using the Bilingual Evaluation Understudy (BLEU) and Rank-based Intuitive Bilingual Evaluation Score (RIBES) metrics.</abstract>
      <url hash="cb1641d9">2021.bucc-1.2</url>
      <bibkey>jp-etal-2021-mining</bibkey>
    </paper>
    <paper id="3">
      <title>Effective Bitext Extraction From Comparable Corpora Using a Combination of Three Different Approaches</title>
      <author><first>Steinþór</first><last>Steingrímsson</last></author>
      <author><first>Pintu</first><last>Lohar</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>8–17</pages>
      <abstract>Parallel sentences extracted from comparable corpora can be useful to supplement parallel corpora when training machine translation (MT) systems. This is even more prominent in low-resource scenarios, where parallel corpora are scarce. In this paper, we present a system which uses three very different measures to identify and score parallel sentences from comparable corpora. We measure the accuracy of our methods in low-resource settings by comparing the results against manually curated test data for English–Icelandic, and by evaluating an MT system trained on the concatenation of the parallel data extracted by our approach and an existing data set. We show that the system is capable of extracting useful parallel sentences with high accuracy, and that the extracted pairs substantially increase translation quality of an MT system trained on the data, as measured by automatic evaluation metrics.</abstract>
      <url hash="4f34426b">2021.bucc-1.3</url>
      <bibkey>steingrimsson-etal-2021-effective</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="4">
      <title>Syntax-aware Transformers for Neural Machine Translation: The Case of Text to Sign Gloss Translation</title>
      <author><first>Santiago</first><last>Egea Gómez</last></author>
      <author><first>Euan</first><last>McGill</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>18–27</pages>
      <abstract>It is well-established that the preferred mode of communication of the deaf and hard of hearing (DHH) community are Sign Languages (SLs), but they are considered low resource languages where natural language processing technologies are of concern. In this paper we study the problem of text to SL gloss Machine Translation (MT) using Transformer-based architectures. Despite the significant advances of MT for spoken languages in the recent couple of decades, MT is in its infancy when it comes to SLs. We enrich a Transformer-based architecture aggregating syntactic information extracted from a dependency parser to word-embeddings. We test our model on a well-known dataset showing that the syntax-aware model obtains performance gains in terms of MT evaluation metrics.</abstract>
      <url hash="2ddf419a">2021.bucc-1.4</url>
      <bibkey>egea-gomez-etal-2021-syntax</bibkey>
      <pwccode url="https://github.com/lastus-taln-upf/syntax-aware-transformer-text2gloss" additional="false">lastus-taln-upf/syntax-aware-transformer-text2gloss</pwccode>
    </paper>
    <paper id="5">
      <title>Employing <fixed-case>W</fixed-case>ikipedia as a resource for Named Entity Recognition in Morphologically complex under-resourced languages</title>
      <author><first>Aravind</first><last>Krishnan</last></author>
      <author><first>Stefan</first><last>Ziehe</last></author>
      <author><first>Franziska</first><last>Pannach</last></author>
      <author><first>Caroline</first><last>Sporleder</last></author>
      <pages>28–39</pages>
      <abstract>We propose a novel approach for rapid prototyping of named entity recognisers through the development of semi-automatically annotated datasets. We demonstrate the proposed pipeline on two under-resourced agglutinating languages: the Dravidian language Malayalam and the Bantu language isiZulu. Our approach is weakly supervised and bootstraps training data from Wikipedia and Google Knowledge Graph. Moreover, our approach is relatively language independent and can consequently be ported quickly (and hence cost-effectively) from one language to another, requiring only minor language-specific tailoring.</abstract>
      <url hash="949c0b5b">2021.bucc-1.5</url>
      <bibkey>krishnan-etal-2021-employing</bibkey>
    </paper>
    <paper id="6">
      <title>Semi-Automated Labeling of Requirement Datasets for Relation Extraction</title>
      <author><first>Jeremias</first><last>Bohn</last></author>
      <author><first>Jannik</first><last>Fischbach</last></author>
      <author><first>Martin</first><last>Schmitt</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <author><first>Andreas</first><last>Vogelsang</last></author>
      <pages>40–45</pages>
      <abstract>Creating datasets manually by human annotators is a laborious task that can lead to biased and inhomogeneous labels. We propose a flexible, semi-automatic framework for labeling data for relation extraction. Furthermore, we provide a dataset of preprocessed sentences from the requirements engineering domain, including a set of automatically created as well as hand-crafted labels. In our case study, we compare the human and automatic labels and show that there is a substantial overlap between both annotations.</abstract>
      <url hash="ca862a62">2021.bucc-1.6</url>
      <bibkey>bohn-etal-2021-semi</bibkey>
    </paper>
    <paper id="7">
      <title>Majority Voting with Bidirectional Pre-translation For Bitext Retrieval</title>
      <author><first>Alexander</first><last>Jones</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>46–59</pages>
      <abstract>Obtaining high-quality parallel corpora is of paramount importance for training NMT systems. However, as many language pairs lack adequate gold-standard training data, a popular approach has been to mine so-called “pseudo-parallel” sentences from paired documents in two languages. In this paper, we outline some drawbacks with current methods that rely on an embedding similarity threshold, and propose a heuristic method in its place. Our method involves translating both halves of a paired corpus before mining, and then performing a majority vote on sentence pairs mined in three ways: after translating documents in language x to language y, after translating language y to x, and using the original documents in languages x and y. We demonstrate success with this novel approach on the Tatoeba similarity search benchmark in 64 low-resource languages, and on NMT in Kazakh and Gujarati. We also uncover the effect of resource-related factors (i.e. how much monolingual/bilingual data is available for a given language) on the optimal choice of bitext mining method, demonstrating that there is currently no one-size-fits-all approach for this task. We make the code and data used in our experiments publicly available.</abstract>
      <url hash="9e9ffbc4">2021.bucc-1.7</url>
      <bibkey>jones-wijaya-2021-majority</bibkey>
      <pwccode url="https://github.com/AlexJonesNLP/alt-bitexts" additional="false">AlexJonesNLP/alt-bitexts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bucc">BUCC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tatoeba">Tatoeba</pwcdataset>
    </paper>
    <paper id="8">
      <title><fixed-case>EM</fixed-case> Corpus: a comparable corpus for a less-resourced language pair <fixed-case>M</fixed-case>anipuri-<fixed-case>E</fixed-case>nglish</title>
      <author><first>Rudali</first><last>Huidrom</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <author><first>Khogendra</first><last>Khomdram</last></author>
      <pages>60–67</pages>
      <abstract>In this paper, we introduce a sentence-level comparable text corpus crawled and created for the less-resourced language pair, Manipuri(mni) and English (eng). Our monolingual corpora comprise 1.88 million Manipuri sentences and 1.45 million English sentences, and our parallel corpus comprises 124,975 Manipuri-English sentence pairs. These data were crawled and collected over a year from August 2020 to March 2021 from a local newspaper website called ‘The Sangai Express.’ The resources reported in this paper are made available to help the low-resourced languages community for MT/NLP tasks.</abstract>
      <url hash="a4e1b714">2021.bucc-1.8</url>
      <bibkey>huidrom-etal-2021-em</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
    </paper>
    <paper id="9">
      <title>On Pronunciations in <fixed-case>W</fixed-case>iktionary: Extraction and Experiments on Multilingual Syllabification and Stress Prediction</title>
      <author><first>Winston</first><last>Wu</last></author>
      <author><first>David</first><last>Yarowsky</last></author>
      <pages>68–74</pages>
      <abstract>We constructed parsers for five non-English editions of Wiktionary, which combined with pronunciations from the English edition, comprises over 5.3 million IPA pronunciations, the largest pronunciation lexicon of its kind. This dataset is a unique comparable corpus of IPA pronunciations annotated from multiple sources. We analyze the dataset, noting the presence of machine-generated pronunciations. We develop a novel visualization method to quantify syllabification. We experiment on the new combined task of multilingual IPA syllabification and stress prediction, finding that training a massively multilingual neural sequence-to-sequence model with copy attention can improve performance on both high- and low-resource languages, and multi-task training on stress prediction helps with syllabification.</abstract>
      <url hash="0a8e6918">2021.bucc-1.9</url>
      <bibkey>wu-yarowsky-2021-pronunciations</bibkey>
    </paper>
    <paper id="10">
      <title>A <fixed-case>D</fixed-case>utch Dataset for Cross-lingual Multilabel Toxicity Detection</title>
      <author><first>Ben</first><last>Burtenshaw</last></author>
      <author><first>Mike</first><last>Kestemont</last></author>
      <pages>75–79</pages>
      <abstract>Multi-label toxicity detection is highly prominent, with many research groups, companies, and individuals engaging with it through shared tasks and dedicated venues. This paper describes a cross-lingual approach to annotating multi-label text classification on a newly developed Dutch language dataset, using a model trained on English data. We present an ensemble model of one Transformer model and an LSTM using Multilingual embeddings. The combination of multilingual embeddings and the Transformer model improves performance in a cross-lingual setting.</abstract>
      <url hash="ae0ef078">2021.bucc-1.10</url>
      <bibkey>burtenshaw-kestemont-2021-dutch</bibkey>
    </paper>
  </volume>
</collection>
