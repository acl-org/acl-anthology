<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.insights">
  <volume id="1" ingest-date="2023-12-21" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Insights from Negative Results in NLP</booktitle>
      <editor><first>Shabnam</first><last>Tafreshi</last></editor>
      <editor><first>Arjun</first><last>Akula</last></editor>
      <editor><first>João</first><last>Sedoc</last></editor>
      <editor><first>Aleksandr</first><last>Drozd</last></editor>
      <editor><first>Anna</first><last>Rogers</last></editor>
      <editor><first>Anna</first><last>Rumshisky</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dubrovnik, Croatia</address>
      <month>May</month>
      <year>2023</year>
      <url hash="1840c003">2023.insights-1</url>
      <venue>insights</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="8bc87b86">2023.insights-1.0</url>
      <bibkey>insights-2023-insights</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in <fixed-case>NLP</fixed-case></title>
      <author><first>Anya</first><last>Belz</last><affiliation>ADAPT Research Centre, Dublin City University</affiliation></author>
      <author><first>Craig</first><last>Thomson</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Ehud</first><last>Reiter</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Gavin</first><last>Abercrombie</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Jose M.</first><last>Alonso-Moral</last><affiliation>Universidade de Santiago de Compostela</affiliation></author>
      <author><first>Mohammad</first><last>Arvan</last><affiliation>University of Illinois Chicago</affiliation></author>
      <author><first>Anouck</first><last>Braggaar</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Mark</first><last>Cieliebak</last><affiliation>Zurich University of Applied Sciences</affiliation></author>
      <author><first>Elizabeth</first><last>Clark</last><affiliation>Google Research</affiliation></author>
      <author><first>Kees</first><last>van Deemter</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Tanvi</first><last>Dinkar</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Ondřej</first><last>Dušek</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>Steffen</first><last>Eger</last><affiliation>Bielefeld University</affiliation></author>
      <author><first>Qixiang</first><last>Fang</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Mingqi</first><last>Gao</last><affiliation>Peking University</affiliation></author>
      <author><first>Albert</first><last>Gatt</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Dimitra</first><last>Gkatzia</last><affiliation>Edinburgh Napier University</affiliation></author>
      <author><first>Javier</first><last>González-Corbelle</last><affiliation>Universidade de Santiago de Compostela</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Manuela</first><last>Hürlimann</last><affiliation>Zurich University of Applied Sciences</affiliation></author>
      <author><first>Takumi</first><last>Ito</last><affiliation>Tohoku University</affiliation></author>
      <author><first>John D.</first><last>Kelleher</last><affiliation>Technological University Dublin</affiliation></author>
      <author><first>Filip</first><last>Klubicka</last><affiliation>Technological University Dublin</affiliation></author>
      <author><first>Emiel</first><last>Krahmer</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Huiyuan</first><last>Lai</last><affiliation>Groningen University</affiliation></author>
      <author><first>Chris</first><last>van der Lee</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Yiru</first><last>Li</last><affiliation>Groningen University</affiliation></author>
      <author><first>Saad</first><last>Mahamood</last><affiliation>trivago</affiliation></author>
      <author><first>Margot</first><last>Mieskes</last><affiliation>University of Applied Sciences Darmstadt</affiliation></author>
      <author><first>Emiel</first><last>van Miltenburg</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Pablo</first><last>Mosteiro</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Malvina</first><last>Nissim</last><affiliation>Groningen University</affiliation></author>
      <author><first>Natalie</first><last>Parde</last><affiliation>University of Illinois Chicago</affiliation></author>
      <author><first>Ondřej</first><last>Plátek</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>Verena</first><last>Rieser</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Jie</first><last>Ruan</last><affiliation>Peking University</affiliation></author>
      <author><first>Joel</first><last>Tetreault</last><affiliation>Dataminr</affiliation></author>
      <author><first>Antonio</first><last>Toral</last><affiliation>Groningen University</affiliation></author>
      <author><first>Xiaojun</first><last>Wan</last><affiliation>Peking University</affiliation></author>
      <author><first>Leo</first><last>Wanner</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author><first>Lewis</first><last>Watson</last><affiliation>Edinburgh Napier University</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Georgia Tech</affiliation></author>
      <pages>1-10</pages>
      <abstract>We report our efforts in identifying a set of previous human evaluations in NLP that would be suitable for a coordinated study examining what makes human evaluations in NLP more/less reproducible. We present our results and findings, which include that just 13% of papers had (i) sufficiently low barriers to reproduction, and (ii) enough obtainable information, to be considered for reproduction, and that all but one of the experiments we selected for reproduction was discovered to have flaws that made the meaningfulness of conducting a reproduction questionable. As a result, we had to change our coordinated study design from a reproduce approach to a standardise-then-reproduce-twice approach. Our overall (negative) finding that the great majority of human evaluations in NLP is not repeatable and/or not reproducible and/or too flawed to justify reproduction, paints a dire picture, but presents an opportunity for a rethink about how to design and report human evaluations in NLP.</abstract>
      <url hash="ff25c0af">2023.insights-1.1</url>
      <bibkey>belz-etal-2023-missing</bibkey>
      <video href="2023.insights-1.1.mp4"/>
      <doi>10.18653/v1/2023.insights-1.1</doi>
      <revision id="1" href="2023.insights-1.1v1" hash="5f5f9172"/>
      <revision id="2" href="2023.insights-1.1v2" hash="ff25c0af" date="2023-12-09">Authors change.</revision>
    </paper>
    <paper id="2">
      <title><fixed-case>ERATE</fixed-case>: Efficient Retrieval Augmented Text Embeddings</title>
      <author><first>Vatsal</first><last>Raina</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Nora</first><last>Kassner</last><affiliation>Meta AI</affiliation></author>
      <author><first>Kashyap</first><last>Popat</last><affiliation>Facebook Inc.</affiliation></author>
      <author><first>Patrick</first><last>Lewis</last><affiliation>Cohere, University College London</affiliation></author>
      <author><first>Nicola</first><last>Cancedda</last><affiliation>Meta</affiliation></author>
      <author><first>Louis</first><last>Martin</last><affiliation>Meta AI</affiliation></author>
      <pages>11-18</pages>
      <abstract>Embedding representations of text are useful for downstream natural language processing tasks. Several universal sentence representation methods have been proposed with a particular focus on self-supervised pre-training approaches to leverage the vast quantities of unlabelled data. However, there are two challenges for generating rich embedding representations for a new document. 1) The latest rich embedding generators are based on very large costly transformer-based architectures. 2) The rich embedding representation of a new document is limited to only the information provided without access to any explicit contextual and temporal information that could potentially further enrich the representation. We propose efficient retrieval-augmented text embeddings (ERATE) that tackles the first issue and offers a method to tackle the second issue. To the best of our knowledge, we are the first to incorporate retrieval to general purpose embeddings as a new paradigm, which we apply to the semantic similarity tasks of SentEval. Despite not reaching state-of-the-art performance, ERATE offers key insights that encourages future work into investigating the potential of retrieval-based embeddings.</abstract>
      <url hash="8270255e">2023.insights-1.2</url>
      <bibkey>raina-etal-2023-erate</bibkey>
      <video href="2023.insights-1.2.mp4"/>
      <doi>10.18653/v1/2023.insights-1.2</doi>
    </paper>
    <paper id="3">
      <title>A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets</title>
      <author><first>Iva</first><last>Bojic</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Josef</first><last>Halim</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Verena</first><last>Suharman</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Sreeja</first><last>Tar</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Qi Chwen</first><last>Ong</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Duy</first><last>Phung</last><affiliation>VinAI Research Vietnam</affiliation></author>
      <author><first>Mathieu</first><last>Ravaut</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>Nanyang Technological University; Salesforce AI Research</affiliation></author>
      <author><first>Josip</first><last>Car</last><affiliation>LKCMedicine, NTU Singapore</affiliation></author>
      <pages>19-32</pages>
      <abstract>Low-quality data can cause downstream problems in high-stakes applications. Data-centric approach emphasizes on improving dataset quality to enhance model performance. High-quality datasets are needed for general-purpose Large Language Models (LLMs) training, as well as for domain-specific models, which are usually small in size as it is costly to engage a large number of domain experts for their creation. Thus, it is vital to ensure high-quality domain-specific training data. In this paper, we propose a framework for enhancing the data quality of original datasets. (Code and dataset are available at https://github.com/IvaBojic/framework). We applied the proposed framework to four biomedical datasets and showed relative improvement of up to 33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when using back translation to enhance the original dataset quality.</abstract>
      <url hash="f7a5876b">2023.insights-1.3</url>
      <bibkey>bojic-etal-2023-data</bibkey>
      <video href="2023.insights-1.3.mp4"/>
      <doi>10.18653/v1/2023.insights-1.3</doi>
    </paper>
    <paper id="4">
      <title>Encoding Sentence Position in Context-Aware Neural Machine Translation with Concatenation</title>
      <author><first>Lorenzo</first><last>Lupo</last><affiliation>LIG</affiliation></author>
      <author><first>Marco</first><last>Dinarelli</last><affiliation>LIG</affiliation></author>
      <author><first>Laurent</first><last>Besacier</last><affiliation>Naver Labs Europe</affiliation></author>
      <pages>33-44</pages>
      <abstract>Context-aware translation can be achieved by processing a concatenation of consecutive sentences with the standard Transformer architecture. This paper investigates the intuitive idea of providing the model with explicit information about the position of the sentences contained in the concatenation window. We compare various methods to encode sentence positions into token representations, including novel methods. Our results show that the Transformer benefits from certain sentence position encoding methods on English to Russian translation, if trained with a context-discounted loss. However, the same benefits are not observed on English to German. Further empirical efforts are necessary to define the conditions under which the proposed approach is beneficial.</abstract>
      <url hash="939d268f">2023.insights-1.4</url>
      <bibkey>lupo-etal-2023-encoding</bibkey>
      <video href="2023.insights-1.4.mp4"/>
      <doi>10.18653/v1/2023.insights-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>S</fixed-case>oc<fixed-case>BERT</fixed-case>: A Pretrained Model for Social Media Text</title>
      <author><first>Yuting</first><last>Guo</last><affiliation>Emory University</affiliation></author>
      <author><first>Abeed</first><last>Sarker</last><affiliation>Emory University</affiliation></author>
      <pages>45-52</pages>
      <abstract>Pretrained language models (PLMs) on domain-specific data have been proven to be effective for in-domain natural language processing (NLP) tasks. Our work aimed to develop a language model which can be effective for the NLP tasks with the data from diverse social media platforms. We pretrained a language model on Twitter and Reddit posts in English consisting of 929M sequence blocks for 112K steps. We benchmarked our model and 3 transformer-based models—BERT, BERTweet, and RoBERTa on 40 social media text classification tasks. The results showed that although our model did not perform the best on all of the tasks, it outperformed the baseline model—BERT on most of the tasks, which illustrates the effectiveness of our model. Also, our work provides some insights of how to improve the efficiency of training PLMs.</abstract>
      <url hash="3849d81f">2023.insights-1.5</url>
      <bibkey>guo-sarker-2023-socbert</bibkey>
      <video href="2023.insights-1.5.mp4"/>
      <doi>10.18653/v1/2023.insights-1.5</doi>
    </paper>
    <paper id="6">
      <title>Edit Aware Representation Learning via <fixed-case>L</fixed-case>evenshtein Prediction</title>
      <author><first>Edison</first><last>Marrese-taylor</last><affiliation>National Institute of Advanced Industrial Science and Technology (AIST)</affiliation></author>
      <author><first>Machel</first><last>Reid</last><affiliation>Google</affiliation></author>
      <author><first>Alfredo</first><last>Solano</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>53-58</pages>
      <url hash="6379ce78">2023.insights-1.6</url>
      <bibkey>marrese-taylor-etal-2023-edit</bibkey>
      <video href="2023.insights-1.6.mp4"/>
      <doi>10.18653/v1/2023.insights-1.6</doi>
    </paper>
    <paper id="7">
      <title>What changes when you randomly choose <fixed-case>BPE</fixed-case> merge operations? Not much.</title>
      <author><first>Jonne</first><last>Saleva</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Constantine</first><last>Lignos</last><affiliation>Brandeis University</affiliation></author>
      <pages>59-66</pages>
      <abstract>We introduce two simple randomized variants of byte pair encoding (BPE) and explore whether randomizing the selection of merge operations substantially affects a downstream machine translation task. We focus on translation into morphologically rich languages, hypothesizing that this task may show sensitivity to the method of choosing subwords. Analysis using a Bayesian linear model indicates that one variant performs nearly indistinguishably compared to standard BPE while the other degrades performance less than we anticipated. We conclude that although standard BPE is widely used, there exists an interesting universe of potential variations on it worth investigating. Our code is available at: https://github.com/bltlab/random-bpe.</abstract>
      <url hash="d8a90db2">2023.insights-1.7</url>
      <bibkey>saleva-lignos-2023-changes</bibkey>
      <video href="2023.insights-1.7.mp4"/>
      <doi>10.18653/v1/2023.insights-1.7</doi>
    </paper>
    <paper id="8">
      <title>Hiding in Plain Sight: Insights into Abstractive Text Summarization</title>
      <author><first>Vivek</first><last>Srivastava</last><affiliation>TCS Research</affiliation></author>
      <author><first>Savita</first><last>Bhat</last><affiliation>TCS Research</affiliation></author>
      <author><first>Niranjan</first><last>Pedanekar</last><affiliation>TCS Research</affiliation></author>
      <pages>67-74</pages>
      <abstract>In recent years, there has been growing interest in the field of abstractive text summarization with focused contributions in relevant model architectures, datasets, and evaluation metrics. Despite notable research advances, previous works have identified certain limitations concerning the quality of datasets and the effectiveness of evaluation techniques for generated summaries. In this context, we examine these limitations further with the help of three quality measures, namely, Information Coverage, Entity Hallucination, and Summarization Complexity. As a part of this work, we investigate two widely used datasets (XSUM and CNNDM) and three existing models (BART, PEGASUS, and BRIO) and report our findings. Some key insights are: 1) Cumulative ROUGE score is an inappropriate evaluation measure since few high-scoring samples dominate the overall performance, 2) Existing summarization models have limited capability for information coverage and hallucinate to generate factual information, and 3) Compared to the model generated summaries, the reference summaries have lowest information coverage and highest entity hallucinations reiterating the need of new and better reference summaries.</abstract>
      <url hash="7806197a">2023.insights-1.8</url>
      <video href="2023.insights-1.8.mp4"/>
      <doi>10.18653/v1/2023.insights-1.8</doi>
      <bibkey>srivastava-etal-2023-hiding</bibkey>
    </paper>
    <paper id="9">
      <title>Annotating <fixed-case>P</fixed-case>ub<fixed-case>M</fixed-case>ed Abstracts with <fixed-case>M</fixed-case>e<fixed-case>SH</fixed-case> Headings using Graph Neural Network</title>
      <author><first>Faizan</first><last>Mustafa</last><affiliation>Quibiq GmbH</affiliation></author>
      <author><first>Rafika</first><last>Boutalbi</last><affiliation>Universität Stuttgart</affiliation></author>
      <author><first>Anastasiia</first><last>Iurshina</last><affiliation>Universität Stuttgart</affiliation></author>
      <pages>75-81</pages>
      <abstract>The number of scientific publications in the biomedical domain is continuously increasing with time. An efficient system for indexing these publications is required to make the information accessible according to the user’s information needs. Task 10a of the BioASQ challenge aims to classify PubMed articles according to the MeSH ontology so that new publications can be grouped with similar preexisting publications in the field without the assistance of time-consuming and costly annotations by human annotators. In this work, we use Graph Neural Network (GNN) in the link prediction setting to exploit potential graph-structured information present in the dataset which could otherwise be neglected by transformer-based models. Additionally, we provide error analysis and a plausible reason for the substandard performance achieved by GNN.</abstract>
      <url hash="dd8c40b3">2023.insights-1.9</url>
      <bibkey>mustafa-etal-2023-annotating</bibkey>
      <video href="2023.insights-1.9.mp4"/>
      <doi>10.18653/v1/2023.insights-1.9</doi>
    </paper>
    <paper id="10">
      <title>Do not Trust the Experts - How the Lack of Standard Complicates <fixed-case>NLP</fixed-case> for Historical <fixed-case>I</fixed-case>rish</title>
      <author><first>Oksana</first><last>Dereza</last><affiliation>Insight Centre for Data Analytics, Data Science Institute, University of Galway</affiliation></author>
      <author><first>Theodorus</first><last>Fransen</last><affiliation>Data Science Institute, Insight Centre for Data Analytics, National University of Ireland, Galway</affiliation></author>
      <author><first>John P.</first><last>Mccrae</last><affiliation>Insight Center for Data Analytics, National University of Ireland Galway</affiliation></author>
      <pages>82-87</pages>
      <abstract>In this paper, we describe how we unearthed some fundamental problems while building an analogy dataset modelled on BATS (Gladkova et al., 2016) to evaluate historical Irish embeddings on their ability to detect orthographic, morphological and semantic similarity.performance of our models in the analogy task was extremely poor regardless of the architecture, hyperparameters and evaluation metrics, while the qualitative evaluation revealed positive tendencies. argue that low agreement between field experts on fundamental lexical and orthographic issues, and the lack of a unified editorial standard in available resources make it impossible to build reliable evaluation datasets for computational models and obtain interpretable results. We emphasise the need for such a standard, particularly for NLP applications, and prompt Celticists and historical linguists to engage in further discussion. We would also like to draw NLP scholars’ attention to the role of data and its (extra)linguistic properties in testing new models, technologies and evaluation scenarios.</abstract>
      <url hash="a695570e">2023.insights-1.10</url>
      <bibkey>dereza-etal-2023-trust</bibkey>
      <video href="2023.insights-1.10.mp4"/>
      <doi>10.18653/v1/2023.insights-1.10</doi>
    </paper>
    <paper id="11">
      <title>Exploring the Reasons for Non-generalizability of <fixed-case>KBQA</fixed-case> systems</title>
      <author><first>Sopan</first><last>Khosla</last><affiliation>Amazon Web Services, Amazon Inc</affiliation></author>
      <author><first>Ritam</first><last>Dutt</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Vinayshekhar</first><last>Bannihatti Kumar</last><affiliation>AWS AI</affiliation></author>
      <author><first>Rashmi</first><last>Gangadharaiah</last><affiliation>AWS AI, Amazon</affiliation></author>
      <pages>88-93</pages>
      <abstract>Recent research has demonstrated impressive generalization capabilities of several Knowledge Base Question Answering (KBQA) models on the GrailQA dataset. We inspect whether these models can generalize to other datasets in a zero-shot setting. We notice a significant drop in performance and investigate the causes for the same. We observe that the models are dependent not only on the structural complexity of the questions, but also on the linguistic styles of framing a question. Specifically, the linguistic dimensions corresponding to explicitness, readability, coherence, and grammaticality have a significant impact on the performance of state-of-the-art KBQA models. Overall our results showcase the brittleness of such models and the need for creating generalizable systems.</abstract>
      <url hash="4f093246">2023.insights-1.11</url>
      <bibkey>khosla-etal-2023-exploring</bibkey>
      <video href="2023.insights-1.11.mp4"/>
      <doi>10.18653/v1/2023.insights-1.11</doi>
    </paper>
    <paper id="12">
      <title>An Empirical Study on Active Learning for Multi-label Text Classification</title>
      <author><first>Mengqi</first><last>Wang</last><affiliation>Deakin University</affiliation></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Deakin University</affiliation></author>
      <pages>94-102</pages>
      <abstract>Active learning has been widely used in the task of text classification for its ability to select the most valuable samples to annotate while improving the model performance. However, the efficiency of active learning in multi-label text classification tasks has been under-explored due to the label imbalanceness problem. In this paper, we conduct an empirical study of active learning on multi-label text classification and evaluate the efficiency of five active learning strategies on six multi-label text classification tasks. The experiments show that some strategies in the single-label setting especially in imbalanced datasets.</abstract>
      <url hash="b2c14d80">2023.insights-1.12</url>
      <bibkey>wang-liu-2023-empirical</bibkey>
      <video href="2023.insights-1.12.mp4"/>
      <doi>10.18653/v1/2023.insights-1.12</doi>
    </paper>
    <paper id="13">
      <title>What Does <fixed-case>BERT</fixed-case> actually Learn about Event Coreference? Probing Structural Information in a Fine-Tuned <fixed-case>D</fixed-case>utch Language Model</title>
      <author><first>Loic</first><last>De Langhe</last><affiliation>Ghent University</affiliation></author>
      <author><first>Orphee</first><last>De Clercq</last><affiliation>LT3, Ghent University</affiliation></author>
      <author><first>Veronique</first><last>Hoste</last><affiliation>LT3, Ghent University</affiliation></author>
      <pages>103-108</pages>
      <abstract>We probe structural and discourse aspects of coreferential relationships in a fine-tuned Dutch BERT event coreference model. Previous research has suggested that no such knowledge is encoded in BERT-based models and the classification of coreferential relationships ultimately rests on outward lexical similarity. While we show that BERT can encode a (very) limited number of these discourse aspects (thus disproving assumptions in earlier research), we also note that knowledge of many structural features of coreferential relationships is absent from the encodings generated by the fine-tuned BERT model.</abstract>
      <url hash="f24b1876">2023.insights-1.13</url>
      <bibkey>de-langhe-etal-2023-bert</bibkey>
    </paper>
    <paper id="14">
      <title>Estimating Numbers without Regression</title>
      <author><first>Avijit</first><last>Thawani</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jay</first><last>Pujara</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Ashwin</first><last>Kalyan</last><affiliation>Allen Institute for Artificial Intelligence (AI2)</affiliation></author>
      <pages>109-116</pages>
      <abstract>Despite recent successes in language models, their ability to represent numbers is insufficient. Humans conceptualize numbers based on their magnitudes, effectively projecting them on a number line; whereas subword tokenization fails to explicitly capture magnitude by splitting numbers into arbitrary chunks. To alleviate this shortcoming, alternative approaches have been proposed that modify numbers at various stages of the language modeling pipeline. These methods change either the (1) notation in which numbers are written (eg scientific vs decimal), the (2) vocabulary used to represent numbers or the entire (3) architecture of the underlying language model, to directly regress to a desired number. Previous work suggests that architectural change helps achieve state-of-the-art on number estimation but we find an insightful ablation - changing the model”s vocabulary instead (eg introduce a new token for numbers in range 10-100) is a far better trade-off. In the context of masked number prediction, a carefully designed tokenization scheme is both the simplest to implement and sufficient, ie with similar performance to the state-of-the-art approach that requires making significant architectural changes. Finally, we report similar trends on the downstream task of numerical fact estimation (for Fermi Problems) and discuss reasons behind our findings.</abstract>
      <url hash="4bb90ecd">2023.insights-1.14</url>
      <bibkey>thawani-etal-2023-estimating</bibkey>
    </paper>
  </volume>
</collection>
