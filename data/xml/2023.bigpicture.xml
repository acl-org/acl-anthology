<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.bigpicture">
  <volume id="1" ingest-date="2023-12-04" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Big Picture Workshop</booktitle>
      <editor><first>Yanai</first><last>Elazar</last></editor>
      <editor><first>Allyson</first><last>Ettinger</last></editor>
      <editor><first>Nora</first><last>Kassner</last></editor>
      <editor><first>Sebastian</first><last>Ruder</last></editor>
      <editor><first>Noah</first><last>A. Smith</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore</address>
      <month>December</month>
      <year>2023</year>
      <url hash="df053ca4">2023.bigpicture-1</url>
      <venue>bigpicture</venue>
    </meta>
    <frontmatter>
      <url hash="7024c7ea">2023.bigpicture-1.0</url>
      <bibkey>bigpicture-2023-big</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Where are We in Event-centric Emotion Analysis? Bridging Emotion Role Labeling and Appraisal-based Approaches</title>
      <author><first>Roman</first><last>Klinger</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>1-17</pages>
      <abstract>The term emotion analysis in text subsumes various natural language processing tasks which have in common the goal to enable computers to understand emotions. Most popular is emotion classification in which one or multiple emotions are assigned to a predefined textual unit. While such setting is appropriate for identifying the reader’s or author’s emotion, emotion role labeling adds the perspective of mentioned entities and extracts text spans that correspond to the emotion cause. The underlying emotion theories agree on one important point; that an emotion is caused by some internal or external event and comprises several subcomponents, including the subjective feeling and a cognitive evaluation. We therefore argue that emotions and events are related in two ways. (1) Emotions are events; and this perspective is the fundament in natural language processing for emotion role labeling. (2) Emotions are caused by events; a perspective that is made explicit with research how to incorporate psychological appraisal theories in NLP models to interpret events. These two research directions, role labeling and (event-focused) emotion classification, have by and large been tackled separately. In this paper, we contextualize both perspectives and discuss open research questions.</abstract>
      <url hash="c0888af3">2023.bigpicture-1.1</url>
      <bibkey>klinger-2023-event</bibkey>
      <doi>10.18653/v1/2023.bigpicture-1.1</doi>
    </paper>
    <paper id="2">
      <title>Working Towards Digital Documentation of <fixed-case>U</fixed-case>ralic Languages With Open-Source Tools and <fixed-case>M</fixed-case>odern <fixed-case>NLP</fixed-case> Methods</title>
      <author><first>Mika</first><last>Hämäläinen</last><affiliation>Rootroo Ltd</affiliation></author>
      <author><first>Jack</first><last>Rueter</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <author><first>Niko</first><last>Partanen</last><affiliation>University of Helsinki</affiliation></author>
      <pages>18-27</pages>
      <abstract>We present our work towards building an infrastructure for documenting endangered languages with the focus on Uralic languages in particular. Our infrastructure consists of tools to write dictionaries so that entries are structured in XML format. These dictionaries are the foundation for rule-based NLP tools such as FSTs. We also work actively towards enhancing these dictionaries and tools by using the latest state-of-the-art neural models by generating training data through rules and lexica</abstract>
      <url hash="db8e77f4">2023.bigpicture-1.2</url>
      <bibkey>hamalainen-etal-2023-working</bibkey>
      <doi>10.18653/v1/2023.bigpicture-1.2</doi>
    </paper>
    <paper id="3">
      <title>Computational Narrative Understanding: A Big Picture Analysis</title>
      <author><first>Andrew</first><last>Piper</last><affiliation>McGill University</affiliation></author>
      <pages>28-39</pages>
      <abstract>This paper provides an overview of outstanding major research goals for the field of computational narrative understanding. Storytelling is an essential human practice, one that provides a sense of personal meaning, shared sense of community, and individual enjoyment. A number of research domains have increasingly focused on storytelling as a key mechanism for explaining human behavior. Now is an opportune moment to provide a vision of the contributions that computational narrative understanding can make towards this collective endeavor and the challenges facing the field. In addition to providing an overview of the elements of narrative, this paper outlines three major lines of inquiry: understanding the multi-modality of narrative; the temporal patterning of narrative (narrative “shape”); and socio-cultural narrative schemas, i.e. collective narratives. The paper concludes with a call for more inter-disciplinary working groups and deeper investment in building cross-cultural and multi-modal narrative datasets.</abstract>
      <url hash="e991bcee">2023.bigpicture-1.3</url>
      <bibkey>piper-2023-computational</bibkey>
      <doi>10.18653/v1/2023.bigpicture-1.3</doi>
    </paper>
    <paper id="4">
      <title>The Case for Scalable, Data-Driven Theory: A Paradigm for Scientific Progress in <fixed-case>NLP</fixed-case></title>
      <author><first>Julian</first><last>Michael</last><affiliation>New York University</affiliation></author>
      <pages>40-52</pages>
      <abstract>I propose a paradigm for scientific progress in NLP centered around developing scalable, data-driven theories of linguistic structure. The idea is to collect data in tightly scoped, carefully defined ways which allow for exhaustive annotation of behavioral phenomena of interest, and then use machine learning to construct explanatory theories of these phenomena which can form building blocks for intelligible AI systems. After laying some conceptual groundwork, I describe several investigations into data-driven theories of shallow semantic structure using Question-Answer driven Semantic Role Labeling (QA-SRL), a schema for annotating verbal predicate-argument relations using highly constrained question-answer pairs. While this only scratches the surface of the complex language behaviors of interest in AI, I outline principles for data collection and theoretical modeling which can inform future scientific progress. This note summarizes and draws heavily on my PhD thesis.</abstract>
      <url hash="e7cc1dfb">2023.bigpicture-1.4</url>
      <bibkey>michael-2023-case</bibkey>
      <doi>10.18653/v1/2023.bigpicture-1.4</doi>
    </paper>
    <paper id="5">
      <title>Thesis Distillation: Investigating The Impact of Bias in <fixed-case>NLP</fixed-case> Models on Hate Speech Detection</title>
      <author><first>Fatma</first><last>Elsafoury</last></author>
      <pages>53-65</pages>
      <abstract>This paper is a summary of the work done in my PhD thesis. Where I investigate the impact of bias in NLP models on the task of hate speech detection from three perspectives: explainability, offensive stereotyping bias, and fairness. Then, I discuss the main takeaways from my thesis and how they can benefit the broader NLP community. Finally, I discuss important future research directions. The findings of my thesis suggest that the bias in NLP models impacts the task of hate speech detection from all three perspectives. And that unless we start incorporating social sciences in studying bias in NLP models, we will not effectively overcome the current limitations of measuring and mitigating bias in NLP models.</abstract>
      <url hash="c4f0455c">2023.bigpicture-1.5</url>
      <bibkey>elsafoury-2023-thesis</bibkey>
      <doi>10.18653/v1/2023.bigpicture-1.5</doi>
    </paper>
    <paper id="6">
      <title>Large Language Models as <fixed-case>S</fixed-case>ocio<fixed-case>T</fixed-case>echnical Systems</title>
      <author><first>Kaustubh</first><last>Dhole</last><affiliation>BITS Pilani</affiliation></author>
      <pages>66-79</pages>
      <abstract>The expectation of Large Language Models (LLMs) to solve various societal problems has ignored the larger socio-technical frame of reference under which they operate. From a socio-technical perspective, LLMs are necessary to look at separately from other ML models as they have radically different implications in society never witnessed before. In this article, we ground Selbst et al.(2019)’s five abstraction traps – The Framing Trap, The Portability Trap, The Formalism Trap, The Ripple Effect Trap and the Solutionism Trap in the context of LLMs discussing the problems associated with the abstraction and fairness of LLMs. Through learnings from previous studies and examples, we discuss each trap that LLMs fall into, and propose ways to address the points of LLM failure by gauging them from a socio-technical lens. We believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.</abstract>
      <url hash="571874e2">2023.bigpicture-1.6</url>
      <bibkey>dhole-2023-large</bibkey>
      <doi>10.18653/v1/2023.bigpicture-1.6</doi>
    </paper>
    <paper id="7">
      <title>Towards Low-resource Language Generation with Limited Supervision</title>
      <author><first>Kaushal</first><last>Maurya</last><affiliation>Indian Institute of Technology Hyderabad</affiliation></author>
      <author><first>Maunendra</first><last>Desarkar</last><affiliation>IIT Hyderabad and Indian Institute of Technology, Hyderabad,</affiliation></author>
      <pages>80-92</pages>
      <abstract>We present a research narrative aimed at enabling language technology for multiple natural language generation (NLG) tasks in low-resource languages (LRLs). With approximately 7,000 languages spoken globally, many lack the resources required for model training. NLG applications for LRLs present two additional key challenges: (i) The training is more pronounced, and (ii) Zero-shot modeling is a viable research direction for scalability; however, generating zero-shot well-formed text in target LRLs is challenging. Addressing these concerns, this narrative introduces three promising research explorations that serve as a step toward enabling language technology for many LRLs. These approaches make effective use of transfer learning and limited supervision techniques for modeling. Evaluations were conducted mostly in the zero-shot setting, enabling scalability. This research narrative is an ongoing doctoral thesis.</abstract>
      <url hash="2e1648fd">2023.bigpicture-1.7</url>
      <bibkey>maurya-desarkar-2023-towards</bibkey>
      <doi>10.18653/v1/2023.bigpicture-1.7</doi>
    </paper>
    <paper id="8">
      <title>Transformers as Graph-to-Graph Models</title>
      <author><first>James</first><last>Henderson</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Alireza</first><last>Mohammadshahi</last></author>
      <author><first>Andrei</first><last>Coman</last></author>
      <author><first>Lesly</first><last>Miculicich</last><affiliation>Google</affiliation></author>
      <pages>93-107</pages>
      <abstract>We argue that Transformers are essentially graph-to-graph models, with sequences just being a special case. Attention weights are functionally equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes this ability explicit, by inputting graph edges into the attention weight computations and predicting graph edges with attention-like functions, thereby integrating explicit graphs into the latent graphs learned by pretrained Transformers. Adding iterative graph refinement provides a joint embedding of input, output, and latent graphs, allowing non-autoregressive graph prediction to optimise the complete graph without any bespoke pipeline or decoding strategy. Empirical results show that this architecture achieves state-of-the-art accuracies for modelling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.</abstract>
      <url hash="ecc64868">2023.bigpicture-1.8</url>
      <bibkey>henderson-etal-2023-transformers</bibkey>
      <doi>10.18653/v1/2023.bigpicture-1.8</doi>
    </paper>
    <paper id="9">
      <title>It’s <fixed-case>MBR</fixed-case> All the Way Down: Modern Generation Techniques Through the Lens of Minimum <fixed-case>B</fixed-case>ayes Risk</title>
      <author><first>Amanda</first><last>Bertsch</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Alex</first><last>Xie</last></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Matthew</first><last>Gormley</last><affiliation>School of Computer Science, Carnegie Mellon University and 3M</affiliation></author>
      <pages>108-122</pages>
      <abstract>Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area.</abstract>
      <url hash="da8e5cdb">2023.bigpicture-1.9</url>
      <bibkey>bertsch-etal-2023-mbr</bibkey>
      <doi>10.18653/v1/2023.bigpicture-1.9</doi>
    </paper>
    <paper id="10">
      <title>Analyzing Pre-trained and Fine-tuned Language Models</title>
      <author><first>Marius</first><last>Mosbach</last><affiliation>Saarland University</affiliation></author>
      <pages>123-134</pages>
      <abstract>Since the introduction of transformer-based language models in 2018, the current generation of natural language processing (NLP) models continues to demonstrate impressive capabilities on a variety of academic benchmarks and real-world applications. This progress is based on a simple but general pipeline which consists of pre-training neural language models on large quantities of text, followed by an adaptation step that fine-tunes the pre-trained model to perform a specific NLP task of interest. However, despite the impressive progress on academic benchmarks and the widespread deployment of pre-trained and fine-tuned language models in industry we still lack a fundamental understanding of how and why pre-trained and fine-tuned language models work as well as the individual steps of the pipeline that produce them. We makes several contributions towards improving our understanding of pre-trained and fine-tuned language models, ranging from analyzing the linguistic knowledge of pre-trained language models and how it is affected by fine-tuning, to a rigorous analysis of the fine-tuning process itself and how the choice of adaptation technique affects the generalization of models and thereby provide new insights about previously unexplained phenomena and the capabilities of pre-trained and fine-tuned language models.</abstract>
      <url hash="79e0e450">2023.bigpicture-1.10</url>
      <bibkey>mosbach-2023-analyzing</bibkey>
      <doi>10.18653/v1/2023.bigpicture-1.10</doi>
    </paper>
  </volume>
</collection>
