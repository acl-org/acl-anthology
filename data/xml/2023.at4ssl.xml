<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.at4ssl">
  <volume id="1" ingest-date="2023-08-27" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second International Workshop on Automatic Translation for Signed and Spoken Languages</booktitle>
      <editor><first>Dimitar</first><last>Shterionov</last></editor>
      <editor><first>Mirella De</first><last>Sisto</last></editor>
      <editor><first>Mathias</first><last>Muller</last></editor>
      <editor><first>Davy Van</first><last>Landuyt</last></editor>
      <editor><first>Rehana</first><last>Omardeen</last></editor>
      <editor><first>Shaun</first><last>Oboyle</last></editor>
      <editor><first>Annelies</first><last>Braffort</last></editor>
      <editor><first>Floris</first><last>Roelofsen</last></editor>
      <editor><first>Fred</first><last>Blain</last></editor>
      <editor><first>Bram</first><last>Vanroy</last></editor>
      <editor><first>Eleftherios</first><last>Avramidis</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Tampere, Finland</address>
      <month>June</month>
      <year>2023</year>
      <url hash="31e2fe8a">2023.at4ssl-1</url>
      <venue>at4ssl</venue>
    </meta>
    <frontmatter>
      <url hash="6f2aee08">2023.at4ssl-1.0</url>
      <bibkey>at4ssl-2023-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Analyzing the Potential of Linguistic Features for Sign Spotting: A Look at Approximative Features</title>
      <author><first>Natalie</first><last>Hollain</last></author>
      <author><first>Martha</first><last>Larson</last></author>
      <author><first>Floris</first><last>Roelofsen</last></author>
      <pages>1–10</pages>
      <abstract>Sign language processing is the field of research that aims to recognize, retrieve, and spot signs in videos. Various approaches have been developed, varying in whether they use linguistic features and whether they use landmark detection tools or not. Incorporating linguistics holds promise for improving sign language processing in terms of performance, generalizability, and explainability. This paper focuses on the task of sign spotting and aims to expand on the approximative linguistic features that have been used in previous work, and to understand when linguistic features deliver an improvement over landmark features. We detect landmarks with Mediapipe and extract linguistically relevant features from them, including handshape, orientation, location, and movement. We compare a sign spotting model using linguistic features with a model operating on landmarks directly, finding that the approximate linguistic features tested in this paper capture some aspects of signs better than the landmark features, while they are worse for others.</abstract>
      <url hash="94b09a20">2023.at4ssl-1.1</url>
      <bibkey>hollain-etal-2023-analyzing</bibkey>
    </paper>
    <paper id="2">
      <title>A Linked Data Approach for linking and aligning Sign Language and Spoken Language Data</title>
      <author><first>Thierry</first><last>Declerck</last></author>
      <author><first>Sam</first><last>Bigeard</last></author>
      <author><first>Fahad</first><last>Khan</last></author>
      <author><first>Irene</first><last>Murtagh</last></author>
      <author><first>Sussi</first><last>Olsen</last></author>
      <author><first>Mike</first><last>Rosner</last></author>
      <author><first>Ineke</first><last>Schuurman</last></author>
      <author><first>Andon</first><last>Tchechmedjiev</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>11–21</pages>
      <abstract>We present work dealing with a Linked Open Data (LOD)-compliant representation of Sign Language (SL) data, with the goal of supporting the cross-lingual alignment of SL data and their linking to Spoken Language (SpL) data. The proposed representation is based on activities of groups of researchers in the field of SL who have investigated the use of Open Multilingual Wordnet (OMW) datasets for (manually) cross-linking SL data or for linking SL and SpL data. Another group of researchers is proposing an XML encoding of articulatory elements of SLs and (manually) linking those to an SpL lexical resource. We propose an RDF-based representation of those various data. This unified formal representation offers a semantic repository of information on SL and SpL data that could be accessed for supporting the creation of datasets for training or evaluating NLP applications dealing with SLs, thinking for example of Machine Translation (MT) between SLs and between SLs and SpLs.</abstract>
      <url hash="d1707be2">2023.at4ssl-1.2</url>
      <bibkey>declerck-etal-2023-linked</bibkey>
    </paper>
    <paper id="3">
      <title>An Open-Source Gloss-Based Baseline for Spoken to Signed Language Translation</title>
      <author><first>Amit</first><last>Moryossef</last></author>
      <author><first>Mathias</first><last>Müller</last></author>
      <author><first>Anne</first><last>Göhring</last></author>
      <author><first>Zifan</first><last>Jiang</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Sarah</first><last>Ebling</last></author>
      <pages>22–33</pages>
      <abstract>Sign language translation systems are complex and require many components. As a result, it is very hard to compare methods across publications. We present an open-source implementation of a text-to-gloss-to-pose-to-video pipeline approach, demonstrating conversion from German to Swiss German Sign Language, French to French Sign Language of Switzerland, and Italian to Italian Sign Language of Switzerland. We propose three different components for the text-to-gloss translation: a lemmatizer, a rule-based word reordering and dropping component, and a neural machine translation system. Gloss-to-pose conversion occurs using data from a lexicon for three different signed languages, with skeletal poses extracted from videos. To generate a sentence, the text-to-gloss system is first run, and the pose representations of the resulting signs are stitched together.</abstract>
      <url hash="6237b318">2023.at4ssl-1.3</url>
      <bibkey>moryossef-etal-2023-open</bibkey>
    </paper>
    <paper id="4">
      <title>A New <fixed-case>E</fixed-case>nglish-<fixed-case>D</fixed-case>utch-<fixed-case>NGT</fixed-case> Corpus for the Hospitality Domain</title>
      <author><first>Mirella De</first><last>Sisto</last></author>
      <author><first>Vincent</first><last>Vandeghinste</last></author>
      <author><first>Dimitar</first><last>Shterionov</last></author>
      <pages>34–37</pages>
      <abstract>One of the major challenges hampering the development of language technology which targets sign languages is the extremely limited availability of good quality data geared towards machine learning and deep learning approaches. In this paper we introduce the NGT-Dutch Hotel Review Corpus (NGT-HoReCo), which addresses this issue by providing multimodal parallel data in English, Dutch and Sign Language of the Netherlands (NGT). The corpus contains 283 hotel reviews in written English, translated into written Dutch and into NGT videos. It will be made publicly available through CLARIN and through the ELG platform.</abstract>
      <url hash="359ff3f6">2023.at4ssl-1.4</url>
      <bibkey>sisto-etal-2023-new</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>BSL</fixed-case>-<fixed-case>H</fixed-case>ansard: A parallel, multimodal corpus of <fixed-case>E</fixed-case>nglish and interpreted <fixed-case>B</fixed-case>ritish <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage data from parliamentary proceedings</title>
      <author><first>Euan</first><last>McGill</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>38–43</pages>
      <abstract>BSL-Hansard is a novel open source and multimodal resource composed by combining Sign Language video data in BSL and English text from the official transcription of British parliamentary sessions. This paper describes the method followed to compile BSL-Hansard including time alignment of text using the MAUS (Schiel, 2015) segmentation system, gives some statistics about this dataset, and suggests experiments. These primarily include end-to-end Sign Language-to-text translation, but is also relevant for broader machine translation, and speech and language processing tasks.</abstract>
      <url hash="25a25c9f">2023.at4ssl-1.5</url>
      <bibkey>mcgill-saggion-2023-bsl</bibkey>
    </paper>
    <paper id="6">
      <title>Towards Accommodating Gerunds within the Sign Language Lexicon</title>
      <author><first>Zaid</first><last>Mohammed</last></author>
      <author><first>Irene</first><last>Murtagh</last></author>
      <pages>44–47</pages>
      <abstract>This work is part of ongoing research work that focuses on the linguistic analysis and computational description of five different Sign Languages (SLs), namely Irish Sign Language (ISL), Flemish Sign Language (VGT), Dutch Sign Language (NGT), Spanish Sign Language (LSE), and British Sign Language (BSL). This work will be leveraged to inform the development of SL lexicon entries for a Sign Language Machine Translation (SLMT) system. In particular, this research focuses on ISL. We investigate the existence of constructions similar to or equivalent in functionality to gerunds in spoken language, in particular, English. The initial findings indicate that such constructions do indeed exist and that they can take many forms.</abstract>
      <url hash="c290724b">2023.at4ssl-1.6</url>
      <bibkey>mohammed-murtagh-2023-towards</bibkey>
      <revision id="1" href="2023.at4ssl-1.6v1" hash="360227f4"/>
      <revision id="2" href="2023.at4ssl-1.6v2" hash="c290724b" date="2023-09-11">Minor update.</revision>
    </paper>
  </volume>
</collection>
