<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.acl">
  <volume id="long" ingest-date="2024-08-27" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</booktitle>
      <editor><first>Lun-Wei</first><last>Ku</last><affiliation>Academia Sinica</affiliation></editor>
      <editor><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico / Instituto de Telecomunicações / Unbabel</affiliation></editor>
      <editor><first>Vivek</first><last>Srikumar</last><affiliation>University of Utah</affiliation></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="e8f6811d">2024.acl-long</url>
      <venue>acl</venue>
      <doi>10.18653/v1/2024.acl-long</doi>
    </meta>
    <frontmatter>
      <url hash="fb124a2f">2024.acl-long.0</url>
      <bibkey>acl-2024-long</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models</title>
      <author><first>Zhengxin</first><last>Zhang</last></author>
      <author><first>Dan</first><last>Zhao</last><affiliation>Peng Cheng Laborotary</affiliation></author>
      <author><first>Xupeng</first><last>Miao</last><affiliation>Purdue University</affiliation></author>
      <author><first>Gabriele</first><last>Oliaro</last></author>
      <author><first>Zhihao</first><last>Zhang</last></author>
      <author><first>Qing</first><last>Li</last><affiliation>Peng Cheng Laboratory</affiliation></author>
      <author><first>Yong</first><last>Jiang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhihao</first><last>Jia</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>1-17</pages>
      <abstract>Finetuning large language models (LLMs) has been empirically effective on a variety of downstream tasks. Existing approaches to finetuning an LLM either focus on parameter-efficient finetuning, which only updates a small number of trainable parameters, or attempt to reduce the memory footprint during the training phase of the finetuning. Typically, the memory footprint during finetuning stems from three contributors: model weights, optimizer states, and intermediate activations. However, existing works still require considerable memory, and none can simultaneously mitigate the memory footprint of all three sources. In this paper, we present quantized side tuing (QST), which enables memory-efficient and fast finetuning of LLMs by operating through a dual-stage process. First, QST quantizes an LLM’s model weights into 4-bit to reduce the memory footprint of the LLM’s original weights. Second, QST introduces a side network separated from the LLM, which utilizes the hidden states of the LLM to make task-specific predictions. Using a separate side network avoids performing back-propagation through the LLM, thus reducing the memory requirement of the intermediate activations. Finally, QST leverages several low-rank adaptors and gradient-free downsample modules to significantly reduce the trainable parameters, so as to save the memory footprint of the optimizer states. Experiments show that QST can reduce the total memory footprint by up to 2.3× and speed up the finetuning process by up to 3<tex-math>\times</tex-math> while achieving competent performance compared with the state-of-the-art. When it comes to full finetuning, QST can reduce the total memory footprint up to 7<tex-math>\times</tex-math>.</abstract>
      <url hash="17bb3556">2024.acl-long.1</url>
      <bibkey>zhang-etal-2024-quantized</bibkey>
      <doi>10.18653/v1/2024.acl-long.1</doi>
    </paper>
    <paper id="2">
      <title>Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal Utterances</title>
      <author><first>Hanlei</first><last>Zhang</last></author>
      <author><first>Hua</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Fei</first><last>Long</last></author>
      <author><first>Xin</first><last>Wang</last><affiliation>Hebei University of Science &amp; Technology</affiliation></author>
      <author><first>Kai</first><last>Gao</last></author>
      <pages>18-35</pages>
      <abstract>Discovering the semantics of multimodal utterances is essential for understanding human language and enhancing human-machine interactions. Existing methods manifest limitations in leveraging nonverbal information for discerning complex semantics in unsupervised scenarios. This paper introduces a novel unsupervised multimodal clustering method (UMC), making a pioneering contribution to this field. UMC introduces a unique approach to constructing augmentation views for multimodal data, which are then used to perform pre-training to establish well-initialized representations for subsequent clustering. An innovative strategy is proposed to dynamically select high-quality samples as guidance for representation learning, gauged by the density of each sample’s nearest neighbors. Besides, it is equipped to automatically determine the optimal value for the top-<tex-math>K</tex-math> parameter in each cluster to refine sample selection. Finally, both high- and low-quality samples are used to learn representations conducive to effective clustering. We build baselines on benchmark multimodal intent and dialogue act datasets. UMC shows remarkable improvements of 2-6% scores in clustering metrics over state-of-the-art methods, marking the first successful endeavor in this domain. The complete code and data are available at https://github.com/thuiar/UMC.</abstract>
      <url hash="b2fdf6b0">2024.acl-long.2</url>
      <bibkey>zhang-etal-2024-unsupervised</bibkey>
      <doi>10.18653/v1/2024.acl-long.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>MAGE</fixed-case>: Machine-generated Text Detection in the Wild</title>
      <author><first>Yafu</first><last>Li</last><affiliation>Westlake University</affiliation></author>
      <author><first>Qintong</first><last>Li</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Wei</first><last>Bi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhilin</first><last>Wang</last></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Linyi</first><last>Yang</last><affiliation>Westlake University</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>36-53</pages>
      <abstract>Large language models (LLMs) have achieved human-level text generation, emphasizing the need for effective deepfake text detection to mitigate risks like the spread of fake news and plagiarism. Existing research has been constrained by evaluating detection methods o specific domains or particular language models. In practical scenarios, however, the detector faces texts from various domains or LLMs without knowing their sources. To this end, we build a comprehensive testbed by gathering texts from diverse human writings and deepfake texts generated by different LLMs. Empirical results on mainstream detection methods demonstrate the difficulties associated with detecting deepfake text in a wide-ranging testbed, particularly in out-of-distribution scenarios. Such difficulties align with the diminishing linguistic differences between the two text sources. Despite challenges, the top-performing detector can identify 84.12% out-of-domain texts generated by a new LLM, indicating the feasibility for application scenarios.</abstract>
      <url hash="31eb75e1">2024.acl-long.3</url>
      <bibkey>li-etal-2024-mage</bibkey>
      <revision id="1" href="2024.acl-long.3v1" hash="5fdb577f"/>
      <revision id="2" href="2024.acl-long.3v2" hash="31eb75e1" date="2024-09-09">Minor updates.</revision>
      <doi>10.18653/v1/2024.acl-long.3</doi>
    </paper>
    <paper id="4">
      <title><fixed-case>P</fixed-case>riv<fixed-case>LM</fixed-case>-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models</title>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Dadi</first><last>Guo</last><affiliation>Peking University</affiliation></author>
      <author><first>Donghao</first><last>Li</last></author>
      <author><first>Wei</first><last>Fan</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Qi</first><last>Hu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Chunkit</first><last>Chan</last></author>
      <author><first>Duanyi</first><last>Yao</last></author>
      <author><first>Yuan</first><last>Yao</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>54-73</pages>
      <abstract>The rapid development of language models (LMs) brings unprecedented accessibility and usage for both models and users. On the one hand, powerful LMs achieve state-of-the-art performance over numerous downstream NLP tasks. On the other hand, more and more attention is paid to unrestricted model accesses that may bring malicious privacy risks of data leakage. To address these issues, many recent works propose privacy-preserving language models (PPLMs) with differential privacy (DP). Unfortunately, different DP implementations make it challenging for a fair comparison among existing PPLMs. In this paper, we present PrivLM-Bench, a multi-perspective privacy evaluation benchmark to empirically and intuitively quantify the privacy leakage of LMs. Instead of only reporting DP parameters, PrivLM-Bench sheds light on the neglected inference data privacy during actual usage. PrivLM-Bench first clearly defines multi-faceted privacy objectives. Then, PrivLM-Bench constructs a unified pipeline to perform private fine-tuning. Lastly, PrivLM-Bench performs existing privacy attacks on LMs with pre-defined privacy objectives as the empirical evaluation results. The empirical attack results are used to fairly and intuitively evaluate the privacy leakage of various PPLMs. We conduct extensive experiments on three datasets of GLUE for mainstream LMs.</abstract>
      <url hash="248f0edb">2024.acl-long.4</url>
      <bibkey>li-etal-2024-privlm</bibkey>
      <doi>10.18653/v1/2024.acl-long.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>G</fixed-case>en<fixed-case>T</fixed-case>ranslate: Large Language Models are Generative Multilingual Speech and Machine Translators</title>
      <author><first>Yuchen</first><last>Hu</last></author>
      <author><first>Chen</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Chao-Han</first><last>Yang</last><affiliation>NVIDIA Research</affiliation></author>
      <author><first>Ruizhe</first><last>Li</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Zhehuai</first><last>Chen</last></author>
      <author><first>EngSiong</first><last>Chng</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>74-90</pages>
      <abstract>Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely GenTranslate, which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the diverse N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model.</abstract>
      <url hash="343feea0">2024.acl-long.5</url>
      <bibkey>hu-etal-2024-gentranslate</bibkey>
      <doi>10.18653/v1/2024.acl-long.5</doi>
    </paper>
    <paper id="6">
      <title>Exploring Chain-of-Thought for Multi-modal Metaphor Detection</title>
      <author><first>Yanzhi</first><last>Xu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Yueying</first><last>Hua</last></author>
      <author><first>Shichen</first><last>Li</last></author>
      <author><first>Zhongqing</first><last>Wang</last><affiliation>Soochow University, China</affiliation></author>
      <pages>91-101</pages>
      <abstract>Metaphors are commonly found in advertising and internet memes. However, the free form of internet memes often leads to a lack of high-quality textual data. Metaphor detection demands a deep interpretation of both textual and visual elements, requiring extensive common-sense knowledge, which poses a challenge to language models. To address these challenges, we propose a compact framework called C4MMD, which utilizes a <b>C</b>hain-of-Thought(CoT) method <b>for</b> <b>M</b>ulti-modal <b>M</b>etaphor <b>D</b>etection. Specifically, our approach designs a three-step process inspired by CoT that extracts and integrates knowledge from Multi-modal Large Language Models(MLLMs) into smaller ones. We also developed a modality fusion architecture to transform knowledge from large models into metaphor features, supplemented by auxiliary tasks to improve model performance. Experimental results on the MET-MEME dataset demonstrate that our method not only effectively enhances the metaphor detection capabilities of small models but also outperforms existing models. To our knowledge, this is the first systematic study leveraging MLLMs in metaphor detection tasks. The code for our method is publicly available at <url>https://github.com/xyz189411yt/C4MMD</url>.</abstract>
      <url hash="269a2a85">2024.acl-long.6</url>
      <bibkey>xu-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.acl-long.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>B</fixed-case>it<fixed-case>D</fixed-case>istiller: Unleashing the Potential of Sub-4-Bit <fixed-case>LLM</fixed-case>s via Self-Distillation</title>
      <author><first>DaYou</first><last>Du</last><affiliation>HKUST(GZ)</affiliation></author>
      <author><first>Yijia</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shijie</first><last>Cao</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Jiaqi</first><last>Guo</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ting</first><last>Cao</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Xiaowen</first><last>Chu</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Ningyi</first><last>Xu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>102-116</pages>
      <abstract>The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit configurations on general language understanding and complex reasoning benchmarks. Notably, BitDistiller is shown to be more cost-effective, demanding fewer data and training resources. The code is available at https://github.com/DD-DuDa/BitDistiller.</abstract>
      <url hash="23198452">2024.acl-long.7</url>
      <bibkey>du-etal-2024-bitdistiller</bibkey>
      <doi>10.18653/v1/2024.acl-long.7</doi>
    </paper>
    <paper id="8">
      <title>A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation</title>
      <author><first>Kai</first><last>Chen</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Ye</first><last>Wang</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Yitong</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Aiping</first><last>Li</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Han</first><last>Yu</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Xin</first><last>Song</last></author>
      <pages>117-132</pages>
      <abstract>Temporal knowledge graph (TKG) reasoning has two settings: interpolation reasoning and extrapolation reasoning. Both of them draw plenty of research interest and have great significance. Methods of the former de-emphasize the temporal correlations among facts sequences, while methods of the latter require strict chronological order of knowledge and ignore inferring clues provided by missing facts of the past. These limit the practicability of TKG applications as almost all of the existing TKG reasoning methods are designed specifically to address either one setting. To this end, this paper proposes an original Temporal PAth-based Reasoning (TPAR) model for both the interpolation and extrapolation reasoning settings. TPAR performs a neural-driven symbolic reasoning fashion that is robust to ambiguous and noisy temporal data, and with fine interpretability as well. Comprehensive experiments show that TPAR outperforms SOTA methods on the link prediction task for both the interpolation and the extrapolation settings. A novel pipeline experimental setting is designed to evaluate the performances of SOTA combinations and the proposed TPAR towards interpolation and extrapolation reasoning. And more diverse experiments are conducted to show the robustness and interpretability of TPAR.</abstract>
      <url hash="d491ebce">2024.acl-long.8</url>
      <bibkey>chen-etal-2024-unified</bibkey>
      <doi>10.18653/v1/2024.acl-long.8</doi>
    </paper>
    <paper id="9">
      <title>Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation</title>
      <author><first>Shicheng</first><last>Xu</last></author>
      <author><first>Liang</first><last>Pang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Mo</first><last>Yu</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>133-145</pages>
      <abstract>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignore it or be misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as “Information Refiner”, which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named INFO-RAG that optimizes LLMs for RAG in an unsupervised manner. INFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that INFO-RAG improves the performance of LLaMA2 by an average of 9.39% relative points. INFO-RAG also shows advantages in in-context learning and robustness of RAG.</abstract>
      <url hash="e91c75a3">2024.acl-long.9</url>
      <bibkey>xu-etal-2024-unsupervised</bibkey>
      <doi>10.18653/v1/2024.acl-long.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>CSCD</fixed-case>-<fixed-case>NS</fixed-case>: a <fixed-case>C</fixed-case>hinese Spelling Check Dataset for Native Speakers</title>
      <author><first>Yong</first><last>Hu</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>146-159</pages>
      <abstract>In this paper, we present CSCD-NS, the first Chinese spelling check (CSC) dataset designed for native speakers, containing 40,000 samples from a Chinese social platform. Compared with existing CSC datasets aimed at Chinese learners, CSCD-NS is ten times larger in scale and exhibits a distinct error distribution, with a significantly higher proportion of word-level errors. To further enhance the data resource, we propose a novel method that simulates the input process through an input method, generating large-scale and high-quality pseudo data that closely resembles the actual error distribution and outperforms existing methods. Moreover, we investigate the performance of various models in this scenario, including large language models (LLMs), such as ChatGPT. The result indicates that generative models underperform BERT-like classification models due to strict length and pronunciation constraints. The high prevalence of word-level errors also makes CSC for native speakers challenging enough, leaving substantial room for improvement.</abstract>
      <url hash="cab519fb">2024.acl-long.10</url>
      <bibkey>hu-etal-2024-cscd</bibkey>
      <doi>10.18653/v1/2024.acl-long.10</doi>
    </paper>
    <paper id="11">
      <title>Evaluating Dynamic Topic Models</title>
      <author><first>Charu</first><last>Karakkaparambil James</last></author>
      <author><first>Mayank</first><last>Nagda</last></author>
      <author><first>Nooshin</first><last>Haji Ghassemi</last><affiliation>Universität Kaiserslautern</affiliation></author>
      <author><first>Marius</first><last>Kloft</last><affiliation>RPTU Kaiserslautern-Landau</affiliation></author>
      <author><first>Sophie</first><last>Fellenz</last><affiliation>Universität Kaiserslautern</affiliation></author>
      <pages>160-176</pages>
      <abstract>There is a lack of quantitative measures to evaluate the progression of topics through time in dynamic topic models (DTMs). Filling this gap, we propose a novel evaluation measure for DTMs that analyzes the changes in the quality of each topic over time. Additionally, we propose an extension combining topic quality with the model’s temporal consistency. We demonstrate the utility of the proposed measure by applying it to synthetic data and data from existing DTMs, including DTMs from large language models (LLMs). We also show that the proposed measure correlates well with human judgment. Our findings may help in identifying changing topics, evaluating different DTMs and LLMs, and guiding future research in this area.</abstract>
      <url hash="a213f8c6">2024.acl-long.11</url>
      <bibkey>karakkaparambil-james-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.acl-long.11</doi>
    </paper>
    <paper id="12">
      <title>How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition</title>
      <author><first>Guanting</first><last>Dong</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Hongyi</first><last>Yuan</last></author>
      <author><first>Keming</first><last>Lu</last></author>
      <author><first>Chengpeng</first><last>Li</last></author>
      <author><first>Mingfeng</first><last>Xue</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Dayiheng</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chang</first><last>Zhou</last></author>
      <author><first>Jingren</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <pages>177-198</pages>
      <abstract>Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, codegeneration, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specificially focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.</abstract>
      <url hash="c3cc16e2">2024.acl-long.12</url>
      <bibkey>dong-etal-2024-abilities</bibkey>
      <doi>10.18653/v1/2024.acl-long.12</doi>
    </paper>
    <paper id="13">
      <title>Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification</title>
      <author><first>Shanshan</first><last>Xu</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Santosh</first><last>T.y.s.s</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Oana</first><last>Ichim</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München and IT University of Copenhagen</affiliation></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technische Universität München</affiliation></author>
      <pages>199-216</pages>
      <abstract>In legal decisions, split votes (SV) occur when judges cannot reach a unanimous decision, posing a difficulty for lawyers who must navigate diverse legal arguments and opinions. In high-stakes domains, %as human-AI interaction systems become increasingly important, understanding the alignment of perceived difficulty between humans and AI systems is crucial to build trust. However, existing NLP calibration methods focus on a classifier’s awareness of predictive performance, measured against the human majority class, overlooking inherent human label variation (HLV). This paper explores split votes as naturally observable human disagreement and value pluralism. We collect judges’ vote distributions from the European Court of Human Rights (ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with SV information. We build a taxonomy of disagreement with SV-specific subcategories. We further assess the alignment of perceived difficulty between models and humans, as well as confidence- and human-calibration of COC models. We observe limited alignment with the judge vote distribution. To our knowledge, this is the first systematic exploration of calibration to human judgements in legal NLP. Our study underscores the necessity for further research on measuring and enhancing model calibration considering HLV in legal decision tasks.</abstract>
      <url hash="424b7f0d">2024.acl-long.13</url>
      <bibkey>xu-etal-2024-lens</bibkey>
      <doi>10.18653/v1/2024.acl-long.13</doi>
    </paper>
    <paper id="14">
      <title>Inference to the Best Explanation in Large Language Models</title>
      <author><first>Dhairya</first><last>Dalal</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <author><first>Paul</first><last>Buitelaar</last><affiliation>University of Galway</affiliation></author>
      <pages>217-235</pages>
      <abstract>While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes <i>IBE-Eval</i>, a framework inspired by philosophical accounts on <i>Inference to the Best Explanation (IBE)</i> to advance the interpretation and evaluation of LLMs’ explanations. <i>IBE-Eval</i> estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: <i>consistency</i>, <i>parsimony</i>, <i>coherence</i>, and <i>uncertainty</i>. Extensive experiments are conducted on <i>Causal Question Answering (CQA)</i>, where <i>IBE-Eval</i> is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that <i>IBE-Eval</i> can successfully identify the best explanation with up to 77% accuracy (<tex-math>\approx 27\%</tex-math> above random), improving upon a GPT 3.5-as-a-Judge baseline (<tex-math>\approx+17\%</tex-math>) while being intrinsically more efficient and interpretable. Additional analyses suggest that, despite model-specific variances, LLM-generated explanations tend to conform to IBE criteria and that <i>IBE-Eval</i> is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools.</abstract>
      <url hash="daf5113a">2024.acl-long.14</url>
      <bibkey>dalal-etal-2024-inference</bibkey>
      <doi>10.18653/v1/2024.acl-long.14</doi>
    </paper>
    <paper id="15">
      <title>A Novel Cartography-Based Curriculum Learning Method Applied on <fixed-case>R</fixed-case>o<fixed-case>NLI</fixed-case>: The First <fixed-case>R</fixed-case>omanian Natural Language Inference Corpus</title>
      <author><first>Eduard</first><last>Poesina</last></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Radu</first><last>Ionescu</last><affiliation>Universitatea Bucuresti</affiliation></author>
      <pages>236-253</pages>
      <abstract>Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is an actively studied topic serving as a proxy for natural language understanding. Despite the relevance of the task in building conversational agents and improving text classification, machine translation and other NLP tasks, to the best of our knowledge, there is no publicly available NLI corpus for the Romanian language. To this end, we introduce the first Romanian NLI corpus (RoNLI) comprising 58K training sentence pairs, which are obtained via distant supervision, and 6K validation and test sentence pairs, which are manually annotated with the correct labels. We conduct experiments with multiple machine learning methods based on distant learning, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines. Furthermore, we improve on the best model by employing a new curriculum learning strategy based on data cartography. Our dataset and code to reproduce the baselines are available at https://github.com/Eduard6421/RONLI.</abstract>
      <url hash="139d28dc">2024.acl-long.15</url>
      <bibkey>poesina-etal-2024-novel</bibkey>
      <doi>10.18653/v1/2024.acl-long.15</doi>
      <revision id="1" href="2024.acl-long.15v1" hash="e337b87b"/>
      <revision id="2" href="2024.acl-long.15v2" hash="139d28dc" date="2024-10-18">This revision corrects two typos in Equation (1).</revision>
    </paper>
    <paper id="16">
      <title><fixed-case>M</fixed-case>in<fixed-case>P</fixed-case>rompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering</title>
      <author><first>Xiusi</first><last>Chen</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Jyun-Yu</first><last>Jiang</last><affiliation>Amazon</affiliation></author>
      <author><first>Wei-Cheng</first><last>Chang</last><affiliation>Amazon</affiliation></author>
      <author><first>Cho-Jui</first><last>Hsieh</last><affiliation>Google and University of California, Los Angeles</affiliation></author>
      <author><first>Hsiang-Fu</first><last>Yu</last></author>
      <author><first>Wei</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>254-266</pages>
      <abstract>Recent advances in few-shot question answering (QA) mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present MinPrompt, a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that MinPrompt is able to achieve comparable or better results than baselines with a high degree of efficiency, bringing consistent improvements in F-1 scores.</abstract>
      <url hash="99262114">2024.acl-long.16</url>
      <bibkey>chen-etal-2024-minprompt</bibkey>
      <doi>10.18653/v1/2024.acl-long.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>S</fixed-case>ports<fixed-case>M</fixed-case>etrics: Blending Text and Numerical Data to Understand Information Fusion in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yebowen</first><last>Hu</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Kaiqiang</first><last>Song</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Sangwoo</first><last>Cho</last><affiliation>Capital One</affiliation></author>
      <author><first>Xiaoyang</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Hassan</first><last>Foroosh</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author id="fei-liu"><first>Fei</first><last>Liu</last><affiliation>Emory University</affiliation></author>
      <pages>267-278</pages>
      <abstract>Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs’ numerical reasoning and fusion skills.</abstract>
      <url hash="bbf4a3ef">2024.acl-long.17</url>
      <bibkey>hu-etal-2024-sportsmetrics</bibkey>
      <doi>10.18653/v1/2024.acl-long.17</doi>
    </paper>
    <paper id="18">
      <title><fixed-case>S</fixed-case>ci<fixed-case>MON</fixed-case>: Scientific Inspiration Machines Optimized for Novelty</title>
      <author><first>Qingyun</first><last>Wang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Doug</first><last>Downey</last><affiliation>Allen Institute for Artificial Intelligence and Northwestern University</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Tom</first><last>Hope</last><affiliation>Allen Institute for Artificial Intelligence and Hebrew University, Hebrew University of Jerusalem</affiliation></author>
      <pages>279-299</pages>
      <abstract>We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature. Work on literature-based hypothesis generation has traditionally focused on binary link prediction—severely limiting the expressivity of hypotheses. This line of work also does not focus on optimizing novelty. We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature. We present SciMON, a modeling framework that uses retrieval of “inspirations” from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved. Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue. Our work represents a first step toward evaluating and developing language models that generate new ideas derived from the scientific literature. Code, data, and resources are publicly available for research purposes: https://github.com/eaglew/clbd.</abstract>
      <url hash="60085671">2024.acl-long.18</url>
      <bibkey>wang-etal-2024-scimon</bibkey>
      <doi>10.18653/v1/2024.acl-long.18</doi>
    </paper>
    <paper id="19">
      <title>Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction</title>
      <author><first>Yiren</first><last>Jian</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Tingkai</first><last>Liu</last><affiliation>Cold Spring Harbor Laboratory</affiliation></author>
      <author><first>Yunzhe</first><last>Tao</last><affiliation>ByteDance</affiliation></author>
      <author><first>Chunhui</first><last>Zhang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Hongxia</first><last>Yang</last></author>
      <pages>300-314</pages>
      <abstract>We introduce <tex-math>\text{EVL}_{\text{Gen}}</tex-math>, a streamlined framework designed for the pre-training of visually conditioned language generation models with high computational demands, utilizing frozen pre-trained large language models (LLMs). The conventional approach in vision-language pre-training (VLP) typically involves a two-stage optimization process: an initial resource-intensive phase dedicated to general-purpose vision-language representation learning, focused on extracting and consolidating relevant visual features. This is followed by a subsequent phase that emphasizes end-to-end alignment between visual and linguistic modalities. Our novel one-stage, single-loss framework bypasses the computationally demanding first training stage by gradually merging similar visual tokens during training, while avoiding model collapse caused by single-stage training of BLIP-2 type models. The gradual merging process effectively condenses visual information while preserving semantic richness, resulting in rapid convergence without compromising performance. Our experimental findings demonstrate that our approach accelerates the training of vision-language models by a factor of 5 without a noticeable impact on overall performance. Furthermore, we illustrate that our models significantly narrow the performance gap to current vision-language models using only 1/10 of the data. Finally, we showcase how our image-text models can seamlessly adapt to video-conditioned language generation tasks through novel soft attentive temporal token contextualizing modules. Code: https://github.com/yiren-jian/EVLGen</abstract>
      <url hash="3a75ffd1">2024.acl-long.19</url>
      <bibkey>jian-etal-2024-expedited</bibkey>
      <doi>10.18653/v1/2024.acl-long.19</doi>
    </paper>
    <paper id="20">
      <title>Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models</title>
      <author><first>Abhishek</first><last>Kumar</last></author>
      <author><first>Robert</first><last>Morabito</last></author>
      <author><first>Sanzhar</first><last>Umbet</last></author>
      <author><first>Jad</first><last>Kabbara</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Ali</first><last>Emami</last><affiliation>Brock University</affiliation></author>
      <pages>315-334</pages>
      <abstract>As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM’s internal confidence, quantified by token probabilities, to the confidence conveyed in the model’s response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models’ internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model’s confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI’s GPT-4 showed the strongest confidence-probability alignment, with an average Spearman’s <tex-math>\hat{\rho}</tex-math> of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness.</abstract>
      <url hash="9b86f37b">2024.acl-long.20</url>
      <bibkey>kumar-etal-2024-confidence</bibkey>
      <doi>10.18653/v1/2024.acl-long.20</doi>
    </paper>
    <paper id="21">
      <title>Retrieval-Augmented Multilingual Knowledge Editing</title>
      <author><first>Weixuan</first><last>Wang</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>335-354</pages>
      <abstract>Knowledge represented in Large Language Models (LLMs) is quite often incorrect and can also become obsolete over time. Updating knowledge via fine-tuning is computationally resource-hungry and not reliable, and so knowledge editing (KE) has developed as an effective and economical alternative to inject new knowledge or to fix factual errors in LLMs. Although there has been considerable interest in this area, current KE research exclusively focuses on monolingual settings, typically in English. However, what happens if the new knowledge is supplied in one language, but we would like to query an LLM in a different language? To address the problem of multilingual knowledge editing, we propose Retrieval-Augmented Multilingual Knowledge Editor (ReMaKE) to update knowledge in LLMs. ReMaKE can be used to perform model-agnostic knowledge editing in a multilingual setting. ReMaKE concatenates the new knowledge retrieved from a multilingual knowledge base with users’ prompts before querying an LLM. Our experimental results show that ReMaKE outperforms baseline knowledge editing methods by a significant margin and is scalable to real-word application scenarios. Our multilingual knowledge editing dataset (MzsRE) in 12 languages, the code, and additional project information are available at https://github.com/weixuan-wang123/ReMaKE.</abstract>
      <url hash="d62e7c11">2024.acl-long.21</url>
      <bibkey>wang-etal-2024-retrieval</bibkey>
      <doi>10.18653/v1/2024.acl-long.21</doi>
    </paper>
    <paper id="22">
      <title>Picturing Ambiguity: A Visual Twist on the <fixed-case>W</fixed-case>inograd Schema Challenge</title>
      <author><first>Brendan</first><last>Park</last></author>
      <author><first>Madeline</first><last>Janecek</last></author>
      <author><first>Naser</first><last>Ezzati-Jivan</last><affiliation>Brock University</affiliation></author>
      <author><first>Yifeng</first><last>Li</last><affiliation>Brock University</affiliation></author>
      <author><first>Ali</first><last>Emami</last><affiliation>Brock University</affiliation></author>
      <pages>355-374</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable success in tasks like the Winograd Schema Challenge (WSC), showcasing advanced textual common-sense reasoning. However, applying this reasoning to multimodal domains, where understanding text and images together is essential, remains a substantial challenge. To address this, we introduce WinoVis, a novel dataset specifically designed to probe text-to-image models on pronoun disambiguation within multimodal contexts. Utilizing GPT-4 for prompt generation and Diffusion Attentive Attribution Maps (DAAM) for heatmap analysis, we propose a novel evaluation framework that isolates the models’ ability in pronoun disambiguation from other visual processing challenges. Evaluation of successive model versions reveals that, despite incremental advancements, Stable Diffusion 2.0 achieves a precision of 56.7% on WinoVis, only marginally surpassing random guessing. Further error analysis identifies important areas for future research aimed at advancing text-to-image models in their ability to interpret and interact with the complex visual world.</abstract>
      <url hash="be65fe99">2024.acl-long.22</url>
      <bibkey>park-etal-2024-picturing</bibkey>
      <doi>10.18653/v1/2024.acl-long.22</doi>
    </paper>
    <paper id="23">
      <title>Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models</title>
      <author><first>Abhishek</first><last>Kumar</last></author>
      <author><first>Sarfaroz</first><last>Yunusov</last></author>
      <author><first>Ali</first><last>Emami</last><affiliation>Brock University</affiliation></author>
      <pages>375-392</pages>
      <abstract>Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models’ outputs toward particular social narratives. This study addresses two such biases within LLMs: representative bias, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and affinity bias, reflecting the models’ evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to ‘bias fingerprints’. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions.</abstract>
      <url hash="6ab87335">2024.acl-long.23</url>
      <bibkey>kumar-etal-2024-subtle</bibkey>
      <doi>10.18653/v1/2024.acl-long.23</doi>
    </paper>
    <paper id="24">
      <title>Framing in the Presence of Supporting Data: A Case Study in <fixed-case>U</fixed-case>.<fixed-case>S</fixed-case>. Economic News</title>
      <author><first>Alexandria</first><last>Leto</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Elliot</first><last>Pickens</last><affiliation>Department of Computer Science, University of Wisconsin - Madison</affiliation></author>
      <author><first>Coen</first><last>Needell</last></author>
      <author><first>David</first><last>Rothschild</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Maria Leonor</first><last>Pacheco</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>393-415</pages>
      <abstract>The mainstream media has much leeway in what it chooses to cover and how it covers it. These choices have real-world consequences on what people know and their subsequent behaviors. However, the lack of objective measures to evaluate editorial choices makes research in this area particularly difficult. In this paper, we argue that there are newsworthy topics where objective measures exist in the form of supporting data and propose a computational framework to analyze editorial choices in this setup. We focus on the economy because the reporting of economic indicators presents us with a relatively easy way to determine both the selection and framing of various publications. Their values provide a ground truth of how the economy is doing relative to how the publications choose to cover it. To do this, we define frame prediction as a set of interdependent tasks. At the article level, we learn to identify the reported stance towards the general state of the economy. Then, for every numerical quantity reported in the article, we learn to identify whether it corresponds to an economic indicator and whether it is being reported in a positive or negative way. To perform our analysis, we track six American publishers and each article that appeared in the top 10 slots of their landing page between 2015 and 2023.</abstract>
      <url hash="09aa02ed">2024.acl-long.24</url>
      <bibkey>leto-etal-2024-framing</bibkey>
      <doi>10.18653/v1/2024.acl-long.24</doi>
    </paper>
    <paper id="25">
      <title>Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences</title>
      <author><first>Xiyao</first><last>Wang</last></author>
      <author><first>Yuhang</first><last>Zhou</last></author>
      <author><first>Xiaoyu</first><last>Liu</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Hongjin</first><last>Lu</last></author>
      <author><first>Yuancheng</first><last>Xu</last></author>
      <author><first>Feihong</first><last>He</last></author>
      <author><first>Jaehong</first><last>Yoon</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Taixi</first><last>Lu</last></author>
      <author><first>Fuxiao</first><last>Liu</last></author>
      <author><first>Gedas</first><last>Bertasius</last><affiliation>University of North Carolina, Chapel Hill</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Huaxiu</first><last>Yao</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Furong</first><last>Huang</last><affiliation>University of Maryland</affiliation></author>
      <pages>416-442</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs’ sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitative analysis and case studies identify three key factors impacting MLLMs’ sequential image reasoning: the correlation between object and behavioral hallucinations, the influence of co-occurring behaviors, and the compounding impact of behavioral hallucinations.</abstract>
      <url hash="9dfebf25">2024.acl-long.25</url>
      <bibkey>wang-etal-2024-mementos</bibkey>
      <doi>10.18653/v1/2024.acl-long.25</doi>
    </paper>
    <paper id="26">
      <title><fixed-case>TTM</fixed-case>-<fixed-case>RE</fixed-case>: Memory-Augmented Document-Level Relation Extraction</title>
      <author><first>Chufan</first><last>Gao</last></author>
      <author><first>Xuan</first><last>Wang</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Jimeng</first><last>Sun</last><affiliation>University of Illinois, Urbana Champaign, College of Computing and Georgia Institute of Technology</affiliation></author>
      <pages>443-458</pages>
      <abstract>Document-level relation extraction aims to categorize the association between any two entities within a document.We find that previous methods for document-level relation extraction are ineffective in exploiting the full potential of large amounts of training data with varied noise levels. For example, in the ReDocRED benchmark dataset, state-of-the-art methods trained on the large-scale, lower-quality, distantly supervised training data generally do not perform better than those trained solely on the smaller, high-quality, human-annotated training data. To unlock the full potential of large-scale noisy training data for document-level relation extraction, we propose TTM-RE, a novel approach that integrates a trainable memory module, known as the Token Turing Machine, with a noisy-robust loss function that accounts for the positive-unlabeled setting. The trainable memory module enhances knowledge extraction from the large-scale noisy training dataset through an explicit learning of the memory tokens and a soft integration of the learned memory tokens into the input representation, thereby improving the model’s effectiveness for the final relation classification. Extensive experiments on ReDocRED, a benchmark dataset for document-level relation extraction, reveal that TTM-RE achieves state-of-the-art performance (with an absolute F1 score improvement of over 3%). Ablation studies further illustrate the superiority of TTM-RE in other domains (the ChemDisGene dataset in the biomedical domain) and under highly unlabeled settings.</abstract>
      <url hash="ecddd7bc">2024.acl-long.26</url>
      <bibkey>gao-etal-2024-ttm</bibkey>
      <doi>10.18653/v1/2024.acl-long.26</doi>
    </paper>
    <paper id="27">
      <title>Answer is All You Need: Instruction-following Text Embedding via Answering the Question</title>
      <author><first>Letian</first><last>Peng</last></author>
      <author><first>Yuwei</first><last>Zhang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Zilong</first><last>Wang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Jayanth</first><last>Srinivasa</last></author>
      <author><first>Gaowen</first><last>Liu</last></author>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>459-477</pages>
      <abstract>This work aims to build a text embedder that can capture characteristics of texts specified by user instructions clarifying the similarity criterion. While previous methods improve general task awareness by injecting the instruction information into encoding, they fail to be sensitive to clearer criteria like “evaluate similarity based on emotion”. We instead propose a different viewpoint, which treats the instruction as a “question” about the input text and encodes the expected answers to obtain the representation accordingly. Intuitively, texts with the same (implicit) semantics would share similar answers following the instruction, thus leading to more similar representations. Specifically, we propose InBedder that instantiates this learning-to-answer idea by only fine-tuning language models via abstractive question answering tasks. Despite its simplicity, InBedder demonstrates significantly improved instruction-following capabilities according to our proposed instruction awareness tests and instruction robustness tests, when applied to language models with large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-based LMs (e.g., roberta-large). Additionally, our qualitative analysis of clustering outcomes, achieved by applying diverse instructions to the same unlabeled corpus, demonstrates a high degree of interpretability in the clusters formed.</abstract>
      <url hash="8e853d94">2024.acl-long.27</url>
      <bibkey>peng-etal-2024-answer</bibkey>
      <doi>10.18653/v1/2024.acl-long.27</doi>
    </paper>
    <paper id="28">
      <title>Explore Spurious Correlations at the Concept Level in Language Models for Text Classification</title>
      <author><first>Yuhang</first><last>Zhou</last></author>
      <author><first>Paiheng</first><last>Xu</last><affiliation>Department of Computer Science, University of Maryland, College Park</affiliation></author>
      <author><first>Xiaoyu</first><last>Liu</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Bang</first><last>An</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Wei</first><last>Ai</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Furong</first><last>Huang</last><affiliation>University of Maryland</affiliation></author>
      <pages>478-492</pages>
      <abstract>Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, thereby balancing label distribution and mitigating spurious correlations. Our method’s efficacy, surpassing traditional token removal approaches, is validated through extensive testing.</abstract>
      <url hash="974ab240">2024.acl-long.28</url>
      <bibkey>zhou-etal-2024-explore</bibkey>
      <doi>10.18653/v1/2024.acl-long.28</doi>
    </paper>
    <paper id="29">
      <title>Every Answer Matters: Evaluating Commonsense with Probabilistic Measures</title>
      <author><first>Qi</first><last>Cheng</last></author>
      <author><first>Michael</first><last>Boratko</last><affiliation>Google</affiliation></author>
      <author><first>Pranay Kumar</first><last>Yelugam</last></author>
      <author><first>Tim</first><last>O’Gorman</last></author>
      <author><first>Nalini</first><last>Singh</last></author>
      <author><first>Andrew</first><last>McCallum</last><affiliation>University of Massachusetts Amherst and University of Massachusetts Amherst</affiliation></author>
      <author><first>Xiang</first><last>Li</last></author>
      <pages>493-506</pages>
      <abstract>Large language models have demonstrated impressive performance on commonsense tasks; however, these tasks are often posed as multiple-choice questions, allowing models to exploit systematic biases. Commonsense is also inherently probabilistic with multiple correct answers. The purpose of “boiling water” could be making tea, cooking but also could be killing germs. Existing tasks do not capture the probabilistic nature of common sense. To this end, we present commonsense frame completion (CFC), a new generative task that evaluates common sense via multiple open-ended generations. We also propose a method of probabilistic evaluation that strongly correlates with human judgments. Humans drastically outperform strong language model baselines on our dataset, indicating this approach is both a challenging and useful evaluation of machine common sense.</abstract>
      <url hash="664f5a62">2024.acl-long.29</url>
      <bibkey>cheng-etal-2024-every</bibkey>
      <doi>10.18653/v1/2024.acl-long.29</doi>
    </paper>
    <paper id="30">
      <title><fixed-case>G</fixed-case>rad<fixed-case>S</fixed-case>afe: Detecting Jailbreak Prompts for <fixed-case>LLM</fixed-case>s via Safety-Critical Gradient Analysis</title>
      <author><first>Yueqi</first><last>Xie</last></author>
      <author><first>Minghong</first><last>Fang</last><affiliation>Duke University</affiliation></author>
      <author><first>Renjie</first><last>Pi</last></author>
      <author><first>Neil</first><last>Gong</last><affiliation>Duke University</affiliation></author>
      <pages>507-518</pages>
      <abstract>Large Language Models (LLMs) face threats from jailbreak prompts. Existing methods for detecting jailbreak prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects jailbreak prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our method is grounded in a pivotal observation: the gradients of an LLM’s loss for jailbreak prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect jailbreak prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard—despite its extensive finetuning with a large dataset—in detecting jailbreak prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on ToxicChat and XSTest. The source code is available at https://github.com/xyq7/GradSafe.</abstract>
      <url hash="29bfb3e4">2024.acl-long.30</url>
      <bibkey>xie-etal-2024-gradsafe</bibkey>
      <doi>10.18653/v1/2024.acl-long.30</doi>
    </paper>
    <paper id="31">
      <title>Pouring Your Heart Out: Investigating the Role of Figurative Language in Online Expressions of Empathy</title>
      <author><first>Gyeongeun</first><last>Lee</last></author>
      <author><first>Christina</first><last>Wong</last></author>
      <author><first>Meghan</first><last>Guo</last></author>
      <author><first>Natalie</first><last>Parde</last><affiliation>University of Illinois Chicago</affiliation></author>
      <pages>519-529</pages>
      <abstract>Empathy is a social mechanism used to support and strengthen emotional connection with others, including in online communities. However, little is currently known about the nature of these online expressions, nor the particular factors that may lead to their improved detection. In this work, we study the role of a specific and complex subcategory of linguistic phenomena, figurative language, in online expressions of empathy. Our extensive experiments reveal that incorporating features regarding the use of metaphor, idiom, and hyperbole into empathy detection models improves their performance, resulting in impressive maximum F1 scores of 0.942 and 0.809 for identifying posts without and with empathy, respectively.</abstract>
      <url hash="3c856355">2024.acl-long.31</url>
      <bibkey>lee-etal-2024-pouring</bibkey>
      <doi>10.18653/v1/2024.acl-long.31</doi>
    </paper>
    <paper id="32">
      <title>An Information-Theoretic Approach to Analyze <fixed-case>NLP</fixed-case> Classification Tasks</title>
      <author><first>Luran</first><last>Wang</last></author>
      <author><first>Mark</first><last>Gales</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Vatsal</first><last>Raina</last></author>
      <pages>530-551</pages>
      <abstract>Understanding the contribution of the inputs on the output is useful across many tasks. This work provides an information-theoretic framework to analyse the influence of inputs for text classification tasks. Natural language processing (NLP) tasks take either a single or multiple text elements to predict an output variable. Each text element has two components: the semantic meaning and a linguistic realization. Multiple-choice reading comprehension (MCRC) and sentiment classification (SC) are selected to showcase the framework. For MCRC, it is found that the relative context influence on the output reduces on more challenging datasets. In particular, more challenging contexts allows greater variation in the question complexity. Hence, test creators need to carefully consider the choice of the context when designing multiple-choice questions for assessment. For SC, it is found the semantic meaning of the input dominates compared to its linguistic realization when determining the sentiment. The framework is made available at: https://github.com/WangLuran/nlp-element-influence.</abstract>
      <url hash="c59af36d">2024.acl-long.32</url>
      <bibkey>wang-etal-2024-information</bibkey>
      <doi>10.18653/v1/2024.acl-long.32</doi>
    </paper>
    <paper id="33">
      <title>Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders</title>
      <author><first>Yuwei</first><last>Zhang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Siffi</first><last>Singh</last></author>
      <author><first>Sailik</first><last>Sengupta</last><affiliation>Amazon</affiliation></author>
      <author><first>Igor</first><last>Shalyminov</last><affiliation>Amazon</affiliation></author>
      <author><first>Hang</first><last>Su</last><affiliation>Amazon</affiliation></author>
      <author><first>Hwanjun</first><last>Song</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Saab</first><last>Mansour</last><affiliation>Amazon</affiliation></author>
      <pages>552-567</pages>
      <abstract>Conversational systems often rely on embedding models for intent classification and intent clustering tasks. The advent of Large Language Models (LLMs), which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts, are being viewed as a panacea for these downstream conversational tasks. However, traditional evaluation benchmarks rely solely on task metrics that don’t particularly measure gaps related to semantic understanding. Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks– (1) intent classification, (2) intent clustering, and (3) a novel triplet task. The triplet task gauges the model’s understanding of two semantic concepts paramount in real-world conversational systems– negation and implicature. We observe that current embedding models fare poorly in semantic understanding of these concepts. To address this, we propose a pre-training approach to improve the embedding model by leveraging augmentation with data generated by an auto-regressive model and a contrastive loss term. Our approach improves the semantic understanding of the intent embedding model on the aforementioned linguistic dimensions while slightly effecting their performance on downstream task metrics.</abstract>
      <url hash="2828f055">2024.acl-long.33</url>
      <bibkey>zhang-etal-2024-model</bibkey>
      <doi>10.18653/v1/2024.acl-long.33</doi>
    </paper>
    <paper id="34">
      <title><fixed-case>W</fixed-case>av2<fixed-case>G</fixed-case>loss: Generating Interlinear Glossed Text from Speech</title>
      <author><first>Taiqi</first><last>He</last></author>
      <author><first>Kwanghee</first><last>Choi</last></author>
      <author><first>Lindia</first><last>Tjuatja</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Nathaniel</first><last>Robinson</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Jiatong</first><last>Shi</last></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>David</first><last>Mortensen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Lori</first><last>Levin</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>568-582</pages>
      <abstract>Thousands of the world’s languages are in danger of extinction—a tremendous threat to cultural identities and human language diversity. Interlinear Glossed Text (IGT) is a form of linguistic annotation that can support documentation and resource creation for these languages’ communities. IGT typically consists of (1) transcriptions, (2) morphological segmentation, (3) glosses, and (4) free translations to a majority language. We propose Wav2Gloss: a task in which these four annotation components are extracted automatically from speech, and introduce the first dataset to this end, Fieldwork: a corpus of speech with all these annotations, derived from the work of field linguists, covering 37 languages, with standard formatting, and train/dev/test splits. We provide various baselines to lay the groundwork for future research on IGT generation from speech, such as end-to-end versus cascaded, monolingual versus multilingual, and single-task versus multi-task approaches.</abstract>
      <url hash="78dc87cc">2024.acl-long.34</url>
      <bibkey>he-etal-2024-wav2gloss</bibkey>
      <doi>10.18653/v1/2024.acl-long.34</doi>
    </paper>
    <paper id="35">
      <title>Leveraging Codebook Knowledge with <fixed-case>NLI</fixed-case> and <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> for Zero-Shot Political Relation Classification</title>
      <author><first>Yibo</first><last>Hu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Erick</first><last>Skorupa Parolin</last></author>
      <author><first>Latifur</first><last>Khan</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Patrick</first><last>Brandt</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Javier</first><last>Osorio</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Vito</first><last>D’Orazio</last><affiliation>West Virginia University</affiliation></author>
      <pages>583-603</pages>
      <abstract>Is it possible accurately classify political relations within evolving event ontologies without extensive annotations? This study investigates zero-shot learning methods that use expert knowledge from existing annotation codebook, and evaluates the performance of advanced ChatGPT (GPT-3.5/4) and a natural language inference (NLI)-based model called ZSP. ChatGPT uses codebook’s labeled summaries as prompts, whereas ZSP breaks down the classification task into context, event mode, and class disambiguation to refine task-specific hypotheses. This decomposition enhances interpretability, efficiency, and adaptability to schema changes. The experiments reveal ChatGPT’s strengths and limitations, and crucially show ZSP’s outperformance of dictionary-based methods and its competitive edge over some supervised models. These findings affirm the value of ZSP for validating event records and advancing ontology development. Our study underscores the efficacy of leveraging transfer learning and existing domain expertise to enhance research efficiency and scalability.</abstract>
      <url hash="9a8ef6f2">2024.acl-long.35</url>
      <bibkey>hu-etal-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.acl-long.35</doi>
    </paper>
    <paper id="36">
      <title><fixed-case>SPOR</fixed-case>: A Comprehensive and Practical Evaluation Method for Compositional Generalization in Data-to-Text Generation</title>
      <author><first>Ziyao</first><last>Xu</last><affiliation>Peking University</affiliation></author>
      <author><first>Houfeng</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <pages>604-621</pages>
      <abstract>Compositional generalization is an important ability of language models and has many different manifestations. For data-to-text generation, previous research on this ability is limited to a single manifestation called Systematicity and lacks consideration of large language models (LLMs), which cannot fully cover practical application scenarios. In this work, we propose SPOR, a comprehensive and practical evaluation method for compositional generalization in data-to-text generation. SPOR includes four aspects of manifestations (Systematicity, Productivity, Order invariance, and Rule learnability) and allows high-quality evaluation without additional manual annotations based on existing datasets. We demonstrate SPOR on two different datasets and evaluate some existing language models including LLMs. We find that the models are deficient in various aspects of the evaluation and need further improvement. Our work shows the necessity for comprehensive research on different manifestations of compositional generalization in data-to-text generation and provides a framework for evaluation.</abstract>
      <url hash="a617524f">2024.acl-long.36</url>
      <bibkey>xu-wang-2024-spor</bibkey>
      <doi>10.18653/v1/2024.acl-long.36</doi>
    </paper>
    <paper id="37">
      <title><fixed-case>OPE</fixed-case>x: A Component-Wise Analysis of <fixed-case>LLM</fixed-case>-Centric Agents in Embodied Instruction Following</title>
      <author><first>Haochen</first><last>Shi</last></author>
      <author><first>Zhiyuan</first><last>Sun</last></author>
      <author><first>Xingdi</first><last>Yuan</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Marc-Alexandre</first><last>Côté</last><affiliation>Microsoft</affiliation></author>
      <author><first>Bang</first><last>Liu</last><affiliation>University of Montreal</affiliation></author>
      <pages>622-636</pages>
      <abstract>Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions. Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components—ranging from visual perception to action execution—on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by integrating a multi-agent design into the Planner component of our LLM-centric architecture, further enhancing task performance. Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance.</abstract>
      <url hash="c6954d4d">2024.acl-long.37</url>
      <bibkey>shi-etal-2024-opex</bibkey>
      <doi>10.18653/v1/2024.acl-long.37</doi>
    </paper>
    <paper id="38">
      <title>Multimodal Instruction Tuning with Conditional Mixture of <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case></title>
      <author><first>Ying</first><last>Shen</last></author>
      <author><first>Zhiyang</first><last>Xu</last></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Wenpeng</first><last>Yin</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>Virginia Tech</affiliation></author>
      <pages>637-648</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in diverse tasks across different domains, with an increasing focus on improving their zero-shot generalization capabilities for unseen multimodal tasks. Multimodal instruction tuning has emerged as a successful strategy for achieving zero-shot generalization by fine-tuning pre-trained models on diverse multimodal tasks through instructions. As MLLMs grow in complexity and size, the need for parameter-efficient fine-tuning methods like Low-Rank Adaption (LoRA), which fine-tunes with a minimal set of parameters, becomes essential. However, applying LoRA in multimodal instruction tuning presents the challenge of task interference, which leads to performance degradation, especially when dealing with a broad array of multimodal tasks. To address this, this paper introduces a novel approach that integrates multimodal instruction tuning with Conditional Mixture-of-LoRA (MixLoRA). It innovates upon LoRA by dynamically constructing low-rank adaptation matrices tailored to the unique demands of each input instance, aiming to mitigate task interference. Experimental results on various multimodal evaluation datasets indicate that MixLoRA not only outperforms the conventional LoRA with the same or even higher ranks, demonstrating its efficacy and adaptability in diverse multimodal tasks.</abstract>
      <url hash="dfe92d83">2024.acl-long.38</url>
      <bibkey>shen-etal-2024-multimodal</bibkey>
      <doi>10.18653/v1/2024.acl-long.38</doi>
    </paper>
    <paper id="39">
      <title><fixed-case>D</fixed-case>oc<fixed-case>L</fixed-case>ens: Multi-aspect Fine-grained Medical Text Evaluation</title>
      <author><first>Yiqing</first><last>Xie</last></author>
      <author><first>Sheng</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hao</first><last>Cheng</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Zelalem</first><last>Gero</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Cliff</first><last>Wong</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Tristan</first><last>Naumann</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Hoifung</first><last>Poon</last><affiliation>Microsoft</affiliation></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>649-679</pages>
      <abstract>Medical text generation aims to assist with administrative work and highlight salient information to support decision-making.To reflect the specific requirements of medical text, in this paper, we propose a set of metrics to evaluate the completeness, conciseness, and attribution of the generated text at a fine-grained level. The metrics can be computed by various types of evaluators including instruction-following (both proprietary and open-source) and supervised entailment models. We demonstrate the effectiveness of the resulting framework, DocLens, with three evaluators on three tasks: clinical note generation, radiology report summarization, and patient question summarization. A comprehensive human study shows that DocLens exhibits substantially higher agreement with the judgments of medical experts than existing metrics. The results also highlight the need to improve open-source evaluators and suggest potential directions. We released the code at https://github.com/yiqingxyq/DocLens.</abstract>
      <url hash="798c42b8">2024.acl-long.39</url>
      <bibkey>xie-etal-2024-doclens</bibkey>
      <doi>10.18653/v1/2024.acl-long.39</doi>
    </paper>
    <paper id="40">
      <title><fixed-case>FOFO</fixed-case>: A Benchmark to Evaluate <fixed-case>LLM</fixed-case>s’ Format-Following Capability</title>
      <author><first>Congying</first><last>Xia</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Chen</first><last>Xing</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Jiangshu</first><last>Du</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Xinyi</first><last>Yang</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Yihao</first><last>Feng</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Ran</first><last>Xu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Wenpeng</first><last>Yin</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <pages>680-699</pages>
      <abstract>This paper presents FoFo, a pioneering benchmark for evaluating large language models’ (LLMs) ability to follow complex, domain-specific formats, a crucial yet under-examined capability for their application as AI agents. Despite LLMs’ advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs’ format-following performance is independent of their content generation quality; and LLMs’ format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo’s role in guiding the selection of domain-specific AI agents. FoFo will be publicly released, contributing a critical tool for advancing LLM evaluation and application.</abstract>
      <url hash="58dd5d33">2024.acl-long.40</url>
      <bibkey>xia-etal-2024-fofo</bibkey>
      <doi>10.18653/v1/2024.acl-long.40</doi>
    </paper>
    <paper id="41">
      <title>Hyper-<fixed-case>CL</fixed-case>: Conditioning Sentence Representations with Hypernetworks</title>
      <author><first>Young</first><last>Yoo</last></author>
      <author><first>Jii</first><last>Cha</last></author>
      <author><first>Changhyeon</first><last>Kim</last></author>
      <author><first>Taeuk</first><last>Kim</last><affiliation>Hanyang University</affiliation></author>
      <pages>700-711</pages>
      <abstract>While the introduction of contrastive learning frameworks in sentence representation learning has significantly contributed to advancements in the field, it still remains unclear whether state-of-the-art sentence embeddings can capture the fine-grained semantics of sentences, particularly when conditioned on specific perspectives.In this paper, we introduce Hyper-CL, an efficient methodology that integrates hypernetworks with contrastive learning to compute conditioned sentence representations.In our proposed approach, the hypernetwork is responsible for transforming pre-computed condition embeddings into corresponding projection layers. This enables the same sentence embeddings to be projected differently according to various conditions.Evaluation on two representative conditioning benchmarks, namely conditional semantic text similarity and knowledge graph completion, demonstrates that Hyper-CL is effective in flexibly conditioning sentence representations, showcasing its computational efficiency at the same time.We also provide a comprehensive analysis of the inner workings of our approach, leading to a better interpretation of its mechanisms.</abstract>
      <url hash="7c5d372a">2024.acl-long.41</url>
      <bibkey>yoo-etal-2024-hyper</bibkey>
      <doi>10.18653/v1/2024.acl-long.41</doi>
    </paper>
    <paper id="42">
      <title>Analysis of Multi-Source Language Training in Cross-Lingual Transfer</title>
      <author><first>Seonghoon</first><last>Lim</last></author>
      <author><first>Taejun</first><last>Yun</last></author>
      <author><first>Jinhyeon</first><last>Kim</last></author>
      <author><first>Jihun</first><last>Choi</last><affiliation>Sony AI</affiliation></author>
      <author><first>Taeuk</first><last>Kim</last><affiliation>Hanyang University</affiliation></author>
      <pages>712-725</pages>
      <abstract>The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the availability of data tailored for that condition. While cross-lingual transfer (XLT) methods have contributed to addressing this data scarcity problem, there still exists ongoing debate about the mechanisms behind their effectiveness.In this work, we focus on one of promising assumptions about inner workings of XLT, that it encourages multilingual LMs to place greater emphasis on language-agnostic or task-specific features. We test this hypothesis by examining how the patterns of XLT change with a varying number of source languages involved in the process.Our experimental findings show that the use of multiple source languages in XLT-a technique we term Multi-Source Language Training (MSLT)-leads to increased mingling of embedding spaces for different languages, supporting the claim that XLT benefits from making use of language-independent information. On the other hand, we discover that using an arbitrary combination of source languages does not always guarantee better performance. We suggest simple heuristics for identifying effective language combinations for MSLT and empirically prove its effectiveness.</abstract>
      <url hash="f137f533">2024.acl-long.42</url>
      <bibkey>lim-etal-2024-analysis</bibkey>
      <doi>10.18653/v1/2024.acl-long.42</doi>
    </paper>
    <paper id="43">
      <title><fixed-case>ABEX</fixed-case>: Data Augmentation for Low-Resource <fixed-case>NLU</fixed-case> via Expanding Abstract Descriptions</title>
      <author><first>Sreyan</first><last>Ghosh</last></author>
      <author><first>Utkarsh</first><last>Tyagi</last></author>
      <author><first>Sonal</first><last>Kumar</last></author>
      <author><first>Chandra Kiran</first><last>Evuru</last></author>
      <author><first>Ramaneswaran</first><last>S</last><affiliation>NVIDIA</affiliation></author>
      <author><first>S</first><last>Sakshi</last></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>726-748</pages>
      <abstract>We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms of an input document – we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction. To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs. Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs. ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution. At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations. We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings. ABEX outperforms all our baselines qualitatively with improvements of 0.04% - 38.8%. Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity.</abstract>
      <url hash="e9b6d670">2024.acl-long.43</url>
      <bibkey>ghosh-etal-2024-abex</bibkey>
      <doi>10.18653/v1/2024.acl-long.43</doi>
    </paper>
    <paper id="44">
      <title>The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants</title>
      <author><first>Lucas</first><last>Bandarkar</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Davis</first><last>Liang</last><affiliation>Abridge</affiliation></author>
      <author><first>Benjamin</first><last>Muller</last><affiliation>Meta</affiliation></author>
      <author><first>Mikel</first><last>Artetxe</last><affiliation>Reka AI</affiliation></author>
      <author><first>Satya Narayan</first><last>Shukla</last><affiliation>Meta</affiliation></author>
      <author><first>Donald</first><last>Husa</last></author>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Abhinandan</first><last>Krishnan</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last><affiliation>University of Washington, Facebook and Meta</affiliation></author>
      <author><first>Madian</first><last>Khabsa</last><affiliation>Facebook</affiliation></author>
      <pages>749-775</pages>
      <abstract>We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the FLORES-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and findings, notably that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.</abstract>
      <url hash="9836d005">2024.acl-long.44</url>
      <bibkey>bandarkar-etal-2024-belebele</bibkey>
      <doi>10.18653/v1/2024.acl-long.44</doi>
    </paper>
    <paper id="45">
      <title>Learn from Failure: Fine-tuning <fixed-case>LLM</fixed-case>s with Trial-and-Error Data for Intuitionistic Propositional Logic Proving</title>
      <author><first>Chenyang</first><last>An</last></author>
      <author><first>Zhibo</first><last>Chen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Qihao</first><last>Ye</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Emily</first><last>First</last><affiliation>Computer Science and Engineering Department, University of California, San Diego</affiliation></author>
      <author><first>Letian</first><last>Peng</last></author>
      <author><first>Jiayun</first><last>Zhang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Sorin</first><last>Lerner</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>776-790</pages>
      <abstract>Recent advances in Automated Theorem Proving have shown the effectiveness of leveraging a (large) language model that generates tactics (i.e. proof steps) to search through proof states. The current model, while trained solely on successful proof paths, faces a discrepancy at the inference stage, as it must sample and try various tactics at each proof state until finding success, unlike its training which does not incorporate learning from failed attempts. Intuitively, a tactic that leads to a failed search path would indicate that similar tactics should receive less attention during the following trials. In this paper, we demonstrate the benefit of training models that additionally learn from failed search paths. Facing the lack of such trial-and-error data in existing open-source theorem-proving datasets, we curate a dataset on intuitionistic propositional logic theorems and formalize it in Lean, such that we can reliably check the correctness of proofs. We compare our model trained on relatively short trial-and-error information (TrialMaster) with models trained only on the correct paths and discover that the former solves more unseen theorems with lower trial searches.</abstract>
      <url hash="21f3d9fe">2024.acl-long.45</url>
      <bibkey>an-etal-2024-learn</bibkey>
      <doi>10.18653/v1/2024.acl-long.45</doi>
    </paper>
    <paper id="46">
      <title>Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach</title>
      <author><first>Saehyung</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sangwon</first><last>Yu</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Junsung</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jihun</first><last>Yi</last></author>
      <author><first>Sungroh</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <pages>791-809</pages>
      <abstract>In this paper, we primarily address the issue of dialogue-form context query within the interactive text-to-image retrieval task. Our methodology, PlugIR, actively utilizes the general instruction-following capability of LLMs in two ways. First, by reformulating the dialogue-form context, we eliminate the necessity of fine-tuning a retrieval model on existing visual dialogue data, thereby enabling the use of any arbitrary black-box model. Second, we construct the LLM questioner to generate non-redundant questions about the attributes of the target image, based on the information of retrieval candidate images in the current context. This approach mitigates the issues of noisiness and redundancy in the generated questions. Beyond our methodology, we propose a novel evaluation metric, Best log Rank Integral (BRI), for a comprehensive assessment of the interactive retrieval system. PlugIR demonstrates superior performance compared to both zero-shot and fine-tuned baselines in various benchmarks. Additionally, the two methodologies comprising PlugIR can be flexibly applied together or separately in various situations.</abstract>
      <url hash="e76165b4">2024.acl-long.46</url>
      <bibkey>lee-etal-2024-interactive</bibkey>
      <doi>10.18653/v1/2024.acl-long.46</doi>
    </paper>
    <paper id="47">
      <title><fixed-case>IMBUE</fixed-case>: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction</title>
      <author><first>Inna</first><last>Lin</last><affiliation>University of Washington</affiliation></author>
      <author><first>Ashish</first><last>Sharma</last></author>
      <author><first>Christopher</first><last>Rytting</last></author>
      <author><first>Adam</first><last>Miner</last></author>
      <author><first>Jina</first><last>Suh</last><affiliation>Research, Microsoft and Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Tim</first><last>Althoff</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>810-840</pages>
      <abstract>Navigating certain communication situations can be challenging due to individuals’ lack of skills and the interference of strong emotions. However, effective learning opportunities are rarely accessible. In this work, we conduct a human-centered study that uses language models to simulate bespoke communication training and provide just-in-time feedback to support the practice and learning of interpersonal effectiveness skills. We apply the interpersonal effectiveness framework from Dialectical Behavioral Therapy (DBT), DEAR MAN, which focuses on both conversational and emotional skills. We present IMBUE, an interactive training system that provides feedback 28% more similar to experts’ feedback, compared to that generated by GPT-4. IMBUE is the first to focus on communication skills and emotion management simultaneously, incorporate experts’ domain knowledge in providing feedback, and be grounded in psychology theory. Through a randomized trial of 86 participants, we find that IMBUE’s simulation-only variant significantly improves participants’ self-efficacy (up to 17%) and reduces negative emotions (up to 25%). With IMBUE’s additional just-in-time feedback, participants demonstrate 17% improvement in skill mastery, along with greater enhancements in self-efficacy (27% more) and reduction of negative emotions (16% more) compared to simulation-only. The improvement in skill mastery is the only measure that is transferred to new and more difficult situations; situation-specific training is necessary for improving self-efficacy and emotion reduction.</abstract>
      <url hash="f7839bce">2024.acl-long.47</url>
      <bibkey>lin-etal-2024-imbue</bibkey>
      <doi>10.18653/v1/2024.acl-long.47</doi>
    </paper>
    <paper id="48">
      <title>Token-wise Influential Training Data Retrieval for Large Language Models</title>
      <author><first>Huawei</first><last>Lin</last><affiliation>Amazon and Rochester Institute of Technology</affiliation></author>
      <author><first>Jikai</first><last>Long</last></author>
      <author><first>Zhaozhuo</first><last>Xu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Weijie</first><last>Zhao</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <pages>841-860</pages>
      <abstract>Given a Large Language Model (LLM) generation, how can we identify which training data led to this generation? In this paper, we proposed RapidIn, a scalable framework adapting to LLMs for estimating the influence of each training data. The proposed framework consists of two stages: caching and retrieval. First, we compress the gradient vectors by over 200,000x, allowing them to be cached on disk or in GPU/CPU memory. Then, given a generation, RapidIn efficiently traverses the cached gradients to estimate the influence within minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports multi-GPU parallelization to substantially accelerate caching and retrieval. Our empirical result confirms the efficiency and effectiveness of RapidIn.</abstract>
      <url hash="0e670a4e">2024.acl-long.48</url>
      <bibkey>lin-etal-2024-token</bibkey>
      <doi>10.18653/v1/2024.acl-long.48</doi>
    </paper>
    <paper id="49">
      <title>Tree-of-Counterfactual Prompting for Zero-Shot Stance Detection</title>
      <author><first>Maxwell</first><last>Weinzierl</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Sanda</first><last>Harabagiu</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>861-880</pages>
      <abstract>Stance detection enables the inference of attitudes from human communications. Automatic stance identification was mostly cast as a classification problem. However, stance decisions involve complex judgments, which can be nowadays generated by prompting Large Language Models (LLMs). In this paper we present a new method for stance identification which (1) relies on a new prompting framework, called Tree-of-Counterfactual prompting; (2) operates not only on textual communications, but also on images; (3) allows more than one stance object type; and (4) requires no examples of stance attribution, thus it is a “Tabula Rasa” Zero-Shot Stance Detection (TR-ZSSD) method. Our experiments indicate surprisingly promising results, outperforming fine-tuned stance detection systems.</abstract>
      <url hash="c82fc26a">2024.acl-long.49</url>
      <bibkey>weinzierl-harabagiu-2024-tree</bibkey>
      <doi>10.18653/v1/2024.acl-long.49</doi>
    </paper>
    <paper id="50">
      <title><fixed-case>V</fixed-case>isual<fixed-case>W</fixed-case>eb<fixed-case>A</fixed-case>rena: Evaluating Multimodal Agents on Realistic Visual Web Tasks</title>
      <author><first>Jing Yu</first><last>Koh</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Robert</first><last>Lo</last></author>
      <author><first>Lawrence</first><last>Jang</last></author>
      <author><first>Vikram</first><last>Duvvur</last></author>
      <author><first>Ming</first><last>Lim</last></author>
      <author><first>Po-Yu</first><last>Huang</last></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Shuyan</first><last>Zhou</last></author>
      <author><first>Russ</first><last>Salakhutdinov</last><affiliation>Carnegie-Mellon University, Carnegie Mellon University and Department of Computer Science</affiliation></author>
      <author><first>Daniel</first><last>Fried</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>881-905</pages>
      <abstract>Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on *realistic visually grounded tasks*. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web.</abstract>
      <url hash="374adec3">2024.acl-long.50</url>
      <bibkey>koh-etal-2024-visualwebarena</bibkey>
      <doi>10.18653/v1/2024.acl-long.50</doi>
    </paper>
    <paper id="51">
      <title><fixed-case>F</fixed-case>ine<fixed-case>S</fixed-case>ur<fixed-case>E</fixed-case>: Fine-grained Summarization Evaluation using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Hwanjun</first><last>Song</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hang</first><last>Su</last><affiliation>Amazon</affiliation></author>
      <author><first>Igor</first><last>Shalyminov</last><affiliation>Amazon</affiliation></author>
      <author><first>Jason</first><last>Cai</last><affiliation>Amazon</affiliation></author>
      <author><first>Saab</first><last>Mansour</last><affiliation>Amazon</affiliation></author>
      <pages>906-922</pages>
      <abstract>Automated evaluation is crucial for streamlining text summarization benchmarking and model development, given the costly and time-consuming nature of human evaluation. Traditional methods like ROUGE do not correlate well with human judgment, while recently proposed LLM-based metrics provide only summary-level assessment using Likert-scale scores. This limits deeper model analysis, e.g., we can only assign one hallucination score at the summary level, while at the sentence level, we can count sentences containing hallucinations. To remedy those limitations, we propose FineSurE, a fine-grained evaluator specifically tailored for the summarization task using large language models (LLMs). It also employs completeness and conciseness criteria, in addition to faithfulness, enabling multi-dimensional assessment. We compare various open-source and proprietary LLMs as backbones for FineSurE. In addition, we conduct extensive benchmarking of FineSurE against SOTA methods including NLI-, QA-, and LLM-based methods, showing improved performance especially on the completeness and conciseness dimensions. The code is available at https://github.com/DISL-Lab/FineSurE.</abstract>
      <url hash="b4d8e119">2024.acl-long.51</url>
      <bibkey>song-etal-2024-finesure</bibkey>
      <doi>10.18653/v1/2024.acl-long.51</doi>
    </paper>
    <paper id="52">
      <title>Tuning Large Multimodal Models for Videos using Reinforcement Learning from <fixed-case>AI</fixed-case> Feedback</title>
      <author><first>Daechul</first><last>Ahn</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Yura</first><last>Choi</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Dongyeop</first><last>Kang</last><affiliation>University of Minnesota</affiliation></author>
      <author><first>Jonghyun</first><last>Choi</last><affiliation>Seoul National University</affiliation></author>
      <pages>923-940</pages>
      <abstract>Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). Previous approaches for VLMMs involve Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and additional learnable parameters. Here, aligning video with text, and vice versa, remains a challenge, primarily due to the insufficient quality and quantity of multimodal instruction-tune data compared to that of text-only. This discrepancy often results in alignments that poorly ground the video content. To address this, we present a novel alignment strategy that employs a multimodal AI system equipped with Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. Our approach uniquely integrates detailed video descriptions as context into a multimodal AI system during the preference feedback generation to enrich the understanding of video content, a process we call context-aware reward modeling. Empirical evaluations on various video benchmarks demonstrate that our VLM-RLAIF outperforms existing approaches, including the SFT model. We commit to open-sourcing our code, models, and datasets to foster further research in this area.</abstract>
      <url hash="45171fd8">2024.acl-long.52</url>
      <bibkey>ahn-etal-2024-tuning</bibkey>
      <doi>10.18653/v1/2024.acl-long.52</doi>
    </paper>
    <paper id="53">
      <title>Prompt Refinement with Image Pivot for Text-to-Image Generation</title>
      <author><first>Jingtao</first><last>Zhan</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Qingyao</first><last>Ai</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yiqun</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yingwei</first><last>Pan</last><affiliation>JD.com</affiliation></author>
      <author><first>Ting</first><last>Yao</last><affiliation>JD AI Research</affiliation></author>
      <author><first>Jiaxin</first><last>Mao</last><affiliation>Renmin University of China, Tsinghua University</affiliation></author>
      <author><first>Shaoping</first><last>Ma</last></author>
      <author><first>Tao</first><last>Mei</last><affiliation>JD Explore Academy</affiliation></author>
      <pages>941-954</pages>
      <abstract>For text-to-image generation, automatically refining user-provided natural language prompts into the keyword-enriched prompts favored by systems is essential for the user experience. Such a prompt refinement process is analogous to translating the prompt from “user languages” into “system languages”. However, the scarcity of such parallel corpora makes it difficult to train a prompt refinement model. Inspired by zero-shot machine translation techniques, we introduce Prompt Refinement with Image Pivot (PRIP). PRIP innovatively uses the latent representation of a user-preferred image as an intermediary “pivot” between the user and system languages. It decomposes the refinement process into two data-rich tasks: inferring representations of user-preferred images from user languages and subsequently translating image representations into system languages. Thus, it can leverage abundant data for training. Extensive experiments show that PRIP substantially outperforms a wide range of baselines and effectively transfers to unseen systems in a zero-shot manner.</abstract>
      <url hash="114bfad9">2024.acl-long.53</url>
      <bibkey>zhan-etal-2024-prompt</bibkey>
      <doi>10.18653/v1/2024.acl-long.53</doi>
    </paper>
    <paper id="54">
      <title>Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation</title>
      <author><first>Masato</first><last>Mita</last><affiliation>CyberAgent Inc.</affiliation></author>
      <author><first>Soichiro</first><last>Murakami</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Akihiko</first><last>Kato</last></author>
      <author><first>Peinan</first><last>Zhang</last><affiliation>CyberAgent AI Lab</affiliation></author>
      <pages>955-972</pages>
      <abstract>In response to the limitations of manual ad creation, significant research has been conducted in the field of automatic ad text generation (ATG). However, the lack of comprehensive benchmarks and well-defined problem sets has made comparing different methods challenging. To tackle these challenges, we standardize the task of ATG and propose a first benchmark dataset, CAMERA, carefully designed and enabling the utilization of multi-modal information and facilitating industry-wise evaluations. Our extensive experiments with a variety of nine baselines, from classical methods to state-of-the-art models including large language models (LLMs), show the current state and the remaining challenges. We also explore how existing metrics in ATG and an LLM-based evaluator align with human evaluations.</abstract>
      <url hash="2f75526d">2024.acl-long.54</url>
      <bibkey>mita-etal-2024-striking</bibkey>
      <doi>10.18653/v1/2024.acl-long.54</doi>
    </paper>
    <paper id="55">
      <title><fixed-case>A</fixed-case>bs<fixed-case>I</fixed-case>nstruct: Eliciting Abstraction Ability from <fixed-case>LLM</fixed-case>s through Explanation Tuning with Plausibility Estimation</title>
      <author><first>Zhaowei</first><last>Wang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Wei</first><last>Fan</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Qing</first><last>Zong</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Sehyun</first><last>Choi</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Ginny</first><last>Wong</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Simon</first><last>See</last><affiliation>NVIDIA</affiliation></author>
      <pages>973-994</pages>
      <abstract>Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study. Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored. In this work, we design the framework AbsInstruct to enhance LLMs’ abstraction ability through instruction tuning. The framework builds instructions with in-depth explanations to assist LLMs in capturing the underlying rationale of abstraction. Meanwhile, we introduce a plausibility estimator to select instructions that are more consistent with the abstraction knowledge of LLMs to be aligned. Then, our framework combines abstraction instructions with general-purpose ones to build a hybrid dataset. Extensive experiments and analyses demonstrate that our framework can considerably enhance LLMs’ abstraction ability with strong generalization performance while maintaining their general instruction-following abilities.</abstract>
      <url hash="fc5002e9">2024.acl-long.55</url>
      <bibkey>wang-etal-2024-absinstruct</bibkey>
      <doi>10.18653/v1/2024.acl-long.55</doi>
    </paper>
    <paper id="56">
      <title>Reflect-<fixed-case>RL</fixed-case>: Two-Player Online <fixed-case>RL</fixed-case> Fine-Tuning for <fixed-case>LM</fixed-case>s</title>
      <author><first>Runlong</first><last>Zhou</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Simon</first><last>Du</last><affiliation>University of Washington</affiliation></author>
      <author><first>Beibin</first><last>Li</last><affiliation>Microsoft Research</affiliation></author>
      <pages>995-1015</pages>
      <abstract>As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective approach to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using SFT and online RL, where a frozen reflection model (player) assists the policy model (player). To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more efficiently. Empirically, we verify that Reflect-RL outperforms SFT and online RL without reflection. Testing results indicate GPT-2 XL 1.56B fine-tuned with Reflect-RL outperforms larger open-source LMs, such as Mistral 7B. The benchmarks, dataset, and code involved in this work are publicly available: https://github.com/zhourunlong/Reflect-RL.</abstract>
      <url hash="e5710411">2024.acl-long.56</url>
      <bibkey>zhou-etal-2024-reflect</bibkey>
      <doi>10.18653/v1/2024.acl-long.56</doi>
    </paper>
    <paper id="57">
      <title>Can <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>’s Performance be Improved on Verb Metaphor Detection Tasks? Bootstrapping and Combining Tacit Knowledge</title>
      <author><first>Cheng</first><last>Yang</last></author>
      <author><first>Puli</first><last>Chen</last></author>
      <author><first>Qingbao</first><last>Huang</last><affiliation>Guangxi University</affiliation></author>
      <pages>1016-1027</pages>
      <abstract>Metaphors detection, as an important task in the field of NLP, has been receiving sustained academic attention in recent years. Current researches focus supervised metaphors detection systems, which usually require large-scale, high-quality labeled data support. The emerge of large language models (e.g., ChatGPT) has made many NLP tasks (e.g., automatic summarization and dialogue systems) a qualitative leap. However, it is worth noting that the use of ChatGPT for unsupervised metaphors detection is often challenged with less-than-expected performance. Therefore, the aim of our work is to explore how to bootstrap and combine ChatGPT by detecting the most prevalent verb metaphors among metaphors. Our approach first utilizes ChatGPT to obtain literal collocations of target verbs and subject-object pairs of verbs in the text to be detected. Subsequently, these literal collocations and subject-object pairs are mapped to the same set of topics, and finally the verb metaphors are detected through the analysis of entailment relations. The experimental results show that our method achieves the best performance on the unsupervised verb metaphors detection task compared to existing unsupervised methods or direct prediction using ChatGPT. Our code is available at https://github.com/VILAN-Lab/Unsupervised-Metaphor-Detection.</abstract>
      <url hash="f8175dbb">2024.acl-long.57</url>
      <bibkey>yang-etal-2024-chatgpts</bibkey>
      <doi>10.18653/v1/2024.acl-long.57</doi>
    </paper>
    <paper id="58">
      <title>Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning</title>
      <author><first>Zhaorui</first><last>Yang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Tianyu</first><last>Pang</last><affiliation>Sea AI Lab</affiliation></author>
      <author><first>Haozhe</first><last>Feng</last><affiliation>State Key Lab of CAD&amp;CG, Zhejiang University</affiliation></author>
      <author><first>Han</first><last>Wang</last></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Minfeng</first><last>Zhu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Qian</first><last>Liu</last><affiliation>Sea AI Lab</affiliation></author>
      <pages>1028-1043</pages>
      <abstract>The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at https://github.com/sail-sg/sdft.</abstract>
      <url hash="57549780">2024.acl-long.58</url>
      <bibkey>yang-etal-2024-self</bibkey>
      <doi>10.18653/v1/2024.acl-long.58</doi>
    </paper>
    <paper id="59">
      <title>An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation</title>
      <author><first>Kun</first><last>Zhu</last></author>
      <author><first>Xiaocheng</first><last>Feng</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xiyuan</first><last>Du</last></author>
      <author><first>Yuxuan</first><last>Gu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Weijiang</first><last>Yu</last></author>
      <author><first>Haotian</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Qianglong</first><last>Chen</last></author>
      <author><first>Zheng</first><last>Chu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jingchang</first><last>Chen</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>1044-1069</pages>
      <abstract>Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data. One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression. In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation. Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage. In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards. Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with 2.5% compression rate.</abstract>
      <url hash="1360d98f">2024.acl-long.59</url>
      <bibkey>zhu-etal-2024-information</bibkey>
      <doi>10.18653/v1/2024.acl-long.59</doi>
    </paper>
    <paper id="60">
      <title><fixed-case>RORA</fixed-case>: Robust Free-Text Rationale Evaluation</title>
      <author><first>Zhengping</first><last>Jiang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Yining</first><last>Lu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Hanjie</first><last>Chen</last><affiliation>Rice University</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Anqi</first><last>Liu</last><affiliation>Johns Hopkins University, California Institute of Technology and University of Illinois, Chicago</affiliation></author>
      <pages>1070-1087</pages>
      <abstract>Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model’s decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing metrics rely on the degree to which a rationale <i>supports</i> a target label, but we find these fall short in evaluating rationales that inadvertently <i>leak the label</i>. To address this problem, we propose RORA, a RObust free-text RAtionale evaluation against label leakage. RORA quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional <tex-math>\mathcal{V}</tex-math>-information (Hewitt et al., 2021) with a predictive family robust against leaky features that can be exploited by a small model. RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-generated rationales, particularly demonstrating robustness against label leakage. We also show that RORA aligns well with human judgment, providing a more reliable and accurate measurement across diverse free-text rationales.</abstract>
      <url hash="fbc32d6e">2024.acl-long.60</url>
      <bibkey>jiang-etal-2024-rora</bibkey>
      <doi>10.18653/v1/2024.acl-long.60</doi>
    </paper>
    <paper id="61">
      <title>Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents</title>
      <author><first>Cheng</first><last>Qian</last></author>
      <author><first>Bingxiang</first><last>He</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhong</first><last>Zhuang</last></author>
      <author><first>Jia</first><last>Deng</last></author>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Xin</first><last>Cong</last></author>
      <author><first>Zhong</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>1088-1113</pages>
      <abstract>Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions. Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users’ implicit intentions through explicit queries. Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction. Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires about user intentions, and refines them into actionable goals before starting downstream agent task execution. Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understanding and execution, revealing that our approach notably excels at identifying vague user tasks, recovering and summarizing critical missing information, setting precise and necessary agent execution goals, and minimizing redundant tool usage, thus boosting overall efficiency.</abstract>
      <url hash="34cc0cea">2024.acl-long.61</url>
      <bibkey>qian-etal-2024-tell</bibkey>
      <doi>10.18653/v1/2024.acl-long.61</doi>
    </paper>
    <paper id="62">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>P</fixed-case>rotein: Aligning Human and Protein Language via Knowledge Instruction</title>
      <author><first>Zeyuan</first><last>Wang</last></author>
      <author><first>Qiang</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Keyan</first><last>Ding</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ming</first><last>Qin</last></author>
      <author><first>Xiang</first><last>Zhuang</last></author>
      <author><first>Xiaotong</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>1114-1136</pages>
      <abstract>Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing the annotation imbalance and the absence of instructional signals in the existing protein-text corpus. In particular, the instructions inherit the structural relations between proteins and function annotations in knowledge graphs, which empowers our model to engage in the causal modeling of protein functions, akin to the chain-of-thought processes in natural languages. Extensive experiments on bidirectional protein-text generation tasks show that InstructProtein outperforms state-of-the-art LLMs by a large margin.</abstract>
      <url hash="9e08b218">2024.acl-long.62</url>
      <bibkey>wang-etal-2024-instructprotein</bibkey>
      <doi>10.18653/v1/2024.acl-long.62</doi>
    </paper>
    <paper id="63">
      <title><fixed-case>C</fixed-case>on<fixed-case>S</fixed-case>i<fixed-case>DERS</fixed-case>-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models</title>
      <author><first>Aparna</first><last>Elangovan</last><affiliation>Amazon</affiliation></author>
      <author><first>Ling</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Lei</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Sravan Babu</first><last>Bodapati</last><affiliation>Amazon, Amazon and Amazon</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>1137-1160</pages>
      <abstract>In this position paper, we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon the insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable. The conclusions from these evaluations, therefore, must consider factors such as usability, aesthetics and cognitive biases. We highlight how cognitive biases can conflate fluent information and truthfulness, and how cognitive uncertainty affects the reliability of rating scores such as Likert. Furthermore, the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models - which requires effective test sets. Scalability of human evaluation is also crucial to wider adoption. Hence, to design an effective human evaluation system in the age of generative NLP we propose the ConSiDERS-The-Human evaluation framework consisting of 6 pillars - Consistency, Scoring Criteria, Differentiating, User Experience, Responsible, and Scalability.</abstract>
      <url hash="338844db">2024.acl-long.63</url>
      <bibkey>elangovan-etal-2024-considers</bibkey>
      <doi>10.18653/v1/2024.acl-long.63</doi>
    </paper>
    <paper id="64">
      <title>Linguistically Conditioned Semantic Textual Similarity</title>
      <author><first>Jingxuan</first><last>Tu</last></author>
      <author><first>Keer</first><last>Xu</last></author>
      <author><first>Liulu</first><last>Yue</last></author>
      <author><first>Bingyang</first><last>Ye</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Kyeongmin</first><last>Rim</last></author>
      <author><first>James</first><last>Pustejovsky</last><affiliation>Brandeis University</affiliation></author>
      <pages>1161-1172</pages>
      <abstract>Semantic textual similarity (STS) is a fundamental NLP task that measures the semantic similarity between a pair of sentences. In order to reduce the inherent ambiguity posed from the sentences, a recent work called Conditional STS (C-STS) has been proposed to measure the sentences’ similarity conditioned on a certain aspect. Despite the popularity of C-STS, we find that the current C-STS dataset suffers from various issues that could impede proper evaluation on this task. In this paper, we reannotate the C-STS validation set and observe an annotator discrepancy on 55% of the instances resulting from the annotation errors in the original label, ill-defined conditions, and the lack of clarity in the task definition. After a thorough dataset analysis, we improve the C-STS task by leveraging the models’ capability to understand the conditions under a QA task setting. With the generated answers, we present an automatic error identification pipeline that is able to identify annotation errors from the C-STS data with over 80% F1 score. We also propose a new method that largely improves the performance over baselines on the C-STS data by training the models with the answers. Finally we discuss the conditionality annotation based on the typed-feature structure (TFS) of entity types. We show in examples that the TFS is able to provide a linguistic foundation for constructing C-STS data with new conditions.</abstract>
      <url hash="2a9e922d">2024.acl-long.64</url>
      <bibkey>tu-etal-2024-linguistically</bibkey>
      <doi>10.18653/v1/2024.acl-long.64</doi>
    </paper>
    <paper id="65">
      <title>Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future</title>
      <author><first>Zheng</first><last>Chu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jingchang</first><last>Chen</last></author>
      <author><first>Qianglong</first><last>Chen</last></author>
      <author><first>Weijiang</first><last>Yu</last></author>
      <author><first>Tao</first><last>He</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Haotian</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Weihua</first><last>Peng</last></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>1173-1203</pages>
      <abstract>Reasoning, a fundamental cognitive process integral to human intelligence, has garnered substantial interest within artificial intelligence.Notably, recent studies have revealed that chain-of-thought prompting significantly enhances LLM’s reasoning capabilities, which attracts widespread attention from both academics and industry.In this paper, we systematically investigate relevant research, summarizing advanced methods through a meticulous taxonomy that offers novel perspectives.Moreover, we delve into the current frontiers and delineate the challenges and future directions, thereby shedding light on future research.Furthermore, we engage in a discussion about open questions.We hope this paper serves as an introduction for beginners and fosters future research.Resources have been made publicly available at https://github.com/zchuz/CoT-Reasoning-Survey</abstract>
      <url hash="60c70426">2024.acl-long.65</url>
      <bibkey>chu-etal-2024-navigate</bibkey>
      <doi>10.18653/v1/2024.acl-long.65</doi>
    </paper>
    <paper id="66">
      <title><fixed-case>T</fixed-case>ime<fixed-case>B</fixed-case>ench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models</title>
      <author><first>Zheng</first><last>Chu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jingchang</first><last>Chen</last></author>
      <author><first>Qianglong</first><last>Chen</last></author>
      <author><first>Weijiang</first><last>Yu</last></author>
      <author><first>Haotian</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>1204-1228</pages>
      <abstract>Grasping the concept of time is a fundamental facet of human cognition, indispensable for truly comprehending the intricacies of the world.Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark.To address this, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena.TimeBench provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models.We conduct extensive experiments on GPT-4, LLaMA2, and other popular LLMs under various settings.Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning.Besides, LLMs exhibit capability discrepancies across different reasoning categories.Furthermore, we thoroughly analyze the impact of multiple aspects on temporal reasoning and emphasize the associated challenges.We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning.Code and data are available at https://github.com/zchuz/TimeBench.</abstract>
      <url hash="f5f0ca82">2024.acl-long.66</url>
      <bibkey>chu-etal-2024-timebench</bibkey>
      <doi>10.18653/v1/2024.acl-long.66</doi>
    </paper>
    <paper id="67">
      <title><fixed-case>B</fixed-case>eam<fixed-case>A</fixed-case>gg<fixed-case>R</fixed-case>: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering</title>
      <author><first>Zheng</first><last>Chu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jingchang</first><last>Chen</last></author>
      <author><first>Qianglong</first><last>Chen</last></author>
      <author><first>Haotian</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Kun</first><last>Zhu</last></author>
      <author><first>Xiyuan</first><last>Du</last></author>
      <author><first>Weijiang</first><last>Yu</last></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>1229-1248</pages>
      <abstract>Large language models (LLMs) have demonstrated strong reasoning capabilities.Nevertheless, they still suffer from factual errors when tackling knowledge-intensive tasks.Retrieval-augmented reasoning represents a promising approach.However, significant challenges still persist, including inaccurate and insufficient retrieval for complex questions, as well as difficulty in integrating multi-source knowledge.To address this, we propose Beam Aggregation Reasoning (BeamAggR), a reasoning framework for knowledge-intensive multi-hop QA.BeamAggR explores and prioritizes promising answers at each hop of question.Concretely, we parse the complex questions into trees, which include atom and composite questions, followed by bottom-up reasoning.For atomic questions, the LLM conducts reasoning on multi-source knowledge to get answer candidates.For composite questions, the LLM combines beam candidates, explores multiple reasoning paths through probabilistic aggregation, and prioritizes the most promising trajectory.Extensive experiments on four open-domain multi-hop reasoning datasets show that our method significantly outperforms SOTA methods by 8.5%.Furthermore, our analysis reveals that BeamAggR elicits better knowledge collaboration and answer aggregation.</abstract>
      <url hash="294e380a">2024.acl-long.67</url>
      <bibkey>chu-etal-2024-beamaggr</bibkey>
      <doi>10.18653/v1/2024.acl-long.67</doi>
    </paper>
    <paper id="68">
      <title><fixed-case>ANALOGYKB</fixed-case>: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base</title>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Jiangjie</first><last>Chen</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Changzhi</first><last>Sun</last></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Deqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <pages>1249-1265</pages>
      <abstract>Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large language models (LLMs), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables both smaller LMs and LLMs to gain better analogical reasoning capabilities. Resources of this paper can be found at https://github.com/siyuyuan/analogykb.</abstract>
      <url hash="4e72856b">2024.acl-long.68</url>
      <bibkey>yuan-etal-2024-analogykb</bibkey>
      <doi>10.18653/v1/2024.acl-long.68</doi>
    </paper>
    <paper id="69">
      <title><fixed-case>T</fixed-case>a<fixed-case>SL</fixed-case>: Continual Dialog State Tracking via Task Skill Localization and Consolidation</title>
      <author><first>Yujie</first><last>Feng</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xu</first><last>Chu</last></author>
      <author><first>Yongxin</first><last>Xu</last></author>
      <author><first>Guangyuan</first><last>Shi</last></author>
      <author><first>Bo</first><last>Liu</last></author>
      <author><first>Xiao-Ming</first><last>Wu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <pages>1266-1279</pages>
      <abstract>A practical dialogue system requires the capacity for ongoing skill acquisition and adaptability to new tasks while preserving prior knowledge. However, current methods for Continual Dialogue State Tracking (DST), a crucial function of dialogue systems, struggle with the catastrophic forgetting issue and knowledge transfer between tasks. We present TaSL, a novel framework for task skill localization and consolidation that enables effective knowledge transfer without relying on memory replay. TaSL uses a novel group-wise technique to pinpoint task-specific and task-shared areas. Additionally, a fine-grained skill consolidation strategy protects task-specific knowledge from being forgotten while updating shared knowledge for bi-directional knowledge transfer. As a result, TaSL strikes a balance between preserving previous knowledge and excelling at new tasks. Comprehensive experiments on various backbones highlight the significant performance improvements of TaSL, with a 7.6% absolute increase in Avg. JGA and an 11% absolute rise in BWT metrics over existing state-of-the-art methods. The source code is provided for reproducibility.</abstract>
      <url hash="5c8d9633">2024.acl-long.69</url>
      <bibkey>feng-etal-2024-tasl</bibkey>
      <doi>10.18653/v1/2024.acl-long.69</doi>
    </paper>
    <paper id="70">
      <title><fixed-case>D</fixed-case>eep<fixed-case>S</fixed-case>eek<fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</title>
      <author><first>Damai</first><last>Dai</last></author>
      <author><first>Chengqi</first><last>Deng</last></author>
      <author><first>Chenggang</first><last>Zhao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>R.x.</first><last>Xu</last></author>
      <author><first>Huazuo</first><last>Gao</last></author>
      <author><first>Deli</first><last>Chen</last><affiliation>DeepSeek AI</affiliation></author>
      <author><first>Jiashi</first><last>Li</last></author>
      <author><first>Wangding</first><last>Zeng</last></author>
      <author><first>Xingkai</first><last>Yu</last></author>
      <author><first>Y.</first><last>Wu</last><affiliation>Deepseek</affiliation></author>
      <author><first>Zhenda</first><last>Xie</last><affiliation>DeepSeek AI</affiliation></author>
      <author><first>Y.k.</first><last>Li</last></author>
      <author><first>Panpan</first><last>Huang</last><affiliation>DeepSeek and High-flyer</affiliation></author>
      <author><first>Fuli</first><last>Luo</last><affiliation>DeepSeek</affiliation></author>
      <author><first>Chong</first><last>Ruan</last></author>
      <author><first>Zhifang</first><last>Sui</last><affiliation>Peking University</affiliation></author>
      <author><first>Wenfeng</first><last>Liang</last></author>
      <pages>1280-1297</pages>
      <abstract>In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-<tex-math>K</tex-math> out of <tex-math>N</tex-math> experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into <tex-math>mN</tex-math> ones and activating <tex-math>mK</tex-math> from them, allowing for a more flexible combination of activated experts; (2) isolating <tex-math>K_s</tex-math> experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 <tex-math>\times</tex-math> expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which sets the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with DeepSeek 7B and LLaMA2 7B, with only about 40% of computations.</abstract>
      <url hash="a2d4c610">2024.acl-long.70</url>
      <bibkey>dai-etal-2024-deepseekmoe</bibkey>
      <doi>10.18653/v1/2024.acl-long.70</doi>
    </paper>
    <paper id="71">
      <title>Grounding Language Model with Chunking-Free In-Context Retrieval</title>
      <author><first>Hongjin</first><last>Qian</last></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Kelong</first><last>Mao</last></author>
      <author><first>Yujia</first><last>Zhou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>1298-1311</pages>
      <abstract>This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval.The CFIC approach addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further enhanced by incorporating two innovative decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies not only improve the efficiency of the retrieval process but also ensure that the fidelity of the generated grounding text evidence is maintained.Our evaluations of CFIC on a range of open question answering datasets demonstrate its superiority in retrieving relevant and accurate information, offering a significant improvement over traditional methods. By doing away with the need for document chunking, CFIC presents a more streamlined, effective, and efficient retrieval solution, making it a valuable advancement in the field of RAG systems.</abstract>
      <url hash="358ee3d4">2024.acl-long.71</url>
      <bibkey>qian-etal-2024-grounding</bibkey>
      <doi>10.18653/v1/2024.acl-long.71</doi>
    </paper>
    <paper id="72">
      <title>Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation</title>
      <author><first>Jiaxin</first><last>Bai</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yicheng</first><last>Wang</last></author>
      <author><first>Tianshi</first><last>Zheng</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yue</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>1312-1329</pages>
      <abstract>Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored. To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG. In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations. We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusions drawn from generated hypotheses according to the KG. Experiments show that, with RLF-KG’s assistance, the generated hypotheses provide better explanations, and achieve state-of-the-art results on three widely used KGs.</abstract>
      <url hash="c7e640b4">2024.acl-long.72</url>
      <bibkey>bai-etal-2024-advancing</bibkey>
      <doi>10.18653/v1/2024.acl-long.72</doi>
    </paper>
    <paper id="73">
      <title>Active Prompting with Chain-of-Thought for Large Language Models</title>
      <author><first>Shizhe</first><last>Diao</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Pengcheng</first><last>Wang</last></author>
      <author><first>Yong</first><last>Lin</last></author>
      <author><first>Rui</first><last>Pan</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xiang</first><last>Liu</last></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>1330-1350</pages>
      <abstract>The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs’ ability to produce high-quality answers. In particular, an effective approach for complex question-and-answering tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving superior performance on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationships demonstrate the effectiveness of our method.</abstract>
      <url hash="42f7b69e">2024.acl-long.73</url>
      <bibkey>diao-etal-2024-active</bibkey>
      <doi>10.18653/v1/2024.acl-long.73</doi>
    </paper>
    <paper id="74">
      <title><fixed-case>E</fixed-case>asy<fixed-case>G</fixed-case>en: Easing Multimodal Generation with <fixed-case>B</fixed-case>i<fixed-case>D</fixed-case>iffuser and <fixed-case>LLM</fixed-case>s</title>
      <author><first>Xiangyu</first><last>Zhao</last></author>
      <author><first>Bo</first><last>Liu</last></author>
      <author><first>Qijiong</first><last>Liu</last></author>
      <author><first>Guangyuan</first><last>Shi</last></author>
      <author><first>Xiao-Ming</first><last>Wu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <pages>1351-1370</pages>
      <abstract>We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge modalities, EasyGen leverages BiDiffuser, a bidirectional conditional diffusion model, to foster more efficient modality interactions. EasyGen achieves text generation by training a projection layer linking BiDiffuser and an LLM, and facilities image generation by training an adapter to align the LLM’s text space with the BiDiffuser’s image space. Comprehensive quantitative and qualitative experiments show that EasyGen excels in data-efficient training, high-quality image generation, and extendibility, effectively addressing the challenges in multimodal generation.</abstract>
      <url hash="e15d6a94">2024.acl-long.74</url>
      <bibkey>zhao-etal-2024-easygen</bibkey>
      <doi>10.18653/v1/2024.acl-long.74</doi>
    </paper>
    <paper id="75">
      <title>Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search</title>
      <author><first>Haochen</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xin</first><last>Zhou</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zhiqi</first><last>Shen</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <pages>1371-1389</pages>
      <abstract>In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%), and fine-tuned dense (up to 23.6%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances. The source code and data are available at https://github.com/Alex-HaochenLi/ReCo.</abstract>
      <url hash="8e5a74b6">2024.acl-long.75</url>
      <bibkey>li-etal-2024-rewriting</bibkey>
      <doi>10.18653/v1/2024.acl-long.75</doi>
    </paper>
    <paper id="76">
      <title>A Multidimensional Framework for Evaluating Lexical Semantic Change with Social Science Applications</title>
      <author><first>Naomi</first><last>Baes</last></author>
      <author><first>Nick</first><last>Haslam</last></author>
      <author><first>Ekaterina</first><last>Vylomova</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>1390-1415</pages>
      <abstract>Historical linguists have identified multiple forms of lexical semantic change. We present a three-dimensional framework for integrating these forms and a unified computational methodology for evaluating them concurrently. The dimensions represent increases or decreases in semantic 1) sentiment (valence of a target word’s collocates), 2) intensity (emotional arousal of collocates or the frequency of intensifiers), and 3) breadth (diversity of contexts in which the target word appears). These dimensions can be complemented by evaluation of shifts in the frequency of the target words and the thematic content of its collocates. This framework enables lexical semantic change to be mapped economically and systematically and has applications in computational social science. We present an illustrative analysis of semantic shifts in <i>mental health</i> and <i>mental illness</i> in two corpora, demonstrating patterns of semantic change that illuminate contemporary concerns about pathologization, stigma, and concept creep.</abstract>
      <url hash="6ce92706">2024.acl-long.76</url>
      <bibkey>baes-etal-2024-multidimensional</bibkey>
      <doi>10.18653/v1/2024.acl-long.76</doi>
    </paper>
    <paper id="77">
      <title>Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal</title>
      <author><first>Jianheng</first><last>Huang</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Ante</first><last>Wang</last></author>
      <author><first>Chengyi</first><last>Yang</last></author>
      <author><first>Xinting</first><last>Liao</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Junfeng</first><last>Yao</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <pages>1416-1428</pages>
      <abstract>Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model’s ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains.</abstract>
      <url hash="69b5a7f5">2024.acl-long.77</url>
      <bibkey>huang-etal-2024-mitigating</bibkey>
      <doi>10.18653/v1/2024.acl-long.77</doi>
    </paper>
    <paper id="78">
      <title>Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency</title>
      <author><first>Baizhou</first><last>Huang</last></author>
      <author><first>Shuai</first><last>Lu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xiaojun</first><last>Wan</last><affiliation>Peking University</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>1429-1450</pages>
      <abstract>Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs’ reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph.MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4.</abstract>
      <url hash="75470d3f">2024.acl-long.78</url>
      <bibkey>huang-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.acl-long.78</doi>
    </paper>
    <paper id="79">
      <title>Citation-Enhanced Generation for <fixed-case>LLM</fixed-case>-based Chatbots</title>
      <author><first>Weitao</first><last>Li</last></author>
      <author><first>Junkai</first><last>Li</last></author>
      <author><first>Weizhi</first><last>Ma</last><affiliation>Tsinghua University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>1451-1466</pages>
      <abstract>Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc Citation-Enhanced Generation (CEG) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various LLMs. Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks. Our code and datasets can be found at https://github.com/Tsinghua-dhy/CEG.</abstract>
      <url hash="70adc7f6">2024.acl-long.79</url>
      <bibkey>li-etal-2024-citation</bibkey>
      <doi>10.18653/v1/2024.acl-long.79</doi>
    </paper>
    <paper id="80">
      <title>Transitive Consistency Constrained Learning for Entity-to-Entity Stance Detection</title>
      <author><first>Haoyang</first><last>Wen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Eduard</first><last>Hovy</last><affiliation>University of Melbourne and Carnegie Mellon University</affiliation></author>
      <author><first>Alexander</first><last>Hauptmann</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>1467-1480</pages>
      <abstract>Entity-to-entity stance detection identifies the stance between a pair of entities with a directed link that indicates the source, target and polarity. It is a streamlined task without the complex dependency structure for structural sentiment analysis, while it is more informative compared to most previous work assuming that the source is the author. Previous work performs entity-to-entity stance detection training on individual entity pairs. However, stances between inter-connected entity pairs may be correlated. In this paper, we propose transitive consistency constrained learning, which first finds connected entity pairs and their stances, and adds an additional objective to enforce the transitive consistency. We explore consistency training on both classification-based and generation-based models and conduct experiments to compare consistency training with previous work and large language models with in-context learning. Experimental results illustrate that the inter-correlation of stances in political news can be used to improve the entity-to-entity stance detection model, while overly strict consistency enforcement may have a negative impact. In addition, we find that large language models struggle with predicting link direction and neutral labels in this task.</abstract>
      <url hash="69b4a056">2024.acl-long.80</url>
      <bibkey>wen-etal-2024-transitive</bibkey>
      <doi>10.18653/v1/2024.acl-long.80</doi>
    </paper>
    <paper id="81">
      <title>Feature-Adaptive and Data-Scalable In-Context Learning</title>
      <author><first>Jiahao</first><last>Li</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Quan</first><last>Wang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Licheng</first><last>Zhang</last></author>
      <author><first>Guoqing</first><last>Jin</last></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>1481-1494</pages>
      <abstract>In-context learning (ICL), which promotes inference with several demonstrations, has become a widespread paradigm to stimulate LLM capabilities for downstream tasks. Due to context length constraints, it cannot be further improved in spite of more training data, and general features directly from LLMs in ICL are not adaptive to the specific downstream task. In this paper, we propose a feature-adaptive and data-scalable in-context learning framework (FADS-ICL), which can leverage task-adaptive features to promote inference on the downstream task, with the supervision of beyond-context samples.Specifically, it first extracts general features of beyond-context samples via the LLM with ICL input form one by one, and introduces a task-specific modulator to perform feature refinement and prediction after fitting a specific downstream task. We conduct extensive experiments on FADS-ICL under varying data settings (4~128 shots) and LLM scale (0.8~70B) settings. Experimental results show that FADS-ICL consistently outperforms previous state-of-the-art methods by a significant margin under all settings, verifying the effectiveness and superiority of FADS-ICL. For example, under the 1.5B and 32 shots setting, FADS-ICL can achieve <b>+14.3</b> average accuracy from feature adaptation over vanilla ICL on 10 datasets, with <b>+6.2</b> average accuracy over the previous state-of-the-art method, and the performance can further improve with increasing training data.</abstract>
      <url hash="0a3c6a0d">2024.acl-long.81</url>
      <bibkey>li-etal-2024-feature-adaptive</bibkey>
      <doi>10.18653/v1/2024.acl-long.81</doi>
    </paper>
    <paper id="82">
      <title>Probing the Multi-turn Planning Capabilities of <fixed-case>LLM</fixed-case>s via 20 Question Games</title>
      <author><first>Yizhe</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Jiarui</first><last>Lu</last><affiliation>Apple</affiliation></author>
      <author><first>Navdeep</first><last>Jaitly</last><affiliation>Apple</affiliation></author>
      <pages>1495-1516</pages>
      <abstract>Large language models (LLMs) are effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging.In this paper, we offer a surrogate problem which assesses an LLMs’s capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This entity-deducing game can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models.We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs like GPT-4 outperform human players by a large margin. We further employ Behavior Cloning (BC) to examine whether a weaker model is capable of imitating a stronger model and generalizing to data or domains, using only the demonstrations from a stronger model. We finally propose to use Reinforcement Learning to enhance reasoning and planning capacity of Vicuna models through episodes of game playing, which lead to significant performance improvement. We hope that this problem offers insights into how autonomous agents could be trained to behave more intelligently in ambiguous circumstances.</abstract>
      <url hash="342ececa">2024.acl-long.82</url>
      <bibkey>zhang-etal-2024-probing</bibkey>
      <doi>10.18653/v1/2024.acl-long.82</doi>
    </paper>
    <paper id="83">
      <title><fixed-case>W</fixed-case>ater<fixed-case>B</fixed-case>ench: Towards Holistic Evaluation of Watermarks for Large Language Models</title>
      <author><first>Shangqing</first><last>Tu</last></author>
      <author><first>Yuliang</first><last>Sun</last></author>
      <author><first>Yushi</first><last>Bai</last></author>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>1517-1542</pages>
      <abstract>To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For benchmarking procedure, to ensure an apples-to-apples comparison, we first adjust each watermarking method’s hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For task selection, we diversify the input and output length to form a five-category taxonomy, covering 9 tasks. (3) For evaluation metric, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate 4 open-source watermarks on 2 LLMs under 2 watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at https://github.com/THU-KEG/WaterBench.</abstract>
      <url hash="7bb21719">2024.acl-long.83</url>
      <bibkey>tu-etal-2024-waterbench</bibkey>
      <doi>10.18653/v1/2024.acl-long.83</doi>
    </paper>
    <paper id="84">
      <title>Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models</title>
      <author><first>Yida</first><last>Zhao</last></author>
      <author><first>Chao</first><last>Lou</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>Kewei</first><last>Tu</last><affiliation>ShanghaiTech University</affiliation></author>
      <pages>1543-1556</pages>
      <abstract>Syntactic Transformer language models aim to achieve better generalization through simultaneously modeling syntax trees and sentences. While prior work has been focusing on adding constituency-based structures to Transformers, we introduce Dependency Transformer Grammars (DTGs), a new class of Transformer language model with explicit dependency-based inductive bias. DTGs simulate dependency transition systems with constrained attention patterns by modifying attention masks, incorporate the stack information through relative positional encoding, and augment dependency arc representation with a combination of token embeddings and operation embeddings. When trained on a dataset of sentences annotated with dependency trees, DTGs achieve better generalization while maintaining comparable perplexity with Transformer language model baselines. DTGs also outperform recent constituency-based models, showing that dependency can better guide Transformer language models. Our code is released at https://github.com/zhaoyd1/Dep_Transformer_Grammars.</abstract>
      <url hash="da9d50ca">2024.acl-long.84</url>
      <bibkey>zhao-etal-2024-dependency</bibkey>
      <doi>10.18653/v1/2024.acl-long.84</doi>
    </paper>
    <paper id="85">
      <title>A Non-autoregressive Generation Framework for End-to-End Simultaneous Speech-to-Any Translation</title>
      <author><first>Zhengrui</first><last>Ma</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Qingkai</first><last>Fang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Shoutao</first><last>Guo</last></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>1557-1575</pages>
      <abstract>Simultaneous translation models play a crucial role in facilitating communication. However, existing research primarily focuses on text-to-text or speech-to-text models, necessitating additional cascade components to achieve speech-to-speech translation. These pipeline methods suffer from error propagation and accumulate delays in each cascade component, resulting in reduced synchronization between the speaker and listener. To overcome these challenges, we propose a novel non-autoregressive generation framework for simultaneous speech translation (NAST-S2<tex-math>x</tex-math>), which integrates speech-to-text and speech-to-speech tasks into a unified end-to-end framework.We develop a non-autoregressive decoder capable of concurrently generating multiple text or acoustic unit tokens upon receiving fixed-length speech chunks. The decoder can generate blank or repeated tokens and employ CTC decoding to dynamically adjust its latency. Experimental results show that NAST-S2<tex-math>x</tex-math> outperforms state-of-the-art models in both speech-to-text and speech-to-speech tasks. It achieves high-quality simultaneous interpretation within a delay of less than 3 seconds and provides a 28× decoding speedup in offline generation.</abstract>
      <url hash="3f90273f">2024.acl-long.85</url>
      <bibkey>ma-etal-2024-non</bibkey>
      <doi>10.18653/v1/2024.acl-long.85</doi>
    </paper>
    <paper id="86">
      <title>Probing Language Models for Pre-training Data Detection</title>
      <author><first>Zhenhua</first><last>Liu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Tong</first><last>Zhu</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Chuanyuan</first><last>Tan</last></author>
      <author><first>Bing</first><last>Liu</last><affiliation>OPPO AI Center</affiliation></author>
      <author><first>Haonan</first><last>Lu</last><affiliation>OPPO Guangdong Mobile Telecommunications Co., Ltd.</affiliation></author>
      <author><first>Wenliang</first><last>Chen</last><affiliation>Soochow University, China</affiliation></author>
      <pages>1576-1587</pages>
      <abstract>Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model’s internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy.</abstract>
      <url hash="c4d6a8ef">2024.acl-long.86</url>
      <bibkey>liu-etal-2024-probing</bibkey>
      <doi>10.18653/v1/2024.acl-long.86</doi>
    </paper>
    <paper id="87">
      <title>Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding</title>
      <author><first>Zhihan</first><last>Zhang</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Chenchen</first><last>Ye</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Yunshan</first><last>Ma</last></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>1588-1606</pages>
      <abstract>The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events.We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.</abstract>
      <url hash="3550d710">2024.acl-long.87</url>
      <bibkey>zhang-etal-2024-analyzing</bibkey>
      <doi>10.18653/v1/2024.acl-long.87</doi>
    </paper>
    <paper id="88">
      <title><fixed-case>IBSEN</fixed-case>: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation</title>
      <author><first>Senyu</first><last>Han</last></author>
      <author><first>Lu</first><last>Chen</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Li-Min</first><last>Lin</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Zhengshan</first><last>Xu</last></author>
      <author><first>Kai</first><last>Yu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>1607-1619</pages>
      <abstract>Large language models have demonstrated their capabilities in storyline creation and human-like character role-playing. Current language model agents mainly focus on reasonable behaviors from the level of individuals, and their behaviors might be hard to constraint on the level of the whole storyline. In this paper we introduce IBSEN, a director-actor coordinate agent framework that generates drama scripts and makes the plot played by agents more controllable. The director agent writes plot outlines that the user desires to see, instructs the actor agents to role-play their characters, and reschedules the plot when human players participate in the scenario to ensure the plot is progressing towards the objective. To evaluate the framework, we create a novel drama plot that involves several actor agents and check the interactions between them under the instruction of the director agent. Evaluation results show that our framework could generate complete, diverse drama scripts from only a rough outline of plot objectives, meanwhile maintaining the characteristics of characters in the drama. Our codes and prompts are available at https://github.com/OpenDFM/ibsen.</abstract>
      <url hash="9aa27df5">2024.acl-long.88</url>
      <bibkey>han-etal-2024-ibsen</bibkey>
      <doi>10.18653/v1/2024.acl-long.88</doi>
    </paper>
    <paper id="89">
      <title>Language Model Adaption for Reinforcement Learning with Natural Language Action Space</title>
      <author><first>Jiangxing</first><last>Wang</last></author>
      <author><first>Jiachen</first><last>Li</last></author>
      <author><first>Xiao</first><last>Han</last></author>
      <author><first>Deheng</first><last>Ye</last><affiliation>Tencent and Tencent</affiliation></author>
      <author><first>Zongqing</first><last>Lu</last><affiliation>Peking University</affiliation></author>
      <pages>1620-1634</pages>
      <abstract>Reinforcement learning with natural language action space often suffers from the curse of dimensionality due to the combinatorial nature of the natural language. Previous research leverages pretrained language models to capture action semantics and reduce the size of the action space. However, since pretrained models are typically trained on general corpora, there can be an unpredictable mismatch between the priors encoded in pretrained models and the characteristics of the specific RL environment. To address this issue, we propose Mutual-Information Regularized Policy Optimization, MIPO. MIPO enables implicit and dynamic reduction of the action space. Starting from the prior provided by the pretrained language model, our method dynamically adjusts the prior during the learning process based on the guidance of mutual information regularization. Theoretically, we demonstrate that this policy optimization process leads to the monotonic improvement on the mutual-information regularized RL objective. Empirically, we conduct experiments in various environments and demonstrate the effectiveness of MIPO.</abstract>
      <url hash="408a021a">2024.acl-long.89</url>
      <bibkey>wang-etal-2024-language-model</bibkey>
      <doi>10.18653/v1/2024.acl-long.89</doi>
    </paper>
    <paper id="90">
      <title>Evaluating Intention Detection Capability of Large Language Models in Persuasive Dialogues</title>
      <author><first>Hiromasa</first><last>Sakurai</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yusuke</first><last>Miyao</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>1635-1657</pages>
      <abstract>We investigate intention detection in persuasive multi-turn dialogs employing the largest available Large Language Models (LLMs).Much of the prior research measures the intention detection capability of machine learning models without considering the conversational history.To evaluate LLMs’ intention detection capability in conversation, we modified the existing datasets of persuasive conversation and created datasets using a multiple-choice paradigm.It is crucial to consider others’ perspectives through their utterances when engaging in a persuasive conversation, especially when making a request or reply that is inconvenient for others.This feature makes the persuasive dialogue suitable for the dataset of measuring intention detection capability.We incorporate the concept of ‘face acts,’ which categorize how utterances affect mental states.This approach enables us to measure intention detection capability by focusing on crucial intentions and to conduct comprehensible analysis according to intention types.</abstract>
      <url hash="d7750f15">2024.acl-long.90</url>
      <bibkey>sakurai-miyao-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.acl-long.90</doi>
    </paper>
    <paper id="91">
      <title><fixed-case>L</fixed-case>ong<fixed-case>LLML</fixed-case>ingua: Accelerating and Enhancing <fixed-case>LLM</fixed-case>s in Long Context Scenarios via Prompt Compression</title>
      <author><first>Huiqiang</first><last>Jiang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qianhui</first><last>Wu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xufang</first><last>Luo</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Dongsheng</first><last>Li</last></author>
      <author><first>Chin-Yew</first><last>Lin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yuqing</first><last>Yang</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Lili</first><last>Qiu</last><affiliation>Microsoft</affiliation></author>
      <pages>1658-1677</pages>
      <abstract>In long context scenarios, large language models (LLMs) face three main challenges: higher computational cost, performance reduction, and position bias. Research indicates that LLM performance hinges on the density and position of key information in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs’ perception of the key information to simultaneously address the three challenges. Our extensive evaluation across various long context scenarios demonstrates that LongLLMLingua not only enhances performance but also significantly reduces costs and latency. For instance, in the NaturalQuestions benchmark, LongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in GPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost reduction in the LooGLE benchmark. Moreover, when compressing prompts of about 10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency by 1.4x-2.6x.</abstract>
      <url hash="525c1ccf">2024.acl-long.91</url>
      <bibkey>jiang-etal-2024-longllmlingua</bibkey>
      <doi>10.18653/v1/2024.acl-long.91</doi>
    </paper>
    <paper id="92">
      <title>Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model</title>
      <author><first>Chuhao</first><last>Jin</last></author>
      <author><first>Kening</first><last>Ren</last><affiliation>Microsoft</affiliation></author>
      <author><first>Lingzhen</first><last>Kong</last></author>
      <author><first>Xiting</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ruihua</first><last>Song</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Huan</first><last>Chen</last></author>
      <pages>1678-1706</pages>
      <abstract>Persuasive dialogue requires multi-turn following and planning abilities to achieve the goal of persuading users, which is still challenging even for state-of-the-art large language models (LLMs). Previous works focus on retrieval-based models or generative models in a specific domain due to a lack of data across multiple domains. In this paper, we leverage GPT-4 to create the first multi-domain persuasive dialogue dataset DailyPersuasion. Then we propose a general method named PersuGPT to learn a persuasion model based on LLMs through intent-to-strategy reasoning, which summarizes the intent of user’s utterance and reasons next strategy to respond. Moreover, we design a simulation-based preference optimization, which utilizes a learned user model and our model to simulate next turns and estimate their rewards more accurately. Experimental results on two datasets indicate that our proposed method outperforms all baselines in terms of automatic evaluation metric Win-Rate and human evaluation. The code and data are available at https://persugpt.github.io.</abstract>
      <url hash="f4bc3d1c">2024.acl-long.92</url>
      <bibkey>jin-etal-2024-persuading</bibkey>
      <doi>10.18653/v1/2024.acl-long.92</doi>
    </paper>
    <paper id="93">
      <title><fixed-case>H</fixed-case>eal<fixed-case>M</fixed-case>e: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy</title>
      <author><first>Mengxi</first><last>Xiao</last></author>
      <author><first>Qianqian</first><last>Xie</last></author>
      <author><first>Ziyan</first><last>Kuang</last><affiliation>Jiangxi Normal University</affiliation></author>
      <author><first>Zhicheng</first><last>Liu</last></author>
      <author><first>Kailai</first><last>Yang</last></author>
      <author><first>Min</first><last>Peng</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Weiguang</first><last>Han</last></author>
      <author><first>Jimin</first><last>Huang</last><affiliation>The Fin AI</affiliation></author>
      <pages>1707-1725</pages>
      <abstract>Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients’ self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing empathetic, actionable suggestions. Moreover, we adopt the first comprehensive and expertly crafted psychological evaluation metrics, specifically designed to rigorously assess the performance of cognitive reframing, in both AI-simulated dialogues and real-world therapeutic conversations. Experimental results show that our model outperforms others in terms of empathy, guidance, and logical coherence, demonstrating its effectiveness and potential positive impact on psychotherapy.</abstract>
      <url hash="c17c7d92">2024.acl-long.93</url>
      <bibkey>xiao-etal-2024-healme</bibkey>
      <doi>10.18653/v1/2024.acl-long.93</doi>
    </paper>
    <paper id="94">
      <title>Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition</title>
      <author><first>Zirun</first><last>Guo</last></author>
      <author><first>Tao</first><last>Jin</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>1726-1736</pages>
      <abstract>The development of multimodal models has significantly advanced multimodal sentiment analysis and emotion recognition. However, in real-world applications, the presence of various missing modality cases often leads to a degradation in the model’s performance. In this work, we propose a novel multimodal Transformer framework using prompt learning to address the issue of missing modalities. Our method introduces three types of prompts: generative prompts, missing-signal prompts, and missing-type prompts. These prompts enable the generation of missing modality features and facilitate the learning of intra- and inter-modality information. Through prompt learning, we achieve a substantial reduction in the number of trainable parameters. Our proposed method outperforms other methods significantly across all evaluation metrics. Extensive experiments and ablation studies are conducted to demonstrate the effectiveness and robustness of our method, showcasing its ability to effectively handle missing modalities. Codes are available at https://github.com/zrguo/MPLMM.</abstract>
      <url hash="9a8ea1fe">2024.acl-long.94</url>
      <bibkey>guo-etal-2024-multimodal</bibkey>
      <doi>10.18653/v1/2024.acl-long.94</doi>
    </paper>
    <paper id="95">
      <title>An Effective Pronunciation Assessment Approach Leveraging Hierarchical Transformers and Pre-training Strategies</title>
      <author><first>Bi-Cheng</first><last>Yan</last></author>
      <author><first>Jiun-Ting</first><last>Li</last></author>
      <author><first>Yi-Cheng</first><last>Wang</last></author>
      <author><first>Hsin Wei</first><last>Wang</last></author>
      <author><first>Tien-Hong</first><last>Lo</last><affiliation>National Taiwan Normal University</affiliation></author>
      <author><first>Yung-Chang</first><last>Hsu</last></author>
      <author><first>Wei-Cheng</first><last>Chao</last></author>
      <author><first>Berlin</first><last>Chen</last><affiliation>National Taiwan Normal University</affiliation></author>
      <pages>1737-1747</pages>
      <abstract>Automatic pronunciation assessment (APA) manages to quantify a second language (L2) learner’s pronunciation proficiency in a target language by providing fine-grained feedback with multiple pronunciation aspect scores at various linguistic levels. Most existing efforts on APA typically parallelize the modeling process, namely predicting multiple aspect scores across various linguistic levels simultaneously. This inevitably makes both the hierarchy of linguistic units and the relatedness among the pronunciation aspects sidelined. Recognizing such a limitation, we in this paper first introduce HierTFR, a hierarchal APA method that jointly models the intrinsic structures of an utterance while considering the relatedness among the pronunciation aspects. We also propose a correlation-aware regularizer to strengthen the connection between the estimated scores and the human annotations. Furthermore, novel pre-training strategies tailored for different linguistic levels are put forward so as to facilitate better model initialization. An extensive set of empirical experiments conducted on the speechocean762 benchmark dataset suggest the feasibility and effectiveness of our approach in relation to several competitive baselines.</abstract>
      <url hash="9d90e78d">2024.acl-long.95</url>
      <bibkey>yan-etal-2024-effective</bibkey>
      <doi>10.18653/v1/2024.acl-long.95</doi>
    </paper>
    <paper id="96">
      <title>Detection-Correction Structure via General Language Model for Grammatical Error Correction</title>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Houfeng</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <pages>1748-1763</pages>
      <abstract>Grammatical error correction (GEC) is a task dedicated to rectifying texts with minimal edits, which can be decoupled into two components: detection and correction. However, previous works have predominantly focused on direct correction, with no prior efforts to integrate both into a single model. Moreover, the exploration of the detection-correction paradigm by large language models (LLMs) remains underdeveloped. This paper introduces an integrated detection-correction structure, named DeCoGLM, based on the General Language Model (GLM). The detection phase employs a fault-tolerant detection template, while the correction phase leverages autoregressive mask infilling for localized error correction. Through the strategic organization of input tokens and modification of attention masks, we facilitate multi-task learning within a single model. Our model demonstrates competitive performance against the state-of-the-art models on English and Chinese GEC datasets. Further experiments present the effectiveness of the detection-correction structure in LLMs, suggesting a promising direction for GEC.</abstract>
      <url hash="ea45a3f6">2024.acl-long.96</url>
      <bibkey>li-wang-2024-detection</bibkey>
      <doi>10.18653/v1/2024.acl-long.96</doi>
    </paper>
    <paper id="97">
      <title>Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer</title>
      <author><first>Yongxin</first><last>Zhu</last></author>
      <author><first>Dan</first><last>Su</last></author>
      <author><first>Liqiang</first><last>He</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Linli</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>1764-1775</pages>
      <abstract>While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce <b>G</b>enerative <b>P</b>re-trained <b>S</b>peech <b>T</b>ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. See <url>https://youngsheen.github.io/GPST/demo</url> for demo samples.</abstract>
      <url hash="5bed8498">2024.acl-long.97</url>
      <bibkey>zhu-etal-2024-generative</bibkey>
      <doi>10.18653/v1/2024.acl-long.97</doi>
    </paper>
    <paper id="98">
      <title>Selene: Pioneering Automated Proof in Software Verification</title>
      <author><first>Lichen</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Shuai</first><last>Lu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>1776-1789</pages>
      <abstract>Ensuring correctness is a pivotal aspect of software engineering. Among the various strategies available, software verification offers a definitive assurance of correctness. Nevertheless, writing verification proofs is resource-intensive and manpower-consuming, and there is a great need to automate this process. We introduce Selene in this paper, which is the first project-level automated proof benchmark constructed based on the real-world industrial-level operating system microkernel, seL4. Selene provides a comprehensive framework for end-to-end proof generation and a lightweight verification environment. Our experimental results with advanced large language models (LLMs), such as GPT-3.5-turbo and GPT-4, highlight the capabilities of LLMs in the domain of automated proof generation. Additionally, our further proposed augmentations indicate that the challenges presented by Selene can be mitigated in future research endeavors.</abstract>
      <url hash="c9f27584">2024.acl-long.98</url>
      <bibkey>zhang-etal-2024-selene</bibkey>
      <doi>10.18653/v1/2024.acl-long.98</doi>
    </paper>
    <paper id="99">
      <title>Dissecting Human and <fixed-case>LLM</fixed-case> Preferences</title>
      <author><first>Junlong</first><last>Li</last></author>
      <author><first>Fan</first><last>Zhou</last></author>
      <author><first>Shichao</first><last>Sun</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yikai</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>1790-1811</pages>
      <abstract>As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not significantly alter the preferences of pretrained-only LLMs. Finally, we show that preference-based evaluation can be intentionally manipulated. In both training-free and training-based settings, aligning a model with the preferences of judges boosts scores, while injecting the least preferred properties lowers them. This results in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94 on AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this strategic adaptation. We have made all resources of this project publicly available.</abstract>
      <url hash="eae93f74">2024.acl-long.99</url>
      <bibkey>li-etal-2024-dissecting</bibkey>
      <doi>10.18653/v1/2024.acl-long.99</doi>
    </paper>
    <paper id="100">
      <title><fixed-case>U</fixed-case>ni<fixed-case>C</fixed-case>oder: Scaling Code Large Language Model via Universal Code</title>
      <author><first>Tao</first><last>Sun</last></author>
      <author><first>Linzheng</first><last>Chai</last></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yuwei</first><last>Yin</last></author>
      <author><first>Hongcheng</first><last>Guo</last></author>
      <author><first>Jiaheng</first><last>Liu</last></author>
      <author><first>Bing</first><last>Wang</last></author>
      <author><first>Liqun</first><last>Yang</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>1812-1824</pages>
      <abstract>Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks.When applying LLMs for code generation, recent works mainly focus on directing the models to articulate intermediate natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and then output code with the natural language or other structured intermediate steps. However, such output is not suitable for code translation or generation tasks since the standard CoT has different logical structures and forms of expression with the code. In this work, we introduce the universal code (UniCode) as the intermediate representation. It is a description of algorithm steps using a mix of conventions of programming languages, such as assignment operator, conditional operator, and loop. Hence, we collect an instruction dataset UniCoder-Instruct to train our model UniCoder on multi-task learning objectives. UniCoder-Instruct comprises natural-language questions, code solutions, and the corresponding universal code. The alignment between the intermediate universal code representation and the final code solution significantly improves the quality of the generated code. The experimental results demonstrate that UniCoder with the universal code significantly outperforms the previous prompting methods by a large margin, showcasing the effectiveness of the structural clues in pseudo-code.</abstract>
      <url hash="82488da1">2024.acl-long.100</url>
      <bibkey>sun-etal-2024-unicoder</bibkey>
      <doi>10.18653/v1/2024.acl-long.100</doi>
    </paper>
    <paper id="101">
      <title><fixed-case>A</fixed-case>o<fixed-case>E</fixed-case>: Angle-optimized Embeddings for Semantic Textual Similarity</title>
      <author><first>Xianming</first><last>Li</last></author>
      <author><first>Jing</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <pages>1825-1839</pages>
      <abstract>Text embedding is pivotal in semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. STS learning largely relies on the cosine function as the optimization objective to reflect semantic similarity. However, the cosine has saturation zones rendering vanishing gradients and hindering learning subtle semantic differences in text embeddings. To address this issue, we propose a novel Angle-optimized Embedding model, AoE. It optimizes angle differences in complex space to explore similarity in saturation zones better. To set up a comprehensive evaluation, we experimented with existing short-text STS, our newly collected long-text STS, and downstream task datasets. Extensive experimental results on STS and MTEB benchmarks show that AoE significantly outperforms popular text embedding models neglecting cosine saturation zones. It highlights that AoE can produce high-quality text embeddings and broadly benefit downstream tasks.</abstract>
      <url hash="2b44cb42">2024.acl-long.101</url>
      <bibkey>li-li-2024-aoe</bibkey>
      <doi>10.18653/v1/2024.acl-long.101</doi>
    </paper>
    <paper id="102">
      <title><fixed-case>I</fixed-case>n<fixed-case>C</fixed-case>haracter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews</title>
      <author><first>Xintao</first><last>Wang</last></author>
      <author><first>Yunze</first><last>Xiao</last></author>
      <author><first>Jen-tse</first><last>Huang</last></author>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Rui</first><last>Xu</last></author>
      <author><first>Haoran</first><last>Guo</last><affiliation>研茵科技</affiliation></author>
      <author><first>Quan</first><last>Tu</last></author>
      <author><first>Yaying</first><last>Fei</last></author>
      <author><first>Ziang</first><last>Leng</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Jiangjie</first><last>Chen</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Cheng</first><last>Li</last></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>1840-1873</pages>
      <abstract>Role-playing agents (RPAs), powered by large language models, have emerged as a flourishing field of applications. However, a key challenge lies in assessing whether RPAs accurately reproduce the personas of target characters, namely their character fidelity. Existing methods mainly focus on the knowledge and linguistic patterns of characters. This paper, instead, introduces a novel perspective to evaluate the personality fidelity of RPAs with psychological scales. Overcoming drawbacks of previous self-report assessments on RPAs, we propose InCharacter, namely **In**terviewing **Character** agents for personality tests. Experiments include various types of RPAs and LLMs, covering 32 distinct characters on 14 widely used psychological scales. The results validate the effectiveness of InCharacter in measuring RPA personalities. Then, with InCharacter, we show that state-of-the-art RPAs exhibit personalities highly aligned with the human-perceived personalities of the characters, achieving an accuracy up to 80.7%.</abstract>
      <url hash="de44de02">2024.acl-long.102</url>
      <bibkey>wang-etal-2024-incharacter</bibkey>
      <doi>10.18653/v1/2024.acl-long.102</doi>
    </paper>
    <paper id="103">
      <title>Does <fixed-case>D</fixed-case>etect<fixed-case>GPT</fixed-case> Fully Utilize Perturbation? Bridging Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better</title>
      <author><first>Shengchao</first><last>Liu</last></author>
      <author><first>Xiaoming</first><last>Liu</last></author>
      <author><first>Yichen</first><last>Wang</last></author>
      <author><first>Zehua</first><last>Cheng</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Chengzhengxu</first><last>Li</last></author>
      <author><first>Zhaohan</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Lan</last></author>
      <author><first>Chao</first><last>Shen</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>1874-1889</pages>
      <abstract>The burgeoning generative capabilities of large language models (LLMs) have raised growing concerns about abuse, demanding automatic machine-generated text detectors. DetectGPT, a zero-shot metric-based detector, first introduces perturbation and shows great performance improvement. However, in DetectGPT, the random perturbation strategy could introduce noise, and logit regression depends on the threshold, harming the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel fine-tuned detector, PECOLA, bridging metric-based and fine-tuned methods by contrastive learning on selective perturbation. Selective strategy retains important tokens during perturbation and weights for multi-pair contrastive learning. The experiments show that PECOLA outperforms the state-of-the-art (SOTA) by 1.20% in accuracy on average on four public datasets. And we further analyze the effectiveness, robustness, and generalization of the method.</abstract>
      <url hash="10acca56">2024.acl-long.103</url>
      <bibkey>liu-etal-2024-detectgpt</bibkey>
      <doi>10.18653/v1/2024.acl-long.103</doi>
    </paper>
    <paper id="104">
      <title><fixed-case>AF</fixed-case>a<fixed-case>CTA</fixed-case>: Assisting the Annotation of Factual Claim Detection with Reliable <fixed-case>LLM</fixed-case> Annotators</title>
      <author><first>Jingwei</first><last>Ni</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Minjing</first><last>Shi</last></author>
      <author><first>Dominik</first><last>Stammbach</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Elliott</first><last>Ash</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Markus</first><last>Leippold</last><affiliation>University of Zurich</affiliation></author>
      <pages>1890-1912</pages>
      <abstract>With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.</abstract>
      <url hash="bc87fe66">2024.acl-long.104</url>
      <bibkey>ni-etal-2024-afacta</bibkey>
      <doi>10.18653/v1/2024.acl-long.104</doi>
    </paper>
    <paper id="105">
      <title>Towards Faithful and Robust <fixed-case>LLM</fixed-case> Specialists for Evidence-Based Question-Answering</title>
      <author><first>Tobias</first><last>Schimanski</last></author>
      <author><first>Jingwei</first><last>Ni</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Mathias</first><last>Kraus</last></author>
      <author><first>Elliott</first><last>Ash</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Markus</first><last>Leippold</last><affiliation>University of Zurich</affiliation></author>
      <pages>1913-1931</pages>
      <abstract>Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.</abstract>
      <url hash="5e249ef7">2024.acl-long.105</url>
      <bibkey>schimanski-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.acl-long.105</doi>
    </paper>
    <paper id="106">
      <title><fixed-case>L</fixed-case>o<fixed-case>RAM</fixed-case>o<fixed-case>E</fixed-case>: Alleviating World Knowledge Forgetting in Large Language Models via <fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>-Style Plugin</title>
      <author><first>Shihan</first><last>Dou</last></author>
      <author><first>Enyu</first><last>Zhou</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yan</first><last>Liu</last></author>
      <author><first>Songyang</first><last>Gao</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Wei</first><last>Shen</last><affiliation>Baichuan</affiliation></author>
      <author><first>Limao</first><last>Xiong</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yuhao</first><last>Zhou</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xiao</first><last>Wang</last></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Xiaoran</first><last>Fan</last></author>
      <author><first>Shiliang</first><last>Pu</last></author>
      <author><first>Jiang</first><last>Zhu</last><affiliation>Hikvision Research Institute</affiliation></author>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>1932-1945</pages>
      <abstract>Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Substantially increasing instruction data is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can significantly improve the ability to process downstream tasks, while maintaining the world knowledge stored in the LLM. Our code is available at https://github.com/Ablustrund/LoRAMoE.</abstract>
      <url hash="3907b0a9">2024.acl-long.106</url>
      <bibkey>dou-etal-2024-loramoe</bibkey>
      <doi>10.18653/v1/2024.acl-long.106</doi>
    </paper>
    <paper id="107">
      <title>Self-Alignment for Factuality: Mitigating Hallucinations in <fixed-case>LLM</fixed-case>s via Self-Evaluation</title>
      <author><first>Xiaoying</first><last>Zhang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Baolin</first><last>Peng</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Ye</first><last>Tian</last></author>
      <author><first>Jingyan</first><last>Zhou</last></author>
      <author><first>Lifeng</first><last>Jin</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Haitao</first><last>Mi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Helen</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>1946-1965</pages>
      <abstract>Despite showing impressive abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e., ”hallucinations”, even when they hold relevant knowledge. To mitigate these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM’s self-evaluation ability by improving the model’s confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN.</abstract>
      <url hash="1ca33576">2024.acl-long.107</url>
      <bibkey>zhang-etal-2024-self</bibkey>
      <doi>10.18653/v1/2024.acl-long.107</doi>
    </paper>
    <paper id="108">
      <title><fixed-case>M</fixed-case>-<fixed-case>RAG</fixed-case>: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions</title>
      <author><first>Zheng</first><last>Wang</last><affiliation>Huawei Singapore</affiliation></author>
      <author><first>Shu</first><last>Teo</last></author>
      <author><first>Jieer</first><last>Ouyang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yongjun</first><last>Xu</last></author>
      <author><first>Wei</first><last>Shi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>1966-1978</pages>
      <abstract>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant memories from an external database. However, existing RAG methods typically organize all memories in a whole database, potentially limiting focus on crucial memories and introducing noise. In this paper, we introduce a multiple partition paradigm for RAG (called M-RAG), where each database partition serves as a basic unit for RAG execution. Based on this paradigm, we propose a novel framework that leverages LLMs with Multi-Agent Reinforcement Learning to optimize different language generation tasks explicitly. Through comprehensive experiments conducted on seven datasets, spanning three language generation tasks and involving three distinct language model architectures, we confirm that M-RAG consistently outperforms various baseline methods, achieving improvements of 11%, 8%, and 12% for text summarization, machine translation, and dialogue generation, respectively.</abstract>
      <url hash="3e3714b1">2024.acl-long.108</url>
      <bibkey>wang-etal-2024-rag</bibkey>
      <doi>10.18653/v1/2024.acl-long.108</doi>
    </paper>
    <paper id="109">
      <title><fixed-case>AIR</fixed-case>-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension</title>
      <author><first>Qian</first><last>Yang</last></author>
      <author><first>Jin</first><last>Xu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wenrui</first><last>Liu</last></author>
      <author><first>Yunfei</first><last>Chu</last></author>
      <author><first>Ziyue</first><last>Jiang</last></author>
      <author><first>Xiaohuan</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yichong</first><last>Leng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yuanjun</first><last>Lv</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <author><first>Chang</first><last>Zhou</last></author>
      <author><first>Jingren</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <pages>1979-1998</pages>
      <abstract>Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as automatic speech recognition, and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement.In this paper, we introduce AIR-Bench (Audio InstRuction Benchmark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: foundation and chat benchmarks. The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research. Dataset and evaluation code are available at https://github.com/OFA-Sys/AIR-Bench.</abstract>
      <url hash="abdc88c9">2024.acl-long.109</url>
      <bibkey>yang-etal-2024-air</bibkey>
      <doi>10.18653/v1/2024.acl-long.109</doi>
    </paper>
    <paper id="110">
      <title>Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies</title>
      <author><first>Tom</first><last>Kocmi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Vilém</first><last>Zouhar</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Christian</first><last>Federmann</last><affiliation>Microsoft</affiliation></author>
      <author><first>Matt</first><last>Post</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <pages>1999-2014</pages>
      <abstract>Ten years ago a single metric, BLEU, governed progress in machine translation research. For better or worse, there is no such consensus today, and consequently it is difficult for researchers to develop and retain intuitions about metric deltas that drove earlier research and deployment decisions. This paper investigates the “dynamic range” of a number of modern metrics in an effort to provide a collective understanding of the meaning of differences in scores both within and among metrics; in other words, we ask “what point difference x in metric y is required between two systems for humans to notice?”. We conduct our evaluation on a new large dataset, ToShip23, using it to discover deltas at which metrics achieve system-level differences that are meaningful to humans, which we measure by pairwise system accuracy. We additionally show that this method of establishing delta-accuracy is more stable than the standard use of statistical p-values in regards to testset size. Where data size permits, we also explore the effect of metric deltas and accuracy across finer-grained features such as translation direction, domain, and system closeness.</abstract>
      <url hash="408ede6d">2024.acl-long.110</url>
      <bibkey>kocmi-etal-2024-navigating</bibkey>
      <doi>10.18653/v1/2024.acl-long.110</doi>
    </paper>
    <paper id="111">
      <title><fixed-case>V</fixed-case>alue<fixed-case>B</fixed-case>ench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models</title>
      <author><first>Yuanyi</first><last>Ren</last></author>
      <author><first>Haoran</first><last>Ye</last><affiliation>Peking University</affiliation></author>
      <author><first>Hanjun</first><last>Fang</last></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Guojie</first><last>Song</last><affiliation>Peking University</affiliation></author>
      <pages>2015-2040</pages>
      <abstract>Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies. This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications. This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and understanding in LLMs. ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions. We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space. With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks.</abstract>
      <url hash="cd51f6e9">2024.acl-long.111</url>
      <bibkey>ren-etal-2024-valuebench</bibkey>
      <doi>10.18653/v1/2024.acl-long.111</doi>
    </paper>
    <paper id="112">
      <title><fixed-case>DM</fixed-case>-<fixed-case>BLI</fixed-case>: Dynamic Multiple Subspaces Alignment for Unsupervised Bilingual Lexicon Induction</title>
      <author><first>Ling</first><last>Hu</last></author>
      <author><first>Yuemei</first><last>Xu</last><affiliation>Beijing Foreign Studies University</affiliation></author>
      <pages>2041-2052</pages>
      <abstract>Unsupervised bilingual lexicon induction (BLI) task aims to find word translations between languages and has achieved great success in similar language pairs. However, related works mostly rely on a single linear mapping for language alignment and fail on distant or low-resource language pairs, achieving less than half the performance observed in rich-resource language pairs. In this paper, we introduce DM-BLI, a Dynamic Multiple subspaces alignment framework for unsupervised BLI. DM-BLI improves language alignment by utilizing multiple subspace alignments instead of a single mapping. We begin via unsupervised clustering to discover these subspaces in source embedding space. Then we identify and align corresponding subspaces in the target space using a rough global alignment. DM-BLI further employs intra-cluster and inter-cluster contrastive learning to refine precise alignment for each subspace pair. Experiments conducted on standard BLI datasets for 12 language pairs (6 rich-resource and 6 low-resource) demonstrate substantial gains achieved by our framework. We release our code at https://github.com/huling-2/DM-BLI.git.</abstract>
      <url hash="fa9ed74d">2024.acl-long.112</url>
      <bibkey>hu-xu-2024-dm</bibkey>
      <doi>10.18653/v1/2024.acl-long.112</doi>
    </paper>
    <paper id="113">
      <title><fixed-case>S</fixed-case>parse<fixed-case>F</fixed-case>it: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations</title>
      <author><first>Jesus</first><last>Solano</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Mardhiyah</first><last>Sanni</last><affiliation>Intron Health</affiliation></author>
      <author><first>Oana-Maria</first><last>Camburu</last><affiliation>Department of Computer Science, University College London, University of London</affiliation></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>2053-2077</pages>
      <abstract>Models that generate natural language explanations (NLEs) for their predictions have recently gained increasing interest. However, this approach usually demands large datasets of human-written NLEs for the ground-truth answers at training time, which can be expensive and potentially infeasible for some applications. When only a few NLEs are available (a few-shot setup), fine-tuning pre-trained language models (PLMs) in conjunction with prompt-based learning has recently shown promising results. However, PLMs typically have billions of parameters, making full fine-tuning expensive. We propose SparseFit, a sparse few-shot fine-tuning strategy that leverages discrete prompts to jointly generate predictions and NLEs. We experiment with SparseFit on three sizes of the T5 language model and four datasets and compare it against existing state-of-the-art Parameter-Efficient Fine-Tuning (PEFT) techniques. We find that fine-tuning only 6.8% of the model parameters leads to competitive results for both the task performance and the quality of the generated NLEs compared to full fine-tuning of the model and produces better results on average than other PEFT methods in terms of predictive accuracy and NLE quality.</abstract>
      <url hash="6eceb83d">2024.acl-long.113</url>
      <bibkey>solano-etal-2024-sparsefit</bibkey>
      <doi>10.18653/v1/2024.acl-long.113</doi>
    </paper>
    <paper id="114">
      <title>Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation</title>
      <author><first>Wen</first><last>Wu</last></author>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Tsinghua University and University College London</affiliation></author>
      <author><first>Chung-Cheng</first><last>Chiu</last><affiliation>Google</affiliation></author>
      <author><first>Qiujia</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Junwen</first><last>Bai</last><affiliation>Google</affiliation></author>
      <author><first>Tara</first><last>Sainath</last><affiliation>Google</affiliation></author>
      <author><first>Phil</first><last>Woodland</last><affiliation>University of Cambridge</affiliation></author>
      <pages>2078-2093</pages>
      <abstract>The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of a single class label. The task is thus re-framed from classification to distribution estimation where every individual annotation is taken into account, not just the majority opinion. The evidential uncertainty measure is extended to quantify the uncertainty in emotion distribution estimation. Experimental results on the IEMOCAP and CREMA-D datasets demonstrate the superior capability of the proposed method in terms of majority class prediction, emotion distribution estimation, and uncertainty estimation.</abstract>
      <url hash="b8ebdc6f">2024.acl-long.114</url>
      <bibkey>wu-etal-2024-handling</bibkey>
      <doi>10.18653/v1/2024.acl-long.114</doi>
    </paper>
    <paper id="115">
      <title><fixed-case>REANO</fixed-case>: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation</title>
      <author><first>Jinyuan</first><last>Fang</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Zaiqiao</first><last>Meng</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Craig</first><last>MacDonald</last><affiliation>University of Glasgow</affiliation></author>
      <pages>2094-2112</pages>
      <abstract>Open domain question answering (ODQA) aims to answer questions with knowledge from an external corpus. Fusion-in-Decoder (FiD) is an effective retrieval-augmented reader model to address this task. Given that FiD independently encodes passages, which overlooks the semantic relationships between passages, some studies use knowledge graphs (KGs) to establish dependencies among passages. However, they only leverage knowledge triples from existing KGs, which suffer from incompleteness and may lack certain information critical for answering given questions. To this end, in order to capture the dependencies between passages while tacking the issue of incompleteness in existing KGs, we propose to enhance the retrieval-augmented reader model with a knowledge graph generation module (REANO). Specifically, REANO consists of a KG generator and an answer predictor. The KG generator aims to generate KGs from the passages and the answer predictor then generates answers based on the passages and the generated KGs. Experimental results on five ODQA datasets indicate that compared with baselines, REANO can improve the exact match score by up to 2.7% on the EntityQuestion dataset, with an average improvement of 1.8% across all the datasets.</abstract>
      <url hash="f14aab2a">2024.acl-long.115</url>
      <bibkey>fang-etal-2024-reano</bibkey>
      <doi>10.18653/v1/2024.acl-long.115</doi>
    </paper>
    <paper id="116">
      <title>Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks</title>
      <author><first>Yingji</first><last>Zhang</last></author>
      <author><first>Danilo</first><last>Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>2113-2134</pages>
      <abstract>Disentangled latent spaces usually have better semantic separability and geometrical properties, which leads to better interpretability and more controllable data generation. While this has been well investigated in Computer Vision, in tasks such as image disentanglement, in the NLP domain, sentence disentanglement is still comparatively under-investigated. Most previous work have concentrated on disentangling task-specific generative factors, such as sentiment, within the context of style transfer. In this work, we focus on a more general form of sentence disentanglement, targeting the localised modification and control of more general sentence semantic features. To achieve this, we contribute to a novel notion of sentence semantic disentanglement and introduce a flow-based invertible neural network (INN) mechanism integrated with a transformer-based language Autoencoder (AE) in order to deliver latent spaces with better separability properties. Experimental results demonstrate that the model can conform the distributed latent space into a better semantically disentangled sentence space, leading to improved language interpretability and controlled generation when compared to the recent state-of-the-art language VAE models.</abstract>
      <url hash="f58aea84">2024.acl-long.116</url>
      <bibkey>zhang-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.acl-long.116</doi>
    </paper>
    <paper id="117">
      <title><fixed-case>M</fixed-case>o<fixed-case>PS</fixed-case>: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation</title>
      <author><first>Yan</first><last>Ma</last></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>2135-2169</pages>
      <abstract>A story premise succinctly defines a story’s main idea, foundation, and trajectory. It serves as the initial trigger in automatic story generation. Existing sources of story premises are limited by a lack of diversity, uneven quality, and high costs that make them difficult to scale. In response, we introduce Modular Story Premise Synthesis (MoPS) which breaks down story premises into modules like background and persona for automated design and generation. MoPS consists of three phases: (1) Pre-collect a consistent set of candidates for each module to form a nested dictionary. (2) Extract a key path from the nested dictionary as the premise design. (3) Instruct an LLM to integrate the design into a coherent premise sentence. Thorough evaluations demonstrate that our synthesized premises excel in diversity, fascination, completeness, and originality compared to those induced from large language models and captured from public story datasets. Similarly, the extended novels and scripts generated from our premises also exhibit higher quality. In supplementary materials, we provide the MoPS code suite, along with 7.5k generated premises and 1k extended stories.</abstract>
      <url hash="181c96d5">2024.acl-long.117</url>
      <bibkey>ma-etal-2024-mops</bibkey>
      <doi>10.18653/v1/2024.acl-long.117</doi>
    </paper>
    <paper id="118">
      <title>Open-Set Semi-Supervised Text Classification via Adversarial Disagreement Maximization</title>
      <author><first>Junfan</first><last>Chen</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Junchi</first><last>Chen</last></author>
      <author><first>Chunming</first><last>Hu</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <pages>2170-2180</pages>
      <abstract>Open-Set Semi-Supervised Text Classification (OSTC) aims to train a classification model on a limited set of labeled texts, alongside plenty of unlabeled texts that include both in-distribution and out-of-distribution examples. In this paper, we revisit the main challenge in OSTC, i.e., outlier detection, from a measurement disagreement perspective and innovatively propose to improve OSTC performance by directly maximizing the measurement disagreements. Based on the properties of in-measurement and cross-measurements, we design an Adversarial Disagreement Maximization (ADM) model that synergeticly optimizes the measurement disagreements. In addition, we develop an abnormal example detection and measurement calibration approach to guarantee the effectiveness of ADM training. Experiment results and comprehensive analysis of three benchmarks demonstrate the effectiveness of our model.</abstract>
      <url hash="4bc77825">2024.acl-long.118</url>
      <bibkey>chen-etal-2024-open</bibkey>
      <doi>10.18653/v1/2024.acl-long.118</doi>
    </paper>
    <paper id="119">
      <title><fixed-case>T</fixed-case>ool<fixed-case>S</fixed-case>word: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages</title>
      <author><first>Junjie</first><last>Ye</last></author>
      <author><first>Sixian</first><last>Li</last></author>
      <author><first>Guanyu</first><last>Li</last></author>
      <author><first>Caishuang</first><last>Huang</last></author>
      <author><first>Songyang</first><last>Gao</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yilong</first><last>Wu</last></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>2181-2211</pages>
      <abstract>Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present <tex-math>ToolSword</tex-math>, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing <tex-math>malicious</tex-math> <tex-math>queries</tex-math> and <tex-math>jailbreak</tex-math> <tex-math>attacks</tex-math> in the input stage, <tex-math>noisy</tex-math> <tex-math>misdirection</tex-math> and <tex-math>risky</tex-math> <tex-math>cues</tex-math> in the execution stage, and <tex-math>harmful</tex-math> <tex-math>feedback</tex-math> and <tex-math>error</tex-math> <tex-math>conflicts</tex-math> in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data will be released upon acceptance of the paper.</abstract>
      <url hash="3c7a930a">2024.acl-long.119</url>
      <bibkey>ye-etal-2024-toolsword</bibkey>
      <doi>10.18653/v1/2024.acl-long.119</doi>
    </paper>
    <paper id="120">
      <title>A synthetic data approach for domain generalization of <fixed-case>NLI</fixed-case> models</title>
      <author><first>Mohammad Javad</first><last>Hosseini</last><affiliation>Google</affiliation></author>
      <author><first>Andrey</first><last>Petrov</last></author>
      <author><first>Alex</first><last>Fabrikant</last><affiliation>Google Research</affiliation></author>
      <author><first>Annie</first><last>Louis</last><affiliation>Google Deepmind</affiliation></author>
      <pages>2212-2226</pages>
      <abstract>Natural Language Inference (NLI) remains an important benchmark task for LLMs. NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text. There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We explore the opportunity for synthetic high-quality datasets to adapt NLI models for zero-shot use in downstream applications across new and unseen text domains. We demonstrate a new approach for generating NLI data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data (685K synthetic examples) have the best generalization to completely new downstream test settings. On the TRUE benchmark, a T5-small model trained with our data improves around 7% on average compared to training on the best alternative dataset. The improvements are more pronounced for smaller models, while still meaningful on a T5 XXL model. We also demonstrate gains on test sets when in-domain training data is augmented with our domain-general synthetic data.</abstract>
      <url hash="950994b9">2024.acl-long.120</url>
      <bibkey>hosseini-etal-2024-synthetic</bibkey>
      <doi>10.18653/v1/2024.acl-long.120</doi>
    </paper>
    <paper id="121">
      <title>Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild</title>
      <author><first>Ting</first><last>Wu</last></author>
      <author><first>Jingyi</first><last>Liu</last></author>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>2227-2239</pages>
      <abstract>The principle of continual relation extraction (CRE) involves adapting to emerging novel relations while preserving old knowledge. Existing CRE approaches excel in preserving old knowledge but falter when confronted with contaminated data streams, likely due to an artificial assumption of no annotation errors. Recognizing the prevalence of noisy labels in real-world datasets, we introduce a more practical learning scenario, termed as <i>noisy-CRE</i>. In response to this challenge, we propose a noise-resistant contrastive framework called Noise-guided Attack in Contrastive Learning (NaCL), aimed at learning incremental corrupted relations. Diverging from conventional approaches like sample discarding or relabeling in the presence of noisy labels, NaCL takes a transformative route by modifying the feature space through targeted attack. This attack aims to align the feature space with the provided, albeit inaccurate, labels, thereby enhancing contrastive representations. Extensive empirical validations demonstrate the consistent performance improvement of NaCL with increasing noise rates, surpassing state-of-the-art methods.</abstract>
      <url hash="ad090a9c">2024.acl-long.121</url>
      <bibkey>wu-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.acl-long.121</doi>
    </paper>
    <paper id="122">
      <title><fixed-case>LRQ</fixed-case>uant: Learnable and Robust Post-Training Quantization for Large Language Models</title>
      <author><first>Jiaqi</first><last>Zhao</last></author>
      <author><first>Miao</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Chao</first><last>Zeng</last></author>
      <author><first>Ming</first><last>Wang</last></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technolgy, Shenzhen</affiliation></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <pages>2240-2255</pages>
      <abstract>Post-training quantization (PTQ) for large language models (LLMs) significantly accelerates model inference and relieves memory constraints, without incurring model training. A “smoothing paradigm” is commonly used in LLM quantization, which transfers the quantization difficulty of activation to weight quantization using mathematically equivalent transformations. However, existing methods face two issues: 1) Most smoothing parameters are hand-crafted defined which leads to suboptimal results; 2) There are significant performance degradations when tested on unseen datasets. To address these challenges, this paper introduces a robust learnable smooth-based PTQ framework, called LRQuant. Firstly, we consider a learnable paradigm to find optimal smoothing parameters which are initialized by logarithmic activation equivalent. In addition, we empirically found that only relying on MSE loss could hardly lead to optimal quantization results, and we then propose a novel loss function based on the negative logarithm of cosine similarity (NLC loss) between outputs of full-precision and quantized block. At last, we pioneeringly introduce Test-time adaptation (TTA) into LLM quantization, which allows for rapid model adaptation during testing to improve generalization performance. More surprisingly, we find that by using our TTA method, we can achieve better results on test sets than directly using test sets for calibration in some cases while avoiding catastrophic forgetting. Codes are available at https://github.com/zjq0455/RLQ.</abstract>
      <url hash="2f14719e">2024.acl-long.122</url>
      <bibkey>zhao-etal-2024-lrquant</bibkey>
      <doi>10.18653/v1/2024.acl-long.122</doi>
    </paper>
    <paper id="123">
      <title><fixed-case>V</fixed-case>ari<fixed-case>E</fixed-case>rr <fixed-case>NLI</fixed-case>: Separating Annotation Error from Human Label Variation</title>
      <author><first>Leon</first><last>Weber-Genzel</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Siyao</first><last>Peng</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Marie-Catherine</first><last>De Marneffe</last><affiliation>UCLouvain</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München and IT University of Copenhagen</affiliation></author>
      <pages>2256-2269</pages>
      <abstract>Human label variation arises when annotators assign different labels to the same item for valid reasons, while annotation errors occur when labels are assigned for invalid reasons. These two issues are prevalent in NLP benchmarks, yet existing research has studied them in isolation. To the best of our knowledge, there exists no prior work that focuses on teasing apart error from signal, especially in cases where signal is beyond black-and-white.To fill this gap, we introduce a systematic methodology and a new dataset, VariErr (variation versus error), focusing on the NLI task in English. We propose a 2-round annotation procedure with annotators explaining each label and subsequently judging the validity of label-explanation pairs.VariErr contains 7,732 validity judgments on 1,933 explanations for 500 re-annotated MNLI items. We assess the effectiveness of various automatic error detection (AED) methods and GPTs in uncovering errors versus human label variation. We find that state-of-the-art AED methods significantly underperform GPTs and humans. While GPT-4 is the best system, it still falls short of human performance. Our methodology is applicable beyond NLI, offering fertile ground for future research on error versus plausible variation, which in turn can yield better and more trustworthy NLP systems.</abstract>
      <url hash="45ef1937">2024.acl-long.123</url>
      <bibkey>weber-genzel-etal-2024-varierr</bibkey>
      <doi>10.18653/v1/2024.acl-long.123</doi>
    </paper>
    <paper id="124">
      <title>Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation</title>
      <author><first>Xunjian</first><last>Yin</last></author>
      <author><first>Xu</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Jie</first><last>Ruan</last></author>
      <author><first>Xiaojun</first><last>Wan</last><affiliation>Peking University</affiliation></author>
      <pages>2270-2286</pages>
      <abstract>In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks.To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs.We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt.Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models.Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust.To explore the knowledge boundary for a given model, we propose projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge.Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods.Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.</abstract>
      <url hash="581fe5a9">2024.acl-long.124</url>
      <bibkey>yin-etal-2024-benchmarking</bibkey>
      <doi>10.18653/v1/2024.acl-long.124</doi>
    </paper>
    <paper id="125">
      <title><fixed-case>L</fixed-case>ist<fixed-case>T</fixed-case>5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval</title>
      <author><first>Soyoung</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Eunbi</first><last>Choi</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Jiyeon</first><last>Kim</last></author>
      <author><first>Hyeongu</first><last>Yun</last></author>
      <author><first>Yireun</first><last>Kim</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>2287-2308</pages>
      <abstract>We propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework will be fully open-sourced.</abstract>
      <url hash="c010579e">2024.acl-long.125</url>
      <bibkey>yoon-etal-2024-listt5</bibkey>
      <doi>10.18653/v1/2024.acl-long.125</doi>
    </paper>
    <paper id="126">
      <title>Exploring the Potential of Large Language Models in Computational Argumentation</title>
      <author><first>Guizhen</first><last>Chen</last></author>
      <author><first>Liying</first><last>Cheng</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Alibaba Group</affiliation></author>
      <pages>2309-2330</pages>
      <abstract>Computational argumentation has become an essential tool in various domains, including law, public policy, and artificial intelligence. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated impressive capabilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on diverse computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both zero-shot and few-shot settings. We organize existing tasks into six main categories and standardize the format of fourteen openly available datasets. In addition, we present a new benchmark dataset on counter speech generation that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation. Our analysis offers valuable suggestions for evaluating computational argumentation and its integration with LLMs in future research endeavors.</abstract>
      <url hash="aa38e7f7">2024.acl-long.126</url>
      <bibkey>chen-etal-2024-exploring-potential</bibkey>
      <doi>10.18653/v1/2024.acl-long.126</doi>
    </paper>
    <paper id="127">
      <title><fixed-case>T</fixed-case>axo<fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>: <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et-based Model for Solving Multiple Lexical Semantic Tasks</title>
      <author><first>Viktor</first><last>Moskvoretskii</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Ekaterina</first><last>Neminova</last></author>
      <author><first>Alina</first><last>Lobanova</last></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <author><first>Irina</first><last>Nikishina</last></author>
      <pages>2331-2350</pages>
      <abstract>In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the “all-in-one” model for taxonomy-related tasks, lightweight due to 4-bit quantization and LoRA. TaxoLLaMA achieves 11 SOTA results, and 4 top-2 results out of 16 tasks on the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates a very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and pre-trained models are available online (code: https://github.com/VityaVitalich/TaxoLLaMA)</abstract>
      <url hash="5a2af627">2024.acl-long.127</url>
      <bibkey>moskvoretskii-etal-2024-taxollama</bibkey>
      <doi>10.18653/v1/2024.acl-long.127</doi>
    </paper>
    <paper id="128">
      <title><fixed-case>CANDLE</fixed-case>: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning</title>
      <author><first>Weiqi</first><last>Wang</last><affiliation>Amazon.com, Johns Hopkins University and The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Chunyang</first><last>Li</last></author>
      <author><first>Haochen</first><last>Shi</last></author>
      <author><first>Wenxuan</first><last>Ding</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Baixuan</first><last>Xu</last></author>
      <author><first>Zhaowei</first><last>Wang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jiaxin</first><last>Bai</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Cheng</first><last>Jiayang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chunkit</first><last>Chan</last></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>2351-2374</pages>
      <abstract>The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows the application of existing knowledge to unfamiliar scenarios. However, existing works tend to undervalue the step of instantiation and heavilyrely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability. To tackle these challenges, we introduce CANDLE (ConceptuAlizationand INstantiation Distillation from Large Language ModEls), a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering. By applying CANDLE to ATOMIC (Sap et al., 2019a), we construct a comprehensive knowledge base comprising six million conceptualizations and instantiated commonsense knowledge triples. Both types of knowledge are firmly rooted in the original ATOMIC dataset, and intrinsic evaluations demonstrate their exceptional quality and diversity. Empirical results indicate that distilling CANDLE on student models provides benefits across three downstream tasks. Our data and models are publicly available at https://github.com/HKUST-KnowComp/CANDLE.</abstract>
      <url hash="2aa51782">2024.acl-long.128</url>
      <bibkey>wang-etal-2024-candle</bibkey>
      <doi>10.18653/v1/2024.acl-long.128</doi>
    </paper>
    <paper id="129">
      <title><fixed-case>MEFT</fixed-case>: Memory-Efficient Fine-Tuning through Sparse Adapter</title>
      <author><first>Jitai</first><last>Hao</last></author>
      <author><first>Weiwei</first><last>Sun</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Xin</first><last>Xin</last></author>
      <author><first>Qi</first><last>Meng</last><affiliation>Microsoft and Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhumin</first><last>Chen</last><affiliation>Shandong University</affiliation></author>
      <author><first>Pengjie</first><last>Ren</last><affiliation>Shandong University</affiliation></author>
      <author><first>Zhaochun</first><last>Ren</last><affiliation>Leiden University</affiliation></author>
      <pages>2375-2388</pages>
      <abstract>Parameter-Efficient Fine-tuning (PEFT) facilitates the fine-tuning of Large Language Models (LLMs) under limited resources. However, the fine-tuning performance with PEFT on complex, knowledge-intensive tasks is limited due to the constrained model capacity, which originates from the limited number of additional trainable parameters. To overcome this limitation, we introduce a novel mechanism that fine-tunes LLMs with adapters of larger size yet memory-efficient. This is achieved by leveraging the inherent activation sparsity in the Feed-Forward Networks (FFNs) of LLMs and utilizing the larger capacity of Central Processing Unit (CPU) memory compared to Graphics Processing Unit (GPU). We store and update the parameters of larger adapters on the CPU. Moreover, we employ a Mixture of Experts (MoE)-like architecture to mitigate unnecessary CPU computations and reduce the communication volume between the GPU and CPU. This is particularly beneficial over the limited bandwidth of PCI Express (PCIe). Our method can achieve fine-tuning results comparable to those obtained with larger memory capacities, even when operating under more limited resources such as a 24GB memory single GPU setup, with acceptable loss in training efficiency. Our codes are available at https://github.com/CURRENTF/MEFT.</abstract>
      <url hash="882306d4">2024.acl-long.129</url>
      <bibkey>hao-etal-2024-meft</bibkey>
      <doi>10.18653/v1/2024.acl-long.129</doi>
    </paper>
    <paper id="130">
      <title>Surgical Feature-Space Decomposition of <fixed-case>LLM</fixed-case>s: Why, When and How?</title>
      <author><first>Arnav</first><last>Chavan</last></author>
      <author><first>Nahush</first><last>Lele</last></author>
      <author><first>Deepak</first><last>Gupta</last><affiliation>University of Tromsø and Transmute AI Research</affiliation></author>
      <pages>2389-2400</pages>
      <abstract>Low-rank approximations, of the weight and feature space can enhance the performance of deep learning models, whether in terms of improving generalization or reducing the latency of inference. However, there is no clear consensus yet on how, when and why these approximations are helpful for large language models (LLMs). In this work, we empirically study the efficacy of weight and feature space decomposition in transformer-based LLMs. We demonstrate that surgical decomposition not only provides critical insights into the trade-off between compression and language modelling performance, but also sometimes enhances commonsense reasoning performance of LLMs. Our empirical analysis identifies specific network segments that intrinsically exhibit a low-rank structure. Furthermore, we extend our investigation to the implications of low-rank approximations on model bias. Overall, our findings offer a novel perspective on optimizing LLMs, presenting the low-rank approximation not only as a tool for performance enhancements, but also as a means to potentially rectify biases within these models.</abstract>
      <url hash="314014b4">2024.acl-long.130</url>
      <bibkey>chavan-etal-2024-surgical</bibkey>
      <doi>10.18653/v1/2024.acl-long.130</doi>
    </paper>
    <paper id="131">
      <title>Reasoning in Flux: Enhancing Large Language Models Reasoning through Uncertainty-aware Adaptive Guidance</title>
      <author><first>Zhangyue</first><last>Yin</last></author>
      <author><first>Qiushi</first><last>Sun</last><affiliation>University of Hong Kong and Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Zhiyuan</first><last>Zeng</last></author>
      <author><first>Xiaonan</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Junqi</first><last>Dai</last></author>
      <author><first>Qinyuan</first><last>Cheng</last></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>2401-2416</pages>
      <abstract>Machine reasoning, which involves solving complex problems through step-by-step deduction and analysis, is a crucial indicator of the capabilities of Large Language Models (LLMs). However, as the complexity of tasks escalates, LLMs often encounter increasing errors in their multi-step reasoning process. This study delves into the underlying factors contributing to these reasoning errors and seeks to leverage uncertainty to refine them. Specifically, we introduce Uncertainty-aware Adaptive Guidance (UAG), a novel approach for guiding LLM reasoning onto an accurate and reliable trajectory. UAG first identifies and evaluates uncertainty signals within each step of the reasoning chain. Upon detecting a significant increase in uncertainty, UAG intervenes by retracting to a previously reliable state and then introduces certified reasoning clues for refinement. By dynamically adjusting the reasoning process, UAG offers a plug-and-play solution for improving LLMs’ performance in complex reasoning. Extensive experiments across various reasoning tasks demonstrate that UAG not only enhances the reasoning abilities of LLMs but also consistently outperforms several strong baselines with minimal computational overhead. Further analysis reveals that UAG is notably effective in identifying and diminishing reasoning errors.</abstract>
      <url hash="f498f668">2024.acl-long.131</url>
      <bibkey>yin-etal-2024-reasoning</bibkey>
      <doi>10.18653/v1/2024.acl-long.131</doi>
    </paper>
    <paper id="132">
      <title>Modality-Aware Integration with Large Language Models for Knowledge-Based Visual Question Answering</title>
      <author><first>Junnan</first><last>Dong</last></author>
      <author><first>Qinggang</first><last>Zhang</last></author>
      <author><first>Huachi</first><last>Zhou</last></author>
      <author><first>Daochen</first><last>Zha</last><affiliation>Airbnb</affiliation></author>
      <author><first>Pai</first><last>Zheng</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xiao</first><last>Huang</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <pages>2417-2429</pages>
      <abstract>Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (<tex-math>\texttt{MAIL}</tex-math>). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, <tex-math>(i)</tex-math> we propose a two-stage prompting strategy with LLMs to densely embody the image into a *scene graph* with detailed visual features; <tex-math>(ii)</tex-math> We construct a coupled *concept graph* by linking the mentioned entities with external facts. <tex-math>(iii)</tex-math> A tailored pseudo-siamese graph medium fusion is designed for sufficient multimodal fusion. We utilize the shared mentioned entities in two graphs as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums. Extensive experiments show the superiority of <tex-math>\texttt{MAIL}</tex-math>.</abstract>
      <url hash="b5fc7f4e">2024.acl-long.132</url>
      <bibkey>dong-etal-2024-modality</bibkey>
      <doi>10.18653/v1/2024.acl-long.132</doi>
    </paper>
    <paper id="133">
      <title>Unlocking Data-free Low-bit Quantization with Matrix Decomposition for <fixed-case>KV</fixed-case> Cache Compression</title>
      <author><first>Peiyu</first><last>Liu</last></author>
      <author><first>Ze-Feng</first><last>Gao</last></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yipeng</first><last>Ma</last></author>
      <author><first>Tao</first><last>Wang</last></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>2430-2440</pages>
      <abstract>Key-value (KV) caching is an important technique to accelerate the inference of large language models (LLMs), but incurs significant memory overhead. To compress the size of KV cache, existing methods often compromise precision or require extra data for calibration, limiting their practicality in LLM deployment. In this paper, we introduce <b>DecoQuant</b>, a novel data-free low-bit quantization technique based on tensor decomposition methods, to effectively compress KV cache. Our core idea is to adjust the outlier distribution of the original matrix by performing tensor decomposition, so that the quantization difficulties are migrated from the matrix to decomposed local tensors. Specially, we find that outliers mainly concentrate on small local tensors, while large tensors tend to have a narrower value range. Based on this finding, we propose to apply low-bit quantization to the large tensor, while maintaining high-precision representation for the small tensor. Furthermore, we utilize the proposed quantization method to compress the KV cache of LLMs to accelerate the inference, and develop an efficient dequantization kernel tailored specifically for DecoQuant. Through extensive experiments, DecoQuant demonstrates remarkable efficiency gains, showcasing up to a 75% reduction in memory footprint while maintaining comparable generation quality.</abstract>
      <url hash="0450a8fc">2024.acl-long.133</url>
      <bibkey>liu-etal-2024-unlocking-data</bibkey>
      <doi>10.18653/v1/2024.acl-long.133</doi>
    </paper>
    <paper id="134">
      <title><fixed-case>V</fixed-case>erifi<fixed-case>NER</fixed-case>: Verification-augmented <fixed-case>NER</fixed-case> via Knowledge-grounded Reasoning with Large Language Models</title>
      <author><first>Seoyeon</first><last>Kim</last></author>
      <author><first>Kwangwook</first><last>Seo</last></author>
      <author><first>Hyungjoo</first><last>Chae</last></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <pages>2441-2461</pages>
      <abstract>Recent approaches in domain-specific named entity recognition (NER), such as biomedical NER, have shown remarkable advances. However, they still lack of faithfulness, producing erroneous predictions. We assume that knowledge of entities can be useful in verifying the correctness of the predictions. Despite the usefulness of knowledge, resolving such errors with knowledge is nontrivial, since the knowledge itself does not directly indicate the ground-truth label. To this end, we propose VerifiNER, a post-hoc verification framework that identifies errors from existing NER methods using knowledge and revises them into more faithful predictions. Our framework leverages the reasoning abilities of large language models to adequately ground on knowledge and the contextual information in the verification process. We validate effectiveness of VerifiNER through extensive experiments on biomedical datasets. The results suggest that VerifiNER can successfully verify errors from existing models as a model-agnostic approach. Further analyses on out-of-domain and low-resource settings show the usefulness of VerifiNER on real-world applications.</abstract>
      <url hash="58a8c36b">2024.acl-long.134</url>
      <bibkey>kim-etal-2024-verifiner</bibkey>
      <doi>10.18653/v1/2024.acl-long.134</doi>
    </paper>
    <paper id="135">
      <title>Making Long-Context Language Models Better Multi-Hop Reasoners</title>
      <author><first>Yanyang</first><last>Li</last></author>
      <author><first>Shuo</first><last>Liang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Michael</first><last>Lyu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Liwei</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>2462-2475</pages>
      <abstract>Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased performance in the presence of noisy contexts. In this paper, we introduce Reasoning with Attributions, a novel approach that prompts LMs to supply attributions for each assertion during their reasoning. We validate our approach through experiments on three multi-hop datasets, employing both proprietary and open-source models, and demonstrate its efficacy and resilience. Furthermore, we explore methods to augment reasoning capabilities via fine-tuning and offer an attribution-annotated dataset and a specialized training strategy. Our fine-tuned model achieves competitive performance on multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as ChatGPT and Claude-instant.</abstract>
      <url hash="a541d316">2024.acl-long.135</url>
      <bibkey>li-etal-2024-making</bibkey>
      <doi>10.18653/v1/2024.acl-long.135</doi>
    </paper>
    <paper id="136">
      <title><fixed-case>T</fixed-case>ransli<fixed-case>C</fixed-case>o: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models</title>
      <author><first>Yihong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Chunlan</first><last>Ma</last></author>
      <author><first>Haotian</first><last>Ye</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>2476-2499</pages>
      <abstract>The world’s more than 7000 languages are written in at least 293 scripts. Due to various reasons, many closely related languages use different scripts, which poses a difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a consequence, mPLMs are faced with a script barrier: representations from different scripts are located in different subspaces, which can result in crosslingual transfer involving languages of different scripts performing suboptimally. To address this problem, we propose TransliCo, a framework that optimizes the Transliteration Contrastive Modeling (TCM) objective to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (in our case Latin), which enhances uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we fine-tune it on a small portion (5%) of its training data, and refer to the resulting model as Furina. We show that Furina not only better aligns representations from distinct scripts but also outperforms the original Glot500-m on various zero-shot crosslingual transfer tasks. Additionally, we achieve consistent improvement in a case study on the Indic group where the languages exhibit areal features but use different scripts. We make our code and models publicly available.</abstract>
      <url hash="9a5af75a">2024.acl-long.136</url>
      <bibkey>liu-etal-2024-translico</bibkey>
      <doi>10.18653/v1/2024.acl-long.136</doi>
    </paper>
    <paper id="137">
      <title>Extreme Miscalibration and the Illusion of Adversarial Robustness</title>
      <author><first>Vyas</first><last>Raina</last></author>
      <author><first>Samson</first><last>Tan</last><affiliation>Amazon</affiliation></author>
      <author><first>Volkan</first><last>Cevher</last><affiliation>EPFL - EPF Lausanne and Amazon Development Center Germany</affiliation></author>
      <author><first>Aditya</first><last>Rawal</last><affiliation>Amazon</affiliation></author>
      <author><first>Sheng</first><last>Zha</last><affiliation>Amazon</affiliation></author>
      <author><first>George</first><last>Karypis</last><affiliation>University of Minnesota, Minneapolis</affiliation></author>
      <pages>2500-2525</pages>
      <abstract>Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify. Adversarial Training (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples. Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can be scaled during training to improve genuine robustness.</abstract>
      <url hash="19a1e64f">2024.acl-long.137</url>
      <bibkey>raina-etal-2024-extreme</bibkey>
      <doi>10.18653/v1/2024.acl-long.137</doi>
    </paper>
    <paper id="138">
      <title><fixed-case>H</fixed-case>y<fixed-case>C</fixed-case>o<fixed-case>R</fixed-case>ec: Hypergraph-Enhanced Multi-Preference Learning for Alleviating Matthew Effect in Conversational Recommendation</title>
      <author><first>Yongsen</first><last>Zheng</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Ruilin</first><last>Xu</last></author>
      <author><first>Ziliang</first><last>Chen</last></author>
      <author><first>Guohua</first><last>Wang</last><affiliation>South China Agricultural University</affiliation></author>
      <author><first>Mingjie</first><last>Qian</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Jinghui</first><last>Qin</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Liang</first><last>Lin</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>2526-2537</pages>
      <abstract>The Matthew effect is a notorious issue in Recommender Systems (RSs), i.e., the rich get richer and the poor get poorer, wherein popular items are overexposed while less popular ones are regularly ignored. Most methods examine Matthew effect in static or nearly-static recommendation scenarios. However, the Matthew effect will be increasingly amplified when the user interacts with the system over time. To address these issues, we propose a novel paradigm, Hypergraph-Enhanced Multi-Preference Learning for Alleviating Matthew Effect in Conversational Recommendation (HyCoRec), which aims to alleviate the Matthew effect in conversational recommendation. Concretely, HyCoRec devotes to alleviate the Matthew effect by learning multi-aspect preferences, i.e., item-, entity-, word-, review-, and knowledge-aspect preferences, to effectively generate responses in the conversational task and accurately predict items in the recommendation task when the user chats with the system over time. Extensive experiments conducted on two benchmarks validate that HyCoRec achieves new state-of-the-art performance and the superior of alleviating Matthew effect.</abstract>
      <url hash="805352d7">2024.acl-long.138</url>
      <bibkey>zheng-etal-2024-hycorec</bibkey>
      <doi>10.18653/v1/2024.acl-long.138</doi>
    </paper>
    <paper id="139">
      <title>Co-training for Low Resource Scientific Natural Language Inference</title>
      <author><first>Mobashir</first><last>Sadat</last></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>2538-2550</pages>
      <abstract>Scientific Natural Language Inference (NLI) is the task of predicting the semantic relation between a pair of sentences extracted from research articles. The automatic annotation method based on distant supervision for the training set of SciNLI, the first and most popular dataset for this task, results in label noise which inevitably degenerates the performance of classifiers. In this paper, we propose a novel co-training method that assigns weights based on the training dynamics of the classifiers to the distantly supervised labels, reflective of the manner they are used in the subsequent training epochs. That is, unlike the existing semi-supervised learning (SSL) approaches, we consider the historical behavior of the classifiers to evaluate the quality of the automatically annotated labels. Furthermore, by assigning importance weights instead of filtering out examples based on an arbitrary threshold on the predicted confidence, we maximize the usage of automatically labeled data, while ensuring that the noisy labels have a minimal impact on model training. The proposed method obtains an improvement of 1.5% in Macro F1 over the distant supervision baseline, and substantial improvements over several other strong SSL baselines. We make our code and data available on Github.</abstract>
      <url hash="e7ca2850">2024.acl-long.139</url>
      <bibkey>sadat-caragea-2024-co</bibkey>
      <doi>10.18653/v1/2024.acl-long.139</doi>
    </paper>
    <paper id="140">
      <title><fixed-case>RLHFP</fixed-case>oison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models</title>
      <author><first>Jiongxiao</first><last>Wang</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Junlin</first><last>Wu</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <author><first>Yevgeniy</first><last>Vorobeychik</last><affiliation>Washington University, St. Louis</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <pages>2551-2570</pages>
      <abstract>Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences, playing an important role in LLMs alignment. Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially. To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates’ selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost). With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance. Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our findings highlight critical security challenges in RLHF, underscoring the necessity for more robust alignment methods for LLMs.</abstract>
      <url hash="8fd69f3b">2024.acl-long.140</url>
      <bibkey>wang-etal-2024-rlhfpoison</bibkey>
      <doi>10.18653/v1/2024.acl-long.140</doi>
    </paper>
    <paper id="141">
      <title>Time is Encoded in the Weights of Finetuned Language Models</title>
      <author><first>Kai</first><last>Nylund</last></author>
      <author><first>Suchin</first><last>Gururangan</last><affiliation>Facebook and University of Washington, Seattle</affiliation></author>
      <author><first>Noah</first><last>Smith</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>2571-2587</pages>
      <abstract>We present time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.</abstract>
      <url hash="6a7dcc54">2024.acl-long.141</url>
      <bibkey>nylund-etal-2024-time</bibkey>
      <doi>10.18653/v1/2024.acl-long.141</doi>
    </paper>
    <paper id="142">
      <title>Long-Context Language Modeling with Parallel Context Encoding</title>
      <author><first>Howard</first><last>Yen</last><affiliation>Princeton University</affiliation></author>
      <author><first>Tianyu</first><last>Gao</last></author>
      <author><first>Danqi</first><last>Chen</last><affiliation>Department of Computer Science, Princeton University</affiliation></author>
      <pages>2588-2610</pages>
      <abstract>Extending large language models (LLMs) to process longer inputs is crucial for numerous applications. However, the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window. We introduce Cross-Attention to Parallel Encodings (CAPE), a framework that can be applied to any existing decoder-only LLMs for context expansion. CAPE leverages a small encoder to process a long input chunk by chunk and enables the frozen decoder to cross-attend to the additional contexts. CAPE is efficient, generalizable, and versatile: trained with 8K-token documents, CAPE extends the context window of LLaMA-2 to 128K tokens, offering <tex-math>10\times</tex-math> of the throughput with only 1/6 of the memory. CAPE yields strong performance on language modeling and in-context learning. CAPE also excels in retrieval-augmented applications, while existing long-context models degenerate with retrieved contexts. We further introduce a CAPE variant that can extend the context window of instruction-tuned models with only unlabeled data, and showcase its effectiveness on LLaMA-2-Chat, leading to a strong instruction-following model that can leverage very long context on downstream tasks.</abstract>
      <url hash="5d302642">2024.acl-long.142</url>
      <bibkey>yen-etal-2024-long</bibkey>
      <doi>10.18653/v1/2024.acl-long.142</doi>
    </paper>
    <paper id="143">
      <title><fixed-case>S</fixed-case>ir<fixed-case>LLM</fixed-case>: Streaming Infinite Retentive <fixed-case>LLM</fixed-case></title>
      <author id="yao-yao"><first>Yao</first><last>Yao</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>2611-2624</pages>
      <abstract>As Large Language Models (LLMs) become increasingly prevalent in various domains, their ability to process inputs of any length and maintain a degree of memory becomes essential. However, the one-off input of overly long texts is limited, as studies have shown that when input lengths exceed the LLMs’ pre-trained text length, there is a dramatic decline in text generation capabilities. Moreover, simply extending the length of pre-training texts is impractical due to the difficulty in obtaining long text data and the substantial memory consumption costs this would entail for LLMs. Recent efforts have employed streaming inputs to alleviate the pressure of excessively long text inputs, but this approach can significantly impair the model’s long-term memory capabilities.Motivated by this challenge, we introduce Streaming Infinite Retentive LLM (SirLLM), which allows LLMs to maintain longer memory during infinite-length dialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy metric and a memory decay mechanism to filter key phrases, endowing LLMs with both long-lasting and flexible memory. We designed three distinct tasks and constructed three datasets to measure the effectiveness of SirLLM from various angles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our experimental results robustly demonstrate that SirLLM can achieve stable and significant improvements across different LLMs and tasks, compellingly proving its effectiveness. When having a coversation, “A sir could forget himself,” but SirLLM never does! Our code is publicly available at https://github.com/Zoeyyao27/SirLLMhttps://github.com/Zoeyyao27/SirLLM</abstract>
      <url hash="23945420">2024.acl-long.143</url>
      <bibkey>yao-etal-2024-sirllm</bibkey>
      <doi>10.18653/v1/2024.acl-long.143</doi>
    </paper>
    <paper id="144">
      <title><fixed-case>IMO</fixed-case>: Greedy Layer-Wise Sparse Representation Learning for Out-of-Distribution Text Classification with Pre-trained Models</title>
      <author><first>Tao</first><last>Feng</last><affiliation>Monash University</affiliation></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Zhuang</first><last>Li</last><affiliation>Monash University</affiliation></author>
      <author><first>Haolan</first><last>Zhan</last><affiliation>Monash University</affiliation></author>
      <author><first>Yuncheng</first><last>Hua</last></author>
      <author><first>Reza</first><last>Haf</last><affiliation>Monash University</affiliation></author>
      <pages>2625-2639</pages>
      <abstract>Machine learning models have made incredible progress, but they still struggle when applied to examples from unseen domains. This study focuses on a specific problem of domain generalization, where a model is trained on one source domain and tested on multiple target domains that are unseen during training. We propose IMO: Invariant features Masks for Out-of-Distribution text classification, to achieve OOD generalization by learning invariant features. During training, IMO would learn sparse mask layers to remove irrelevant features for prediction, where the remaining features keep invariant. Additionally, IMO has an attention module at the token level to focus on tokens that are useful for prediction. Our comprehensive experiments show that IMO substantially outperforms strong baselines in terms of various evaluation metrics and settings.</abstract>
      <url hash="c29f47ae">2024.acl-long.144</url>
      <bibkey>feng-etal-2024-imo</bibkey>
      <doi>10.18653/v1/2024.acl-long.144</doi>
    </paper>
    <paper id="145">
      <title>Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale</title>
      <author><first>Xiang</first><last>Hu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Pengyu</first><last>Ji</last></author>
      <author><first>Qingyang</first><last>Zhu</last></author>
      <author><first>Wei</first><last>Wu</last><affiliation>Ant Research</affiliation></author>
      <author><first>Kewei</first><last>Tu</last><affiliation>ShanghaiTech University</affiliation></author>
      <pages>2640-2657</pages>
      <abstract>A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner.We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion.We pre-train GPST on OpenWebText, a corpus with billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering both language understanding and language generation. Meanwhile, GPST also significantly outperforms existing unsupervised SLMs on left-to-right grammar induction, while holding a substantial acceleration on training.</abstract>
      <url hash="b5b79735">2024.acl-long.145</url>
      <bibkey>hu-etal-2024-generative</bibkey>
      <doi>10.18653/v1/2024.acl-long.145</doi>
    </paper>
    <paper id="146">
      <title><fixed-case>MELA</fixed-case>: Multilingual Evaluation of Linguistic Acceptability</title>
      <author><first>Ziyin</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yikang</first><last>Liu</last></author>
      <author><first>Weifang</first><last>Huang</last></author>
      <author><first>Junyu</first><last>Mao</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hai</first><last>Hu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>2658-2674</pages>
      <abstract>In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability—MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language—Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks.</abstract>
      <url hash="8d9ea482">2024.acl-long.146</url>
      <bibkey>zhang-etal-2024-mela</bibkey>
      <doi>10.18653/v1/2024.acl-long.146</doi>
    </paper>
    <paper id="147">
      <title><fixed-case>C</fixed-case>opy<fixed-case>NE</fixed-case>: Better Contextual <fixed-case>ASR</fixed-case> by Copying Named Entities</title>
      <author><first>Shilin</first><last>Zhou</last><affiliation>Soochow University</affiliation></author>
      <author><first>Zhenghua</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Yu</first><last>Hong</last><affiliation>Suzhou University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Zhefeng</first><last>Wang</last></author>
      <author><first>Baoxing</first><last>Huai</last></author>
      <pages>2675-2686</pages>
      <abstract>End-to-end automatic speech recognition (ASR) systems have made significant progress in general scenarios. However, it remains challenging to transcribe contextual named entities (NEs) in the contextual ASR scenario. Previous approaches have attempted to address this by utilizing the NE dictionary. These approaches treat entities as individual tokens and generate them token-by-token, which may result in incomplete transcriptions of entities. In this paper, we treat entities as indivisible wholes and introduce the idea of copying into ASR. We design a systematic mechanism called CopyNE, which can copy entities from the NE dictionary. By copying all tokens of an entity at once, we can reduce errors during entity transcription, ensuring the completeness of the entity. Experiments demonstrate that CopyNE consistently improves the accuracy of transcribing entities compared to previous approaches. Even when based on the strong Whisper, CopyNE still achieves notable improvements.</abstract>
      <url hash="c4263f2a">2024.acl-long.147</url>
      <bibkey>zhou-etal-2024-copyne</bibkey>
      <doi>10.18653/v1/2024.acl-long.147</doi>
    </paper>
    <paper id="148">
      <title>Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval</title>
      <author><first>Peter Baile</first><last>Chen</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>2687-2699</pages>
      <abstract>Retrieving relevant tables containing the necessary information to accurately answer a given question over tables is critical to open-domain question-answering (QA) systems. Previous methods assume the answer to such a question can be found either in a single table or multiple tables identified through question decomposition or rewriting. However, neither of these approaches is sufficient, as many questions require retrieving multiple tables and joining them through a join plan that cannot be discerned from the user query itself. If the join plan is not considered in the retrieval stage, the subsequent steps of reasoning and answering based on those retrieved tables are likely to be incorrect. To address this problem, we introduce a method that uncovers useful join relations for any query and database during table retrieval. We use a novel re-ranking method formulated as a mixed-integer program that considers not only table-query relevance but also table-table relevance that requires inferring join relationships. Our method outperforms the state-of-the-art approaches for table retrieval by up to 9.3% in F1 score and for end-to-end QA by up to 5.4% in accuracy.</abstract>
      <url hash="1a272263">2024.acl-long.148</url>
      <bibkey>chen-etal-2024-table</bibkey>
      <doi>10.18653/v1/2024.acl-long.148</doi>
    </paper>
    <paper id="149">
      <title>Generalizing Conversational Dense Retrieval via <fixed-case>LLM</fixed-case>-Cognition Data Augmentation</title>
      <author><first>Haonan</first><last>Chen</last></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Kelong</first><last>Mao</last></author>
      <author><first>Jiongnan</first><last>Liu</last></author>
      <author><first>Ziliang</first><last>Zhao</last></author>
      <pages>2700-2718</pages>
      <abstract>Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem – that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). We first generate multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware prompting process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space. A contrastive learning objective is then employed to train a better conversational context encoder. Extensive experiments conducted on four public datasets, under both normal and zero-shot settings, demonstrate the effectiveness, generalizability, and applicability of ConvAug. The code is released at https://github.com/haon-chen/ConvAug.</abstract>
      <url hash="5ebf05f6">2024.acl-long.149</url>
      <bibkey>chen-etal-2024-generalizing</bibkey>
      <doi>10.18653/v1/2024.acl-long.149</doi>
    </paper>
    <paper id="150">
      <title><fixed-case>I</fixed-case>t<fixed-case>D</fixed-case>: Large Language Models Can Teach Themselves Induction through Deduction</title>
      <author><first>Wangtao</first><last>Sun</last></author>
      <author><first>Haotian</first><last>Xu</last></author>
      <author><first>Xuanqing</first><last>Yu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Pei</first><last>Chen</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Shizhu</first><last>He</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>2719-2731</pages>
      <abstract>Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt “post processes” paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search &amp; refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36% and 10% compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different LLMs and deductors. The data and code of this paper can be found at https://github.com/forangel2014/ItD.</abstract>
      <url hash="c91f5b31">2024.acl-long.150</url>
      <bibkey>sun-etal-2024-itd</bibkey>
      <doi>10.18653/v1/2024.acl-long.150</doi>
    </paper>
    <paper id="151">
      <title><fixed-case>M</fixed-case>ath<fixed-case>G</fixed-case>enie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zimu</first><last>Lu</last></author>
      <author><first>Aojun</first><last>Zhou</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Houxing</first><last>Ren</last></author>
      <author><first>Ke</first><last>Wang</last></author>
      <author><first>Weikang</first><last>Shi</last></author>
      <author><first>Junting</first><last>Pan</last></author>
      <author><first>Mingjie</first><last>Zhan</last></author>
      <author><first>Hongsheng</first><last>Li</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>2732-2747</pages>
      <abstract>Large language models (LLMs) have exhibited great potential in mathematical reasoning. However, there remains a performance gap in this area between existing open-source models and closed-source models such as GPT-4. In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems by leveraging the ground-truth solutions of the seed data. We augment these ground-truth solutions and use a specially finetuned model to translate these augmented solutions back into new questions. Subsequently, we generate code-integrated solutions for these questions. To ensure the correctness of the code-integrated solutions, we employ rationale-based verification for filtering. Then, we finetune various pretrained models, ranging from 7B to 70B, on the newly curated data, resulting in a family of models known as MathGenie. These models consistently outperform previous open-source models across five representative mathematical reasoning datasets, achieving state-of-the-art performance. In particular, MathGenie-InternLM2 achieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best overall score.</abstract>
      <url hash="09c9f082">2024.acl-long.151</url>
      <bibkey>lu-etal-2024-mathgenie</bibkey>
      <doi>10.18653/v1/2024.acl-long.151</doi>
    </paper>
    <paper id="152">
      <title>Rethinking Task-Oriented Dialogue Systems: From Complex Modularity to Zero-Shot Autonomous Agent</title>
      <author><first>Heng-Da</first><last>Xu</last></author>
      <author><first>Xian-Ling</first><last>Mao</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Puhai</first><last>Yang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Fanshu</first><last>Sun</last></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>2748-2763</pages>
      <abstract>Task-oriented dialogue (TOD) systems are predominantly designed to be composed of several functional modules (e.g. dialogue state tracker, dialogue policy, natural language generation) whether they are pipeline or end-to-end architectures. However, this modular design not only heavily relies on massive fully-annotated data, but also suffers from many intrinsic drawbacks, such as serious error accumulation, poor generalization ability, high customization cost, and low fault tolerance rate. In this paper, we rethink the architecture of the task-oriented dialogue systems and propose a novel fully zero-shot autonomous TOD agent, named AutoTOD, where all the delicate modules in traditional TOD systems are deprecated and all it needs is a general-purpose instruction-following language model (e.g. GPT-4). AutoTOD only leverages a simple instruction schema consisting of the description of tasks and external APIs, and can autonomously decide what to do at each dialogue turn, including asking for information, calling APIs, summarizing API results, and correcting previous mistakes. Moreover, we propose a simulation-based evaluation framework to better validate the abilities of TOD models in real-life scenarios. Extensive experiments conducted on the MultiWOZ and SGD datasets show the superior task completion ability and flexible language skills of AutoTOD.</abstract>
      <url hash="8a2138fd">2024.acl-long.152</url>
      <bibkey>xu-etal-2024-rethinking</bibkey>
      <doi>10.18653/v1/2024.acl-long.152</doi>
    </paper>
    <paper id="153">
      <title>On Context Utilization in Summarization with Large Language Models</title>
      <author><first>Mathieu</first><last>Ravaut</last><affiliation>Abu Dhabi Investment Authority</affiliation></author>
      <author><first>Aixin</first><last>Sun</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <pages>2764-2781</pages>
      <abstract>Large language models (LLMs) excel in abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, exceeding 100k tokens. However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source document(s). Besides, in summarization, mapping facts from the source to the summary is not trivial as salient content is usually re-phrased. In this paper, we conduct the first comprehensive study on context utilization and position bias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5 evaluation metrics. We introduce a new evaluation benchmark called MiddleSum on the which we benchmark two alternative inference methods to alleviate position bias: hierarchical summarization and incremental summarization. Our code and data can be found here: https://github.com/ntunlp/MiddleSum.</abstract>
      <url hash="a2187e79">2024.acl-long.153</url>
      <bibkey>ravaut-etal-2024-context</bibkey>
      <doi>10.18653/v1/2024.acl-long.153</doi>
    </paper>
    <paper id="154">
      <title><fixed-case>INTERS</fixed-case>: Unlocking the Power of Large Language Models in Search with Instruction Tuning</title>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Peitian</first><last>Zhang</last></author>
      <author><first>Chenghao</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yifei</first><last>Chen</last></author>
      <author><first>Binyu</first><last>Xie</last></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>2782-2809</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating a comprehensive understanding and execution of IR tasks, thereby limiting LLMs’ applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs’ proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 20 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly available LLMs, such as LLaMA, Mistral, and Falcon, in IR tasks. Furthermore, we conduct extensive experiments to analyze the effects of instruction design, template diversity, few-shot demonstrations, and the volume of instructions on performance. We make our dataset and the fine-tuned models publicly accessible at https://github.com/DaoD/INTERS.</abstract>
      <url hash="b3b2ff40">2024.acl-long.154</url>
      <bibkey>zhu-etal-2024-inters</bibkey>
      <doi>10.18653/v1/2024.acl-long.154</doi>
    </paper>
    <paper id="155">
      <title>Enhancing In-Context Learning via Implicit Demonstration Augmentation</title>
      <author><first>Xiaoling</first><last>Zhou</last></author>
      <author><first>Wei</first><last>Ye</last><affiliation>Peking University</affiliation></author>
      <author><first>Yidong</first><last>Wang</last></author>
      <author><first>Chaoya</first><last>Jiang</last><affiliation>Shanghai Jiaotong University, Wuhan University, Tsinghua University, Tsinghua University, Microsoft, University of the Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing University of Aeronautics and Astronautics, South China University of Technology, SUN YAT-SEN UNIVERSITY, University of Electronic Science and Technology of China, Huazhong University of Science and Technology, Harbin Institute of Technology, Shandong University, nanjing university, Beijing University of Posts and Telecommunications, Shanghai Artificial Intelligence Laboratory, Shanghai University of Science and Technology, Tianjin University, Northeastern University, Southeast University, Xi’an Jiaotong University, Xiamen University, Fudan University, Renmin University of China, Nankai University, Meituan, Kuaishou- 快手科技, East China Normal University, Xi’an University of Electronic Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing University of Science and Technology, Southern University of Science and Technology, Northwest Polytechnical University Xi’an, Chongqing University, Jilin University, Beijing Normal University, University of Science and Technology Beijing and Zhejiang University</affiliation></author>
      <author><first>Zhemg</first><last>Lee</last></author>
      <author><first>Rui</first><last>Xie</last></author>
      <author><first>Shikun</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>2810-2828</pages>
      <abstract>The emergence of in-context learning (ICL) enables large pre-trained language models (PLMs) to make predictions for unseen inputs without updating parameters. Despite its potential, ICL’s effectiveness heavily relies on the quality, quantity, and permutation of demonstrations, commonly leading to suboptimal and unstable performance. In this paper, we tackle this challenge for the first time from the perspective of demonstration augmentation. Specifically, we start with enriching representations of demonstrations by leveraging their deep feature distribution. We then theoretically reveal that when the number of augmented copies approaches infinity, the augmentation is approximately equal to a novel logit calibration mechanism integrated with specific statistical properties. This insight results in a simple yet highly efficient method that significantly improves the average and worst-case accuracy across diverse PLMs and tasks. Moreover, our method effectively reduces performance variance among varying demonstrations, permutations, and templates, and displays the capability to address imbalanced class distributions.</abstract>
      <url hash="e8f3b890">2024.acl-long.155</url>
      <bibkey>zhou-etal-2024-enhancing-context</bibkey>
      <doi>10.18653/v1/2024.acl-long.155</doi>
    </paper>
    <paper id="156">
      <title><fixed-case>PR</fixed-case>o<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Partial Rotation Empowers More Parameter-Efficient <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case></title>
      <author><first>Sheng</first><last>Wang</last></author>
      <author><first>Boyang</first><last>Xue</last></author>
      <author><first>Jiacheng</first><last>Ye</last></author>
      <author><first>Jiyue</first><last>Jiang</last></author>
      <author><first>Liheng</first><last>Chen</last></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Chuan</first><last>Wu</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>2829-2841</pages>
      <abstract>With the rapid scaling of large language models (LLMs), serving numerouslow-rank adaptations (LoRAs) concurrently has become increasingly impractical,leading to unaffordable costs and necessitating more parameter-efficientfinetuning methods. In this work, we introduce <tex-math>\text{\textbf{P}artially \textbf{Ro}tation-enhanced \textbf{Lo}w-\textbf{R}ank \textbf{A}daptation (PRoLoRA)}</tex-math>, an intra-layer sharing mechanism comprising fouressential components: broadcast reduction, rotation enhancement,partially-sharing refinement, and rectified initialization strategy. As asuperset of LoRA, PRoLoRA retains its advantages, and effectively circumventthe drawbacks of peer parameter-sharing methods with superior model capacity,practical feasibility, and broad applicability. Empirical experimentsdemonstrate the remarkably higher parameter efficiency of PRoLoRA in bothspecific parameter budget and performance target scenarios, and its scalabilityto larger LLMs. Notably, with one time less trainable parameters, PRoLoRA stilloutperforms LoRA on multiple instruction tuning datasets. Subsequently, anablation study is conducted to validate the necessity of individual componentsand highlight the superiority of PRoLoRA over three potential variants.Hopefully, the conspicuously higher parameter efficiency can establish PRoLoRAas a resource-friendly alternative to LoRA.</abstract>
      <url hash="dfce26f1">2024.acl-long.156</url>
      <bibkey>wang-etal-2024-prolora</bibkey>
      <doi>10.18653/v1/2024.acl-long.156</doi>
    </paper>
    <paper id="157">
      <title>Improving Event Definition Following For Zero-Shot Event Detection</title>
      <author><first>Zefan</first><last>Cai</last></author>
      <author><first>Po-Nien</first><last>Kung</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Ashima</first><last>Suvarna</last></author>
      <author><first>Mingyu</first><last>Ma</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Hritik</first><last>Bansal</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Baobao</first><last>Chang</last><affiliation>Peking University</affiliation></author>
      <author><first>P. Jeffrey</first><last>Brantingham</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>2842-2863</pages>
      <abstract>Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions. These approaches yield sporadic successes, yet generally fall short of expectations.In this work, we aim to improve zero-shot event detection by training models to better follow event definitions. We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types. To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies. Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten examples per event type.Beyond scaling, we incorporate event ontology information and hard-negative samples during training, further boosting the performance. Based on these findings, we fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that surpasses SOTA large language models like GPT-3.5 across three open benchmarks on zero-shot event detection.</abstract>
      <url hash="f6a5a659">2024.acl-long.157</url>
      <bibkey>cai-etal-2024-improving-event</bibkey>
      <doi>10.18653/v1/2024.acl-long.157</doi>
    </paper>
    <paper id="158">
      <title>Through the <fixed-case>MUD</fixed-case>: A Multi-Defendant Charge Prediction Benchmark with Linked Crime Elements</title>
      <author><first>Xiao</first><last>Wei</last><affiliation>Shanghai University</affiliation></author>
      <author><first>Qi</first><last>Xu</last><affiliation>Shanghai University</affiliation></author>
      <author><first>Hang</first><last>Yu</last><affiliation>Shanghai University</affiliation></author>
      <author><first>Qian</first><last>Liu</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>2864-2878</pages>
      <abstract>The current charge prediction datasets mostly focus on single-defendant criminal cases.However, real-world criminal cases usually involve multiple defendants whose criminal facts are intertwined. In an early attempt to fill this gap, we introduce a new benchmark that encompasses legal cases involving multiple defendants, where each defendant is labeled with a charge and four types of crime elements, <i>i.e.,</i> <i>Object Element</i>, <i>Objective Element</i>, <i>Subject Element</i>, and <i>Subjective Element</i>. Based on the dataset, we further develop an interpretable model called EJudge that incorporates crime elements and legal rules to infer charges. We observe that predicting crime charges while providing corresponding rationales benefits the interpretable AI system. Extensive experiments show that EJudge significantly surpasses state-of-the-art methods, which verify the importance of crime elements and legal rules in multi-defendant charge prediction. The source code and dataset are available at <url>https://anonymous.4open.science/r/MCP_1-6010</url>.</abstract>
      <url hash="d48218a3">2024.acl-long.158</url>
      <bibkey>wei-etal-2024-mud</bibkey>
      <doi>10.18653/v1/2024.acl-long.158</doi>
    </paper>
    <paper id="159">
      <title>Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding</title>
      <author><first>Yiruo</first><last>Cheng</last></author>
      <author><first>Kelong</first><last>Mao</last></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>2879-2893</pages>
      <abstract>Conversational dense retrieval has shown to be effective in conversational search. However, a major limitation of conversational dense retrieval is their lack of interpretability, hindering intuitive understanding of model behaviors for targeted improvements. This paper presents CONVINV, a simple yet effective approach to shed light on interpretable conversational dense retrieval models. CONVINV transforms opaque conversational session embeddings into explicitly interpretable text while faithfully maintaining their original retrieval performance as much as possible. Such transformation is achieved by training a recently proposed Vec2Text model based on the ad-hoc query encoder, leveraging the fact that the session and query embeddings share the same space in existing conversational dense retrieval.To further enhance interpretability, we propose to incorporate external interpretable query rewrites into the transformation process. Extensive evaluations on three conversational search benchmarks demonstrate that CONVINV can yield more interpretable text and faithfully preserve original retrieval performance than baselines. Our work connects opaque session embeddings with transparent query rewriting, paving the way toward trustworthy conversational search.</abstract>
      <url hash="176d833c">2024.acl-long.159</url>
      <bibkey>cheng-etal-2024-interpreting</bibkey>
      <doi>10.18653/v1/2024.acl-long.159</doi>
    </paper>
    <paper id="160">
      <title>Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks</title>
      <author><first>Yichen</first><last>Wang</last></author>
      <author><first>Shangbin</first><last>Feng</last><affiliation>University of Washington</affiliation></author>
      <author><first>Abe</first><last>Hou</last></author>
      <author><first>Xiao</first><last>Pu</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Chao</first><last>Shen</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Xiaoming</first><last>Liu</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Tianxing</first><last>He</last></author>
      <pages>2894-2925</pages>
      <abstract>The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors’ robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, co-generating, and prompting. Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches.</abstract>
      <url hash="3af7c3cb">2024.acl-long.160</url>
      <bibkey>wang-etal-2024-stumbling</bibkey>
      <doi>10.18653/v1/2024.acl-long.160</doi>
    </paper>
    <paper id="161">
      <title>Training Language Models to Generate Text with Citations via Fine-grained Rewards</title>
      <author><first>Chengyu</first><last>Huang</last></author>
      <author><first>Zeqiu</first><last>Wu</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Yushi</first><last>Hu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Wenya</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>2926-2949</pages>
      <abstract>While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly prompted LLMs to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller LLMs. In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common LLM training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on Question Answering (QA) datasets taken from the ALCE benchmark and validate the model’s generalizability using EXPERTQA. On LLaMA-2-7B, the incorporation of fine-grained rewards achieves the best performance among the baselines, even surpassing that of GPT-3.5-turbo.</abstract>
      <url hash="b92da393">2024.acl-long.161</url>
      <bibkey>huang-etal-2024-training</bibkey>
      <doi>10.18653/v1/2024.acl-long.161</doi>
    </paper>
    <paper id="162">
      <title>Hypergraph based Understanding for Document Semantic Entity Recognition</title>
      <author><first>Qiwei</first><last>Li</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Ping</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Haojun</first><last>Ai</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>2950-2960</pages>
      <abstract>Semantic entity recognition is an important task in the field of visually-rich document understanding. It distinguishes the semantic types of text by analyzing the position relationship between text nodes and the relation between text content. The existing document understanding models mainly focus on entity categories while ignoring the extraction of entity boundaries. We build a novel hypergraph attention document semantic entity recognition framework, HGA, which uses hypergraph attention to focus on entity boundaries and entity categories at the same time. It can conduct a more detailed analysis of the document text representation analyzed by the upstream model and achieves a better performance of semantic information. We apply this method on the basis of GraphLayoutLM to construct a new semantic entity recognition model HGALayoutLM. Our experiment results on FUNSD, CORD, XFUND and SROIE show that our method can effectively improve the performance of semantic entity recognition tasks based on the original model. The results of HGALayoutLM on FUNSD and XFUND reach the new state-of-the-art results.</abstract>
      <url hash="f5158187">2024.acl-long.162</url>
      <bibkey>li-etal-2024-hypergraph</bibkey>
      <doi>10.18653/v1/2024.acl-long.162</doi>
    </paper>
    <paper id="163">
      <title><fixed-case>GSM</fixed-case>-Plus: A Comprehensive Benchmark for Evaluating the Robustness of <fixed-case>LLM</fixed-case>s as Mathematical Problem Solvers</title>
      <author><first>Qintong</first><last>Li</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Xueliang</first><last>Zhao</last></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Wei</first><last>Bi</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>2961-2984</pages>
      <abstract>Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs’ math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.</abstract>
      <url hash="168b2bd0">2024.acl-long.163</url>
      <bibkey>li-etal-2024-gsm</bibkey>
      <doi>10.18653/v1/2024.acl-long.163</doi>
    </paper>
    <paper id="164">
      <title>Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models</title>
      <author><first>Qingkai</first><last>Min</last></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Xiangkun</first><last>Hu</last><affiliation>Amazon</affiliation></author>
      <author><first>Songfang</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zheng</first><last>Zhang</last><affiliation>New York University</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>2985-3002</pages>
      <abstract>Cross-document event coreference resolution (CDECR) involves clustering event mentions across multiple documents that refer to the same real-world events. Existing approaches utilize fine-tuning of small language models (SLMs) like BERT to address the compatibility among the contexts of event mentions. However, due to the complexity and diversity of contexts, these models are prone to learning simple co-occurrences. Recently, large language models (LLMs) like ChatGPT have demonstrated impressive contextual understanding, yet they encounter challenges in adapting to specific information extraction (IE) tasks. In this paper, we propose a collaborative approach for CDECR, leveraging the capabilities of both a universally capable LLM and a task-specific SLM. The collaborative strategy begins with the LLM accurately and comprehensively summarizing events through prompting. Then, the SLM refines its learning of event representations based on these insights during fine-tuning. Experimental results demonstrate that our approach surpasses the performance of both the large and small language models individually, forming a complementary advantage. Across various datasets, our approach achieves state-of-the-art performance, underscoring its effectiveness in diverse scenarios.</abstract>
      <url hash="7b017d2b">2024.acl-long.164</url>
      <bibkey>min-etal-2024-synergetic</bibkey>
      <doi>10.18653/v1/2024.acl-long.164</doi>
    </paper>
    <paper id="165">
      <title><fixed-case>A</fixed-case>uto<fixed-case>A</fixed-case>ct: Automatic Agent Learning from Scratch for <fixed-case>QA</fixed-case> via Self-Planning</title>
      <author><first>Shuofei</first><last>Qiao</last></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Runnan</first><last>Fang</last></author>
      <author><first>Yujie</first><last>Luo</last></author>
      <author><first>Wangchunshu</first><last>Zhou</last><affiliation>AIWaves Inc.</affiliation></author>
      <author><first>Yuchen</first><last>Jiang</last><affiliation>AIWaves Inc.</affiliation></author>
      <author><first>Chengfei</first><last>Lv</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3003-3021</pages>
      <abstract>Language agents have achieved considerable performance on various complex question-answering tasks by planning with external tools. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework for QA that does not rely on large-scale annotated data and synthetic planning trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. Further analysis demonstrates the effectiveness of the division-of-labor strategy, with the trajectory quality generated by AutoAct generally outperforming that of others.</abstract>
      <url hash="48791369">2024.acl-long.165</url>
      <bibkey>qiao-etal-2024-autoact</bibkey>
      <doi>10.18653/v1/2024.acl-long.165</doi>
    </paper>
    <paper id="166">
      <title><fixed-case>C</fixed-case>hronos<fixed-case>L</fixed-case>ex: Time-aware Incremental Training for Temporal Generalization of Legal Classification Tasks</title>
      <author><first>Santosh</first><last>T.y.s.s</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Tuan-Quang</first><last>Vuong</last></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technische Universität München</affiliation></author>
      <pages>3022-3039</pages>
      <abstract>This study investigates the challenges posed by the dynamic nature of legal multi-label text classification tasks, where legal concepts evolve over time. Existing models often overlook the temporal dimension in their training process, leading to suboptimal performance of those models over time, as they treat training data as a single homogeneous block. To address this, we introduce ChronosLex, an incremental training paradigm that trains models on chronological splits, preserving the temporal order of the data. However, this incremental approach raises concerns about overfitting to recent data, prompting an assessment of mitigation strategies using continual learning and temporal invariant methods. Our experimental results over six legal multi-label text classification datasets reveal that continual learning methods prove effective in preventing overfitting thereby enhancing temporal generalizability, while temporal invariant methods struggle to capture these dynamics of temporal shifts.</abstract>
      <url hash="ed4377c7">2024.acl-long.166</url>
      <bibkey>t-y-s-s-etal-2024-chronoslex</bibkey>
      <doi>10.18653/v1/2024.acl-long.166</doi>
    </paper>
    <paper id="167">
      <title>Virtual Compiler Is All You Need For Assembly Code Search</title>
      <author><first>Zeyu</first><last>Gao</last></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuanda</first><last>Wang</last></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>3040-3051</pages>
      <abstract>Assembly code search is vital for reducing the burden on reverse engineers, allowing them to quickly identify specific functions using natural language within vast binary programs.Despite its significance, this critical task is impeded by the complexities involved in building high-quality datasets. This paper explores training a Large Language Model (LLM) to emulate a general compiler. By leveraging Ubuntu packages to compile a dataset of 20 billion tokens, we further continue pre-train CodeLlama as a Virtual Compiler (ViC), capable of compiling any source code to assembly code. This approach allows for “virtual” compilation across a wide range of programming languages without the need for a real compiler, preserving semantic equivalency and expanding the possibilities for assembly code dataset construction. Furthermore, we use ViC to construct a sufficiently large dataset for assembly code search. Employing this extensive dataset, we achieve a substantial improvement in assembly code search performance, with our model surpassing the leading baseline by 26%.</abstract>
      <url hash="e1a476d3">2024.acl-long.167</url>
      <bibkey>gao-etal-2024-virtual</bibkey>
      <doi>10.18653/v1/2024.acl-long.167</doi>
    </paper>
    <paper id="168">
      <title><fixed-case>MEL</fixed-case>o<fixed-case>RA</fixed-case>: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning</title>
      <author><first>Pengjie</first><last>Ren</last><affiliation>Shandong University</affiliation></author>
      <author><first>Chengshun</first><last>Shi</last></author>
      <author><first>Shiguang</first><last>Wu</last></author>
      <author><first>Mengqi</first><last>Zhang</last><affiliation>Shandong University</affiliation></author>
      <author><first>Zhaochun</first><last>Ren</last><affiliation>Leiden University</affiliation></author>
      <author><first>Maarten</first><last>de Rijke</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Zhumin</first><last>Chen</last><affiliation>Shandong University</affiliation></author>
      <author><first>Jiahuan</first><last>Pei</last><affiliation>Centrum voor Wiskunde en Informatica</affiliation></author>
      <pages>3052-3064</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models’ scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential.The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theoretical analysis and empirical studies on various NLP tasks. Our experimental results show that, compared to LoRA, MELoRA achieves better performance with 8 times fewer trainable parameters on natural language understanding tasks and 36 times fewer trainable parameters on instruction following tasks, which demonstrates the effectiveness of MELoRA.</abstract>
      <url hash="fa4aef59">2024.acl-long.168</url>
      <bibkey>ren-etal-2024-melora</bibkey>
      <doi>10.18653/v1/2024.acl-long.168</doi>
    </paper>
    <paper id="169">
      <title>Can <fixed-case>LLM</fixed-case>s Learn from Previous Mistakes? Investigating <fixed-case>LLM</fixed-case>s’ Errors to Boost for Reasoning</title>
      <author><first>Yongqi</first><last>Tong</last></author>
      <author><first>Dawei</first><last>Li</last></author>
      <author><first>Sizhe</first><last>Wang</last></author>
      <author><first>Yujia</first><last>Wang</last></author>
      <author><first>Fei</first><last>Teng</last></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>3065-3080</pages>
      <abstract>Large language models (LLMs) have demonstrated striking reasoning capability. Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: <i>can LLMs learn and benefit from their mistakes, especially for their reasoning?</i>This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing CoTErrorSet, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) <b>Self-rethinking</b> prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) <b>Mistake tuning</b> involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind LLMs’ errors, which provides directions that future research needs to overcome. CoTErrorSet will be published soon on <url>https://github.com/YookiTong/Learn-from-Mistakes-CotErrorSet</url>.</abstract>
      <url hash="d7769c74">2024.acl-long.169</url>
      <bibkey>tong-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.acl-long.169</doi>
    </paper>
    <paper id="170">
      <title>An Iterative Associative Memory Model for Empathetic Response Generation</title>
      <author><first>Zhou</first><last>Yang</last></author>
      <author><first>Zhaochun</first><last>Ren</last><affiliation>Leiden University</affiliation></author>
      <author><first>Wang</first><last>Yufeng</last></author>
      <author><first>Haizhou</first><last>Sun</last><affiliation>SmartMore</affiliation></author>
      <author><first>Chao</first><last>Chen</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xiaofei</first><last>Zhu</last></author>
      <author><first>Xiangwen</first><last>Liao</last><affiliation>Fuzhou University</affiliation></author>
      <pages>3081-3092</pages>
      <abstract>Empathetic response generation aims to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances.We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Variant experiments on LLMs also demonstrate that attending to associated words improves empathetic comprehension and expression.</abstract>
      <url hash="5b64aa98">2024.acl-long.170</url>
      <bibkey>yang-etal-2024-iterative</bibkey>
      <doi>10.18653/v1/2024.acl-long.170</doi>
    </paper>
    <paper id="171">
      <title>Detoxifying Large Language Models via Knowledge Editing</title>
      <author><first>Mengru</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ziwen</first><last>Xu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zekun</first><last>Xi</last></author>
      <author><first>Shumin</first><last>Deng</last></author>
      <author><first>Yunzhi</first><last>Yao</last></author>
      <author><first>Qishen</first><last>Zhang</last></author>
      <author><first>Linyi</first><last>Yang</last><affiliation>Westlake University</affiliation></author>
      <author><first>Jindong</first><last>Wang</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3093-3118</pages>
      <abstract>This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments with several knowledge editing approaches, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxifying approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs.</abstract>
      <url hash="b4117740">2024.acl-long.171</url>
      <bibkey>wang-etal-2024-detoxifying</bibkey>
      <doi>10.18653/v1/2024.acl-long.171</doi>
    </paper>
    <paper id="172">
      <title><fixed-case>L</fixed-case>ong<fixed-case>B</fixed-case>ench: A Bilingual, Multitask Benchmark for Long Context Understanding</title>
      <author><first>Yushi</first><last>Bai</last></author>
      <author><first>Xin</first><last>Lv</last></author>
      <author><first>Jiajie</first><last>Zhang</last></author>
      <author><first>Hongchang</first><last>Lyu</last></author>
      <author><first>Jiankai</first><last>Tang</last></author>
      <author><first>Zhidian</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhengxiao</first><last>Du</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Aohan</first><last>Zeng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>3119-3137</pages>
      <abstract>Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs’ long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.</abstract>
      <url hash="2469a3a1">2024.acl-long.172</url>
      <bibkey>bai-etal-2024-longbench</bibkey>
      <doi>10.18653/v1/2024.acl-long.172</doi>
    </paper>
    <paper id="173">
      <title><fixed-case>D</fixed-case>r.<fixed-case>A</fixed-case>cademy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models</title>
      <author><first>Yuyan</first><last>Chen</last></author>
      <author><first>Chenwei</first><last>Wu</last></author>
      <author><first>Songzhou</first><last>Yan</last></author>
      <author><first>Panjun</first><last>Liu</last></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>3138-3167</pages>
      <abstract>Teachers are important to imparting knowledge and guiding learners, and the role of large language models (LLMs) as potential educators is emerging as an important area of study. Recognizing LLMs’ capability to generate educational content can lead to advances in automated and personalized learning. While LLMs have been tested for their comprehension and problem-solving skills, their capability in teaching remains largely unexplored.In teaching, questioning is a key skill that guides students to analyze, evaluate, and synthesize core concepts and principles.Therefore, our research introduces a benchmark to evaluate the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions, utilizing Anderson and Krathwohl’s taxonomy across general, monodisciplinary, and interdisciplinary domains. We shift the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs’ outputs. Our results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses; Claude2 appears more apt as an interdisciplinary teacher. Furthermore, the automatic scores align with human perspectives.</abstract>
      <url hash="b0f4339d">2024.acl-long.173</url>
      <bibkey>chen-etal-2024-dr</bibkey>
      <doi>10.18653/v1/2024.acl-long.173</doi>
    </paper>
    <paper id="174">
      <title><fixed-case>U</fixed-case>ni<fixed-case>B</fixed-case>ridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages</title>
      <author><first>Trinh</first><last>Pham</last><affiliation>Ho Chi Minh City University of Technology</affiliation></author>
      <author><first>Khoi</first><last>Le</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>3168-3184</pages>
      <abstract>In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with Optimized Embeddings and Vocabulary), a comprehensive approach developed to improve the effectiveness of Cross-Lingual Transfer Learning, particularly in languages with limited resources. Our approach tackles two essential elements of a language model: the initialization of embeddings and the optimal vocabulary size. Specifically, we propose a novel embedding initialization method that leverages both lexical and semantic alignment for a language. In addition, we present a method for systematically searching for the optimal vocabulary size, ensuring a balance between model complexity and linguistic coverage. Our experiments across multilingual datasets show that our approach greatly improves the F1-Score in several languages. UniBridge is a robust and adaptable solution for cross-lingual systems in various languages, highlighting the significance of initializing embeddings and choosing the right vocabulary size in cross-lingual environments.</abstract>
      <url hash="4dba9cd8">2024.acl-long.174</url>
      <bibkey>pham-etal-2024-unibridge</bibkey>
      <doi>10.18653/v1/2024.acl-long.174</doi>
    </paper>
    <paper id="175">
      <title><fixed-case>VISTA</fixed-case>: Visualized Text Embedding For Universal Multi-Modal Retrieval</title>
      <author><first>Junjie</first><last>Zhou</last></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Shitao</first><last>Xiao</last></author>
      <author><first>Bo</first><last>Zhao</last><affiliation>BAAI</affiliation></author>
      <author><first>Yongping</first><last>Xiong</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>3185-3200</pages>
      <abstract>Multi-modal retrieval becomes increasingly popular in practice. However, the existing retrievers are mostly text-oriented, which lack the capability to process visual information. Despite the presence of vision-language models like CLIP, the current methods are severely limited in representing the text-only and image-only data. In this work, we present a new embedding model VISTA for universal multi-modal retrieval. Our work brings forth threefold technical contributions. Firstly, we introduce a flexible architecture which extends a powerful text encoder with the image understanding capability by introducing visual token embeddings. Secondly, we develop two data generation strategies, which bring high-quality composed image-text to facilitate the training of the embedding model. Thirdly, we introduce a multi-stage training algorithm, which first aligns the visual token embedding with the text encoder using massive weakly labeled data, and then develops multi-modal representation capability using the generated composed image-text data. In our experiments, VISTA achieves superior performances across a variety of multi-modal retrieval tasks in both zero-shot and supervised settings. Our model, data, and source code are available at https://github.com/FlagOpen/FlagEmbedding.</abstract>
      <url hash="56ec522b">2024.acl-long.175</url>
      <bibkey>zhou-etal-2024-vista</bibkey>
      <doi>10.18653/v1/2024.acl-long.175</doi>
    </paper>
    <paper id="176">
      <title>Black-Box Prompt Optimization: Aligning Large Language Models without Model Training</title>
      <author><first>Jiale</first><last>Cheng</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Kehan</first><last>Zheng</last></author>
      <author><first>Pei</first><last>Ke</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Hongning</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>3201-3219</pages>
      <abstract>Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them; that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods primarily focus on further training them. However, the extra training of LLMs is usually expensive in terms of GPU computing; even worse, some LLMs are not accessible for user-demanded training, such as GPTs. In this work, we take a different perspective—Black-Box Prompt Optimization (BPO)—to perform alignments. The idea is to optimize user prompts to suit LLMs’ input understanding, so as to best realize users’ intents without updating LLMs’ parameters. BPO leverages human preferences to optimize prompts, thus making it superior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is model-agnostic, and the empirical results demonstrate that the BPO-aligned ChatGPT yields a 22% increase in the win rate against its original version and 10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO. Code and datasets are released at https://github.com/thu-coai/BPO.</abstract>
      <url hash="a3aac7c4">2024.acl-long.176</url>
      <bibkey>cheng-etal-2024-black</bibkey>
      <doi>10.18653/v1/2024.acl-long.176</doi>
    </paper>
    <paper id="177">
      <title>Open <fixed-case>K</fixed-case>o-<fixed-case>LLM</fixed-case> Leaderboard: Evaluating Large Language Models in <fixed-case>K</fixed-case>orean with <fixed-case>K</fixed-case>o-H5 Benchmark</title>
      <author><first>Chanjun</first><last>Park</last><affiliation>Upstage</affiliation></author>
      <author><first>Hyeonwoo</first><last>Kim</last></author>
      <author><first>Dahyun</first><last>Kim</last><affiliation>Upstage AI Research</affiliation></author>
      <author><first>SeongHwan</first><last>Cho</last></author>
      <author><first>Sanghoon</first><last>Kim</last><affiliation>Upstage</affiliation></author>
      <author><first>Sukyung</first><last>Lee</last></author>
      <author><first>Yungi</first><last>Kim</last><affiliation>Upstage</affiliation></author>
      <author><first>Hwalsuk</first><last>Lee</last><affiliation>Upstage</affiliation></author>
      <pages>3220-3234</pages>
      <abstract>This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.</abstract>
      <url hash="b2f13ca2">2024.acl-long.177</url>
      <bibkey>park-etal-2024-open</bibkey>
      <doi>10.18653/v1/2024.acl-long.177</doi>
    </paper>
    <paper id="178">
      <title>Unified Hallucination Detection for Multimodal Large Language Models</title>
      <author><first>Xiang</first><last>Chen</last></author>
      <author><first>Chenxi</first><last>Wang</last></author>
      <author><first>Yida</first><last>Xue</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xiaoyan</first><last>Yang</last></author>
      <author><first>Qiang</first><last>Li</last></author>
      <author><first>Yue</first><last>Shen</last><affiliation>antgroup</affiliation></author>
      <author><first>Lei</first><last>Liang</last></author>
      <author><first>Jinjie</first><last>Gu</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3235-3252</pages>
      <abstract>Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.</abstract>
      <url hash="d718fcb9">2024.acl-long.178</url>
      <bibkey>chen-etal-2024-unified-hallucination</bibkey>
      <doi>10.18653/v1/2024.acl-long.178</doi>
    </paper>
    <paper id="179">
      <title>Empowering Character-level Text Infilling by Eliminating Sub-Tokens</title>
      <author><first>Houxing</first><last>Ren</last></author>
      <author><first>Mingjie</first><last>Zhan</last></author>
      <author><first>Zhongyuan</first><last>Wu</last></author>
      <author><first>Hongsheng</first><last>Li</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>3253-3267</pages>
      <abstract>In infilling tasks, sub-tokens, representing instances where a complete token is segmented into two parts, often emerge at the boundaries of prefixes, middles, and suffixes. Traditional methods focused on training models at the token level, leading to sub-optimal performance in character-level infilling tasks during the inference stage. Alternately, some approaches considered character-level infilling, but they relied on predicting sub-tokens in inference, yet this strategy diminished ability in character-level infilling tasks due to the large perplexity of the model on sub-tokens. In this paper, we introduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and Ending character constraints. The proposed method addresses character-level infilling tasks by utilizing a line-level format to avoid predicting any sub-token in inference. In addition, we incorporate two special tokens to signify the rest of the incomplete lines, thereby enhancing generation guidance. Extensive experiments demonstrate that our proposed approach surpasses previous methods, offering a significant advantage. Code is available at https://github.com/SenseLLM/FIM-SE.</abstract>
      <url hash="f4c0eb3d">2024.acl-long.179</url>
      <bibkey>ren-etal-2024-empowering</bibkey>
      <doi>10.18653/v1/2024.acl-long.179</doi>
    </paper>
    <paper id="180">
      <title>Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models</title>
      <author><first>Kun</first><last>Luo</last></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Shitao</first><last>Xiao</last></author>
      <author><first>Tong</first><last>Zhou</last></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>3268-3281</pages>
      <abstract>Retrieval augmentation is a promising approach to handle long-context language modeling. However, the existing retrieval methods usually work with the chunked context, which is prone to inferior quality of semantic representation and incomplete retrieval of useful information. In this work, we propose a new method for the retrieval augmentation of long-context language modeling, called Landmark Embedding. Our method is characterized by threefold technical contributions. Firstly, we introduce a <i>chunking-free architecture</i>, which keeps the long context coherent such that high-quality embeddings can be generated for the fine-grained units within the context. Secondly, we present a position-aware objective function, which prioritizes the ultimate boundary for a consecutive span of information. By learning to discriminate such a special position, the useful information can be comprehensively retrieved for the query. Thirdly, we design a novel multi-stage learning algorithm, which makes the best use of readily available data and synthetic data for cost-effective training of the landmark embedding. In our experimental study, landmark embedding is able to substantially improve the performance for both LLaMA-2 and ChatGPT in a variety of long-context tasks; meanwhile, it also outperforms the existing retrieval methods with a notable advantage. Our model and source code will be made publicly available.</abstract>
      <url hash="0ebb5b7c">2024.acl-long.180</url>
      <bibkey>luo-etal-2024-landmark</bibkey>
      <doi>10.18653/v1/2024.acl-long.180</doi>
    </paper>
    <paper id="181">
      <title><fixed-case>G</fixed-case>row<fixed-case>OVER</fixed-case>: How Can <fixed-case>LLM</fixed-case>s Adapt to Growing Real-World Knowledge?</title>
      <author><first>Dayoon</first><last>Ko</last></author>
      <author><first>Jinyoung</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hahyeon</first><last>Choi</last></author>
      <author><first>Gunhee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>3282-3308</pages>
      <abstract>In the real world, knowledge is constantly evolving, which can render existing knowledge-based datasets outdated. This unreliability highlights the critical need for continuous updates to ensure both accuracy and relevance in knowledge-intensive tasks. To address this, we propose GrowOVER-QA and GrowOVER-Dialogue, dynamic open-domain QA and dialogue benchmarks that undergo a continuous cycle of updates, keeping pace with the rapid evolution of knowledge. Our research indicates that retrieval-augmented language models (RaLMs) struggle with knowledge that has not been trained on or recently updated. Consequently, we introduce a novel retrieval-interactive language model framework, where the language model evaluates and reflects on its answers for further re-retrieval. Our exhaustive experiments demonstrate that our training-free framework significantly improves upon existing methods, performing comparably to or even surpassing continuously trained language models.</abstract>
      <url hash="7229f22d">2024.acl-long.181</url>
      <bibkey>ko-etal-2024-growover</bibkey>
      <doi>10.18653/v1/2024.acl-long.181</doi>
    </paper>
    <paper id="182">
      <title>Attribute First, then Generate: Locally-attributable Grounded Text Generation</title>
      <author><first>Aviv</first><last>Slobodkin</last></author>
      <author><first>Eran</first><last>Hirsch</last></author>
      <author><first>Arie</first><last>Cattan</last></author>
      <author><first>Tal</first><last>Schuster</last><affiliation>Google DeepMind and Google</affiliation></author>
      <author><first>Ido</first><last>Dagan</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>3309-3344</pages>
      <abstract>Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable text generation approach, prioritizing concise attributions. Our method, named “Attribute First, then Generate“, breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments (“select first“) and then conditioning the generation process on them (“then generate“), we ensure these segments also act as the output’s fine-grained attributions (“select“ becomes “attribute“). Tested on Multi-document Summarization and Long-form Question-answering, our method not only yields more concise citations than the baselines but also maintains - and in some cases enhances - both generation quality and attribution accuracy. Furthermore, it significantly reduces the time required for fact verification by human assessors.</abstract>
      <url hash="8b685acf">2024.acl-long.182</url>
      <bibkey>slobodkin-etal-2024-attribute</bibkey>
      <doi>10.18653/v1/2024.acl-long.182</doi>
    </paper>
    <paper id="183">
      <title><fixed-case>T</fixed-case>2<fixed-case>S</fixed-case>-<fixed-case>GPT</fixed-case>: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text</title>
      <author><first>Aoxiong</first><last>Yin</last><affiliation>Microsoft and Zhejiang University</affiliation></author>
      <author><first>Haoyuan</first><last>Li</last></author>
      <author><first>Kai</first><last>Shen</last></author>
      <author><first>Siliang</first><last>Tang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yueting</first><last>Zhuang</last></author>
      <pages>3345-3356</pages>
      <abstract>In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts. Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.</abstract>
      <url hash="9742924b">2024.acl-long.183</url>
      <bibkey>yin-etal-2024-t2s</bibkey>
      <doi>10.18653/v1/2024.acl-long.183</doi>
    </paper>
    <paper id="184">
      <title><fixed-case>O</fixed-case>cean<fixed-case>GPT</fixed-case>: A Large Language Model for Ocean Science Tasks</title>
      <author><first>Zhen</first><last>Bi</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yida</first><last>Xue</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yixin</first><last>Ou</last></author>
      <author><first>Daxiong</first><last>Ji</last></author>
      <author><first>Guozhou</first><last>Zheng</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3357-3372</pages>
      <abstract>Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet’s surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, OceanGPT not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.</abstract>
      <url hash="5e3199aa">2024.acl-long.184</url>
      <bibkey>bi-etal-2024-oceangpt</bibkey>
      <doi>10.18653/v1/2024.acl-long.184</doi>
    </paper>
    <paper id="185">
      <title>Beyond Memorization: The Challenge of Random Memory Access in Language Models</title>
      <author><first>Tongyao</first><last>Zhu</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Qian</first><last>Liu</last><affiliation>Sea AI Lab</affiliation></author>
      <author><first>Liang</first><last>Pang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhengbao</first><last>Jiang</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Min</first><last>Lin</last><affiliation>Sea AI Lab</affiliation></author>
      <pages>3373-3388</pages>
      <abstract>Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks.However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering. The code to reproduce our experiments can be found at https://github.com/sail-sg/lm-random-memory-access.</abstract>
      <url hash="bd73c41b">2024.acl-long.185</url>
      <bibkey>zhu-etal-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.acl-long.185</doi>
    </paper>
    <paper id="186">
      <title><fixed-case>BIPED</fixed-case>: Pedagogically Informed Tutoring System for <fixed-case>ESL</fixed-case> Education</title>
      <author><first>Soonwoo</first><last>Kwon</last><affiliation>Twelvelabs</affiliation></author>
      <author><first>Sojung</first><last>Kim</last></author>
      <author><first>Minju</first><last>Park</last></author>
      <author><first>Seunghyun</first><last>Lee</last><affiliation>Riiid</affiliation></author>
      <author><first>Kyuseok</first><last>Kim</last><affiliation>Riiid AI Research</affiliation></author>
      <pages>3389-3414</pages>
      <abstract>Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English. Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies. To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human English tutoring interactions. Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset. Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies.</abstract>
      <url hash="6fe91056">2024.acl-long.186</url>
      <bibkey>kwon-etal-2024-biped</bibkey>
      <doi>10.18653/v1/2024.acl-long.186</doi>
    </paper>
    <paper id="187">
      <title>Timeline-based Sentence Decomposition with In Context Learning for Temporal Fact Extraction</title>
      <author><first>Jianhao</first><last>Chen</last></author>
      <author><first>Haoyuan</first><last>Ouyang</last></author>
      <author><first>Junyang</first><last>Ren</last></author>
      <author><first>Wentao</first><last>Ding</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Wei</first><last>Hu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yuzhong</first><last>Qu</last><affiliation>Nanjing University</affiliation></author>
      <pages>3415-3432</pages>
      <abstract>Facts extraction is pivotal for constructing knowledge graphs. Recently, the increasing demand for temporal facts in downstream tasks has led to the emergence of the task of temporal fact extraction. In this paper, we specifically address the extraction of temporal facts from natural language text. Previous studies fail to handle the challenge of establishing time-to-fact correspondences in complex sentences. To overcome this hurdle, we propose a timeline-based sentence decomposition strategy using large language models (LLMs) with in-context learning, ensuring a fine-grained understanding of the timeline associated with various facts. In addition, we evaluate the performance of LLMs for direct temporal fact extraction and get unsatisfactory results. To this end, we introduce TSDRE, a method that incorporates the decomposition capabilities of LLMs into the traditional fine-tuning of smaller pre-trained language models (PLMs). To support the evaluation, we construct ComplexTRED, a complex temporal fact extraction dataset. Our experiments show that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and ComplexTRED datasets.</abstract>
      <url hash="0821e3e1">2024.acl-long.187</url>
      <bibkey>chen-etal-2024-timeline</bibkey>
      <doi>10.18653/v1/2024.acl-long.187</doi>
    </paper>
    <paper id="188">
      <title>Collaboration or Corporate Capture? Quantifying <fixed-case>NLP</fixed-case>’s Reliance on Industry Artifacts and Contributions</title>
      <author><first>Will</first><last>Aitken</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Mohamed</first><last>Abdalla</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Karen</first><last>Rudie</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Catherine</first><last>Stinson</last><affiliation>Queen’s University</affiliation></author>
      <pages>3433-3448</pages>
      <abstract>Impressive performance of pre-trained models has garnered public attention and made news headlines in recent years. Almost always, these models are produced by or in collaboration with industry. Using them is critical for competing on natural language processing (NLP) benchmarks and correspondingly to stay relevant in NLP research. We surveyed 100 papers published at EMNLP 2022 to determine the degree to which researchers rely on industry models, other artifacts, and contributions to publish in prestigious NLP venues and found that the ratio of their citation is at least three times greater than what would be expected. Our work serves as a scaffold to enable future researchers to more accurately address whether: 1) Collaboration with industry is still collaboration in the absence of an alternative or 2) if NLP inquiry has been captured by the motivations and research direction of private corporations.</abstract>
      <url hash="8870050f">2024.acl-long.188</url>
      <bibkey>aitken-etal-2024-collaboration</bibkey>
      <doi>10.18653/v1/2024.acl-long.188</doi>
    </paper>
    <paper id="189">
      <title>Prompt Expansion for Adaptive Text-to-Image Generation</title>
      <author><first>Siddhartha</first><last>Datta</last></author>
      <author><first>Alexander</first><last>Ku</last><affiliation>Princeton University and Google</affiliation></author>
      <author><first>Deepak</first><last>Ramachandran</last><affiliation>Google</affiliation></author>
      <author><first>Peter</first><last>Anderson</last><affiliation>Balyasny Asset Management</affiliation></author>
      <pages>3449-3476</pages>
      <abstract>Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes the Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, they generate a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.</abstract>
      <url hash="43ff7c68">2024.acl-long.189</url>
      <bibkey>datta-etal-2024-prompt</bibkey>
      <doi>10.18653/v1/2024.acl-long.189</doi>
    </paper>
    <paper id="190">
      <title>Progressively Modality Freezing for Multi-Modal Entity Alignment</title>
      <author><first>Yani</first><last>Huang</last></author>
      <author><first>Xuefeng</first><last>Zhang</last><affiliation>Beijing University of Aeronautics and Astronautics, Tsinghua University</affiliation></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Junfan</first><last>Chen</last></author>
      <author><first>Jaein</first><last>Kim</last></author>
      <pages>3477-3489</pages>
      <abstract>Multi-Modal Entity Alignment aims to discover identical entities across heterogeneous knowledge graphs. While recent studies have delved into fusion paradigms to represent entities holistically, the elimination of features irrelevant to alignment and modal inconsistencies is overlooked, which are caused by inherent differences in multi-modal features. To address these challenges, we propose a novel strategy of progressive modality freezing, called PMF, that focuses on alignment-relevant features and enhances multi-modal feature fusion. Notably, our approach introduces a pioneering cross-modal association loss to foster modal consistency.Empirical evaluations across nine datasets confirm PMF’s superiority, demonstrating state-of-the-art performance and the rationale for freezing modalities. Our code is available at https://github.com/ninibymilk/PMF-MMEA.</abstract>
      <url hash="942ff870">2024.acl-long.190</url>
      <bibkey>huang-etal-2024-progressively</bibkey>
      <doi>10.18653/v1/2024.acl-long.190</doi>
    </paper>
    <paper id="191">
      <title><fixed-case>L</fixed-case>lama2<fixed-case>V</fixed-case>ec: Unsupervised Adaptation of Large Language Models for Dense Retrieval</title>
      <author><first>Chaofan</first><last>Li</last></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Shitao</first><last>Xiao</last></author>
      <author><first>Yingxia</first><last>Shao</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>3490-3500</pages>
      <abstract>Dense retrieval calls for discriminative embeddings to represent the semantic relationship between query and document. It may benefit from the using of large language models (LLMs), given LLMs’ strong capability on semantic understanding. However, the LLMs are learned by auto-regression, whose working mechanism is completely different from representing whole text as one discriminative embedding. Thus, it is imperative to study how to adapt LLMs properly so that they can be effectively initialized as the backbone encoder for dense retrieval. In this paper, we propose a novel approach, called <b>Llama2Vec</b>, which performs unsupervised adaptation of LLM for its dense retrieval application. Llama2Vec consists of two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression), where the LLM is prompted to <i>reconstruct the input sentence</i> and <i>predict the next sentence</i> based on its text embeddings. Llama2Vec is simple, lightweight, but highly effective. It is used to adapt LLaMA-2-7B on the Wikipedia corpus. With a moderate steps of adaptation, it substantially improves the model’s fine-tuned performances on a variety of dense retrieval benchmarks. Notably, it results in the new state-of-the-art performances on popular benchmarks, such as passage and document retrieval on MSMARCO, and zero-shot retrieval on BEIR. The model and source code will be made publicly available to facilitate the future research. Our model is available at https://github.com/FlagOpen/FlagEmbedding.</abstract>
      <url hash="e0092648">2024.acl-long.191</url>
      <bibkey>li-etal-2024-llama2vec</bibkey>
      <doi>10.18653/v1/2024.acl-long.191</doi>
    </paper>
    <paper id="192">
      <title>Democratizing <fixed-case>LLM</fixed-case>s for Low-Resource Languages by Leveraging their <fixed-case>E</fixed-case>nglish Dominant Abilities with Linguistically-Diverse Prompts</title>
      <author><first>Xuan-Phi</first><last>Nguyen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Mahani</first><last>Aljunied</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Alibaba Group</affiliation></author>
      <pages>3501-3516</pages>
      <abstract>Large language models (LLMs) are known to effectively perform tasks by simply observing few exemplars. However, in low-resource languages, obtaining such hand-picked exemplars can still be challenging, where unsupervised techniques may be necessary. Moreover, competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance. To elicit LLMs’ ability onto low-resource languages without any supervised data, we propose to assemble synthetic exemplars from a diverse set of high-resource languages to prompt the LLMs to translate from any language into English. These prompts are then used to create intra-lingual exemplars to perform tasks in the target languages. Our unsupervised prompting method performs on par with supervised few-shot learning in LLMs of different sizes for translations between English and 13 Indic and 21 African low-resource languages. We also show that fine-tuning a 7B model on data generated from our method helps it perform competitively with a 175B model. In non-English translation tasks, our method even outperforms supervised prompting by up to 3 chrF++ in many low-resource languages. When evaluated on zero-shot multilingual summarization, our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is also favored by GPT-4.</abstract>
      <url hash="fe6a2cbc">2024.acl-long.192</url>
      <bibkey>nguyen-etal-2024-democratizing</bibkey>
      <doi>10.18653/v1/2024.acl-long.192</doi>
    </paper>
    <paper id="193">
      <title>Metaphor Understanding Challenge Dataset for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Xiaoyu</first><last>Tong</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <author><first>Rochelle</first><last>Choenni</last></author>
      <author><first>Martha</first><last>Lewis</last><affiliation>University of Bristol and University of Bristol</affiliation></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>3517-3536</pages>
      <abstract>Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical reasoning and categorisation, and are deeply rooted in everyday communication. Metaphor understanding is therefore an essential task for large language models (LLMs). We release the Metaphor Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor understanding capabilities of LLMs. The dataset provides over 10k paraphrases for sentences containing metaphor use, as well as 1.5k instances containing inapt paraphrases. The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity. All apt and inapt paraphrases were manually annotated. The metaphorical sentences cover natural metaphor uses across 4 genres (academic, news, fiction, and conversation), and they exhibit different levels of novelty. Experiments with LLaMA and GPT-3.5 demonstrate that MUNCH presents a challenging task for LLMs. The dataset is freely accessible at https://github.com/xiaoyuisrain/metaphor-understanding-challenge.</abstract>
      <url hash="36d6cb28">2024.acl-long.193</url>
      <bibkey>tong-etal-2024-metaphor</bibkey>
      <doi>10.18653/v1/2024.acl-long.193</doi>
    </paper>
    <paper id="194">
      <title>A Multi-Task Embedder For Retrieval Augmented <fixed-case>LLM</fixed-case>s</title>
      <author><first>Peitian</first><last>Zhang</last></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Shitao</first><last>Xiao</last></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jian-Yun</first><last>Nie</last><affiliation>University of Montreal</affiliation></author>
      <pages>3537-3553</pages>
      <abstract>LLMs confront inherent limitations in terms of its knowledge, memory, and action. The retrieval augmentation stands as a vital mechanism to address these limitations, which brings in useful information from external sources to augment the LLM. However, existing retrieval methods encounter two pressing issues. On one hand, the general retrievers are not properly optimized for retrieval augmentation hence exhibit limited effectiveness; on the other hand, the task-specific retrievers excel in the targeted retrieval augmentation scenario, while lack the versatility to handle diverse scenarios. In this work, we propose LLM-Embedder for the unified support of diverse retrieval augmentation scenarios. Our method presents three technical contributions. Firstly, we introduce a new reward formulation, namely rank-aware reward. It exploits the ranking position of the desired output among N sampled outputs from the LLM, which leads to fine-grained and robust computation of reward from the LLM’s feedback. Secondly, we design a novel distillation objective, called graded distillation. It incorporates both the absolute value and the relative order of the reward for more sufficient utilization of the LLM’s feedback. Thirdly, we systematically optimize the multi-task learning, which effectively unifies the multiple retrieval functionalities into one model. In our experiment, LLM-Embedder substantially improves the LLM’s performances in various downstream tasks, while introducing superior retrieval augmentation’s effect over both general and task-specifc retrievers. Our data, code, and model have been released at https://github.com/FlagOpen/FlagEmbedding.</abstract>
      <url hash="72fd1e4b">2024.acl-long.194</url>
      <bibkey>zhang-etal-2024-multi-task</bibkey>
      <doi>10.18653/v1/2024.acl-long.194</doi>
    </paper>
    <paper id="195">
      <title>Language Models Don’t Learn the Physical Manifestation of Language</title>
      <author><first>Bruce</first><last>Lee</last></author>
      <author><first>Jaehyuk</first><last>Lim</last></author>
      <pages>3554-3579</pages>
      <abstract>We argue that language-only models don’t learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test.These tasks highlight a fundamental gap between human linguistic understanding and the sensory-deprived linguistic understanding of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -&gt; LLaMA 2 70B) has no significant effect on H-Test performance. We bring in the philosophical case of Mary, who learns about the world in a sensory-deprived environment as a useful conceptual framework to understand how language-only models learn about the world (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limitations of linguistic knowledge acquired in the absence of sensory experience. Our code and data are available at &lt;github.com/brucewlee/h-test&gt;.</abstract>
      <url hash="a45e5f06">2024.acl-long.195</url>
      <bibkey>lee-lim-2024-language</bibkey>
      <doi>10.18653/v1/2024.acl-long.195</doi>
    </paper>
    <paper id="196">
      <title>What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection</title>
      <author><first>Shangbin</first><last>Feng</last><affiliation>University of Washington</affiliation></author>
      <author><first>Herun</first><last>Wan</last></author>
      <author><first>Ningnan</first><last>Wang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Zhaoxuan</first><last>Tan</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Minnan</first><last>Luo</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>3580-3601</pages>
      <abstract>Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems.</abstract>
      <url hash="ec3267ec">2024.acl-long.196</url>
      <bibkey>feng-etal-2024-bot</bibkey>
      <doi>10.18653/v1/2024.acl-long.196</doi>
    </paper>
    <paper id="197">
      <title>Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives</title>
      <author><first>Wenqi</first><last>Zhang</last></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Linjuan</first><last>Wu</last></author>
      <author><first>Qiuying</first><last>Peng</last><affiliation>OPPO Research Institute</affiliation></author>
      <author><first>Jun</first><last>Wang</last><affiliation>OPPO Research Institute</affiliation></author>
      <author><first>Yueting</first><last>Zhuang</last></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3602-3622</pages>
      <abstract>The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM’s response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM’s intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can catalyze more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.</abstract>
      <url hash="e15ebd95">2024.acl-long.197</url>
      <bibkey>zhang-etal-2024-self-contrast</bibkey>
      <doi>10.18653/v1/2024.acl-long.197</doi>
    </paper>
    <paper id="198">
      <title>Relying on the Unreliable: The Impact of Language Models’ Reluctance to Express Uncertainty</title>
      <author><first>Kaitlyn</first><last>Zhou</last><affiliation>Stanford University</affiliation></author>
      <author><first>Jena</first><last>Hwang</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>3623-3643</pages>
      <abstract>As natural language becomes the default interface for human-AI interaction, there is a need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence in responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are reluctant to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (an average of 47%) among confident responses. We test the risks of LM overconfidence by conducting human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in post training alignment and find that humans are biased against texts with uncertainty. Our work highlights new safety harms facing human-LM interactions and proposes design recommendations and mitigating strategies moving forward.</abstract>
      <url hash="a90d658f">2024.acl-long.198</url>
      <bibkey>zhou-etal-2024-relying</bibkey>
      <doi>10.18653/v1/2024.acl-long.198</doi>
    </paper>
    <paper id="199">
      <title>Unity in Diversity: Collaborative Pre-training Across Multimodal Medical Sources</title>
      <author><first>Xiaochen</first><last>Wang</last></author>
      <author><first>Junyu</first><last>Luo</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jiaqi</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Zhong</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Xiaokun</first><last>Zhang</last></author>
      <author><first>Yaqing</first><last>Wang</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Parminder</first><last>Bhatia</last><affiliation>GEHC</affiliation></author>
      <author><first>Cao</first><last>Xiao</last><affiliation>GE Healthcare</affiliation></author>
      <author><first>Fenglong</first><last>Ma</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>3644-3656</pages>
      <abstract>Although pre-training has become a prevalent approach for addressing various biomedical tasks, the current efficacy of pre-trained models is hindered by their reliance on a limited scope of medical sources. This limitation results in data scarcity during pre-training and restricts the range of applicable downstream tasks. In response to these challenges, we develop MedCSP, a new pre-training strategy designed to bridge the gap between multimodal medical sources. MedCSP employs modality-level aggregation to unify patient data within individual sources. Additionally, leveraging temporal information and diagnosis history, MedCSP effectively captures explicit and implicit correlations between patients across different sources. To evaluate the proposed strategy, we conduct comprehensive experiments, where the experiments are based on 6 modalities from 2 real-world medical data sources, and MedCSP is evaluated on 4 tasks against 19 baselines, marking an initial yet essential step towards cross-source modeling in the medical domain.</abstract>
      <url hash="536f003d">2024.acl-long.199</url>
      <bibkey>wang-etal-2024-unity</bibkey>
      <doi>10.18653/v1/2024.acl-long.199</doi>
    </paper>
    <paper id="200">
      <title>When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in <fixed-case>NLP</fixed-case></title>
      <author><first>Sara</first><last>Papi</last></author>
      <author><first>Marco</first><last>Gaido</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Andrea</first><last>Pilzer</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>3657-3672</pages>
      <abstract>Despite its crucial role in research experiments, code correctness is often presumed solely based on the perceived quality of results. This assumption, however, comes with the risk of erroneous outcomes and, in turn, potentially misleading findings. To mitigate this risk, we posit that the current focus on reproducibility should go hand in hand with the emphasis on software quality. We support our arguments with a case study in which we identify and fix three bugs in widely used implementations of the state-of-the-art Conformer architecture. Through experiments on speech recognition and translation in various languages, we demonstrate that the presence of bugs does not prevent the achievement of good and reproducible results, which however can lead to incorrect conclusions that potentially misguide future research. As countermeasures, we release pangoliNN, a library dedicated to testing neural models, and propose a Code-quality Checklist, with the goal of promoting coding best practices and improving software quality within the NLP community.</abstract>
      <url hash="f0f9d975">2024.acl-long.200</url>
      <bibkey>papi-etal-2024-good</bibkey>
      <doi>10.18653/v1/2024.acl-long.200</doi>
    </paper>
    <paper id="201">
      <title><fixed-case>SBAAM</fixed-case>! Eliminating Transcript Dependency in Automatic Subtitling</title>
      <author><first>Marco</first><last>Gaido</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Sara</first><last>Papi</last></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Mauro</first><last>Cettolo</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Luisa</first><last>Bentivogli</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>3673-3691</pages>
      <abstract>Subtitling plays a crucial role in enhancing the accessibility of audiovisual content and encompasses three primary subtasks: translating spoken dialogue, segmenting translations into concise textual units, and estimating timestamps that govern their on-screen duration. Past attempts to automate this process rely, to varying degrees, on automatic transcripts, employed diversely for the three subtasks. In response to the acknowledged limitations associated with this reliance on transcripts, recent research has shifted towards transcription-free solutions for translation and segmentation, leaving the direct generation of timestamps as uncharted territory. To fill this gap, we introduce the first direct model capable of producing automatic subtitles, entirely eliminating any dependence on intermediate transcripts also for timestamp prediction. Experimental results, backed by manual evaluation, showcase our solution’s new state-of-the-art performance across multiple language pairs and diverse conditions.</abstract>
      <url hash="c10971bc">2024.acl-long.201</url>
      <bibkey>gaido-etal-2024-sbaam</bibkey>
      <doi>10.18653/v1/2024.acl-long.201</doi>
    </paper>
    <paper id="202">
      <title><fixed-case>S</fixed-case>tream<fixed-case>A</fixed-case>tt: Direct Streaming Speech-to-Text Translation with Attention-based Audio History Selection</title>
      <author><first>Sara</first><last>Papi</last></author>
      <author><first>Marco</first><last>Gaido</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Luisa</first><last>Bentivogli</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>3692-3707</pages>
      <abstract>Streaming speech-to-text translation (StreamST) is the task of automatically translating speech while incrementally receiving an audio stream. Unlike simultaneous ST (SimulST), which deals with pre-segmented speech, StreamST faces the challenges of handling continuous and unbounded audio streams. This requires additional decisions about what to retain of the previous history, which is impractical to keep entirely due to latency and computational constraints. Despite the real-world demand for real-time ST, research on streaming translation remains limited, with existing works solely focusing on SimulST. To fill this gap, we introduce StreamAtt, the first StreamST policy, and propose StreamLAAL, the first StreamST latency metric designed to be comparable with existing metrics for SimulST. Extensive experiments across all 8 languages of MuST-C v1.0 show the effectiveness of StreamAtt compared to a naive streaming baseline and the related state-of-the-art SimulST policy, providing a first step in StreamST research.</abstract>
      <url hash="1fd0e72b">2024.acl-long.202</url>
      <bibkey>papi-etal-2024-streamatt</bibkey>
      <doi>10.18653/v1/2024.acl-long.202</doi>
    </paper>
    <paper id="203">
      <title><fixed-case>ARL</fixed-case>2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling</title>
      <author><first>LingXi</first><last>Zhang</last></author>
      <author><first>Yue</first><last>Yu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Kuan</first><last>Wang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>3708-3719</pages>
      <abstract>Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to separate training processes and the inherent black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score adaptive relevance evidence, enabling the retriever to learn from robust LLM supervision. Furthermore, ARL2 incorporates a self-training strategy to minimize the cost of API calls. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities.</abstract>
      <url hash="0a32c1ad">2024.acl-long.203</url>
      <bibkey>zhang-etal-2024-arl2</bibkey>
      <doi>10.18653/v1/2024.acl-long.203</doi>
    </paper>
    <paper id="204">
      <title>Crayon: Customized On-Device <fixed-case>LLM</fixed-case> via Instant Adapter Blending and Edge-Server Hybrid Inference</title>
      <author><first>Jihwan</first><last>Bang</last><affiliation>Qualcomm Inc, QualComm and KAIST</affiliation></author>
      <author><first>Juntae</first><last>Lee</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Kyuhong</first><last>Shim</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Seunghan</first><last>Yang</last><affiliation>Qualcomm AI Research</affiliation></author>
      <author><first>Simyung</first><last>Chang</last><affiliation>QualComm AI Research</affiliation></author>
      <pages>3720-3731</pages>
      <abstract>The customization of large language models (LLMs) for user-specified tasks gets important. However, maintaining all the customized LLMs on cloud servers incurs substantial memory and computational overheads, and uploading user data can also lead to privacy concerns. On-device LLMs can offer a promising solution by mitigating these issues. Yet, the performance of on-device LLMs is inherently constrained by the limitations of small-scaled models. To overcome these restrictions, we first propose Crayon, a novel approach for on-device LLM customization. Crayon begins by constructing a pool of diverse base adapters, and then we instantly blend them into a customized adapter without extra training. In addition, we develop a device-server hybrid inference strategy, which deftly allocates more demanding queries or non-customized tasks to a larger, more capable LLM on a server. This ensures optimal performance without sacrificing the benefits of on-device customization. We carefully craft a novel benchmark from multiple question-answer datasets, and show the efficacy of our method in the LLM customization.</abstract>
      <url hash="d44305b4">2024.acl-long.204</url>
      <bibkey>bang-etal-2024-crayon</bibkey>
      <doi>10.18653/v1/2024.acl-long.204</doi>
    </paper>
    <paper id="205">
      <title><fixed-case>FLEUR</fixed-case>: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model</title>
      <author><first>Yebin</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Imseong</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Myungjoo</first><last>Kang</last><affiliation>Seoul National University</affiliation></author>
      <pages>3732-3746</pages>
      <abstract>Most existing image captioning evaluation metrics focus on assigning a single numerical score to a caption by comparing it with reference captions. However, these methods do not provide an explanation for the assigned score. Moreover, reference captions are expensive to acquire. In this paper, we propose FLEUR, an explainable reference-free metric to introduce explainability into image captioning evaluation metrics. By leveraging a large multimodal model, FLEUR can evaluate the caption against the image without the need for reference captions, and provide the explanation for the assigned score. We introduce score smoothing to align as closely as possible with human judgment and to be robust to user-defined grading criteria. FLEUR achieves high correlations with human judgment across various image captioning evaluation benchmarks and reaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S within the domain of reference-free evaluation metrics. Our source code and results are publicly available at: https://github.com/Yebin46/FLEUR.</abstract>
      <url hash="e0e967bd">2024.acl-long.205</url>
      <bibkey>lee-etal-2024-fleur</bibkey>
      <doi>10.18653/v1/2024.acl-long.205</doi>
    </paper>
    <paper id="206">
      <title><fixed-case>M</fixed-case>ental<fixed-case>M</fixed-case>anip: A Dataset For Fine-grained Analysis of Mental Manipulation in Conversations</title>
      <author><first>Yuxin</first><last>Wang</last></author>
      <author><first>Ivory</first><last>Yang</last></author>
      <author><first>Saeed</first><last>Hassanpour</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>3747-3764</pages>
      <abstract>Mental manipulation, a significant form of abuse in interpersonal conversations, presents a challenge to identify due to its context-dependent and often subtle nature. The detection of manipulative language is essential for protecting potential victims, yet the field of Natural Language Processing (NLP) currently faces a scarcity of resources and research on this topic. Our study addresses this gap by introducing a new dataset, named <tex-math>\textsc{MentalManip}</tex-math>, which consists of 4,000 annotated fictional dialogues. This dataset enables a comprehensive analysis of mental manipulation, pinpointing both the techniques utilized for manipulation and the vulnerabilities targeted in victims. Our research further explores the effectiveness of leading-edge models in recognizing manipulative dialogue and its components through a series of experiments with various configurations. The results demonstrate that these models inadequately identify and categorize manipulative content. Attempts to improve their performance by fine-tuning with existing datasets on mental health and toxicity have not overcome these limitations. We anticipate that <tex-math>\textsc{MentalManip}</tex-math> will stimulate further research, leading to progress in both understanding and mitigating the impact of mental manipulation in conversations.</abstract>
      <url hash="c02004c4">2024.acl-long.206</url>
      <bibkey>wang-etal-2024-mentalmanip</bibkey>
      <doi>10.18653/v1/2024.acl-long.206</doi>
    </paper>
    <paper id="207">
      <title><fixed-case>MPC</fixed-case>oder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning</title>
      <author><first>Zhenlong</first><last>Dai</last></author>
      <author><first>Chang</first><last>Yao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>WenKang</first><last>Han</last></author>
      <author><first>Yuanying</first><last>Yuanying</last><affiliation>Zhejiang Police College</affiliation></author>
      <author><first>Zhipeng</first><last>Gao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jingyuan</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3765-3780</pages>
      <abstract>Large Language Models (LLMs) have demonstrated great potential for assisting developers in their daily development. However, most research focuses on generating correct code, how to use LLMs to generate personalized code has seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user Personalized Code Generator) to generate personalized code for multiple users. To better learn coding style features, we utilize explicit coding style residual learning to capture the syntax code style standards and implicit style learning to capture the semantic code style conventions. We train a multi-user style adapter to better differentiate the implicit feature representations of different users through contrastive learning, ultimately enabling personalized code generation for multiple users. We further propose a novel evaluation metric for estimating similarities between codes of different coding styles. The experimental results show the effectiveness of our approach for this novel task.</abstract>
      <url hash="12b3d4e9">2024.acl-long.207</url>
      <bibkey>dai-etal-2024-mpcoder</bibkey>
      <doi>10.18653/v1/2024.acl-long.207</doi>
    </paper>
    <paper id="208">
      <title><fixed-case>D</fixed-case>ata<fixed-case>D</fixed-case>reamer: A Tool for Synthetic Data Generation and Reproducible <fixed-case>LLM</fixed-case> Workflows</title>
      <author><first>Ajay</first><last>Patel</last></author>
      <author><first>Colin</first><last>Raffel</last><affiliation>Department of Computer Science, University of Toronto and Hugging Face</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>Allen Institute for Artificial Intelligence and University of Pennsylvania</affiliation></author>
      <pages>3781-3799</pages>
      <abstract>Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this ACL 2024 theme track paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at: https://github.com/datadreamer-dev/DataDreamer.</abstract>
      <url hash="f88d7a93">2024.acl-long.208</url>
      <bibkey>patel-etal-2024-datadreamer</bibkey>
      <doi>10.18653/v1/2024.acl-long.208</doi>
    </paper>
    <paper id="209">
      <title>Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective</title>
      <author><first>Chenze</first><last>Shao</last><affiliation>Tencent Inc</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>3800-3814</pages>
      <abstract>Neural Machine Translation (NMT) has made remarkable progress over the past years. However, under-translation and over-translation remain two challenging problems in state-of-the-art NMT systems. In this work, we conduct an in-depth analysis on the underlying cause of under-translation in NMT, providing an explanation from the perspective of decoding objective. To optimize the beam search objective, the model tends to overlook words it is less confident about, leading to the under-translation phenomenon. Correspondingly, the model’s confidence in predicting the End Of Sentence (EOS) diminishes when under-translation occurs, serving as a mild penalty for under-translated candidates. Building upon this analysis, we propose employing the confidence of predicting EOS as a detector for under-translation, and strengthening the confidence-based penalty to penalize candidates with a high risk of under-translation.Experiments on both synthetic and real-world data show that our method can accurately detect and rectify under-translated outputs, with minor impact on other correct translations.</abstract>
      <url hash="6fcf799b">2024.acl-long.209</url>
      <bibkey>shao-etal-2024-understanding</bibkey>
      <doi>10.18653/v1/2024.acl-long.209</doi>
    </paper>
    <paper id="210">
      <title>Identifying while Learning for Document Event Causality Identification</title>
      <author><first>Cheng</first><last>Liu</last></author>
      <author><first>Wei</first><last>Xiang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Bang</first><last>Wang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>3815-3827</pages>
      <abstract>Event Causality Identification (ECI) aims to detect whether there exists a causal relation between two events in a document. Existing studies adopt a kind of *identifying after learning* paradigm, where events’ representations are first learned and then used for the identification. Furthermore, they mainly focus on the causality existence, but ignoring causal direction. In this paper, we take care of the causal direction and propose a new *identifying while learning* mode for the ECI task. We argue that a few causal relations can be easily identified with high confidence, and the directionality and structure of these identified causalities can be utilized to update events’ representations for boosting next round of causality identification. To this end, this paper designs an *iterative learning and identifying framework*: In each iteration, we construct an event causality graph, on which events’ causal structure representations are updated for boosting causal identification. Experiments on two public datasets show that our approach outperforms the state-of-the-art algorithms in both evaluations for causality existence identification and direction identification.</abstract>
      <url hash="3c603638">2024.acl-long.210</url>
      <bibkey>liu-etal-2024-identifying</bibkey>
      <doi>10.18653/v1/2024.acl-long.210</doi>
    </paper>
    <paper id="211">
      <title><fixed-case>O</fixed-case>lympiad<fixed-case>B</fixed-case>ench: A Challenging Benchmark for Promoting <fixed-case>AGI</fixed-case> with Olympiad-Level Bilingual Multimodal Scientific Problems</title>
      <author><first>Chaoqun</first><last>He</last></author>
      <author><first>Renjie</first><last>Luo</last></author>
      <author><first>Yuzhuo</first><last>Bai</last></author>
      <author><first>Shengding</first><last>Hu</last></author>
      <author><first>Zhen</first><last>Thai</last></author>
      <author><first>Junhao</first><last>Shen</last></author>
      <author><first>Jinyi</first><last>Hu</last></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yujie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuxiang</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Liu</last></author>
      <author><first>Lei</first><last>Qi</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>3828-3850</pages>
      <abstract>Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97% on OlympiadBench, with a mere 10.74% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at <url>https://github.com/OpenBMB/OlympiadBench</url></abstract>
      <url hash="2282cd03">2024.acl-long.211</url>
      <bibkey>he-etal-2024-olympiadbench</bibkey>
      <doi>10.18653/v1/2024.acl-long.211</doi>
    </paper>
    <paper id="212">
      <title>Insert or Attach: Taxonomy Completion via Box Embedding</title>
      <author><first>Wei</first><last>Xue</last></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Wenqi</first><last>Ren</last><affiliation>Hikvision Research Institute</affiliation></author>
      <author><first>Jietian</first><last>Guo</last><affiliation>Hikvision Research Institute</affiliation></author>
      <author><first>Shiliang</first><last>Pu</last></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3851-3863</pages>
      <abstract>Taxonomy completion, enriching existing taxonomies by inserting new concepts as parents or attaching them as children, has gained significant interest. Previous approaches embed concepts as vectors in Euclidean space, which makes it difficult to model asymmetric relations in taxonomy. In addition, they introduce pseudo-leaves to convert attachment cases into insertion cases, leading to an incorrect bias in network learning dominated by numerous pseudo-leaves. Addressing these, our framework, TaxBox, leverages box containment and center closeness to design two specialized geometric scorers within the box embedding space. These scorers are tailored for insertion and attachment operations and can effectively capture intrinsic relationships between concepts by optimizing on a granular box constraint loss. We employ a dynamic ranking loss mechanism to balance the scores from these scorers, allowing adaptive adjustments of insertion and attachment scores. Experiments on four real-world datasets show that TaxBox significantly outperforms previous methods, yielding substantial improvements over prior methods in real-world datasets, with average performance boosts of 6.7%, 34.9%, and 51.4% in MRR, Hit@1, and Prec@1, respectively.</abstract>
      <url hash="02f8a02f">2024.acl-long.212</url>
      <bibkey>xue-etal-2024-insert</bibkey>
      <doi>10.18653/v1/2024.acl-long.212</doi>
    </paper>
    <paper id="213">
      <title>Semiparametric Token-Sequence Co-Supervision</title>
      <author><first>Hyunji</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Doyoung</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Jihoon</first><last>Jun</last></author>
      <author><first>Se June</first><last>Joo</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Joel</first><last>Jang</last></author>
      <author><first>Kyoung-Woon</first><last>On</last><affiliation>Kakao</affiliation></author>
      <author><first>Minjoon</first><last>Seo</last><affiliation>Twelve Labs and Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>3864-3882</pages>
      <abstract>In this work, we introduce a semiparametric token-sequence co-supervision training method. It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space. The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding. Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently. Analysis suggests that this co-supervision encourages a broader generalization capability across the model. Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametric sequence embedding space, a new space established by another language model.</abstract>
      <url hash="700e2da8">2024.acl-long.213</url>
      <bibkey>lee-etal-2024-semiparametric</bibkey>
      <doi>10.18653/v1/2024.acl-long.213</doi>
    </paper>
    <paper id="214">
      <title>Instruction Fusion: Advancing Prompt Evolution through Hybridization</title>
      <author><first>Weidong</first><last>Guo</last><affiliation>Tencent</affiliation></author>
      <author><first>Jiuding</first><last>Yang</last></author>
      <author><first>Kaitong</first><last>Yang</last></author>
      <author><first>Xiangyang</first><last>Li</last></author>
      <author><first>Zhuwei</first><last>Rao</last></author>
      <author><first>Yu</first><last>Xu</last><affiliation>Tencent</affiliation></author>
      <author><first>Di</first><last>Niu</last><affiliation>University of Alberta and University of Alberta</affiliation></author>
      <pages>3883-3893</pages>
      <abstract>The fine-tuning of Large Language Models (LLMs) specialized in code generation has seen notable advancements through the use of open-domain coding queries. Despite the successes, existing methodologies like Evol-Instruct encounter performance limitations, impeding further enhancements in code generation tasks. This paper examines the constraints of existing prompt evolution techniques and introduces a novel approach, Instruction Fusion (IF). IF innovatively combines two distinct prompts through a hybridization process, thereby enhancing the evolution of training prompts for code LLMs. Our experimental results reveal that the proposed novel method effectively addresses the shortcomings of prior methods, significantly improving the performance of Code LLMs across five code generation benchmarks, namely HumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the effectiveness of Instruction Fusion in advancing the capabilities of LLMs in code generation.</abstract>
      <url hash="2bf29198">2024.acl-long.214</url>
      <bibkey>guo-etal-2024-instruction</bibkey>
      <doi>10.18653/v1/2024.acl-long.214</doi>
    </paper>
    <paper id="215">
      <title><fixed-case>T</fixed-case>ime<fixed-case>A</fixed-case>rena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation</title>
      <author><first>Yikai</first><last>Zhang</last></author>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Caiyu</first><last>Hu</last></author>
      <author><first>Kyle</first><last>Richardson</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jiangjie</first><last>Chen</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>3894-3916</pages>
      <abstract>Despite remarkable advancements in emulating human-like behavior through Large Language Models (LLMs), current textual simulations do not adequately address the notion of time. To this end, we introduce TimeArena, a novel textual simulated environment that incorporates complex temporal dynamics and constraints that better reflect real-life planning scenarios. In TimeArena, agents are asked to complete multiple tasks as soon as possible, allowing for parallel processing to save time. We implement the dependency between actions, the time duration for each action, and the occupancy of the agent and the objects in the environment. TimeArena grounds to 30 real-world tasks in cooking, household activity, and laboratory work. We conduct extensive experiments with various LLMs using TimeArena. Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the development of language agents.</abstract>
      <url hash="87b58e30">2024.acl-long.215</url>
      <bibkey>zhang-etal-2024-timearena</bibkey>
      <doi>10.18653/v1/2024.acl-long.215</doi>
    </paper>
    <paper id="216">
      <title>Exploring Memorization in Fine-tuned Language Models</title>
      <author><first>Shenglai</first><last>Zeng</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Yaxin</first><last>Li</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Jie</first><last>Ren</last><affiliation>Baidu and Michigan State University</affiliation></author>
      <author><first>Yiding</first><last>Liu</last><affiliation>Baidu</affiliation></author>
      <author><first>Han</first><last>Xu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Pengfei</first><last>He</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Yue</first><last>Xing</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Shuaiqiang</first><last>Wang</last></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <pages>3917-3948</pages>
      <abstract>Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns. While prior works have studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared to pre-training, fine-tuning typically involves more sensitive data and diverse objectives, thus may bring distinct privacy risks and unique memorization behaviors. In this work, we conduct the first comprehensive analysis to explore language models’ (LMs) memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that memorization presents a strong disparity among different fine-tuning tasks. We provide an intuitive explanation of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention score distribution.</abstract>
      <url hash="8337c113">2024.acl-long.216</url>
      <bibkey>zeng-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.acl-long.216</doi>
    </paper>
    <paper id="217">
      <title>Towards Real-world Scenario: Imbalanced New Intent Discovery</title>
      <author><first>Shun</first><last>Zhang</last><affiliation>Beihang University</affiliation></author>
      <author><first>Yan</first><last>Chaoran</last></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jiaheng</first><last>Liu</last></author>
      <author><first>Ying</first><last>Mo</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Jiaqi</first><last>Bai</last><affiliation>Guangzhou University</affiliation></author>
      <author><first>Tongliang</first><last>Li</last><affiliation>Beijing Information Science and Technology University</affiliation></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>3949-3963</pages>
      <abstract>New Intent Discovery (NID) aims at detecting known and previously undefined categories of user intent by utilizing limited labeled and massive unlabeled data. Most prior works often operate under the unrealistic assumption that the distribution of both familiar and new intent classes is uniform, overlooking the skewed and long-tailed distributions frequently encountered in real-world scenarios. To bridge the gap, our work introduces the imbalanced new intent discovery i-NID task, which seeks to identify familiar and novel intent categories within long-tailed distributions. A new benchmark baNID-Bench comprised of three datasets is created to simulate the real-world long-tail distributions. ImbaNID-Bench ranges from broad cross-domain to specific single-domain intent categories, providing a thorough representation of practical use cases. Besides, a robust baseline model ImbaNID is proposed to achieve cluster-friendly intent representations. It includes three stages: model pre-training, generation of reliable pseudo-labels, and robust representation learning that strengthens the model performance to handle the intricacies of real-world data distributions. Our extensive experiments on previous benchmarks and the newly established benchmark demonstrate the superior performance of ImbaNID in addressing the i-NID task, highlighting its potential as a powerful baseline for uncovering and categorizing user intents in imbalanced and long-tailed distributions.</abstract>
      <url hash="106fcf21">2024.acl-long.217</url>
      <bibkey>zhang-etal-2024-towards-real</bibkey>
      <doi>10.18653/v1/2024.acl-long.217</doi>
    </paper>
    <paper id="218">
      <title><fixed-case>M</fixed-case>4<fixed-case>GT</fixed-case>-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Jonibek</first><last>Mansurov</last></author>
      <author><first>Petar</first><last>Ivanov</last></author>
      <author><first>Jinyan</first><last>Su</last><affiliation>Cornell University</affiliation></author>
      <author><first>Artem</first><last>Shelmanov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Akim</first><last>Tsvigun</last><affiliation>Semrush</affiliation></author>
      <author><first>Osama</first><last>Mohammed Afzal</last></author>
      <author><first>Tarek</first><last>Mahmoud</last></author>
      <author><first>Giovanni</first><last>Puccetti</last><affiliation>CNR</affiliation></author>
      <author><first>Thomas</first><last>Arnold</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Alham</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Amazon</affiliation></author>
      <author><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>3964-3992</pages>
      <abstract>The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multi-domain and multi-generator corpus of MGTs — M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed human-machine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at https://github.com/mbzuai-nlp/M4GT-Bench.</abstract>
      <url hash="e3004271">2024.acl-long.218</url>
      <bibkey>wang-etal-2024-m4gt</bibkey>
      <doi>10.18653/v1/2024.acl-long.218</doi>
    </paper>
    <paper id="219">
      <title>Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue</title>
      <author><first>Jian</first><last>Wang</last></author>
      <author><first>Chak Tou</first><last>Leong</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Jiashuo</first><last>Wang</last></author>
      <author><first>Dongding</first><last>Lin</last></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xiaoyong</first><last>Wei</last><affiliation>Hong Kong Polytechnic University and Sichuan University, China</affiliation></author>
      <pages>3993-4010</pages>
      <abstract>Tuning language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner often leads to unsatisfactory chat consistency for the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. With this in mind, we propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models. The adapters make use of respective utterances round by round in alternating order and they are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our framework performs superior to traditional fine-tuning and harbors the tremendous potential for improving dialogue consistency.</abstract>
      <url hash="65e7f285">2024.acl-long.219</url>
      <bibkey>wang-etal-2024-instruct</bibkey>
      <doi>10.18653/v1/2024.acl-long.219</doi>
    </paper>
    <paper id="220">
      <title><fixed-case>S</fixed-case>oft<fixed-case>D</fixed-case>edup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training</title>
      <author><first>Nan</first><last>He</last></author>
      <author><first>Weichen</first><last>Xiong</last></author>
      <author><first>Hanwen</first><last>Liu</last></author>
      <author><first>Yi</first><last>Liao</last></author>
      <author><first>Lei</first><last>Ding</last></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Guohua</first><last>Tang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xiao</first><last>Han</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yang</first><last>Wei</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>4011-4022</pages>
      <abstract>The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and neglects the varying degrees of duplication. To address this, we propose a soft deduplication method that maintains dataset integrity while selectively reducing the sampling weight of data with high commonness. Central to our approach is the concept of “data commonness”, a metric we introduce to quantify the degree of duplication by measuring the occurrence probabilities of samples using an n-gram model. Empirical analysis shows that this method significantly improves training efficiency, achieving comparable perplexity scores with at least a 26% reduction in required training steps. Additionally, it enhances average few-shot downstream accuracy by 1.77% when trained for an equivalent duration. Importantly, this approach consistently improves performance, even on rigorously deduplicated datasets, indicating its potential to complement existing methods and become a standard pre-training process for LLMs.</abstract>
      <url hash="b827c24b">2024.acl-long.220</url>
      <bibkey>he-etal-2024-softdedup</bibkey>
      <doi>10.18653/v1/2024.acl-long.220</doi>
    </paper>
    <paper id="221">
      <title>Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?</title>
      <author><first>Ning</first><last>Bian</last></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>4023-4043</pages>
      <abstract>Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning. These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities.</abstract>
      <url hash="57febfd5">2024.acl-long.221</url>
      <bibkey>bian-etal-2024-rule</bibkey>
      <doi>10.18653/v1/2024.acl-long.221</doi>
    </paper>
    <paper id="222">
      <title>Learning Global Controller in Latent Space for Parameter-Efficient Fine-Tuning</title>
      <author><first>Zeqi</first><last>Tan</last></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Xiaoxia</first><last>Cheng</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Chang</first><last>Zong</last></author>
      <author><first>Wenqi</first><last>Zhang</last></author>
      <author><first>Jian</first><last>Shao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yueting</first><last>Zhuang</last></author>
      <pages>4044-4055</pages>
      <abstract>While large language models (LLMs) have showcased remarkable prowess in various natural language processing tasks, their training costs are exorbitant. Consequently, a plethora of parameter-efficient fine-tuning methods have emerged to tailor large models for downstream tasks, including low-rank training. Recent approaches either amalgamate existing fine-tuning methods or dynamically adjust rank allocation. Nonetheless, these methods continue to grapple with issues like local optimization, inability to train with full rank and lack of focus on specific tasks. In this paper, we introduce an innovative parameter-efficient method for exploring optimal solutions within latent space. More specifically, we introduce a set of latent units designed to iteratively extract input representations from LLMs, continuously refining informative features that enhance downstream task performance. Due to the small and independent nature of the latent units in relation to input size, this significantly reduces training memory requirements. Additionally, we employ an asymmetric attention mechanism to facilitate bidirectional interaction between latent units and freezed LLM representations, thereby mitigating issues associated with non-full-rank training. Furthermore, we apply distillation over hidden states during the interaction, which guarantees a trimmed number of trainable parameters.Experimental results demonstrate that our approach achieves state-of-the-art performance on a range of natural language understanding, generation and reasoning tasks.</abstract>
      <url hash="c56c290d">2024.acl-long.222</url>
      <bibkey>tan-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.acl-long.222</doi>
    </paper>
    <paper id="223">
      <title><fixed-case>C</fixed-case>a<fixed-case>MML</fixed-case>: Context-Aware Multimodal Learner for Large Models</title>
      <author><first>Yixin</first><last>Chen</last></author>
      <author><first>Shuai</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Boran</first><last>Han</last></author>
      <author><first>Tong</first><last>He</last><affiliation>Amazon</affiliation></author>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>University of Illinois, Urbana Champaign and University of California Berkeley</affiliation></author>
      <pages>4056-4071</pages>
      <abstract>In this work, we introduce Context-Aware MultiModal Learner (CaMML), for tuning large multimodal models (LMMs). CaMML, a lightweight module, is crafted to seamlessly integrate multimodal contextual samples into large models, thereby empowering the model to derive knowledge from analogous, domain-specific, up-to-date information and make grounded inferences. Importantly, CaMML is highly scalable and can efficiently handle lengthy multimodal context examples owing to its hierarchical design. Based on CaMML, we have developed two multimodal models, CaMML-7B and CaMML-13B, that have shown exceptional performance across an array of benchmark datasets for multimodal tasks. Remarkably, CaMML-13B achieves the state-of-the-art performance on over ten widely recognized multimodal benchmark datasets, surpassing LLaVA-1.5 (13B) with a noticeable margin, without integration of any external resources. Moreover, we have conducted extensive ablative studies to inspect the inner workings of CaMML and performed qualitative analyses to showcase its effectiveness in handling real-world challenging cases. Code and models are available at: https://github.com/amazon-science/camml.</abstract>
      <url hash="66df09a6">2024.acl-long.223</url>
      <bibkey>chen-etal-2024-camml</bibkey>
      <doi>10.18653/v1/2024.acl-long.223</doi>
    </paper>
    <paper id="224">
      <title><fixed-case>MAVEN</fixed-case>-<fixed-case>ARG</fixed-case>: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation</title>
      <author><first>Xiaozhi</first><last>Wang</last><affiliation>Department of Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Hao</first><last>Peng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yong</first><last>Guan</last></author>
      <author><first>Kaisheng</first><last>Zeng</last></author>
      <author><first>Jianhui</first><last>Chen</last></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ruobing</first><last>Xie</last><affiliation>Tencent</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>4072-4091</pages>
      <abstract>Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and codes can be obtained from https://github.com/THU-KEG/MAVEN-Argument.</abstract>
      <url hash="d0a6afda">2024.acl-long.224</url>
      <bibkey>wang-etal-2024-maven</bibkey>
      <doi>10.18653/v1/2024.acl-long.224</doi>
    </paper>
    <paper id="225">
      <title><fixed-case>NPH</fixed-case>ard<fixed-case>E</fixed-case>val: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes</title>
      <author><first>Lizhou</first><last>Fan</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Wenyue</first><last>Hua</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Lingyao</first><last>Li</last></author>
      <author><first>Haoyang</first><last>Ling</last></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <pages>4092-4114</pages>
      <abstract>Complex reasoning ability is one of the most important features of Large Language Models (LLMs). Numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, they are inadequate in offering a rigorous evaluation and prone to the risk of overfitting, as these publicly accessible and static benchmarks allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, we introduce a new benchmark NPHardEval. It contains a broad spectrum of 900 algorithmic questions belonging up to the NP-Hard complexity class, offering a rigorous measure of the reasoning ability of LLMs utilizing computational complexity. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https://github.com/casmlab/NPHardEval.</abstract>
      <url hash="2f959910">2024.acl-long.225</url>
      <bibkey>fan-etal-2024-nphardeval</bibkey>
      <doi>10.18653/v1/2024.acl-long.225</doi>
    </paper>
    <paper id="226">
      <title>Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models</title>
      <author><first>Zhiwei</first><last>He</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Binglin</first><last>Zhou</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Hongkun</first><last>Hao</last></author>
      <author><first>Aiwei</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xing</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>4115-4129</pages>
      <abstract>Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of cross-lingual consistency in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks, decreasing the AUCs to a random-guessing level without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose X-SIR as a defense method against CWRA.</abstract>
      <url hash="93425477">2024.acl-long.226</url>
      <bibkey>he-etal-2024-watermarks</bibkey>
      <doi>10.18653/v1/2024.acl-long.226</doi>
    </paper>
    <paper id="227">
      <title>Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors</title>
      <author><first>Alicja</first><last>Chaszczewicz</last></author>
      <author><first>Raj</first><last>Shah</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Ryan</first><last>Louie</last><affiliation>Stanford University</affiliation></author>
      <author><first>Bruce</first><last>Arnow</last><affiliation>Stanford University</affiliation></author>
      <author><first>Robert</first><last>Kraut</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>4130-4161</pages>
      <abstract>Realistic practice and tailored feedback are key processes for training peer counselors with clinical skills. However, existing mechanisms of providing feedback largely rely on human supervision. Peer counselors often lack mechanisms to receive detailed feedback from experienced mentors, making it difficult for them to support the large number of people with mental health issues who use peer counseling. Our work aims to leverage large language models to provide contextualized and multi-level feedback to empower peer counselors, especially novices, at scale. To achieve this, we co-design with a group of senior psychotherapy supervisors to develop a multi-level feedback taxonomy, and then construct a publicly available dataset with comprehensive feedback annotations of 400 emotional support conversations. We further design a self-improvement method on top of large language models to enhance the automatic generation of feedback. Via qualitative and quantitative evaluation with domain experts, we demonstrate that our method minimizes the risk of potentially harmful and low-quality feedback generation which is desirable in such high-stakes scenarios.</abstract>
      <url hash="0c00d969">2024.acl-long.227</url>
      <bibkey>chaszczewicz-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.acl-long.227</doi>
    </paper>
    <paper id="228">
      <title>In-context Mixing (<fixed-case>ICM</fixed-case>): Code-mixed Prompts for Multilingual <fixed-case>LLM</fixed-case>s</title>
      <author><first>Bhavani</first><last>Shankar</last></author>
      <author><first>Preethi</first><last>Jyothi</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>4162-4176</pages>
      <abstract>We introduce a simple and effective prompting technique called in-context mixing (ICM) for effective in-context learning (ICL) with multilingual large language models (MLLMs). With ICM, we modify the few-shot examples within ICL prompts to be intra-sententially code-mixed by randomly swapping content words in the target languages with their English translations. We observe that ICM prompts yield superior performance in NLP tasks such as disfluency correction, grammar error correction and text simplification that demand a close correspondence between the input and output sequences. Significant improvements are observed mainly for low-resource languages that are under-represented during the pretraining and finetuning of MLLMs. We present an extensive set of experiments to analyze when ICM is effective and what design choices contribute towards its effectiveness. ICM works consistently and significantly better than other prompting techniques across models of varying capacity such as mT0-XXL, BloomZ and GPT-4.</abstract>
      <url hash="30c6d498">2024.acl-long.228</url>
      <bibkey>shankar-etal-2024-context</bibkey>
      <doi>10.18653/v1/2024.acl-long.228</doi>
    </paper>
    <paper id="229">
      <title>Respond in my Language: Mitigating Language Inconsistency in Response Generation based on Large Language Models</title>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Qin</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Haoyang</first><last>Huang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Dongdong</first><last>Zhang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>4177-4192</pages>
      <abstract>Large Language Models (LLMs) show strong instruction understanding ability across multiple languages. However, they are easily biased towards English in instruction tuning, and generate English responses even given non-English instructions. In this paper, we investigate the language inconsistent generation problem in monolingual instruction tuning. We find that instruction tuning in English increases the models’ preference for English responses. It attaches higher probabilities to English responses than to responses in the same language as the instruction. Based on the findings, we alleviate the language inconsistent generation problem by counteracting the model preference for English responses in both the training and inference stages. Specifically, we propose Pseudo-Inconsistent Penalization (PIP) which prevents the model from generating English responses when given non-English language prompts during training, and Prior Enhanced Decoding (PED) which improves the language-consistent prior by leveraging the untuned base language model. Experimental results show that our two methods significantly improve the language consistency of the model without requiring any multilingual data. Our code, data, and models will be released.</abstract>
      <url hash="d0b48952">2024.acl-long.229</url>
      <bibkey>zhang-etal-2024-respond</bibkey>
      <doi>10.18653/v1/2024.acl-long.229</doi>
    </paper>
    <paper id="230">
      <title>Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries</title>
      <author><first>Yu-Hsiang</first><last>Huang</last></author>
      <author><first>Yuche</first><last>Tsai</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Hsiang</first><last>Hsiao</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Hong-Yi</first><last>Lin</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Shou-De</first><last>Lin</last><affiliation>National Taiwan University and National Taiwan University</affiliation></author>
      <pages>4193-4205</pages>
      <abstract>This study investigates the privacy risks associated with text embeddings, focusing on the scenario where attackers cannot access the original embedding model. Contrary to previous research requiring direct model access, we explore a more realistic threat model by developing a transfer attack method. This approach uses a surrogate model to mimic the victim model’s behavior, allowing the attacker to infer sensitive information from text embeddings without direct access. Our experiments across various embedding models and a clinical dataset demonstrate that our transfer attack significantly outperforms traditional methods, revealing the potential privacy vulnerabilities in embedding technologies and emphasizing the need for enhanced security measures.</abstract>
      <url hash="3a8059b2">2024.acl-long.230</url>
      <bibkey>huang-etal-2024-transferable</bibkey>
      <doi>10.18653/v1/2024.acl-long.230</doi>
    </paper>
    <paper id="231">
      <title>Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding</title>
      <author><first>Kuo</first><last>Liao</last><affiliation>Tencent</affiliation></author>
      <author><first>Shuang</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Meng</first><last>Zhao</last><affiliation>Tencent</affiliation></author>
      <author><first>Liqun</first><last>Liu</last></author>
      <author><first>Mengge</first><last>Xue</last></author>
      <author><first>Zhenyu</first><last>Hu</last></author>
      <author><first>Honglin</first><last>Han</last><affiliation>Tencent</affiliation></author>
      <author><first>Chengguo</first><last>Yin</last></author>
      <pages>4206-4220</pages>
      <abstract>Recent strides in large language models (LLMs) have yielded remarkable performance, leveraging reinforcement learning from human feedback (RLHF) to significantly enhance generation and alignment capabilities. However, RLHF encounters numerous challenges, including the objective mismatch issue, leading to suboptimal performance in Natural Language Understanding (NLU) tasks.To address this limitation, we propose a novel Reinforcement Learning framework enhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs in NLU tasks. By incorporating label-sensitive pairs into reinforcement learning, our method aims to adeptly capture nuanced label-sensitive semantic features during RL, thereby enhancing natural language understanding.Experiments conducted on five diverse foundation models across eight tasks showcase promising results. In comparison to Supervised Fine-tuning models (SFT), RLLR demonstrates an average performance improvement of 1.54%. Compared with RLHF models, the improvement averages at 0.69%. These results reveal the effectiveness of our method for LLMs in NLU tasks.</abstract>
      <url hash="666c24df">2024.acl-long.231</url>
      <bibkey>liao-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.acl-long.231</doi>
    </paper>
    <paper id="232">
      <title>Intuitive or Dependent? Investigating <fixed-case>LLM</fixed-case>s’ Behavior Style to Conflicting Prompts</title>
      <author><first>Jiahao</first><last>Ying</last></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Kai</first><last>Xiong</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Long</first><last>Cui</last></author>
      <author><first>Yidong</first><last>He</last></author>
      <author><first>Yongbin</first><last>Liu</last></author>
      <pages>4221-4246</pages>
      <abstract>This study investigates the behaviors of Large Language Models (LLMs) when faced with conflicting prompts versus their internal memory. This will not only help to understand LLMs’ decision mechanism but also benefit real-world applications, such as retrieval-augmented generation (RAG).Drawing on cognitive theory, we target the first scenario of decision-making styles where there is no superiority in the conflict and categorize LLMs’ preference into dependent, intuitive, and rational/irrational styles.Another scenario of factual robustness considers the correctness of prompt and memory in knowledge-intensive tasks, which can also distinguish if LLMs behave rationally or irrationally in the first scenario.To quantify them, we establish a complete benchmarking framework including a dataset, a robustness evaluation pipeline, and corresponding metrics. Extensive experiments with seven LLMs reveal their varying behaviors. And, with role play intervention, we can change the styles, but different models present distinct adaptivity and upper-bound. One of our key takeaways is to optimize models or the prompts according to the identified style. For instance, RAG models with high role play adaptability may dynamically adjust the interventions according to the quality of retrieval results — being dependent to better leverage informative context; and, being intuitive when external prompt is noisy.</abstract>
      <url hash="9c0ff43c">2024.acl-long.232</url>
      <bibkey>ying-etal-2024-intuitive</bibkey>
      <doi>10.18653/v1/2024.acl-long.232</doi>
    </paper>
    <paper id="233">
      <title><fixed-case>C</fixed-case>o<fixed-case>CA</fixed-case>: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending</title>
      <author><first>Shiyi</first><last>Zhu</last></author>
      <author><first>Jing</first><last>Ye</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wei</first><last>Jiang</last></author>
      <author><first>Siqiao</first><last>Xue</last><affiliation>Alibaba</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Yifan</first><last>Wu</last><affiliation>Peking University</affiliation></author>
      <author><first>Jianguo</first><last>Li</last><affiliation>Ant Group</affiliation></author>
      <pages>4247-4262</pages>
      <abstract>Self-attention and position embedding are two crucial modules in transformer-based Large Language Models (LLMs). However, the potential relationship between them is far from well studied, especially for long context window extending. In fact, anomalous behaviors that hinder long context extrapolation exist between Rotary Position Embedding (RoPE) and vanilla self-attention.Incorrect initial angles between <tex-math>Q</tex-math> and <tex-math>K</tex-math> can cause misestimation in modeling rotary position embedding of the closest tokens.To address this issue, we propose <tex-math>\textbf{Co}</tex-math>llinear <tex-math>\textbf{C}</tex-math>onstrained <tex-math>\textbf{A}</tex-math>ttention mechanism, namely CoCA. Specifically, we enforce a collinear constraint between <tex-math>Q</tex-math> and <tex-math>K</tex-math> to seamlessly integrate RoPE and self-attention.While only adding minimal computational and spatial complexity, this integration significantly enhances long context window extrapolation ability. We provide an optimized implementation, making it a drop-in replacement for any existing transformer-based models.Extensive experiments demonstrate that CoCA excels in extending context windows. A CoCA-based GPT model, trained with a context length of 512, can extend the context window up to 32K (60<tex-math>\times</tex-math>) without any fine-tuning.Additionally, incorporating CoCA into LLaMA-7B achieves extrapolation up to 32K within a training length of only 2K.Our code is publicly available at: https://github.com/codefuse-ai/Collinear-Constrained-Attention</abstract>
      <url hash="1753c452">2024.acl-long.233</url>
      <bibkey>zhu-etal-2024-coca</bibkey>
      <revision id="1" href="2024.acl-long.233v1" hash="4eabb3e5"/>
      <revision id="2" href="2024.acl-long.233v2" hash="1753c452" date="2024-09-09">The author's affiliation changed.</revision>
      <doi>10.18653/v1/2024.acl-long.233</doi>
    </paper>
    <paper id="234">
      <title><fixed-case>I</fixed-case>nfo<fixed-case>L</fixed-case>oss<fixed-case>QA</fixed-case>: Characterizing and Recovering Information Loss in Text Simplification</title>
      <author><first>Jan</first><last>Trienes</last></author>
      <author><first>Sebastian</first><last>Joseph</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Jörg</first><last>Schlötterer</last><affiliation>Universität Mannheim and Phillips-Universität Marburg</affiliation></author>
      <author><first>Christin</first><last>Seifert</last><affiliation>Phillips-Universität Marburg and University of Twente</affiliation></author>
      <author><first>Kyle</first><last>Lo</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Byron</first><last>Wallace</last><affiliation>Northeastern University, Brown University and Northeastern University</affiliation></author>
      <author><first>Junyi Jessy</first><last>Li</last><affiliation>University of Texas, Austin</affiliation></author>
      <pages>4263-4294</pages>
      <abstract>Text simplification aims to make technical texts more accessible to laypeople but often results in deletion of information and vagueness. This work proposes InfoLossQA, a framework to characterize and recover simplification-induced information loss in form of question-and-answer (QA) pairs. Building on the theory of Questions Under Discussion, the QA pairs are designed to help readers deepen their knowledge of a text. First, we collect a dataset of 1,000 linguist-curated QA pairs derived from 104 LLM simplifications of English medical study abstracts. Our analyses of this data reveal that information loss occurs frequently, and that the QA pairs give a high-level overview of what information was lost. Second, we devise two methods for this task: end-to-end prompting of open-source and commercial language models, and a natural language inference pipeline. With a novel evaluation framework considering the correctness of QA pairs and their linguistic suitability, our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss.</abstract>
      <url hash="3dc2b5c0">2024.acl-long.234</url>
      <bibkey>trienes-etal-2024-infolossqa</bibkey>
      <doi>10.18653/v1/2024.acl-long.234</doi>
    </paper>
    <paper id="235">
      <title><fixed-case>C</fixed-case>o<fixed-case>G</fixed-case>enesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following</title>
      <author><first>Kaiyan</first><last>Zhang</last><affiliation>Electronic Engineering, Tsinghua University</affiliation></author>
      <author><first>Jianyu</first><last>Wang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Ermo</first><last>Hua</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Biqing</first><last>Qi</last><affiliation>Tsinghua University and Harbin Institute of Technology</affiliation></author>
      <author><first>Ning</first><last>Ding</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Bowen</first><last>Zhou</last><affiliation>Tsinghua University</affiliation></author>
      <pages>4295-4312</pages>
      <abstract>With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional open-source datasets, indicate that: 1) Large-scale models perform well when provided with user context but struggle in the absence of such context. 2) While specialized smaller models fine-tuned on the synthetic dataset show promise, they still lag behind their larger counterparts. 3) Our CoGenesis framework, utilizing mixed-scale models, showcases competitive performance, providing a feasible solution to privacy issues.</abstract>
      <url hash="17b92a26">2024.acl-long.235</url>
      <bibkey>zhang-etal-2024-cogenesis</bibkey>
      <doi>10.18653/v1/2024.acl-long.235</doi>
    </paper>
    <paper id="236">
      <title><fixed-case>DAPR</fixed-case>: A Benchmark on Document-Aware Passage Retrieval</title>
      <author><first>Kexin</first><last>Wang</last><affiliation>Ubiquitous Knowledge Processing Lab, Technical University of Darmstadt</affiliation></author>
      <author><first>Nils</first><last>Reimers</last><affiliation>HuggingFace</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>4313-4330</pages>
      <abstract>The work of neural retrieval so far focuses on ranking short texts and is challenged with long documents. There are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. Wikipedia articles, research papers, etc. We propose and name this task <i>Document-Aware Passage Retrieval</i> (DAPR). While analyzing the errors of the State-of-The-Art (SoTA) passage retrievers, we find the major errors (53.5%) are due to missing document context. This drives us to build a benchmark for this task including multiple datasets from heterogeneous domains. In the experiments, we extend the SoTA passage retrievers with document context via (1) hybrid retrieval with BM25 and (2) contextualized passage representations, which inform the passage representation with document context. We find despite that hybrid retrieval performs the strongest on the mixture of the easy and the hard queries, it completely fails on the hard queries that require document-context understanding. On the other hand, contextualized passage representations (e.g. prepending document titles) achieve good improvement on these hard queries, but overall they also perform rather poorly. Our created benchmark enables future research on developing and comparing retrieval systems for the new task. The code and the data are available.</abstract>
      <url hash="2f398529">2024.acl-long.236</url>
      <bibkey>wang-etal-2024-dapr</bibkey>
      <doi>10.18653/v1/2024.acl-long.236</doi>
    </paper>
    <paper id="237">
      <title>Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors</title>
      <author><first>Mengge</first><last>Xue</last></author>
      <author><first>Zhenyu</first><last>Hu</last></author>
      <author><first>Liqun</first><last>Liu</last></author>
      <author><first>Kuo</first><last>Liao</last><affiliation>Tencent</affiliation></author>
      <author><first>Shuang</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Honglin</first><last>Han</last><affiliation>Tencent</affiliation></author>
      <author><first>Meng</first><last>Zhao</last><affiliation>Tencent</affiliation></author>
      <author><first>Chengguo</first><last>Yin</last></author>
      <pages>4331-4344</pages>
      <abstract>Multiple-Choice Questions (MCQs) constitute a critical area of research in the study of Large Language Models (LLMs). Previous works have investigated the selection bias problem in MCQs within few-shot scenarios, in which the LLM’s performance may be influenced by the presentation of answer choices, leaving the selection bias during Supervised Fine-Tuning (SFT) unexplored. In this paper, we reveal that selection bias persists in the SFT phase , primarily due to the LLM’s inadequate Multiple Choice Symbol Binding (MCSB) ability. This limitation implies that the model struggles to associate the answer options with their corresponding symbols (e.g., A/B/C/D) effectively. To enhance the model’s MCSB capability, we first incorporate option contents into the loss function and subsequently adjust the weights of the option symbols and contents, guiding the model to understand the option content of the current symbol. Based on this, we introduce an efficient SFT algorithm for MCQs, termed Point-wise Intelligent Feedback (PIF). PIF constructs negative instances by randomly combin- ing the incorrect option contents with all candidate symbols, and proposes a point-wise loss to provide feedback on these negative samples into LLMs. Our experimental results demonstrate that PIF significantly reduces the model’s selection bias by improving its MCSB capability. Remarkably, PIF exhibits a substantial enhancement in the accuracy for MCQs.</abstract>
      <url hash="5c02e659">2024.acl-long.237</url>
      <bibkey>xue-etal-2024-strengthened</bibkey>
      <doi>10.18653/v1/2024.acl-long.237</doi>
    </paper>
    <paper id="238">
      <title><fixed-case>SAC</fixed-case>-<fixed-case>KG</fixed-case>: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graph</title>
      <author><first>Hanzhu</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xu</first><last>Shen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Qitan</first><last>Lv</last></author>
      <author><first>Jie</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xiaoqi</first><last>Ni</last></author>
      <author><first>Jieping</first><last>Ye</last><affiliation>Alibaba Group</affiliation></author>
      <pages>4345-4360</pages>
      <abstract>Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks across specialized domains, where the acquisition of precise and dependable knowledge is crucial. However, existing KG construction methods heavily rely on human intervention to attain qualified KGs, which severely hinders the practical applicability in real-world scenarios. To address this challenge, we propose a general KG construction framework, named **SAC-KG**, to exploit large language models (LLMs) as **S**killed **A**utomatic **C**onstructors for domain **K**nowledge **G**raph. SAC-KG effectively involves LLMs as domain experts to generate specialized and precise multi-level KGs. Specifically, SAC-KG consists of three components: Generator, Verifier, and Pruner. For a given entity, Generator produces its relations and tails from raw domain corpora, to construct a specialized single-level KG. Verifier and Pruner then work together to ensure precision by correcting generation errors and determining whether newly produced tails require further iteration for the next-level KG. Experiments demonstrate that SAC-KG automatically constructs a domain KG at the scale of over one million nodes and achieves a precision of 89.32%, leading to a superior performance with over 20% increase in precision rate compared to existing state-of-the-art methods for the KG construction task.</abstract>
      <url hash="2d8bb22e">2024.acl-long.238</url>
      <bibkey>chen-etal-2024-sac</bibkey>
      <doi>10.18653/v1/2024.acl-long.238</doi>
    </paper>
    <paper id="239">
      <title>Uncertainty-Guided Modal Rebalance for Hateful Memes Detection</title>
      <author><first>Chuanpeng</first><last>Yang</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Yaxin</first><last>Liu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Fuqing</first><last>Zhu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jizhong</first><last>Han</last><affiliation>Institute of Information Engineering</affiliation></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>4361-4371</pages>
      <abstract>Hateful memes detection is a challenging multimodal understanding task that requires comprehensive learning of vision, language, and cross-modal interactions. Previous research has focused on developing effective fusion strategies for integrating hate information from different modalities. However, these methods excessively rely on cross-modal fusion features, ignoring the modality uncertainty caused by the contribution degree of each modality to hate sentiment and the modality imbalance caused by the dominant modality suppressing the optimization of another modality. To this end, this paper proposes an Uncertainty-guided Modal Rebalance (UMR) framework for hateful memes detection. The uncertainty of each meme is explicitly formulated by designing stochastic representation drawn from a Gaussian distribution for aggregating cross-modal features with unimodal features adaptively. The modality imbalance is alleviated by improving cosine loss from the perspectives of inter-modal feature and weight vectors constraints. In this way, the suppressed unimodal representation ability in multimodal models would be unleashed, while the learning of modality contribution would be further promoted. Extensive experimental results demonstrate that the proposed UMR produces the state-of-the-art performance on four widely-used datasets.</abstract>
      <url hash="4db5f93e">2024.acl-long.239</url>
      <bibkey>yang-etal-2024-uncertainty-guided</bibkey>
      <doi>10.18653/v1/2024.acl-long.239</doi>
    </paper>
    <paper id="240">
      <title>Missci: Reconstructing Fallacies in Misrepresented Science</title>
      <author><first>Max</first><last>Glockner</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Yufang</first><last>Hou</last><affiliation>Technische Universität Darmstadt and IBM Research Ireland</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>4372-4405</pages>
      <abstract>Health-related misinformation on social networks can lead to poor decision-making and real-world dangers. Such misinformation often misrepresents scientific publications and cites them as “proof” to gain perceived credibility. To effectively counter such claims automatically, a system must explain how the claim was falsely derived from the cited publication. Current methods for automated fact-checking or fallacy detection neglect to assess the (mis)used evidence in relation to misinformation claims, which is required to detect the mismatch between them. To address this gap, we introduce Missci, a novel argumentation theoretical model for fallacious reasoning together with a new dataset for real-world misinformation detection that misrepresents biomedical publications. Unlike previous fallacy detection datasets, Missci (i) focuses on implicit fallacies between the relevant content of the cited publication and the inaccurate claim, and (ii) requires models to verbalize the fallacious reasoning in addition to classifying it. We present Missci as a dataset to test the critical reasoning abilities of large language models (LLMs), that are required to reconstruct real-world fallacious arguments, in a zero-shot setting. We evaluate two representative LLMs and the impact of different levels of detail about the fallacy classes provided to the LLM via prompts. Our experiments and human evaluation show promising results for GPT 4, while also demonstrating the difficulty of this task.</abstract>
      <url hash="268d6ad0">2024.acl-long.240</url>
      <bibkey>glockner-etal-2024-missci</bibkey>
      <doi>10.18653/v1/2024.acl-long.240</doi>
    </paper>
    <paper id="241">
      <title>Uncovering the Full Potential of Visual Grounding Methods in <fixed-case>VQA</fixed-case></title>
      <author><first>Daniel</first><last>Reich</last><affiliation>Universität Bremen</affiliation></author>
      <author><first>Tanja</first><last>Schultz</last><affiliation>Universität Bremen</affiliation></author>
      <pages>4406-4419</pages>
      <abstract>Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt to improve VQA performance by strengthening a model’s reliance on question-relevant visual information. The presence of such relevant information in the visual input is typically assumed in training and testing. This assumption, however, is inherently flawed when dealing with imperfect image representations common in large-scale VQA, where the information carried by visual features frequently deviates from expected ground-truth contents. As a result, training and testing of VG-methods is performed with largely inaccurate data, which obstructs proper assessment of their potential benefits.In this study, we demonstrate that current evaluation schemes for VG-methods are problematic due to the flawed assumption of availability of relevant visual information. Our experiments show that these methods can be much more effective when evaluation conditions are corrected. Code is provided.</abstract>
      <url hash="f5e0a352">2024.acl-long.241</url>
      <bibkey>reich-schultz-2024-uncovering</bibkey>
      <doi>10.18653/v1/2024.acl-long.241</doi>
    </paper>
    <paper id="242">
      <title>Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jiejun</first><last>Tan</last></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Peidong</first><last>Guo</last></author>
      <author><first>Kun</first><last>Fang</last></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>4420-4436</pages>
      <abstract>The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM’s knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the missing knowledge in questions that the LLM does not know. Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs.</abstract>
      <url hash="6f2473b1">2024.acl-long.242</url>
      <bibkey>tan-etal-2024-small</bibkey>
      <doi>10.18653/v1/2024.acl-long.242</doi>
    </paper>
    <paper id="243">
      <title>Favi-Score: A Measure for Favoritism in Automated Preference Ratings for Generative <fixed-case>AI</fixed-case> Evaluation</title>
      <author><first>Pius</first><last>Von Däniken</last><affiliation>University of Zurich and ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Jan</first><last>Deriu</last><affiliation>ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Don</first><last>Tuggener</last><affiliation>ZHAW School of Engineering</affiliation></author>
      <author><first>Mark</first><last>Cieliebak</last><affiliation>ZHAW School of Engineering</affiliation></author>
      <pages>4437-4454</pages>
      <abstract>Generative AI systems have become ubiquitous for all kinds of modalities, which makes the issue of the evaluation of such models more pressing. One popular approach is preference ratings, where the generated outputs of different systems are shown to evaluators who choose their preferences. In recent years the field shifted towards the development of automated (trained) metrics to assess generated outputs, which can be used to create preference ratings automatically. In this work, we investigate the evaluation of the metrics themselves, which currently rely on measuring the correlation to human judgments or computing sign accuracy scores. These measures only assess how well the metric agrees with the human ratings. However, our research shows that this does not tell the whole story. Most metrics exhibit a disagreement with human system assessments which is often skewed in favor of particular text generation systems, exposing a degree of favoritism in automated metrics. This paper introduces a formal definition of favoritism in preference metrics, and derives the Favi-Score, which measures this phenomenon. In particular we show that favoritism is strongly related to errors in final system rankings. Thus, we propose that preference-based metrics ought to be evaluated on both sign accuracy scores and favoritism.</abstract>
      <url hash="e2609ad9">2024.acl-long.243</url>
      <bibkey>von-daniken-etal-2024-favi</bibkey>
      <doi>10.18653/v1/2024.acl-long.243</doi>
    </paper>
    <paper id="244">
      <title><fixed-case>LLM</fixed-case>-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback</title>
      <author><first>Timon</first><last>Ziegenbein</last><affiliation>Universität Hannover</affiliation></author>
      <author><first>Gabriella</first><last>Skitalinskaya</last><affiliation>Universität Hannover</affiliation></author>
      <author><first>Alireza</first><last>Bayat Makou</last></author>
      <author><first>Henning</first><last>Wachsmuth</last><affiliation>Leibniz Universität Hannover</affiliation></author>
      <pages>4455-4476</pages>
      <abstract>Ensuring that online discussions are civil and productive is a major challenge for social media platforms. Such platforms usually rely both on users and on automated detection tools to flag inappropriate arguments of other users, which moderators then review. However, this kind of post-hoc moderation is expensive and time-consuming, and moderators are often overwhelmed by the amount and severity of flagged content. Instead, a promising alternative is to prevent negative behavior during content creation. This paper studies how inappropriate language in arguments can be computationally mitigated. We propose a reinforcement learning-based rewriting approach that balances content preservation and appropriateness based on existing classifiers, prompting an instruction-finetuned large language model (LLM) as our initial policy. Unlike related style transfer tasks, rewriting inappropriate arguments allows deleting and adding content permanently. It is therefore tackled on document level rather than sentence level. We evaluate different weighting schemes for the reward function in both absolute and relative human assessment studies. Systematic experiments on non-parallel data provide evidence that our approach can mitigate the inappropriateness of arguments while largely preserving their content. It significantly outperforms competitive baselines, including few-shot learning, prompting, and humans.</abstract>
      <url hash="9062a374">2024.acl-long.244</url>
      <bibkey>ziegenbein-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.acl-long.244</doi>
    </paper>
    <paper id="245">
      <title>Graph Language Models</title>
      <author><first>Moritz</first><last>Plenz</last><affiliation>Institute for Computational Linguistics, Heidelberg University, Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <author><first>Anette</first><last>Frank</last><affiliation>Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <pages>4477-4494</pages>
      <abstract>While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs – which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure – but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM’s architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot setting, demonstrating their versatility.</abstract>
      <url hash="dbbf211f">2024.acl-long.245</url>
      <bibkey>plenz-frank-2024-graph</bibkey>
      <doi>10.18653/v1/2024.acl-long.245</doi>
    </paper>
    <paper id="246">
      <title>Analyzing Semantic Change through Lexical Replacements</title>
      <author><first>Francesco</first><last>Periti</last><affiliation>University of Milan</affiliation></author>
      <author><first>Pierluigi</first><last>Cassotti</last><affiliation>Göteborg University</affiliation></author>
      <author><first>Haim</first><last>Dubossarsky</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Nina</first><last>Tahmasebi</last><affiliation>Göteborg University</affiliation></author>
      <pages>4495-4510</pages>
      <abstract>Modern language models are capable of contextualizing words based on their surrounding context. However, this capability is often compromised due to semantic change that leads to words being used in new, unexpected contexts not encountered during pre-training. In this paper, we model semantic change by studying the effect of unexpected contexts introduced by lexical replacements. We propose a replacement schema where a target word is substituted with lexical replacements of varying relatedness, thus simulating different kinds of semantic change. Furthermore, we leverage the replacement schema as a basis for a novel interpretable model for semantic change. We are also the first to evaluate the use of LLaMa for semantic change detection.</abstract>
      <url hash="987a3998">2024.acl-long.246</url>
      <bibkey>periti-etal-2024-analyzing</bibkey>
      <doi>10.18653/v1/2024.acl-long.246</doi>
    </paper>
    <paper id="247">
      <title>Exploiting Intrinsic Multilateral Logical Rules for Weakly Supervised Natural Language Video Localization</title>
      <author><first>Zhe</first><last>Xu</last><affiliation>Xidian University</affiliation></author>
      <author><first>Kun</first><last>Wei</last><affiliation>Xidian University</affiliation></author>
      <author><first>Xu</first><last>Yang</last><affiliation>Xi’an University of Electronic Science and Technology</affiliation></author>
      <author><first>Cheng</first><last>Deng</last><affiliation>Xidian University</affiliation></author>
      <pages>4511-4521</pages>
      <abstract>Weakly supervised natural language video localization (WS-NLVL) aims to retrieve the moment corresponding to a language query in a video with only video-language pairs utilized during training. Despite great success, existing WS-NLVL methods seldomly consider the complex temporal relations enclosing the language query (e.g., between the language query and sub-queries decomposed from it or its synonymous query), yielding illogical predictions. In this paper, we propose a novel plug-and-play method, Intrinsic Multilateral Logical Rules, namely IMLR, to exploit intrinsic temporal relations and logical rules for WS-NLVL. Specifically, we formalize queries derived from the original language query as the nodes of a directed graph, i.e., intrinsic temporal relation graph (ITRG), and the temporal relations between them as the edges. Instead of directly prompting a pre-trained language model, a relation-guided prompting method is introduced to generate ITRG in a hierarchical manner. We customize four types of multilateral temporal logical rules (i.e., identity, inclusion, synchronization, and succession) from ITRG and utilize them to train our model. Experiments demonstrate the effectiveness and superiority of our method on the Charades-STA and ActivityNet Captions datasets.</abstract>
      <url hash="8ff10891">2024.acl-long.247</url>
      <bibkey>xu-etal-2024-exploiting</bibkey>
      <doi>10.18653/v1/2024.acl-long.247</doi>
    </paper>
    <paper id="248">
      <title>Interpretability of Language Models via Task Spaces</title>
      <author><first>Lucas</first><last>Weber</last></author>
      <author><first>Jaap</first><last>Jumelet</last></author>
      <author><first>Elia</first><last>Bruni</last><affiliation>Universität Osnabrück</affiliation></author>
      <author><first>Dieuwke</first><last>Hupkes</last><affiliation>Facebook</affiliation></author>
      <pages>4522-4538</pages>
      <abstract>The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes.In this paper, we present an alternative approach, concentrating on the _quality_ of LM processing, with a focus on their language abilities.To this end, we construct ‘linguistic task spaces’ – representations of an LM’s language conceptualisation – that shed light on the connections LMs draw between language phenomena.Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call ‘similarity probing’.To disentangle the learning signals of linguistic phenomena, we further introduce a method called ‘fine-tuning via gradient differentials’ (FTGD).We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs.</abstract>
      <url hash="7e196137">2024.acl-long.248</url>
      <bibkey>weber-etal-2024-interpretability</bibkey>
      <doi>10.18653/v1/2024.acl-long.248</doi>
    </paper>
    <paper id="249">
      <title>Using Synchronic Definitions and Semantic Relations to Classify Semantic Change Types</title>
      <author><first>Pierluigi</first><last>Cassotti</last><affiliation>Göteborg University</affiliation></author>
      <author><first>Stefano</first><last>De Pascale</last><affiliation>Vrije Universiteit Brussel and KU Leuven</affiliation></author>
      <author><first>Nina</first><last>Tahmasebi</last><affiliation>Göteborg University</affiliation></author>
      <pages>4539-4553</pages>
      <abstract>There is abundant evidence of the fact that the way words change their meaning can be classified in different types of change, highlighting the relationship between the old and new meanings (among which generalisation, specialisation and co-hyponymy transfer).In this paper, we present a way of detecting these types of change by constructing a model that leverages information both from synchronic lexical relations and definitions of word meanings. Specifically, we use synset definitions and hierarchy information from WordNet and test it on a digitized version of Blank’s (1997) dataset of semantic change types. Finally, we show how the sense relationships can improve models for both approximation of human judgments of semantic relatedness as well as binary Lexical Semantic Change Detection.</abstract>
      <url hash="72632cad">2024.acl-long.249</url>
      <bibkey>cassotti-etal-2024-using</bibkey>
      <doi>10.18653/v1/2024.acl-long.249</doi>
    </paper>
    <paper id="250">
      <title>Factual Confidence of <fixed-case>LLM</fixed-case>s: on Reliability and Robustness of Current Estimators</title>
      <author><first>Matéo</first><last>Mahaut</last></author>
      <author><first>Laura</first><last>Aina</last><affiliation>Amazon</affiliation></author>
      <author><first>Paula</first><last>Czarnowska</last><affiliation>Amazon AWS</affiliation></author>
      <author><first>Momchil</first><last>Hardalov</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Thomas</first><last>Müller</last><affiliation>Amazon</affiliation></author>
      <author><first>Lluis</first><last>Marquez</last></author>
      <pages>4554-4570</pages>
      <abstract>Large Language Models (LLMs) tend to be unreliable on fact-based answers.To address this problem, NLP researchers have proposed a range of techniques to estimate LLM’s confidence over facts. However, due to the lack of a systematic comparison, it is not clear how the different methods compare to one other.To fill this gap, we present a rigorous survey and empirical comparison of estimators of factual confidence.We define an experimental framework allowing for fair comparison, covering both fact-verification and QA. Our experiments across a series of LLMs indicate that trained hidden-state probes provide the most reliable confidence estimates; albeit at the expense of requiring access to weights and supervision data. We also conduct a deeper assessment of the methods, in which we measure the consistency of model behavior under meaning-preserving variations in the input. We find that the factual confidence of LLMs is often unstable across semantically equivalent inputs, suggesting there is much room for improvement for the stability of models’ parametric knowledge.</abstract>
      <url hash="ce5d62e5">2024.acl-long.250</url>
      <bibkey>mahaut-etal-2024-factual</bibkey>
      <doi>10.18653/v1/2024.acl-long.250</doi>
    </paper>
    <paper id="251">
      <title><fixed-case>S</fixed-case>tep<fixed-case>C</fixed-case>oder: Improving Code Generation with Reinforcement Learning from Compiler Feedback</title>
      <author><first>Shihan</first><last>Dou</last></author>
      <author><first>Yan</first><last>Liu</last></author>
      <author><first>Haoxiang</first><last>Jia</last></author>
      <author><first>Enyu</first><last>Zhou</last><affiliation>Fudan University</affiliation></author>
      <author><first>Limao</first><last>Xiong</last><affiliation>Fudan University</affiliation></author>
      <author><first>Junjie</first><last>Shan</last></author>
      <author><first>Caishuang</first><last>Huang</last></author>
      <author><first>Xiao</first><last>Wang</last></author>
      <author><first>Xiaoran</first><last>Fan</last></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Yuhao</first><last>Zhou</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tao</first><last>Ji</last></author>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>4571-4585</pages>
      <abstract>The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce <b>StepCoder</b>, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. The code and dataset will be made available upon publication.</abstract>
      <url hash="48da7208">2024.acl-long.251</url>
      <bibkey>dou-etal-2024-stepcoder</bibkey>
      <doi>10.18653/v1/2024.acl-long.251</doi>
    </paper>
    <paper id="252">
      <title>One-Shot Learning as Instruction Data Prospector for Large Language Models</title>
      <author><first>Yunshui</first><last>Li</last></author>
      <author><first>Binyuan</first><last>Hui</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaobo</first><last>Xia</last><affiliation>The University of Sydney</affiliation></author>
      <author><first>Jiaxi</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lei</first><last>Zhang</last><affiliation>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shuzheng</first><last>Si</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ling-Hao</first><last>Chen</last></author>
      <author><first>Junhao</first><last>Liu</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Tongliang</first><last>Liu</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>4586-4601</pages>
      <abstract>Contemporary practices in instruction tuning often hinge on enlarging data scaling without a clear strategy for ensuring data quality, inadvertently introducing noise that may compromise model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that leverages one-shot learning to discern and select high-quality instruction data from extensive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one-shot learning instances, thereby identifying those that can significantly improve performance across diverse tasks. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most advantageous data for instruction tuning. Through rigorous evaluations on two benchmarks, namely MT-Bench and Alpaca-Eval, our study illustrates that instruction tuning with the top 1% of examples curated by Nuggets substantially outperforms conventional methods employing the entire dataset.</abstract>
      <url hash="06c7a0f2">2024.acl-long.252</url>
      <bibkey>li-etal-2024-one</bibkey>
      <doi>10.18653/v1/2024.acl-long.252</doi>
    </paper>
    <paper id="253">
      <title>Navigating the <fixed-case>O</fixed-case>ver<fixed-case>K</fixed-case>ill in Large Language Models</title>
      <author><first>Chenyu</first><last>Shi</last></author>
      <author><first>Xiao</first><last>Wang</last></author>
      <author><first>Qiming</first><last>Ge</last></author>
      <author><first>Songyang</first><last>Gao</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xianjun</first><last>Yang</last></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xun</first><last>Zhao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>4602-4614</pages>
      <abstract>Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to excessive attention to harmful words like ‘kill’ and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such excessive attention by amplifying the difference in the model’s output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the excessive attention via contrastive decoding. Empirical results have indicated that our method has achieved an average reduction of the refusal rate by 20 % while having almost no impact on safety.</abstract>
      <url hash="728cb20a">2024.acl-long.253</url>
      <bibkey>shi-etal-2024-navigating</bibkey>
      <doi>10.18653/v1/2024.acl-long.253</doi>
    </paper>
    <paper id="254">
      <title>A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains</title>
      <author><first>Alon</first><last>Jacovi</last><affiliation>Google</affiliation></author>
      <author><first>Yonatan</first><last>Bitton</last><affiliation>Google</affiliation></author>
      <author><first>Bernd</first><last>Bohnet</last></author>
      <author><first>Jonathan</first><last>Herzig</last><affiliation>Research, Google</affiliation></author>
      <author><first>Or</first><last>Honovich</last></author>
      <author><first>Michael</first><last>Tseng</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Michael</first><last>Collins</last><affiliation>Google and Columbia University</affiliation></author>
      <author><first>Roee</first><last>Aharoni</last><affiliation>Google</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Tel Aviv University and Google Research</affiliation></author>
      <pages>4615-4634</pages>
      <abstract>Prompting language models to provide step-by-step answers (e.g., “Chain-of-Thought”) is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model’s answer, across a variety of datasets and state-of-the-art language models. Evaluation on REVEAL shows that verifiers struggle at verifying reasoning chains - in particular, verifying logical correctness and detecting contradictions. Available at https://reveal-dataset.github.io/ .</abstract>
      <url hash="286fc00a">2024.acl-long.254</url>
      <bibkey>jacovi-etal-2024-chain</bibkey>
      <doi>10.18653/v1/2024.acl-long.254</doi>
    </paper>
    <paper id="255">
      <title>Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision</title>
      <author><first>Qian</first><last>Ruan</last></author>
      <author><first>Ilia</first><last>Kuznetsov</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>4635-4655</pages>
      <abstract>Collaborative review and revision of textual documents is the core of knowledge work and a promising target for empirical analysis and NLP assistance. Yet, a holistic framework that would allow modeling complex relationships between document revisions, reviews and author responses is lacking. To address this gap, we introduce Re3, a framework for joint analysis of collaborative document revision. We instantiate this framework in the scholarly domain, and present Re3-Sci, a large corpus of aligned scientific paper revisions manually labeled according to their action and intent, and supplemented with the respective peer reviews and human-written edit summaries. We use the new data to provide first empirical insights into collaborative document revision in the academic domain, and to assess the capabilities of state-of-the-art LLMs at automating edit analysis and facilitating text-based collaboration. We make our annotation environment and protocols, the resulting data and experimental code publicly available.</abstract>
      <url hash="a2dceae3">2024.acl-long.255</url>
      <bibkey>ruan-etal-2024-re3</bibkey>
      <doi>10.18653/v1/2024.acl-long.255</doi>
    </paper>
    <paper id="256">
      <title><fixed-case>N</fixed-case>ext<fixed-case>L</fixed-case>evel<fixed-case>BERT</fixed-case>: Masked Language Modeling with Higher-Level Representations for Long Documents</title>
      <author><first>Tamara</first><last>Czinczoll</last><affiliation>Hasso Plattner Institute</affiliation></author>
      <author><first>Christoph</first><last>Hönes</last><affiliation>Universität Potsdam and Hasso Plattner Institute</affiliation></author>
      <author><first>Maximilian</first><last>Schall</last><affiliation>Hasso Plattner Institute</affiliation></author>
      <author><first>Gerard</first><last>De Melo</last><affiliation>Hasso Plattner Institute and University of Potsdam</affiliation></author>
      <pages>4656-4666</pages>
      <abstract>While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings. We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three types of tasks: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering. We find that next-level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail of semantic information is not too fine. Our models and code are publicly available online.</abstract>
      <url hash="dae22bda">2024.acl-long.256</url>
      <bibkey>czinczoll-etal-2024-nextlevelbert</bibkey>
      <doi>10.18653/v1/2024.acl-long.256</doi>
    </paper>
    <paper id="257">
      <title><fixed-case>F</fixed-case>ollow<fixed-case>B</fixed-case>ench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models</title>
      <author><first>Yuxin</first><last>Jiang</last></author>
      <author><first>Yufei</first><last>Wang</last></author>
      <author><first>Xingshan</first><last>Zeng</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Wanjun</first><last>Zhong</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Liangyou</first><last>Li</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>4667-4688</pages>
      <abstract>The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating pure response quality, rather than assessing whether the response follows constraints stated in the instruction. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Situation, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each increased level. To assess whether LLMs’ outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths to handle challenging open-ended instructions. By evaluating 13 closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.</abstract>
      <url hash="05b9bf23">2024.acl-long.257</url>
      <bibkey>jiang-etal-2024-followbench</bibkey>
      <doi>10.18653/v1/2024.acl-long.257</doi>
    </paper>
    <paper id="258">
      <title>Learning to Edit: Aligning <fixed-case>LLM</fixed-case>s with Knowledge Editing</title>
      <author><first>Yuxin</first><last>Jiang</last></author>
      <author><first>Yufei</first><last>Wang</last></author>
      <author><first>Chuhan</first><last>Wu</last><affiliation>Noah’s Ark Lab, Huawei</affiliation></author>
      <author><first>Wanjun</first><last>Zhong</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Xingshan</first><last>Zeng</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Jiahui</first><last>Gao</last></author>
      <author><first>Liangyou</first><last>Li</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ruiming</first><last>Tang</last></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>4689-4705</pages>
      <abstract>Knowledge editing techniques, aiming to efficiently modify a minor proportion of knowledge in large language models (LLMs) without negatively impacting performance across other inputs, have garnered widespread attention. However, existing methods predominantly rely on memorizing the updated knowledge, impeding LLMs from effectively combining the new knowledge with their inherent knowledge when answering questions. To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of “Teach a man to fish.” LTE features a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on a meticulously curated parallel dataset to make reliable, in-scope edits while preserving out-of-scope information and linguistic proficiency; and (ii) the Inference Phase, which employs a retrieval-based mechanism for real-time and mass knowledge editing. By comparing our approach with seven advanced baselines across four popular knowledge editing benchmarks and two LLM architectures, we demonstrate LTE’s superiority in knowledge editing performance, robustness in both batch and sequential editing, minimal interference on general tasks, and rapid editing speeds. The data and code are publicly available at https://github.com/YJiangcm/LTE.</abstract>
      <url hash="0fcee2a1">2024.acl-long.258</url>
      <bibkey>jiang-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.acl-long.258</doi>
    </paper>
    <paper id="259">
      <title><fixed-case>D</fixed-case>olph<fixed-case>C</fixed-case>oder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning</title>
      <author><first>Yejie</first><last>Wang</last></author>
      <author><first>Keqing</first><last>He</last><affiliation>Meituan Group</affiliation></author>
      <author><first>Guanting</first><last>Dong</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Pei</first><last>Wang</last></author>
      <author><first>Weihao</first><last>Zeng</last></author>
      <author><first>Muxi</first><last>Diao</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <author><first>Jingang</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Mengdi</first><last>Zhang</last></author>
      <author><first>Xunliang</first><last>Cai</last></author>
      <pages>4706-4721</pages>
      <abstract>Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Various instruction finetuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model DolphCoder with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with more distinct reasoning paths increases the code capability of LLMs. (2) Improving one’s ability to evaluate the correctness of code also enhances their ability to create it.</abstract>
      <url hash="f2ababbf">2024.acl-long.259</url>
      <bibkey>wang-etal-2024-dolphcoder</bibkey>
      <doi>10.18653/v1/2024.acl-long.259</doi>
    </paper>
    <paper id="260">
      <title>When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality</title>
      <author><first>Brielen</first><last>Madureira</last><affiliation>University of Potsdam, Universität des Saarlandes, German Research Center for AI, Fraunhofer and Universidade de São Paulo</affiliation></author>
      <author><first>Patrick</first><last>Kahardipraja</last><affiliation>Fraunhofer HHI</affiliation></author>
      <author><first>David</first><last>Schlangen</last><affiliation>University of Potsdam</affiliation></author>
      <pages>4722-4749</pages>
      <abstract>Incremental models that process sentences one token at a time will sometimes encounter points where more than one interpretation is possible. Causal models are forced to output one interpretation and continue, whereas models that can revise may edit their previous output as the ambiguity is resolved. In this work, we look at how restart-incremental Transformers build and update internal states, in an effort to shed light on what processes cause revisions not viable in autoregressive models. We propose an interpretable way to analyse the incremental states, showing that their sequential structure encodes information on the garden path effect and its resolution. Our method brings insights on various bidirectional encoders for contextualised meaning representation and dependency parsing, contributing to show their advantage over causal models when it comes to revisions.</abstract>
      <url hash="07ca5433">2024.acl-long.260</url>
      <bibkey>madureira-etal-2024-time</bibkey>
      <doi>10.18653/v1/2024.acl-long.260</doi>
    </paper>
    <paper id="261">
      <title><fixed-case>S</fixed-case>pa<fixed-case>RC</fixed-case> and <fixed-case>S</fixed-case>pa<fixed-case>RP</fixed-case>: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models</title>
      <author><first>Md Imbesat</first><last>Rizvi</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Xiaodan</first><last>Zhu</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>4750-4767</pages>
      <abstract>Spatial reasoning is a crucial component of both biological and artificial intelligence. In this work, we present a comprehensive study of the capability of current state-of-the-art large language models (LLMs) on spatial reasoning. To support our study, we created and contribute a novel Spatial Reasoning Characterization (SpaRC) framework and Spatial Reasoning Paths (SpaRP) datasets, to enable an in-depth understanding of the spatial relations and compositions as well as the usefulness of spatial reasoning chains. We found that all the state-of-the-art LLMs do not perform well on the datasets—their performances are consistently low across different setups. The spatial reasoning capability improves substantially as model sizes scale up. Finetuning both large language models (e.g., Llama-2-70B) and smaller ones (e.g., Llama-2-13B) can significantly improve their F1-scores by 7–32 absolute points. We also found that the top proprietary LLMs still significantly outperform their open-source counterparts in topological spatial understanding and reasoning.</abstract>
      <url hash="89d32c5a">2024.acl-long.261</url>
      <bibkey>rizvi-etal-2024-sparc</bibkey>
      <doi>10.18653/v1/2024.acl-long.261</doi>
    </paper>
    <paper id="262">
      <title>Planning Like Human: A Dual-process Framework for Dialogue Planning</title>
      <author><first>Tao</first><last>He</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yuanxing</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zerui</first><last>Chen</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>4768-4791</pages>
      <abstract>In proactive dialogue, the challenge lies not just in generating responses but in steering conversations toward predetermined goals, a task where Large Language Models (LLMs) typically struggle due to their reactive nature. Traditional approaches to enhance dialogue planning in LLMs, ranging from elaborate prompt engineering to the integration of policy networks, either face efficiency issues or deliver suboptimal performance. Inspired by the dual-process theory in psychology, which identifies two distinct modes of thinking—intuitive (fast) and analytical (slow), we propose the Dual-Process Dialogue Planning (DPDP) framework. DPDP embodies this theory through two complementary planning systems: an instinctive policy model for familiar contexts and a deliberative Monte Carlo Tree Search (MCTS) mechanism for complex, novel scenarios. This dual strategy is further coupled with a novel two-stage training regimen: offline Reinforcement Learning for robust initial policy model formation followed by MCTS-enhanced on-the-fly learning, which ensures a dynamic balance between efficiency and strategic depth. Our empirical evaluations across diverse dialogue tasks affirm DPDP’s superiority in achieving both high-quality dialogues and operational efficiency, outpacing existing methods.</abstract>
      <url hash="9365f7cf">2024.acl-long.262</url>
      <bibkey>he-etal-2024-planning</bibkey>
      <doi>10.18653/v1/2024.acl-long.262</doi>
    </paper>
    <paper id="263">
      <title>Spectral Filters, Dark Signals, and Attention Sinks</title>
      <author><first>Nicola</first><last>Cancedda</last><affiliation>Meta</affiliation></author>
      <pages>4792-4808</pages>
      <abstract>Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens (Nostalgebraist). We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands. We find that the signals exchanged in the tail end of the spectrum, i.e. corresponding to the singular vectors with smallest singular values, are responsible for attention sinking (Xiao et al., 2023), of which we provide an explanation. We find that the negative log-likelihood of pretrained models can be kept low despite suppressing sizeable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved. Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum, and likely act as additional attention sinks.</abstract>
      <url hash="8827a72c">2024.acl-long.263</url>
      <bibkey>cancedda-2024-spectral</bibkey>
      <doi>10.18653/v1/2024.acl-long.263</doi>
    </paper>
    <paper id="264">
      <title><fixed-case>D</fixed-case>iffu<fixed-case>COMET</fixed-case>: Contextual Commonsense Knowledge Diffusion</title>
      <author><first>Silin</first><last>Gao</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Mete</first><last>Ismayilzada</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Mengjie</first><last>Zhao</last><affiliation>Sony</affiliation></author>
      <author><first>Hiromi</first><last>Wakaki</last><affiliation>Sony Group Corporation</affiliation></author>
      <author><first>Yuki</first><last>Mitsufuji</last><affiliation>Sony AI, Sony Group Corporation, Tokyo Institute of Technology, Tokyo Institute of Technology and Sony Group Corporation</affiliation></author>
      <author><first>Antoine</first><last>Bosselut</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <pages>4809-4831</pages>
      <abstract>Inferring contextually-relevant and diverse commonsense to understand narratives remains challenging for knowledge models. In this work, we develop a series of knowledge models, DiffuCOMET, that leverage diffusion to learn to reconstruct the implicit semantic connections between narrative contexts and relevant commonsense knowledge. Across multiple diffusion steps, our method progressively refines a representation of commonsense facts that is anchored to a narrative, producing contextually-relevant and diverse commonsense inferences for an input context. To evaluate DiffuCOMET, we introduce new metrics for commonsense inference that more closely measure knowledge diversity and contextual relevance. Our results on two different benchmarks, ComFact and WebNLG+, show that knowledge generated by DiffuCOMET achieves a better trade-off between commonsense diversity, contextual relevance and alignment to known gold references, compared to baseline knowledge models.</abstract>
      <url hash="5f05ae2a">2024.acl-long.264</url>
      <bibkey>gao-etal-2024-diffucomet</bibkey>
      <doi>10.18653/v1/2024.acl-long.264</doi>
    </paper>
    <paper id="265">
      <title>Systematic Task Exploration with <fixed-case>LLM</fixed-case>s: A Study in Citation Text Generation</title>
      <author><first>Furkan</first><last>Şahinuç</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Ilia</first><last>Kuznetsov</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Yufang</first><last>Hou</last><affiliation>Technische Universität Darmstadt and IBM Research Ireland</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>4832-4855</pages>
      <abstract>Large language models (LLMs) bring unprecedented flexibility in defining and executing complex, creative natural language generation (NLG) tasks. Yet, this flexibility brings new challenges, as it introduces new degrees of freedom in formulating the task inputs and instructions and in evaluating model performance. To facilitate the exploration of creative NLG tasks, we propose a three-component research framework that consists of systematic input manipulation, reference data, and output measurement. We use this framework to explore citation text generation – a popular scholarly NLP task that lacks consensus on the task definition and evaluation metric and has not yet been tackled within the LLM paradigm. Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs, and reveal non-trivial relationships between different evaluation metrics used for citation text generation. Additional human generation and human evaluation experiments provide new qualitative insights into the task to guide future research in citation text generation. We make our code and data publicly available.</abstract>
      <url hash="ce543161">2024.acl-long.265</url>
      <bibkey>sahinuc-etal-2024-systematic</bibkey>
      <doi>10.18653/v1/2024.acl-long.265</doi>
    </paper>
    <paper id="266">
      <title>Limits of Theory of Mind Modelling in Dialogue-Based Collaborative Plan Acquisition</title>
      <author><first>Matteo</first><last>Bortoletto</last></author>
      <author><first>Constantin</first><last>Ruhdorfer</last><affiliation>Universität Stuttgart</affiliation></author>
      <author><first>Adnen</first><last>Abdessaied</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Lei</first><last>Shi</last><affiliation>Universität Stuttgart</affiliation></author>
      <author><first>Andreas</first><last>Bulling</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>4856-4871</pages>
      <abstract>Recent work on dialogue-based collaborative plan acquisition (CPA) has suggested that Theory of Mind (ToM) modelling can improve missing knowledge prediction in settings with asymmetric skill-sets and knowledge. Although ToM was claimed to be important for effective collaboration, its real impact on this novel task remains under-explored. By representing plans as graphs and by exploiting task-specific constraints we show that, as performance on CPA nearly doubles when predicting one’s own missing knowledge, the improvements due to ToM modelling diminish. This phenomenon persists even when evaluating existing baseline methods. To better understand the relevance of ToM for CPA, we report a principled performance comparison of models with and without ToM features. Results across different models and ablations consistently suggest that learned ToM features are indeed more likely to reflect latent patterns in the data with no perceivable link to ToM. This finding calls for a deeper understanding of the role of ToM in CPA and beyond, as well as new methods for modelling and evaluating mental states in computational collaborative agents.</abstract>
      <url hash="af32b132">2024.acl-long.266</url>
      <bibkey>bortoletto-etal-2024-limits</bibkey>
      <doi>10.18653/v1/2024.acl-long.266</doi>
    </paper>
    <paper id="267">
      <title>Temporal Knowledge Question Answering via Abstract Reasoning Induction</title>
      <author><first>Ziyang</first><last>Chen</last></author>
      <author><first>Dongfang</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xiang</first><last>Zhao</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Baotian</first><last>Hu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>4872-4889</pages>
      <abstract>In this study, we address the challenge of enhancing temporal knowledge reasoning in Large Language Models (LLMs). LLMs often struggle with this task, leading to the generation of inaccurate or misleading responses. This issue mainly arises from their limited ability to handle evolving factual knowledge and complex temporal logic. To overcome these limitations, we propose Abstract Reasoning Induction (ARI) framework, which divides temporal reasoning into two distinct phases: Knowledge agnostic and Knowledge-based. This framework offers factual knowledge support to LLMs while minimizing the incorporation of extraneous noisy data. Concurrently, informed by the principles of constructivism, ARI provides LLMs the capability to engage in proactive, self-directed learning from both correct and incorrect historical reasoning samples. By teaching LLMs to actively construct knowledge and methods, it can significantly boosting their temporal reasoning abilities. Our approach achieves significant improvements, with relative gains of 29.7% and 9.27% on two temporal QA datasets, underscoring its efficacy in advancing temporal reasoning in LLMs. The code can be found at https: //github.com/czy1999/ARI-QA.</abstract>
      <url hash="33c7ec90">2024.acl-long.267</url>
      <bibkey>chen-etal-2024-temporal</bibkey>
      <doi>10.18653/v1/2024.acl-long.267</doi>
    </paper>
    <paper id="268">
      <title>Who Wrote this Code? Watermarking for Code Generation</title>
      <author><first>Taehyun</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seokhee</first><last>Hong</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Jaewoo</first><last>Ahn</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Ilgee</first><last>Hong</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Hwaran</first><last>Lee</last><affiliation>NAVER AI Lab</affiliation></author>
      <author><first>Sangdoo</first><last>Yun</last><affiliation>NAVER</affiliation></author>
      <author><first>Jamin</first><last>Shin</last><affiliation>NAVER</affiliation></author>
      <author><first>Gunhee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>4890-4911</pages>
      <abstract>Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to detect machine-generated text by embedding watermarks are being developed.However, we discover that the existing works fail to function appropriately in code generation tasks due to the task’s nature of having low entropy.Extending a logit-modifying watermark method, we propose Selective WatErmarking via Entropy Thresholding (SWEET), which enhances detection ability and mitigates code quality degeneration by removing low-entropy segments at generating and detecting watermarks.Our experiments show that SWEET significantly improves code quality preservation while outperforming all baselines, including post-hoc detection methods, in detecting machine-generated code text.Our code is available inhttps://github.com/hongcheki/sweet-watermark.</abstract>
      <url hash="9c296fda">2024.acl-long.268</url>
      <bibkey>lee-etal-2024-wrote</bibkey>
      <doi>10.18653/v1/2024.acl-long.268</doi>
    </paper>
    <paper id="269">
      <title><fixed-case>M</fixed-case>ap<fixed-case>C</fixed-case>oder: Multi-Agent Code Generation for Competitive Problem Solving</title>
      <author><first>Md. Ashraful</first><last>Islam</last><affiliation>Bangladesh University of Engineering and Technology and Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Mohammed Eunus</first><last>Ali</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Md Rizwan</first><last>Parvez</last><affiliation>Qatar Computing Research Institute and Bosch</affiliation></author>
      <pages>4912-4944</pages>
      <abstract>Code synthesis, which requires a deep understanding of complex natural language (NL) problem descriptions, generation of code instructions for complex algorithms and data structures, and the successful execution of comprehensive unit tests, presents a significant challenge. Thus, while large language models (LLMs) demonstrate impressive proficiency in natural language processing (NLP), their performance in code generation tasks remains limited. In this paper, we introduce a new approach to code generation tasks leveraging the multi-agent prompting that uniquely replicates the full cycle of program synthesis as observed in human developers. Our framework, MapCoder, consists of four LLM agents specifically designed to emulate the stages of this cycle: recalling relevant examples, planning, code generation, and debugging. After conducting thorough experiments, with multiple LLMs ablations and analyses across eight challenging competitive problem-solving and program synthesis benchmarks—MapCoder showcases remarkable code generation capabilities, achieving their new state-of-the-art (pass@1) results—(HumanEval 93.9%, MBPP 83.1%, APPS 22.0%, CodeContests 28.5%, and xCodeEval 45.3%). Moreover, our method consistently delivers superior performance across various programming languages and varying problem difficulties. We open-source our framework at https://github.com/Md-Ashraful-Pramanik/MapCoder.</abstract>
      <url hash="cf50dcd7">2024.acl-long.269</url>
      <bibkey>islam-etal-2024-mapcoder</bibkey>
      <doi>10.18653/v1/2024.acl-long.269</doi>
    </paper>
    <paper id="270">
      <title><fixed-case>R</fixed-case>elay<fixed-case>A</fixed-case>ttention for Efficient Large Language Model Serving with Long System Prompts</title>
      <author><first>Lei</first><last>Zhu</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Xinjiang</first><last>Wang</last><affiliation>SenseTime Group</affiliation></author>
      <author><first>Wayne</first><last>Zhang</last><affiliation>SenseTime Research</affiliation></author>
      <author><first>Rynson</first><last>Lau</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>4945-4957</pages>
      <abstract>A practical large language model (LLM) service may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens. RelayAttention is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention. We have observed significant performance improvements to a production-level system, vLLM, through integration with RelayAttention. The improvements are even more profound with longer system prompts.</abstract>
      <url hash="af4bdfcd">2024.acl-long.270</url>
      <bibkey>zhu-etal-2024-relayattention</bibkey>
      <doi>10.18653/v1/2024.acl-long.270</doi>
    </paper>
    <paper id="271">
      <title>Boosting Language Models Reasoning with Chain-of-Knowledge Prompting</title>
      <author><first>Jianing</first><last>Wang</last></author>
      <author><first>Qiushi</first><last>Sun</last><affiliation>University of Hong Kong and Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiang</first><last>Li</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>4958-4981</pages>
      <abstract>Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like “Let’s think step by step” or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with hallucinations, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce an F2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.</abstract>
      <url hash="969961db">2024.acl-long.271</url>
      <bibkey>wang-etal-2024-boosting-language</bibkey>
      <doi>10.18653/v1/2024.acl-long.271</doi>
    </paper>
    <paper id="272">
      <title>Open Grounded Planning: Challenges and Benchmark Construction</title>
      <author><first>Shiguang</first><last>Guo</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Ziliang</first><last>Deng</last></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>4982-5003</pages>
      <abstract>The emergence of large language models (LLMs) has increasingly drawn attention to the use of LLMs for human-like planning. Existing work on LLM-based planning either focuses on leveraging the inherent language generation capabilities of LLMs to produce free-style plans, or employs reinforcement learning approaches to learn decision-making for a limited set of actions within restricted environments. However, both approaches exhibit significant discrepancies from the open and executable requirements in real-world planning. In this paper, we propose a new planning task–open grounded planning. The primary objective of open grounded planning is to ask the model to generate an executable plan based on a variable action set, thereby ensuring the executability of the produced plan. To this end, we establishes a benchmark for open grounded planning spanning a wide range of domains. Then we test current state-of-the-art LLMs along with five planning approaches, revealing that existing LLMs and methods still struggle to address the challenges posed by grounded planning in open domains. The outcomes of this paper define and establish a foundational dataset for open grounded planning, and shed light on the potential challenges and future directions of LLM-based planning.</abstract>
      <url hash="b8e4a781">2024.acl-long.272</url>
      <bibkey>guo-etal-2024-open</bibkey>
      <doi>10.18653/v1/2024.acl-long.272</doi>
    </paper>
    <paper id="273">
      <title><fixed-case>LLM</fixed-case> Knows Body Language, Too: Translating Speech Voices into Human Gestures</title>
      <author><first>Chenghao</first><last>Xu</last><affiliation>Xi’an University of Electronic Science and Technology</affiliation></author>
      <author><first>Guangtao</first><last>Lyu</last></author>
      <author><first>Jiexi</first><last>Yan</last></author>
      <author><first>Muli</first><last>Yang</last><affiliation>A*STAR</affiliation></author>
      <author><first>Cheng</first><last>Deng</last><affiliation>Xidian University</affiliation></author>
      <pages>5004-5013</pages>
      <abstract>In response to the escalating demand for digital human representations, progress has been made in the generation of realistic human gestures from given speeches. Despite the remarkable achievements of recent research, the generation process frequently includes unintended, meaningless, or non-realistic gestures. To address this challenge, we propose a gesture translation paradigm, GesTran, which leverages large language models (LLMs) to deepen the understanding of the connection between speech and gesture and sequentially generates human gestures by interpreting gestures as a unique form of body language. The primary stage of the proposed framework employs a transformer-based auto-encoder network to encode human gestures into discrete symbols. Following this, the subsequent stage utilizes a pre-trained LLM to decipher the relationship between speech and gesture, translating the speech into gesture by interpreting the gesture as unique language tokens within the LLM. Our method has demonstrated state-of-the-art performance improvement through extensive and impartial experiments conducted on public TED and TED-Expressive datasets.</abstract>
      <url hash="9170aa08">2024.acl-long.273</url>
      <bibkey>xu-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.acl-long.273</doi>
    </paper>
    <paper id="274">
      <title><fixed-case>Q</fixed-case>uery<fixed-case>A</fixed-case>gent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction</title>
      <author><first>Xiang</first><last>Huang</last></author>
      <author><first>Sitao</first><last>Cheng</last></author>
      <author><first>Shanshan</first><last>Huang</last><affiliation>nanjing university</affiliation></author>
      <author><first>Jiayu</first><last>Shen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yong</first><last>Xu</last></author>
      <author><first>Chaoyun</first><last>Zhang</last></author>
      <author><first>Yuzhong</first><last>Qu</last><affiliation>Nanjing University</affiliation></author>
      <pages>5014-5035</pages>
      <abstract>Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs stepwise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 5.7 and 15.0 points. Furthermore, our approach exhibits superiority in terms of efficiency, including run-time, query overhead, and API invocation costs. By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, validating the strong transferability of our approach.</abstract>
      <url hash="5a198f37">2024.acl-long.274</url>
      <bibkey>huang-etal-2024-queryagent</bibkey>
      <doi>10.18653/v1/2024.acl-long.274</doi>
    </paper>
    <paper id="275">
      <title><fixed-case>PITA</fixed-case>: Prompting Task Interaction for Argumentation Mining</title>
      <author><first>Yang</first><last>Sun</last></author>
      <author><first>Muyi</first><last>Wang</last></author>
      <author><first>Jianzhu</first><last>Bao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Xiaoyan</first><last>Zhao</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Caihua</first><last>Yang</last></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>5036-5049</pages>
      <abstract>Argumentation mining (AM) aims to detect the arguments and their inherent relations from argumentative textual compositions. Generally, AM comprises three key challenging subtasks, including argument component type classification (ACTC), argumentative relation identification (ARI), and argumentative relation type classification (ARTC). Prior methods are afflicted by a sequential feature decoding paradigm, wherein they initially address the features of argumentation components (ACs) for the task of ACTC. Then, these features are amalgamated in pairs to tackle the task of ARI. Finally, the AC pairs and ascertained pertinent relations are employed for ARTC. However, the explicit and comprehensive inter-relationship among the three subtasks is neglected. In this paper, we propose a novel method PITA for PromptIng Task interAction to model the inter-relationships among the three subtasks within a generative framework. Specifically, we employ a dynamic prompt template to indicate all ACs and AC pairs in the three subtasks. Then, from a multi-relational perspective, we construct an undirected heterogeneous graph to capture the various relationships within and between ACs and AC pairs. We apply the Relational Graph Convolutional Network (RGCN) on the graph and inject the task interaction information into the soft prompts with continuous representations. PITA jointly decodes all ACs and AC pairs using the prompt template with task interaction information, which thus explicitly and comprehensively harmonizes the information propagation across the three subtasks. Extensive experiments show PITA achieves state-of-the-art performances on two AM benchmarks.</abstract>
      <url hash="23ed8458">2024.acl-long.275</url>
      <bibkey>sun-etal-2024-pita</bibkey>
      <doi>10.18653/v1/2024.acl-long.275</doi>
    </paper>
    <paper id="276">
      <title>Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models</title>
      <author><first>Jinhao</first><last>Duan</last><affiliation>Drexel University</affiliation></author>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Shiqi</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Alex</first><last>Zavalny</last></author>
      <author><first>Chenan</first><last>Wang</last></author>
      <author><first>Renjing</first><last>Xu</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Bhavya</first><last>Kailkhura</last><affiliation>Lawrence Livermore National Laboratory</affiliation></author>
      <author><first>Kaidi</first><last>Xu</last><affiliation>Drexel University</affiliation></author>
      <pages>5050-5063</pages>
      <abstract>Large Language Models (LLMs) show promising results in language generation and instruction following but frequently “hallucinate”, making their outputs less reliable. Despite Uncertainty Quantification’s (UQ) potential solutions, implementing it accurately within LLMs is challenging. Our research introduces a simple heuristic: not all tokens in auto-regressive LLM text equally represent the underlying meaning, as “linguistic redundancy” often allows a few keywords to convey the essence of long sentences. However, current methods underestimate this inequality when assessing uncertainty, causing tokens with limited semantics to be equally or excessively weighted in UQ. To correct this, we propose Shifting Attention to more Relevant (SAR) components at both token- and sentence-levels for better UQ. We conduct extensive experiments involving a range of popular “off-the-shelf” LLMs, such as Vicuna, WizardLM, and LLaMA-2-chat, with model sizes extending up to 33B parameters. We evaluate various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&amp;A, and medical Q&amp;A. Our experimental results, coupled with a comprehensive demographic analysis, demonstrate the superior performance of SAR. The code is available at https://github.com/jinhaoduan/SAR.</abstract>
      <url hash="58c05b88">2024.acl-long.276</url>
      <bibkey>duan-etal-2024-shifting</bibkey>
      <doi>10.18653/v1/2024.acl-long.276</doi>
    </paper>
    <paper id="277">
      <title>Babel-<fixed-case>I</fixed-case>mage<fixed-case>N</fixed-case>et: Massively Multilingual Evaluation of Vision-and-Language Representations</title>
      <author><first>Gregor</first><last>Geigle</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Radu</first><last>Timofte</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg and ETH Zurich</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <pages>5064-5084</pages>
      <abstract>Vision-and-language (VL) models with separate encoders for each modality (e.g., CLIP) have become the go-to models for zero-shot image classification and image-text retrieval. They are, however, mostly evaluated in English as multilingual benchmarks are limited in availability. We introduce Babel-ImageNet, a massively multilingual benchmark that offers (partial) translations of ImageNet labels to 100 languages, built without machine translation or manual annotation. We instead automatically obtain reliable translations by linking them – via shared WordNet synsets – to BabelNet, a massively multilingual lexico-semantic network. We evaluate 11 public multilingual CLIP models on zero-shot image classification (ZS-IC) on our benchmark, demonstrating a significant gap between English ImageNet performance and that of high-resource languages (e.g., German or Chinese), and an even bigger gap for low-resource languages (e.g., Sinhala or Lao). Crucially, we show that the models’ ZS-IC performance highly correlates with their performance in image-text retrieval, validating the use of Babel-imageNet to evaluate multilingual models for the vast majority of languages without gold image-text data. Finally, we show that the performance of multilingual CLIP can be drastically improved for low-resource languages with parameter-efficient language-specific training. We make our code and data publicly available: <url>https://github.com/gregor-ge/Babel-ImageNet</url></abstract>
      <url hash="34f9b42d">2024.acl-long.277</url>
      <bibkey>geigle-etal-2024-babel</bibkey>
      <doi>10.18653/v1/2024.acl-long.277</doi>
    </paper>
    <paper id="278">
      <title>Estimating Agreement by Chance for Sequence Annotation</title>
      <author><first>Diya</first><last>Li</last><affiliation>Freenome</affiliation></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Ao</first><last>Yuan</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Chunxiao</first><last>Zhou</last><affiliation>National Institutes of Health</affiliation></author>
      <pages>5085-5097</pages>
      <abstract>In the field of natural language processing, correction of performance assessment for chance agreement plays a crucial role in evaluating the reliability of annotations. However, there is a notable dearth of research focusing on chance correction for assessing the reliability of sequence annotation tasks, despite their widespread prevalence in the field. To address this gap, this paper introduces a novel model for generating random annotations, which serves as the foundation for estimating chance agreement in sequence annotation tasks. Utilizing the proposed randomization model and a related comparison approach, we successfully derive the analytical form of the distribution, enabling the computation of the probable location of each annotated text segment and subsequent chance agreement estimation. Through a combination simulation and corpus-based evaluation, we successfully assess its applicability and validate its accuracy and efficacy.</abstract>
      <url hash="0b4cbe30">2024.acl-long.278</url>
      <bibkey>li-etal-2024-estimating</bibkey>
      <doi>10.18653/v1/2024.acl-long.278</doi>
    </paper>
    <paper id="279">
      <title>Are Emergent Abilities in Large Language Models just In-Context Learning?</title>
      <author><first>Sheng</first><last>Lu</last></author>
      <author><first>Irina</first><last>Bigoulaeva</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Rachneet</first><last>Sachdeva</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last><affiliation>University of Bath</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>5098-5139</pages>
      <abstract>Large language models, comprising billions of parameters and pre-trained on extensive web-scale corpora, have been claimed to acquire certain capabilities without having been specifically trained on them. These capabilities, referred to as “emergent abilities,” have been a driving force in discussions regarding the potentials and risks of language models. A key challenge in evaluating emergent abilities is that they are confounded by model competencies that arise through alternative prompting techniques, including in-context learning, which is the ability of models to complete a task based on a few examples. We present a novel theory that explains emergent abilities, taking into account their potential confounding factors, and rigorously substantiate this theory through over 1000 experiments. Our findings suggest that purported emergent abilities are not truly emergent, but result from a combination of in-context learning, model memory, and linguistic knowledge. Our work is a foundational step in explaining language model performance, providing a template for their efficient use and clarifying the paradox of their ability to excel in some instances while faltering in others. Thus, we demonstrate that their capabilities should not be overestimated.</abstract>
      <url hash="a29c5c91">2024.acl-long.279</url>
      <bibkey>lu-etal-2024-emergent</bibkey>
      <doi>10.18653/v1/2024.acl-long.279</doi>
    </paper>
    <paper id="280">
      <title><fixed-case>W</fixed-case>ave<fixed-case>C</fixed-case>oder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning</title>
      <author><first>Zhaojian</first><last>Yu</last></author>
      <author><first>Xin</first><last>Zhang</last></author>
      <author><first>Ning</first><last>Shang</last></author>
      <author><first>Yangyu</first><last>Huang</last></author>
      <author><first>Can</first><last>Xu</last><affiliation>Microsoft and Peking University</affiliation></author>
      <author><first>Yishujie</first><last>Zhao</last></author>
      <author><first>Wenxiang</first><last>Hu</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Qiufeng</first><last>Yin</last><affiliation>Microsoft</affiliation></author>
      <pages>5140-5153</pages>
      <abstract>Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeOcean, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks.</abstract>
      <url hash="180700fc">2024.acl-long.280</url>
      <bibkey>yu-etal-2024-wavecoder</bibkey>
      <doi>10.18653/v1/2024.acl-long.280</doi>
    </paper>
    <paper id="281">
      <title>Eliciting Better Multilingual Structured Reasoning from <fixed-case>LLM</fixed-case>s through Code</title>
      <author><first>Bryan</first><last>Li</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Tamer</first><last>Alkhouli</last><affiliation>Amazon</affiliation></author>
      <author><first>Daniele</first><last>Bonadiman</last><affiliation>Amazon</affiliation></author>
      <author><first>Nikolaos</first><last>Pappas</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Saab</first><last>Mansour</last><affiliation>Amazon</affiliation></author>
      <pages>5154-5169</pages>
      <abstract>The development of large language models (LLM) has shown progress on reasoning, though studies have largely considered either English or simple reasoning tasks. To address this, we introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks.We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multilingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reasoning subtask. Furthermore, the models show no regression on non-reasoning tasks, thus demonstrating our techniques maintain general-purpose abilities.</abstract>
      <url hash="f5318264">2024.acl-long.281</url>
      <bibkey>li-etal-2024-eliciting-better</bibkey>
      <doi>10.18653/v1/2024.acl-long.281</doi>
    </paper>
    <paper id="282">
      <title><fixed-case>OLIVE</fixed-case>: Object Level In-Context Visual Embeddings</title>
      <author><first>Timothy</first><last>Ossowski</last></author>
      <author><first>Junjie</first><last>Hu</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <pages>5170-5185</pages>
      <abstract>Recent generalist vision-language models (VLMs) have demonstrated impressive reasoning capabilities across diverse multimodal tasks. However, these models still struggle with fine-grained object-level understanding and grounding. In terms of modeling, existing VLMs implicitly align text tokens with image patch tokens, which is ineffective for embedding alignment at the same granularity and inevitably introduces noisy spurious background features. Additionally, these models struggle when generalizing to unseen visual concepts and may not be reliable for domain-specific tasks without further fine-tuning. To address these limitations, we propose a novel method to prompt large language models with in-context visual object vectors, thereby enabling controllable object-level reasoning. This eliminates the necessity of fusing a lengthy array of image patch features and significantly speeds up training. Furthermore, we propose region-level retrieval using our object representations, facilitating rapid adaptation to new objects without additional training. Our experiments reveal that our method achieves competitive referring object classification and captioning performance, while also offering zero-shot generalization and robustness to visually challenging contexts.</abstract>
      <url hash="b74fa85b">2024.acl-long.282</url>
      <bibkey>ossowski-hu-2024-olive</bibkey>
      <doi>10.18653/v1/2024.acl-long.282</doi>
    </paper>
    <paper id="283">
      <title>Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness</title>
      <author><first>Jiuhai</first><last>Chen</last></author>
      <author><first>Jonas</first><last>Mueller</last><affiliation>Cleanlab</affiliation></author>
      <pages>5186-5200</pages>
      <abstract>We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, whose training data remains unknown. By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without extra training steps. In applications involving automated evaluation with LLMs, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both GPT 3.5 and 4).</abstract>
      <url hash="4edd940e">2024.acl-long.283</url>
      <bibkey>chen-mueller-2024-quantifying</bibkey>
      <doi>10.18653/v1/2024.acl-long.283</doi>
    </paper>
    <paper id="284">
      <title>Marathon: A Race Through the Realm of Long Context with Large Language Models</title>
      <author><first>Lei</first><last>Zhang</last><affiliation>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yunshui</first><last>Li</last></author>
      <author><first>Ziqiang</first><last>Liu</last></author>
      <author><first>Jiaxi</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Junhao</first><last>Liu</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Longze</first><last>Chen</last></author>
      <author><first>Run</first><last>Luo</last></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <pages>5201-5217</pages>
      <abstract>With the advancement of large language models (LLMs) and the expansion of their context windows, existing long-context benchmarks fall short in effectively evaluating the models’ comprehension and reasoning abilities in extended texts. Moreover, conventional benchmarks relying on F1 metrics often inaccurately score responses: they may undervalue correct answers that differ from the reference responses and overvalue incorrect ones that resemble the reference texts. In response to these limitations, we introduce Marathon, a novel evaluation benchmark that adopts a multiple-choice question format. It is specifically designed to overcome the constraints of previous benchmarks and provide a rapid, precise, and unbiased appraisal of the long-context comprehension skills of large language models. We conducted comprehensive evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and assessed the effectiveness of various optimization strategies tailored for long-context generation. We anticipate that the Marathon benchmark and its associated leaderboard will enable a more precise and equitable evaluation of LLMs’ capabilities in understanding and reasoning over extended contexts.</abstract>
      <url hash="c79a7ddb">2024.acl-long.284</url>
      <bibkey>zhang-etal-2024-marathon</bibkey>
      <doi>10.18653/v1/2024.acl-long.284</doi>
    </paper>
    <paper id="285">
      <title>Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph</title>
      <author><first>Xiaochen</first><last>Gao</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Feng</first><last>Yao</last></author>
      <author><first>Kewen</first><last>Zhao</last></author>
      <author><first>Beilei</first><last>He</last></author>
      <author><first>Animesh</first><last>Kumar</last></author>
      <author><first>Vish</first><last>Krishnan</last></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>5218-5234</pages>
      <abstract>Model scaling is becoming the default choice for many language tasks due to the success of large language models (LLMs). However, it can fall short in specific scenarios where simple customized methods excel. In this paper, we delve into the patent approval prediction task and unveil that simple domain-specific graph methods outperform enlarging the model, using the intrinsic dependencies within the patent data. Specifically, we first extend the embedding-based state-of-the-art (SOTA) by scaling up its backbone model with various sizes of open-source LLMs, then explore prompt-based methods to harness proprietary LLMs’ potential, but find the best results close to random guessing, underlining the ineffectiveness of model scaling-up. Hence, we propose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous patent data analyses, capturing the inherent dependencies across segments of the patent text. As it is model-agnostic, we apply cost-effective graph models to our FLAN Graph to obtain representations for approval prediction. Extensive experiments and detailed analyses prove that incorporating FLAN Graph via various graph models consistently outperforms all LLM baselines significantly. We hope that our observations and analyses in this paper can bring more attention to this challenging task and prompt further research into the limitations of LLMs.</abstract>
      <url hash="699962d8">2024.acl-long.285</url>
      <bibkey>gao-etal-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.acl-long.285</doi>
    </paper>
    <paper id="286">
      <title><fixed-case>PCAD</fixed-case>: Towards <fixed-case>ASR</fixed-case>-Robust Spoken Language Understanding via Prototype Calibration and Asymmetric Decoupling</title>
      <author><first>Xianwei</first><last>Zhuang</last></author>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Liming</first><last>Liang</last></author>
      <author><first>Yuxin</first><last>Xie</last></author>
      <author><first>Zhichang</first><last>Wang</last></author>
      <author><first>Zhiqi</first><last>Huang</last><affiliation>Tencent Game</affiliation></author>
      <author><first>Yuexian</first><last>Zou</last><affiliation>Peking University</affiliation></author>
      <pages>5235-5246</pages>
      <abstract>Spoken language understanding (SLU) inevitably suffers from error propagation from automatic speech recognition (ASR) in actual scenarios. Some recent works attempt to alleviate this issue through contrastive learning. However, they (1) sample negative pairs incorrectly in pre-training; (2) only focus on implicit metric learning while neglecting explicit erroneous predictions; (3) treat manual and ASR transcripts indiscriminately. In this paper, we propose a novel framework termed <tex-math>\textbf{PCAD}</tex-math>, which can calibrate bias and errors and achieve adaptive-balanced decoupling training. Specifically, PCAD utilizes a prototype-based loss to aggregate label and prediction priors and calibrate bias and error-prone semantics for better inter-class discrimination and intra-class consistency. We theoretically analyze the effect of this loss on robustness enhancement. Further, we leverage a teacher-student model for asymmetric decoupling training between different transcripts and formulate a novel gradient-sensitive exponential moving averaging (GS-EMA) algorithm for adaptive balance of accuracy and robustness. Experiments on three datasets show that PCAD significantly outperforms existing approaches and achieves new state-of-the-art performance.</abstract>
      <url hash="1d4f6bc8">2024.acl-long.286</url>
      <bibkey>zhuang-etal-2024-pcad</bibkey>
      <doi>10.18653/v1/2024.acl-long.286</doi>
    </paper>
    <paper id="287">
      <title>Rethinking the Multimodal Correlation of Multimodal Sequential Learning via Generalizable Attentional Results Alignment</title>
      <author><first>Tao</first><last>Jin</last></author>
      <author><first>Wang</first><last>Lin</last></author>
      <author><first>Ye</first><last>Wang</last></author>
      <author><first>Linjun</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xize</first><last>Cheng</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>5247-5265</pages>
      <abstract>Transformer-based methods have gone mainstream in multimodal sequential learning. The intra and inter modality interactions are captured by the query-key associations of multi-head attention. In this way, the calculated multimodal contexts (attentional results) are expected to be relevant to the query modality. However, in existing literature, the alignment degree between different calculated attentional results of the same query are under-explored. Based on this concern, we propose a new constrained scheme called Multimodal Contextual Contrast (MCC), which could align the multiple attentional results from both local and global perspectives, making the information capture more efficient. Concretely, the calculated attentional results of different modalities are mapped into a common feature space, those attentional vectors with the same query are considered as a positive group and the remaining sets are negative. From local perspective, we sample the negative groups for a positive group by randomly changing the sequential step of one specific context and keeping the other stay the same. From coarse global perspective, we divide all the contextual groups into two sets (i.e., aligned and unaligned), making the total score of aligned group relatively large. We extend the vectorial inner product operation for more input and calculate the aligned score for each multimodal group. Considering that the computational complexity scales exponentially to the number of modalities, we adopt stochastic expectation approximation (SEA) for the real process. The extensive experimental results on several tasks reveal the effectiveness of our contributions.</abstract>
      <url hash="48035ced">2024.acl-long.287</url>
      <bibkey>jin-etal-2024-rethinking</bibkey>
      <doi>10.18653/v1/2024.acl-long.287</doi>
    </paper>
    <paper id="288">
      <title><fixed-case>UHGE</fixed-case>val: Benchmarking the Hallucination of <fixed-case>C</fixed-case>hinese Large Language Models via Unconstrained Generation</title>
      <author><first>Xun</first><last>Liang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Shichao</first><last>Song</last></author>
      <author><first>Simin</first><last>Niu</last></author>
      <author><first>Zhiyu</first><last>Li</last></author>
      <author><first>Feiyu</first><last>Xiong</last><affiliation>Institute for Advanced Algorithms Research, Shanghai</affiliation></author>
      <author><first>Bo</first><last>Tang</last></author>
      <author><first>Yezhaohui</first><last>Wang</last><affiliation>Institute for Advanced Algorithms Research, Shanghai</affiliation></author>
      <author><first>Dawei</first><last>He</last></author>
      <author><first>Cheng</first><last>Peng</last></author>
      <author><first>Zhonghao</first><last>Wang</last></author>
      <author><first>Haiying</first><last>Deng</last></author>
      <pages>5266-5293</pages>
      <abstract>Large language models (LLMs) produce hallucinated text, compromising their practical utility in professional contexts. To assess the reliability of LLMs, numerous initiatives have developed benchmark evaluations for hallucination phenomena. However, they often employ constrained generation techniques to produce the evaluation dataset due to cost and time limitations. For instance, this may involve employing directed hallucination induction or deliberately modifying authentic text to generate hallucinations. These are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, containing hallucinations generated by LLMs with minimal restrictions. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also evaluated prominent Chinese LLMs and the GPT series models to derive insights regarding hallucination.</abstract>
      <url hash="acb8fbc0">2024.acl-long.288</url>
      <bibkey>liang-etal-2024-uhgeval</bibkey>
      <doi>10.18653/v1/2024.acl-long.288</doi>
    </paper>
    <paper id="289">
      <title><fixed-case>P</fixed-case>re<fixed-case>FLMR</fixed-case>: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers</title>
      <author><first>Weizhe</first><last>Lin</last></author>
      <author><first>Jingbiao</first><last>Mei</last></author>
      <author><first>Jinghong</first><last>Chen</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last><affiliation>Amazon and University of Cambridge</affiliation></author>
      <pages>5294-5316</pages>
      <abstract>Large Multimodal Models (LMMs) excel in natural language and visual understanding but are challenged by exacting tasks such as Knowledge-based Visual Question Answering (KB-VQA) which involve the retrieval of relevant information from document collections to use in shaping answers to questions. We present an extensive training and evaluation framework, M2KR, for KB-VQA. M2KR contains a collection of vision and language tasks which we have incorporated into a single suite of benchmark tasks for training and evaluating general-purpose multi-modal retrievers. We use M2KR to develop PreFLMR, a pre-trained version of the recently developed Fine-grained Late-interaction Multi-modal Retriever (FLMR) approach to KB-VQA, and we report new state-of-the-art results across a range of tasks. We also present investigations into the scaling behaviors of PreFLMR intended to be useful in future developments in general-purpose multi-modal retrievers.</abstract>
      <url hash="de51615a">2024.acl-long.289</url>
      <bibkey>lin-etal-2024-preflmr</bibkey>
      <doi>10.18653/v1/2024.acl-long.289</doi>
    </paper>
    <paper id="290">
      <title>Triple-Encoders: Representations That Fire Together, Wire Together</title>
      <author><first>Justus-Jonas</first><last>Erker</last></author>
      <author><first>Florian</first><last>Mai</last></author>
      <author><first>Nils</first><last>Reimers</last><affiliation>HuggingFace</affiliation></author>
      <author><first>Gerasimos</first><last>Spanakis</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>5317-5332</pages>
      <abstract>Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost.Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency.While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective in a self-organizing manner, without using any weights, i.e., merely through local interactions. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding. Our code (https://github.com/UKPLab/acl2024-triple-encoders) and model (https://huggingface.co/UKPLab/triple-encoders-dailydialog) are publicly available.</abstract>
      <url hash="f5a68917">2024.acl-long.290</url>
      <bibkey>erker-etal-2024-triple</bibkey>
      <doi>10.18653/v1/2024.acl-long.290</doi>
    </paper>
    <paper id="291">
      <title>Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning</title>
      <author><first>Jingbiao</first><last>Mei</last></author>
      <author><first>Jinghong</first><last>Chen</last></author>
      <author><first>Weizhe</first><last>Lin</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last><affiliation>Amazon and University of Cambridge</affiliation></author>
      <author><first>Marcus</first><last>Tomalin</last></author>
      <pages>5333-5347</pages>
      <abstract>Hateful memes have emerged as a significant concern on the Internet. Detecting hateful memes requires the system to jointly understand the visual and textual modalities. Our investigation reveals that the embedding space of existing CLIP-based systems lacks sensitivity to subtle differences in memes that are vital for correct hatefulness classification. We propose constructing a hatefulness-aware embedding space through retrieval-guided contrastive training. Our approach achieves state-of-the-art performance on the HatefulMemes dataset with an AUROC of 87.0, outperforming much larger fine-tuned large multimodal models. We demonstrate a retrieval-based hateful memes detection system, which is capable of identifying hatefulness based on data unseen in training. This allows developers to update the hateful memes detection system by simply adding new examples without retraining — a desirable feature for real services in the constantly evolving landscape of hateful memes on the Internet.</abstract>
      <url hash="b3138f50">2024.acl-long.291</url>
      <bibkey>mei-etal-2024-improving</bibkey>
      <doi>10.18653/v1/2024.acl-long.291</doi>
    </paper>
    <paper id="292">
      <title>Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization</title>
      <author><first>Wenqi</first><last>Zhang</last></author>
      <author><first>Ke</first><last>Tang</last></author>
      <author><first>Hai</first><last>Wu</last></author>
      <author><first>Mengna</first><last>Wang</last></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Guiyang</first><last>Hou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zeqi</first><last>Tan</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Yueting</first><last>Zhuang</last></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>5348-5375</pages>
      <abstract>Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, “fine-tuning” its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold’em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.</abstract>
      <url hash="a2d569db">2024.acl-long.292</url>
      <bibkey>zhang-etal-2024-agent</bibkey>
      <doi>10.18653/v1/2024.acl-long.292</doi>
    </paper>
    <paper id="293">
      <title>Your Transformer is Secretly Linear</title>
      <author><first>Anton</first><last>Razzhigaev</last></author>
      <author><first>Matvey</first><last>Mikhalchuk</last><affiliation>Artificial Intelligence Research Institute (AIRI) and Lomonosov Moscow State University</affiliation></author>
      <author><first>Elizaveta</first><last>Goncharova</last></author>
      <author><first>Nikolai</first><last>Gerasimenko</last></author>
      <author><first>Ivan</first><last>Oseledets</last><affiliation>Artificial Intelligence Research Institute and Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Denis</first><last>Dimitrov</last><affiliation>AIRI and Sber</affiliation></author>
      <author><first>Andrey</first><last>Kuznetsov</last><affiliation>AIRI, Sber and Samara National Research University</affiliation></author>
      <pages>5376-5384</pages>
      <abstract>This paper reveals a novel linear characteristic exclusive to transformer decoders, including models like GPT, LLaMA, OPT, BLOOM and others. We analyze embedding transformations between sequential layers, uncovering an almost perfect linear relationship (Procrustes similarity score of 0.99). However, linearity decreases when the residual component is removed, due to a consistently low transformer layer output norm. Our experiments show that pruning or linearly approximating some of the layers does not impact loss or model performance significantly. Moreover, we introduce a cosine-similarity-based regularization in our pretraining experiments on smaller models, aimed at reducing layer linearity. This regularization not only improves performance metrics on benchmarks like Tiny Stories and SuperGLUE but as well successfully decreases the linearity of the models. This study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed.</abstract>
      <url hash="b7567a72">2024.acl-long.293</url>
      <bibkey>razzhigaev-etal-2024-transformer</bibkey>
      <doi>10.18653/v1/2024.acl-long.293</doi>
    </paper>
    <paper id="294">
      <title>Noise Correction on Subjective Datasets</title>
      <author><first>Uthman</first><last>Jinadu</last></author>
      <author><first>Yi</first><last>Ding</last><affiliation>Georgia State University</affiliation></author>
      <pages>5385-5395</pages>
      <abstract>Incorporating every annotator’s perspective is crucial for unbiased data modeling. Annotator fatigue and changing opinions over time can distort dataset annotations. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, this method provides a controllable way to encourage or discourage disagreement. We demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.</abstract>
      <url hash="941dc220">2024.acl-long.294</url>
      <bibkey>jinadu-ding-2024-noise</bibkey>
      <doi>10.18653/v1/2024.acl-long.294</doi>
    </paper>
    <paper id="295">
      <title>Generative Explore-Exploit: Training-free Optimization of Generative Recommender Systems using <fixed-case>LLM</fixed-case> Optimizers</title>
      <author><first>Lütfi Kerem</first><last>Senel</last><affiliation>The Center for Information and Language Processing</affiliation></author>
      <author><first>Besnik</first><last>Fetahu</last><affiliation>Amazon</affiliation></author>
      <author><first>Davis</first><last>Yoshida</last></author>
      <author><first>Zhiyu</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Giuseppe</first><last>Castellucci</last><affiliation>Amazon</affiliation></author>
      <author><first>Nikhita</first><last>Vedula</last><affiliation>Amazon</affiliation></author>
      <author><first>Jason Ingyu</first><last>Choi</last><affiliation>Amazon</affiliation></author>
      <author><first>Shervin</first><last>Malmasi</last><affiliation>Amazon</affiliation></author>
      <pages>5396-5420</pages>
      <abstract>Recommender systems are widely used to suggest engaging content, and Large Language Models (LLMs) have given rise to generative recommenders. Such systems can directly generate items, including for open-set tasks like question suggestion. While the world knowledge of LLMs enables good recommendations, improving the generated content through user feedback is challenging as continuously fine-tuning LLMs is prohibitively expensive. We present a training-free approach for optimizing generative recommenders by connecting user feedback loops to LLM-based optimizers. We propose a generative explore-exploit method that can not only exploit generated items with known high engagement, but also actively explore and discover hidden population preferences to improve recommendation quality. We evaluate our approach on question generation in two domains (e-commerce and general knowledge), and model user feedback with Click Through Rate (CTR). Experiments show our LLM-based explore-exploit approach can iteratively improve recommendations and consistently increase CTR. Ablation analysis shows that generative exploration is key to learning user preferences, avoiding the pitfalls of greedy exploit-only approaches. A human evaluation strongly supports our quantitative findings.</abstract>
      <url hash="44cf7f36">2024.acl-long.295</url>
      <bibkey>senel-etal-2024-generative</bibkey>
      <doi>10.18653/v1/2024.acl-long.295</doi>
    </paper>
    <paper id="296">
      <title>Instruction-tuned Language Models are Better Knowledge Learners</title>
      <author><first>Zhengbao</first><last>Jiang</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Zhiqing</first><last>Sun</last><affiliation>OpenAI</affiliation></author>
      <author><first>Weijia</first><last>Shi</last></author>
      <author><first>Pedro</first><last>Rodriguez</last><affiliation>Meta FAIR</affiliation></author>
      <author><first>Chunting</first><last>Zhou</last><affiliation>Meta AI</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Xi</first><last>Lin</last><affiliation>Facebook and Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Wen-tau</first><last>Yih</last><affiliation>Meta Platforms, Inc.</affiliation></author>
      <author><first>Srini</first><last>Iyer</last><affiliation>University of Washington, Seattle and Department of Computer Science, University of Washington</affiliation></author>
      <pages>5421-5434</pages>
      <abstract>In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that pre-instruction-tuning significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.</abstract>
      <url hash="6ee869fa">2024.acl-long.296</url>
      <bibkey>jiang-etal-2024-instruction</bibkey>
      <doi>10.18653/v1/2024.acl-long.296</doi>
    </paper>
    <paper id="297">
      <title>What Do Language Models Hear? Probing for Auditory Representations in Language Models</title>
      <author><first>Jerry</first><last>Ngo</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yoon</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>5435-5448</pages>
      <abstract>This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.</abstract>
      <url hash="e9999260">2024.acl-long.297</url>
      <bibkey>ngo-kim-2024-language</bibkey>
      <doi>10.18653/v1/2024.acl-long.297</doi>
    </paper>
    <paper id="298">
      <title>Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs</title>
      <author><first>Zae Myung</first><last>Kim</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Kwang</first><last>Lee</last><affiliation>Kumoh National University of Technology</affiliation></author>
      <author><first>Preston</first><last>Zhu</last></author>
      <author><first>Vipul</first><last>Raheja</last><affiliation>Columbia University, Grammarly and International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Dongyeop</first><last>Kang</last><affiliation>University of Minnesota</affiliation></author>
      <pages>5449-5474</pages>
      <abstract>With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers’ overall performance in distinguishing between human-written and machine-generated texts, even on out-of-distribution and paraphrased samples. This underscores the significance of incorporating hierarchical discourse features in the analysis of text patterns. The code and dataset will be available at [TBA].</abstract>
      <url hash="d9170e2e">2024.acl-long.298</url>
      <bibkey>kim-etal-2024-threads</bibkey>
      <doi>10.18653/v1/2024.acl-long.298</doi>
    </paper>
    <paper id="299">
      <title>Jailbreak Open-Sourced Large Language Models via Enforced Decoding</title>
      <author><first>Hangfan</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Zhimeng</first><last>Guo</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Huaisheng</first><last>Zhu</last></author>
      <author><first>Bochuan</first><last>Cao</last></author>
      <author><first>Lu</first><last>Lin</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Jinyuan</first><last>Jia</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Jinghui</first><last>Chen</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Dinghao</first><last>Wu</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>5475-5493</pages>
      <abstract>Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is “could alignment really prevent those open-sourced large language models from being misused to generate undesired content?”. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation process of open-sourced LLMs to misguide it to generate undesired content including harmful or biased information and even private data. We evaluate our method on 4 open-sourced LLMs accessible publicly and our finding highlights the need for more advanced mitigation strategies for open-sourced LLMs.</abstract>
      <url hash="5306389a">2024.acl-long.299</url>
      <bibkey>zhang-etal-2024-jailbreak</bibkey>
      <doi>10.18653/v1/2024.acl-long.299</doi>
    </paper>
    <paper id="300">
      <title><fixed-case>NICE</fixed-case>: To Optimize In-Context Examples or Not?</title>
      <author><first>Pragya</first><last>Srivastava</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Satvik</first><last>Golechha</last><affiliation>Microsoft Research and Wadhwani AI</affiliation></author>
      <author><first>Amit</first><last>Deshpande</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Amit</first><last>Sharma</last><affiliation>Microsoft</affiliation></author>
      <pages>5494-5510</pages>
      <abstract>Recent work shows that in-context learning and optimization of in-context examples (ICE) can significantly improve the accuracy of large language models (LLMs) on a wide range of tasks, leading to an apparent consensus that ICE optimization is crucial for better performance. However, most of these studies assume a fixed or no instruction provided in the prompt. We challenge this consensus by investigating the necessity of optimizing ICE when task-specific instructions are provided and find that there are many tasks for which it yields diminishing returns. In particular, using a diverse set of tasks and a systematically created instruction set with gradually added details, we find that as the prompt instruction becomes more detailed, the returns on ICE optimization diminish. To characterize this behavior, we introduce a task-specific metric called Normalized Invariability to Choice of Examples (NICE) that quantifies the learnability of tasks from a given instruction, and provides a heuristic to help decide whether to optimize instructions or ICE for a new task. Given a task, the proposed metric can reliably predict the utility of optimizing ICE compared to using random ICE. Our code is available at [https://github.com/microsoft/nice-icl](https://github.com/microsoft/nice-icl).</abstract>
      <url hash="2d100ea6">2024.acl-long.300</url>
      <bibkey>srivastava-etal-2024-nice</bibkey>
      <doi>10.18653/v1/2024.acl-long.300</doi>
    </paper>
    <paper id="301">
      <title><fixed-case>C</fixed-case>ode<fixed-case>S</fixed-case>cope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating <fixed-case>LLM</fixed-case>s on Code Understanding and Generation</title>
      <author><first>Weixiang</first><last>Yan</last></author>
      <author><first>Haitian</first><last>Liu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Yunkun</first><last>Wang</last></author>
      <author><first>Yunzhe</first><last>Li</last></author>
      <author><first>Qian</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wen</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Tingyu</first><last>Lin</last><affiliation>Technische Universität Wien</affiliation></author>
      <author><first>Weishan</first><last>Zhao</last></author>
      <author><first>Li</first><last>Zhu</last></author>
      <author><first>Hari</first><last>Sundaram</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Shuiguang</first><last>Deng</last><affiliation>Zhejiang University</affiliation></author>
      <pages>5511-5558</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce **CodeScope**, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers **43 programming languages** and **eight coding tasks**. It evaluates the coding performance of LLMs from three dimensions (perspectives): **length**, **difficulty**, and **efficiency**. To facilitate execution-based evaluations of code generation, we develop **MultiCodeEngine**, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at https://github.com/WeixiangYAN/CodeScope.</abstract>
      <url hash="2f883121">2024.acl-long.301</url>
      <bibkey>yan-etal-2024-codescope</bibkey>
      <doi>10.18653/v1/2024.acl-long.301</doi>
    </paper>
    <paper id="302">
      <title>Digital Socrates: Evaluating <fixed-case>LLM</fixed-case>s through Explanation Critiques</title>
      <author><first>Yuling</first><last>Gu</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Oyvind</first><last>Tafjord</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>5559-5586</pages>
      <abstract>While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reasoning chains, and how it can provide high-quality, nuanced, automatic evaluation of those model explanations for the first time. Digital Socrates thus fills an important gap in evaluation tools for understanding and improving the explanation behavior of models.</abstract>
      <url hash="4526974f">2024.acl-long.302</url>
      <bibkey>gu-etal-2024-digital</bibkey>
      <doi>10.18653/v1/2024.acl-long.302</doi>
    </paper>
    <paper id="303">
      <title><fixed-case>S</fixed-case>afe<fixed-case>D</fixed-case>ecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding</title>
      <author><first>Zhangchen</first><last>Xu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Fengqing</first><last>Jiang</last><affiliation>University of Washington</affiliation></author>
      <author><first>Luyao</first><last>Niu</last></author>
      <author><first>Jinyuan</first><last>Jia</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Radha</first><last>Poovendran</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>5587-5605</pages>
      <abstract>As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, which aim to provoke unintended and unsafe behaviors from LLMs, remain a significant LLM safety threat. We analyze tokens, which are the smallest unit of text that can be processed by LLMs and make the following observations: (1) probabilities of tokens representing harmful responses are higher than those of harmless responses, and (2) responses containing safety disclaimers appear among the top tokens when token probabilities are sorted in descending order. In this paper, we leverage (1) and (2) to develop SafeDecoding, a safety-aware decoding strategy for LLMs, to defend against jailbreak attacks. We perform extensive experiments to evaluate SafeDecoding against six SOTA jailbreak attacks (GCG, AutoDAN, PAIR, DeepInception, SAP30, and template based attack) on five LLMs (Vicuna, Llama2, Guanaco, falcon, and Dolphin) using four benchmark datasets (AdvBench, HEx-PHI, MT-Bench, and Just-Eval). Our results show that SafeDecoding significantly reduces attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries while outperforming six defense methods (Perpelexity, Paraphrase, Retokenization, Self-Reminder, ICD, and Self-Examination).</abstract>
      <url hash="1045fa4c">2024.acl-long.303</url>
      <bibkey>xu-etal-2024-safedecoding</bibkey>
      <doi>10.18653/v1/2024.acl-long.303</doi>
    </paper>
    <paper id="304">
      <title>Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?</title>
      <author><first>Guijin</first><last>Son</last></author>
      <author><first>SangWon</first><last>Baek</last><affiliation>Kookmin University</affiliation></author>
      <author><first>Sangdae</first><last>Nam</last><affiliation>VESSL AI</affiliation></author>
      <author><first>Ilgyun</first><last>Jeong</last></author>
      <author><first>Seungone</first><last>Kim</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>5606-5627</pages>
      <abstract>Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle <i>multiple</i> instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench (Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by <tex-math>\times 1.46</tex-math> times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench dataset and our code at this [link](https://anonymous.4open.science/r/MTI-Bench-6F01).</abstract>
      <url hash="e2ef52d3">2024.acl-long.304</url>
      <bibkey>son-etal-2024-multi-task</bibkey>
      <doi>10.18653/v1/2024.acl-long.304</doi>
    </paper>
    <paper id="305">
      <title>Experiential Co-Learning of Software-Developing Agents</title>
      <author><first>Chen</first><last>Qian</last></author>
      <author><first>Yufan</first><last>Dang</last></author>
      <author><first>Jiahao</first><last>Li</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Wei</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zihao</first><last>Xie</last></author>
      <author><first>YiFei</first><last>Wang</last></author>
      <author><first>Weize</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Cheng</first><last>Yang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Xin</first><last>Cong</last></author>
      <author><first>Xiaoyin</first><last>Che</last><affiliation>Siemens Corporate Research</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>5628-5640</pages>
      <abstract>Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce Experiential Co-Learning, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at https://github.com/OpenBMB/ChatDev.</abstract>
      <url hash="cf2ecd9a">2024.acl-long.305</url>
      <bibkey>qian-etal-2024-experiential</bibkey>
      <doi>10.18653/v1/2024.acl-long.305</doi>
    </paper>
    <paper id="306">
      <title>Learning Geometry-Aware Representations for New Intent Discovery</title>
      <author><first>Kai</first><last>Tang</last></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Runze</first><last>Wu</last><affiliation>NetEase Corp</affiliation></author>
      <author><first>Lei</first><last>Feng</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Gang</first><last>Chen</last></author>
      <author><first>Haobo</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>5641-5654</pages>
      <abstract>New intent discovery (NID) is an important problem for deploying practical dialogue systems, which trains intent classifiers on a semi-supervised corpus where unlabeled user utterances contain both known and novel intents. Most existing NID algorithms place hope on the sample similarity to cluster unlabeled corpus to known or new samples. Lacking supervision on new intents, we experimentally find the intent classifier fails to fully distinguish new intents since they tend to assemble into intertwined centers.To address this problem, we propose a novel GeoID framework that learns geometry-aware representations to maximally separate all intents. Specifically, we are motivated by the recent findings on Neural Collapse (NC) in classification tasks to derive optimal intent center structure. Meanwhile, we devise a dual pseudo-labeling strategy based on optimal transport assignments and semi-supervised clustering, ensuring proper utterances-to-center arrangement.Extensive results show that our GeoID method establishes a new state-of-the-art performance, achieving a +3.49% average accuracy improvement on three standardized benchmarking datasets. We also verify its usefulness in assisting large language models for improved in-context performance.</abstract>
      <url hash="a081f5ff">2024.acl-long.306</url>
      <bibkey>tang-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.acl-long.306</doi>
    </paper>
    <paper id="307">
      <title>Speaker Verification in Agent-generated Conversations</title>
      <author><first>Yizhe</first><last>Yang</last></author>
      <author><first>Palakorn</first><last>Achananuparp</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Ee-Peng</first><last>Lim</last><affiliation>Singapore Management University</affiliation></author>
      <pages>5655-5676</pages>
      <abstract>The recent success of large language models (LLMs) has attracted widespread interest to develop role-playing conversational agents personalized to the characteristics and styles of different speakers to enhance their abilities to perform both general and special purpose dialogue tasks. However, the ability to personalize the generated utterances to speakers, whether conducted by human or LLM, has not been well studied. To bridge this gap, our study introduces a novel evaluation challenge: speaker verification in agent-generated conversations, which aimed to verify whether two sets of utterances originate from the same speaker. To this end, we assemble a large dataset collection encompassing thousands of speakers and their utterances. We also develop and evaluate speaker verification models under experiment setups. We further utilize the speaker verification models to evaluate the personalization abilities of LLM-based role-playing models. Comprehensive experiments suggest that the current role-playing models fail in accurately mimicking speakers, primarily due to their inherent linguistic characteristics.</abstract>
      <url hash="13ffeb02">2024.acl-long.307</url>
      <bibkey>yang-etal-2024-speaker</bibkey>
      <doi>10.18653/v1/2024.acl-long.307</doi>
    </paper>
    <paper id="308">
      <title>Benchmarking Data Science Agents</title>
      <author><first>Yuge</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qiyang</first><last>Jiang</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>XingyuHan</first><last>XingyuHan</last></author>
      <author><first>Nan</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yuqing</first><last>Yang</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Kan</first><last>Ren</last><affiliation>ShanghaiTech University</affiliation></author>
      <pages>5677-5700</pages>
      <abstract>In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval – a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.</abstract>
      <url hash="7d21f375">2024.acl-long.308</url>
      <bibkey>zhang-etal-2024-benchmarking-data</bibkey>
      <doi>10.18653/v1/2024.acl-long.308</doi>
    </paper>
    <paper id="309">
      <title>Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models</title>
      <author><first>Tianyi</first><last>Tang</last></author>
      <author><first>Wenyang</first><last>Luo</last></author>
      <author><first>Haoyang</first><last>Huang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Dongdong</first><last>Zhang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author id="xiaolei-wang-fudan"><first>Xiaolei</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>5701-5715</pages>
      <abstract>Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora.It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts.In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions.Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs.Based on LAPE, we conduct comprehensive experiments on several representative LLMs, such as LLaMA-2, BLOOM, and Mistral. Our findings indicate that LLMs’ proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models’ top and bottom layers.Furthermore, we showcase the feasibility to “steer” the output language of LLMs by selectively activating or deactivating language-specific neurons. Our research provides important evidence to the understanding and exploration of the multilingual capabilities of LLMs.</abstract>
      <url hash="d6a1ba2d">2024.acl-long.309</url>
      <bibkey>tang-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.acl-long.309</doi>
    </paper>
    <paper id="310">
      <title>Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models</title>
      <author><first>Shiwen</first><last>Ni</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Dingwei</first><last>Chen</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Chengming</first><last>Li</last><affiliation>Shenzhen MSU-BIT University</affiliation></author>
      <author><first>Xiping</first><last>Hu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <pages>5716-5731</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have showcased their remarkable capabilities in text understanding and generation. However, even stronger LLMs are susceptible to acquiring erroneous or obsolete information from the training corpus. Direct secondary fine-tuning with data containing new knowledge may be ineffective in updating knowledge due to the conflict between old and new knowledge. In this paper, we propose a new paradigm for fine-tuning called F-Learning (Forgetting before Learning), which employs parametric arithmetic to facilitate the forgetting of old knowledge and learning of new knowledge. Experimental results on two publicly available datasets demonstrate that our proposed F-Learning can obviously improve the knowledge updating performance of both full fine-tuning and LoRA fine-tuning, simultaneously outperforming the existing baselines in most cases. Moreover, we have also discovered that forgetting old knowledge by subtracting the parameters of LoRA can yield a similar effect to subtracting the parameters of full fine-tuning, and occasionally even surpass it significantly.</abstract>
      <url hash="3517d47e">2024.acl-long.310</url>
      <bibkey>ni-etal-2024-forgetting</bibkey>
      <doi>10.18653/v1/2024.acl-long.310</doi>
    </paper>
    <paper id="311">
      <title>A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques</title>
      <author><first>Megh</first><last>Thakkar</last></author>
      <author><first>Quentin</first><last>Fournier</last><affiliation>Mila - Quebec AI Institute</affiliation></author>
      <author><first>Matthew</first><last>Riemer</last><affiliation>Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal</affiliation></author>
      <author><first>Pin-Yu</first><last>Chen</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Amal</first><last>Zouaq</last><affiliation>Polytechnique Montreal</affiliation></author>
      <author><first>Payel</first><last>Das</last></author>
      <author><first>Sarath</first><last>Chandar</last><affiliation>Polytechnique Montreal</affiliation></author>
      <pages>5732-5745</pages>
      <abstract>Large language models are first pre-trained on trillions of tokens and then instruction-tuned or aligned to specific preferences. While pre-training remains out of reach for most researchers due to the compute required, fine-tuning has become affordable thanks to parameter-efficient methods such as LoRA and QLoRA. Alignment is known to be sensitive to the many factors involved, including the quantity and quality of data, the alignment method, and the adapter rank. However, there has not yet been an extensive study of their effect on downstream performance. To address this gap, we conduct an in-depth investigation of the impact of popular choices for three crucial axes: (i) the alignment dataset (HH-RLHF and BeaverTails), (ii) the alignment technique (SFT and DPO), and (iii) the model (LLaMA-1, Vicuna-v1.3, Mistral-7b, and Mistral-7b-Instruct). Our extensive setup spanning over 300 experiments reveals consistent trends and unexpected findings. We observe how more informative data helps with preference alignment, cases where supervised fine-tuning outperforms preference optimization, and how aligning to a distinct preference boosts performance on downstream tasks. Through our in-depth analyses, we put forward key guidelines to help researchers perform more effective parameter-efficient LLM alignment.</abstract>
      <url hash="119c0892">2024.acl-long.311</url>
      <bibkey>thakkar-etal-2024-deep</bibkey>
      <doi>10.18653/v1/2024.acl-long.311</doi>
    </paper>
    <paper id="312">
      <title>Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation</title>
      <author><first>Xiang</first><last>Luo</last></author>
      <author><first>Zhiwen</first><last>Tang</last><affiliation>Yunnan University</affiliation></author>
      <author><first>Jin</first><last>Wang</last><affiliation>Yunnan University</affiliation></author>
      <author><first>Xuejie</first><last>Zhang</last><affiliation>Yunnan University</affiliation></author>
      <pages>5746-5765</pages>
      <abstract>Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to transition to unfamiliar domains without manual annotation or extensive retraining. Prior research has approached this objective by embedding prompts into language models (LMs). Common methodologies include integrating prompts at the input layer or introducing learnable variables at each transformer layer. Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at the input layer risk underutilization, with their impact potentially diminishing across successive transformer layers. Conversely, the addition of learnable variables to each layer can complicate the training process and increase inference latency. To tackle the issues mentioned above, this paper proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank Adaptation (LoRA) components, targeting both dialogue context processing and prompt optimization, to ensure the comprehensive influence of prompts throughout the transformer model layers. This is achieved without incurring additional inference latency, showcasing an efficient integration into existing architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets, DualLoRA demonstrates notable improvements across multiple domains, outperforming traditional baseline methods in zero-shot settings.</abstract>
      <url hash="89984751">2024.acl-long.312</url>
      <bibkey>luo-etal-2024-zero</bibkey>
      <doi>10.18653/v1/2024.acl-long.312</doi>
    </paper>
    <paper id="313">
      <title><fixed-case>PRP</fixed-case>-Graph: Pairwise Ranking Prompting to <fixed-case>LLM</fixed-case>s with Graph Aggregation for Effective Text Re-ranking</title>
      <author><first>Jian</first><last>Luo</last></author>
      <author><first>Xuanang</first><last>Chen</last></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>5766-5776</pages>
      <abstract>Pairwise Ranking Prompting (PRP) demonstrates impressive effectiveness in zero-shot document re-ranking tasks with large language models (LLMs). However, in the existing methods, PRP only outputs the same label for the comparison results of different confidence intervals without considering the uncertainty of pairwise comparison, which implies an underutilization of the generation probability information of LLMs. To bridge this gap, we propose PRP-Graph, a novel pairwise re-ranking approach, based on a refined scoring PRP unit that exploits the output probabilities of target labels to capture the degree of certainty of the comparison results. Specifically, the PRP-Graph consists of two stages, namely ranking graph construction and ranking graph aggregation. Extensive experiments conducted on the BEIR benchmark demonstrate the superiority of our approach over existing PRP-based methods. Comprehensive analysis reveals that the PRP-Graph displays strong robustness towards the initial ranking order and delivers exceptional re-ranking results with acceptable efficiency. Our code and data are available at https://github.com/Memelank/PRP-Graph.</abstract>
      <url hash="424ea93d">2024.acl-long.313</url>
      <bibkey>luo-etal-2024-prp</bibkey>
      <doi>10.18653/v1/2024.acl-long.313</doi>
    </paper>
    <paper id="314">
      <title><fixed-case>R</fixed-case>ep<fixed-case>C</fixed-case>odec: A Speech Representation Codec for Speech Tokenization</title>
      <author><first>Zhichao</first><last>Huang</last><affiliation>Bytedance</affiliation></author>
      <author><first>Chutong</first><last>Meng</last></author>
      <author><first>Tom</first><last>Ko</last><affiliation>ByteDance AI Lab</affiliation></author>
      <pages>5777-5790</pages>
      <abstract>With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec.We believe our method can facilitate large language modeling research on speech processing.</abstract>
      <url hash="5cfe2d96">2024.acl-long.314</url>
      <bibkey>huang-etal-2024-repcodec</bibkey>
      <doi>10.18653/v1/2024.acl-long.314</doi>
    </paper>
    <paper id="315">
      <title><fixed-case>G</fixed-case>umbel<fixed-case>S</fixed-case>oft: Diversified Language Model Watermarking via the <fixed-case>G</fixed-case>umbel<fixed-case>M</fixed-case>ax-trick</title>
      <author><first>Jiayi</first><last>Fu</last></author>
      <author><first>Xuandong</first><last>Zhao</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Ruihan</first><last>Yang</last></author>
      <author><first>Yuansen</first><last>Zhang</last></author>
      <author><first>Jiangjie</first><last>Chen</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>5791-5808</pages>
      <abstract>Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the watermark based on the GumbelMax trick (GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we introduce a new type of GM watermark, the Logits-Addition watermark, as well as three variants that aim to enhance diversity, particularly the GumbelSoft watermark (i.e., the softmax variant of the Logits-Addition watermark). When assessed for detectability in high diversity settings, our Gumbelsoft demonstrates superior performance, with its AUROC score exceeding those of the two alternative variants by a margin of 0.1 to 0.3 and outperforming other decoding-based watermarking methods by a minimum of 0.1.</abstract>
      <url hash="2133605c">2024.acl-long.315</url>
      <bibkey>fu-etal-2024-gumbelsoft</bibkey>
      <doi>10.18653/v1/2024.acl-long.315</doi>
    </paper>
    <paper id="316">
      <title>Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection</title>
      <author><first>Zihan</first><last>Ma</last></author>
      <author><first>Minnan</first><last>Luo</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Hao</first><last>Guo</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Zhi</first><last>Zeng</last></author>
      <author><first>Yiran</first><last>Hao</last></author>
      <author><first>Xiang</first><last>Zhao</last><affiliation>National University of Defense Technology</affiliation></author>
      <pages>5809-5821</pages>
      <abstract>The swift detection of multimedia fake news has emerged as a crucial task in combating malicious propaganda and safeguarding the security of the online environment. While existing methods have achieved commendable results in modeling entity-level inconsistency, addressing event-level inconsistency following the inherent subject-predicate logic of news and robustly learning news representations from poor-quality news samples remain two challenges. In this paper, we propose an Event-diven fake news detection framework (Event-Radar) based on multi-view learning, which integrates visual manipulation, textual emotion and multimodal inconsistency at event-level for fake news detection. Specifically, leveraging the capability of graph structures to capture interactions between events and parameters, Event-Radar captures event-level multimodal inconsistency by constructing an event graph that includes multimodal entity subject-predicate logic. Additionally, to mitigate the interference of poor-quality news, Event-Radar introduces a multi-view fusion mechanism, learning comprehensive and robust representations by computing the credibility of each view as a clue, thereby detecting fake news. Extensive experiments demonstrate that Event-Radar achieves outstanding performance on three large-scale fake news detection benchmarks. Our studies also confirm that Event-Radar exhibits strong robustness, providing a paradigm for detecting fake news from noisy news samples.</abstract>
      <url hash="561824fe">2024.acl-long.316</url>
      <bibkey>ma-etal-2024-event</bibkey>
      <doi>10.18653/v1/2024.acl-long.316</doi>
    </paper>
    <paper id="317">
      <title>Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions</title>
      <author><first>Liyan</first><last>Xu</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Jiangnan</first><last>Li</last><affiliation>WeChat, Tencent Inc.</affiliation></author>
      <author><first>Mo</first><last>Yu</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>5822-5838</pages>
      <abstract>This work introduces an original and practical paradigm for narrative comprehension, stemming from the characteristics that individual passages within narratives tend to be more cohesively related than isolated.Complementary to the common end-to-end paradigm, we propose a fine-grained modeling of narrative context, by formulating a graph dubbed NarCo, which explicitly depicts task-agnostic coherence dependencies that are ready to be consumed by various downstream tasks. In particular, edges in NarCo encompass free-form retrospective questions between context snippets, inspired by human cognitive perception that constantly reinstates relevant events from prior context. Importantly, our graph formalism is practically instantiated by LLMs without human annotations, through our designed two-stage prompting scheme.To examine the graph properties and its utility, we conduct three studies in narratives, each from a unique angle: edge relation efficacy, local context enrichment, and broader application in QA. All tasks could benefit from the explicit coherence captured by NarCo.</abstract>
      <url hash="7115c519">2024.acl-long.317</url>
      <bibkey>xu-etal-2024-fine</bibkey>
      <doi>10.18653/v1/2024.acl-long.317</doi>
    </paper>
    <paper id="318">
      <title>Stealthy Attack on Large Language Model based Recommendation</title>
      <author><first>Jinghao</first><last>Zhang</last></author>
      <author><first>Yuting</first><last>Liu</last></author>
      <author><first>Qiang</first><last>Liu</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shu</first><last>Wu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Guibing</first><last>Guo</last></author>
      <author><first>Liang</first><last>Wang</last><affiliation>CASIA</affiliation></author>
      <pages>5839-5857</pages>
      <abstract>Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item’s exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model’s training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior efficacy and stealthiness of our approach. Our work unveils a significant security gap in LLM-based recommendation systems and paves the way for future research on protecting these systems.</abstract>
      <url hash="cf00b380">2024.acl-long.318</url>
      <bibkey>zhang-etal-2024-stealthy</bibkey>
      <doi>10.18653/v1/2024.acl-long.318</doi>
    </paper>
    <paper id="319">
      <title>Multi-Dimensional Optimization for Text Summarization via Reinforcement Learning</title>
      <author><first>Sangwon</first><last>Ryu</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Heejin</first><last>Do</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Yunsu</first><last>Kim</last><affiliation>aiXplain, Inc.</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <author><first>Jungseul</first><last>Ok</last><affiliation>POSTECH</affiliation></author>
      <pages>5858-5871</pages>
      <abstract>The evaluation of summary quality encompasses diverse dimensions such as consistency, coherence, relevance, and fluency. However, existing summarization methods often target a specific dimension, facing challenges in generating well-balanced summaries across multiple dimensions. In this paper, we propose multi-objective reinforcement learning tailored to generate balanced summaries across all four dimensions. We introduce two multi-dimensional optimization (MDO) strategies for adaptive learning: 1) MDO_min, rewarding the current lowest dimension score, and 2) MDO_pro, optimizing multiple dimensions similar to multi-task learning, resolves conflicting gradients across dimensions through gradient projection. Unlike prior ROUGE-based rewards relying on reference summaries, we use a QA-based reward model that aligns with human preferences. Further, we discover the capability to regulate the length of summaries by adjusting the discount factor, seeking the generation of concise yet informative summaries that encapsulate crucial points. Our approach achieved substantial performance gains compared to baseline models on representative summarization datasets, particularly in the overlooked dimensions.</abstract>
      <url hash="74f19986">2024.acl-long.319</url>
      <bibkey>ryu-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.acl-long.319</doi>
    </paper>
    <paper id="320">
      <title>Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models</title>
      <author><first>Changyu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xiting</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ting-En</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ang</first><last>Lv</last></author>
      <author><first>Yuchuan</first><last>Wu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xin</first><last>Gao</last></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>5872-5900</pages>
      <abstract>In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models insuch domains. Earlier fine-tuning approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a techniquewe found to be particularly effective for reasoning tasks. When applied to fine-tuning with GSM8K on Llama-2-7B, this method achieveda 5% improvement in GSM8K accuracy and a 10% improvement in GSM-IC accuracy over standard supervised fine-tuning with a few codes modified. Furthermore, it is complementary to existing methods. When integrated with related explicit data augmentation methods, it leads to improvements across five datasets of various augmentation methods, as well as two different base models. We further investigate the mechanisms behind this improvement through case studies and quantitative analysis, suggesting that our approach may provide superior support for the model in capturing long-distance dependencies, especially those related to questions. This enhancement could deepen understanding of the premises in questions and prior steps.</abstract>
      <url hash="3023d574">2024.acl-long.320</url>
      <bibkey>chen-etal-2024-masked</bibkey>
      <doi>10.18653/v1/2024.acl-long.320</doi>
    </paper>
    <paper id="321">
      <title><fixed-case>SEER</fixed-case>: Facilitating Structured Reasoning and Explanation via Reinforcement Learning</title>
      <author><first>Guoxin</first><last>Chen</last></author>
      <author><first>Kexin</first><last>Tang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Chao</first><last>Yang</last></author>
      <author><first>Fuying</first><last>Ye</last></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Yiming</first><last>Qian</last><affiliation>IHPC</affiliation></author>
      <pages>5901-5921</pages>
      <abstract>Elucidating the reasoning process with structured explanations from question to answer is crucial, as it significantly enhances the interpretability, traceability, and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricately structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Moreover, existing reinforcement learning (RL) based methods overlook the structured relationships, underutilizing the potential of RL in structured reasoning. In this paper, we propose SEER, a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between different reasoning steps. In addition, we introduce a fine-grained reward function to meticulously delineate diverse reasoning steps. Extensive experiments show that SEER significantly outperforms state-of-the-art methods, achieving an absolute improvement of 6.9% over RL-based methods on EntailmentBank, a 4.4% average improvement on STREET benchmark, and exhibiting outstanding efficiency and cross-dataset generalization performance.</abstract>
      <url hash="49a0baac">2024.acl-long.321</url>
      <bibkey>chen-etal-2024-seer</bibkey>
      <doi>10.18653/v1/2024.acl-long.321</doi>
    </paper>
    <paper id="322">
      <title>Towards Robust and Generalized Parameter-Efficient Fine-Tuning for Noisy Label Learning</title>
      <author><first>Yeachan</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Junho</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>SangKeun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <pages>5922-5936</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) has enabled the efficient optimization of cumbersome language models in real-world settings. However, as datasets in such environments often contain noisy labels that adversely affect performance, PEFT methods are inevitably exposed to noisy labels. Despite this challenge, the adaptability of PEFT to noisy environments remains underexplored. To bridge this gap, we investigate various PEFT methods under noisy labels. Interestingly, our findings reveal that PEFT has difficulty in memorizing noisy labels due to its inherently limited capacity, resulting in robustness. However, we also find that such limited capacity simultaneously makes PEFT more vulnerable to interference of noisy labels, impeding the learning of clean samples. To address this issue, we propose Clean Routing (CleaR), a novel routing-based PEFT approach that adaptively activates PEFT modules. In CleaR, PEFT modules are preferentially exposed to clean data while bypassing the noisy ones, thereby minimizing the noisy influence. To verify the efficacy of CleaR, we perform extensive experiments on diverse configurations of noisy labels. The results convincingly demonstrate that CleaR leads to substantially improved performance in noisy environments</abstract>
      <url hash="f99cc56c">2024.acl-long.322</url>
      <bibkey>kim-etal-2024-towards-robust</bibkey>
      <doi>10.18653/v1/2024.acl-long.322</doi>
    </paper>
    <paper id="323">
      <title><fixed-case>S</fixed-case>parse<fixed-case>F</fixed-case>low: Accelerating Transformers by Sparsifying Information Flows</title>
      <author><first>Yeachan</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>SangKeun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <pages>5937-5948</pages>
      <abstract>Transformers have become the de-facto standard for natural language processing. However, dense information flows within transformers pose significant challenges for real-time and resource-constrained devices, as computational complexity grows quadratically with sequence length. To counteract such dense information flows, we propose SparseFlow, a novel efficient method designed to sparsify the dense pathways of token representations across all transformer blocks. To this end, SparseFlow parameterizes the information flows linking token representations to transformer blocks. These parameterized information flows are optimized to be sparse, allowing only the salient information to pass through into the blocks. To validate the efficacy of SparseFlow, we conduct comprehensive experiments across diverse benchmarks (understanding and generation), scales (ranging from millions to billions), architectures (including encoders, decoders, and seq-to-seq models), and modalities (such as language-only and vision-language). The results convincingly demonstrate that sparsifying the dense information flows leads to substantial speedup gains without compromising task accuracy. For instance, SparseFlow reduces computational costs by half on average, without a significant loss in accuracy.</abstract>
      <url hash="e11c1db5">2024.acl-long.323</url>
      <bibkey>kim-lee-2024-sparseflow</bibkey>
      <doi>10.18653/v1/2024.acl-long.323</doi>
    </paper>
    <paper id="324">
      <title><fixed-case>P</fixed-case>rot<fixed-case>T</fixed-case>3: Protein-to-Text Generation for Text-based Protein Understanding</title>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>An</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Enzhi</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Kenji</first><last>Kawaguchi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>5949-5966</pages>
      <abstract>Language Models (LMs) excel in understanding textual descriptions of proteins, as evident in biomedical question-answering tasks. However, their capability falters with raw protein data, such as amino acid sequences, due to a deficit in pretraining on such data. Conversely, Protein Language Models (PLMs) can understand and convert protein data into high-quality representations, but struggle to process texts. To address their limitations, we introduce ProtT3, a framework for Protein-to-Text Generation for Text-based Protein Understanding. ProtT3 empowers an LM to understand protein sequences of amino acids by incorporating a PLM as its protein understanding module, enabling effective protein-to-text generation. This collaboration between PLM and LM is facilitated by a cross-modal projector (i.e., Q-Former) that bridges the modality gap between the PLM’s representation space and the LM’s input space. Unlike previous studies focusing on protein property prediction and protein-text retrieval, we delve into the largely unexplored field of protein-to-text generation. To facilitate comprehensive benchmarks and promote future research, we establish quantitative evaluations for protein-text modeling tasks, including protein captioning, protein question-answering, and protein-text retrieval. Our experiments show that ProtT3 substantially surpasses current baselines, with ablation studies further highlighting the efficacy of its core components. Our code is available at https://github.com/acharkq/ProtT3.</abstract>
      <url hash="888b65a0">2024.acl-long.324</url>
      <bibkey>liu-etal-2024-prott3</bibkey>
      <doi>10.18653/v1/2024.acl-long.324</doi>
    </paper>
    <paper id="325">
      <title><fixed-case>KIE</fixed-case>val: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models</title>
      <author><first>Zhuohao</first><last>Yu</last><affiliation>Peking University</affiliation></author>
      <author><first>Chang</first><last>Gao</last></author>
      <author><first>Wenjin</first><last>Yao</last></author>
      <author><first>Yidong</first><last>Wang</last></author>
      <author><first>Wei</first><last>Ye</last><affiliation>Peking University</affiliation></author>
      <author><first>Jindong</first><last>Wang</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <author><first>Shikun</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>5967-5985</pages>
      <abstract>Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered “interactor” role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model’s response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval’s effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models’ real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.</abstract>
      <url hash="98f2321c">2024.acl-long.325</url>
      <bibkey>yu-etal-2024-kieval</bibkey>
      <doi>10.18653/v1/2024.acl-long.325</doi>
    </paper>
    <paper id="326">
      <title><fixed-case>E</fixed-case>mo<fixed-case>B</fixed-case>ench: Evaluating the Emotional Intelligence of Large Language Models</title>
      <author><first>Sahand</first><last>Sabour</last></author>
      <author><first>Siyang</first><last>Liu</last></author>
      <author><first>Zheyuan</first><last>Zhang</last><affiliation>Computer Science and Technology, Tsinghua University, Tsinghua University and Department of Computer Science and Technology, Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>June</first><last>Liu</last></author>
      <author><first>Jinfeng</first><last>Zhou</last></author>
      <author><first>Alvionna</first><last>Sunaryo</last></author>
      <author><first>Tatia</first><last>Lee</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>5986-6004</pages>
      <abstract>Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench.</abstract>
      <url hash="499203b5">2024.acl-long.326</url>
      <bibkey>sabour-etal-2024-emobench</bibkey>
      <doi>10.18653/v1/2024.acl-long.326</doi>
    </paper>
    <paper id="327">
      <title>Are <fixed-case>AI</fixed-case>-Generated Text Detectors Robust to Adversarial Perturbations?</title>
      <author><first>Guanhua</first><last>Huang</last></author>
      <author><first>Yuchen</first><last>Zhang</last><affiliation>ByteDance Research</affiliation></author>
      <author><first>Zhe</first><last>Li</last></author>
      <author><first>Yongjian</first><last>You</last></author>
      <author><first>Mingze</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhouwang</first><last>Yang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>6005-6024</pages>
      <abstract>The widespread use of large language models (LLMs) has sparked concerns about the potential misuse of AI-generated text, as these models can produce content that closely resembles human-generated text. Current detectors for AI-generated text (AIGT) lack robustness against adversarial perturbations, with even minor changes in characters or words causing a reversal in distinguishing between human-created and AI-generated text. This paper investigates the robustness of existing AIGT detection methods and introduces a novel detector, the Siamese Calibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction network to add and remove noise from text, extracting a semantic representation that is robust to local perturbations. We also propose a siamese calibration technique to train the model to make equally confident predictions under different noise, which improves the model’s robustness against adversarial perturbations. Experiments on four publicly available datasets show that the SCRN outperforms all baseline methods, achieving 6.5%-18.25% absolute accuracy improvement over the best baseline method under adversarial attacks. Moreover, it exhibits superior generalizability in cross-domain, cross-genre, and mixed-source scenarios. The code is available at <url>https://github.com/CarlanLark/Robust-AIGC-Detector</url>.</abstract>
      <url hash="193f47e7">2024.acl-long.327</url>
      <bibkey>huang-etal-2024-ai</bibkey>
      <doi>10.18653/v1/2024.acl-long.327</doi>
    </paper>
    <paper id="328">
      <title><fixed-case>F</fixed-case>in<fixed-case>T</fixed-case>ext<fixed-case>QA</fixed-case>: A Dataset for Long-form Financial Question Answering</title>
      <author><first>Jian</first><last>Chen</last><affiliation>HSBC GPS LAB</affiliation></author>
      <author><first>Peilin</first><last>Zhou</last></author>
      <author><first>Yining</first><last>Hua</last></author>
      <author><first>Loh</first><last>Xin</last></author>
      <author><first>Kehui</first><last>Chen</last></author>
      <author><first>Ziyuan</first><last>Li</last></author>
      <author><first>Bing</first><last>Zhu</last><affiliation>HSBC Lab</affiliation></author>
      <author><first>Junwei</first><last>Liang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>6025-6047</pages>
      <abstract>Accurate evaluation of financial question answering (QA) systems necessitates a comprehensive dataset encompassing diverse question types and contexts. However, current financial QA datasets lack scope diversity and question complexity. This work introduces FinTextQA, a novel dataset for long-form question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality, source-attributed QA pairs extracted and selected from finance textbooks and government agency websites.Moreover, we developed a Retrieval-Augmented Generation (RAG)-based LFQA system, comprising an embedder, retriever, reranker, and generator. A multi-faceted evaluation approach, including human ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the performance of different LFQA system configurations under heightened noisy conditions. The results indicate that: (1) Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The most effective system configuration on our dataset involved setting the embedder, retriever, reranker, and generator as Ada2, Automated Merged Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are less susceptible to noise after the length of contexts reaching a specific threshold. The dataset is publicly available at: https://huggingface.co/datasets/GPS-Lab/FinTextQA.</abstract>
      <url hash="0a5825d4">2024.acl-long.328</url>
      <bibkey>chen-etal-2024-fintextqa</bibkey>
      <doi>10.18653/v1/2024.acl-long.328</doi>
    </paper>
    <paper id="329">
      <title>On Measuring Faithfulness or Self-consistency of Natural Language Explanations</title>
      <author><first>Letitia</first><last>Parcalabescu</last><affiliation>Heidelberg University</affiliation></author>
      <author><first>Anette</first><last>Frank</last><affiliation>Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <pages>6048-6089</pages>
      <abstract>Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models’ inner workings – but rather their self-consistency at output level.Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks – including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model’s input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to compare LLM behaviour when making predictions and to analyse the effect of other consistency tests at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests.</abstract>
      <url hash="8a044b2a">2024.acl-long.329</url>
      <bibkey>parcalabescu-frank-2024-measuring</bibkey>
      <revision id="1" href="2024.acl-long.329v1" hash="ab7d495d"/>
      <revision id="2" href="2024.acl-long.329v2" hash="8a044b2a" date="2024-09-18">This revision mentions a sponsor in the acknowledgements and fixes the typo in Eq. 4.</revision>
      <doi>10.18653/v1/2024.acl-long.329</doi>
    </paper>
    <paper id="330">
      <title>Learning or Self-aligning? Rethinking Instruction Fine-tuning</title>
      <author><first>Mengjie</first><last>Ren</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Boxi</first><last>Cao</last></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Cao</first><last>Liu</last></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Ke</first><last>Zeng</last></author>
      <author><first>Wan</first><last>Guanglu</last><affiliation>Meituan</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>6090-6105</pages>
      <abstract>Instruction Fine-tuning (IFT) is a crucial phase in building large language models (LLMs). Previous works mainly focus on the IFT’s role in the transfer of behavioral norms and the learning of additional world knowledge. However, the understanding of the underlying mechanisms of IFT remains significantly limited. In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors. Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects. Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT. Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works.</abstract>
      <url hash="110d4a03">2024.acl-long.330</url>
      <bibkey>ren-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.acl-long.330</doi>
    </paper>
    <paper id="331">
      <title>Rethinking the Bounds of <fixed-case>LLM</fixed-case> Reasoning: Are Multi-Agent Discussions the Key?</title>
      <author><first>Qineng</first><last>Wang</last><affiliation>Northwestern University and Computer Science Department, Stanford University</affiliation></author>
      <author><first>Zihao</first><last>Wang</last></author>
      <author><first>Ying</first><last>Su</last></author>
      <author><first>Hanghang</first><last>Tong</last></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>6106-6131</pages>
      <abstract>Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same best performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observed that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion. Our code can be found in <url>https://github.com/HKUST-KnowComp/LLM-discussion</url>.</abstract>
      <url hash="03149ed3">2024.acl-long.331</url>
      <bibkey>wang-etal-2024-rethinking-bounds</bibkey>
      <doi>10.18653/v1/2024.acl-long.331</doi>
    </paper>
    <paper id="332">
      <title>Soft Knowledge Prompt: Help External Knowledge Become a Better Teacher to Instruct <fixed-case>LLM</fixed-case> in Knowledge-based <fixed-case>VQA</fixed-case></title>
      <author><first>Qunbo</first><last>Wang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ruyi</first><last>Ji</last></author>
      <author><first>Tianhao</first><last>Peng</last></author>
      <author><first>Wenjun</first><last>Wu</last></author>
      <author><first>Zechao</first><last>Li</last><affiliation>Nanjing University of Science and Techonolgy</affiliation></author>
      <author><first>Jing</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>6132-6143</pages>
      <abstract>LLM has achieved impressive performance on multi-modal tasks, which have received ever-increasing research attention. Recent research focuses on improving prediction performance and reliability (e.g., addressing the hallucination problem). They often prepend relevant external knowledge to the input text as an extra prompt. However, these methods would be affected by the noise in the knowledge and the context length limitation of LLM. In our work, we focus on making better use of external knowledge and propose a method to actively extract valuable information in the knowledge to produce the latent vector as a soft prompt, which is then fused with the image embedding to form a knowledge-enhanced context to instruct LLM. The experimental results on knowledge-based VQA benchmarks show that the proposed method enjoys better utilization of external knowledge and helps the model achieve better performance.</abstract>
      <url hash="7d3efebf">2024.acl-long.332</url>
      <bibkey>wang-etal-2024-soft-knowledge</bibkey>
      <doi>10.18653/v1/2024.acl-long.332</doi>
    </paper>
    <paper id="333">
      <title><fixed-case>T</fixed-case>as<fixed-case>T</fixed-case>e: Teaching Large Language Models to Translate through Self-Reflection</title>
      <author><first>Yutong</first><last>Wang</last></author>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technolgy, Shenzhen</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>6144-6158</pages>
      <abstract>Large language models (LLMs) have exhibited remarkable performance in various natural language processing tasks. Techniques like instruction tuning have effectively enhanced the proficiency of LLMs in the downstream task of machine translation. However, the existing approaches fail to yield satisfactory translation outputs that match the quality of supervised neural machine translation (NMT) systems. One plausible explanation for this discrepancy is that the straightforward prompts employed in these methodologies are unable to fully exploit the acquired instruction-following capabilities. To this end, we propose the <tex-math>\textbf{TasTe}</tex-math> framework, which stands for translating through self-reflection. The self-reflection process includes two stages of inference. In the first stage, LLMs are instructed to generate preliminary translations and conduct self-assessments on these translations simultaneously. In the second stage, LLMs are tasked to refine these preliminary translations according to the evaluation results. The evaluation results in four language directions on the WMT22 benchmark reveal the effectiveness of our approach compared to existing methods. Our work presents a promising approach to unleash the potential of LLMs and enhance their capabilities in MT. The codes and datasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT.</abstract>
      <url hash="752cba37">2024.acl-long.333</url>
      <bibkey>wang-etal-2024-taste</bibkey>
      <doi>10.18653/v1/2024.acl-long.333</doi>
    </paper>
    <paper id="334">
      <title>Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models</title>
      <author><first>Xudong</first><last>Lu</last></author>
      <author><first>Qi</first><last>Liu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yuhui</first><last>Xu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Aojun</first><last>Zhou</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Siyuan</first><last>Huang</last></author>
      <author><first>Bo</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Junchi</first><last>Yan</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hongsheng</first><last>Li</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>6159-6172</pages>
      <abstract>A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer active parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Code will be made available at https://github.com/Lucky-Lance/Expert_Sparsity.</abstract>
      <url hash="15387278">2024.acl-long.334</url>
      <bibkey>lu-etal-2024-experts</bibkey>
      <doi>10.18653/v1/2024.acl-long.334</doi>
    </paper>
    <paper id="335">
      <title><fixed-case>UNIMO</fixed-case>-<fixed-case>G</fixed-case>: Unified Image Generation through Multimodal Conditional Diffusion</title>
      <author><first>Wei</first><last>Li</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xue</first><last>Xu</last></author>
      <author><first>Jiachen</first><last>Liu</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Xinyan</first><last>Xiao</last><affiliation>Baidu</affiliation></author>
      <pages>6173-6188</pages>
      <abstract>Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.</abstract>
      <url hash="78c7fe91">2024.acl-long.335</url>
      <bibkey>li-etal-2024-unimo</bibkey>
      <doi>10.18653/v1/2024.acl-long.335</doi>
    </paper>
    <paper id="336">
      <title>The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing <fixed-case>LLM</fixed-case> Abilities</title>
      <author><first>David</first><last>Stap</last></author>
      <author><first>Eva</first><last>Hasler</last><affiliation>Amazon</affiliation></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last><affiliation>Amazon and University of Cambridge</affiliation></author>
      <author><first>Christof</first><last>Monz</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <author><first>Ke</first><last>Tran</last><affiliation>Amazon</affiliation></author>
      <pages>6189-6206</pages>
      <abstract>Fine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality. However, it is unclear what is the impact of fine-tuning on desirable LLM behaviors that are not present in neural machine translation models, such as steerability, inherent document-level translation abilities, and the ability to produce less literal translations. We perform an extensive translation evaluation on the LLaMA and Falcon family of models with model size ranging from 7 billion up to 65 billion parameters.Our results show that while fine-tuning improves the general translation quality of LLMs, several abilities degrade. In particular, we observe a decline in the ability to perform formality steering, to produce technical translations through few-shot examples, and to perform document-level translation. On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data. We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality. Our findings emphasize the need for fine-tuning strategies that preserve the benefits of LLMs for machine translation.</abstract>
      <url hash="d4ccfe62">2024.acl-long.336</url>
      <bibkey>stap-etal-2024-fine</bibkey>
      <doi>10.18653/v1/2024.acl-long.336</doi>
    </paper>
    <paper id="337">
      <title>Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?</title>
      <author><first>Hexiang</first><last>Tan</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Fei</first><last>Sun</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wanli</first><last>Yang</last></author>
      <author><first>Yuanzhuo</first><last>Wang</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Qi</first><last>Cao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences, China</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <pages>6207-6227</pages>
      <abstract>While auxiliary information has become a key to enhancing Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically contexts generated by LLMs and those retrieved from external sources.To investigate this, we formulate a systematic framework to identify whether LLMs’ responses are attributed to either generated or retrieved contexts.To easily trace the origin of the response, we construct datasets with conflicting contexts, i.e., each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer.Our experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to favor generated contexts, even when they provide incorrect information.We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of being selected; ii) the segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs.Our analysis enhances the understanding of how LLMs merge diverse contexts, offers valuable insights for advancing current LLM augmentation methods, and highlights the risk of generated misinformation for retrieval-augmented LLMs.</abstract>
      <url hash="3cfc6db8">2024.acl-long.337</url>
      <bibkey>tan-etal-2024-blinded</bibkey>
      <doi>10.18653/v1/2024.acl-long.337</doi>
    </paper>
    <paper id="338">
      <title>Unveiling Linguistic Regions in Large Language Models</title>
      <author><first>Zhihao</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>6228-6247</pages>
      <abstract>Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs’ cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependence, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct monolingual regions exist for different languages, and disruption to these specific regions substantially reduces the LLMs’ proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common phenomenon observed during further pre-training of LLMs. Overall, exploring the LLMs’ functional regions provides insights into the foundation of their intelligence.</abstract>
      <url hash="bf7913d1">2024.acl-long.338</url>
      <bibkey>zhang-etal-2024-unveiling-linguistic</bibkey>
      <doi>10.18653/v1/2024.acl-long.338</doi>
    </paper>
    <paper id="339">
      <title>Text-to-Song: Towards Controllable Music Generation Incorporating Vocal and Accompaniment</title>
      <author><first>Zhiqing</first><last>Hong</last></author>
      <author><first>Rongjie</first><last>Huang</last><affiliation>FAIR</affiliation></author>
      <author><first>Xize</first><last>Cheng</last></author>
      <author><first>Yongqi</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ruiqi</first><last>Li</last></author>
      <author><first>Fuming</first><last>You</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <author><first>Zhimeng</first><last>Zhang</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>6248-6261</pages>
      <abstract>A song is a combination of singing voice and accompaniment. However, existing works focus on singing voice synthesis and music generation independently. Little attention was paid to exploring song synthesis. In this work, we propose a novel task called Text-to-Song synthesis which incorporates both vocal and accompaniment generation. We develop Melodist, a two-stage text-to-song method that consists of singing voice synthesis (SVS) and vocal-to-accompaniment (V2A) synthesis. Melodist leverages tri-tower contrastive pretraining to learn more effective text representation for controllable V2A synthesis. A Chinese song dataset mined from a music website is built to alleviate data scarcity for our research. The evaluation results on our dataset demonstrate that Melodist can synthesize songs with comparable quality and style consistency. Audio samples can be found in https://text2songMelodist.github.io/Sample/.</abstract>
      <url hash="f9e7d11c">2024.acl-long.339</url>
      <bibkey>hong-etal-2024-text</bibkey>
      <doi>10.18653/v1/2024.acl-long.339</doi>
    </paper>
    <paper id="340">
      <title><fixed-case>F</fixed-case>ast<fixed-case>F</fixed-case>i<fixed-case>D</fixed-case>: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection</title>
      <author><first>Yufei</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>6262-6276</pages>
      <abstract>Open Domain Question Answering (ODQA) has been advancing rapidly in recent times, driven by significant developments in dense passage retrieval and pretrained language models. State-of-the-art models typically incorporate the FiD framework, which is composed by a neural retriever alongside an encoder-decoder neural reader. In the answer generation process, the retriever will retrieve numerous passages (around 100 for instance), each of which is then individually encoded by the encoder. Subsequently, the decoder makes predictions based on these encoded passages. Nevertheless, this framework can be relatively time-consuming, particularly due to the extensive length of the gathered passages. To address this, we introduce FastFiD in this paper, a novel approach that executes sentence selection on the encoded passages. This aids in retaining valuable sentences while reducing the context length required for generating answers. Experiments on three commonly used datasets (Natural Questions, TriviaQA and ASQA) demonstrate that our method can enhance the inference speed by **2.3X-5.7X**, while simultaneously maintaining the model’s performance. Moreover, an in-depth analysis of the model’s attention reveals that the selected sentences indeed hold a substantial contribution towards the final answer. The codes are publicly available at https://github.com/thunlp/FastFiD.</abstract>
      <url hash="ad6996d0">2024.acl-long.340</url>
      <bibkey>huang-etal-2024-fastfid</bibkey>
      <doi>10.18653/v1/2024.acl-long.340</doi>
    </paper>
    <paper id="341">
      <title>Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models’ Understanding of Discourse Relations</title>
      <author><first>Yisong</first><last>Miao</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Hongfu</first><last>Liu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <pages>6277-6295</pages>
      <abstract>While large language models have significantly enhanced the effectiveness of discourse relation classifications, it remains unclear whether their comprehension is faithful and reliable. We provide DiSQ, a new method for evaluating the faithfulness of understanding discourse based on question answering. We first employ in-context learning to annotate the reasoning for discourse comprehension, based on the connections among key events within the discourse. Following this, DiSQ interrogates the model with a sequence of questions to assess its grasp of core event relations, its resilience to counterfactual queries, as well as its consistency to its previous responses. then evaluate language models with different architectural designs using DiSQ, finding: (1) DiSQ presents a significant challenge for all models, with the top-performing GPT model attaining only 41% of the ideal performance in PDTB; (2) DiSQ is robust to domain shifts and paraphrase variations; (3) Open-source models generally lag behind their closed-source GPT counterparts, with notable exceptions being those enhanced with chat and code/math features; (4) Our analysis validates the effectiveness of explicitly signalled discourse connectives, the role of contextual information, and the benefits of using historical QA data.</abstract>
      <url hash="a4744631">2024.acl-long.341</url>
      <bibkey>miao-etal-2024-discursive</bibkey>
      <doi>10.18653/v1/2024.acl-long.341</doi>
    </paper>
    <paper id="342">
      <title>An Open Multilingual System for Scoring Readability of <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Mykola</first><last>Trokhymovych</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author><first>Indira</first><last>Sen</last><affiliation>Rheinisch Westfälische Technische Hochschule Aachen</affiliation></author>
      <author><first>Martin</first><last>Gerlach</last><affiliation>Wikimedia Foundation</affiliation></author>
      <pages>6296-6311</pages>
      <abstract>With over 60M articles, Wikipedia has become the largest platform for open and freely accessible knowledge. While it has more than 15B monthly visits, its content is believed to be inaccessible to many readers due to the lack of readability of its text. However, previous investigations of the readability of Wikipedia have been restricted to English only, and there are currently no systems supporting the automatic readability assessment of the 300+ languages in Wikipedia. To bridge this gap, we develop a multilingual model to score the readability of Wikipedia articles. To train and evaluate this model, we create a novel multilingual dataset spanning 14 languages, by matching articles from Wikipedia to simplified Wikipedia and online children encyclopedias. We show that our model performs well in a zero-shot scenario, yielding a ranking accuracy of more than 80% across 14 languages and improving upon previous benchmarks. These results demonstrate the applicability of the model at scale for languages in which there is no ground-truth data available for model fine-tuning. Furthermore, we provide the first overview on the state of readability in Wikipedia beyond English.</abstract>
      <url hash="e66c7366">2024.acl-long.342</url>
      <bibkey>trokhymovych-etal-2024-open</bibkey>
      <doi>10.18653/v1/2024.acl-long.342</doi>
    </paper>
    <paper id="343">
      <title>Unlearning Traces the Influential Training Data of Language Models</title>
      <author><first>Masaru</first><last>Isonuma</last></author>
      <author><first>Ivan</first><last>Titov</last><affiliation>University of Edinburgh and University of Amsterdam</affiliation></author>
      <pages>6312-6325</pages>
      <abstract>Identifying the training datasets that influence a language model’s outputs is essential for minimizing the generation of harmful content and enhancing its performance. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac: unlearning traces the influence of a training dataset on the model’s performance. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model’s predictions change after unlearning. Furthermore, we propose a more scalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the unlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being efficient for massive training datasets. In the experiments, we examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Our methods estimate their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple checkpoints.</abstract>
      <url hash="a52fdd9b">2024.acl-long.343</url>
      <bibkey>isonuma-titov-2024-unlearning</bibkey>
      <doi>10.18653/v1/2024.acl-long.343</doi>
    </paper>
    <paper id="344">
      <title>Exploring Alignment in Shared Cross-lingual Spaces</title>
      <author><first>Basel</first><last>Mousi</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Nadir</first><last>Durrani</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Fahim</first><last>Dalvi</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <author><first>Majd</first><last>Hawasly</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Ahmed</first><last>Abdelali</last><affiliation>National Center for AI , Saudi Data and AI Authority</affiliation></author>
      <pages>6326-6348</pages>
      <abstract>Despite their remarkable ability to capture linguistic nuances across diverse languages, questions persist regarding the degree of alignment between languages in multilingual embeddings. Drawing inspiration from research on high-dimensional representations in neural language models, we employ clustering to uncover latent concepts within multilingual models. Our analysis focuses on quantifying the alignment and overlap of these concepts across various languages within the latent space. To this end, we introduce two metrics CALIGN and COLAP aimed at quantifying these aspects, enabling a deeper exploration of multilingual embeddings. Our study encompasses three multilingual models (mT5, mBERT, and XLM-R) and three downstream tasks (Machine Translation, Named Entity Recognition, and Sentiment Analysis). Key findings from our analysis include: i) deeper layers in the network demonstrate increased cross-lingual alignment due to the presence of language-agnostic concepts, ii) fine-tuning of the models enhances alignment within the latent space, and iii) such task-specific calibration helps in explaining the emergence of zero-shot capabilities in the models.</abstract>
      <url hash="8107a9fd">2024.acl-long.344</url>
      <bibkey>mousi-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.acl-long.344</doi>
    </paper>
    <paper id="345">
      <title>Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models</title>
      <author><first>Wenxuan</first><last>Wang</last></author>
      <author><first>Wenxiang</first><last>Jiao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Jingyuan</first><last>Huang</last></author>
      <author><first>Ruyi</first><last>Dai</last></author>
      <author><first>Jen-tse</first><last>Huang</last></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Michael</first><last>Lyu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>6349-6384</pages>
      <abstract>This paper identifies a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training (e.g., ChatGPT). LLMs often provide inappropriate English-culture-related answers that are not relevant to the expected culture when users ask in non-English languages. To systematically evaluate the cultural dominance issue, we build a benchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and opinions) cultural objects. Empirical results show that the representative GPT models suffer from the culture dominance problem, where GPT-4 is the most affected while text-davinci-003 suffers the least from this problem. Our study emphasizes the need to critically examine cultural dominance and ethical considerations in their development and deployment. We show that two straightforward methods in model development (i.e., pretraining on more diverse data) and deployment (e.g., culture-aware prompting) can significantly mitigate the cultural dominance issue in LLMs.</abstract>
      <url hash="53a81a20">2024.acl-long.345</url>
      <bibkey>wang-etal-2024-countries</bibkey>
      <doi>10.18653/v1/2024.acl-long.345</doi>
    </paper>
    <paper id="346">
      <title>Self-Evolving <fixed-case>GPT</fixed-case>: A Lifelong Autonomous Experiential Learner</title>
      <author><first>Jinglong</first><last>Gao</last><affiliation>Research Center for Social Computing and Information Retrieval</affiliation></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Yiming</first><last>Cui</last><affiliation>iFLYTEK Research</affiliation></author>
      <author><first>Jianbai</first><last>Zhao</last></author>
      <author><first>Hepeng</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>6385-6432</pages>
      <abstract>To improve the performance of large language models (LLMs), researchers have explored providing LLMs with textual task-solving experience via prompts. However, they rely on manual efforts to acquire and apply such experience for each task, which is not feasible for the growing demand for LLMs and the variety of user questions.To address this issue, we design a lifelong autonomous experiential learning framework based on LLMs to explore whether LLMs can imitate human ability for learning and utilizing experience. It autonomously learns and accumulates experience through experience transfer and induction, categorizing the types of input questions to select which accumulated experience to employ for them.Experimental results on six widely used NLP datasets show that our framework performs reliably in each intermediate step and effectively improves the performance of GPT-3.5 and GPT-4. This validates the feasibility of using LLMs to mimic human experiential learning and application capabilities, offering a new path worth further exploration for the evolution of machine intelligence. Additionally, we provide a detailed analysis of the behavior of our framework at each step.We will open source codes after the acceptance, fostering open research in the NLP community and beyond.</abstract>
      <url hash="24d5cb2a">2024.acl-long.346</url>
      <bibkey>gao-etal-2024-self-evolving</bibkey>
      <doi>10.18653/v1/2024.acl-long.346</doi>
    </paper>
    <paper id="347">
      <title><fixed-case>WRP</fixed-case>: Weight Recover Prune for Structured Sparsity</title>
      <author><first>Zhendong</first><last>Tan</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Xingjun</first><last>Zhang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Zheng</first><last>Wei</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>6433-6443</pages>
      <abstract>As the scale of Large Language Models (LLMs) increases, it is necessary to compress the models to reduce the substantial demand on computational resources. Network pruning significantly reduces the model size by converting the weight matrix from dense to sparse data format. Current methodologies advocate for one-shot pruning to avoid the expense of retraining, ensuring the maintenance of model performance under conditions of 50%-60% unstructured pruning. Nevertheless, matrices characterized by this level of sparsity could not be treated as sparse matrices, because the indices would incur significant costs. To mitigate this problem, NVIDIA introduced the 2:4 structured sparsity. However, we observe a notable decline in model performance when adopting 2:4 structured sparsity due to group constraints. In this paper, we introduce the Weight Recover Prune (WRP) approach. By recovering a minimal set of critical weights, WRP aims to enhance model performance while maintaining the efficiency of the compression. Our evaluation of the WRP method on the LLAMA2 and OPT models shows that it outperforms other 2:4 pattern one-shot pruning methods. Meanwhile, WRP can guarantee that the size of the pruned model is about 60% of the dense model. Our code is available at: https://github.com/TanZhendong/WRP.</abstract>
      <url hash="7419db9b">2024.acl-long.347</url>
      <bibkey>tan-etal-2024-wrp</bibkey>
      <doi>10.18653/v1/2024.acl-long.347</doi>
    </paper>
    <paper id="348">
      <title>Error-preserving Automatic Speech Recognition of Young <fixed-case>E</fixed-case>nglish Learners’ Language</title>
      <author><first>Janick</first><last>Michot</last></author>
      <author><first>Manuela</first><last>Hürlimann</last><affiliation>ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Jan</first><last>Deriu</last><affiliation>ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Luzia</first><last>Sauer</last></author>
      <author><first>Katsiaryna</first><last>Mlynchyk</last><affiliation>ZHAW - Zürcher Hochschule für Angewandte Wissenschaften and ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Mark</first><last>Cieliebak</last><affiliation>ZHAW School of Engineering</affiliation></author>
      <pages>6444-6454</pages>
      <abstract>One of the central skills that language learners need to practice is speaking the language. Currently, students in school do not get enough speaking opportunities and lack conversational practice. The recent advances in speech technology and natural language processing allow the creation of novel tools to practice their speaking skills. In this work, we tackle the first component of such a pipeline, namely, the automated speech recognition module (ASR). State-of-the-art models are often trained on adult read-aloud data by native speakers and do not transfer well to young language learners’ speech. Second, most ASR systems contain a powerful language model, which smooths out mistakes made by the speakers. To give corrective feedback, which is a crucial part of language learning, the ASR systems in our setting need to preserve the mistakes made by the language learners. In this work, we build an ASR system that satisfies these requirements: it works on spontaneous speech by young language learners and preserves their mistakes. For this, we collected a corpus containing around 85 hours of English audio spoken by Swiss learners from grades 4 to 6 on different language learning tasks, which we used to train an ASR model. Our experiments show that our model benefits from direct fine-tuning of children’s voices and has a much higher error preservation rate.</abstract>
      <url hash="26a48146">2024.acl-long.348</url>
      <bibkey>michot-etal-2024-error</bibkey>
      <doi>10.18653/v1/2024.acl-long.348</doi>
    </paper>
    <paper id="349">
      <title><fixed-case>D</fixed-case>i<fixed-case>F</fixed-case>i<fixed-case>N</fixed-case>et: Boundary-Aware Semantic Differentiation and Filtration Network for Nested Named Entity Recognition</title>
      <author><first>Yuxiang</first><last>Cai</last></author>
      <author><first>Qiao</first><last>Liu</last><affiliation>UESTC</affiliation></author>
      <author><first>Yanglei</first><last>Gan</last></author>
      <author><first>Run</first><last>Lin</last></author>
      <author><first>Changlin</first><last>Li</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Xueyi</first><last>Liu</last></author>
      <author><first>Da</first><last>Luo</last></author>
      <author><first>JiayeYang</first><last>JiayeYang</last></author>
      <pages>6455-6471</pages>
      <abstract>Nested Named Entity Recognition (Nested NER) entails identifying and classifying entity spans within the text, including the detection of named entities that are embedded within external entities. Prior approaches primarily employ span-based techniques, utilizing the power of exhaustive searches to address the challenge of overlapping entities. Nonetheless, these methods often grapple with the absence of explicit guidance for boundary detection, resulting insensitivity in discerning minor variations within nested spans. To this end, we propose a Boundary-aware Semantic <tex-math>\underline{Di}</tex-math>fferentiation and <tex-math>\underline{Fi}</tex-math>ltration <tex-math>\underline{Net}</tex-math>work (DiFiNet) tailored for nested NER. Specifically, DiFiNet leverages a biaffine attention mechanism to generate a span representation matrix. This matrix undergoes further refinement through a self-adaptive semantic differentiation module, specifically engineered to discern semantic variances across spans. Furthermore, DiFiNet integrates a boundary filtration module, designed to mitigate the impact of non-entity noise by leveraging semantic relations among spans. Extensive experiments on three benchmark datasets demonstrate our model yields a new state-of-the-art performance.</abstract>
      <url hash="46c65212">2024.acl-long.349</url>
      <bibkey>cai-etal-2024-difinet</bibkey>
      <doi>10.18653/v1/2024.acl-long.349</doi>
    </paper>
    <paper id="350">
      <title>Legal Case Retrieval: A Survey of the State of the Art</title>
      <author><first>Yi</first><last>Feng</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Chuanyi</first><last>Li</last><affiliation>nanjing university</affiliation></author>
      <author><first>Vincent</first><last>Ng</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>6472-6485</pages>
      <abstract>Recent years have seen increasing attention on Legal Case Retrieval (LCR), a key task in the area of Legal AI that concerns the retrieval of cases from a large legal database of historical cases that are similar to a given query. This paper presents a survey of the major milestones made in LCR research, targeting researchers who are finding their way into the field and seek a brief account of the relevant datasets and the recent neural models and their performances.</abstract>
      <url hash="2768d931">2024.acl-long.350</url>
      <bibkey>feng-etal-2024-legal</bibkey>
      <doi>10.18653/v1/2024.acl-long.350</doi>
    </paper>
    <paper id="351">
      <title>Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation</title>
      <author><first>Tianqi</first><last>Zhong</last></author>
      <author><first>Zhaoyi</first><last>Li</last><affiliation>City University of Hong Kong and University of Science and Technology of China</affiliation></author>
      <author><first>Quan</first><last>Wang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Linqi</first><last>Song</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Ying</first><last>Wei</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>6486-6517</pages>
      <abstract>Compositional generalization, representing the model’s ability to generate text with new attribute combinations obtained by recombining single attributes from the training data, is a crucial property for multi-aspect controllable text generation (MCTG) methods. Nonetheless, a comprehensive compositional generalization evaluation benchmark of MCTG is still lacking. We propose CompMCTG, a benchmark encompassing diverse multi-aspect labeled datasets and a crafted three-dimensional evaluation protocol, to holistically evaluate the compositional generalization of MCTG approaches. We observe that existing MCTG works generally confront a noticeable performance drop in compositional testing. To mitigate this issue, we introduce Meta-MCTG, a training framework incorporating meta-learning, where we enable models to learn how to generalize by simulating compositional generalization scenarios in the training phase. We demonstrate the effectiveness of Meta-MCTG through achieving obvious improvement (by at most 3.64%) for compositional testing performance in 94.4%.</abstract>
      <url hash="01eddc01">2024.acl-long.351</url>
      <bibkey>zhong-etal-2024-benchmarking</bibkey>
      <doi>10.18653/v1/2024.acl-long.351</doi>
    </paper>
    <paper id="352">
      <title><fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case> Pro: Progressive <fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case> with Block Expansion</title>
      <author><first>Chengyue</first><last>Wu</last></author>
      <author><first>Yukang</first><last>Gan</last></author>
      <author><first>Yixiao</first><last>Ge</last><affiliation>Tencent</affiliation></author>
      <author><first>Zeyu</first><last>Lu</last></author>
      <author><first>Jiahao</first><last>Wang</last></author>
      <author><first>Ye</first><last>Feng</last><affiliation>Beijing Language and Culture University</affiliation></author>
      <author><first>Ying</first><last>Shan</last><affiliation>Tencent AI Lab Center of Visual Computing and Tencent PCG ARC Lab</affiliation></author>
      <author><first>Ping</first><last>Luo</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>6518-6537</pages>
      <abstract>Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model’s knowledge while mitigating forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro - Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.</abstract>
      <url hash="a9cee2d6">2024.acl-long.352</url>
      <bibkey>wu-etal-2024-llama</bibkey>
      <doi>10.18653/v1/2024.acl-long.352</doi>
    </paper>
    <paper id="353">
      <title>Generating Contrastive Narratives Using the Brownian Bridge Process for Narrative Coherence Learning</title>
      <author><first>Feiteng</first><last>Mu</last></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <pages>6538-6555</pages>
      <abstract>A major challenge for narrative reasoning is to learn narrative coherence. Existing works mainly follow the contrastive learning paradigm. However, the negative samples in their methods can be easily distinguished, which makes their methods unsatisfactory. In this work, we devise two strategies for mining hard negatives, including (1) crisscrossing a narrative and its contrastive variants; and (2) event-level replacement. To obtain contrastive variants, we utilize the Brownian Bridge process to guarantee the quality of generated contrastive narratives. We evaluate our model on several tasks. The result proves the effectiveness of our method, and shows that our method is applicable to many applications.</abstract>
      <url hash="f92da9f5">2024.acl-long.353</url>
      <bibkey>mu-li-2024-generating</bibkey>
      <doi>10.18653/v1/2024.acl-long.353</doi>
    </paper>
    <paper id="354">
      <title>A Causal Approach for Counterfactual Reasoning in Narratives</title>
      <author><first>Feiteng</first><last>Mu</last></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <pages>6556-6569</pages>
      <abstract>Counterfactual reasoning in narratives requires predicting how alternative conditions, contrary to what actually happened, might have resulted in different outcomes.One major challenge is to maintain the causality between the counterfactual condition and the generated counterfactual outcome. In this paper, we propose a basic VAE module for counterfactual reasoning in narratives. We further introduce a pre-trained classifier and external event commonsense to mitigate the posterior collapse problem in the VAE approach, and improve the causality between the counterfactual condition and the generated counterfactual outcome. We evaluate our method on two public benchmarks. Experiments show that our method is effective.</abstract>
      <url hash="7c4a716a">2024.acl-long.354</url>
      <bibkey>mu-li-2024-causal</bibkey>
      <doi>10.18653/v1/2024.acl-long.354</doi>
    </paper>
    <paper id="355">
      <title><fixed-case>SIP</fixed-case>: Injecting a Structural Inductive Bias into a <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Model by Simulation</title>
      <author><first>Matthias</first><last>Lindemann</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Alexander</first><last>Koller</last><affiliation>Saarland University</affiliation></author>
      <author><first>Ivan</first><last>Titov</last><affiliation>University of Edinburgh and University of Amsterdam</affiliation></author>
      <pages>6570-6587</pages>
      <abstract>Strong inductive biases enable learning from little data and help generalization outside the training distribution. Popular neural architectures such as Transformers lack strong structural inductive biases for seq2seq NLP tasks on their own. Consequently, they struggle with systematic generalization beyond the training distribution, e.g. with extrapolating to longer inputs, even when pre-trained on large amounts of text.We show how a structural inductive bias can be efficiently injected into a seq2seq model by pre-training it to simulate structural transformations on synthetic data. Specifically, we inject an inductive bias towards Finite State Transducers (FSTs) into a Transformer by pre-training it to simulate FSTs given their descriptions. Our experiments show that our method imparts the desired inductive bias, resulting in improved systematic generalization and better few-shot learning for FST-like tasks. Our analysis shows that fine-tuned models accurately capture the state dynamics of the unseen underlying FSTs, suggesting that the simulation process is internalized by the fine-tuned model.</abstract>
      <url hash="cc35ca57">2024.acl-long.355</url>
      <bibkey>lindemann-etal-2024-sip</bibkey>
      <doi>10.18653/v1/2024.acl-long.355</doi>
    </paper>
    <paper id="356">
      <title>The Hidden Space of Transformer Language Adapters</title>
      <author><first>Jesujoba</first><last>Alabi</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Marius</first><last>Mosbach</last><affiliation>McGill University and Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Matan</first><last>Eyal</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last><affiliation>Saarland University</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Tel Aviv University and Google Research</affiliation></author>
      <pages>6588-6607</pages>
      <abstract>We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model’s frozen representation space while largely preserving its structure, rather than on an isolated subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.</abstract>
      <url hash="3d667776">2024.acl-long.356</url>
      <bibkey>alabi-etal-2024-hidden</bibkey>
      <doi>10.18653/v1/2024.acl-long.356</doi>
    </paper>
    <paper id="357">
      <title>A Ship of Theseus: Curious Cases of Paraphrasing in <fixed-case>LLM</fixed-case>-Generated Texts</title>
      <author><first>Nafis Irtiza</first><last>Tripto</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Saranya</first><last>Venkatraman</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Dominik</first><last>Macko</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Robert</first><last>Moro</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Ivan</first><last>Srba</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Adaku</first><last>Uchendu</last><affiliation>MIT Lincoln Laboratory, Massachusetts Institute of Technology</affiliation></author>
      <author><first>Thai</first><last>Le</last><affiliation>Indiana University</affiliation></author>
      <author><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>6608-6625</pages>
      <abstract>In the realm of text manipulation and linguistic transformation, the question of authorship has been a subject of fascination and philosophical inquiry. Much like the Ship of Theseus paradox, which ponders whether a ship remains the same when each of its original planks is replaced, our research delves into an intriguing question: Does a text retain its original authorship when it undergoes numerous paraphrasing iterations? Specifically, since Large Language Models (LLMs) have demonstrated remarkable proficiency in both the generation of original content and the modification of human-authored texts, a pivotal question emerges concerning the determination of authorship in instances where LLMs or similar paraphrasing tools are employed to rephrase the text–i.e., whether authorship should be attributed to the original human author or the AI-powered tool. Therefore, we embark on a philosophical voyage through the seas of language and authorship to unravel this intricate puzzle. Using a computational approach, we discover that the diminishing performance in text classification models, with each successive paraphrasing iteration, is closely associated with the extent of deviation from the original author’s style, thus provoking a reconsideration of the current notion of authorship.</abstract>
      <url hash="07a21de3">2024.acl-long.357</url>
      <bibkey>tripto-etal-2024-ship</bibkey>
      <doi>10.18653/v1/2024.acl-long.357</doi>
    </paper>
    <paper id="358">
      <title>Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations</title>
      <author><first>Guan-Ting</first><last>Lin</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Cheng-Han</first><last>Chiang</last></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <pages>6626-6642</pages>
      <abstract>In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly. Our goal is to teach the LLM that “even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different”. Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses will be different. To teach LLMs to understand and respond properly to the speaking styles, we propose the <b>Spoken-LLM</b> framework that can model the linguistic content and the speaking styles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles. Based on extensive experiments, we show that Spoken-LLM outperforms text-only baselines and prior speech LLMs methods.</abstract>
      <url hash="2bfbf311">2024.acl-long.358</url>
      <bibkey>lin-etal-2024-advancing</bibkey>
      <doi>10.18653/v1/2024.acl-long.358</doi>
    </paper>
    <paper id="359">
      <title><fixed-case>R</fixed-case>etina<fixed-case>QA</fixed-case>: A Robust Knowledge Base Question Answering Model for both Answerable and Unanswerable Questions</title>
      <author><first>Prayushi</first><last>Faldu</last></author>
      <author><first>Indrajit</first><last>Bhattacharya</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Mausam</first><last>.</last><affiliation>Indian Institute of Technology Delhi</affiliation></author>
      <pages>6643-6656</pages>
      <abstract>An essential requirement for a real-world Knowledge Base Question Answering (KBQA) system is the ability to detect the answerability of questions when generating logical forms. However, state-of-the-art KBQA models assume all questions to be answerable. Recent research has found that such models, when superficially adapted to detect answerability, struggle to satisfactorily identify the different categories of unanswerable questions, and simultaneously preserve good performance for answerable questions. Towards addressing this issue, we propose RetinaQA, a new KBQA model that unifies two key ideas in a single KBQA architecture: (a) discrimination over candidate logical forms, rather than generating these, for handling schema-related unanswerability, and (b) sketch-filling-based construction of candidate logical forms for handling data-related unaswerability. Our results show that RetinaQA significantly outperforms adaptations of state-of-the-art KBQA models in handling both answerable and unanswerable questions and demonstrates robustness across all categories of unanswerability. Notably, RetinaQA also sets a new state-of-the-art for answerable KBQA, surpassing existing models. We release our code base for further research: https://github.com/dair-iitd/RetinaQA.</abstract>
      <url hash="5b258949">2024.acl-long.359</url>
      <bibkey>faldu-etal-2024-retinaqa</bibkey>
      <doi>10.18653/v1/2024.acl-long.359</doi>
    </paper>
    <paper id="360">
      <title><fixed-case>G</fixed-case>rounding<fixed-case>GPT</fixed-case>: Language Enhanced Multi-modal Grounding Model</title>
      <author><first>Zhaowei</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Xu</last></author>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Hang</first><last>Song</last></author>
      <author><first>YiQing</first><last>Cai</last></author>
      <author><first>Qi</first><last>Qi</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Ran</first><last>Zhou</last></author>
      <author><first>Junting</first><last>Pan</last></author>
      <author><first>Zefeng</first><last>Li</last></author>
      <author><first>Vu</first><last>Tu</last></author>
      <author><first>Zhida</first><last>Huang</last></author>
      <author><first>Tao</first><last>Wang</last><affiliation>Bytedance group</affiliation></author>
      <pages>6657-6678</pages>
      <abstract>Multi-modal large language models (MLLMs) have demonstrated remarkable performance across various tasks. However, these models often prioritize capturing global information and overlook the importance of perceiving local information. This limitation hinders their ability to effectively understand fine-grained details and handle grounding tasks that necessitate nuanced comprehension. Although some recent works have made strides in this, they have primarily focused on single-modality inputs. Therefore, we propose <b>GroundingGPT</b>, an end-to-end language enhanced multi-modal grounding model. It is designed to perform fine-grained grounding tasks for three modalities: image, video and audio. To enhance the model’s performance, we adopt a coarse-to-fine training strategy, utilizing a three-stage training approach to progressively enhance the model’s semantic awareness and fine-grained understanding capabilities. Additionally, we employ a diversified stage-specific dataset construction pipeline, developing a multi-modal, multi-granularity dataset tailored for training the model in different stages. Extensive experiments conducted on multiple multi-modal benchmarks demonstrate that our model achieves impressive fine-grained understanding of multi-modal inputs on grounding tasks while maintaining or improving its global comprehension capabilities. Our code, model, and dataset are available at https://github.com/lzw-lzw/GroundingGPT.</abstract>
      <url hash="08f29138">2024.acl-long.360</url>
      <bibkey>li-etal-2024-groundinggpt</bibkey>
      <doi>10.18653/v1/2024.acl-long.360</doi>
    </paper>
    <paper id="361">
      <title>Automated Justification Production for Claim Veracity in Fact Checking: A Survey on Architectures and Approaches</title>
      <author><first>Islam</first><last>Eldifrawi</last></author>
      <author><first>Shengrui</first><last>Wang</last><affiliation>Université de Sherbrooke</affiliation></author>
      <author><first>Amine</first><last>Trabelsi</last><affiliation>Université de Sherbrooke</affiliation></author>
      <pages>6679-6692</pages>
      <abstract>Automated Fact-Checking (AFC) is the automated verification of claim accuracy. AFC is crucial in discerning truth from misinformation, especially given the huge amounts of content are generated online daily. Current research focuses on predicting claim veracity through metadata analysis and language scrutiny, with an emphasis on justifying verdicts. This paper surveys recent methodologies, proposinga comprehensive taxonomy and presenting the evolution of research in that landscape. A comparative analysis of methodologies and futuredirections for improving fact-checking explainability are also discussed.</abstract>
      <url hash="1a475e40">2024.acl-long.361</url>
      <bibkey>eldifrawi-etal-2024-automated</bibkey>
      <doi>10.18653/v1/2024.acl-long.361</doi>
    </paper>
    <paper id="362">
      <title>Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen Languages</title>
      <author><first>Carlos</first><last>Mullov</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Quan</first><last>Pham</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>6693-6709</pages>
      <abstract>Multilingual neural machine translation systems learn to map sentences of different languages into a common representation space. Intuitively, with a growing number of seen languages the encoder sentence representation grows more flexible and easily adaptable to new languages. In this work, we test this hypothesis by zero-shot translating from unseen languages. To deal with unknown vocabularies from unknown languages we propose a setup where we decouple learning of vocabulary and syntax, i.e. for each language we learn word representations in a separate step (using cross-lingual word embeddings), and then train to translate while keeping those word representations frozen. We demonstrate that this setup enables zero-shot translation from entirely unseen languages. Zero-shot translating with a model trained on Germanic and Romance languages we achieve scores of 42.6 BLEU for Portuguese-English and 20.7 BLEU for Russian-English on TED domain. We explore how this zero-shot translation capability develops with varying number of languages seen by the encoder. Lastly, we explore the effectiveness of our decoupled learning strategy for unsupervised machine translation. By exploiting our model’s zero-shot translation capability for iterative back-translation we attain near parity with a supervised setting.</abstract>
      <url hash="2e5b0070">2024.acl-long.362</url>
      <bibkey>mullov-etal-2024-decoupled</bibkey>
      <doi>10.18653/v1/2024.acl-long.362</doi>
    </paper>
    <paper id="363">
      <title><fixed-case>S</fixed-case>wap<fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>: Serving Off-the-shelf <fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>-based Large Language Models with Tunable Memory Budget</title>
      <author><first>Rui</first><last>Kong</last></author>
      <author><first>Yuanchun</first><last>Li</last><affiliation>Institute for AI Industry Research, Tsinghua University</affiliation></author>
      <author><first>Qingtian</first><last>Feng</last></author>
      <author><first>Weijun</first><last>Wang</last></author>
      <author><first>Xiaozhou</first><last>Ye</last></author>
      <author><first>Ye</first><last>Ouyang</last></author>
      <author><first>Linghe</first><last>Kong</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yunxin</first><last>Liu</last><affiliation>Institute for AI Industry Research, Tsinghua University</affiliation></author>
      <pages>6710-6720</pages>
      <abstract>Mixture of experts (MoE) is a popular technique to improve capacity of Large Language Models (LLMs) with conditionally-activated parallel experts. However, serving MoE models on memory-constrained devices is challenging due to the large parameter size. Typical solutions such as memory swapping or expert pruning may lead to significantly higher latency or severe accuracy loss.In this paper, we introduce SwapMoE, a framework for efficient serving of MoE-based large language models with tunable memory budgets. The main idea of SwapMoE is to keep a small dynamic set of important experts, namely Virtual Experts, in the main memory for inference, while seamlessly maintaining how the Virtual Experts map to the actual experts. Experiments have shown that SwapMoE can reduce the memory footprint while maintaining reasonable accuracy. For example, on text summarization tasks with Switch Transformer, SwapMoE can reduce the memory consumption from 14.2 GiB to 4.7 GiB, together with 50% latency reduction and a slight Rouge-2 score drop of 0.041.</abstract>
      <url hash="5729fc3e">2024.acl-long.363</url>
      <bibkey>kong-etal-2024-swapmoe</bibkey>
      <doi>10.18653/v1/2024.acl-long.363</doi>
    </paper>
    <paper id="364">
      <title><fixed-case>P</fixed-case>ix<fixed-case>T</fixed-case>3: Pixel-based Table-To-Text Generation</title>
      <author><first>Iñigo</first><last>Alonso</last><affiliation>University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Eneko</first><last>Agirre</last><affiliation>University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>6721-6736</pages>
      <abstract>Table-to-text generation involves generating appropriate textual descriptions given structured tabular data. It has attracted increasing attention in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. A common feature across existing methods is their treatment of the input as a string, i.e., by employing linearization techniques that do not always preserve information in the table, are verbose, and lack space efficiency. We propose to rethink data-to-text generation as a visual recognition task, removing the need for rendering the input in a string format. We present PixT3, a multimodal table-to-text model that overcomes the challenges of linearization and input size limitations encountered by existing models. PixT3 is trained with a new self-supervised learning objective to reinforce table structure awareness and is applicable to open-ended and controlled generation settings. Experiments on the ToTTo and Logic2Text benchmarks show that PixT3 is competitive and, in some settings, superior to generators that operate solely on text.</abstract>
      <url hash="78afcd39">2024.acl-long.364</url>
      <bibkey>alonso-etal-2024-pixt3</bibkey>
      <doi>10.18653/v1/2024.acl-long.364</doi>
    </paper>
    <paper id="365">
      <title>Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers</title>
      <author><first>Gal</first><last>Yona</last><affiliation>Research, Google</affiliation></author>
      <author><first>Roee</first><last>Aharoni</last><affiliation>Google</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Tel Aviv University and Google Research</affiliation></author>
      <pages>6737-6751</pages>
      <abstract>Factual questions typically can be answered correctly at different levels of granularity. For example, both “August 4, 1961” and “1961” are correct answers to the question “When was Barack Obama born?”. Standard question answering (QA) evaluation protocols, however, do not explicitly take this into account and compare a predicted answer against answers of a single granularity level. In this work, we propose GRANOLA QA, a novel evaluation setting where a predicted answer is evaluated in terms of accuracy and informativeness against a set of multi-granularity answers. We present a simple methodology for enriching existing datasets with multi-granularity answers, and create GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm, called Decoding with Response Aggregation (DRAG), that is geared towards aligning the response granularity with the model’s uncertainty. Our experiments show that large language models with standard decoding tend to generate specific answers, which are often incorrect. In contrast, when evaluated on multi-granularity answers, DRAG yields a nearly 20 point increase in accuracy on average, which further increases for rare entities. Overall, this reveals that standard evaluation and decoding schemes may significantly underestimate the knowledge encapsulated in LMs.</abstract>
      <url hash="7826cbb9">2024.acl-long.365</url>
      <bibkey>yona-etal-2024-narrowing</bibkey>
      <doi>10.18653/v1/2024.acl-long.365</doi>
    </paper>
    <paper id="366">
      <title><fixed-case>TAMS</fixed-case>: Translation-Assisted Morphological Segmentation</title>
      <author><first>Enora</first><last>Rice</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Ali</first><last>Marashian</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Luke</first><last>Gessler</last><affiliation>Indiana University</affiliation></author>
      <author><first>Alexis</first><last>Palmer</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Katharina</first><last>von der Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <pages>6752-6765</pages>
      <abstract>Canonical morphological segmentation is the process of analyzing words into the standard (<i>aka</i> underlying) forms of their constituent morphemes.This is a core task in endangered language documentation, and NLP systems have the potential to dramatically speed up this process. In typical language documentation settings, training data for canonical morpheme segmentation is scarce, making it difficult to train high quality models. However, translation data is often much more abundant, and, in this work, we present a method that attempts to leverage translation data in the canonical segmentation task. We propose a character-level sequence-to-sequence model that incorporates representations of translations obtained from pretrained high-resource monolingual language models as an additional signal. Our model outperforms the baseline in a super-low resource setting but yields mixed results on training splits with more data. Additionally, we find that we can achieve strong performance even without needing difficult-to-obtain word level alignments. While further work is needed to make translations useful in higher-resource settings, our model shows promise in severely resource-constrained settings.</abstract>
      <url hash="41d9ed81">2024.acl-long.366</url>
      <bibkey>rice-etal-2024-tams</bibkey>
      <doi>10.18653/v1/2024.acl-long.366</doi>
    </paper>
    <paper id="367">
      <title><fixed-case>XC</fixed-case>ode<fixed-case>E</fixed-case>val: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval</title>
      <author><first>Mohammad Abdullah Matin</first><last>Khan</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>M Saiful</first><last>Bari</last><affiliation>National Centre of Artificial Intelligence, Saudi Data and AI Authority</affiliation></author>
      <author><first>Do</first><last>Long</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Weishi</first><last>Wang</last></author>
      <author><first>Md Rizwan</first><last>Parvez</last><affiliation>Qatar Computing Research Institute and Bosch</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <pages>6766-6805</pages>
      <abstract>Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce *xCodeEval*, the largest executable multilingual multitask benchmark to date consisting of 25 M document-level coding examples (16.5 B tokens) from about 7.5 K unique problems covering up to 11 programming languages with execution-level parallelism. It features a total of 7 tasks involving code understanding, generation, translation and retrieval. *xCodeEval* adopts an execution-based evaluation and offers a multilingual code execution engine, *ExecEval* that supports unit test based execution in all the 11 languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI’s LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate to be quite challenging as per the current advancements in language models.</abstract>
      <url hash="e2e65a30">2024.acl-long.367</url>
      <bibkey>khan-etal-2024-xcodeeval</bibkey>
      <doi>10.18653/v1/2024.acl-long.367</doi>
    </paper>
    <paper id="368">
      <title><fixed-case>P</fixed-case>roxy<fixed-case>QA</fixed-case>: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models</title>
      <author><first>Haochen</first><last>Tan</last></author>
      <author><first>Zhijiang</first><last>Guo</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Zhan</first><last>Shi</last></author>
      <author><first>Lu</first><last>Xu</last></author>
      <author><first>Zhili</first><last>Liu</last></author>
      <author><first>Yunlong</first><last>Feng</last></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Linqi</first><last>Song</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>6806-6827</pages>
      <abstract>Large Language Models (LLMs) have succeeded remarkably in understanding long-form contents. However, exploring their capability for generating long-form contents, such as reports and articles, has been relatively unexplored and inadequately assessed by existing benchmarks. The prevalent evaluation methods, which predominantly rely on crowdsourcing, are recognized for their labor-intensive nature and lack of efficiency, whereas automated metrics, such as the ROUGE score, demonstrate discordance with human judgment criteria. In this paper, we propose ProxyQA, an innovative framework dedicated to assessing long-text generation. ProxyQA comprises in-depth human-curated meta-questions spanning various domains, each accompanied by specific proxy-questions with pre-annotated answers. LLMs are tasked to generate extensive content in response to these meta-questions, by engaging an evaluator and incorporating the generated texts as contextual background, ProxyQA assesses the generated content’s quality through the evaluator’s accuracy in addressing the proxy-questions. We examine multiple LLMs, emphasizing ProxyQA’s demanding nature as a high-quality assessment tool. Human evaluation demonstrates that the proxy-question method is notably self-consistent and aligns closely with human evaluative standards. The dataset and leaderboard is available at <url>https://proxy-qa.com</url>.</abstract>
      <url hash="187a4938">2024.acl-long.368</url>
      <bibkey>tan-etal-2024-proxyqa</bibkey>
      <doi>10.18653/v1/2024.acl-long.368</doi>
    </paper>
    <paper id="369">
      <title>A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia</title>
      <author><first>Giovanni</first><last>Monea</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Maxime</first><last>Peyrard</last><affiliation>CNRS</affiliation></author>
      <author><first>Martin</first><last>Josifoski</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Vishrav</first><last>Chaudhary</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jason</first><last>Eisner</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Emre</first><last>Kiciman</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hamid</first><last>Palangi</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Barun</first><last>Patra</last><affiliation>Microsoft</affiliation></author>
      <author><first>Robert</first><last>West</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <pages>6828-6844</pages>
      <abstract>Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model’s internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.</abstract>
      <url hash="8b26f0d8">2024.acl-long.369</url>
      <bibkey>monea-etal-2024-glitch</bibkey>
      <doi>10.18653/v1/2024.acl-long.369</doi>
    </paper>
    <paper id="370">
      <title>Muffin or <fixed-case>C</fixed-case>hihuahua? Challenging Multimodal Large Language Models with Multipanel <fixed-case>VQA</fixed-case></title>
      <author><first>Yue</first><last>Fan</last></author>
      <author><first>Jing</first><last>Gu</last></author>
      <author><first>Kaiwen</first><last>Zhou</last></author>
      <author><first>Qianqi</first><last>Yan</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Shan</first><last>Jiang</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Ching-Chen</first><last>Kuo</last></author>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Xinze</first><last>Guan</last><affiliation>Oregon State University</affiliation></author>
      <author><first>Xin</first><last>Wang</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <pages>6845-6863</pages>
      <abstract>Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Multimodal Large Language Models (MLLMs) tested, even though humans can attain approximately 99% accuracy on these questions. Distinctively, the MultipanelVQA benchmark features synthetically generated multipanel images specifically crafted to isolate and assess the impact of various factors, such as the layout, on MLLMs’ multipanel image comprehension abilities. As a result, in addition to benchmarking the capabilities of MLLMs in understanding multipanel images, we analyze various factors of the multipanel image that affect MLLMs’ performance with synthetic data and offer insights for enhancement.</abstract>
      <url hash="129c9b06">2024.acl-long.370</url>
      <bibkey>fan-etal-2024-muffin</bibkey>
      <doi>10.18653/v1/2024.acl-long.370</doi>
    </paper>
    <paper id="371">
      <title><fixed-case>W</fixed-case>eb<fixed-case>V</fixed-case>oyager: Building an End-to-End Web Agent with Large Multimodal Models</title>
      <author><first>Hongliang</first><last>He</last><affiliation>Westlake University</affiliation></author>
      <author><first>Wenlin</first><last>Yao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Kaixin</first><last>Ma</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wenhao</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yong</first><last>Dai</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Zhenzhong</first><last>Lan</last><affiliation>Westlake University</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>6864-6890</pages>
      <abstract>The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1% task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents.</abstract>
      <url hash="389bb686">2024.acl-long.371</url>
      <bibkey>he-etal-2024-webvoyager</bibkey>
      <doi>10.18653/v1/2024.acl-long.371</doi>
    </paper>
    <paper id="372">
      <title>Translation-based Lexicalization Generation and Lexical Gap Detection: Application to Kinship Terms</title>
      <author><first>Senyu</first><last>Li</last></author>
      <author><first>Bradley</first><last>Hauer</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Ning</first><last>Shi</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Grzegorz</first><last>Kondrak</last><affiliation>University of Alberta</affiliation></author>
      <pages>6891-6900</pages>
      <abstract>Constructing lexicons with explicitly identified lexical gaps is a vital part of building multilingual lexical resources. Prior work has leveraged bilingual dictionaries and linguistic typologies for semi-automatic identification of lexical gaps. Instead, we propose a generally-applicable algorithmic method to automatically generate concept lexicalizations, which is based on machine translation and hypernymy relations between concepts. The absence of a lexicalization implies a lexical gap. We apply our method to kinship terms, which make a suitable case study because of their explicit definitions and regular structure. Empirical evaluations demonstrate that our approach yields higher accuracy than BabelNet and ChatGPT. Our error analysis indicates that enhancing the quality of translations can further improve the accuracy of our method.</abstract>
      <url hash="e59de623">2024.acl-long.372</url>
      <bibkey>li-etal-2024-translation</bibkey>
      <doi>10.18653/v1/2024.acl-long.372</doi>
    </paper>
    <paper id="373">
      <title>Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations</title>
      <author><first>Ritam</first><last>Dutt</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zhen</first><last>Wu</last></author>
      <author><first>Jiaxin</first><last>Shi</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Divyanshu</first><last>Sheth</last></author>
      <author><first>Prakhar</first><last>Gupta</last></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>6901-6929</pages>
      <abstract>We present a generalizable classification approach that leverages Large Language Models (LLMs) to facilitate the detection of implicitly encoded social meaning in conversations. We design a multi-faceted prompt to extract a textual explanation of the reasoning that connects visible cues to underlying social meanings. These extracted explanations or rationales serve as augmentations to the conversational text to facilitate dialogue understanding and transfer. Our empirical results over 2,340 experimental settings demonstrate the significant positive impact of adding these rationales. Our findings hold true for in-domain classification, zero-shot, and few-shot domain transfer for two different social meaning detection tasks, each spanning two different corpora.</abstract>
      <url hash="bba1ce6e">2024.acl-long.373</url>
      <bibkey>dutt-etal-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.acl-long.373</doi>
    </paper>
    <paper id="374">
      <title>Robust Frame-Semantic Models with Lexical Unit Trees and Negative Samples</title>
      <author><first>Jacob</first><last>Devasier</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Yogesh</first><last>Gurjar</last></author>
      <author><first>Chengkai</first><last>Li</last><affiliation>University of Texas at Arlington</affiliation></author>
      <pages>6930-6941</pages>
      <abstract>We present novel advancements in frame-semantic parsing, specifically focusing on target identification and frame identification. Our target identification model employs a novel prefix tree modification to enable robust support for multi-word lexical units, resulting in a coverage of 99.4% of the targets in the FrameNet 1.7 fulltext annotations. It utilizes a RoBERTa-based filter to achieve an F1 score of 0.775, surpassing the previous state-of-the-art solution by +0.012. For frame identification, we introduce a modification to the standard multiple-choice classification paradigm by incorporating additional negative frames for targets with limited candidate frames, resulting in a +0.014 accuracy improvement over the frame-only model of FIDO, the previous state-of-the-art system, and +0.002 over its full system. Our approach significantly enhances performance on rare frames, exhibiting an improvement of +0.044 over FIDO’s accuracy on frames with 5 or fewer samples, and on under-utilized frames, with an improvement of +0.139 on targets with a single candidate frame. Overall, our contributions address critical challenges and advance the state-of-the-art in frame-semantic parsing.</abstract>
      <url hash="fd7f6528">2024.acl-long.374</url>
      <bibkey>devasier-etal-2024-robust</bibkey>
      <doi>10.18653/v1/2024.acl-long.374</doi>
    </paper>
    <paper id="375">
      <title>Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation</title>
      <author><first>Yuan</first><last>Yang</last></author>
      <author><first>Siheng</first><last>Xiong</last></author>
      <author><first>Ali</first><last>Payani</last><affiliation>Cisco</affiliation></author>
      <author><first>Ehsan</first><last>Shareghi</last><affiliation>Monash University and University of Cambridge</affiliation></author>
      <author><first>Faramarz</first><last>Fekri</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>6942-6959</pages>
      <abstract>Advancements in logical reasoning, utilizing LLMs to convert natural language into logical symbolism, combined with the use of external theorem provers, have repositioned the symbolic approach as a central point of interest. The main challenge within this paradigm lies in the LLMs’ capability to accurately translate natural language (NL) statements into first-order-logic (FOL) expressions. Although LLMs have shown notable success, there remains a gap in understanding the limitations and challenges they encounter in NL-FOL translation. This is primarily due to the absence of datasets and evaluation test beds at the required fine-grained level. We present MALLS, a dataset of 28K diverse and verified sentence-level NL-FOL pairs collected from GPT4. We utilize a combined strategy of FOL rule parsing, human annotation, and automatic filtering to ensure quality. We also present LogicLLaMA, a LLaMA2-7B/13B fine-tuned on MALLS for NL-FOL translation, which can be used standalone or to correct previously generated rules by GPT3.5 after being further fine-tuned via a novel reinforcement learning with human feedback (RLHF) framework. We benchmark a wide range of LLMs on MALLS and previous datasets, highlighting weaknesses in them in NL-FOL translation and demonstrating the advantages of MALLS. We also show that LogicLLaMA achieves GPT4-level performance and can generalize to other datasets. Project repo is available at https://github.com/gblackout/LogicLLaMA</abstract>
      <url hash="d849bb0c">2024.acl-long.375</url>
      <bibkey>yang-etal-2024-harnessing</bibkey>
      <doi>10.18653/v1/2024.acl-long.375</doi>
    </paper>
    <paper id="376">
      <title>Lightweight reranking for language model generations</title>
      <author><first>Siddhartha</first><last>Jain</last><affiliation>Amazon</affiliation></author>
      <author><first>Xiaofei</first><last>Ma</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Anoop</first><last>Deoras</last><affiliation>Amazon</affiliation></author>
      <author><first>Bing</first><last>Xiang</last><affiliation>Amazon</affiliation></author>
      <pages>6960-6984</pages>
      <abstract>Large Language Models (LLMs) can exhibit considerable variation in the quality of their sampled outputs. Reranking and selecting the best generation from the sampled set is a popular way of obtaining strong gains in generation quality. In this paper, we present a novel approach for reranking LLM generations. Unlike other techniques that might involve additional inferences or training a specialized reranker, our approach relies on easy to compute pairwise statistics between the generations that have minimal compute overhead. We show that our approach can be formalized as an extension of self-consistency and analyze its performance in that framework, theoretically as well as via simulations. We show strong improvements for selecting the best k generations for code generation tasks as well as robust improvements for the best generation for the tasks of autoformalization, summarization, and translation. While our approach only assumes black-box access to LLMs, we show that additional access to token probabilities can improve performance even further.</abstract>
      <url hash="44173a1a">2024.acl-long.376</url>
      <bibkey>jain-etal-2024-lightweight</bibkey>
      <doi>10.18653/v1/2024.acl-long.376</doi>
    </paper>
    <paper id="377">
      <title><fixed-case>ARIES</fixed-case>: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews</title>
      <author><first>Mike</first><last>D’Arcy</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Alexis</first><last>Ross</last><affiliation>Massachusetts Institute of Technology and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Erin</first><last>Bransom</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Jonathan</first><last>Bragg</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Tom</first><last>Hope</last><affiliation>Allen Institute for Artificial Intelligence and Hebrew University, Hebrew University of Jerusalem</affiliation></author>
      <author><first>Doug</first><last>Downey</last><affiliation>Allen Institute for Artificial Intelligence and Northwestern University</affiliation></author>
      <pages>6985-7001</pages>
      <abstract>We introduce the task of automatically revising scientific papers based on peer feedback and release ARIES, a dataset of review comments and their corresponding paper edits. The data is drawn from real reviewer-author interactions from computer science, and we provide labels linking each reviewer comment to the specific paper edits made by the author in response. We automatically create a high-precision silver training set, as well as an expert-labeled test set that shows high inter-annotator agreement. In experiments with 10 models covering the state of the art, we find that they struggle even to identify which edits correspond to a comment—especially when the relationship between the edit and the comment is indirect and requires reasoning to uncover. We also extensively analyze GPT-4’s ability to generate edits given a comment and the original paper. We find that it often succeeds on a superficial level, but tends to rigidly follow the wording of the feedback rather than the underlying intent, and lacks technical details compared to human-written edits.</abstract>
      <url hash="1deda05d">2024.acl-long.377</url>
      <bibkey>darcy-etal-2024-aries</bibkey>
      <doi>10.18653/v1/2024.acl-long.377</doi>
    </paper>
    <paper id="378">
      <title>The Unreasonable Effectiveness of Easy Training Data for Hard Tasks</title>
      <author><first>Peter</first><last>Hase</last></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Sarah</first><last>Wiegreffe</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>7002-7024</pages>
      <abstract>How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current pretrained language models often generalize relatively well from easy to hard data, even performing as well as oracle models finetuned on hard data. We demonstrate this kind of easy-to-hard generalization using simple finetuning methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect easy data rather than hard data for finetuning, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied.</abstract>
      <url hash="a33af53a">2024.acl-long.378</url>
      <bibkey>hase-etal-2024-unreasonable</bibkey>
      <doi>10.18653/v1/2024.acl-long.378</doi>
    </paper>
    <paper id="379">
      <title><fixed-case>PLUG</fixed-case>: Leveraging Pivot Language in Cross-Lingual Instruction Tuning</title>
      <author><first>Zhihan</first><last>Zhang</last></author>
      <author><first>Dong-Ho</first><last>Lee</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Yuwei</first><last>Fang</last><affiliation>Snap Inc.</affiliation></author>
      <author><first>Wenhao</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Mengzhao</first><last>Jia</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Francesco</first><last>Barbieri</last><affiliation>Snap Inc.</affiliation></author>
      <pages>7025-7046</pages>
      <abstract>Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following abilities of LLMs by 29% on average, compared to directly responding in the target language alone. Further experiments validate the versatility of our approach by employing alternative pivot languages beyond English to assist languages where LLMs exhibit lower proficiency. Code and data are available at https://github.com/ytyz1307zzh/PLUG.</abstract>
      <url hash="0a686249">2024.acl-long.379</url>
      <bibkey>zhang-etal-2024-plug</bibkey>
      <doi>10.18653/v1/2024.acl-long.379</doi>
    </paper>
    <paper id="380">
      <title><fixed-case>MIDGARD</fixed-case>: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning</title>
      <author><first>Inderjeet</first><last>Nair</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>7047-7065</pages>
      <abstract>We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability. Additionally, relying solely on a single sample may result in the omission of true nodes and edges. To counter this, we draw inspiration from self-consistency (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer. To tackle the substantial challenge of applying SC on generated graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of Reasoning in Directed acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM. This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision. Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts.</abstract>
      <url hash="23cd6547">2024.acl-long.380</url>
      <bibkey>nair-wang-2024-midgard</bibkey>
      <doi>10.18653/v1/2024.acl-long.380</doi>
    </paper>
    <paper id="381">
      <title><fixed-case>R</fixed-case>e<fixed-case>C</fixed-case>oncile: Round-Table Conference Improves Reasoning via Consensus among Diverse <fixed-case>LLM</fixed-case>s</title>
      <author><first>Justin</first><last>Chen</last></author>
      <author><first>Swarnadeep</first><last>Saha</last><affiliation>Department of Computer Science, University of North Carolina, Chapel Hill</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>7066-7085</pages>
      <abstract>Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents. ReConcile enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, ReConcile initiates discussion between agents via a ‘discussion prompt’ that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that ReConcile significantly improves LLMs’ reasoning – both individually and as a team – surpassing prior single-agent and multi-agent baselines by up to 11.4% and even outperforming GPT-4 on three datasets. ReConcile also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8% improvement on MATH. Finally, we analyze the individual components of ReConcile, demonstrating that the diversity originating from different models is critical to its superior performance.</abstract>
      <url hash="608e249e">2024.acl-long.381</url>
      <bibkey>chen-etal-2024-reconcile</bibkey>
      <doi>10.18653/v1/2024.acl-long.381</doi>
    </paper>
    <paper id="382">
      <title>Mirror: Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning</title>
      <author><first>Hanqi</first><last>Yan</last></author>
      <author><first>Qinglin</first><last>Zhu</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Xinyu</first><last>Wang</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>7086-7103</pages>
      <abstract>While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by the Reasoner. The experiments on five reasoning datasets demonstrate that Mirror’s superiority over several contemporary self-reflection approaches. Additionally, the ablation study studies clearly indicate that our strategies alleviate the aforementioned challenges.</abstract>
      <url hash="12324b35">2024.acl-long.382</url>
      <bibkey>yan-etal-2024-mirror</bibkey>
      <doi>10.18653/v1/2024.acl-long.382</doi>
    </paper>
    <paper id="383">
      <title>Where Do People Tell Stories Online? Story Detection Across Online Communities</title>
      <author><first>Maria</first><last>Antoniak</last></author>
      <author><first>Joel</first><last>Mire</last></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Elliott</first><last>Ash</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Andrew</first><last>Piper</last><affiliation>McGill University</affiliation></author>
      <pages>7104-7130</pages>
      <abstract>Story detection in online communities is a challenging task as stories are scattered across communities and interwoven with non-storytelling spans within a single text. We address this challenge by building and releasing the StorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts and comments, a detailed codebook adapted to the social media context, and models to predict storytelling at the document and span levels. Our dataset is sampled from hundreds of popular English-language Reddit communities ranging across 33 topic categories, and it contains fine-grained expert annotations, including binary story labels, story spans, and event spans. We evaluate a range of detection methods using our data, and we identify the distinctive textual features of online storytelling, focusing on storytelling spans, which we introduce as a new task. We illuminate distributional characteristics of storytelling on a large community-centric social media platform, and we also conduct a case study on r/ChangeMyView, where storytelling is used as one of many persuasive strategies, illustrating that our data and models can be used for both inter- and intra-community research. Finally, we discuss implications of our tools and analyses for narratology and the study of online communities.</abstract>
      <url hash="29bb052a">2024.acl-long.383</url>
      <bibkey>antoniak-etal-2024-people</bibkey>
      <doi>10.18653/v1/2024.acl-long.383</doi>
    </paper>
    <paper id="384">
      <title>Large Language Models Are No Longer Shallow Parsers</title>
      <author><first>Yuanhe</first><last>Tian</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Fei</first><last>Xia</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Yan</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>7131-7142</pages>
      <abstract>The development of large language models (LLMs) brings significant changes to the field of natural language processing (NLP), enabling remarkable performance in various high-level tasks, such as machine translation, question-answering, dialogue generation, etc., under end-to-end settings without requiring much training data. Meanwhile, fundamental NLP tasks, particularly syntactic parsing, are also essential for language study as well as evaluating the capability of LLMs for instruction understanding and usage. In this paper, we focus on analyzing and improving the capability of current state-of-the-art LLMs on a classic fundamental task, namely constituency parsing, which is the representative syntactic task in both linguistics and natural language processing. We observe that these LLMs are effective in shallow parsing but struggle with creating correct full parse trees. To improve the performance of LLMs on deep syntactic parsing, we propose a three-step approach that firstly prompts LLMs for chunking, then filters out low-quality chunks, and finally adds the remaining chunks to prompts to instruct LLMs for parsing, with later enhancement by chain-of-thought prompting. Experimental results on English and Chinese benchmark datasets demonstrate the effectiveness of our approach on improving LLMs’ performance on constituency parsing.</abstract>
      <url hash="77765fa8">2024.acl-long.384</url>
      <bibkey>tian-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.acl-long.384</doi>
    </paper>
    <paper id="385">
      <title>Dialogue Summarization with Mixture of Experts based on Large Language Models</title>
      <author><first>Yuanhe</first><last>Tian</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Fei</first><last>Xia</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Yan</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>7143-7155</pages>
      <abstract>Dialogue summarization is an important task that requires to generate highlights for a conversation from different aspects (e.g., content of various speakers). While several studies successfully employ large language models (LLMs) and achieve satisfying results, they are limited by using one model at a time or treat it as a black box, which makes it hard to discriminatively learn essential content in a dialogue from different aspects, therefore may lead to anticipation bias and potential loss of information in the produced summaries. In this paper, we propose an LLM-based approach with role-oriented routing and fusion generation to utilize mixture of experts (MoE) for dialogue summarization. Specifically, the role-oriented routing is an LLM-based module that selects appropriate experts to process different information; fusion generation is another LLM-based module to locate salient information and produce finalized dialogue summaries. The proposed approach offers an alternative solution to employing multiple LLMs for dialogue summarization by leveraging their capabilities of in-context processing and generation in an effective manner. We run experiments on widely used benchmark datasets for this task, where the results demonstrate the superiority of our approach in producing informative and accurate dialogue summarization.</abstract>
      <url hash="4951222b">2024.acl-long.385</url>
      <bibkey>tian-etal-2024-dialogue</bibkey>
      <doi>10.18653/v1/2024.acl-long.385</doi>
    </paper>
    <paper id="386">
      <title><fixed-case>C</fixed-case>hi<fixed-case>M</fixed-case>ed-<fixed-case>GPT</fixed-case>: A <fixed-case>C</fixed-case>hinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences</title>
      <author><first>Yuanhe</first><last>Tian</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Ruyi</first><last>Gan</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yan</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jiaxing</first><last>Zhang</last><affiliation>IDEA</affiliation></author>
      <author><first>Yongdong</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>7156-7173</pages>
      <abstract>Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical infrastructure. With big data, especially texts, forming the foundation of medical services, there is an exigent need for effective natural language processing (NLP) solutions tailored to the healthcare domain. Conventional approaches leveraging pre-trained models present promising results in this domain and current large language models (LLMs) offer advanced foundation for medical text processing. However, most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. In this work, we propose ChiMed-GPT, a new benchmark LLM designed explicitly for Chinese medical domain, and undergoes a comprehensive training regime with pre-training, SFT, and RLHF. Evaluations on tasks including information extraction, question answering, and dialogue generation demonstrate ChiMed-GPT’s superior performance over general domain LLMs. Furthermore, we analyze possible biases through prompting ChiMed-GPT to perform attitude scales regarding discrimination of patients, so as to contribute to further responsible development of LLMs in the medical domain.</abstract>
      <url hash="80aab468">2024.acl-long.386</url>
      <bibkey>tian-etal-2024-chimed</bibkey>
      <revision id="1" href="2024.acl-long.386v1" hash="90d101e0"/>
      <revision id="2" href="2024.acl-long.386v2" hash="80aab468" date="2024-08-17">Fix a typo.</revision>
      <doi>10.18653/v1/2024.acl-long.386</doi>
    </paper>
    <paper id="387">
      <title>An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Daking</first><last>Rai</last></author>
      <author><first>Ziyu</first><last>Yao</last><affiliation>George Mason University</affiliation></author>
      <pages>7174-7193</pages>
      <abstract>Large language models (LLMs) have shown strong arithmetic reasoning capabilities when prompted with Chain-of-Thought (CoT) prompts. However, we have only a limited understanding of how they are processed by LLMs. To demystify it, prior work has primarily focused on ablating different components in the CoT prompt and empirically observing their resulting LLM performance change. Yet, the reason why these components are important to LLM reasoning is not explored. To fill this gap, in this work, we investigate “neuron activation” as a lens to provide a unified explanation to observations made by prior work. Specifically, we look into neurons within the feed-forward layers of LLMs that may have activated their arithmetic reasoning capabilities, using Llama2 as an example. To facilitate this investigation, we also propose an approach based on GPT-4 to automatically identify neurons that imply arithmetic reasoning. Our analyses revealed that the activation of reasoning neurons in the feed-forward layers of an LLM can explain the importance of various components in a CoT prompt, and future research can extend it for a more complete understanding.</abstract>
      <url hash="364a0886">2024.acl-long.387</url>
      <bibkey>rai-yao-2024-investigation</bibkey>
      <revision id="1" href="2024.acl-long.387v1" hash="96778503"/>
      <revision id="2" href="2024.acl-long.387v2" hash="ac2dab44" date="2024-08-29">Minor updates.</revision>
      <revision id="3" href="2024.acl-long.387v3" hash="364a0886" date="2024-09-09">Minor updates.</revision>
      <doi>10.18653/v1/2024.acl-long.387</doi>
    </paper>
    <paper id="388">
      <title>Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling</title>
      <author><first>Hang</first><last>Jiang</last></author>
      <author><first>Xiajie</first><last>Zhang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Robert</first><last>Mahari</last><affiliation>Harvard University and Massachusetts Institute of Technology</affiliation></author>
      <author><first>Daniel</first><last>Kessler</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Eric</first><last>Ma</last></author>
      <author><first>Tal</first><last>August</last></author>
      <author><first>Irene</first><last>Li</last></author>
      <author><first>Alex</first><last>Pentland</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yoon</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Deb</first><last>Roy</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jad</first><last>Kabbara</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>7194-7219</pages>
      <abstract>Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 294 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop approach to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through randomized controlled trials (RCTs) with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond.</abstract>
      <url hash="fdc44dc7">2024.acl-long.388</url>
      <bibkey>jiang-etal-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.acl-long.388</doi>
    </paper>
    <paper id="389">
      <title>Intrinsic Task-based Evaluation for Referring Expression Generation</title>
      <author><first>Guanyi</first><last>Chen</last><affiliation>Central China Normal University</affiliation></author>
      <author><first>Fahime</first><last>Same</last><affiliation>Trivago N.V.</affiliation></author>
      <author><first>Kees</first><last>Van Deemter</last><affiliation>Utrecht University</affiliation></author>
      <pages>7220-7231</pages>
      <abstract>Recently, a human evaluation study of Referring Expression Generation (REG) models had an unexpected conclusion: on WEBNLG, Referring Expressions (REs) generated by the state-of-the-art neural models were not only indistinguishable from the REs in WEBNLG but also from the REs generated by a simple rule-based system. Here, we argue that this limitation could stem from the use of a purely ratings-based human evaluation (which is a common practice in Natural Language Generation). To investigate these issues, we propose an intrinsic task-based evaluation for REG models, in which, in addition to rating the quality of REs, participants were asked to accomplish two meta-level tasks. One of these tasks concerns the referential success of each RE; the other task asks participants to suggest a better alternative for each RE. The outcomes suggest that, in comparison to previous evaluations, the new evaluation protocol assesses the performance of each REG model more comprehensively and makes the participants’ ratings more reliable and discriminable.</abstract>
      <url hash="ab85ba30">2024.acl-long.389</url>
      <bibkey>chen-etal-2024-intrinsic</bibkey>
      <doi>10.18653/v1/2024.acl-long.389</doi>
    </paper>
    <paper id="390">
      <title>From Moments to Milestones: Incremental Timeline Summarization Leveraging Large Language Models</title>
      <author><first>Qisheng</first><last>Hu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Geonsik</first><last>Moon</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Hwee Tou</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>7232-7246</pages>
      <abstract>Timeline summarization (TLS) is essential for distilling coherent narratives from a vast collection of texts, tracing the progression of events and topics over time. Prior research typically focuses on either event or topic timeline summarization, neglecting the potential synergy of these two forms. In this study, we bridge this gap by introducing a novel approach that leverages large language models (LLMs) for generating both event and topic timelines. Our approach diverges from conventional TLS by prioritizing event detection, leveraging LLMs as pseudo-oracles for incremental event clustering and the construction of timelines from a text stream. As a result, it produces a more interpretable pipeline. Empirical evaluation across four TLS benchmarks reveals that our approach outperforms the best prior published approaches, highlighting the potential of LLMs in timeline summarization for real-world applications.</abstract>
      <url hash="02b432f3">2024.acl-long.390</url>
      <bibkey>hu-etal-2024-moments</bibkey>
      <doi>10.18653/v1/2024.acl-long.390</doi>
    </paper>
    <paper id="391">
      <title>End-to-end Learning of Logical Rules for Enhancing Document-level Relation Extraction</title>
      <author><first>Kunxun</first><last>Qi</last></author>
      <author><first>Jianfeng</first><last>Du</last><affiliation>Guangdong University of Foreign Studies</affiliation></author>
      <author><first>Hai</first><last>Wan</last></author>
      <pages>7247-7263</pages>
      <abstract>Document-level relation extraction (DocRE) aims to extract relations between entities in a whole document. One of the pivotal challenges of DocRE is to capture the intricate interdependencies between relations of entity pairs. Previous methods have shown that logical rules can explicitly help capture such interdependencies. These methods either learn logical rules to refine the output of a trained DocRE model, or first learn logical rules from annotated data and then inject the learnt rules into a DocRE model using an auxiliary training objective. However, these learning pipelines may suffer from the issue of error propagation. To mitigate this issue, we propose <i>Joint Modeling Relation extraction and Logical rules</i> or <i>JMRL</i> for short, a novel rule-based framework that jointly learns both a DocRE model and logical rules in an end-to-end fashion. Specifically, we parameterize a rule reasoning module in JMRL to simulate the inference of logical rules, thereby explicitly modeling the reasoning process. We also introduce an auxiliary loss and a residual connection mechanism in JMRL to better reconcile the DocRE model and the rule reasoning module. Experimental results on four benchmark datasets demonstrate that our proposed JMRL framework is consistently superior to existing rule-based frameworks, improving five baseline models for DocRE by a significant margin.</abstract>
      <url hash="1d3e11e4">2024.acl-long.391</url>
      <bibkey>qi-etal-2024-end</bibkey>
      <doi>10.18653/v1/2024.acl-long.391</doi>
    </paper>
    <paper id="392">
      <title>Can We Achieve High-quality Direct Speech-to-Speech Translation without Parallel Speech Data?</title>
      <author><first>Qingkai</first><last>Fang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Zhengrui</first><last>Ma</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>7264-7277</pages>
      <abstract>Recently proposed two-pass direct speech-to-speech translation (S2ST) models decompose the task into speech-to-text translation (S2TT) and text-to-speech (TTS) within an end-to-end model, yielding promising results. However, the training of these models still relies on parallel speech data, which is extremely challenging to collect. In contrast, S2TT and TTS have accumulated a large amount of data and pretrained models, which have not been fully utilized in the development of S2ST models. Inspired by this, in this paper, we first introduce a composite S2ST model named ComSpeech, which can seamlessly integrate any pretrained S2TT and TTS models into a direct S2ST model. Furthermore, to eliminate the reliance on parallel speech data, we propose a novel training method ComSpeech-ZS that solely utilizes S2TT and TTS data. It aligns representations in the latent space through contrastive learning, enabling the speech synthesis capability learned from the TTS data to generalize to S2ST in a zero-shot manner. Experimental results on the CVSS dataset show that when the parallel speech data is available, ComSpeech surpasses previous two-pass models like UnitY and Translatotron 2 in both translation quality and decoding speed. When there is no parallel speech data, ComSpeech-ZS lags behind by only 0.7 ASR-BLEU and outperforms the cascaded models.</abstract>
      <url hash="d86374e1">2024.acl-long.392</url>
      <bibkey>fang-etal-2024-achieve</bibkey>
      <doi>10.18653/v1/2024.acl-long.392</doi>
    </paper>
    <paper id="393">
      <title>Enhancing <fixed-case>EEG</fixed-case>-to-Text Decoding through Transferable Representations from Pre-trained Contrastive <fixed-case>EEG</fixed-case>-Text Masked Autoencoder</title>
      <author><first>Jiaqi</first><last>Wang</last></author>
      <author><first>Zhenxi</first><last>Song</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Zhengyu</first><last>Ma</last><affiliation>Peng Cheng Lab</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zhiguo</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>7278-7292</pages>
      <abstract>Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modules alongside the EEG stream from CET-MAE and further enables an LLM (specifically BART) to decode text from EEG sequences. Comprehensive experiments conducted on the popular text-evoked EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms the baseline framework in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%, respectively. Our proposed pre-trained EEG-Text model shows the potential to improve downstream tasks involving EEG and text. This opens up promising avenues for its application in inner speech BCI paradigms, meriting further investigation.</abstract>
      <url hash="80fe1c74">2024.acl-long.393</url>
      <bibkey>wang-etal-2024-enhancing-eeg</bibkey>
      <doi>10.18653/v1/2024.acl-long.393</doi>
    </paper>
    <paper id="394">
      <title><fixed-case>CQIL</fixed-case>: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers</title>
      <author><first>Longwei</first><last>Zou</last></author>
      <author><first>Qingyang</first><last>Wang</last></author>
      <author><first>Han</first><last>Zhao</last></author>
      <author><first>Jiangangkong</first><last>Jiangangkong</last></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Didi Research</affiliation></author>
      <author><first>Yangdong</first><last>Deng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>7293-7307</pages>
      <abstract>The fast-growing large scale language models are delivering unprecedented performance on almost all natural language processing tasks. However, the effectiveness of large language models are reliant on an exponentially increasing number of parameters. The overwhelming computation complexity incurs a high inference latency that negatively affects user experience. Existing methods to improve inference efficiency, such as tensor parallelism and quantization, target to reduce per-layer computing latency, yet overlook the cumulative latency due to the number of layers. Recent works on reducing the cumulative latency through layer removing, however, lead to significant performance drop. Motivated by the similarity of inputs among adjacent layers, we propose to identify quasi-independent layers, which can be concurrently computed to significantly decrease inference latency. We also introduce a bypassing technique to mitigate the effect of information loss. Empirical experiments of the proposed approach on the LLaMA models confirm that Concurrent Computation of Quasi-Independent Layers (CQIL) can reduce latency by up to 48.3% on LLaMA-33B, while maintaining a close level of performance.</abstract>
      <url hash="76ce97ec">2024.acl-long.394</url>
      <bibkey>zou-etal-2024-cqil</bibkey>
      <doi>10.18653/v1/2024.acl-long.394</doi>
    </paper>
    <paper id="395">
      <title>Prompt Optimization via Adversarial In-Context Learning</title>
      <author><first>Do</first><last>Long</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yiran</first><last>Zhao</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Hannah</first><last>Brown</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yuxi</first><last>Xie</last></author>
      <author><first>James</first><last>Zhao</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <author><first>Kenji</first><last>Kawaguchi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Michael</first><last>Shieh</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Junxian</first><last>He</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>7308-7327</pages>
      <abstract>We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompts for in-context learning (ICL). Inspired by adversarial learning, adv-ICL is implemented as a two-player game between a generator and discriminator, with LLMs acting as both. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator then classifies the generator’s input-output pair as model-generated or real data. Based on the discriminator’s loss, a prompt modifier LLM proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that applying adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on 13 generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, our method is computationally efficient, easily extensible to other LLMs and tasks, and effective in low-resource settings.</abstract>
      <url hash="3efc3de1">2024.acl-long.395</url>
      <bibkey>long-etal-2024-prompt</bibkey>
      <doi>10.18653/v1/2024.acl-long.395</doi>
    </paper>
    <paper id="396">
      <title><fixed-case>S</fixed-case>tream<fixed-case>V</fixed-case>oice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion</title>
      <author><first>Zhichao</first><last>Wang</last></author>
      <author><first>Yuanzhe</first><last>Chen</last></author>
      <author><first>Xinsheng</first><last>Wang</last><affiliation>Northwest Polytechnical University Xi’an</affiliation></author>
      <author><first>Lei</first><last>Xie</last><affiliation>Northwest Polytechnical University</affiliation></author>
      <author><first>Yuping</first><last>Wang</last></author>
      <pages>7328-7338</pages>
      <abstract>Recent language model (LM) advancements have showcased impressive zero-shot voice conversion (VC) performance. However, existing LM-based VC models usually apply offline conversion from source semantics to acoustic features, demanding the complete source speech and limiting their deployment to real-time applications. In this paper, we introduce StreamVoice, a novel streaming LM-based model for zero-shot VC, facilitating real-time conversion given arbitrary speaker prompts and source speech. Specifically, to enable streaming capability, StreamVoice employs a fully causal context-aware LM with a temporal-independent acoustic predictor, while alternately processing semantic and acoustic features at each time step of autoregression which eliminates the dependence on complete source speech. To address the potential performance degradation from the incomplete context in streaming processing, we enhance the context-awareness of the LM through two strategies: 1) teacher-guided context foresight, using a teacher model to summarize the present and future semantic context during training to guide the model’s forecasting for missing context; 2) semantic masking strategy, promoting acoustic prediction from preceding corrupted semantic and acoustic input, enhancing context-learning ability. Notably, StreamVoice is the first LM-based streaming zero-shot VC model without any future look-ahead. Experiments demonstrate StreamVoice’s streaming conversion capability while achieving zero-shot performance comparable to non-streaming VC systems.</abstract>
      <url hash="acc1886d">2024.acl-long.396</url>
      <bibkey>wang-etal-2024-streamvoice</bibkey>
      <doi>10.18653/v1/2024.acl-long.396</doi>
    </paper>
    <paper id="397">
      <title>Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering</title>
      <author><first>Zhengliang</first><last>Shi</last></author>
      <author><first>Shuo</first><last>Zhang</last></author>
      <author><first>Weiwei</first><last>Sun</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Shen</first><last>Gao</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Pengjie</first><last>Ren</last><affiliation>Shandong University</affiliation></author>
      <author><first>Zhumin</first><last>Chen</last><affiliation>Shandong University</affiliation></author>
      <author><first>Zhaochun</first><last>Ren</last><affiliation>Leiden University</affiliation></author>
      <pages>7339-7353</pages>
      <abstract>Multi-Hop Question Answering (MHQA) task presents a significant challenge for large language models (LLMs) due to the intensive knowledge required. Current solutions, like Retrieval-Augmented Generation, typically retrieve potential documents from an external corpus to read an answer. However, the performance of this retrieve-then-read paradigm is constrained by the retriever and the inevitable noise in the retrieved documents. To mitigate these challenges, we introduce a novel generate-then-ground (GenGround) framework, synergizing the parametric knowledge of LLMs and external documents to solve a multi-hop question. GenGround empowers LLMs to alternate two phases until the final answer is derived: (1) formulate a simpler, single-hop question and directly generate the answer; (2) ground the question-answer pair into retrieved documents, amending any wrong predictions in the answer. We also propose an instructional grounding distillation method to generalize our method into smaller models. Extensive experiments conducted on four datasets illustrate the superiority of our method. To further facilitate future research, we have collected a dataset that traces the reasoning process.</abstract>
      <url hash="172aac88">2024.acl-long.397</url>
      <bibkey>shi-etal-2024-generate</bibkey>
      <doi>10.18653/v1/2024.acl-long.397</doi>
    </paper>
    <paper id="398">
      <title>Multimodal Contextualized Semantic Parsing from Speech</title>
      <author><first>Jordan</first><last>Voas</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>David</first><last>Harwath</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Ray</first><last>Mooney</last><affiliation>, University of Texas, Austin</affiliation></author>
      <pages>7354-7369</pages>
      <abstract>We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agents’ contextual awareness by integrating multimodal inputs with prior contexts. SPICE goes beyond traditional semantic parsing by offering a structured, interpretable framework for dynamically updating an agent’s knowledge with new information, mirroring the complexity of human communication. We develop the VG-SPICE dataset, crafted to challenge agents with visual scene graph construction from spoken conversational exchanges, highlighting speech and visual data integration. We also present the Audio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE. These innovations aim to improve multimodal information processing and integration. Both the VG-SPICE dataset and the AViD-SP model are publicly available.</abstract>
      <url hash="e3bee456">2024.acl-long.398</url>
      <bibkey>voas-etal-2024-multimodal</bibkey>
      <doi>10.18653/v1/2024.acl-long.398</doi>
    </paper>
    <paper id="399">
      <title><fixed-case>L</fixed-case>a<fixed-case>MP</fixed-case>: When Large Language Models Meet Personalization</title>
      <author><first>Alireza</first><last>Salemi</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Sheshera</first><last>Mysore</last></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <author><first>Hamed</first><last>Zamani</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <pages>7370-7392</pages>
      <abstract>This paper highlights the importance of personalization in large language models and introduces the LaMP benchmark — a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on LaMP for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.</abstract>
      <url hash="0238f62f">2024.acl-long.399</url>
      <bibkey>salemi-etal-2024-lamp</bibkey>
      <doi>10.18653/v1/2024.acl-long.399</doi>
    </paper>
    <paper id="400">
      <title><fixed-case>A</fixed-case>bout<fixed-case>M</fixed-case>e: Using Self-Descriptions in Webpages to Document the Effects of <fixed-case>E</fixed-case>nglish Pretraining Data Filters</title>
      <author><first>Li</first><last>Lucy</last><affiliation>Allen Institute for Artificial Intelligence and University of California Berkeley</affiliation></author>
      <author><first>Suchin</first><last>Gururangan</last><affiliation>Facebook and University of Washington, Seattle</affiliation></author>
      <author><first>Luca</first><last>Soldaini</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Emma</first><last>Strubell</last><affiliation>Allen Institute for Artificial Intelligence and Carnegie Mellon University</affiliation></author>
      <author><first>David</first><last>Bamman</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Lauren</first><last>Klein</last><affiliation>Emory University</affiliation></author>
      <author><first>Jesse</first><last>Dodge</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>7393-7420</pages>
      <abstract>Large language models’ (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage are under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten “quality” and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications.</abstract>
      <url hash="97cc1cea">2024.acl-long.400</url>
      <bibkey>lucy-etal-2024-aboutme</bibkey>
      <doi>10.18653/v1/2024.acl-long.400</doi>
    </paper>
    <paper id="401">
      <title><fixed-case>MT</fixed-case>-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues</title>
      <author><first>Ge</first><last>Bai</last></author>
      <author><first>Jie</first><last>Liu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xingyuan</first><last>Bu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yancheng</first><last>He</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jiaheng</first><last>Liu</last></author>
      <author><first>Zhanhui</first><last>Zhou</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zhuoran</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wenbo</first><last>Su</last></author>
      <author><first>Tiezheng</first><last>Ge</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wanli</first><last>Ouyang</last><affiliation>Shanghai AI Lab</affiliation></author>
      <pages>7421-7454</pages>
      <abstract>The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities. The data and code are available at https://github.com/mtbench101/mt-bench-101.</abstract>
      <url hash="4ccd7c8e">2024.acl-long.401</url>
      <bibkey>bai-etal-2024-mt</bibkey>
      <doi>10.18653/v1/2024.acl-long.401</doi>
    </paper>
    <paper id="402">
      <title><fixed-case>EFSA</fixed-case>: Towards Event-Level Financial Sentiment Analysis</title>
      <author><first>Tianyu</first><last>Chen</last></author>
      <author><first>Yiming</first><last>Zhang</last></author>
      <author><first>Guoxin</first><last>Yu</last></author>
      <author><first>Dapeng</first><last>Zhang</last></author>
      <author><first>Li</first><last>Zeng</last></author>
      <author><first>Qing</first><last>He</last></author>
      <author><first>Xiang</first><last>Ao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences and University of the Chinese Academy of Sciences</affiliation></author>
      <pages>7455-7467</pages>
      <abstract>In this paper, we extend financial sentiment analysis (FSA) to event-level since events usually serve as the subject of the sentiment in financial text. Though extracting events from the financial text may be conducive to accurate sentiment predictions, it has specialized challenges due to the lengthy and discontinuity of events in a financial text. To this end, we reconceptualize the event extraction as a classification task by designing a categorization comprising coarse-grained and fine-grained event categories. Under this setting, we formulate the Event-Level Financial Sentiment Analysis(EFSA for short) task that outputs quintuples consisting of (company, industry, coarse-grained event, fine-grained event, sentiment) from financial text. A large-scale Chinese dataset containing 12,160 news articles and 13,725 quintuples is publicized as a brand new testbed for our task. A four-hop Chain-of-Thought LLM-based approach is devised for this task. Systematically investigations are conducted on our dataset, and the empirical results demonstrate the benchmarking scores of existing methods and our proposed method can reach the current state-of-the-art. Our dataset and framework implementation are available at https://github.com/cty1934/EFSA</abstract>
      <url hash="26363b7d">2024.acl-long.402</url>
      <bibkey>chen-etal-2024-efsa</bibkey>
      <doi>10.18653/v1/2024.acl-long.402</doi>
    </paper>
    <paper id="403">
      <title>What Evidence Do Language Models Find Convincing?</title>
      <author><first>Alexander</first><last>Wan</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Eric</first><last>Wallace</last><affiliation>OpenAI and University of California Berkeley</affiliation></author>
      <author><first>Dan</first><last>Klein</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>7468-7484</pages>
      <abstract>Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as “is aspartame linked to cancer”. To resolve these ambiguous queries, one must search through a large range of websites and consider “which, if any, of this evidence do I find convincing?”. In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone. Taken together, these results highlight the importance of RAG corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how LLMs are trained to better align with human judgements.</abstract>
      <url hash="4c219e3e">2024.acl-long.403</url>
      <bibkey>wan-etal-2024-evidence</bibkey>
      <doi>10.18653/v1/2024.acl-long.403</doi>
    </paper>
    <paper id="404">
      <title>Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models</title>
      <author><first>Qihang</first><last>Ai</last></author>
      <author><first>Jiafan</first><last>Li</last></author>
      <author><first>Jincheng</first><last>Dai</last><affiliation>College of Computer Science and Technology, Zhejiang University</affiliation></author>
      <author><first>Jianwu</first><last>Zhou</last></author>
      <author><first>Lemao</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Haiyun</first><last>Jiang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>7485-7501</pages>
      <abstract>Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data understanding and reasoning.Instead of adopting complex graph neural models or heuristic graph-to-text instruction design, we leverage Vision-Language Models (VLMs) to encode the graph images with varying structures across different domains. This paper first evaluates the capabilities of public VLMs in graph learning from multiple aspects. Then it introduces a novel instruction-following dataset for multimodal graph understanding and reasoning in English and Chinese. Besides, by fine-tuning MiniGPT-4 and LLaVA on our dataset, we achieved an accuracy increase of 5%-15% compared to baseline models, with the best-performing model attaining scores comparable to Gemini in GPT-asissted Evaluation. This research not only showcases the potential of integrating VLMs with graph data but also opens new avenues for advancements in graph data understanding.</abstract>
      <url hash="eca15a02">2024.acl-long.404</url>
      <bibkey>ai-etal-2024-advancement</bibkey>
      <doi>10.18653/v1/2024.acl-long.404</doi>
    </paper>
    <paper id="405">
      <title><fixed-case>L</fixed-case>ang<fixed-case>B</fixed-case>ridge: Multilingual Reasoning Without Multilingual Supervision</title>
      <author><first>Dongkeun</first><last>Yoon</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Joel</first><last>Jang</last></author>
      <author><first>Sungdong</first><last>Kim</last><affiliation>KAIST AI and NAVER</affiliation></author>
      <author><first>Seungone</first><last>Kim</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Sheikh</first><last>Shafayat</last><affiliation>KAIST</affiliation></author>
      <author><first>Minjoon</first><last>Seo</last><affiliation>Twelve Labs and Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>7502-7522</pages>
      <abstract>We introduce LangBridge, a <tex-math>\textit{zero-shot}</tex-math> approach to adapt language models for multilingual reasoning tasks without multilingual supervision. LangBridge operates by bridging two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., MetaMath). LangBridge connects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, LangBridge considerably enhances the performance of language models on low-resource languages across mathematical reasoning, code completion, logical reasoning, and commonsense reasoning. Our analysis suggests that the efficacy of LangBridge stems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models.</abstract>
      <url hash="ae493fa2">2024.acl-long.405</url>
      <bibkey>yoon-etal-2024-langbridge</bibkey>
      <doi>10.18653/v1/2024.acl-long.405</doi>
    </paper>
    <paper id="406">
      <title>Can <fixed-case>LLM</fixed-case>s Reason with Rules? Logic Scaffolding for Stress-Testing and Improving <fixed-case>LLM</fixed-case>s</title>
      <author><first>Siyuan</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>7523-7543</pages>
      <abstract>Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs’ logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reasoning tasks. Overall, our work sheds light on LLMs’ limitations in grasping inferential rule and suggests ways to enhance their logical reasoning abilities .</abstract>
      <url hash="073723a3">2024.acl-long.406</url>
      <bibkey>wang-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.acl-long.406</doi>
    </paper>
    <paper id="407">
      <title><fixed-case>SEGO</fixed-case>: Sequential Subgoal Optimization for Mathematical Problem-Solving</title>
      <author><first>Xueliang</first><last>Zhao</last></author>
      <author><first>Xinting</first><last>Huang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wei</first><last>Bi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <pages>7544-7565</pages>
      <abstract>Large Language Models (LLMs) have driven substantial progress in artificial intelligence in recent years, exhibiting impressive capabilities across a wide range of tasks, including mathematical problem-solving. Inspired by the success of subgoal-based methods, we propose a novel framework called <b>SE</b>quential sub<b>G</b>oal <b>O</b>ptimization (SEGO) to enhance LLMs’ ability to solve mathematical problems. By establishing a connection between the subgoal breakdown process and the probability of solving problems, SEGO aims to identify better subgoals with theoretical guarantees. Addressing the challenge of identifying suitable subgoals in a large solution space, our framework generates problem-specific subgoals and adjusts them according to carefully designed criteria. Incorporating these optimized subgoals into the policy model training leads to significant improvements in problem-solving performance. We validate SEGO’s efficacy through experiments on two benchmarks, GSM8K and MATH, where our approach outperforms existing methods, highlighting the potential of SEGO in AI-driven mathematical problem-solving.</abstract>
      <url hash="eb75e116">2024.acl-long.407</url>
      <bibkey>zhao-etal-2024-sego</bibkey>
      <doi>10.18653/v1/2024.acl-long.407</doi>
    </paper>
    <paper id="408">
      <title>Unlocking the Power of Large Language Models for Entity Alignment</title>
      <author><first>Xuhui</first><last>Jiang</last></author>
      <author><first>Yinghan</first><last>Shen</last></author>
      <author><first>Zhichao</first><last>Shi</last><affiliation>institute of computing technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengjin</first><last>Xu</last><affiliation>International Digital Economy Academy</affiliation></author>
      <author><first>Wei</first><last>Li</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zixuan</first><last>Li</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jian</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuanzhuo</first><last>Wang</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <pages>7566-7583</pages>
      <abstract>Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs’ capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy while preserving efficiency. Our experimental results affirm ChatEA’s superior performance, highlighting LLMs’ potential in facilitating EA tasks.The source code is available at https://anonymous.4open.science/r/ChatEA/.</abstract>
      <url hash="04dcc7e7">2024.acl-long.408</url>
      <bibkey>jiang-etal-2024-unlocking</bibkey>
      <doi>10.18653/v1/2024.acl-long.408</doi>
    </paper>
    <paper id="409">
      <title>Trial and Error: Exploration-Based Trajectory Optimization of <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Yifan</first><last>Song</last></author>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jie</first><last>Huang</last></author>
      <author><first>Sujian</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <pages>7584-7600</pages>
      <abstract>Large Language Models (LLMs) have become integral components in various autonomous agent systems.In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.</abstract>
      <url hash="0da5e409">2024.acl-long.409</url>
      <bibkey>song-etal-2024-trial</bibkey>
      <doi>10.18653/v1/2024.acl-long.409</doi>
    </paper>
    <paper id="410">
      <title><fixed-case>R</fixed-case>e<fixed-case>FT</fixed-case>: Reasoning with Reinforced Fine-Tuning</title>
      <author><first>Luong</first><last>Trung</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Xinbo</first><last>Zhang</last></author>
      <author><first>Zhanming</first><last>Jie</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Peng</first><last>Sun</last><affiliation>ByteDance</affiliation></author>
      <author><first>Xiaoran</first><last>Jin</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Hang</first><last>Li</last></author>
      <pages>7601-7614</pages>
      <abstract>One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.</abstract>
      <url hash="e556ce21">2024.acl-long.410</url>
      <bibkey>trung-etal-2024-reft</bibkey>
      <doi>10.18653/v1/2024.acl-long.410</doi>
    </paper>
    <paper id="411">
      <title>Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment</title>
      <author><first>Yunxin</first><last>Li</last></author>
      <author><first>Xinyu</first><last>Chen</last></author>
      <author><first>Baotian</first><last>Hu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Haoyuan</first><last>Shi</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>7615-7626</pages>
      <abstract>Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.</abstract>
      <url hash="65530356">2024.acl-long.411</url>
      <bibkey>li-etal-2024-cognitive</bibkey>
      <doi>10.18653/v1/2024.acl-long.411</doi>
    </paper>
    <paper id="412">
      <title><fixed-case>F</fixed-case>ree<fixed-case>C</fixed-case>trl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation</title>
      <author><first>Zijian</first><last>Feng</last></author>
      <author><first>Hanzhang</first><last>Zhou</last></author>
      <author><first>Kezhi</first><last>Mao</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zixiao</first><last>Zhu</last></author>
      <pages>7627-7640</pages>
      <abstract>Controllable text generation (CTG) seeks to craft texts adhering to specific attributes, traditionally employing learning-based techniques such as training, fine-tuning, or prefix-tuning with attribute-specific datasets. These approaches, while effective, demand extensive computational and data resources. In contrast, some proposed learning-free alternatives circumvent learning but often yield inferior results, exemplifying the fundamental machine learning trade-off between computational expense and model efficacy. To overcome these limitations, we propose FreeCtrl, a learning-free approach that dynamically adjusts the weights of selected feedforward neural network (FFN) vectors to steer the outputs of large language models (LLMs). FreeCtrl hinges on the principle that the weights of different FFN vectors influence the likelihood of different tokens appearing in the output. By identifying and adaptively adjusting the weights of attribute-related FFN vectors, FreeCtrl can control the output likelihood of attribute keywords in the generated content. Extensive experiments on single- and multi-attribute control reveal that the learning-free FreeCtrl outperforms other learning-free and learning-based methods, successfully resolving the dilemma between learning costs and model performance.</abstract>
      <url hash="455cdfcd">2024.acl-long.412</url>
      <bibkey>feng-etal-2024-freectrl</bibkey>
      <doi>10.18653/v1/2024.acl-long.412</doi>
    </paper>
    <paper id="413">
      <title><fixed-case>HD</fixed-case>-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition</title>
      <author><first>Yuxuan</first><last>Liu</last></author>
      <author><first>Tianchi</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Haizhen</first><last>Huang</last></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Weiwei</first><last>Deng</last></author>
      <author><first>Feng</first><last>Sun</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>7641-7660</pages>
      <abstract>Large language models (LLMs) have emerged as a promising alternative to expensive human evaluations. However, the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria. To address this challenge, we propose HD-Eval, a novel framework that iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the evaluation mindset of human experts and enhances the alignment of LLM-based evaluators by decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria. By integrating these steps within an iterative alignment training process, we obtain a hierarchical decomposition of criteria that comprehensively captures aspects of natural language at multiple levels of granularity. Implemented as a white box, the human preference-guided aggregator is efficient to train and more explainable than relying solely on prompting, and its independence from model parameters makes it applicable to closed-source LLMs. Extensive experiments on three evaluation domains demonstrate the superiority of HD-Eval in further aligning state-of-the-art evaluators and providing deeper insights into the explanation of evaluation results and the task itself.</abstract>
      <url hash="47df31be">2024.acl-long.413</url>
      <bibkey>liu-etal-2024-hd</bibkey>
      <doi>10.18653/v1/2024.acl-long.413</doi>
    </paper>
    <paper id="414">
      <title>Conundrums in Cross-Prompt Automated Essay Scoring: Making Sense of the State of the Art</title>
      <author><first>Shengjie</first><last>Li</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Vincent</first><last>Ng</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>7661-7681</pages>
      <abstract>Cross-prompt automated essay scoring (AES), an under-investigated but challenging task that has gained increasing popularity in the AES community, aims to train an AES system that can generalize well to prompts that are unseen during model training. While recently-developed cross-prompt AES models have combined essay representations that are learned via sophisticated neural architectures with so-called prompt-independent features, an intriguing question is: are complex neural models needed to achieve state-of-the-art results? We answer this question by abandoning sophisticated neural architectures and developing a purely feature-based approach to cross-prompt AES that adopts a simple neural architecture. Experiments on the ASAP dataset demonstrate that our simple approach to cross-prompt AES can achieve state-of-the-art results.</abstract>
      <url hash="54316dd4">2024.acl-long.414</url>
      <bibkey>li-ng-2024-conundrums</bibkey>
      <doi>10.18653/v1/2024.acl-long.414</doi>
    </paper>
    <paper id="415">
      <title>Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution</title>
      <author><first>Flor Miriam</first><last>Plaza Del Arco</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Amanda</first><last>Curry</last></author>
      <author><first>Alba</first><last>Cercas Curry</last><affiliation>University of Leeds</affiliation></author>
      <author><first>Gavin</first><last>Abercrombie</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>7682-7696</pages>
      <abstract>Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men’s anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like ‘When I had a serious argument with a dear person’. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.</abstract>
      <url hash="4e3aacbb">2024.acl-long.415</url>
      <bibkey>plaza-del-arco-etal-2024-angry</bibkey>
      <doi>10.18653/v1/2024.acl-long.415</doi>
    </paper>
    <paper id="416">
      <title>Label Augmentation for Zero-Shot Hierarchical Text Classification</title>
      <author><first>Lorenzo</first><last>Paletto</last></author>
      <author><first>Valerio</first><last>Basile</last><affiliation>University of Turin</affiliation></author>
      <author><first>Roberto</first><last>Esposito</last><affiliation>University of Turin</affiliation></author>
      <pages>7697-7706</pages>
      <abstract>Hierarchical Text Classification poses the difficult challenge of classifying documents into multiple labels organized in a hierarchy. The vast majority of works aimed to address this problem relies on supervised methods which are difficult to implement due to the scarcity of labeled data in many real world applications. This paper focuses on strict Zero-Shot Classification, the setting in which the system lacks both labeled instances and training data.We propose a novel approach that uses a Large Language Model to augment the deepest layer of the labels hierarchy in order to enhance its specificity. We achieve this by generating semantically relevant labels as children connected to the existing branches, creating a deeper taxonomy that better overlaps with the input texts. We leverage the enriched hierarchy to perform Zero-Shot Hierarchical Classification by using the Upward score Propagation technique. We test our method on four public datasets, obtaining new state-of-the art results on three of them. We introduce two cosine similarity-based metrics to quantify the density and granularity of a label taxonomy and we show a strong correlation between the metric values and the classification performance of our method on the datasets.</abstract>
      <url hash="52ea5a09">2024.acl-long.416</url>
      <bibkey>paletto-etal-2024-label</bibkey>
      <doi>10.18653/v1/2024.acl-long.416</doi>
    </paper>
    <paper id="417">
      <title><fixed-case>STICKERCONV</fixed-case>: Generating Multimodal Empathetic Responses from Scratch</title>
      <author><first>Yiqun</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Fanheng</first><last>Kong</last></author>
      <author><first>Peidong</first><last>Wang</last></author>
      <author><first>Shuang</first><last>Sun</last></author>
      <author><first>SWangLing</first><last>SWangLing</last></author>
      <author><first>Shi</first><last>Feng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Daling</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yifei</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Kaisong</first><last>Song</last><affiliation>Alibaba Group</affiliation></author>
      <pages>7707-7733</pages>
      <abstract>Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research, notably due to the challenge of a lack of comprehensive datasets. In this paper, we introduce the Agent for STICKERCONV (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, STICKERCONV, comprising 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios. This dataset serves as a benchmark for multimodal empathetic generation. To advance further, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation framework, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS’s effectiveness in generating contextually relevant and emotionally resonant multimodal empathetic responses, contributing to the advancement of more nuanced and engaging empathetic dialogue systems.</abstract>
      <url hash="b5a817a4">2024.acl-long.417</url>
      <bibkey>zhang-etal-2024-stickerconv</bibkey>
      <doi>10.18653/v1/2024.acl-long.417</doi>
    </paper>
    <paper id="418">
      <title><fixed-case>EIT</fixed-case>: Enhanced Interactive Transformer</title>
      <author><first>Tong</first><last>Zheng</last></author>
      <author><first>Bei</first><last>Li</last><affiliation>Meituan</affiliation></author>
      <author><first>Huiwen</first><last>Bao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>JingBo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <pages>7734-7751</pages>
      <abstract>Two principles: the complementary principle and the consensus principle are widely acknowledged in the literature of multi-view learning. However, the current design of multi-head self-attention, an instance of multi-view learning, prioritizes the complementarity while ignoring the consensus. To address this problem, we propose an enhanced multi-head self-attention (EMHA). First, to satisfy the complementary principle, EMHA removes the one-to-one mapping constraint among queries and keys in multiple subspaces and allows each query to attend to multiple keys. On top of that, we develop a method to fully encourage consensus among heads by introducing two interaction models, namely inner-subspace interaction and cross-subspace interaction. Extensive experiments on a wide range of language tasks (e.g., machine translation, abstractive summarization and grammar correction, language modeling), show its superiority, with a very modest increase in model size. Our code would be available at: https://github.com/zhengkid/EIT-Enhanced-Interactive-Transformer.</abstract>
      <url hash="39a1c27c">2024.acl-long.418</url>
      <bibkey>zheng-etal-2024-eit</bibkey>
      <doi>10.18653/v1/2024.acl-long.418</doi>
    </paper>
    <paper id="419">
      <title><fixed-case>MARS</fixed-case>: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yavuz Faruk</first><last>Bakman</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Duygu Nur</first><last>Yaldiz</last></author>
      <author><first>Baturalp</first><last>Buyukates</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Chenyang</first><last>Tao</last><affiliation>Amazon</affiliation></author>
      <author><first>Dimitrios</first><last>Dimitriadis</last><affiliation>Amazon</affiliation></author>
      <author><first>Salman</first><last>Avestimehr</last><affiliation>University of Southern California</affiliation></author>
      <pages>7752-7767</pages>
      <abstract>Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical QA dataset. Code can be found here.</abstract>
      <url hash="a716753c">2024.acl-long.419</url>
      <bibkey>bakman-etal-2024-mars</bibkey>
      <doi>10.18653/v1/2024.acl-long.419</doi>
    </paper>
    <paper id="420">
      <title><fixed-case>EXAMS</fixed-case>-<fixed-case>V</fixed-case>: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models</title>
      <author><first>Rocktim</first><last>Das</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Simeon</first><last>Hristov</last></author>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Dimitar</first><last>Dimitrov</last></author>
      <author><first>Ivan</first><last>Koychev</last><affiliation>Sofia University “St. Kliment Ohridski”</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>7768-7791</pages>
      <abstract>We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11 languages from 7 language families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content in the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision–text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark.</abstract>
      <url hash="82bebe81">2024.acl-long.420</url>
      <bibkey>das-etal-2024-exams</bibkey>
      <doi>10.18653/v1/2024.acl-long.420</doi>
    </paper>
    <paper id="421">
      <title>Order-Agnostic Data Augmentation for Few-Shot Named Entity Recognition</title>
      <author><first>Huiming</first><last>Wang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Liying</first><last>Cheng</last></author>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>De Wen</first><last>Soh</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Alibaba Group</affiliation></author>
      <pages>7792-7807</pages>
      <abstract>Data augmentation (DA) methods have been proven to be effective for pre-trained language models (PLMs) in low-resource settings, including few-shot named entity recognition (NER). However, existing NER DA techniques either perform rule-based manipulations on words that break the semantic coherence of the sentence, or exploit generative models for entity or context substitution, which requires a substantial amount of labeled data and contradicts the objective of operating in low-resource settings. In this work, we propose order-agnostic data augmentation (OaDA), an alternative solution that exploits the often overlooked order-agnostic property in the training data construction phase of sequence-to-sequence NER methods for data augmentation. To effectively utilize the augmented data without suffering from the one-to-many issue, where multiple augmented target sequences exist for one single sentence, we further propose the use of ordering instructions and an innovative OaDA-XE loss. Specifically, by treating each permutation of entity types as an ordering instruction, we rearrange the entity set accordingly, ensuring a distinct input-output pair, while OaDA-XE assigns loss based on the best match between the target sequence and model predictions. We conduct comprehensive experiments and analyses across three major NER benchmarks and significantly enhance the few-shot capabilities of PLMs with OaDA.</abstract>
      <url hash="7d736921">2024.acl-long.421</url>
      <bibkey>wang-etal-2024-order</bibkey>
      <doi>10.18653/v1/2024.acl-long.421</doi>
    </paper>
    <paper id="422">
      <title>Text Embedding Inversion Security for Multilingual Language Models</title>
      <author><first>Yiyi</first><last>Chen</last></author>
      <author><first>Heather</first><last>Lent</last><affiliation>Aalborg University</affiliation></author>
      <author><first>Johannes</first><last>Bjerva</last><affiliation>Aalborg University</affiliation></author>
      <pages>7808-7827</pages>
      <abstract>Textual data is often represented as real-numbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be susceptible to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages potentially exposed to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and crosslingual inversion attacks, and explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English-based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for both monolingual and multilingual models. This study is the first to investigate multilingual inversion attacks, shedding light on the differences in attacks and defenses across monolingual and multilingual settings.</abstract>
      <url hash="aaa2be34">2024.acl-long.422</url>
      <bibkey>chen-etal-2024-text</bibkey>
      <doi>10.18653/v1/2024.acl-long.422</doi>
    </paper>
    <paper id="423">
      <title>Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment</title>
      <author><first>Keming</first><last>Lu</last></author>
      <author><first>Bowen</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chang</first><last>Zhou</last></author>
      <author><first>Jingren</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <pages>7828-7840</pages>
      <abstract>Considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, we introduce Ditto, the first self-alignment method for role-play, which encourages an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension, and creates a role-play training set comprising 4000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Subsequently, we fine-tune the LLM using this self-generated dataset to augment its role-playing capabilities. Upon evaluating our meticulously constructed role-play benchmark and the roleplay subset of MT-Bench, Ditto, in various parameter scales, consistently maintains a consistent role identity and provides accurate role-specific knowledge in multi-turn role-play conversations, outperforming all open-source role-play baselines. Furthermore, we present the first cross-supervision role-play experiment, revealing that the role-play styles can be easily acquired, while the intrinsic capabilities of LLMs confine the knowledge within role-play.</abstract>
      <url hash="94166c6e">2024.acl-long.423</url>
      <bibkey>lu-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.acl-long.423</doi>
    </paper>
    <paper id="424">
      <title><fixed-case>P</fixed-case>lato<fixed-case>LM</fixed-case>: Teaching <fixed-case>LLM</fixed-case>s in Multi-Round Dialogue via a User Simulator</title>
      <author><first>Chuyi</first><last>Kong</last></author>
      <author><first>Yaxin</first><last>Fan</last></author>
      <author><first>Xiang</first><last>Wan</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Feng</first><last>Jiang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>7841-7863</pages>
      <abstract>The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT dialogues, as evidenced by Vicuna. However, due to challenges in gathering dialogues involving human participation, current endeavors like Baize and UltraChat rely on ChatGPT conducting roleplay to simulate humans based on instructions, resulting in overdependence on seeds, diminished human-likeness, limited topic diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we propose a paradigm to simulate human behavior better and explore the benefits of incorporating more human-like questions in multi-turn conversations. Specifically, we directly target human questions extracted from genuine human-machine conversations as a learning goal and provide a novel user simulator called ‘Socratic‘. The experimental results show our response model, ‘PlatoLM‘, achieves SoTA performance among LLaMA-based 7B models in MT-Bench. Our findings further demonstrate that our method introduces highly human-like questioning patterns and rich topic structures, which can teach the response model better than previous works in multi-round conversations.</abstract>
      <url hash="479e9b3a">2024.acl-long.424</url>
      <bibkey>kong-etal-2024-platolm</bibkey>
      <doi>10.18653/v1/2024.acl-long.424</doi>
    </paper>
    <paper id="425">
      <title>Synthesizing Text-to-<fixed-case>SQL</fixed-case> Data from Weak and Strong <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jiaxi</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Binyuan</first><last>Hui</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junyang</first><last>Lin</last></author>
      <author><first>Chang</first><last>Zhou</last></author>
      <pages>7864-7875</pages>
      <abstract>The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models). The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning. Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models.</abstract>
      <url hash="86548d57">2024.acl-long.425</url>
      <bibkey>yang-etal-2024-synthesizing</bibkey>
      <doi>10.18653/v1/2024.acl-long.425</doi>
    </paper>
    <paper id="426">
      <title><fixed-case>STRUCTSUM</fixed-case> Generation for Faster Text Comprehension</title>
      <author><first>Parag</first><last>Jain</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Andreea</first><last>Marzoca</last><affiliation>Google</affiliation></author>
      <author><first>Francesco</first><last>Piccinno</last><affiliation>Google</affiliation></author>
      <pages>7876-7896</pages>
      <abstract>We consider the task of generating structured representations of text using large language models (LLMs). We focus on tables and mind maps as representative modalities. Tables are more organized way of representing data, while mind maps provide a visually dynamic and flexible approach, particularly suitable for sparse content. Despite the effectiveness of LLMs on different tasks, we show that current models struggle with generating structured outputs. In response, we present effective prompting strategies for both of these tasks. We introduce a taxonomy of problems around factuality, global and local structure, common to both modalities and propose a set of critiques to tackle these issues resulting in an absolute improvement in accuracy of <tex-math>+37</tex-math>pp (79%) for mind maps and <tex-math>+15</tex-math>pp (78%) for tables. To evaluate semantic coverage of generated structured representations we propose Auto-QA, and we verify the adequacy of Auto-QA using SQuAD dataset. We further evaluate the usefulness of structured representations via a text comprehension user study. The results show a significant reduction in comprehension time compared to text when using table (42.9%) and mind map (31.9%), without loss in accuracy.</abstract>
      <url hash="d2f4f958">2024.acl-long.426</url>
      <bibkey>jain-etal-2024-structsum</bibkey>
      <doi>10.18653/v1/2024.acl-long.426</doi>
    </paper>
    <paper id="427">
      <title>Analysing The Impact of Sequence Composition on Language Model Pre-Training</title>
      <author><first>Yu</first><last>Zhao</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Yuanbin</first><last>Qu</last></author>
      <author><first>Konrad</first><last>Staniszewski</last><affiliation>University of Warsaw and IDEAS NCBR Sp. z o.o.</affiliation></author>
      <author><first>Szymon</first><last>Tworkowski</last></author>
      <author><first>Wei</first><last>Liu</last><affiliation>xiaomi</affiliation></author>
      <author><first>Piotr</first><last>Miłoś</last><affiliation>IDEAS NCBR and Polish Academy of Science</affiliation></author>
      <author><first>Yuxiang</first><last>Wu</last></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>7897-7912</pages>
      <abstract>Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use <i>causal masking</i> to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored.In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In <i>intra-document causal masking</i>, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, Bm25Chunk, can improve in-context learning (+11.6%), knowledge memorisation (+9.8%), and context utilisation (+7.2%) abilities of language models without sacrificing efficiency.</abstract>
      <url hash="a297242e">2024.acl-long.427</url>
      <bibkey>zhao-etal-2024-analysing</bibkey>
      <doi>10.18653/v1/2024.acl-long.427</doi>
    </paper>
    <paper id="428">
      <title><fixed-case>NACL</fixed-case>: A General and Effective <fixed-case>KV</fixed-case> Cache Eviction Framework for <fixed-case>LLM</fixed-case> at Inference Time</title>
      <author><first>Yilong</first><last>Chen</last></author>
      <author><first>Guoxia</first><last>Wang</last></author>
      <author><first>Junyuan</first><last>Shang</last><affiliation>Baidu</affiliation></author>
      <author><first>Shiyao</first><last>Cui</last></author>
      <author><first>Zhenyu</first><last>Zhang</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Tingwen</first><last>Liu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shuohuan</first><last>Wang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Dianhai</first><last>Yu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <pages>7913-7926</pages>
      <abstract>Large Language Models (LLMs) have ignited an innovative surge of AI applications, marking a new era of exciting possibilities equipped with extended context windows. However, hosting these models is cost-prohibitive mainly due to the extensive memory consumption of KV Cache involving long-context modeling. Despite several works proposing to evict unnecessary tokens from the KV Cache, most of them rely on the biased local statistics of accumulated attention scores and report performance using unconvincing metric like perplexity on inadequate short-text evaluation. In this paper, we propose NACL, a general framework for long-context KV cache eviction that achieves more optimal and efficient eviction in a single operation during the encoding phase. Due to NACL’s efficiency, we combine more accurate attention score statistics in Proxy-Tokens Eviction with the diversified random eviction strategy of Random Eviction, aiming to alleviate the issue of attention bias and enhance the robustness in maintaining pivotal tokens for long-context modeling tasks. Notably, our method significantly improves the performance on short- and long-text tasks by 80% and 76% respectively, reducing KV Cache by up to <tex-math>5\times</tex-math> with over 95% performance maintenance. Code available at https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.</abstract>
      <url hash="81584608">2024.acl-long.428</url>
      <bibkey>chen-etal-2024-nacl</bibkey>
      <doi>10.18653/v1/2024.acl-long.428</doi>
    </paper>
    <paper id="429">
      <title><fixed-case>S</fixed-case>pike<fixed-case>V</fixed-case>oice: High-Quality Text-to-Speech Via Efficient Spiking Neural Network</title>
      <author><first>Kexin</first><last>Wang</last></author>
      <author><first>Jiahong</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yong</first><last>Ren</last></author>
      <author><first>Man</first><last>Yao</last><affiliation>Institute of automation, Chinese academy of sciences</affiliation></author>
      <author><first>Di</first><last>Shang</last></author>
      <author><first>Bo</first><last>Xu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Guoqi</first><last>Li</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>7927-7940</pages>
      <abstract>Brain-inspired Spiking Neural Network (SNN) has demonstrated its effectiveness and efficiency in vision, natural language, and speech understanding tasks, indicating their capacity to “see”, “listen”, and “read”. In this paper, we design SpikeVoice, which performs high-quality Text-To-Speech (TTS) via SNN, to explore the potential of SNN to “speak”. A major obstacle to using SNN for such generative tasks lies in the demand for models to grasp long-term dependencies. The serial nature of spiking neurons, however, leads to the invisibility of information at future spiking time steps, limiting SNN models to capture sequence dependencies solely within the same time step. We term this phenomenon “partial-time dependency”. To address this issue, we introduce Spiking Temporal-Sequential Attention (STSA) in the SpikeVoice. To the best of our knowledge, SpikeVoice is the first TTS work in the SNN field. We perform experiments using four well-established datasets that cover both Chinese and English languages, encompassing scenarios with both single-speaker and multi-speaker configurations. The results demonstrate that SpikeVoice can achieve results comparable to Artificial Neural Networks (ANN) with only 10.5% energy consumption of ANN. Both our demo and code are available as supplementary material.</abstract>
      <url hash="5bc86990">2024.acl-long.429</url>
      <bibkey>wang-etal-2024-spikevoice</bibkey>
      <doi>10.18653/v1/2024.acl-long.429</doi>
    </paper>
    <paper id="430">
      <title>Context-aware Difference Distilling for Multi-change Captioning</title>
      <author><first>Yunbin</first><last>Tu</last></author>
      <author><first>Liang</first><last>Li</last></author>
      <author><first>Li</first><last>Su</last></author>
      <author><first>Zheng-Jun</first><last>Zha</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Chenggang</first><last>Yan</last><affiliation>Hangzhou Dianzi University, Tsinghua University</affiliation></author>
      <author><first>Qingming</first><last>Huang</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <pages>7941-7956</pages>
      <abstract>Multi-change captioning aims to describe complex and coupled changes within an image pair in natural language. Compared with single-change captioning, this task requires the model to have higher-level cognition ability to reason an arbitrary number of changes. In this paper, we propose a novel context-aware difference distilling (CARD) network to capture all genuine changes for yielding sentences. Given an image pair, CARD first decouples context features that aggregate all similar/dissimilar semantics, termed common/difference context features. Then, the consistency and independence constraints are designed to guarantee the alignment/discrepancy of common/difference context features. Further, the common context features guide the model to mine locally unchanged features, which are subtracted from the pair to distill locally difference features. Next, the difference context features augment the locally difference features to ensure that all changes are distilled. In this way, we obtain an omni-representation of all changes, which is translated into linguistic sentences by a transformer decoder. Extensive experiments on three public datasets show CARD performs favourably against state-of-the-art methods. The code is available at https://github.com/tuyunbin/CARD.</abstract>
      <url hash="1c5b0298">2024.acl-long.430</url>
      <bibkey>tu-etal-2024-context</bibkey>
      <doi>10.18653/v1/2024.acl-long.430</doi>
    </paper>
    <paper id="431">
      <title>Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion</title>
      <author><first>Wei</first><last>Cheng</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yuhan</first><last>Wu</last><affiliation>nanjing university</affiliation></author>
      <author><first>Wei</first><last>Hu</last><affiliation>Nanjing University</affiliation></author>
      <pages>7957-7977</pages>
      <abstract>Recent years have witnessed the deployment of code language models (LMs) in various code intelligence tasks such as code completion. Yet, it is challenging for pre-trained LMs to generate correct completions in private repositories. Previous studies retrieve cross-file context based on import relations or text similarity, which is insufficiently relevant to completion targets. In this paper, we propose a dataflow-guided retrieval augmentation approach, called DraCo, for repository-level code completion. DraCo parses a private repository into code entities and establishes their relations through an extended dataflow analysis, forming a repo-specific context graph. Whenever triggering code completion, DraCo precisely retrieves relevant background knowledge from the repo-specific context graph and generates well-formed prompts to query code LMs. Furthermore, we construct a large Python dataset, ReccEval, with more diverse completion targets. Our experiments demonstrate the superior accuracy and applicable efficiency of DraCo, improving code exact match by 3.43% and identifier F1-score by 3.27% on average compared to the state-of-the-art approach.</abstract>
      <url hash="2b3109ca">2024.acl-long.431</url>
      <bibkey>cheng-etal-2024-dataflow</bibkey>
      <doi>10.18653/v1/2024.acl-long.431</doi>
    </paper>
    <paper id="432">
      <title>Chain-of-Exemplar: Enhancing Distractor Generation for Multimodal Educational Question Generation</title>
      <author><first>Haohao</first><last>Luo</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Ying</first><last>Shen</last></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>7978-7993</pages>
      <abstract>Multiple-choice questions (MCQs) are important in enhancing concept learning and student engagement for educational purposes. Despite the multimodal nature of educational content, current methods focus mainly on text-based inputs and often neglect the integration of visual information. In this work, we study the problem of multimodal educational question generation, which aims at generating subject-specific educational questions with plausible yet incorrect distractors based on multimodal educational content. To tackle this problem, we introduce a novel framework, named Chain-of-Exemplar (CoE), which utilizes multimodal large language models (MLLMs) with Chain-of-Thought reasoning to improve the generation of challenging distractors. Furthermore, CoE leverages three-stage contextualized exemplar retrieval to retrieve exemplary questions as guides for generating more subject-specific educational questions. Experimental results on the ScienceQA benchmark demonstrate the superiority of CoE in both question generation and distractor generation over existing methods across various subjects and educational levels.</abstract>
      <url hash="d332ba1e">2024.acl-long.432</url>
      <bibkey>luo-etal-2024-chain</bibkey>
      <doi>10.18653/v1/2024.acl-long.432</doi>
    </paper>
    <paper id="433">
      <title><fixed-case>LLME</fixed-case>mbed: Rethinking Lightweight <fixed-case>LLM</fixed-case>’s Genuine Function in Text Classification</title>
      <author><first>Chun</first><last>Liu</last><affiliation>AMS</affiliation></author>
      <author><first>Hongguang</first><last>Zhang</last><affiliation>Systems Engineering Institute, AMS</affiliation></author>
      <author><first>Kainan</first><last>Zhao</last><affiliation>AMS</affiliation></author>
      <author><first>Xinghai</first><last>Ju</last><affiliation>Information Engineering University</affiliation></author>
      <author><first>Lin</first><last>Yang</last></author>
      <pages>7994-8004</pages>
      <abstract>With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas. Recently, many attempts based on prompt-learning have been made to improve the performance of text classification. However, most of these methods are based on heuristic Chain-of-Thought (CoT), and tend to be more complex but less efficient. In this paper, we rethink the LLM-based text classification methodology, propose a simple and effective transfer learning strategy, namely LLMEmbed, to address this classical but challenging task. To illustrate, we first study how to properly extract and fuse the text embeddings via various lightweight LLMs at different network depths to improve their robustness and discrimination, then adapt such embeddings to train the classifier. We perform extensive experiments on publicly available datasets, and the results show that LLMEmbed achieves strong performance while enjoys low training overhead using lightweight LLM backbones compared to recent methods based on larger LLMs, *i.e.* GPT-3, and sophisticated prompt-based strategies. Our LLMEmbed achieves adequate accuracy on publicly available benchmarks without any fine-tuning while merely use 4% model parameters, 1.8% electricity consumption and 1.5% runtime compared to its counterparts. Code is available at: https://github.com/ChunLiu-cs/LLMEmbed-ACL2024.</abstract>
      <url hash="06fc7249">2024.acl-long.433</url>
      <bibkey>liu-etal-2024-llmembed</bibkey>
      <doi>10.18653/v1/2024.acl-long.433</doi>
    </paper>
    <paper id="434">
      <title><fixed-case>LEMON</fixed-case>: Reviving Stronger and Smaller <fixed-case>LM</fixed-case>s from Larger <fixed-case>LM</fixed-case>s with Linear Parameter Fusion</title>
      <author><first>Yilong</first><last>Chen</last></author>
      <author><first>Junyuan</first><last>Shang</last><affiliation>Baidu</affiliation></author>
      <author><first>Zhenyu</first><last>Zhang</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Shiyao</first><last>Cui</last></author>
      <author><first>Tingwen</first><last>Liu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shuohuan</first><last>Wang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <pages>8005-8019</pages>
      <abstract>In the new era of language models, small models (with billions of parameter sizes) are receiving increasing attention due to their flexibility and cost-effectiveness in deployment. However, limited by the model size, the performance of small models trained from scratch may often be unsatisfactory. Learning a stronger and smaller model with the help of larger models is an intuitive idea. Inspired by the observing modular structures in preliminary analysis, we propose LEMON to learn competent initial points for smaller models by fusing parameters from larger models, thereby laying a solid foundation for subsequent training. Specifically, the parameter fusion process involves two operators for layer and dimension, respectively, and we also introduce controllable receptive fields to model the prior parameter characteristics. In this way, the larger model could be transformed into any specific smaller scale and architecture. Starting from LLaMA 2-7B, we revive two stronger and smaller models with 1.3B and 2.7B. Experimental results demonstrate that the fusion-based method exhibits flexibility and outperforms a series of competitive baselines in terms of both effectiveness and efficiency.</abstract>
      <url hash="0b53c1ad">2024.acl-long.434</url>
      <bibkey>chen-etal-2024-lemon</bibkey>
      <doi>10.18653/v1/2024.acl-long.434</doi>
    </paper>
    <paper id="435">
      <title>Speech Sense Disambiguation: Tackling Homophone Ambiguity in End-to-End Speech Translation</title>
      <author><first>Tengfei</first><last>Yu</last></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technolgy, Shenzhen</affiliation></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>8020-8035</pages>
      <abstract>End-to-end speech translation (ST) presents notable disambiguation challenges as it necessitates simultaneous cross-modal and cross-lingual transformations. While word sense disambiguation is an extensively investigated topic in textual machine translation, the exploration of disambiguation strategies for ST models remains limited. Addressing this gap, this paper introduces the concept of speech sense disambiguation (SSD), specifically emphasizing homophones - words pronounced identically but with different meanings. To facilitate this, we first create a comprehensive homophone dictionary and an annotated dataset rich with homophone information established based on speech-text alignment. Building on this unique dictionary, we introduce AmbigST, an innovative homophone-aware contrastive learning approach that integrates a homophone-aware masking strategy. Our experiments on different MuST-C and CoVoST ST benchmarks demonstrate that AmbigST sets new performance standards. Specifically, it achieves SOTA results on BLEU scores for English to German, Spanish, and French ST tasks, underlining its effectiveness in reducing speech sense ambiguity. Data, code and scripts are freely available at https://github.com/ytf-philp/AmbigST.</abstract>
      <url hash="4398ad2a">2024.acl-long.435</url>
      <bibkey>yu-etal-2024-speech</bibkey>
      <doi>10.18653/v1/2024.acl-long.435</doi>
    </paper>
    <paper id="436">
      <title>To be Continuous, or to be Discrete, Those are Bits of Questions</title>
      <author><first>Yiran</first><last>Wang</last><affiliation>National Institute of Information and Communications Technology</affiliation></author>
      <author><first>Masao</first><last>Utiyama</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>8036-8049</pages>
      <abstract>Recently, binary representation has been proposed as a novel representation that lies between continuous and discrete representations. It exhibits considerable information-preserving capability when being used to replace continuous input vectors. In this paper, we investigate the feasibility of further introducing it to the output side, aiming to allow models to output binary labels instead. To preserve the structural information on the output side along with label information, we extend the previous contrastive hashing method as structured contrastive hashing. More specifically, we upgrade CKY from label-level to bit-level, define a new similarity function with span marginal probabilities, and introduce a novel contrastive loss function with a carefully designed instance selection strategy. Our model achieves competitive performance on various structured prediction tasks, and demonstrates that binary representation can be considered a novel representation that further bridges the gap between the continuous nature of deep learning and the discrete intrinsic property of natural languages.</abstract>
      <url hash="444545aa">2024.acl-long.436</url>
      <bibkey>wang-utiyama-2024-continuous</bibkey>
      <doi>10.18653/v1/2024.acl-long.436</doi>
    </paper>
    <paper id="437">
      <title>Moûsai: Efficient Text-to-Music Diffusion Models</title>
      <author><first>Flavio</first><last>Schneider</last></author>
      <author><first>Ojasv</first><last>Kamal</last></author>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Bernhard</first><last>Schölkopf</last><affiliation>ELLIS Institute and Max Planck Institute for Intelligent Systems, Max-Planck Institute</affiliation></author>
      <pages>8050-8068</pages>
      <abstract>Recent years have seen the rapid development of large generative models for text; however, much less research has explored the connection between text and another “language” of communication – music. Music, much like text, can convey emotions, stories, and ideas, and has its own unique structure and syntax. In our work, we bridge text and music via a text-to-music generation model that is highly efficient, expressive, and can handle long-term structure. Specifically, we develop Moûsai, a cascading two-stage latent diffusion model that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. Moreover, our model features high efficiency, which enables real-time inference on a single consumer GPU with a reasonable speed. Through experiments and property analyses, we show our model’s competence over a variety of criteria compared with existing music generation models. Lastly, to promote the open-source culture, we provide a collection of open-source libraries with the hope of facilitating future work in the field. We open-source the following: Codes: https://github.com/archinetai/audio-diffusion-pytorch. Music samples for this paper: http://bit.ly/44ozWDH. Music samples for all models: https://bit.ly/audio-diffusion.</abstract>
      <url hash="4b05a8a8">2024.acl-long.437</url>
      <bibkey>schneider-etal-2024-mousai</bibkey>
      <doi>10.18653/v1/2024.acl-long.437</doi>
    </paper>
    <paper id="438">
      <title><fixed-case>P</fixed-case>oke<fixed-case>MQA</fixed-case>: Programmable knowledge editing for Multi-hop Question Answering</title>
      <author><first>Hengrui</first><last>Gu</last><affiliation>Jilin University</affiliation></author>
      <author><first>Kaixiong</first><last>Zhou</last><affiliation>Rice University</affiliation></author>
      <author><first>Xiaotian</first><last>Han</last></author>
      <author><first>Ninghao</first><last>Liu</last><affiliation>University of Georgia</affiliation></author>
      <author><first>Ruobing</first><last>Wang</last></author>
      <author><first>Xin</first><last>Wang</last><affiliation>Jilin University</affiliation></author>
      <pages>8069-8083</pages>
      <abstract>Multi-hop question answering (MQA) is one of the challenging tasks to evaluate machine’s comprehension and reasoning abilities, where large language models (LLMs) have widely achieved the human-comparable performance. Due to the dynamics of knowledge facts in real world, knowledge editing has been explored to update model with the up-to-date facts while avoiding expensive re-training or fine-tuning. Starting from the edited fact, the updated model needs to provide cascading changes in the chain of MQA. The previous art simply adopts a mix-up prompt to instruct LLMs conducting multiple reasoning tasks sequentially, including question decomposition, answer generation, and conflict checking via comparing with edited facts. However, the coupling of these functionally-diverse reasoning tasks inhibits LLMs’ advantages in comprehending and answering questions while disturbing them with the unskilled task of conflict checking. We thus propose a framework, Programmable knowledge editing for Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically, we prompt LLMs to decompose knowledge-augmented multi-hop question, while interacting with a detached trainable scope detector to modulate LLMs behavior depending on external conflict signal. The experiments on three LLM backbones and two benchmark datasets validate our superiority in knowledge editing of MQA, outperforming all competitors by a large margin in almost all settings and consistently producing reliable reasoning process.</abstract>
      <url hash="0ddd2ddb">2024.acl-long.438</url>
      <bibkey>gu-etal-2024-pokemqa</bibkey>
      <doi>10.18653/v1/2024.acl-long.438</doi>
    </paper>
    <paper id="439">
      <title><fixed-case>M</fixed-case>eme<fixed-case>G</fixed-case>uard: An <fixed-case>LLM</fixed-case> and <fixed-case>VLM</fixed-case>-based Framework for Advancing Content Moderation via Meme Intervention</title>
      <author><first>Prince</first><last>Jha</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Raghav</first><last>Jain</last><affiliation>Indian Institute of Technology, Patna.</affiliation></author>
      <author><first>Konika</first><last>Mandal</last></author>
      <author><first>Aman</first><last>Chadha</last><affiliation>Amazon</affiliation></author>
      <author><first>Sriparna</first><last>Saha</last><affiliation>Indian Institute of Technology Patna, India</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>8084-8104</pages>
      <abstract>In the digital world, memes present a unique challenge for content moderation due to their potential to spread harmful content. Although detection methods have improved, proactive solutions such as intervention are still limited, with current research focusing mostly on text-based content, neglecting the widespread influence of multimodal content like memes. Addressing this gap, we present <i>MemeGuard</i>, a comprehensive framework leveraging Large Language Models (LLMs) and Visual Language Models (VLMs) for meme intervention. <i>MemeGuard</i> harnesses a specially fine-tuned VLM, <i>VLMeme</i>, for meme interpretation, and a multimodal knowledge selection and ranking mechanism (<i>MKS</i>) for distilling relevant knowledge. This knowledge is then employed by a general-purpose LLM to generate contextually appropriate interventions. Another key contribution of this work is the <i>
          <b>I</b>ntervening</i>
        <i>
          <b>C</b>yberbullying in <b>M</b>ultimodal <b>M</b>emes (ICMM)</i> dataset, a high-quality, labeled dataset featuring toxic memes and their corresponding human-annotated interventions. We leverage <i>ICMM</i> to test <i>MemeGuard</i>, demonstrating its proficiency in generating relevant and effective responses to toxic memes. red <b>Disclaimer</b>: <i>This paper contains harmful content that may be disturbing to some readers.</i></abstract>
      <url hash="f7c5f42f">2024.acl-long.439</url>
      <bibkey>jha-etal-2024-memeguard</bibkey>
      <doi>10.18653/v1/2024.acl-long.439</doi>
    </paper>
    <paper id="440">
      <title>Efficient <fixed-case>OCR</fixed-case> for Building a Diverse Digital History</title>
      <author><first>Jacob</first><last>Carlson</last><affiliation>Harvard University</affiliation></author>
      <author><first>Tom</first><last>Bryan</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <author><first>Melissa</first><last>Dell</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <pages>8105-8115</pages>
      <abstract>Many users consult digital archives daily, but the information they can access is unrepresentative of the diversity of documentary history. The sequence-to-sequence architecture typically used for optical character recognition (OCR) – which jointly learns a vision and language model – is poorly extensible to low-resource document collections, as learning a language-vision model requires extensive labeled sequences and compute. This study models OCR as a character level image retrieval problem, using a contrastively trained vision encoder. Because the model only learns characters’ visual features, it is more sample efficient and extensible than existing architectures, enabling accurate OCR in settings where existing solutions fail. Crucially, it opens new avenues for community engagement in making digital history more representative of documentary history.</abstract>
      <url hash="1c15e22a">2024.acl-long.440</url>
      <bibkey>carlson-etal-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.acl-long.440</doi>
    </paper>
    <paper id="441">
      <title>Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space</title>
      <author><first>Zongru</first><last>Wu</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Pengzhou</first><last>Cheng</last></author>
      <author><first>Gongshen</first><last>Liu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>8116-8134</pages>
      <abstract>Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose <b>Mu</b>lti-<b>Sc</b>a<b>le</b> <b>Lo</b>w-<b>R</b>ank <b>A</b>daptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, and Llama2. The codes are publicly available at Anonymous.</abstract>
      <url hash="bb00fe1f">2024.acl-long.441</url>
      <bibkey>wu-etal-2024-acquiring</bibkey>
      <doi>10.18653/v1/2024.acl-long.441</doi>
    </paper>
    <paper id="442">
      <title><fixed-case>ANAH</fixed-case>: Analytical Annotation of Hallucinations in Large Language Models</title>
      <author><first>Ziwei</first><last>Ji</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yuzhe</first><last>Gu</last><affiliation>Shanghai Jiaotong University and Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Wenwei</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Chengqi</first><last>Lyu</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>8135-8158</pages>
      <abstract>Reducing the ‘<tex-math>\textit{hallucination}</tex-math>' problem of Large Language Models (LLMs) is crucial for their wide applications. A comprehensive and fine-grained measurement of the hallucination is the first key step for the governance of this issue but is under-explored in the community.Thus, we present <tex-math>\textbf{ANAH}</tex-math>, a bilingual dataset that offers <tex-math>\textbf{AN}</tex-math>alytical <tex-math>\textbf{A}</tex-math>nnotation of <tex-math>\textbf{H}</tex-math>allucinations in LLMs within Generative Question Answering.Each answer sentence in our dataset undergoes rigorous annotation, involving the retrieval of a reference fragment, the judgment of the hallucination type, and the correction of hallucinated content. ANAH consists of ~12k sentence-level annotations for ~4.3k LLM responses covering over 700 topics, constructed by a human-in-the-loop pipeline.Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer and use ANAH to train and evaluate hallucination annotators. We conduct extensive experiments on studying generative and discriminative annotators and show that, although current open-source LLMs have difficulties in fine-grained hallucination annotation, the generative annotator trained with ANAH can surpass all open-source LLMs and GPT-3.5, obtain performance competitive with GPT-4, and exhibits better generalization ability on unseen questions.</abstract>
      <url hash="e2202208">2024.acl-long.442</url>
      <bibkey>ji-etal-2024-anah</bibkey>
      <doi>10.18653/v1/2024.acl-long.442</doi>
    </paper>
    <paper id="443">
      <title>Aligning Large Language Models for Controllable Recommendations</title>
      <author><first>Wensheng</first><last>Lu</last></author>
      <author><first>Jianxun</first><last>Lian</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Guanghua</first><last>Li</last></author>
      <author><first>Mingyang</first><last>Zhou</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Hao</first><last>Liao</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft</affiliation></author>
      <pages>8159-8172</pages>
      <abstract>Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems — systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy using a fixed task template, often overlooking the diversity of recommendation tasks and the ability of LLMs to follow recommendation-specific instructions. To address this gap, we first introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs’ proficiency in adhering to recommendation-specific instructions. Next, we propose a reinforcement learning-based alignment procedure to enhance LLMs’ generalization ability. Extensive experiments on two real-world datasets demonstrate that our approach significantly improves the capability of LLMs to respond to instructions within recommender systems, reducing formatting errors while maintaining a high level of accuracy.</abstract>
      <url hash="c3a4d8ad">2024.acl-long.443</url>
      <bibkey>lu-etal-2024-aligning</bibkey>
      <doi>10.18653/v1/2024.acl-long.443</doi>
    </paper>
    <paper id="444">
      <title>Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods</title>
      <author><first>Haeun</first><last>Yu</last></author>
      <author><first>Pepa</first><last>Atanasova</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>8173-8186</pages>
      <abstract>Language Models (LMs) acquire parametric knowledge from their training process, embedding it within their weights. The increasing scalability of LMs, however, poses significant challenges for understanding a model’s inner workings and further for updating or correcting this embedded knowledge without the significant cost of retraining. This underscores the importance of unveiling exactly what knowledge is stored and its association with specific model components. Instance Attribution (IA) and Neuron Attribution (NA) offer insights into this training-acquired knowledge, though they have not been compared systematically. Our study introduces a novel evaluation framework to quantify and compare the knowledge revealed by IA and NA. To align the results of the methods we introduce the attribution method NA-Instances to apply NA for retrieving influential training instances, and IA-Neurons to discover important neurons of influential instances discovered by IA. We further propose a comprehensive list of faithfulness tests to evaluate the comprehensiveness and sufficiency of the explanations provided by both methods. Through extensive experiments and analysis, we demonstrate that NA generally reveals more diverse and comprehensive information regarding the LM’s parametric knowledge compared to IA. Nevertheless, IA provides unique and valuable insights into the LM’s parametric knowledge, which are not revealed by NA. Our findings further suggest the potential of a synergistic approach of combining the diverse findings of IA and NA for a more holistic understanding of an LM’s parametric knowledge.</abstract>
      <url hash="4283e77d">2024.acl-long.444</url>
      <bibkey>yu-etal-2024-revealing</bibkey>
      <doi>10.18653/v1/2024.acl-long.444</doi>
    </paper>
    <paper id="445">
      <title>Full Parameter Fine-tuning for Large Language Models with Limited Resources</title>
      <author><first>Kai</first><last>Lv</last></author>
      <author><first>Yuqing</first><last>Yang</last></author>
      <author><first>Tengxiao</first><last>Liu</last><affiliation>Fudan University and Amazon</affiliation></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>8187-8198</pages>
      <abstract>Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 <tex-math>\times</tex-math> RTX 3090, each with 24GB memory. Code and data are available at https://github.com/OpenLMLab/LOMO.</abstract>
      <url hash="60dac715">2024.acl-long.445</url>
      <bibkey>lv-etal-2024-full</bibkey>
      <doi>10.18653/v1/2024.acl-long.445</doi>
    </paper>
    <paper id="446">
      <title><fixed-case>M</fixed-case><tex-math>^3</tex-math><fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought</title>
      <author><first>Qiguang</first><last>Chen</last></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jin</first><last>Zhang</last></author>
      <author><first>Zhi</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Xiao</first><last>Xu</last></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>8199-8221</pages>
      <abstract>Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M<tex-math>^3</tex-math>CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M<tex-math>^3</tex-math>CoT and there is a large gap between VLLMs and human performance in M<tex-math>^3</tex-math>CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M<tex-math>^3</tex-math>CoT will serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.</abstract>
      <url hash="43e12b72">2024.acl-long.446</url>
      <bibkey>chen-etal-2024-m3cot</bibkey>
      <doi>10.18653/v1/2024.acl-long.446</doi>
    </paper>
    <paper id="447">
      <title>Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models</title>
      <author><first>Longze</first><last>Chen</last></author>
      <author><first>Ziqiang</first><last>Liu</last></author>
      <author><first>Wanwei</first><last>He</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yinhe</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Yunshui</first><last>Li</last></author>
      <author><first>Run</first><last>Luo</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <pages>8222-8234</pages>
      <abstract>Long-context modeling capabilities are important for large language models (LLMs) in various applications. However, directly training LLMs with long context windows is insufficient to enhance this capability since some training samples do not exhibit strong semantic dependencies across long contexts.In this study, we propose a data mining framework ProLong that can assign each training sample with a long dependency score, which can be used to rank and filter samples that are more advantageous for enhancing long-context modeling abilities in LLM training. Specifically, we first use delta perplexity scores to measure the Dependency Strength between text segments in a given document. Then, we refine this metric based on the Dependency Distance of these segments to incorporate spatial relationships across long contexts. Final results are calibrated with a Dependency Specificity metric to prevent trivial dependencies introduced by repetitive patterns. Moreover, a random sampling approach is proposed to optimize the computational efficiency of ProLong. Comprehensive experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies, and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities.</abstract>
      <url hash="2b5f71ac">2024.acl-long.447</url>
      <bibkey>chen-etal-2024-long</bibkey>
      <doi>10.18653/v1/2024.acl-long.447</doi>
    </paper>
    <paper id="448">
      <title>Label-Synchronous Neural Transducer for <fixed-case>E</fixed-case>2<fixed-case>E</fixed-case> Simultaneous Speech Translation</title>
      <author><first>Keqi</first><last>Deng</last></author>
      <author><first>Phil</first><last>Woodland</last><affiliation>University of Cambridge</affiliation></author>
      <pages>8235-8251</pages>
      <abstract>While the neural transducer is popular for online speech recognition, simultaneous speech translation (SST) requires both streaming and re-ordering capabilities. This paper presents the LS-Transducer-SST, a label-synchronous neural transducer for SST, which naturally possesses these two properties. The LS-Transducer-SST dynamically decides when to emit translation tokens based on an Auto-regressive Integrate-and-Fire (AIF) mechanism. A latency-controllable AIF is also proposed, which can control the quality-latency trade-off either only during decoding, or it can be used in both decoding and training. The LS-Transducer-SST can naturally utilise monolingual text-only data via its prediction network which helps alleviate the key issue of data sparsity for E2E SST. During decoding, a chunk-based incremental joint decoding technique is designed to refine and expand the search space. Experiments on the Fisher-CallHome Spanish (Es-En) and MuST-C En-De data show that the LS-Transducer-SST gives a better quality-latency trade-off than existing popular methods. For example, the LS-Transducer-SST gives a 3.1/2.9 point BLEU increase (Es-En/En-De) relative to CAAT at a similar latency and a 1.4 s reduction in average lagging latency with similar BLEU scores relative to Wait-k.</abstract>
      <url hash="2557e213">2024.acl-long.448</url>
      <bibkey>deng-woodland-2024-label</bibkey>
      <doi>10.18653/v1/2024.acl-long.448</doi>
    </paper>
    <paper id="449">
      <title>Hard Prompts Made Interpretable: Sparse Entropy Regularization for Prompt Tuning with <fixed-case>RL</fixed-case></title>
      <author><first>Yunseon</first><last>Choi</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sangmin</first><last>Bae</last></author>
      <author><first>Seonghyun</first><last>Ban</last></author>
      <author><first>Minchan</first><last>Jeong</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Chuheng</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <author><first>Li</first><last>Zhao</last></author>
      <author><first>Jiang</first><last>Bian</last><affiliation>Microsoft</affiliation></author>
      <author><first>Kee-Eung</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>8252-8271</pages>
      <abstract>With the advent of foundation models, prompt tuning has positioned itself as an important technique for directing model behaviors and eliciting desired responses. Prompt tuning regards selecting appropriate keywords included into the input, thereby adapting to the downstream task without adjusting or fine-tuning the model parameters. There is a wide range of work in prompt tuning, from approaches that directly harness the backpropagated gradient signals from the model, to those employing black-box optimization such as reinforcement learning (RL) methods. Our primary focus is on RLPrompt, which aims to find optimal prompt tokens leveraging soft Q-learning. While the results show promise, we have observed that the prompts frequently appear unnatural, which impedes their interpretability. We address this limitation by using sparse Tsallis entropy regularization, a principled approach to filtering out unlikely tokens from consideration. We extensively evaluate our approach across various tasks, including few-shot text classification, unsupervised text style transfer, and textual inversion from images. The results indicate a notable improvement over baselines, highlighting the efficacy of our approach in addressing the challenges of prompt tuning. Moreover, we show that the prompts discovered using our method are more natural and interpretable compared to those from other baselines.</abstract>
      <url hash="24ccc0f6">2024.acl-long.449</url>
      <bibkey>choi-etal-2024-hard</bibkey>
      <doi>10.18653/v1/2024.acl-long.449</doi>
    </paper>
    <paper id="450">
      <title>A Modular Approach for Multimodal Summarization of <fixed-case>TV</fixed-case> Shows</title>
      <author><first>Louis</first><last>Mahon</last></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>8272-8291</pages>
      <abstract>In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PRISMA (**P**recision and **R**ecall Evaluat**i**on of **s**ummary F**a**cts), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset (Papalampidi &amp; Lapata, 2023), our method produces higher quality summaries than comparison models, as measured with ROUGE and our new fact-based metric.</abstract>
      <url hash="b64c7a7a">2024.acl-long.450</url>
      <bibkey>mahon-lapata-2024-modular</bibkey>
      <doi>10.18653/v1/2024.acl-long.450</doi>
    </paper>
    <paper id="451">
      <title>Think Twice: Perspective-Taking Improves Large Language Models’ Theory-of-Mind Capabilities</title>
      <author><first>Alex</first><last>Wilf</last></author>
      <author><first>Sihyun</first><last>Lee</last></author>
      <author><first>Paul Pu</first><last>Liang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Louis-Philippe</first><last>Morency</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>8292-8308</pages>
      <abstract>Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs’ reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought (CoT) have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory “Simulation Theory” to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory’s notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs’ ToM capabilities.</abstract>
      <url hash="d858df8d">2024.acl-long.451</url>
      <bibkey>wilf-etal-2024-think</bibkey>
      <doi>10.18653/v1/2024.acl-long.451</doi>
    </paper>
    <paper id="452">
      <title><fixed-case>B</fixed-case>iz<fixed-case>B</fixed-case>ench: A Quantitative Reasoning Benchmark for Business and Finance</title>
      <author><first>Michael</first><last>Krumdick</last><affiliation>Kensho</affiliation></author>
      <author><first>Rik</first><last>Koncel-Kedziorski</last><affiliation>Apple</affiliation></author>
      <author><first>Viet Dac</first><last>Lai</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Varshini</first><last>Reddy</last></author>
      <author><first>Charles</first><last>Lovering</last><affiliation>Kensho</affiliation></author>
      <author><first>Chris</first><last>Tanner</last><affiliation>Massachusetts Institute of Technology and Kensho</affiliation></author>
      <pages>8309-8332</pages>
      <abstract>Answering questions within business and finance requires reasoning, precision, and a wide-breadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models’ ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. We include three financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model’s financial background knowledge, ability to parse financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, comparing and contrasting the behavior of code-focused and language-focused models. We demonstrate that the current bottleneck in performance is due to LLMs’ limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.</abstract>
      <url hash="c0a7c63c">2024.acl-long.452</url>
      <bibkey>krumdick-etal-2024-bizbench</bibkey>
      <doi>10.18653/v1/2024.acl-long.452</doi>
    </paper>
    <paper id="453">
      <title>Direct Metric Optimization for Image Captioning through Reward-Weighted Augmented Data Utilization</title>
      <author><first>Takumi</first><last>Takada</last><affiliation>SoftBank Corp.</affiliation></author>
      <author><first>Yuma</first><last>Suzuki</last></author>
      <author><first>Hiroki</first><last>Takushima</last><affiliation>Softbank Corp.</affiliation></author>
      <author><first>Hayato</first><last>Tanoue</last><affiliation>SoftBank Corp</affiliation></author>
      <author><first>Haruki</first><last>Sato</last><affiliation>SoftBank Corp.</affiliation></author>
      <author><first>Aiswariya</first><last>Kumar</last><affiliation>SoftBank Corp.</affiliation></author>
      <author><first>Hiroki</first><last>Nishihara</last><affiliation>SoftBank Corp.</affiliation></author>
      <author><first>Takayuki</first><last>Hori</last><affiliation>Softbank Corp. and Waseda University</affiliation></author>
      <author><first>Kazuya</first><last>Ueki</last></author>
      <pages>8333-8346</pages>
      <abstract>While image captioning is an essential field of vision language models (VLM), a lack of continuity between the learning objective and final performance metrics of VLMs complicates their training and optimization. Reinforcement learning (RL) can directly optimize such metrics, but it is accompanied by a significant computational cost, making it difficult to apply to recent large-scale VLMs. In this paper, we propose Direct Metric Optimization (DMO), which is a lightweight final-metric-optimizing training method. We replace the computationally expensive exploration process in RL with an offline, diverse text data augmentation and show that self-supervised training on reward-weighted augmented data leads to direct and stable metric optimization. Our experiments demonstrate that DMO achieves performance comparable to those of the state-of-the-art RL method while saving hundreds of times more model forwarding iterations and greater amounts of computation time. This suggests that DMO constitutes a promising alternative for metric optimization in the era of large-scale VLMs.</abstract>
      <url hash="c3fa20e5">2024.acl-long.453</url>
      <bibkey>takada-etal-2024-direct</bibkey>
      <doi>10.18653/v1/2024.acl-long.453</doi>
    </paper>
    <paper id="454">
      <title>Deciphering Hate: Identifying Hateful Memes and Their Targets</title>
      <author><first>Eftekhar</first><last>Hossain</last></author>
      <author><first>Omar</first><last>Sharif</last><affiliation>Dartmouth College and Chittagong University of Engineering and Techology</affiliation></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Sarah Masud</first><last>Preum</last><affiliation>Dartmouth College</affiliation></author>
      <pages>8347-8359</pages>
      <abstract>Internet memes have become a powerful means for individuals to express emotions, thoughts, and perspectives on social media. While often considered as a source of humor and entertainment, memes can also disseminate hateful content targeting individuals or communities. Most existing research focuses on the negative aspects of memes in high-resource languages, overlooking the distinctive challenges associated with low-resource languages like Bengali (also known as Bangla). Furthermore, while previous work on Bengali memes has focused on detecting hateful memes, there has been no work on detecting their targeted entities. To bridge this gap and facilitate research in this arena, we introduce a novel multimodal dataset for Bengali, BHM (Bengali Hateful Memes). The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, tailored for two tasks: (i) detecting hateful memes, and (ii) detecting the social entities they target (i.e., Individual, Organization, Community, and Society). To solve these tasks, we propose DORA (Dual cO-attention fRAmework), a multimodal deep neural network that systematically extracts the significant modality features from the memes and jointly evaluates them with the modality-specific features to understand the context better. Our experiments show that DORA is generalizable on other low-resource hateful meme datasets and outperforms several state-of-the-art rivaling baselines.</abstract>
      <url hash="c57a657c">2024.acl-long.454</url>
      <bibkey>hossain-etal-2024-deciphering</bibkey>
      <doi>10.18653/v1/2024.acl-long.454</doi>
    </paper>
    <paper id="455">
      <title>Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings</title>
      <author><first>Yichen</first><last>Jiang</last></author>
      <author><first>Xiang</first><last>Zhou</last><affiliation>Google</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>8360-8383</pages>
      <abstract>Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes structurally equivalent sentences using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers even with low-complexity data. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empirically, we show SQ-Transformer achieves stronger compositional generalization than the vanilla Transformer on multiple low-complexity semantic parsing and machine translation datasets. In our analysis, we show SoVQ indeed learns a syntactically clustered embedding space, and SAL/SRL induces generalizable attention patterns, altogether leading to improved systematicity.</abstract>
      <url hash="82f5e972">2024.acl-long.455</url>
      <bibkey>jiang-etal-2024-inducing</bibkey>
      <doi>10.18653/v1/2024.acl-long.455</doi>
    </paper>
    <paper id="456">
      <title>Label-Efficient Model Selection for Text Generation</title>
      <author><first>Shir</first><last>Ashury Tahan</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Ariel</first><last>Gera</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Benjamin</first><last>Sznajder</last></author>
      <author><first>Leshem</first><last>Choshen</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Liat</first><last>Ein-Dor</last></author>
      <author><first>Eyal</first><last>Shnarch</last><affiliation>International Business Machines</affiliation></author>
      <pages>8384-8402</pages>
      <abstract>Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models based on preference annotations. DiffUse reduces the required amount of annotations, thus saving valuable time and resources in performing evaluation.DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model for selecting between models, prompts and configurations. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations – by up to 75% – while maintaining high evaluation reliability.</abstract>
      <url hash="c70646ff">2024.acl-long.456</url>
      <bibkey>ashury-tahan-etal-2024-label</bibkey>
      <doi>10.18653/v1/2024.acl-long.456</doi>
    </paper>
    <paper id="457">
      <title>Machine Unlearning of Pre-trained Large Language Models</title>
      <author><first>Jin</first><last>Yao</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Eli</first><last>Chien</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Minxin</first><last>Du</last></author>
      <author><first>Xinyao</first><last>Niu</last><affiliation>Inspir.ai</affiliation></author>
      <author><first>Tianhao</first><last>Wang</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Zezhou</first><last>Cheng</last></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>8403-8419</pages>
      <abstract>This study investigates the concept of the ‘right to be forgotten’ within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models–a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over <tex-math>10^5</tex-math> times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development.</abstract>
      <url hash="5b7b2821">2024.acl-long.457</url>
      <bibkey>yao-etal-2024-machine</bibkey>
      <doi>10.18653/v1/2024.acl-long.457</doi>
    </paper>
    <paper id="458">
      <title>Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals</title>
      <author><first>Francesco</first><last>Ortu</last></author>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Diego</first><last>Doimo</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Alberto</first><last>Cazzaniga</last><affiliation>AREA Science Park</affiliation></author>
      <author><first>Bernhard</first><last>Schölkopf</last><affiliation>ELLIS Institute and Max Planck Institute for Intelligent Systems, Max-Planck Institute</affiliation></author>
      <pages>8420-8436</pages>
      <abstract>Interpretability research aims to bridge the gap between the empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research in this area focused on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose the formulation of competition of mechanisms, which instead of individual mechanisms focuses on the interplay of multiple mechanisms, and traces how one of them becomes dominant in the final prediction. We uncover how and where the competition of mechanisms happens within LLMs using two interpretability methods, logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components, and reveal attention positions that effectively control the strength of certain mechanisms.</abstract>
      <url hash="fa7bd05f">2024.acl-long.458</url>
      <bibkey>ortu-etal-2024-competition</bibkey>
      <doi>10.18653/v1/2024.acl-long.458</doi>
    </paper>
    <paper id="459">
      <title><fixed-case>F</fixed-case>act<fixed-case>PICO</fixed-case>: Factuality Evaluation for Plain Language Summarization of Medical Evidence</title>
      <author><first>Sebastian</first><last>Joseph</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Lily</first><last>Chen</last></author>
      <author><first>Jan</first><last>Trienes</last></author>
      <author><first>Hannah</first><last>Göke</last><affiliation>University Hospital Essen</affiliation></author>
      <author><first>Monika</first><last>Coers</last></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Byron</first><last>Wallace</last><affiliation>Northeastern University, Brown University and Northeastern University</affiliation></author>
      <author><first>Junyi Jessy</first><last>Li</last><affiliation>University of Texas, Austin</affiliation></author>
      <pages>8437-8464</pages>
      <abstract>Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? This paper presents FactPICO, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. FactPICO consists of 345 plain language summaries of RCT abstracts generated from three LLMs (i.e., GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. We also evaluate the correctness of the extra information (e.g., explanations) added by LLMs. Using FactPICO, we benchmark a range of existing factuality metrics, including the newly devised ones based on LLMs. We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level.</abstract>
      <url hash="6a90a628">2024.acl-long.459</url>
      <bibkey>joseph-etal-2024-factpico</bibkey>
      <doi>10.18653/v1/2024.acl-long.459</doi>
    </paper>
    <paper id="460">
      <title><fixed-case>B</fixed-case>v<fixed-case>SP</fixed-case>: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction</title>
      <author><first>Yinhao</first><last>Bai</last></author>
      <author><first>Yalan</first><last>Xie</last><affiliation>Nankai University</affiliation></author>
      <author><first>Xiaoyi</first><last>Liu</last></author>
      <author><first>Yuhua</first><last>Zhao</last></author>
      <author><first>Zhixin</first><last>Han</last></author>
      <author><first>Mengting</first><last>Hu</last><affiliation>Nankai University</affiliation></author>
      <author><first>Hang</first><last>Gao</last><affiliation>Tianjin University of Science and Technology</affiliation></author>
      <author><first>Renhong</first><last>Cheng</last></author>
      <pages>8465-8482</pages>
      <abstract>Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including aspect term, opinion term, aspect category, and sentiment polarity. In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model. Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications. Therefore, we first construct a few-shot ASQP dataset (FSQP) that contains richer categories and is more balanced for the few-shot study. Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence. However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates. To tackle this issue, we further propose a Broad-view Soft Prompting (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates. Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensen–Shannon divergence. BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates. Then, we aggregate the results of multi-templates by voting mechanism. Empirical results demonstrate that BvSP significantly outperforms the state-of-the-art methods under four few-shot settings and other public datasets. Our code and dataset are available at https://github.com/byinhao/BvSP.</abstract>
      <url hash="e2dac863">2024.acl-long.460</url>
      <bibkey>bai-etal-2024-bvsp</bibkey>
      <doi>10.18653/v1/2024.acl-long.460</doi>
    </paper>
    <paper id="461">
      <title>Safety Alignment in <fixed-case>NLP</fixed-case> Tasks: Weakly Aligned Summarization as an In-Context Attack</title>
      <author><first>Yu</first><last>Fu</last><affiliation>University of California, Riverside</affiliation></author>
      <author><first>Yufei</first><last>Li</last><affiliation>University of California, Riverside</affiliation></author>
      <author><first>Wen</first><last>Xiao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Cong</first><last>Liu</last><affiliation>University of California, Riverside</affiliation></author>
      <author><first>Yue</first><last>Dong</last><affiliation>University of California, Riverside and McGill University</affiliation></author>
      <pages>8483-8502</pages>
      <abstract>Recent developments in balancing the usefulness and safety of Large Language Models (LLMs) have raised a critical question: Are mainstream NLP tasks adequately aligned with safety consideration? Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks. For instance, LLMs can effectively summarize malicious long documents but often refuse to translate them. This discrepancy highlights a previously unidentified vulnerability: attacks exploiting tasks with weaker safety alignment, like summarization, can potentially compromise the integrity of tasks traditionally deemed more robust, such as translation and question-answering (QA). Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content. We demonstrate these vulnerabilities in various safety-aligned LLMs, particularly Llama2 models, Gemini and GPT-4, indicating an urgent need for strengthening safety alignments across a broad spectrum of NLP tasks.</abstract>
      <url hash="ae64d74b">2024.acl-long.461</url>
      <bibkey>fu-etal-2024-safety</bibkey>
      <doi>10.18653/v1/2024.acl-long.461</doi>
    </paper>
    <paper id="462">
      <title>Speech language models lack important brain-relevant semantics</title>
      <author><first>Subba Reddy</first><last>Oota</last><affiliation>MPI-SWS</affiliation></author>
      <author><first>Emin</first><last>Çelik</last><affiliation>MPI-SWS</affiliation></author>
      <author><first>Fatma</first><last>Deniz</last></author>
      <author><first>Mariya</first><last>Toneva</last><affiliation>Max Planck Institute for Software Systems</affiliation></author>
      <pages>8503-8528</pages>
      <abstract>Despite known differences between reading and listening in the brain, recent work has shown that text-based language models predict both text-evoked and speech-evoked brain activity to an impressive degree. This poses the question of what types of information language models truly predict in the brain. We investigate this question via a direct approach, in which we systematically remove specific low-level stimulus features (textual, speech, and visual) from language model representations to assess their impact on alignment with fMRI brain recordings during reading and listening. Comparing these findings with speech-based language models reveals starkly different effects of low-level features on brain alignment. While text-based models show reduced alignment in early sensory regions post-removal, they retain significant predictive power in late language regions. In contrast, speech-based models maintain strong alignment in early auditory regions even after feature removal but lose all predictive power in late language regions. These results suggest that speech-based models provide insights into additional information processed by early auditory regions, but caution is needed when using them to model processing in late language regions. We make our code publicly available. [https://github.com/subbareddy248/speech-llm-brain]</abstract>
      <url hash="e3f3ed96">2024.acl-long.462</url>
      <bibkey>oota-etal-2024-speech</bibkey>
      <doi>10.18653/v1/2024.acl-long.462</doi>
    </paper>
    <paper id="463">
      <title><fixed-case>D</fixed-case>oc<fixed-case>LLM</fixed-case>: A Layout-Aware Generative Language Model for Multimodal Document Understanding</title>
      <author><first>Dongsheng</first><last>Wang</last><affiliation>JPMorgan AI Research</affiliation></author>
      <author><first>Natraj</first><last>Raman</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Mathieu</first><last>Sibue</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Zhiqiang</first><last>Ma</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Petr</first><last>Babkin</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Simerjot</first><last>Kaur</last><affiliation>JPMorgan Chase and Co</affiliation></author>
      <author><first>Yulong</first><last>Pei</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Armineh</first><last>Nourbakhsh</last><affiliation>School of Computer Science, Carnegie Mellon University and J.P. Morgan Chase</affiliation></author>
      <author><first>Xiaomo</first><last>Liu</last><affiliation>JP Morgan AI Research</affiliation></author>
      <pages>8529-8548</pages>
      <abstract>Enterprise documents such as forms, receipts, reports, and other such records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.</abstract>
      <url hash="80f0991f">2024.acl-long.463</url>
      <bibkey>wang-etal-2024-docllm</bibkey>
      <doi>10.18653/v1/2024.acl-long.463</doi>
    </paper>
    <paper id="464">
      <title>Bypassing <fixed-case>LLM</fixed-case> Watermarks with Color-Aware Substitutions</title>
      <author><first>Qilong</first><last>Wu</last></author>
      <author><first>Varun</first><last>Chandrasekaran</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <pages>8549-8581</pages>
      <abstract>Watermarking approaches are proposed to identify if text being circulated is human- or large language model- (LLM) generated. The state-of-the-art watermarking strategy of Kirchenbauer et al. (2023a) biases the LLM to generate specific (“green”) tokens. However, determining the robustness of this watermarking method under finite (low) edit budgets is an open problem. Additionally, existing attack methods failto evade detection for longer text segments. We overcome these limitations, and propose Self Color Testing-based Substitution (SCTS), thefirst “color-aware” attack. SCTS obtains color information by strategically prompting the watermarked LLM and comparing output tokensfrequencies. It uses this information to determine token colors, and substitutes green tokens with non-green ones. In our experiments, SCTS successfully evades watermark detection using fewer number of edits than related work. Additionally, we show both theoretically and empirically that SCTS can remove the watermark for arbitrarily long watermarked text.</abstract>
      <url hash="b712b3f2">2024.acl-long.464</url>
      <bibkey>wu-chandrasekaran-2024-bypassing</bibkey>
      <doi>10.18653/v1/2024.acl-long.464</doi>
    </paper>
    <paper id="465">
      <title>Parallel Structures in Pre-training Data Yield In-Context Learning</title>
      <author><first>Yanda</first><last>Chen</last><affiliation>Columbia University</affiliation></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>New York University Shanghai</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>He</first><last>He</last><affiliation>New York University</affiliation></author>
      <pages>8582-8592</pages>
      <abstract>Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs’ ICL ability depends on <tex-math>\textit{parallel structures}</tex-math> in the pre-training data—pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs’ ICL accuracy by <tex-math>\textbf{51}</tex-math>% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gram repetitions and long-range dependency, showing the diversity and generality of parallel structures. A closer look at the detected parallel structures indicates that they cover diverse linguistic tasks and span long distances in the data.</abstract>
      <url hash="efd1b39a">2024.acl-long.465</url>
      <bibkey>chen-etal-2024-parallel</bibkey>
      <doi>10.18653/v1/2024.acl-long.465</doi>
    </paper>
    <paper id="466">
      <title><fixed-case>O</fixed-case>pen<fixed-case>T</fixed-case>o<fixed-case>M</fixed-case>: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models</title>
      <author><first>Hainiu</first><last>Xu</last></author>
      <author><first>Runcong</first><last>Zhao</last></author>
      <author><first>Lixing</first><last>Zhu</last></author>
      <author><first>Jinhua</first><last>Du</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>8593-8623</pages>
      <abstract>Neural Theory-of-Mind (N-ToM), machine’s ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters’ psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs’ capabilities of modeling characters’ mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters’ mental states in the psychological world.</abstract>
      <url hash="53abdee1">2024.acl-long.466</url>
      <bibkey>xu-etal-2024-opentom</bibkey>
      <doi>10.18653/v1/2024.acl-long.466</doi>
    </paper>
    <paper id="467">
      <title>Towards Privacy-Aware Sign Language Translation at Scale</title>
      <author><first>Phillip</first><last>Rust</last></author>
      <author><first>Bowen</first><last>Shi</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Skyler</first><last>Wang</last></author>
      <author><first>Necati Cihan</first><last>Camgoz</last><affiliation>Meta</affiliation></author>
      <author><first>Jean</first><last>Maillard</last><affiliation>Meta (FAIR)</affiliation></author>
      <pages>8624-8641</pages>
      <abstract>A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we further discuss the advantages and limitations of self-supervised pretraining and anonymization via facial obfuscation for SLT.</abstract>
      <url hash="c95132a2">2024.acl-long.467</url>
      <bibkey>rust-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.acl-long.467</doi>
    </paper>
    <paper id="468">
      <title>Arithmetic Control of <fixed-case>LLM</fixed-case>s for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards</title>
      <author><first>Haoxiang</first><last>Wang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Yong</first><last>Lin</last></author>
      <author><first>Wei</first><last>Xiong</last><affiliation>Google and University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Rui</first><last>Yang</last></author>
      <author><first>Shizhe</first><last>Diao</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Shuang</first><last>Qiu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Han</first><last>Zhao</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>8642-8655</pages>
      <abstract>Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO).</abstract>
      <url hash="d73bbe09">2024.acl-long.468</url>
      <bibkey>wang-etal-2024-arithmetic</bibkey>
      <doi>10.18653/v1/2024.acl-long.468</doi>
    </paper>
    <paper id="469">
      <title>Towards Real-World Writing Assistance: A <fixed-case>C</fixed-case>hinese Character Checking Benchmark with Faked and Misspelled Characters</title>
      <author><first>Yinghui</first><last>Li</last></author>
      <author><first>Zishan</first><last>Xu</last></author>
      <author><first>Shaoshen</first><last>Chen</last></author>
      <author><first>Haojing</first><last>Huang</last></author>
      <author><first>Yangning</first><last>Li</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Shirong</first><last>Ma</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Zhongli</first><last>Li</last><affiliation>Baidu Ernie-bot</affiliation></author>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Hai-Tao</first><last>Zheng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Ying</first><last>Shen</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>8656-8668</pages>
      <abstract>Writing assistance aims to improve the correctness and quality of input texts, with character checking being crucial in detecting and correcting wrong characters. In the real world where handwriting occupies the vast majority, characters that humans get wrong include faked characters (i.e., untrue characters created due to writing errors) and misspelled characters (i.e., true characters used incorrectly due to spelling errors). However, existing datasets and related studies only focus on misspelled characters that can be represented by computer text encoding systems, thereby ignoring faked characters which are more common and difficult. To break through this dilemma, we present <tex-math>\textbf{Visual-C}</tex-math><tex-math>^3</tex-math>, a human-annotated <tex-math>\textbf{Visual}</tex-math> <tex-math>\textbf{C}</tex-math>hinese <tex-math>\textbf{C}</tex-math>haracter <tex-math>\textbf{C}</tex-math>hecking dataset with faked and misspelled Chinese characters. To the best of our knowledge, Visual-C<tex-math>^3</tex-math> is the first real-world visual and the largest human-crafted dataset for the Chinese character checking scenario. Additionally, we also propose and evaluate novel baseline methods on Visual-C<tex-math>^3</tex-math>. Extensive empirical results and analyses show that Visual-C<tex-math>^3</tex-math> is high-quality yet challenging. As the first study focusing on Chinese faked characters, the dataset and the baseline methods are publicly available at https://github.com/THUKElab/Visual-C3.</abstract>
      <url hash="9b2d2a49">2024.acl-long.469</url>
      <bibkey>li-etal-2024-towards-real</bibkey>
      <doi>10.18653/v1/2024.acl-long.469</doi>
    </paper>
    <paper id="470">
      <title><fixed-case>RAVEL</fixed-case>: Evaluating Interpretability Methods on Disentangling Language Model Representations</title>
      <author><first>Jing</first><last>Huang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Zhengxuan</first><last>Wu</last><affiliation>Stanford University</affiliation></author>
      <author><first>Christopher</first><last>Potts</last><affiliation>Stanford University</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Tel Aviv University and Google Research</affiliation></author>
      <author><first>Atticus</first><last>Geiger</last><affiliation>Pr(Ai)²R Group</affiliation></author>
      <pages>8669-8687</pages>
      <abstract>Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.</abstract>
      <url hash="29b21978">2024.acl-long.470</url>
      <bibkey>huang-etal-2024-ravel</bibkey>
      <doi>10.18653/v1/2024.acl-long.470</doi>
    </paper>
    <paper id="471">
      <title>Large Language Models as Zero-shot Dialogue State Tracker through Function Calling</title>
      <author><first>Zekun</first><last>Li</last></author>
      <author><first>Zhiyu</first><last>Chen</last></author>
      <author><first>Mike</first><last>Ross</last><affiliation>Facebook</affiliation></author>
      <author><first>Patrick</first><last>Huber</last><affiliation>Facebook</affiliation></author>
      <author><first>Seungwhan</first><last>Moon</last><affiliation>Facebook</affiliation></author>
      <author><first>Zhaojiang</first><last>Lin</last><affiliation>Facebook</affiliation></author>
      <author><first>Xin</first><last>Dong</last><affiliation>Facebook</affiliation></author>
      <author><first>Adithya</first><last>Sagar</last></author>
      <author><first>Xifeng</first><last>Yan</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Paul</first><last>Crook</last><affiliation>Meta</affiliation></author>
      <pages>8688-8704</pages>
      <abstract>Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT’s performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available at https://github.com/facebookresearch/FnCTOD.</abstract>
      <url hash="07ae6ae9">2024.acl-long.471</url>
      <bibkey>li-etal-2024-large-language-models</bibkey>
      <doi>10.18653/v1/2024.acl-long.471</doi>
    </paper>
    <paper id="472">
      <title>Faithful Chart Summarization with <fixed-case>C</fixed-case>ha<fixed-case>TS</fixed-case>-Pi</title>
      <author><first>Syrine</first><last>Krichene</last><affiliation>Google</affiliation></author>
      <author><first>Francesco</first><last>Piccinno</last><affiliation>Google</affiliation></author>
      <author><first>Fangyu</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Julian</first><last>Eisenschlos</last><affiliation>Google DeepMind</affiliation></author>
      <pages>8705-8723</pages>
      <abstract>Chart-to-summary generation can help explore data, communicate insights, and help the visually impaired people. Multi-modal generative models have been used to produce fluent summaries, but they can suffer from factual and perceptual errors. In this work we present CHATS-CRITIC, a reference-free chart summarization metric for scoring faithfulness. CHATS-CRITIC is composed of an image-to-text model to recover the table from a chart, and a tabular entailment model applied to score the summary sentence by sentence. We find that CHATS-CRITIC evaluates the summary quality according to human ratings better than reference-based metrics, either learned or n-gram based, and can be further used to fix candidate summaries by removing not supported sentences. We then introduce CHATS-PI, a chart-to-summary pipeline that leverages CHATS-CRITIC during inference to fix and rank sampled candidates from any chart-summarization model. We evaluate CHATS-PI and CHATS-CRITIC using human raters, establishing state-of-the-art results on two popular chart-to-summary datasets.</abstract>
      <url hash="c80ae593">2024.acl-long.472</url>
      <bibkey>krichene-etal-2024-faithful</bibkey>
      <doi>10.18653/v1/2024.acl-long.472</doi>
    </paper>
    <paper id="473">
      <title>Enhancing Dialogue State Tracking Models through <fixed-case>LLM</fixed-case>-backed User-Agents Simulation</title>
      <author><first>Cheng</first><last>Niu</last></author>
      <author><first>Xingguang</first><last>Wang</last></author>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Juntong</first><last>Song</last></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>8724-8741</pages>
      <abstract>Dialogue State Tracking (DST) is designed to monitor the evolving dialogue state in the conversations and plays a pivotal role in developing task-oriented dialogue systems. However, obtaining the annotated data for the DST task is usually a costly endeavor. In this paper, we focus on employing LLMs to generate dialogue data to reduce dialogue collection and annotation costs. Specifically, GPT-4 is used to simulate the user and agent interaction, generating thousands of dialogues annotated with DST labels. Then a two-stage fine-tuning on LLaMA 2 is performed on the generated data and the real data for the DST prediction. Experimental results on two public DST benchmarks show that with the generated dialogue data, our model performs better than the baseline trained solely on real data. In addition, our approach is also capable of adapting to the dynamic demands in real-world scenarios, generating dialogues in new domains swiftly. After replacing dialogue segments in any domain with the corresponding generated ones, the model achieves comparable performance to the model trained on real data. The source code and generated dialogue data are available at https://github.com/ParticleMedia/LUAS.</abstract>
      <url hash="a9d7b259">2024.acl-long.473</url>
      <bibkey>niu-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.acl-long.473</doi>
    </paper>
    <paper id="474">
      <title><fixed-case>M</fixed-case>eta<fixed-case>S</fixed-case>um<fixed-case>P</fixed-case>erceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking</title>
      <author><first>Ting-Chih</first><last>Chen</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Chia-Wei</first><last>Tang</last></author>
      <author><first>Chris</first><last>Thomas</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <pages>8742-8757</pages>
      <abstract>Fact-checking real-world claims often requires reviewing multiple multimodal documents in order to assess the claim’s truthfulness, a highly laborious and time-consuming task. In this paper, we present a summarization model crafted to generate claim-specific summaries useful for fact-checking from multimodal multi-document datasets. The model takes inputs in the form of documents, images, and a claim, with the objective of assisting in fact-checking tasks. We introduce a dynamic perceiver-based model that is able to handle inputs from multiple modalities of arbitrary lengths. To train our model, we leverage a novel reinforcement learning-based entailment objective in order to generate summaries that provide evidence distinguishing between different truthfulness labels. To assess the efficacy of our approach, we conduct experiments on both an existing benchmark as well as a new dataset of multi-document claims which we contribute. Our approach outperforms the SOTA approach by 4.6% in the claim verification task on the MOCHEG dataset and demonstrates strong performance on our new Multi-News-Fact-Checking dataset.</abstract>
      <url hash="90614a1e">2024.acl-long.474</url>
      <bibkey>chen-etal-2024-metasumperceiver</bibkey>
      <doi>10.18653/v1/2024.acl-long.474</doi>
    </paper>
    <paper id="475">
      <title><fixed-case>K</fixed-case>now<fixed-case>C</fixed-case>oder: Coding Structured Knowledge into <fixed-case>LLM</fixed-case>s for Universal Information Extraction</title>
      <author><first>Zixuan</first><last>Li</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yutao</first><last>Zeng</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yuxin</first><last>Zuo</last></author>
      <author><first>Weicheng</first><last>Ren</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wenxuan</first><last>Liu</last></author>
      <author><first>Miao</first><last>Su</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yucan</first><last>Guo</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yantao</first><last>Liu</last></author>
      <author><first>Lixiang</first><last>Lixiang</last></author>
      <author><first>Zhilei</first><last>Hu</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>Long</first><last>Bai</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wei</first><last>Li</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yidan</first><last>Liu</last></author>
      <author><first>Pan</first><last>Yang</last></author>
      <author><first>Xiaolong</first><last>Jin</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technolgy, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <pages>8758-8779</pages>
      <abstract/>
      <url hash="f6d06e20">2024.acl-long.475</url>
      <bibkey>li-etal-2024-knowcoder</bibkey>
      <doi>10.18653/v1/2024.acl-long.475</doi>
    </paper>
    <paper id="476">
      <title><fixed-case>ERA</fixed-case>-<fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>: Improving Chain-of-Thought through Entity Relationship Analysis</title>
      <author><first>Yanming</first><last>Liu</last></author>
      <author><first>Xinyue</first><last>Peng</last></author>
      <author><first>Tianyu</first><last>Du</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jianwei</first><last>Yin</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Weihao</first><last>Liu</last></author>
      <author><first>Xuhong</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>8780-8794</pages>
      <abstract>Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT).Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM’s understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs.</abstract>
      <url hash="8ff7d0c7">2024.acl-long.476</url>
      <bibkey>liu-etal-2024-era</bibkey>
      <doi>10.18653/v1/2024.acl-long.476</doi>
    </paper>
    <paper id="477">
      <title>On the Multi-turn Instruction Following for Conversational Web Agents</title>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Xuan</first><last>Zhang</last></author>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Yifei</first><last>Yuan</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>8795-8812</pages>
      <abstract>Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the proposed method.</abstract>
      <url hash="962bba5b">2024.acl-long.477</url>
      <bibkey>deng-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.acl-long.477</doi>
    </paper>
    <paper id="478">
      <title>Mobile-Bench: An Evaluation Benchmark for <fixed-case>LLM</fixed-case>-based Mobile Agents</title>
      <author><first>Shihan</first><last>Deng</last></author>
      <author><first>Weikai</first><last>Xu</last></author>
      <author><first>Hongda</first><last>Sun</last></author>
      <author><first>Wei</first><last>Liu</last><affiliation>xiaomi</affiliation></author>
      <author><first>Tao</first><last>Tan</last></author>
      <author><first>Liujianfeng</first><last>Liujianfeng</last></author>
      <author><first>Ang</first><last>Li</last><affiliation>Software, Xiaomi Inc.</affiliation></author>
      <author><first>Jian</first><last>Luan</last></author>
      <author><first>Bin</first><last>Wang</last><affiliation>AI Lab, Xiaomi Inc.</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Shuo</first><last>Shang</last></author>
      <pages>8813-8831</pages>
      <abstract>With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction.However, there is a scarcity of benchmarks available for LLM-based mobile agents.Benchmarking these agents generally faces three main challenges:(1) The inefficiency of UI-only operations imposes limitations to task evaluation.(2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents.(3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents.First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion.Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs.To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios.Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform will be released in the future.</abstract>
      <url hash="d53f074f">2024.acl-long.478</url>
      <bibkey>deng-etal-2024-mobile</bibkey>
      <doi>10.18653/v1/2024.acl-long.478</doi>
    </paper>
    <paper id="479">
      <title><fixed-case>MC</fixed-case><tex-math>^2</tex-math>: Towards Transparent and Culturally-Aware <fixed-case>NLP</fixed-case> for Minority Languages in <fixed-case>C</fixed-case>hina</title>
      <author><first>Chen</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Mingxu</first><last>Tao</last></author>
      <author><first>Quzhe</first><last>Huang</last></author>
      <author><first>Jiuheng</first><last>Lin</last></author>
      <author><first>Zhibin</first><last>Chen</last></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <pages>8832-8850</pages>
      <abstract>Current large language models demonstrate deficiencies in understanding low-resource languages, particularly the minority languages in China. This limitation stems from the scarcity of available pre-training data. To address this accessibility challenge, we present MC<tex-math>^2</tex-math>, a Multilingual Corpus of Minority Languages in China, which is the largest open-source corpus of its kind so far. MC<tex-math>^2</tex-math> includes four underrepresented languages: Tibetan, Uyghur, Kazakh, and Mongolian. Notably, we focus on the less common writing systems of Kazakh and Mongolian, i.e., Kazakh Arabic script and traditional Mongolian script, respectively, which have been long neglected in previous corpus construction efforts. Recognizing the prevalence of language contamination within existing corpora, we adopt a quality-centric solution for collecting MC<tex-math>^2</tex-math>, prioritizing accuracy while enhancing diversity. Furthermore, we underscore the importance of attending to the multiplicity of writing systems, which is closely related to the cultural awareness of the resulting models. The MC<tex-math>^2</tex-math> corpus and related models are made public to the community.</abstract>
      <url hash="d4322edd">2024.acl-long.479</url>
      <bibkey>zhang-etal-2024-mc2</bibkey>
      <doi>10.18653/v1/2024.acl-long.479</doi>
    </paper>
    <paper id="480">
      <title>Decoder-only Streaming Transformer for Simultaneous Translation</title>
      <author><first>Shoutao</first><last>Guo</last></author>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>8851-8864</pages>
      <abstract>Simultaneous Machine Translation (SiMT) generates translation while reading source tokens, essentially producing the target prefix based on the source prefix. To achieve good performance, it leverages the relationship between source and target prefixes to exact a policy to guide the generation of translations. Although existing SiMT methods primarily focus on the Encoder-Decoder architecture, we explore the potential of Decoder-only architecture, owing to its superior performance in various tasks and its inherent compatibility with SiMT. However, directly applying the Decoder-only architecture to SiMT poses challenges in terms of training and inference. To alleviate the above problems, we propose the first Decoder-only SiMT model, named Decoder-only Streaming Transformer (DST). Specifically, DST separately encodes the positions of the source and target prefixes, ensuring that the position of the target prefix remains unaffected by the expansion of the source prefix. Furthermore, we propose a Streaming Self-Attention (SSA) mechanism tailored for the Decoder-only architecture. It is capable of obtaining translation policy by assessing the sufficiency of input source information and integrating with the soft-attention mechanism to generate translations. Experiments demonstrate that our approach achieves state-of-the-art performance on three translation tasks.</abstract>
      <url hash="b5d30fae">2024.acl-long.480</url>
      <bibkey>guo-etal-2024-decoder</bibkey>
      <doi>10.18653/v1/2024.acl-long.480</doi>
    </paper>
    <paper id="481">
      <title>Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization</title>
      <author><first>Zhexin</first><last>Zhang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Junxiao</first><last>Yang</last></author>
      <author><first>Pei</first><last>Ke</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Hongning</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>8865-8887</pages>
      <abstract>While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of effort in defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the intrinsic conflict between the goals of being helpful and ensuring safety. Accordingly, we propose to integrate goal prioritization at both training and inference stages to counteract. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And integrating goal prioritization into model training reduces the ASR from 71.0% to 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks, both because of their stronger ability in instruction following. Our work thus contributes to the comprehension of jailbreaking attacks and defenses, and sheds light on the relationship between LLMs’ capability and safety. Our code is available at https://github.com/thu-coai/JailbreakDefense_GoalPriority.</abstract>
      <url hash="9c880a57">2024.acl-long.481</url>
      <bibkey>zhang-etal-2024-defending</bibkey>
      <doi>10.18653/v1/2024.acl-long.481</doi>
    </paper>
    <paper id="482">
      <title><fixed-case>I</fixed-case> am a Strange Dataset: Metalinguistic Tests for Language Models</title>
      <author><first>Tristan</first><last>Thrush</last><affiliation>Stanford University</affiliation></author>
      <author><first>Jared</first><last>Moore</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <author><first>Miguel</first><last>Monares</last></author>
      <author><first>Christopher</first><last>Potts</last><affiliation>Stanford University</affiliation></author>
      <author><first>Douwe</first><last>Kiela</last><affiliation>Stanford University</affiliation></author>
      <pages>8888-8907</pages>
      <abstract>Statements involving metalinguistic self-reference (“This paper has six sections.”) are prevalent in many domains. Can large language models (LLMs) handle such language? In this paper, we present “I am a Strange Dataset”, a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like “The penultimate word in this sentence is” (where a correct continuation is “is”). In verification, models judge the truth of statements like “The penultimate word in this sentence is sentence.” (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60% range, while our untrained human annotators score well in the 89-93% range. The dataset and evaluation toolkit are available at https://github.com/TristanThrush/i-am-a-strange-dataset</abstract>
      <url hash="4799befa">2024.acl-long.482</url>
      <bibkey>thrush-etal-2024-strange</bibkey>
      <doi>10.18653/v1/2024.acl-long.482</doi>
    </paper>
    <paper id="483">
      <title><fixed-case>T</fixed-case>ruth<fixed-case>X</fixed-case>: Alleviating Hallucinations by Editing Large Language Models in Truthful Space</title>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Tian</first><last>Yu</last></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>8908-8949</pages>
      <abstract>Large Language Models (LLMs) sometimes suffer from producing hallucinations, especially LLMs may generate untruthful responses despite knowing the correct knowledge. Activating the truthfulness within LLM is the key to fully unlocking LLM’s knowledge potential. In this paper, we propose TruthX, an inference-time intervention method to activate the truthfulness of LLM by identifying and editing the features within LLM’s internal representations that govern the truthfulness. TruthX employs an auto-encoder to map LLM’s representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM’s internal representations in truthful space, TruthX effectively enhances the truthfulness of LLM. Experiments show that TruthX improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to produce truthful or hallucinatory responses via editing only one vector in LLM’s internal representations.</abstract>
      <url hash="5e2ca9c5">2024.acl-long.483</url>
      <bibkey>zhang-etal-2024-truthx</bibkey>
      <doi>10.18653/v1/2024.acl-long.483</doi>
    </paper>
    <paper id="484">
      <title><fixed-case>P</fixed-case>rot<fixed-case>LLM</fixed-case>: An Interleaved Protein-Language <fixed-case>LLM</fixed-case> with Protein-as-Word Pre-Training</title>
      <author><first>Le</first><last>Zhuo</last></author>
      <author><first>Zewen</first><last>Chi</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Minghao</first><last>Xu</last></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Jianan</first><last>Zhao</last></author>
      <author><first>Heqi</first><last>Zheng</last></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Xian-Ling</first><last>Mao</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Wentao</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>8950-8963</pages>
      <abstract>We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic supervised protein-centric tasks and explore its novel protein-language applications. Experimental results demonstrate that ProtLLM not only achieves superior performance against protein-specialized baselines on protein-centric tasks but also induces zero-shot and in-context learning capabilities on protein-language tasks.</abstract>
      <url hash="fcf7ef53">2024.acl-long.484</url>
      <bibkey>zhuo-etal-2024-protllm</bibkey>
      <doi>10.18653/v1/2024.acl-long.484</doi>
    </paper>
    <paper id="485">
      <title><fixed-case>S</fixed-case>tream<fixed-case>S</fixed-case>peech: Simultaneous Speech-to-Speech Translation with Multi-task Learning</title>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Qingkai</first><last>Fang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shoutao</first><last>Guo</last></author>
      <author><first>Zhengrui</first><last>Ma</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>8964-8986</pages>
      <abstract>Simultaneous speech-to-speech translation (Simul-S2ST, a.k.a streaming speech translation) outputs target speech while receiving streaming speech inputs, which is critical for real-time communication. Beyond accomplishing translation between speech, Simul-S2ST requires a policy to control the model to generate corresponding target speech at the opportune moment within speech inputs, thereby posing a double challenge of translation and policy. In this paper, we propose StreamSpeech, a direct Simul-S2ST model that jointly learns translation and simultaneous policy in a unified framework of multi-task learning. Adhering to a multi-task learning approach, StreamSpeech can perform offline and simultaneous speech recognition, speech translation and speech synthesis via an “All-in-One” seamless model. Experiments on CVSS benchmark demonstrate that StreamSpeech achieves state-of-the-art performance in both offline S2ST and Simul-S2ST tasks. Besides, StreamSpeech is able to present high-quality intermediate results (i.e., ASR or translation results) during simultaneous translation process, offering a more comprehensive real-time communication experience.</abstract>
      <url hash="a6918646">2024.acl-long.485</url>
      <bibkey>zhang-etal-2024-streamspeech</bibkey>
      <doi>10.18653/v1/2024.acl-long.485</doi>
    </paper>
    <paper id="486">
      <title>Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models</title>
      <author><first>Tianjie</first><last>Ju</last></author>
      <author><first>Yijin</first><last>Chen</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Xinwei</first><last>Yuan</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Wei</first><last>Du</last></author>
      <author><first>Yubin</first><last>Zheng</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Gongshen</first><last>Liu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>8987-9001</pages>
      <abstract>Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximately 20% of the failures are attributed to shortcuts, and the initial and terminal entities in these failure instances usually have higher co-occurrences in the pre-training corpus. Finally, we propose erasing shortcut neurons to mitigate the associated risks and find that this approach significantly reduces failures in multiple-hop knowledge editing caused by shortcuts. Code is publicly available at https://github.com/Jometeorie/MultiHopShortcuts.</abstract>
      <url hash="152eff0e">2024.acl-long.486</url>
      <bibkey>ju-etal-2024-investigating</bibkey>
      <doi>10.18653/v1/2024.acl-long.486</doi>
    </paper>
    <paper id="487">
      <title>Why Don’t Prompt-Based Fairness Metrics Correlate?</title>
      <author><first>Abdelrahman</first><last>Zayed</last></author>
      <author><first>Goncalo</first><last>Mordido</last></author>
      <author><first>Ioana</first><last>Baldini</last></author>
      <author><first>Sarath</first><last>Chandar</last><affiliation>Polytechnique Montreal</affiliation></author>
      <pages>9002-9019</pages>
      <abstract>The widespread use of large language models has brought up essential questions about the potential biases these models might learn. This led to the development of several metrics aimed at evaluating and mitigating these biases. In this paper, we first demonstrate that prompt-based fairness metrics exhibit poor agreement, as measured by correlation, raising important questions about the reliability of fairness assessment using prompts. Then, we outline six relevant reasons why such a low correlation is observed across existing metrics. Based on these insights, we propose a method called Correlated Fairness Output (CAIRO) to enhance the correlation between fairness metrics. CAIRO augments the original prompts of a given fairness metric by using several pre-trained language models and then selects the combination of the augmented prompts that achieves the highest correlation across metrics. We show a significant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and 0.98 across metrics for gender and religion biases, respectively. Our code is available at https://github.com/chandar-lab/CAIRO.</abstract>
      <url hash="da86bfdc">2024.acl-long.487</url>
      <bibkey>zayed-etal-2024-dont</bibkey>
      <doi>10.18653/v1/2024.acl-long.487</doi>
    </paper>
    <paper id="488">
      <title><fixed-case>N</fixed-case>aija<fixed-case>H</fixed-case>ate: Evaluating Hate Speech Detection on <fixed-case>N</fixed-case>igerian <fixed-case>T</fixed-case>witter Using Representative Data</title>
      <author><first>Manuel</first><last>Tonneau</last><affiliation>Oxford Internet Institute, University of Oxford</affiliation></author>
      <author><first>Pedro</first><last>Quinta De Castro</last></author>
      <author><first>Karim</first><last>Lasri</last><affiliation>The World Bank</affiliation></author>
      <author><first>Ibrahim</first><last>Farouq</last></author>
      <author><first>Lakshmi</first><last>Subramanian</last><affiliation>New York University</affiliation></author>
      <author><first>Victor</first><last>Orozco-Olvera</last></author>
      <author><first>Samuel</first><last>Fraiberger</last><affiliation>World Bank</affiliation></author>
      <pages>9020-9040</pages>
      <abstract>To address the global issue of online hate, hate speech detection (HSD) systems are typically developed on datasets from the United States, thereby failing to generalize to English dialects from the Majority World. Furthermore, HSD models are often evaluated on non-representative samples, raising concerns about overestimating model performance in real-world settings. In this work, we introduce NaijaHate, the first dataset annotated for HSD which contains a representative sample of Nigerian tweets. We demonstrate that HSD evaluated on biased datasets traditionally used in the literature consistently overestimates real-world performance by at least two-fold. We then propose NaijaXLM-T, a pretrained model tailored to the Nigerian Twitter context, and establish the key role played by domain-adaptive pretraining and finetuning in maximizing HSD performance. Finally, owing to the modest performance of HSD systems in real-world conditions, we find that content moderators would need to review about ten thousand Nigerian tweets flagged as hateful daily to moderate 60% of all hateful content, highlighting the challenges of moderating hate speech at scale as social media usage continues to grow globally. Taken together, these results pave the way towards robust HSD systems and a better protection of social media users from hateful content in low-resource settings.</abstract>
      <url hash="e234902f">2024.acl-long.488</url>
      <bibkey>tonneau-etal-2024-naijahate</bibkey>
      <doi>10.18653/v1/2024.acl-long.488</doi>
      <revision id="1" href="2024.acl-long.488v1" hash="d84cccc4"/>
      <revision id="2" href="2024.acl-long.488v2" hash="e234902f" date="2024-10-25">Minor updates.</revision>
    </paper>
    <paper id="489">
      <title><fixed-case>M</fixed-case><tex-math>^3</tex-math><fixed-case>AV</fixed-case>: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset</title>
      <author><first>Zhe</first><last>Chen</last></author>
      <author><first>Heyang</first><last>Liu</last></author>
      <author><first>Wenyi</first><last>Yu</last></author>
      <author><first>Guangzhi</first><last>Sun</last></author>
      <author><first>Hongcheng</first><last>Liu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Ji</first><last>Wu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Tsinghua University and University College London</affiliation></author>
      <author><first>Yu</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yanfeng</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>9041-9060</pages>
      <abstract>Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M<tex-math>^3</tex-math>AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the slide text and spoken words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recognition and understanding tasks. Evaluations performed on contextual speech recognition, speech synthesis, and slide and script generation tasks demonstrate that the diversity of M<tex-math>^3</tex-math>AV makes it a challenging dataset.</abstract>
      <url hash="41d859d0">2024.acl-long.489</url>
      <bibkey>chen-etal-2024-m3av</bibkey>
      <doi>10.18653/v1/2024.acl-long.489</doi>
    </paper>
    <paper id="490">
      <title>Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination</title>
      <author><first>Nakyeong</first><last>Yang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Taegwan</first><last>Kang</last></author>
      <author><first>Stanley Jungkyu</first><last>Choi</last><affiliation>Language Lab, LG AI Research</affiliation></author>
      <author><first>Honglak</first><last>Lee</last><affiliation>University of Michigan - Ann Arbor and LG AI Research</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>9061-9073</pages>
      <abstract>Instruction-following language models often show undesirable biases. These undesirable biases may be accelerated in the real-world usage of language models, where a wide range of instructions is used through zero-shot example prompting. To solve this problem, we first define the bias neuron, which significantly affects biased outputs, and prove its existence empirically. Furthermore, we propose a novel and practical bias mitigation method, CRISPR, to eliminate bias neurons of language models in instruction-following settings. CRISPR automatically determines biased outputs and categorizes neurons that affect the biased outputs as bias neurons using an explainability method. Experimental results demonstrate the effectiveness of our method in mitigating biases under zero-shot instruction-following settings without losing the model’s task performance and existing knowledge. The experimental results reveal the generalizability of our method as it shows robustness under various instructions and datasets. Surprisingly, our method can mitigate the bias in language models by eliminating only a few neurons (at least three).</abstract>
      <url hash="509207eb">2024.acl-long.490</url>
      <bibkey>yang-etal-2024-mitigating</bibkey>
      <doi>10.18653/v1/2024.acl-long.490</doi>
    </paper>
    <paper id="491">
      <title>Domain Adaptation for Subjective Induction Questions Answering on Products by Adversarial Disentangled Learning</title>
      <author><first>Yufeng</first><last>Zhang</last></author>
      <author><first>Jianxing</first><last>Yu</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Yanghui</first><last>Rao</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Libin</first><last>Zheng</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Qinliang</first><last>Su</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Huaijie</first><last>Zhu</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Jian</first><last>Yin</last></author>
      <pages>9074-9089</pages>
      <abstract>This paper focuses on answering subjective questions about products. Different from the factoid question with a single answer span, this subjective one involves multiple viewpoints. For example, the question of ‘how the phone’s battery is?’ not only involves facts of battery capacity but also contains users’ opinions on the battery’s pros and cons. A good answer should be able to integrate these heterogeneous and even inconsistent viewpoints, which is formalized as a subjective induction QA task. For this task, the data distributions are often imbalanced across different product domains. It is hard for traditional methods to work well without considering the shift of domain patterns. To address this problem, we propose a novel domain-adaptive model. Concretely, for each sample in the source and target domain, we first retrieve answer-related knowledge and represent them independently. To facilitate knowledge transferring, we then disentangle the representations into domain-invariant and domain-specific latent factors. Moreover, we develop an adversarial discriminator with contrastive learning to reduce the impact of out-of-domain bias. Based on learned latent vectors in a target domain, we yield multi-perspective summaries as inductive answers. Experiments on popular datasets show the effectiveness of our method.</abstract>
      <url hash="eb46ca78">2024.acl-long.491</url>
      <bibkey>zhang-etal-2024-domain</bibkey>
      <doi>10.18653/v1/2024.acl-long.491</doi>
    </paper>
    <paper id="492">
      <title>Revisiting Demonstration Selection Strategies in In-Context Learning</title>
      <author><first>Keqin</first><last>Peng</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Yancheng</first><last>Yuan</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technolgy, Shenzhen</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Yuanxin</first><last>Ouyang</last><affiliation>BEIHANG UNIVERSITY</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>University of Sydney</affiliation></author>
      <pages>9090-9101</pages>
      <abstract>Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model. However, the performance of ICL varies significantly with the choice of demonstrations, and previous research usually focuses on the data aspect ignoring the model’s effect. In this work, we first revisit the factors contributing to this variance from the model aspect, and find that the demonstration choice is both data- and model-dependent. We further propose a conjecture that the performance of a demonstration positively correlates with its contribution to the model’s understanding of the test samples, and accordingly propose a data- and model-dependent demonstration selection method, TopK + ConE. Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales. Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods. Code is publicly available at https://github.com/Romainpkq/revisit_demon_selection_in_ICL.</abstract>
      <url hash="5f538804">2024.acl-long.492</url>
      <bibkey>peng-etal-2024-revisiting</bibkey>
      <doi>10.18653/v1/2024.acl-long.492</doi>
    </paper>
    <paper id="493">
      <title>Multimodal Table Understanding</title>
      <author><first>Mingyu</first><last>Zheng</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Xinwei</first><last>Feng</last><affiliation>Baidu</affiliation></author>
      <author><first>Qingyi</first><last>Si</last></author>
      <author><first>Qiaoqiao</first><last>She</last></author>
      <author><first>Zheng</first><last>Lin</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wenbin</first><last>Jiang</last><affiliation>Beijing Normal University</affiliation></author>
      <author><first>Weiping</first><last>Wang</last></author>
      <pages>9102-9124</pages>
      <abstract>Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications. In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various table-related requests based on the given table image. To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks. On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings.</abstract>
      <url hash="b924c90e">2024.acl-long.493</url>
      <bibkey>zheng-etal-2024-multimodal</bibkey>
      <doi>10.18653/v1/2024.acl-long.493</doi>
    </paper>
    <paper id="494">
      <title>Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding</title>
      <author><first>Huang</first><last>Lei</last></author>
      <author><first>Jiaming</first><last>Guo</last></author>
      <author><first>Guanhua</first><last>He</last></author>
      <author><first>Xishan</first><last>Zhang</last><affiliation>, Chinese Academy of Sciences and , Cambricon Techonologies</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Institute of Computing Technology, CAS</affiliation></author>
      <author><first>Shaohui</first><last>Peng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shaoli</first><last>Liu</last><affiliation>,Cambricon Technologies</affiliation></author>
      <author><first>Tianshi</first><last>Chen</last><affiliation>, Chinese Academy of Sciences and , Cambricon Technologies</affiliation></author>
      <pages>9125-9146</pages>
      <abstract>Generating long-term texts such as novels using artificial intelligence has always been a challenge. A common approach is to use large language models (LLMs) to construct a hierarchical framework that first plans and then writes. Despite the fact that the generated novels reach a sufficient length, they exhibit poor logical coherence and appeal in their plots and deficiencies in character and event depiction, ultimately compromising the overall narrative quality. In this paper, we propose a method named Extracting Excelsior and Expanding. Ex3 initially extract structural information by learning from raw novel data. By combining this structure information with the novel data, an instruction-following dataset is meticulously crafted. This dataset is then utilized to fine-tune the LLM, aiming for excelsior generation performance. In the final stage, a tree-like expansion method is deployed to facilitate the generation of arbitrarily long novels.Evaluation against previous methods showcases Ex3’s ability to produce higher-quality long-form novels.</abstract>
      <url hash="679c1b9f">2024.acl-long.494</url>
      <bibkey>lei-etal-2024-ex3</bibkey>
      <doi>10.18653/v1/2024.acl-long.494</doi>
    </paper>
    <paper id="495">
      <title>Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning</title>
      <author><first>Mayur</first><last>Patidar</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Riya</first><last>Sawhney</last></author>
      <author><first>Avinash</first><last>Singh</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Biswajit</first><last>Chatterjee</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Mausam</first><last>.</last><affiliation>Indian Institute of Technology Delhi</affiliation></author>
      <author><first>Indrajit</first><last>Bhattacharya</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <pages>9147-9165</pages>
      <abstract>Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms, which are further refined using execution-guided feedback. Experiments over four source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments in the in-domain setting show that FuSIC-KBQA also outperforms SoTA KBQA models when training data is limited.</abstract>
      <url hash="bee2784e">2024.acl-long.495</url>
      <bibkey>patidar-etal-2024-shot</bibkey>
      <doi>10.18653/v1/2024.acl-long.495</doi>
    </paper>
    <paper id="496">
      <title><fixed-case>W</fixed-case>at<fixed-case>ME</fixed-case>: Towards Lossless Watermarking Through Lexical Redundancy</title>
      <author><first>Liang</first><last>Chen</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yatao</first><last>Bian</last></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Deng</first><last>Cai</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Shuaiyi</first><last>Li</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Peilin</first><last>Zhao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>9166-9180</pages>
      <abstract>Text watermarking has emerged as a pivotal technique for identifying machine-generated text. However, existing methods often rely on arbitrary vocabulary partitioning during decoding to embed watermarks, which compromises the availability of suitable tokens and significantly degrades the quality of responses. This study assesses the impact of watermarking on different capabilities of large language models (LLMs) from a cognitive science lens. Our finding highlights a significant disparity; knowledge recall and logical reasoning are more adversely affected than language generation. These results suggest a more profound effect of watermarking on LLMs than previously understood. To address these challenges, we introduce Watermarking with Mutual Exclusion (WatME), a novel approach leveraging linguistic prior knowledge of inherent lexical redundancy in LLM vocabularies to seamlessly integrate watermarks. Specifically, WatME dynamically optimizes token usage during the decoding process by applying a mutually exclusive rule to the identified lexical redundancies. This strategy effectively prevents the unavailability of appropriate tokens and preserves the expressive power of LLMs. We provide both theoretical analysis and empirical evidence showing that WatME effectively preserves the diverse capabilities of LLMs while ensuring watermark detectability.</abstract>
      <url hash="a169dcc5">2024.acl-long.496</url>
      <bibkey>chen-etal-2024-watme</bibkey>
      <doi>10.18653/v1/2024.acl-long.496</doi>
    </paper>
    <paper id="497">
      <title>Text-like Encoding of Collaborative Information in Large Language Models for Recommendation</title>
      <author><first>Yang</first><last>Zhang</last></author>
      <author><first>Keqin</first><last>Bao</last></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xiangnan</first><last>He</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>9181-9191</pages>
      <abstract>When adapting Large Language Models for Recommendation (LLMRec), it is crucial to integrate collaborative information. Existing methods achieve this by learning collaborative embeddings in LLMs’ latent space from scratch or by mapping from external models. However, they fail to represent the information in a text-like format, which may not align optimally with LLMs. To bridge this gap, we introduce BinLLM, a novel LLMRec method that seamlessly integrates collaborative information through text-like encoding. BinLLM converts collaborative embeddings from external models into binary sequences — a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs. Additionally, BinLLM provides options to compress the binary sequence using dot-decimal notation to avoid excessively long lengths. Extensive experiments validate that BinLLM introduces collaborative information in a manner better aligned with LLMs, resulting in enhanced performance. We release our code at https://github.com/zyang1580/BinLLM.</abstract>
      <url hash="60129b6d">2024.acl-long.497</url>
      <bibkey>zhang-etal-2024-text</bibkey>
      <doi>10.18653/v1/2024.acl-long.497</doi>
    </paper>
    <paper id="498">
      <title><fixed-case>MM</fixed-case>-<fixed-case>SAP</fixed-case>: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception</title>
      <author><first>Yuhao</first><last>Wang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yusheng</first><last>Liao</last></author>
      <author><first>Heyang</first><last>Liu</last></author>
      <author><first>Hongcheng</first><last>Liu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yanfeng</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yu</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>9192-9205</pages>
      <abstract>Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models’ struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been overlooked in prior studies. In this paper, we aim to define and evaluate the self-awareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of popular MLLMs, offering a comprehensive analysis of their self-awareness and providing detailed insights. The experiment results reveal that current MLLMs possess limited self-awareness capabilities, pointing to a crucial area for future advancement in the development of trustworthy MLLMs. Code and data are available at https://github.com/YHWmz/MM-SAP.</abstract>
      <url hash="46bfd84e">2024.acl-long.498</url>
      <bibkey>wang-etal-2024-mm</bibkey>
      <doi>10.18653/v1/2024.acl-long.498</doi>
    </paper>
    <paper id="499">
      <title>Focus on Your Question! Interpreting and Mitigating Toxic <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> Problems in Commonsense Reasoning</title>
      <author><first>Jiachun</first><last>Li</last></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chenhao</first><last>Wang</last></author>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Daojian</first><last>Zeng</last><affiliation>Hunan Normal University</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>9206-9230</pages>
      <abstract>Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by <tex-math>\textbf{23.6}</tex-math>%), but also effectively improves the model’s overall commonsense reasoning performance (increased by <tex-math>\textbf{5.5}</tex-math>%).</abstract>
      <url hash="c1377409">2024.acl-long.499</url>
      <bibkey>li-etal-2024-focus</bibkey>
      <doi>10.18653/v1/2024.acl-long.499</doi>
    </paper>
    <paper id="500">
      <title>Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation</title>
      <author><first>Yi</first><last>Liu</last><affiliation>nanjing university</affiliation></author>
      <author><first>Xiangyu</first><last>Liu</last><affiliation>nanjing university</affiliation></author>
      <author><first>Xiangrong</first><last>Zhu</last></author>
      <author><first>Wei</first><last>Hu</last><affiliation>Nanjing University</affiliation></author>
      <pages>9231-9253</pages>
      <abstract>Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects (e.g., “positive” from sentiment and “sport” from topic). Existing works neglect attribute correlations formed by the intertwining of different attributes. Particularly, the stereotype formed by imbalanced attribute correlations significantly affects multi-aspect control. In this paper, we propose MAGIC, a new multi-aspect controllable text generation method with disentangled counterfactual augmentation. We alleviate the issue of imbalanced attribute correlations during training using counterfactual feature vectors in the attribute latent space by disentanglement. During inference, we enhance attribute correlations by target-guided counterfactual augmentation to further improve multi-aspect control. Experiments show that MAGIC outperforms state-of-the-art baselines in both imbalanced and balanced attribute correlation scenarios.</abstract>
      <url hash="698c96df">2024.acl-long.500</url>
      <bibkey>liu-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.acl-long.500</doi>
    </paper>
    <paper id="501">
      <title>Reward-based Input Construction for Cross-document Relation Extraction</title>
      <author><first>Byeonghu</first><last>Na</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Suhyeon</first><last>Jo</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Yeongmin</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Il-chul</first><last>Moon</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>9254-9270</pages>
      <abstract>Relation extraction (RE) is a fundamental task in natural language processing, aiming to identify relations between target entities in text. While many RE methods are designed for a single sentence or document, cross-document RE has emerged to address relations across multiple long documents. Given the nature of long documents in cross-document RE, extracting document embeddings is challenging due to the length constraints of pre-trained language models. Therefore, we propose REward-based Input Construction (REIC), the first learning-based sentence selector for cross-document RE. REIC extracts sentences based on relational evidence, enabling the RE module to effectively infer relations. Since supervision of evidence sentences is generally unavailable, we train REIC using reinforcement learning with RE prediction scores as rewards. Experimental results demonstrate the superiority of our method over heuristic methods for different RE structures and backbones in cross-document RE. Our code is publicly available at https://github.com/aailabkaist/REIC.</abstract>
      <url hash="c7fa11af">2024.acl-long.501</url>
      <bibkey>na-etal-2024-reward</bibkey>
      <doi>10.18653/v1/2024.acl-long.501</doi>
    </paper>
    <paper id="502">
      <title>Hyperspherical Multi-Prototype with Optimal Transport for Event Argument Extraction</title>
      <author><first>Guangjun</first><last>Zhang</last></author>
      <author><first>Hu</first><last>Zhang</last><affiliation>Shanxi University</affiliation></author>
      <author><first>YuJie</first><last>Wang</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Ru</first><last>Li</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Hongye</first><last>Tan</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Jiye</first><last>Liang</last><affiliation>Shanxi University</affiliation></author>
      <pages>9271-9284</pages>
      <abstract>Event Argument Extraction (EAE) aims to extract arguments for specified events from a text. Previous research has mainly focused on addressing long-distance dependencies of arguments, modeling co-occurrence relationships between roles and events, but overlooking potential inductive biases: (i) semantic differences among arguments of the same type and (ii) large margin separation between arguments of the different types. Inspired by prototype networks, we introduce a new model named HMPEAE, which takes the two inductive biases above as targets to locate prototypes and guide the model to learn argument representations based on these prototypes.Specifically, we set multiple prototypes to represent each role to capture intra-class differences. Simultaneously, we use hypersphere as the output space for prototypes, defining large margin separation between prototypes to encourage the model to learn significant differences between different types of arguments effectively.We solve the “argument-prototype” assignment as an optimal transport problem to optimize the argument representation and minimize the absolute distance between arguments and prototypes to achieve compactness within sub-clusters. Experimental results on the RAMS and WikiEvents datasets show that HMPEAE achieves state-of-the-art performances.</abstract>
      <url hash="f8399670">2024.acl-long.502</url>
      <bibkey>zhang-etal-2024-hyperspherical</bibkey>
      <doi>10.18653/v1/2024.acl-long.502</doi>
    </paper>
    <paper id="503">
      <title>Understanding Retrieval Robustness for Retrieval-augmented Image Captioning</title>
      <author><first>Wenyan</first><last>Li</last></author>
      <author><first>Jiaang</first><last>Li</last></author>
      <author><first>Rita</first><last>Ramos</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Raphael</first><last>Tang</last><affiliation>Comcast</affiliation></author>
      <author><first>Desmond</first><last>Elliott</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>9285-9299</pages>
      <abstract>Recent advances in retrieval-augmented models for image captioning highlight the benefit of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities. While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice: the retrieved information can sometimes mislead the model, resulting in incorrect generation and worse performance. In this paper, we analyze the robustness of a retrieval-augmented captioning model SmallCap. Our analysis shows that the model is sensitive to tokens that appear in the majority of the retrieved captions, and the input attribution shows that those tokens are likely copied into the generated output. Given these findings, we propose to train the model by sampling retrieved captions from more diverse sets. This decreases the chance that the model learns to copy majority tokens, and improves both in-domain and cross-domain performance.</abstract>
      <url hash="3bc4f91b">2024.acl-long.503</url>
      <bibkey>li-etal-2024-understanding-retrieval</bibkey>
      <doi>10.18653/v1/2024.acl-long.503</doi>
    </paper>
    <paper id="504">
      <title>Semi-Supervised Spoken Language Glossification</title>
      <author><first>Huijie</first><last>Yao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Wengang</first><last>Zhou</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Hao</first><last>Zhou</last><affiliation>Baidu</affiliation></author>
      <author><first>Houqiang</first><last>Li</last></author>
      <pages>9300-9312</pages>
      <abstract>Spoken language glossification (SLG) aims to translate the spoken language text into the sign language gloss, i.e., a written record of sign language. In this work, we present a framework named <tex-math>S</tex-math>emi-<tex-math>S</tex-math>upervised <tex-math>S</tex-math>poken <tex-math>L</tex-math>anguage <tex-math>G</tex-math>lossification (<tex-math>S^3</tex-math>LG) for SLG. To tackle the bottleneck of limited parallel data in SLG, our <tex-math>S^3</tex-math>LG incorporates large-scale monolingual spoken language text into SLG training. The proposed framework follows the self-training structure that iteratively annotates and learns from pseudo labels. Considering the lexical similarity and syntactic difference between sign language and spoken language, our <tex-math>S^3</tex-math>LG adopts both the rule-based heuristic and model-based approach for auto-annotation. During training, we randomly mix these complementary synthetic datasets and mark their differences with a special token. As the synthetic data may be less quality, the <tex-math>S^3</tex-math>LG further leverages consistency regularization to reduce the negative impact of noise in the synthetic data. Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the <tex-math>S^3</tex-math>LG. Our code is available at <url>https://github.com/yaohj11/S3LG</url>.</abstract>
      <url hash="636c9a37">2024.acl-long.504</url>
      <bibkey>yao-etal-2024-semi</bibkey>
      <doi>10.18653/v1/2024.acl-long.504</doi>
    </paper>
    <paper id="505">
      <title><fixed-case>S</fixed-case>ee<fixed-case>C</fixed-case>lick: Harnessing <fixed-case>GUI</fixed-case> Grounding for Advanced Visual <fixed-case>GUI</fixed-case> Agents</title>
      <author><first>Kanzhi</first><last>Cheng</last><affiliation>nanjing university</affiliation></author>
      <author><first>Qiushi</first><last>Sun</last><affiliation>University of Hong Kong and Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yougang</first><last>Chu</last></author>
      <author><first>Fangzhi</first><last>Xu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Li</first><last>YanTao</last></author>
      <author><first>Jianbing</first><last>Zhang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zhiyong</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>9313-9332</pages>
      <abstract>Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a novel visual GUI agent – SeeClick, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding – the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance SeeClick with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created ScreenSpot, the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvement in ScreenSpot over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks. The model, data and code will be open-sourced.</abstract>
      <url hash="2237e771">2024.acl-long.505</url>
      <bibkey>cheng-etal-2024-seeclick</bibkey>
      <doi>10.18653/v1/2024.acl-long.505</doi>
    </paper>
    <paper id="506">
      <title><fixed-case>I</fixed-case>nterrogate<fixed-case>LLM</fixed-case>: Zero-Resource Hallucination Detection in <fixed-case>LLM</fixed-case>-Generated Answers</title>
      <author><first>Yakir</first><last>Yehuda</last></author>
      <author><first>Itzik</first><last>Malkiel</last></author>
      <author><first>Oren</first><last>Barkan</last><affiliation>Open University of Israel</affiliation></author>
      <author><first>Jonathan</first><last>Weill</last><affiliation>Microsoft and Tel Aviv University</affiliation></author>
      <author><first>Royi</first><last>Ronen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Noam</first><last>Koenigstein</last><affiliation>Tel Aviv University</affiliation></author>
      <pages>9333-9347</pages>
      <abstract>Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 87% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy of 81%, all without relying on external knowledge.</abstract>
      <url hash="92f3b058">2024.acl-long.506</url>
      <bibkey>yehuda-etal-2024-interrogatellm</bibkey>
      <doi>10.18653/v1/2024.acl-long.506</doi>
    </paper>
    <paper id="507">
      <title><fixed-case>F</fixed-case>-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods</title>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Keyuchen</first><last>Keyuchen</last></author>
      <author><first>Shujie</first><last>Wang</last></author>
      <author><first>Peiji</first><last>Li</last></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Hang</first><last>Yan</last><affiliation>AI lab</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>9348-9369</pages>
      <abstract>Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced LLMs. Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators. Additionally, we discuss the influence of different model sizes, dimensions, and normalization methods. We anticipate that F-Eval will facilitate the study of LLMs’ fundamental abilities.</abstract>
      <url hash="5498e69b">2024.acl-long.507</url>
      <bibkey>sun-etal-2024-f</bibkey>
      <doi>10.18653/v1/2024.acl-long.507</doi>
    </paper>
    <paper id="508">
      <title>Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning</title>
      <author><first>Philipp</first><last>Mondorf</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München and IT University of Copenhagen</affiliation></author>
      <pages>9370-9402</pages>
      <abstract>Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like <tex-math>\textit{supposition following}</tex-math> or <tex-math>\textit{chain construction}</tex-math>. Moreover, our research demonstrates that the architecture and scale of the model significantly affect its preferred method of reasoning, with more advanced models tending to adopt strategies more frequently than less sophisticated ones. Importantly, we assert that a model’s accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its reasoning process. This distinction underscores the necessity for more nuanced evaluation procedures in the field.</abstract>
      <url hash="6b9471f1">2024.acl-long.508</url>
      <bibkey>mondorf-plank-2024-comparing</bibkey>
      <doi>10.18653/v1/2024.acl-long.508</doi>
    </paper>
    <paper id="509">
      <title>Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of <fixed-case>AI</fixed-case> Utilizing Human Feedback</title>
      <author><first>Maria</first><last>Lerner</last></author>
      <author><first>Florian</first><last>Dorner</last><affiliation>Max Planck Institute for Intelligent Systems, Max-Planck Institute and ETHZ - ETH Zurich</affiliation></author>
      <author><first>Elliott</first><last>Ash</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Naman</first><last>Goel</last><affiliation>University of Oxford</affiliation></author>
      <pages>9403-9425</pages>
      <abstract>There is a growing body of work on learning from human feedback to align various aspects of machine learning systems with human values and preferences. We consider the setting of fairness in content moderation, in which human feedback is used to determine how two comments — referencing different sensitive attribute groups — should be treated in comparison to one another. With a novel dataset collected from Prolific and MTurk, we find significant gaps in fairness preferences depending on the race, age, political stance, educational level, and LGBTQ+ identity of annotators. We also demonstrate that demographics mentioned in text have a strong influence on how users perceive individual fairness in moderation. Further, we find that differences also exist in downstream classifiers trained to predict human preferences. Finally, we observe that an ensemble, giving equal weight to classifiers trained on annotations from different demographics, performs better for different demographic intersections; compared to a single classifier that gives equal weight to each annotation.</abstract>
      <url hash="dac19ee0">2024.acl-long.509</url>
      <bibkey>lerner-etal-2024-whose</bibkey>
      <doi>10.18653/v1/2024.acl-long.509</doi>
    </paper>
    <paper id="510">
      <title>Math-Shepherd: Verify and Reinforce <fixed-case>LLM</fixed-case>s Step-by-step without Human Annotations</title>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Lei</first><last>Li</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Zhihong</first><last>Shao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Runxin</first><last>Xu</last></author>
      <author><first>Damai</first><last>Dai</last></author>
      <author><first>Yifei</first><last>Li</last></author>
      <author><first>Deli</first><last>Chen</last><affiliation>DeepSeek AI</affiliation></author>
      <author><first>Yu</first><last>Wu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Zhifang</first><last>Sui</last><affiliation>Peking University</affiliation></author>
      <pages>9426-9439</pages>
      <abstract>In this paper, we present an innovative process-oriented math process reward model called Math-shepherd, which assigns a reward score to each step of math problem solutions. The training of Math-shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-shepherd in two scenarios: 1) <tex-math>\textit{Verification}</tex-math>: Math-shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) <tex-math>\textit{Reinforcement Learning (RL)}</tex-math>: Math-shepherd is employed to reinforce LLMs.With Math-shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, process RL with Math-shepherd significantly enhances Mistral-7B (77.9%<tex-math>\to</tex-math>84.1% on GSM8K and 28.6%<tex-math>\to</tex-math>33.0% on MATH).The accuracy can be further improved to 89.1% and 43.5% on two benchmarks with verification of Math-shepherd.We believe that automatic process supervision holds significant potential for the future evolution of LLMs.</abstract>
      <url hash="3d4ee222">2024.acl-long.510</url>
      <bibkey>wang-etal-2024-math</bibkey>
      <doi>10.18653/v1/2024.acl-long.510</doi>
    </paper>
    <paper id="511">
      <title>Large Language Models are not Fair Evaluators</title>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Lei</first><last>Li</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Liang</first><last>Chen</last></author>
      <author><first>Zefan</first><last>Cai</last></author>
      <author><first>Dawei</first><last>Zhu</last></author>
      <author><first>Binghuai</first><last>Lin</last><affiliation>Tencent</affiliation></author>
      <author><first>Yunbo</first><last>Cao</last><affiliation>Tencent</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Zhifang</first><last>Sui</last><affiliation>Peking University</affiliation></author>
      <pages>9440-9450</pages>
      <abstract>In this paper, we uncover a positional bias in the evaluation paradigm of adopting large language models (LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. We propose a simple yet effective calibration framework to address our discovered positional bias.To evaluate the effectiveness of our framework, we manually annotate the “win/tie/lose” outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark’s question prompt. Extensive experiments demonstrate that our approach successfully alleviates evaluation bias, resulting in closer alignment with human judgments.</abstract>
      <url hash="6ff7b8cb">2024.acl-long.511</url>
      <bibkey>wang-etal-2024-large-language-models-fair</bibkey>
      <doi>10.18653/v1/2024.acl-long.511</doi>
    </paper>
    <paper id="512">
      <title>Improving Large Language Models in Event Relation Logical Prediction</title>
      <author><first>Meiqi</first><last>Chen</last><affiliation>Peking University</affiliation></author>
      <author><first>Yubo</first><last>Ma</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Kaitao</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Dongsheng</first><last>Li</last></author>
      <pages>9451-9478</pages>
      <abstract>Event relations are crucial for narrative understanding and reasoning. Governed by nuanced logic, event relation extraction (ERE) is a challenging task that demands thorough semantic understanding and rigorous logical reasoning. In this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in understanding and applying event relation logic. More in detail, we first investigate the deficiencies of LLMs in logical reasoning across different tasks. Our study reveals that LLMs are not logically consistent reasoners, which results in their suboptimal performance on tasks that need rigorous reasoning. To address this, we explore three different approaches to endow LLMs with event relation logic, and thus enable them to generate more coherent answers across various scenarios. Based on our approach, we also contribute a synthesized dataset (LLM-ERL) involving high-order reasoning for evaluation and fine-tuning. Extensive quantitative and qualitative analyses on different tasks also validate the effectiveness of our approach and provide insights for solving practical tasks with LLMs in future work. Codes are available at https://github.com/chenmeiqii/Teach-LLM-LR.</abstract>
      <url hash="9abc82c2">2024.acl-long.512</url>
      <bibkey>chen-etal-2024-improving-large</bibkey>
      <doi>10.18653/v1/2024.acl-long.512</doi>
    </paper>
    <paper id="513">
      <title>Synchronized Video Storytelling: Generating Video Narrations with Structured Storyline</title>
      <author><first>Dingyi</first><last>Yang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Chunru</first><last>Zhan</last></author>
      <author><first>Ziheng</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Biao</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Tiezheng</first><last>Ge</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Qin</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <pages>9479-9493</pages>
      <abstract>Video storytelling is engaging multimedia content that utilizes video and its accompanying narration to share a story and attract the audience, where a key challenge is creating narrations for recorded visual scenes. Previous studies on dense video captioning and video story generation have made some progress. However, in practical applications, we typically require synchronized narrations for ongoing visual scenes. In this work, we introduce a new task of Synchronized Video Storytelling, which aims to generate synchronous and informative narrations for videos. These narrations, associated with each video clip, should relate to the visual content, integrate relevant knowledge, and have an appropriate word count corresponding to the clip’s duration. Specifically, a structured storyline is beneficial to guide the generation process, ensuring coherence and integrity. To support the exploration of this task, we introduce a new benchmark dataset E-SyncVidStory with rich annotations. Since existing Multimodal LLMs are not effective in addressing this task in one-shot or few-shot settings, we propose a framework named VideoNarrator that can generate a storyline for input videos and simultaneously generate narrations with the guidance of the generated or predefined storyline. We further introduce a set of evaluation metrics to thoroughly assess the generation. Both automatic and human evaluations validate the effectiveness of our approach. Our dataset, codes, and evaluations will be released.</abstract>
      <url hash="a9f97d79">2024.acl-long.513</url>
      <bibkey>yang-etal-2024-synchronized</bibkey>
      <doi>10.18653/v1/2024.acl-long.513</doi>
    </paper>
    <paper id="514">
      <title>Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation</title>
      <author><first>Wenting</first><last>Chen</last></author>
      <author><first>Linlin</first><last>Shen</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Jingyang</first><last>Lin</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Jiebo</first><last>Luo</last><affiliation>University of Rochester and University of Rochester</affiliation></author>
      <author><first>Xiang</first><last>Li</last><affiliation>Massachusetts General Hospital, Harvard University</affiliation></author>
      <author><first>Yixuan</first><last>Yuan</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>9494-9509</pages>
      <abstract>Fine-grained vision-language models (VLM) have been widely used for inter-modality local alignment between the predefined fixed patches and textual words. However, in medical analysis, lesions exhibit varying sizes and positions, and using fixed patches may cause incomplete representations of lesions. Moreover, these methods provide explainability by using heatmaps to show the general image areas potentially associated with texts rather than specific regions, making their explanations not explicit and specific enough. To address these issues, we propose a novel Adaptive patch-word Matching (AdaMatch) model to correlate chest X-ray (CXR) image regions with words in medical reports and apply it to CXR-report generation to provide explainability for the generation process. AdaMatch exploits the fine-grained relation between adaptive patches and words to provide explanations of specific image regions with corresponding words. To capture the abnormal regions of varying sizes and positions, we introduce an Adaptive Patch extraction (AdaPatch) module to acquire adaptive patches for these regions adaptively. Aiming to provide explicit explainability for the CXR-report generation task, we propose an AdaMatch-based bidirectional LLM for Cyclic CXR-report generation (AdaMatch-Cyclic). It employs AdaMatch to obtain the keywords for CXR images and ‘keypatches’ for medical reports as hints to guide CXR-report generation. Extensive experiments on two publicly available CXR datasets validate the effectiveness of our method and its superior performance over existing methods. Source code will be released.</abstract>
      <url hash="55870dd3">2024.acl-long.514</url>
      <bibkey>chen-etal-2024-fine</bibkey>
      <doi>10.18653/v1/2024.acl-long.514</doi>
    </paper>
    <paper id="515">
      <title><fixed-case>T</fixed-case>-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step</title>
      <author><first>Zehui</first><last>Chen</last></author>
      <author><first>Weihua</first><last>Du</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Wenwei</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Kuikun</first><last>Liu</last></author>
      <author><first>Jiangning</first><last>Liu</last></author>
      <author><first>Miao</first><last>Zheng</last><affiliation>sensetime</affiliation></author>
      <author><first>Jingming</first><last>Zhuo</last><affiliation>Shanghai AI Lab and Jilin University</affiliation></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Feng</first><last>Zhao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>9510-9529</pages>
      <abstract>Large language models (LLMs) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce T-Eval to evaluate the tool-utilization capability step by step. T-Eval disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of various LLMs. T-Eval not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, providing a new perspective in LLM evaluation on tool-utilization ability. The benchmark will be available.</abstract>
      <url hash="dc42e8e9">2024.acl-long.515</url>
      <bibkey>chen-etal-2024-eval</bibkey>
      <doi>10.18653/v1/2024.acl-long.515</doi>
    </paper>
    <paper id="516">
      <title>Are <fixed-case>LLM</fixed-case>-based Evaluators Confusing <fixed-case>NLG</fixed-case> Quality Criteria?</title>
      <author><first>Xinyu</first><last>Hu</last><affiliation>Peking University</affiliation></author>
      <author><first>Mingqi</first><last>Gao</last><affiliation>Peking University</affiliation></author>
      <author><first>Sen</first><last>Hu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yang</first><last>Zhang</last></author>
      <author><first>Yicheng</first><last>Chen</last></author>
      <author><first>Teng</first><last>Xu</last></author>
      <author><first>Xiaojun</first><last>Wan</last><affiliation>Peking University</affiliation></author>
      <pages>9530-9570</pages>
      <abstract>Some prior work has shown that LLMs perform well in NLG evaluation for different tasks. However, we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability. For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing NLG quality criteria themselves. So we summarize a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved. Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different LLMs. We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations. Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation.</abstract>
      <url hash="133e7fff">2024.acl-long.516</url>
      <bibkey>hu-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.acl-long.516</doi>
    </paper>
    <paper id="517">
      <title>Synergistic Interplay between Search and Large Language Models for Information Retrieval</title>
      <author><first>Jiazhan</first><last>Feng</last></author>
      <author><first>Chongyang</first><last>Tao</last><affiliation>Beihang University</affiliation></author>
      <author><first>Xiubo</first><last>Geng</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tao</first><last>Shen</last><affiliation>Oracle</affiliation></author>
      <author><first>Can</first><last>Xu</last><affiliation>Microsoft and Peking University</affiliation></author>
      <author><first>Guodong</first><last>Long</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Daxin</first><last>Jiang</last><affiliation>Microsoft</affiliation></author>
      <pages>9571-9583</pages>
      <abstract>Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern retrieval models (RMs). The emergence of large language models (LLMs) has further revolutionized the IR field by enabling users to interact with search systems in natural languages. In this paper, we explore the advantages and disadvantages of LLMs and RMs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose **InteR**, a novel framework that facilitates information refinement through synergy between RMs and LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using retrieved documents. This iterative refinement process augments the inputs of RMs and LLMs, leading to more accurate retrieval. Experiments on large-scale retrieval benchmarks involving web search and low-resource retrieval tasks show that InteR achieves overall superior **zero-shot** retrieval performance compared to state-of-the-art methods, even those using relevance judgment. Source code is available at https://github.com/Cyril-JZ/InteR.</abstract>
      <url hash="9c3b6ff0">2024.acl-long.517</url>
      <bibkey>feng-etal-2024-synergistic</bibkey>
      <doi>10.18653/v1/2024.acl-long.517</doi>
    </paper>
    <paper id="518">
      <title>Linear Transformers with Learnable Kernel Functions are Better In-Context Models</title>
      <author><first>Yaroslav</first><last>Aksenov</last></author>
      <author><first>Nikita</first><last>Balagansky</last></author>
      <author><first>Sofia</first><last>Lo Cicero Vaina</last></author>
      <author><first>Boris</first><last>Shaposhnikov</last></author>
      <author><first>Alexey</first><last>Gorbatovski</last></author>
      <author><first>Daniil</first><last>Gavrilov</last></author>
      <pages>9584-9597</pages>
      <abstract>Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities – a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer’s in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.</abstract>
      <url hash="c4eb154f">2024.acl-long.518</url>
      <bibkey>aksenov-etal-2024-linear</bibkey>
      <doi>10.18653/v1/2024.acl-long.518</doi>
    </paper>
    <paper id="519">
      <title>Temperature-scaling surprisal estimates improve fit to human reading times – but does it do so for the “right reasons”?</title>
      <author><first>Tong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Iza</first><last>Škrjanec</last></author>
      <author><first>Vera</first><last>Demberg</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>9598-9619</pages>
      <abstract>A wide body of evidence shows that human language processing difficulty is predicted by the information-theoretic measure surprisal, a word’s negative log probability in context. However, it is still unclear how to best estimate these probabilities needed for predicting human processing difficulty – while a long-standing belief held that models with lower perplexity would provide more accurate estimates of word predictability, and therefore lead to better reading time predictions, recent work has shown that for very large models, psycholinguistic predictive power decreases. One reason could be that language models might be more confident of their predictions than humans, because they have had exposure to several magnitudes more data. In this paper, we test what effect temperature-scaling of large language model (LLM) predictions has on surprisal estimates and their predictive power of reading times of English texts. Firstly, we show that calibration of large language models typically improves with model size, i.e. poorer calibration cannot account for poorer fit to reading times. Secondly, we find that temperature-scaling probabilities lead to a systematically better fit to reading times (up to 89% improvement in delta log likelihood), across several reading time corpora. Finally, we show that this improvement in fit is chiefly driven by words that are composed of multiple subword tokens.</abstract>
      <url hash="58921ea0">2024.acl-long.519</url>
      <bibkey>liu-etal-2024-temperature</bibkey>
      <doi>10.18653/v1/2024.acl-long.519</doi>
    </paper>
    <paper id="520">
      <title>Beyond Recognising Entailment: Formalising Natural Language Inference from an Argumentative Perspective</title>
      <author><first>Ameer</first><last>Saadat-Yazdi</last></author>
      <author><first>Nadin</first><last>Kökciyan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>9620-9636</pages>
      <abstract>In argumentation theory, argument schemes are a characterisation of stereotypical patterns of inference. There has been little work done to develop computational approaches to identify these schemes in natural language. Moreover, advancements in recognizing textual entailment lack a standardized definition of inference, which makes it challenging to compare methods trained on different datasets and rely on the generalisability of their results. In this work, we propose a rigorous approach to align entailment recognition with argumentation theory. Wagemans’ Periodic Table of Arguments (PTA), a taxonomy of argument schemes, provides the appropriate framework to unify these two fields. To operationalise the theoretical model, we introduce a tool to assist humans in annotating arguments according to the PTA. Beyond providing insights into non-expert annotator training, we present Kialo-PTA24, the first multi-topic dataset for the PTA. Finally, we benchmark the performance of pre-trained language models on various aspects of argument analysis. Our experiments show that the task of argument canonicalisation poses a significant challenge for state-of-the-art models, suggesting an inability to represent argumentative reasoning and a direction for future investigation.</abstract>
      <url hash="d63f1ab7">2024.acl-long.520</url>
      <bibkey>saadat-yazdi-kokciyan-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.acl-long.520</doi>
    </paper>
    <paper id="521">
      <title><fixed-case>A</fixed-case>ny<fixed-case>GPT</fixed-case>: Unified Multimodal <fixed-case>LLM</fixed-case> with Discrete Sequence Modeling</title>
      <author><first>Jun</first><last>Zhan</last></author>
      <author><first>Junqi</first><last>Dai</last></author>
      <author><first>Jiasheng</first><last>Ye</last></author>
      <author><first>Yunhua</first><last>Zhou</last></author>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Zhigeng</first><last>Liu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xin</first><last>Zhang</last></author>
      <author><first>Ruibin</first><last>Yuan</last></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Hang</first><last>Yan</last><affiliation>AI lab</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Yu-Gang</first><last>Jiang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>9637-9662</pages>
      <abstract>We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages.We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs.Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/.</abstract>
      <url hash="0b42d024">2024.acl-long.521</url>
      <bibkey>zhan-etal-2024-anygpt</bibkey>
      <doi>10.18653/v1/2024.acl-long.521</doi>
    </paper>
    <paper id="522">
      <title><fixed-case>C</fixed-case>ofi<fixed-case>P</fixed-case>ara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target Identification with Large Multimodal Models</title>
      <author><first>Zixin</first><last>Chen</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Hongzhan</first><last>Lin</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Ziyang</first><last>Luo</last><affiliation>National University of Singapore and Hong Kong Baptist University</affiliation></author>
      <author><first>Mingfei</first><last>Cheng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Guang</first><last>Chen</last></author>
      <pages>9663-9687</pages>
      <abstract>Social media abounds with multimodal sarcasm, and identifying sarcasm targets is particularly challenging due to the implicit incongruity not directly evident in the text and image modalities. Current methods for Multimodal Sarcasm Target Identification (MSTI) predominantly focus on superficial indicators in an end-to-end manner, overlooking the nuanced understanding of multimodal sarcasm conveyed through both the text and image. This paper proposes a versatile MSTI framework with a coarse-to-fine paradigm, by augmenting sarcasm explainability with reasoning and pre-training knowledge. Inspired by the powerful capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first engage LMMs to generate competing rationales for coarser-grained pre-training of a small language model on multimodal sarcasm detection. We then propose fine-tuning the model for finer-grained sarcasm target identification. Our framework is thus empowered to adeptly unveil the intricate targets within multimodal sarcasm and mitigate the negative impact posed by potential noise inherently in LMMs. Experimental results demonstrate that our model far outperforms state-of-the-art MSTI methods, and markedly exhibits explainability in deciphering sarcasm as well.</abstract>
      <url hash="865bdf96">2024.acl-long.522</url>
      <bibkey>chen-etal-2024-cofipara</bibkey>
      <doi>10.18653/v1/2024.acl-long.522</doi>
    </paper>
    <paper id="523">
      <title>Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation</title>
      <author><first>Aiwei</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Haoping</first><last>Bai</last><affiliation>Apple</affiliation></author>
      <author><first>Zhiyun</first><last>Lu</last><affiliation>Apple</affiliation></author>
      <author><first>Xiang</first><last>Kong</last><affiliation>Apple</affiliation></author>
      <author><first>Xiaoming</first><last>Wang</last><affiliation>Didi Research US</affiliation></author>
      <author><first>Jiulong</first><last>Shan</last><affiliation>Apple</affiliation></author>
      <author><first>Meng</first><last>Cao</last><affiliation>Apple</affiliation></author>
      <author><first>Lijie</first><last>Wen</last><affiliation>School of Software, Tsinghua University</affiliation></author>
      <pages>9688-9712</pages>
      <abstract>Aligning large language models (LLMs) with human expectations without human-annotated preference data is an important problem. In this paper, we propose a method to evaluate the response preference by using the output probabilities of response pairs under contrastive prompt pairs, which could achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based on this, we propose an automatic alignment method, Direct Large Model Alignment (DLMA). First, we use contrastive prompt pairs to automatically generate preference data. Then, we continue to evaluate the generated preference data using contrastive prompt pairs and calculate a self-rewarding score. Finally, we use the DPO algorithm to effectively align LLMs by combining this self-rewarding score. In the experimental stage, our DLMA method could surpass the RLHF method without relying on human-annotated preference data.</abstract>
      <url hash="e494a311">2024.acl-long.523</url>
      <bibkey>liu-etal-2024-direct</bibkey>
      <doi>10.18653/v1/2024.acl-long.523</doi>
    </paper>
    <paper id="524">
      <title>Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines</title>
      <author><first>Michael</first><last>Toker</last></author>
      <author><first>Hadas</first><last>Orgad</last><affiliation>Computer Science Departmen, Technion-Israel Institute of Technology and Technion - Israel Institute of Technology, Technion - Israel Institute of Technology</affiliation></author>
      <author><first>Mor</first><last>Ventura</last></author>
      <author><first>Dana</first><last>Arad</last><affiliation>Computer Science Departmen, Technion-Israel Institute of Technology</affiliation></author>
      <author><first>Yonatan</first><last>Belinkov</last><affiliation>Technion, Technion</affiliation></author>
      <pages>9713-9728</pages>
      <abstract>Text-to-image diffusion models (T2I) use a latent representation of a text prompt to guide the image generation process. However, the process by which the encoder produces the text representation is unknown. We propose the Diffusion Lens, a method for analyzing the text encoder of T2I models by generating images from its intermediate representations. Using the Diffusion Lens, we perform an extensive analysis of two recent T2I models. Exploring compound prompts, we find that complex scenes describing multiple objects are composed progressively and more slowly compared to simple scenes; Exploring knowledge retrieval, we find that representation of uncommon concepts require further computation compared to common concepts, and that knowledge retrieval is gradual across layers. Overall, our findings provide valuable insights into the text encoder component in T2I pipelines.</abstract>
      <url hash="730d1ab1">2024.acl-long.524</url>
      <bibkey>toker-etal-2024-diffusion</bibkey>
      <doi>10.18653/v1/2024.acl-long.524</doi>
    </paper>
    <paper id="525">
      <title>Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models</title>
      <author><first>Yuchong</first><last>Sun</last></author>
      <author><first>Che</first><last>Liu</last></author>
      <author><first>Kun</first><last>Zhou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jinwen</first><last>Huang</last></author>
      <author><first>Ruihua</first><last>Song</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Di</first><last>Zhang</last><affiliation>Kuaishou Technology</affiliation></author>
      <author><first>Kun</first><last>Gai</last></author>
      <pages>9729-9750</pages>
      <abstract>Humans often interact with large language models (LLMs) in multi-turn interaction to obtain desired answers or more information. However, most existing studies overlook the multi-turn instruction following ability of LLMs, in terms of training dataset, training method, and evaluation benchmark. In this paper, we introduce Parrot, a solution aiming to enhance multi-turn instruction following for LLMs. First, we introduce an efficient but effective method for collecting multi-turn instructions that feature human-like queries, such as anaphora and ellipsis. Second, we propose a context-aware preference optimization strategy to further enhance LLMs for complex queries in multi-turn interaction. Moreover, to quantitatively evaluate LLMs in multi-turn instruction following, we manually build a multi-turn benchmark derived from existing ones. Extensive experiments show that Parrot improves current LLMs by up to 7.2% in multi-turn instruction following. Our dataset and codes will be open-sourced to facilitate future research.</abstract>
      <url hash="76d89985">2024.acl-long.525</url>
      <bibkey>sun-etal-2024-parrot</bibkey>
      <doi>10.18653/v1/2024.acl-long.525</doi>
    </paper>
    <paper id="526">
      <title>Robust Singing Voice Transcription Serves Synthesis</title>
      <author><first>Ruiqi</first><last>Li</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Yongqi</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhiqing</first><last>Hong</last></author>
      <author><first>Rongjie</first><last>Huang</last><affiliation>FAIR</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>9751-9766</pages>
      <abstract>Note-level Automatic Singing Voice Transcription (AST) converts singing recordings into note sequences, facilitating the automatic annotation of singing datasets for Singing Voice Synthesis (SVS) applications. Current AST methods, however, struggle with accuracy and robustness when used for practical annotation. This paper presents ROSVOT, the first robust AST model that serves SVS, incorporating a multi-scale framework that effectively captures coarse-grained note information and ensures fine-grained frame-level segmentation, coupled with an attention-based pitch decoder for reliable pitch prediction. We also established a comprehensive annotation-and-training pipeline for SVS to test the model in real-world settings. Experimental findings reveal that the proposed model achieves state-of-the-art transcription accuracy with either clean or noisy inputs. Moreover, when trained on enlarged, automatically annotated datasets, the SVS model outperforms its baseline, affirming the capability for practical application. Audio samples are available at https://rosvot.github.io. Codes can be found at https://github.com/RickyL-2000/ROSVOT.</abstract>
      <url hash="852d11f8">2024.acl-long.526</url>
      <bibkey>li-etal-2024-robust</bibkey>
      <doi>10.18653/v1/2024.acl-long.526</doi>
    </paper>
    <paper id="527">
      <title><fixed-case>V</fixed-case>ul<fixed-case>L</fixed-case>ib<fixed-case>G</fixed-case>en: Generating Names of Vulnerability-Affected Packages via a Large Language Model</title>
      <author><first>Tianyu</first><last>Chen</last><affiliation>Peking University</affiliation></author>
      <author><first>Lin</first><last>Li</last></author>
      <author><first>ZhuLiuchuan</first><last>ZhuLiuchuan</last></author>
      <author><first>Zongyang</first><last>Li</last></author>
      <author><first>Xueqing</first><last>Liu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Guangtai</first><last>Liang</last></author>
      <author><first>Qianxiang</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Tao</first><last>Xie</last><affiliation>Peking University</affiliation></author>
      <pages>9767-9780</pages>
      <abstract>Security practitioners maintain vulnerability reports (e.g., GitHub Advisory) to help developers mitigate security risks. An important task for these databases is automatically extracting structured information mentioned in the report, e.g., the affected software packages, to accelerate the defense of the vulnerability ecosystem.However, it is challenging for existing work on affected package identification to achieve high precision. One reason is that all existing work focuses on relatively smaller models, thus they cannot harness the knowledge and semantic capabilities of large language models.To address this limitation, we propose VulLibGen, the first method to use LLM for affected package identification. In contrast to existing work, VulLibGen proposes the novel idea to directly generate the affected package. To improve the precision, VulLibGen employs supervised fine-tuning (SFT), retrieval augmented generation (RAG) and a local search algorithm. The local search algorithm is a novel post-processing algorithm we introduce for reducing the hallucination of the generated packages. Our evaluation results show that VulLibGen has an average precision of 0.806 for identifying vulnerable packages in the four most popular ecosystems in GitHub Advisory (Java, JS, Python, Go) while the best average precision in previous work is 0.721. Additionally, VulLibGen has high value to security practice: we submitted 60 &lt;vulnerability, affected package&gt; pairs to GitHub Advisory (covers four ecosystems) and 34 of them have been accepted and merged.</abstract>
      <url hash="086f3780">2024.acl-long.527</url>
      <bibkey>chen-etal-2024-vullibgen</bibkey>
      <doi>10.18653/v1/2024.acl-long.527</doi>
    </paper>
    <paper id="528">
      <title>Self-Modifying State Modeling for Simultaneous Machine Translation</title>
      <author><first>Donglei</first><last>Yu</last></author>
      <author><first>Xiaomian</first><last>Kang</last></author>
      <author><first>Yuchen</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yu</first><last>Zhou</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>9781-9795</pages>
      <abstract>Simultaneous Machine Translation (SiMT) generates target outputs while receiving stream source inputs and requires a read/write policy to decide whether to wait for the next source token or generate a new target token, whose decisions form a decision path. Existing SiMT methods, which learn the policy by exploring various decision paths in training, face inherent limitations. These methods not only fail to precisely optimize the policy due to the inability to accurately assess the individual impact of each decision on SiMT performance, but also cannot sufficiently explore all potential paths because of their vast number. Besides, building decision paths requires unidirectional encoders to simulate streaming source inputs, which impairs the translation quality of SiMT models. To solve these issues, we propose Self-Modifying State Modeling (SM<tex-math>^2</tex-math>), a novel training paradigm for SiMT task. Without building decision paths, SM<tex-math>^2</tex-math> individually optimizes decisions at each state during training. To precisely optimize the policy, SM<tex-math>^2</tex-math> introduces Self-Modifying process to independently assess and adjust decisions at each state. For sufficient exploration, SM<tex-math>^2</tex-math> proposes Prefix Sampling to efficiently traverse all potential states. Moreover, SM<tex-math>^2</tex-math> ensures compatibility with bidirectional encoders, thus achieving higher translation quality. Experiments show that SM<tex-math>^2</tex-math> outperforms strong baselines. Furthermore, SM<tex-math>^2</tex-math> allows offline machine translation models to acquire SiMT ability with fine-tuning.</abstract>
      <url hash="dd5b0f66">2024.acl-long.528</url>
      <bibkey>yu-etal-2024-self</bibkey>
      <doi>10.18653/v1/2024.acl-long.528</doi>
    </paper>
    <paper id="529">
      <title><fixed-case>M</fixed-case>ap<fixed-case>GPT</fixed-case>: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation</title>
      <author><first>Jiaqi</first><last>Chen</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Bingqian</first><last>Lin</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Ran</first><last>Xu</last><affiliation>MeiTuan</affiliation></author>
      <author><first>Zhenhua</first><last>Chai</last><affiliation>Meituan</affiliation></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <author><first>Kwan-Yee</first><last>Wong</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>9796-9810</pages>
      <abstract>Embodied agents equipped with GPT as their brain have exhibited extraordinary decision-making and generalization abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt the GPT-4 to select potential locations within localized environments, without constructing an effective “global-view” for the agent to understand the overall environment. In this work, we present a novel **map**-guided **GPT**-based agent, dubbed **MapGPT**, which introduces an online linguistic-formed map to encourage the global exploration. Specifically, we build an online map and incorporate it into the prompts that include node information and topological relationships, to help GPT understand the spatial environment. Benefiting from this design, we further propose an adaptive planning mechanism to assist the agent in performing multi-step path planning based on a map, systematically exploring multiple candidate nodes or sub-goals step by step. Extensive experiments demonstrate that our MapGPT is applicable to both GPT-4 and GPT-4V, achieving state-of-the-art zero-shot performance on the R2R and REVERIE simultaneously (~10% and ~12% improvements in SR), and showcasing the newly emergent global thinking and path planning abilities of the GPT.</abstract>
      <url hash="10f18145">2024.acl-long.529</url>
      <bibkey>chen-etal-2024-mapgpt</bibkey>
      <doi>10.18653/v1/2024.acl-long.529</doi>
    </paper>
    <paper id="530">
      <title><fixed-case>B</fixed-case>ad<fixed-case>A</fixed-case>gent: Inserting and Activating Backdoor Attacks in <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Yifei</first><last>Wang</last></author>
      <author><first>Dizhan</first><last>Xue</last></author>
      <author><first>Shengjie</first><last>Zhang</last></author>
      <author><first>Shengsheng</first><last>Qian</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>9811-9827</pages>
      <abstract>With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent</abstract>
      <url hash="fe478edf">2024.acl-long.530</url>
      <bibkey>wang-etal-2024-badagent</bibkey>
      <doi>10.18653/v1/2024.acl-long.530</doi>
    </paper>
    <paper id="531">
      <title><fixed-case>D</fixed-case>eterm<fixed-case>LR</fixed-case>: Augmenting <fixed-case>LLM</fixed-case>-based Logical Reasoning from Indeterminacy to Determinacy</title>
      <author><first>Hongda</first><last>Sun</last></author>
      <author><first>Weikai</first><last>Xu</last></author>
      <author><first>Wei</first><last>Liu</last><affiliation>xiaomi</affiliation></author>
      <author><first>Jian</first><last>Luan</last></author>
      <author><first>Bin</first><last>Wang</last><affiliation>AI Lab, Xiaomi Inc.</affiliation></author>
      <author><first>Shuo</first><last>Shang</last></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>9828-9862</pages>
      <abstract>Recent advances in large language models (LLMs) have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior studies have focused on modeling reasoning steps using various thought structures like chains, trees, or graphs. However, LLM-based reasoning still encounters the following challenges: (1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps. To this end, we propose DetermLR, a novel perspective that rethinks the reasoning process as an evolution from indeterminacy to determinacy. First, we categorize known conditions into two types: determinate and indeterminate premises, facilitating the transformation process. Subsequently, we leverage quantitative measurements to prioritize more relevant premises to explore new insights. Furthermore, we automate the storage and extraction of available premises and reasoning paths with reasoning memory, preserving historical reasoning details for subsequent reasoning steps. Comprehensive experimental results demonstrate that DetermLR surpasses all baselines on various logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and LogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR achieves higher accuracy with fewer reasoning steps, highlighting its superior efficiency and effectiveness in solving logical reasoning tasks.</abstract>
      <url hash="3c2e7c7c">2024.acl-long.531</url>
      <bibkey>sun-etal-2024-determlr</bibkey>
      <doi>10.18653/v1/2024.acl-long.531</doi>
    </paper>
    <paper id="532">
      <title><fixed-case>L</fixed-case>e<fixed-case>P</fixed-case>a<fixed-case>RD</fixed-case>: A Large-Scale Dataset of Judicial Citations to Precedent</title>
      <author><first>Robert</first><last>Mahari</last><affiliation>Harvard University and Massachusetts Institute of Technology</affiliation></author>
      <author><first>Dominik</first><last>Stammbach</last></author>
      <author><first>Elliott</first><last>Ash</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Alex</first><last>Pentland</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>9863-9877</pages>
      <abstract>We present the Legal Passage Retrieval Dataset, LePaRD. LePaRD contains millions of examples of U.S. federal judges citing precedent in context. The dataset aims to facilitate work on legal passage retrieval, a challenging practice-oriented legal retrieval and reasoning task. Legal passage retrieval seeks to predict relevant passages from precedential court decisions given the context of a legal argument. We extensively evaluate various approaches on LePaRD, and find that classification-based retrieval appears to work best. Our best models only achieve a recall of 59% when trained on data corresponding to the 10,000 most-cited passages, underscoring the difficulty of legal passage retrieval. By publishing LePaRD, we provide a large-scale and high quality resource to foster further research on legal passage retrieval. We hope that research on this practice-oriented NLP task will help expand access to justice by reducing the burden associated with legal research via computational assistance. Warning: Extracts from judicial opinions may contain offensive language.</abstract>
      <url hash="22690746">2024.acl-long.532</url>
      <bibkey>mahari-etal-2024-lepard</bibkey>
      <doi>10.18653/v1/2024.acl-long.532</doi>
    </paper>
    <paper id="533">
      <title>To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering</title>
      <author><first>Giacomo</first><last>Frisoni</last></author>
      <author><first>Alessio</first><last>Cocchieri</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Alex</first><last>Presepi</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Gianluca</first><last>Moro</last><affiliation>DISI - University of Bologna</affiliation></author>
      <author><first>Zaiqiao</first><last>Meng</last><affiliation>University of Glasgow</affiliation></author>
      <pages>9878-9919</pages>
      <abstract>Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, “to generate or to retrieve” is the modern equivalent of Hamlet’s dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new state-of-the-art in the open-book setting of each testbed, allowing a small-scale reader to outcompete zero-shot closed-book 175B baselines while using up to 706x fewer parameters. Our findings reveal that generated passages are more effective than retrieved ones in attaining higher accuracy.</abstract>
      <url hash="8224d756">2024.acl-long.533</url>
      <bibkey>frisoni-etal-2024-generate</bibkey>
      <doi>10.18653/v1/2024.acl-long.533</doi>
    </paper>
    <paper id="534">
      <title><fixed-case>MERA</fixed-case>: A Comprehensive <fixed-case>LLM</fixed-case> Evaluation in <fixed-case>R</fixed-case>ussian</title>
      <author><first>Alena</first><last>Fenogenova</last><affiliation>SaluteDevices</affiliation></author>
      <author><first>Artem</first><last>Chervyakov</last></author>
      <author><first>Nikita</first><last>Martynov</last><affiliation>New Economic School</affiliation></author>
      <author><first>Anastasia</first><last>Kozlova</last></author>
      <author><first>Maria</first><last>Tikhonova</last><affiliation>Higher School of Economics</affiliation></author>
      <author><first>Albina</first><last>Akhmetgareeva</last></author>
      <author><first>Anton</first><last>Emelyanov</last></author>
      <author><first>Denis</first><last>Shevelev</last><affiliation>Salute Devices</affiliation></author>
      <author><first>Pavel</first><last>Lebedev</last></author>
      <author><first>Leonid</first><last>Sinev</last><affiliation>SaluteDevices</affiliation></author>
      <author><first>Ulyana</first><last>Isaeva</last></author>
      <author><first>Katerina</first><last>Kolomeytseva</last></author>
      <author><first>Daniil</first><last>Moskovskiy</last></author>
      <author><first>Elizaveta</first><last>Goncharova</last></author>
      <author><first>Nikita</first><last>Savushkin</last><affiliation>Stevenson University</affiliation></author>
      <author><first>Polina</first><last>Mikhailova</last><affiliation>salute devices</affiliation></author>
      <author><first>Anastasia</first><last>Minaeva</last></author>
      <author><first>Denis</first><last>Dimitrov</last><affiliation>AIRI and Sber</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <author><first>Sergey</first><last>Markov</last></author>
      <pages>9920-9948</pages>
      <abstract>Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). However, despite researchers’ attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce a new instruction benchmark, MERA, oriented towards the FMs’ performance on the Russian language. The benchmark encompasses 21 evaluation tasks for generative models covering 10 skills and is supplied with private answer scoring to prevent data leakage. The paper introduces a methodology to evaluate FMs and LMs in fixed zero- and few-shot instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential ethical concerns and drawbacks.</abstract>
      <url hash="a1060ab6">2024.acl-long.534</url>
      <bibkey>fenogenova-etal-2024-mera</bibkey>
      <doi>10.18653/v1/2024.acl-long.534</doi>
    </paper>
    <paper id="535">
      <title><fixed-case>SC</fixed-case>2: Towards Enhancing Content Preservation and Style Consistency in Long Text Style Transfer</title>
      <author><first>Jie</first><last>Zhao</last></author>
      <author><first>Ziyu</first><last>Guan</last><affiliation>Xidian University</affiliation></author>
      <author><first>Cai</first><last>Xu</last><affiliation>Xidian University</affiliation></author>
      <author><first>Wei</first><last>Zhao</last></author>
      <author><first>Yue</first><last>Jiang</last></author>
      <pages>9949-9960</pages>
      <abstract>Text style transfer (TST) aims to vary the style polarity of text while preserving the semantic content. Although recent advancements have demonstrated remarkable progress in short TST, it remains a relatively straightforward task with limited practical applications. The more comprehensive long TST task presents two challenges: (1) existing methods encounter difficulties in accurately evaluating content attributes in multiple words, leading to content degradation; (2) the conventional vanilla style classifier loss encounters obstacles in maintaining consistent style across multiple generated sentences.In this paper, we propose a novel method SC2, where a multilayer Joint Style-Content Weighed (JSCW) module and a Style Consistency loss are designed to address the two issues. The JSCW simultaneously assesses the amounts of style and content attributes within a token, aiming to acquire a lossless content representation and thereby enhancing content preservation. The multiple JSCW layers further progressively refine content representations. We design a style consistency loss to ensure the generated multiple sentences consistently reflect the target style polarity. Moreover, we incorporate a denoising non-autoregressive decoder to accelerate the training. We conduct plentiful experiments and the results show significant improvements of SC2 over competitive baselines. Our code: https://github.com/jiezhao6/SC2.</abstract>
      <url hash="64c9a1ee">2024.acl-long.535</url>
      <bibkey>zhao-etal-2024-sc2</bibkey>
      <doi>10.18653/v1/2024.acl-long.535</doi>
    </paper>
    <paper id="536">
      <title>Dodo: Dynamic Contextual Compression for Decoder-only <fixed-case>LM</fixed-case>s</title>
      <author><first>Guanghui</first><last>Qin</last></author>
      <author><first>Corby</first><last>Rosset</last></author>
      <author><first>Ethan</first><last>Chau</last><affiliation>Microsoft</affiliation></author>
      <author><first>Nikhil</first><last>Rao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <pages>9961-9975</pages>
      <abstract>Transformer-based language models (LMs) are inefficient in long contexts. We propose Dodo, a solution for context compression. Instead of one vector per token in a standard transformer model, Dodo represents text with a dynamic number of hidden states at each layer, reducing the cost of self-attention to a fraction of typical time and space. Moreover, off-the-shelf models such as LLaMA can be adapted to Dodo by efficient parameter tuning methods such as LoRA. In use, Dodo can act as either an autoregressive LM or a context compressor for downstream tasks. We demonstrate through experiments in language modeling, question answering, and summarization that Dodo retains capabilities in these tasks, while drastically reducing the overhead during decoding. For example, in the autoencoding task, Dodo shrinks context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.</abstract>
      <url hash="08931032">2024.acl-long.536</url>
      <bibkey>qin-etal-2024-dodo</bibkey>
      <doi>10.18653/v1/2024.acl-long.536</doi>
    </paper>
    <paper id="537">
      <title><fixed-case>POMP</fixed-case>: Probability-driven Meta-graph Prompter for <fixed-case>LLM</fixed-case>s in Low-resource Unsupervised Neural Machine Translation</title>
      <author><first>Shilong</first><last>Pan</last></author>
      <author><first>Zhiliang</first><last>Tian</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Haoqi</first><last>Zheng</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Zhen</first><last>Huang</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Zhihua</first><last>Wen</last><affiliation>National University of Defence Technology</affiliation></author>
      <author><first>Dongsheng</first><last>Li</last></author>
      <pages>9976-9992</pages>
      <abstract>Low-resource languages (LRLs) face challenges in supervised neural machine translation (NMT) due to limited parallel data, prompting research in unsupervised NMT.Unsupervised NMT (UNMT), without requiring ground truth, provides solutions for LRL translations using synthetic pseudo-parallel data and parallel data from auxiliary language pairs. However, they usually encounter translation errors, including errors from synthetic data and from auxiliary language pairs with linguistic biases.We argue that large language models (LLMs) mitigate UNMT’s translation errors by dynamically organizing auxiliary languages in prompts to improve LRL translations. In this paper, we propose <tex-math>\textbf{P}</tex-math>r<tex-math>\textbf{O}</tex-math>bability-driven <tex-math>\textbf{M}</tex-math>eta-graph <tex-math>\textbf{P}</tex-math>rompter (POMP), an approach employing a dynamic graph to organize multiple auxiliary languages, to prompt LLMs in LRL translations. POMP proposes a language-specific meta-graph that dynamically samples multiple translation paths to organize auxiliary languages in constructing prompts. Following the path, POMP prompts LLMs to translate with a mixture of auxiliary languages. We achieve the meta-graph’s evolution by back-propagating evaluation scores to update probabilities on the graph.Our experimental improvements show POMP’s effectiveness on LRLs’ translation.</abstract>
      <url hash="e2551e3f">2024.acl-long.537</url>
      <bibkey>pan-etal-2024-pomp</bibkey>
      <doi>10.18653/v1/2024.acl-long.537</doi>
    </paper>
    <paper id="538">
      <title><fixed-case>N</fixed-case>ews<fixed-case>B</fixed-case>ench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in <fixed-case>C</fixed-case>hinese Journalism</title>
      <author><first>Miao</first><last>Li</last></author>
      <author><first>Ming-Bin</first><last>Chen</last></author>
      <author><first>Bo</first><last>Tang</last></author>
      <author><first>ShengbinHou</first><last>ShengbinHou</last></author>
      <author><first>Pengyu</first><last>Wang</last></author>
      <author><first>Haiying</first><last>Deng</last></author>
      <author><first>Zhiyu</first><last>Li</last></author>
      <author><first>Feiyu</first><last>Xiong</last><affiliation>Institute for Advanced Algorithms Research, Shanghai</affiliation></author>
      <author><first>Keming</first><last>Mao</last></author>
      <author><first>Cheng</first><last>Peng</last></author>
      <author><first>Yi</first><last>Luo</last></author>
      <pages>9993-10014</pages>
      <abstract>We present NewsBench, a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. Our constructed benchmark dataset is focused on four facets of writing proficiency and six facets of safety adherence, and it comprises manually and carefully designed 1,267 test samples in the types of multiple choice questions and short answer questions for five editorial tasks in 24 news domains. To measure performances, we propose different GPT-4 based automatic evaluation protocols to assess LLM generations for short answer questions in terms of writing proficiency and safety adherence, and both are validated by the high correlations with human evaluations. Based on the systematic evaluation framework, we conduct a comprehensive analysis of eleven popular LLMs which can handle Chinese. The experimental results highlight GPT-4 and ERNIE Bot as top performers, yet reveal a relative deficiency in journalistic safety adherence in creative writing tasks. Our findings also underscore the need for enhanced ethical guidance in machine-generated journalistic content, marking a step forward in aligning LLMs with journalistic standards and safety considerations. The evaluation framework and experimental results are expected to provide an in-depth understanding of the editorial capabilities of LLMs and speed up the development of LLMs in journalism.</abstract>
      <url hash="10d7c409">2024.acl-long.538</url>
      <bibkey>li-etal-2024-newsbench</bibkey>
      <doi>10.18653/v1/2024.acl-long.538</doi>
    </paper>
    <paper id="539">
      <title><fixed-case>MAPO</fixed-case>: Advancing Multilingual Reasoning through Multilingual-Alignment-as-Preference Optimization</title>
      <author><first>Shuaijie</first><last>She</last></author>
      <author><first>Wei</first><last>Zou</last></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Wenhao</first><last>Zhu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Xiang</first><last>Liu</last></author>
      <author><first>Xiang</first><last>Geng</last></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <pages>10015-10027</pages>
      <abstract>Intuitively, reasoning abilities are considered language-agnostic. However, existing LLMs exhibit inconsistent reasoning abilities across different languages, e.g., reasoning in the dominant language like English is superior to other languages due to the imbalance of multilingual training data. To enhance reasoning abilities in non-dominant languages, we propose a Multilingual-Alignment-as-Preference Optimization framework (MAPO) to align the reasoning processes in other languages with the dominant language. Specifically, we harness an off-the-shelf translation model for the consistency between answers in non-dominant and dominant languages, which we adopt as the preference for optimization, e.g., Direct Preference Optimization(DPO) or Proximal Policy Optimization (PPO). Experiments show that MAPO stably achieves significant improvements in the multilingual reasoning of various models on all three benchmarks (MSVAMP +16.2%, MGSM +6.1%, and MNumGLUESub +13.3%), with improved reasoning consistency across languages. The project is available at https://github.com/NJUNLP/MAPO.</abstract>
      <url hash="282e0c00">2024.acl-long.539</url>
      <bibkey>she-etal-2024-mapo</bibkey>
      <doi>10.18653/v1/2024.acl-long.539</doi>
    </paper>
    <paper id="540">
      <title>Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training</title>
      <author><first>Feiteng</first><last>Fang</last></author>
      <author><first>Yuelin</first><last>Bai</last></author>
      <author><first>Shiwen</first><last>Ni</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiaojun</first><last>Chen</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>10028-10039</pages>
      <abstract>Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs’ capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model’s training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model’s capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we will release our code and data upon acceptance.</abstract>
      <url hash="60ecf472">2024.acl-long.540</url>
      <bibkey>fang-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.acl-long.540</doi>
    </paper>
    <paper id="541">
      <title>Predicting Text Preference Via Structured Comparative Reasoning</title>
      <author><first>Jing Nathan</first><last>Yan</last></author>
      <author><first>Tianqi</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>Justin</first><last>Chiu</last><affiliation>Cornell University</affiliation></author>
      <author><first>Jiaming</first><last>Shen</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Zhen</first><last>Qin</last><affiliation>Google</affiliation></author>
      <author><first>Yue</first><last>Yu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Charumathi</first><last>Lakshmanan</last></author>
      <author><first>Yair</first><last>Kurzion</last></author>
      <author><first>Alexander</first><last>Rush</last><affiliation>Cornell University and School of Engineering and Applied Sciences, Harvard University</affiliation></author>
      <author><first>Jialu</first><last>Liu</last><affiliation>Google Research</affiliation></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <pages>10040-10060</pages>
      <abstract>Comparative reasoning plays a crucial role in predicting text preferences; however, large language models (LLMs) often demonstrate inconsistencies in their reasoning, leading to incorrect preference predictions. While approaches like Chain-of-Thought improve accuracy in many settings, they struggle to consistently distinguish the similarities and differences of complex texts. We introduce <tex-math>SC^2</tex-math>, a model that prompts LLMs to predict text preferences by generating structured intermediate comparisons. <tex-math>SC^2</tex-math> begins by proposing aspects for comparison, followed by generating textual comparisons under each aspect. We select consistent comparisons with a pairwise comparator that ensures each comparison of a given aspect clearly distinguishes differences between texts, significantly reducing hallucination and improving consistency. Our empirical studies across various NLP tasks, including summarization, retrieval, and automatic rating, demonstrate that <tex-math>SC^2</tex-math>‘s enhanced performance in text preference prediction is significant.</abstract>
      <url hash="c00c968a">2024.acl-long.541</url>
      <bibkey>yan-etal-2024-predicting</bibkey>
      <doi>10.18653/v1/2024.acl-long.541</doi>
    </paper>
    <paper id="542">
      <title><fixed-case>C</fixed-case>o<fixed-case>ELM</fixed-case>: Construction-Enhanced Language Modeling</title>
      <author><first>Lvxiaowei</first><last>Xu</last></author>
      <author><first>Zhilin</first><last>Gong</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jianhua</first><last>Dai</last></author>
      <author><first>Tianxiang</first><last>Wang</last></author>
      <author><first>Ming</first><last>Cai</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jiawei</first><last>Peng</last></author>
      <pages>10061-10081</pages>
      <abstract>Recent studies have shown that integrating constructional information can improve the performance of pre-trained language models (PLMs) in natural language understanding. However, exploration into leveraging constructional information to enhance generative language models for natural language generation has been limited. Additionally, probing studies indicate that PLMs primarily grasp the syntactic structure of constructions but struggle to capture their semantics. In this work, we encode constructions as inductive biases to explicitly embed constructional semantics and guide the generation process. We begin by presenting a construction grammar induction framework designed to automatically identify constructions from corpora. Subsequently, we propose the Construction-Enhanced Language Model (CoELM). It introduces a construction-guided language modeling approach that employs a dynamic sequence reassembly strategy during pre-training. Extensive experiments have demonstrated the superiority of CoELM across various benchmarks.</abstract>
      <url hash="161541dd">2024.acl-long.542</url>
      <bibkey>xu-etal-2024-coelm</bibkey>
      <doi>10.18653/v1/2024.acl-long.542</doi>
    </paper>
    <paper id="543">
      <title>Uni-Dubbing: Zero-Shot Speech Synthesis from Visual Articulation</title>
      <author><first>Songju</first><last>Lei</last><affiliation>Honor Device Co., Ltd</affiliation></author>
      <author><first>Xize</first><last>Cheng</last></author>
      <author><first>Mengjiao</first><last>Lyu</last></author>
      <author><first>Jianqiao</first><last>Hu</last></author>
      <author><first>Jintao</first><last>Tan</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Runlin</first><last>Liu</last></author>
      <author><first>Lingyu</first><last>Xiong</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Tao</first><last>Jin</last></author>
      <author><first>Xiandong</first><last>Li</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>10082-10099</pages>
      <abstract>In the field of speech synthesis, there is a growing emphasis on employing multimodal speech to enhance robustness. A key challenge in this area is the scarcity of datasets that pair audio with corresponding video. We employ a methodology that incorporates modality alignment during the pre-training phase on multimodal datasets, uniquely facilitating zero-shot generalization through the process of freezing the video modality feature extraction component and the encoder module within the pretrained weights, thereby enabling effective cross-modal and cross-lingual transfer. We have named this method ‘Uni-Dubbing’. Our method finely tunes with both multimodal and single-modality audio data. In multimodal scenarios, it achieves a reduced word error rate (WER) of 31.73%, surpassing the previous best of 33.9%. It also excels in metrics like tone quality and synchronization. With single-modality audio, it achieves a WER of 36.08%, demonstrating adaptability to limited data. Its domain generalization capabilities are proven across various language tasks in video translation and audio generation. Trained on 433 hours of audio data, it surpasses techniques using 200 hours of audiovisual data. The code and demo are available at https://diracer.github.io/unidubbing.</abstract>
      <url hash="f3618514">2024.acl-long.543</url>
      <bibkey>lei-etal-2024-uni</bibkey>
      <doi>10.18653/v1/2024.acl-long.543</doi>
    </paper>
    <paper id="544">
      <title>On the Impact of Calibration Data in Post-training Quantization and Pruning</title>
      <author><first>Miles</first><last>Williams</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Nikolaos</first><last>Aletras</last><affiliation>University of Sheffield, University of Sheffield and Amazon</affiliation></author>
      <pages>10100-10118</pages>
      <abstract>Quantization and pruning form the foundation of compression for neural networks, enabling efficient inference for large language models (LLMs). Recently, various quantization and pruning techniques have demonstrated remarkable performance in a post-training setting. They rely upon calibration data, a small set of unlabeled examples that are used to generate layer activations. However, no prior work has systematically investigated how the calibration data impacts the effectiveness of model compression methods. In this paper, we present the first extensive empirical study on the effect of calibration data upon LLM performance. We trial a variety of quantization and pruning methods, datasets, tasks, and models. Surprisingly, we find substantial variations in downstream task performance, contrasting existing work that suggests a greater level of robustness to the calibration data. Finally, we make a series of recommendations for the effective use of calibration data in LLM quantization and pruning.</abstract>
      <url hash="de1ae8ae">2024.acl-long.544</url>
      <bibkey>williams-aletras-2024-impact</bibkey>
      <doi>10.18653/v1/2024.acl-long.544</doi>
    </paper>
    <paper id="545">
      <title><fixed-case>S</fixed-case>ym<fixed-case>KGQA</fixed-case>: Few-Shot Knowledge Graph Question Answering via Symbolic Program Generation and Execution</title>
      <author><first>Prerna</first><last>Agarwal</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <author><first>Nishant</first><last>Kumar</last></author>
      <author><first>Srikanta</first><last>Bedathur</last><affiliation>Indian Institute of Technology, Delhi and Indian Institute of Technology Delhi</affiliation></author>
      <pages>10119-10140</pages>
      <abstract>Semantic Parsing of natural language questions into their executable logical form (LF) has shown state-of-the-art (SOTA) performance for Knowledge Graph Question Answering (KGQA). However, these methods are not applicable for real-world applications, due to lack of KG-specific training data. Recent advances in the capabilities of Large Language Models (LLMs) has led towards generating low-level LFs such as SPARQL and S-Expression in a few-shot setting. Unfortunately, these methods: (1) are limited to the knowledge of underlying LLM about the LF, (2) performs inferior for the harder complex benchmarks such as KQA Pro, (3) suffers while grounding the generated LF to a specific Knowledge Graph. Recently, a new LF called KoPL has been introduced that explicitly models complex reasoning process step-by-step in a symbolic manner and has shown SOTA on KQA Pro in fully-supervised setting. Inspired by this, we propose SymKGQA framework that generates step-by-step Symbolic LF i.e., KoPL in a few-shot in-context learning setting using LLM. Our framework is not dependent on pre-trained information of LLM about KoPL. We further build a Retrieval-Augmented Generation based Question-Aware Contextual KoPL (QUACK) resolver to ground the generated LF. Our experiments with different LLMs and few-shot settings demonstrate that SymKGQA outperforms all other few-shot and even many of the fully-supervised KGQA approaches.</abstract>
      <url hash="40701314">2024.acl-long.545</url>
      <bibkey>agarwal-etal-2024-symkgqa</bibkey>
      <doi>10.18653/v1/2024.acl-long.545</doi>
    </paper>
    <paper id="546">
      <title>Meta-Task Prompting Elicits Embeddings from Large Language Models</title>
      <author><first>Yibin</first><last>Lei</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Di</first><last>Wu</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Tao</first><last>Shen</last><affiliation>Oracle</affiliation></author>
      <author><first>Yu</first><last>Cao</last></author>
      <author><first>Chongyang</first><last>Tao</last><affiliation>Beihang University</affiliation></author>
      <author><first>Andrew</first><last>Yates</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>10141-10157</pages>
      <abstract>We introduce a new unsupervised text embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks are versatile embeddings that yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law, offering a versatile and resource-efficient approach for embedding generation across diverse scenarios.</abstract>
      <url hash="696cfbcd">2024.acl-long.546</url>
      <bibkey>lei-etal-2024-meta</bibkey>
      <doi>10.18653/v1/2024.acl-long.546</doi>
    </paper>
    <paper id="547">
      <title>A Sentiment Consolidation Framework for Meta-Review Generation</title>
      <author><first>Miao</first><last>Li</last></author>
      <author><first>Jey Han</first><last>Lau</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Eduard</first><last>Hovy</last><affiliation>University of Melbourne and Carnegie Mellon University</affiliation></author>
      <pages>10158-10177</pages>
      <abstract>Modern natural language generation systems with Large Language Models (LLMs) exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if they truly possess the capability of information consolidation to generate summaries, especially on documents with opinionated information. We focus on meta-review generation, a form of sentiment summarisation for the scientific domain. To make scientific sentiment summarization more grounded, we hypothesize that human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews. Based on the framework, we propose novel prompting methods for LLMs to generate meta-reviews and evaluation metrics to assess the quality of generated meta-reviews. Our framework is validated empirically as we find that prompting LLMs based on the framework — compared with prompting them with simple instructions — generates better meta-reviews.</abstract>
      <url hash="85733eea">2024.acl-long.547</url>
      <bibkey>li-etal-2024-sentiment</bibkey>
      <doi>10.18653/v1/2024.acl-long.547</doi>
    </paper>
    <paper id="548">
      <title>Revisiting Structured Sentiment Analysis as Latent Dependency Graph Parsing</title>
      <author><first>Chengjie</first><last>Zhou</last></author>
      <author><first>Bobo</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Fei</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Chong</first><last>Teng</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <pages>10178-10191</pages>
      <abstract>Structured Sentiment Analysis (SSA) was cast as a problem of bi-lexical dependency graph parsing by prior studies.Multiple formulations have been proposed to construct the graph, which share several intrinsic drawbacks:(1) The internal structures of spans are neglected, thus only the boundary tokens of spans are used for relation prediction and span recognition, thus hindering the model’s expressiveness;(2) Long spans occupy a significant proportion in the SSA datasets, which further exacerbates the problem of internal structure neglect.In this paper, we treat the SSA task as a dependency parsing task on partially-observed dependency trees, regarding flat spans without determined tree annotations as latent subtrees to consider internal structures of spans.We propose a two-stage parsing method and leverage TreeCRFs with a novel constrained inside algorithm to model latent structures explicitly, which also takes advantages of joint scoring graph arcs and headed spans for global optimization and inference. Results of extensive experiments on five benchmark datasets reveal that our method performs significantly better than all previous bi-lexical methods, achieving new state-of-the-art.</abstract>
      <url hash="da53d48c">2024.acl-long.548</url>
      <bibkey>zhou-etal-2024-revisiting-structured</bibkey>
      <doi>10.18653/v1/2024.acl-long.548</doi>
    </paper>
    <paper id="549">
      <title><fixed-case>OWSM</fixed-case>-<fixed-case>CTC</fixed-case>: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification</title>
      <author><first>Yifan</first><last>Peng</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yui</first><last>Sudo</last></author>
      <author><first>Muhammad</first><last>Shakeel</last><affiliation>Honda Research Institution Japan Co., Ltd.</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>10192-10209</pages>
      <abstract>There has been an increasing interest in large speech models that can perform multiple tasks in a single model. Such models usually adopt an encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and language identification (LID). Compared to encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up to 24% relative improvement on ST, while it is more robust and 3 to 4 times faster for inference. OWSM-CTC also improves the long-form ASR result with 20x speed-up.We will publicly release our code, pre-trained model, and training logs to promote open science in speech foundation models.</abstract>
      <url hash="03bc20b8">2024.acl-long.549</url>
      <bibkey>peng-etal-2024-owsm</bibkey>
      <doi>10.18653/v1/2024.acl-long.549</doi>
    </paper>
    <paper id="550">
      <title>Do Large Language Models Latently Perform Multi-Hop Reasoning?</title>
      <author><first>Sohee</first><last>Yang</last><affiliation>University College London, University of London, Department of Computer Science, University College London, University of London, DeepMind and Google</affiliation></author>
      <author><first>Elena</first><last>Gribovskaya</last><affiliation>Deepmind Google</affiliation></author>
      <author><first>Nora</first><last>Kassner</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Tel Aviv University and Google Research</affiliation></author>
      <author><first>Sebastian</first><last>Riedel</last><affiliation>Google and University College London</affiliation></author>
      <pages>10210-10229</pages>
      <abstract>We study whether Large Language Models (LLMs) latently perform multi-hop reasoning with complex prompts such as “The mother of the singer of ‘Superstition’ is”. We look for evidence of a latent reasoning pathway where an LLM (1) latently identifies “the singer of ‘Superstition’” as Stevie Wonder, the bridge entity, and (2) uses its knowledge of Stevie Wonder’s mother to complete the prompt. We analyze these two hops individually and consider their co-occurrence as indicative of latent multi-hop reasoning. For the first hop, we test if changing the prompt to indirectly mention the bridge entity instead of any other entity increases the LLM’s internal recall of the bridge entity. For the second hop, we test if increasing this recall causes the LLM to better utilize what it knows about the bridge entity. We find strong evidence of latent multi-hop reasoning for the prompts of certain relation types, with the reasoning pathway used in more than 80% of the prompts. However, the utilization is highly contextual, varying across different types of prompts. Also, on average, the evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop. Moreover, we find a clear scaling trend with increasing model size for the first hop of reasoning but not for the second hop. Our experimental findings suggest potential challenges and opportunities for future development and applications of LLMs.</abstract>
      <url hash="a0836a43">2024.acl-long.550</url>
      <bibkey>yang-etal-2024-large-language-models</bibkey>
      <doi>10.18653/v1/2024.acl-long.550</doi>
    </paper>
    <paper id="551">
      <title><fixed-case>M</fixed-case>uggle<fixed-case>M</fixed-case>ath: Assessing the Impact of Query and Response Augmentation on Math Reasoning</title>
      <author><first>Chengpeng</first><last>Li</last></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hongyi</first><last>Yuan</last></author>
      <author><first>Guanting</first><last>Dong</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Keming</first><last>Lu</last></author>
      <author><first>Jiancan</first><last>Wu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Chuanqi</first><last>Tan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiang</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Chang</first><last>Zhou</last></author>
      <pages>10230-10258</pages>
      <abstract>In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks?To this end, we create two new dataset AugGSM8K and AugMATH, by complicating and diversifying the queries and sampling multiple reasoning paths from GSM8K and MATH.We obtained a series of LLMs called MuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath substantially achieves new state-of-the-art on GSM8K and MATH.A log-linear relationship and a segmented log-linear are presented between MuggleMath’s performance and the amount of augmented data on GSM8K and MATH, respectively.We also find that it is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization.</abstract>
      <url hash="6a830564">2024.acl-long.551</url>
      <bibkey>li-etal-2024-mugglemath</bibkey>
      <doi>10.18653/v1/2024.acl-long.551</doi>
    </paper>
    <paper id="552">
      <title>Harnessing Toulmin’s theory for zero-shot argument explication</title>
      <author><first>Ankita</first><last>Gupta</last></author>
      <author><first>Ethan</first><last>Zuckerman</last></author>
      <author><first>Brendan</first><last>O’Connor</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <pages>10259-10276</pages>
      <abstract>To better analyze informal arguments on public forums, we propose the task of argument explication, which makes explicit a text’s argumentative structure and implicit reasoning by outputting triples of propositions ⟨claim, reason warrant⟩. The three slots, or argument components, are derived from the widely known Toulmin (1958) model of argumentation. While prior research applies Toulmin or related theories to annotate datasets and train supervised models, we develop an effective method to prompt generative large language models (LMs) to output explicitly named argument components proposed by Toulmin by prompting with the theory name (e.g., ‘According to Toulmin model’). We evaluate the outputs’ coverage and validity through a human study and automatic evaluation based on prior argumentation datasets and perform robustness checks over alternative LMs, prompts, and argumentation theories. Finally, we conduct a proof-of-concept case study to extract an interpretable argumentation (hyper)graph from a large corpus of critical public comments on whether to allow the COVID-19 vaccine for children, suggesting future directions for corpus analysis and argument visualization.</abstract>
      <url hash="03a05780">2024.acl-long.552</url>
      <bibkey>gupta-etal-2024-harnessing</bibkey>
      <doi>10.18653/v1/2024.acl-long.552</doi>
    </paper>
    <paper id="553">
      <title><fixed-case>B</fixed-case>inary<fixed-case>A</fixed-case>lign: Word Alignment as Binary Sequence Labeling</title>
      <author><first>Gaetan</first><last>Latouche</last><affiliation>Ubisoft</affiliation></author>
      <author><first>Marc-André</first><last>Carbonneau</last><affiliation>Ubisoft</affiliation></author>
      <author><first>Benjamin</first><last>Swanson</last><affiliation>Ubisoft</affiliation></author>
      <pages>10277-10288</pages>
      <abstract>Real world deployments of word alignment are almost certain to cover both high and low resource languages. However, the state-of-the-art for this task recommends a different model class depending on the availability of gold alignment training data for a particular language pair. We propose BinaryAlign, a novel word alignment technique based on binary sequence labeling that outperforms existing approaches in both scenarios, offering a unifying approach to the task. Additionally, we vary the specific choice of multilingual foundation model, perform stratified error analysis over alignment error type, and explore the performance of BinaryAlign on non-English language pairs. We make our source code publicly available.</abstract>
      <url hash="81acea89">2024.acl-long.553</url>
      <bibkey>latouche-etal-2024-binaryalign</bibkey>
      <doi>10.18653/v1/2024.acl-long.553</doi>
    </paper>
    <paper id="554">
      <title>Quantifying the Persona Effect in <fixed-case>LLM</fixed-case> Simulations</title>
      <author><first>Tiancheng</first><last>Hu</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Nigel</first><last>Collier</last><affiliation>University of Cambridge</affiliation></author>
      <pages>10289-10307</pages>
      <abstract>Large language models (LLMs) have shown remarkable promise in simulating human language and behavior. This study investigates how integrating persona variables—demographic, social, and behavioral factors—impacts LLMs’ ability to simulate diverse perspectives. We find that persona variables account for &lt;10% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating persona variables via prompting in LLMs provides modest but statistically significant improvements. Persona prompting is most effective in samples where many annotators disagree, but their disagreements are relatively minor. Notably, we find a linear relationship in our setting: the stronger the correlation between persona variables and human annotations, the more accurate the LLM predictions are using persona prompting. In a zero-shot setting, a powerful 70b model with persona prompting captures 81% of the annotation variance achievable by linear regression trained on ground truth annotations. However, for most subjective NLP datasets, where persona variables have limited explanatory power, the benefits of persona prompting are limited.</abstract>
      <url hash="262bfcd3">2024.acl-long.554</url>
      <bibkey>hu-collier-2024-quantifying</bibkey>
      <doi>10.18653/v1/2024.acl-long.554</doi>
    </paper>
    <paper id="555">
      <title>Artifacts or Abduction: How Do <fixed-case>LLM</fixed-case>s Answer Multiple-Choice Questions Without the Question?</title>
      <author><first>Nishant</first><last>Balepur</last></author>
      <author><first>Abhilasha</first><last>Ravichander</last><affiliation>Allen Institute for Artificial Intelligence and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Rachel</first><last>Rudinger</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>10308-10330</pages>
      <abstract>Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices. In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference. Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices. Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question. We hope to motivate the use of stronger baselines in MCQA benchmarks, the design of robust MCQA datasets, and further efforts to explain LLM decision-making.</abstract>
      <url hash="9e39d4dd">2024.acl-long.555</url>
      <bibkey>balepur-etal-2024-artifacts</bibkey>
      <doi>10.18653/v1/2024.acl-long.555</doi>
    </paper>
    <paper id="556">
      <title>Retrieval Augmented Fact Verification by Synthesizing Contrastive Arguments</title>
      <author><first>Zhenrui</first><last>Yue</last></author>
      <author><first>Huimin</first><last>Zeng</last></author>
      <author><first>Lanyu</first><last>Shang</last></author>
      <author><first>Yifan</first><last>Liu</last></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Dong</first><last>Wang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>10331-10343</pages>
      <abstract>The rapid propagation of misinformation poses substantial risks to public interest. To combat misinformation, large language models (LLMs) are adapted to automatically verify claim credibility. Nevertheless, existing methods heavily rely on the embedded knowledge within LLMs and / or black-box APIs for evidence collection, leading to subpar performance with smaller LLMs or upon unreliable context. In this paper, we propose retrieval augmented fact verification through the synthesis of contrasting arguments (RAFTS). Upon input claims, RAFTS starts with evidence retrieval, where we design a retrieval pipeline to collect and re-rank relevant documents from verifiable sources. Then, RAFTS forms contrastive arguments (i.e., supporting or refuting) conditioned on the retrieved evidence. In addition, RAFTS leverages an embedding model to identify informative demonstrations, followed by in-context prompting to generate the prediction and explanation. Our method effectively retrieves relevant documents as evidence and evaluates arguments from varying perspectives, incorporating nuanced information for fine-grained decision-making. Combined with informative in-context examples as prior, RAFTS achieves significant improvements to supervised and LLM baselines without complex prompts. We demonstrate the effectiveness of our method through extensive experiments, where RAFTS can outperform GPT-based methods with a significantly smaller 7B LLM.</abstract>
      <url hash="a7b31673">2024.acl-long.556</url>
      <bibkey>yue-etal-2024-retrieval</bibkey>
      <doi>10.18653/v1/2024.acl-long.556</doi>
    </paper>
    <paper id="557">
      <title><fixed-case>S</fixed-case>yllabus<fixed-case>QA</fixed-case>: A Course Logistics Question Answering Dataset</title>
      <author><first>Nigel</first><last>Fernandez</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Alexander</first><last>Scarlatos</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Andrew</first><last>Lan</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <pages>10344-10369</pages>
      <abstract>Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We introduce Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of predicted answers. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision.</abstract>
      <url hash="ba9594a7">2024.acl-long.557</url>
      <bibkey>fernandez-etal-2024-syllabusqa</bibkey>
      <doi>10.18653/v1/2024.acl-long.557</doi>
    </paper>
    <paper id="558">
      <title><fixed-case>M</fixed-case>ind<fixed-case>M</fixed-case>ap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models</title>
      <author><first>Yilin</first><last>Wen</last></author>
      <author><first>Zifeng</first><last>Wang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Jimeng</first><last>Sun</last><affiliation>University of Illinois, Urbana Champaign, College of Computing and Georgia Institute of Technology</affiliation></author>
      <pages>10370-10388</pages>
      <abstract>Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named MindMap, that leverages knowledge graphs (KGs) to enhance LLMs’ inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question &amp; answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference.</abstract>
      <url hash="744de9f3">2024.acl-long.558</url>
      <bibkey>wen-etal-2024-mindmap</bibkey>
      <doi>10.18653/v1/2024.acl-long.558</doi>
    </paper>
    <paper id="559">
      <title><fixed-case>AGB</fixed-case>-<fixed-case>DE</fixed-case>: A Corpus for the Automated Legal Assessment of Clauses in <fixed-case>G</fixed-case>erman Consumer Contracts</title>
      <author><first>Daniel</first><last>Braun</last><affiliation>University of Twente</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>10389-10405</pages>
      <abstract>Legal tasks and datasets are often used as benchmarks for the capabilities of language models. However, openly available annotated datasets are rare. In this paper, we introduce AGB-DE, a corpus of 3,764 clauses from German consumer contracts that have been annotated and legally assessed by legal experts. Together with the data, we present a first baseline for the task of detecting potentially void clauses, comparing the performance of an SVM baseline with three fine-tuned open language models and the performance of GPT-3.5. Our results show the challenging nature of the task, with no approach exceeding an F1-score of 0.54. While the fine-tuned models often performed better with regard to precision, GPT-3.5 outperformed the other approaches with regard to recall. An analysis of the errors indicates that one of the main challenges could be the correct interpretation of complex clauses, rather than the decision boundaries of what is permissible and what is not.</abstract>
      <url hash="a6c4c02f">2024.acl-long.559</url>
      <bibkey>braun-matthes-2024-agb</bibkey>
      <doi>10.18653/v1/2024.acl-long.559</doi>
    </paper>
    <paper id="560">
      <title>Examining the robustness of <fixed-case>LLM</fixed-case> evaluation to the distributional assumptions of benchmarks</title>
      <author><first>Charlotte</first><last>Siska</last></author>
      <author><first>Katerina</first><last>Marazopoulou</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst, Imperial College London, National Technical University of Athens and Microsoft</affiliation></author>
      <author><first>Melissa</first><last>Ailem</last><affiliation>Microsoft</affiliation></author>
      <author><first>James</first><last>Bono</last><affiliation>Microsoft</affiliation></author>
      <pages>10406-10421</pages>
      <abstract>Benchmarks have emerged as the central approach for evaluating Large Language Models (LLMs). The research community often relies on a model’s average performance across the test prompts of a benchmark to evaluate the model’s performance. This is consistent with the assumption that the test prompts within a benchmark represent a random sample from some real-world distribution of interest. We note that this is generally not the case; instead, we hold that the distribution of interest varies according to the specific use case. Hence, we analyze the robustness of LLM benchmarks to their underlying distributional assumptions. We find that (1) the correlation in model performance across test prompts is non-random, (2) accounting for correlations across test prompts can change model rankings on major benchmarks, (3) explanatory factors for these correlations include semantic similarity and common LLM failure points.</abstract>
      <url hash="4ec351e2">2024.acl-long.560</url>
      <bibkey>siska-etal-2024-examining</bibkey>
      <doi>10.18653/v1/2024.acl-long.560</doi>
    </paper>
    <paper id="561">
      <title>Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning</title>
      <author><first>Eric</first><last>Pasewark</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Kyle</first><last>Montgomery</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Kefei</first><last>Duan</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Dawn</first><last>Song</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Chenguang</first><last>Wang</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <pages>10422-10437</pages>
      <abstract>We present a new method for large language models to solve compositional tasks. Although they have shown strong performance on traditional language understanding tasks, large language models struggle to solve compositional tasks, where the solution depends on solving smaller instances of the same problem. We propose a natural approach to solve compositional tasks recursively. Our method, Re-Tuning, tunes models to break down a problem into subproblems, solve those subproblems, and combine the results. We show that our method significantly improves model performance on three representative compositional tasks: integer addition, dynamic programming, and parity. Compared to state-of-the-art methods that keep intermediate steps towards solving the problems, Re-Tuning achieves significantly higher accuracy and is more GPU memory efficient.</abstract>
      <url hash="a0f9f728">2024.acl-long.561</url>
      <bibkey>pasewark-etal-2024-tuning</bibkey>
      <doi>10.18653/v1/2024.acl-long.561</doi>
    </paper>
    <paper id="562">
      <title>Bridging the Preference Gap between Retrievers and <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zixuan</first><last>Ke</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Weize</first><last>Kong</last><affiliation>Google</affiliation></author>
      <author><first>Cheng</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Mingyang</first><last>Zhang</last><affiliation>Google</affiliation></author>
      <author><first>Qiaozhu</first><last>Mei</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <pages>10438-10451</pages>
      <abstract>Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-”friendly” information and assembling a LLM-”friendly” context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.</abstract>
      <url hash="615cdcf9">2024.acl-long.562</url>
      <bibkey>ke-etal-2024-bridging</bibkey>
      <doi>10.18653/v1/2024.acl-long.562</doi>
    </paper>
    <paper id="563">
      <title>Large Language Models Can Learn Temporal Reasoning</title>
      <author><first>Siheng</first><last>Xiong</last></author>
      <author><first>Ali</first><last>Payani</last><affiliation>Cisco</affiliation></author>
      <author><first>Ramana</first><last>Kompella</last><affiliation>Cisco</affiliation></author>
      <author><first>Faramarz</first><last>Fekri</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>10452-10470</pages>
      <abstract>While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards language-based TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation.</abstract>
      <url hash="f798baf1">2024.acl-long.563</url>
      <bibkey>xiong-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.acl-long.563</doi>
    </paper>
    <paper id="564">
      <title>Learning Relational Decomposition of Queries for Question Answering from Tables</title>
      <author><first>Raphaël</first><last>Mouravieff</last></author>
      <author><first>Benjamin</first><last>Piwowarski</last><affiliation>CNRS / ISIR, Sorbonne Université and CNRS</affiliation></author>
      <author><first>Sylvain</first><last>Lamprier</last><affiliation>Université d’Angers</affiliation></author>
      <pages>10471-10485</pages>
      <abstract>Table Question-Answering involves both understanding the natural language query and grounding it in the context of the input table to extract relevant information. In this context, many methods have highlighted the benefits of intermediate pre-training using SQL queries. However, while most approaches aim at generating final answers directly from inputs, we claim that there is better to do with SQL queries during training.By learning to imitate a restricted subset of SQL-like algebraic operations, we demonstrate that their execution flow provides intermediate supervision steps that allow for increased generalization and structural reasoning compared to classical approaches. Our method, bridges the gap between semantic parsing and direct answering methods, offering valuable insights into which types of operations should be predicted by a generative architecture and which should be executed by an external algorithm. Our code can be found at https://github.com/RaphaelMouravieff/Partial-Exec.</abstract>
      <url hash="35122bb5">2024.acl-long.564</url>
      <bibkey>mouravieff-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.acl-long.564</doi>
    </paper>
    <paper id="565">
      <title>Characterizing Similarities and Divergences in Conversational Tones in Humans and <fixed-case>LLM</fixed-case>s by Sampling with People</title>
      <author><first>Dun-Ming</first><last>Huang</last></author>
      <author><first>Pol</first><last>Van Rijn</last><affiliation>Max-Planck Institute</affiliation></author>
      <author><first>Ilia</first><last>Sucholutsky</last><affiliation>Princeton University</affiliation></author>
      <author><first>Raja</first><last>Marjieh</last><affiliation>Princeton University</affiliation></author>
      <author><first>Nori</first><last>Jacoby</last></author>
      <pages>10486-10512</pages>
      <abstract>Conversational tones — the manners and attitudes in which speakers communicate — are essential to effective communication. As Large Language Models (LLMs) become increasingly popular, it is necessary to characterize the divergences in their conversational tones relative to humans. Prior research relied on pre-existing taxonomies or text corpora, which suffer from experimenter bias and may not be representative of real-world distributions. Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone. We run 50 iterations of this process with both human participants and GPT-4 and obtain a dataset of sentences and frequent conversational tones. In an additional experiment, humans and GPT-4 annotated all sentences with all tones. With data from 1,339 participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between tones in humans and GPT-4. This work showcases how combining ideas from machine learning and cognitive science can address challenges in human-computer interactions.</abstract>
      <url hash="c5726fe9">2024.acl-long.565</url>
      <bibkey>huang-etal-2024-characterizing</bibkey>
      <doi>10.18653/v1/2024.acl-long.565</doi>
    </paper>
    <paper id="566">
      <title><fixed-case>P</fixed-case>areto Optimal Learning for Estimating Large Language Model Errors</title>
      <author><first>Theodore</first><last>Zhao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mu</first><last>Wei</last><affiliation>Microsoft</affiliation></author>
      <author><first>J.</first><last>Preston</last></author>
      <author><first>Hoifung</first><last>Poon</last><affiliation>Microsoft</affiliation></author>
      <pages>10513-10529</pages>
      <abstract>Large Language Models (LLMs) have shown impressive abilities in many applications. When a concrete and precise answer is desired, it is important to have a quantitative estimation of the potential error rate. However, this can be challenging due to the text-in-text-out nature of the generative models. We present a method based on Pareto optimization that generates a risk score to estimate the probability of error in an LLM response by integrating multiple sources of information. We prove theoretically that the error estimator optimized in our framework aligns with the LLM and the information sources in an Pareto optimal manner. Experimental results show that the risk scores estimated by our method are well correlated with the true LLM error rate, thus facilitating error correction. By dynamically combining with prompting strategies such as self-verification and information retrieval, we demonstrate the proposed method can be utilized to increase the performance of an LLM, surpassing state-of-the-art task specific model.</abstract>
      <url hash="fad088cb">2024.acl-long.566</url>
      <bibkey>zhao-etal-2024-pareto</bibkey>
      <doi>10.18653/v1/2024.acl-long.566</doi>
    </paper>
    <paper id="567">
      <title>Simul-<fixed-case>LLM</fixed-case>: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models</title>
      <author><first>Victor</first><last>Agostinelli</last><affiliation>, Oregon State University</affiliation></author>
      <author><first>Max</first><last>Wild</last></author>
      <author><first>Matthew</first><last>Raffel</last></author>
      <author><first>Kazi</first><last>Fuad</last></author>
      <author><first>Lizhong</first><last>Chen</last><affiliation>Oregon State University</affiliation></author>
      <pages>10530-10541</pages>
      <abstract>Large language models (LLMs) with billions of parameters and pretrained on massive amounts of data are now capable of near or better than state-of-the-art performance in a variety of downstream natural language processing tasks. Neural machine translation (NMT) is one such task that LLMs have been applied to with great success. However, little research has focused on applying LLMs to the more difficult subset of NMT called simultaneous translation (SimulMT), where translation begins before the entire source context is available to the model. In this paper, we address key challenges facing LLMs fine-tuned for SimulMT, validate classical SimulMT concepts and practices in the context of LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT, and introduce Simul-LLM, the first open-source fine-tuning and evaluation pipeline development framework for LLMs focused on SimulMT.</abstract>
      <url hash="64a02b41">2024.acl-long.567</url>
      <bibkey>agostinelli-etal-2024-simul</bibkey>
      <doi>10.18653/v1/2024.acl-long.567</doi>
    </paper>
    <paper id="568">
      <title>Defending Against Alignment-Breaking Attacks via Robustly Aligned <fixed-case>LLM</fixed-case></title>
      <author><first>Bochuan</first><last>Cao</last></author>
      <author><first>Yuanpu</first><last>Cao</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Lu</first><last>Lin</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Jinghui</first><last>Chen</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>10542-10560</pages>
      <abstract>Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments on open-source large language models, we demonstrate that RA-LLM can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100% to around 10% or less.</abstract>
      <url hash="e3dd6a78">2024.acl-long.568</url>
      <bibkey>cao-etal-2024-defending</bibkey>
      <doi>10.18653/v1/2024.acl-long.568</doi>
    </paper>
    <paper id="569">
      <title>Interactive-<fixed-case>KBQA</fixed-case>: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models</title>
      <author><first>Guanming</first><last>Xiong</last></author>
      <author><first>Junwei</first><last>Bao</last></author>
      <author><first>Wen</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <pages>10561-10582</pages>
      <abstract>This study explores the realm of knowledge base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. Yet, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results on the WebQuestionsSP, ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of examples (shots). Importantly, our approach supports manual intervention, allowing for the iterative refinement of LLM outputs. By annotating a dataset with step-wise reasoning processes, we showcase our model’s adaptability and highlight its potential for contributing significant enhancements to the field.</abstract>
      <url hash="5bb68e39">2024.acl-long.569</url>
      <bibkey>xiong-etal-2024-interactive</bibkey>
      <doi>10.18653/v1/2024.acl-long.569</doi>
    </paper>
    <paper id="570">
      <title><fixed-case>LLM</fixed-case>s in the Imaginarium: Tool Learning through Simulated Trial and Error</title>
      <author><first>Boshi</first><last>Wang</last><affiliation>Ohio State University</affiliation></author>
      <author><first>Hao</first><last>Fang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jason</first><last>Eisner</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Yu</first><last>Su</last><affiliation>Ohio State University and Microsoft</affiliation></author>
      <pages>10583-10604</pages>
      <abstract>Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained. We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an LLM’s ‘imagination’ to simulate plausible scenarios for using a tool, after which the LLM interacts with the tool to learn from its execution feedback. Both short-term and long-term memory are employed to improve the depth and breadth of the exploration, respectively. Comprehensive experiments on ToolBench show that STE substantially improves tool learning for LLMs under both in-context learning and fine-tuning settings, bringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform GPT-4. We also show effective continual learning of tools via a simple experience replay strategy.</abstract>
      <url hash="efcca944">2024.acl-long.570</url>
      <bibkey>wang-etal-2024-llms-imaginarium</bibkey>
      <doi>10.18653/v1/2024.acl-long.570</doi>
    </paper>
    <paper id="571">
      <title><fixed-case>H</fixed-case>yper<fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>: Towards Better Mixture of Experts via Transferring Among Experts</title>
      <author><first>Hao</first><last>Zhao</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zihan</first><last>Qiu</last></author>
      <author><first>Huijia</first><last>Wu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zili</first><last>Wang</last></author>
      <author><first>Zhaofeng</first><last>He</last><affiliation>Beijing University of Post and Telecommunication</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>10605-10618</pages>
      <abstract>The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multiple datasets and backbones establish that HyperMoE significantly outperforms existing MoE methods under identical conditions concerning the number of experts. Our code is publicly available at https://github.com/Bumble666/Hyper_MoE</abstract>
      <url hash="c3ad8850">2024.acl-long.571</url>
      <bibkey>zhao-etal-2024-hypermoe</bibkey>
      <doi>10.18653/v1/2024.acl-long.571</doi>
    </paper>
    <paper id="572">
      <title>Aligning Large Language Models with Human Preferences through Representation Engineering</title>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Xiaohua</first><last>Wang</last></author>
      <author><first>Muling</first><last>Wu</last></author>
      <author><first>Tianlong</first><last>Li</last></author>
      <author><first>Changze</first><last>Lv</last></author>
      <author><first>Zixuan</first><last>Ling</last></author>
      <author><first>Zhu</first><last>JianHao</last></author>
      <author><first>Cenyuan</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xiaoqing</first><last>Zheng</last></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>10619-10638</pages>
      <abstract>Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involve employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses. Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in implementation. Drawing inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to implement. Extensive experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). RAHF’s versatility in accommodating diverse human preferences shows its potential for advancing LLM performance.</abstract>
      <url hash="a6e804f3">2024.acl-long.572</url>
      <bibkey>liu-etal-2024-aligning</bibkey>
      <doi>10.18653/v1/2024.acl-long.572</doi>
    </paper>
    <paper id="573">
      <title><fixed-case>CODIS</fixed-case>: Benchmarking Context-dependent Visual Comprehension for Multimodal Large Language Models</title>
      <author><first>Fuwen</first><last>Luo</last></author>
      <author><first>Chi</first><last>Chen</last></author>
      <author><first>Zihao</first><last>Wan</last></author>
      <author><first>Zhaolu</first><last>Kang</last></author>
      <author><first>Qidong</first><last>Yan</last></author>
      <author><first>Yingjie</first><last>Li</last></author>
      <author><first>Xiaolong</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Siyu</first><last>Wang</last></author>
      <author><first>Ziyue</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xiaoyue</first><last>Mi</last></author>
      <author><first>Peng</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ning</first><last>Ma</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>10639-10659</pages>
      <abstract>Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a context-dependent manner.</abstract>
      <url hash="ad938049">2024.acl-long.573</url>
      <bibkey>luo-etal-2024-codis</bibkey>
      <doi>10.18653/v1/2024.acl-long.573</doi>
    </paper>
    <paper id="574">
      <title><fixed-case>ARAIDA</fixed-case>: Analogical Reasoning-Augmented Interactive Data Annotation</title>
      <author><first>Chen</first><last>Huang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Yiping</first><last>Jin</last><affiliation>Pompeu Fabra University</affiliation></author>
      <author><first>Ilija</first><last>Ilievski</last></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Jiancheng</first><last>Lv</last><affiliation>Sichuan University</affiliation></author>
      <pages>10660-10675</pages>
      <abstract>Human annotation is a time-consuming task that requires a significant amount of effort. To address this issue, interactive data annotation utilizes an annotation model to provide suggestions for humans to approve or correct. However, annotation models trained with limited labeled data are prone to generating incorrect suggestions, leading to extra human correction effort. To tackle this challenge, we propose Araida, an analogical reasoning-based approach that enhances automatic annotation accuracy in the interactive data annotation setting and reduces the need for human corrections. Araida involves an error-aware integration strategy that dynamically coordinates an annotation model and a k-nearest neighbors (KNN) model, giving more importance to KNN’s predictions when predictions from the annotation model are deemed inaccurate. Empirical studies demonstrate that Araida is adaptable to different annotation tasks and models. On average, it reduces human correction labor by 11.02% compared to vanilla interactive data annotation methods.</abstract>
      <url hash="00c59d61">2024.acl-long.574</url>
      <bibkey>huang-etal-2024-araida</bibkey>
      <doi>10.18653/v1/2024.acl-long.574</doi>
    </paper>
    <paper id="575">
      <title><fixed-case>P</fixed-case>ol<fixed-case>CLIP</fixed-case>: A Unified Image-Text Word Sense Disambiguation Model via Generating Multimodal Complementary Representations</title>
      <author><first>Qihao</first><last>Yang</last><affiliation>South China Normal University</affiliation></author>
      <author><first>Yong</first><last>Li</last></author>
      <author><first>Xuelin</first><last>Wang</last><affiliation>Jinan University</affiliation></author>
      <author><first>Fu Lee</first><last>Wang</last><affiliation>Hong Kong Metropolitan University</affiliation></author>
      <author><first>Tianyong</first><last>Hao</last></author>
      <pages>10676-10690</pages>
      <abstract>Word sense disambiguation (WSD) can be viewed as two subtasks: textual word sense disambiguation (Textual-WSD) and visual word sense disambiguation (Visual-WSD). They aim to identify the most semantically relevant senses or images to a given context containing ambiguous target words. However, existing WSD models seldom address these two subtasks jointly due to lack of images in Textual-WSD datasets or lack of senses in Visual-WSD datasets. To bridge this gap, we propose PolCLIP, a unified image-text WSD model. By employing an image-text complementarity strategy, it not only simulates stable diffusion models to generate implicit visual representations for word senses but also simulates image captioning models to provide implicit textual representations for images. Additionally, a disambiguation-oriented image-sense dataset is constructed for the training objective of learning multimodal polysemy representations. To the best of our knowledge, PolCLIP is the first model that can cope with both Textual-WSD and Visual-WSD. Extensive experimental results on benchmarks demonstrate the effectiveness of our method, achieving a 2.53% F1-score increase over the state-of-the-art models on Textual-WSD and a 2.22% HR@1 improvement on Visual-WSD.</abstract>
      <url hash="eb6c9b9b">2024.acl-long.575</url>
      <bibkey>yang-etal-2024-polclip</bibkey>
      <doi>10.18653/v1/2024.acl-long.575</doi>
    </paper>
    <paper id="576">
      <title>Prompted Aspect Key Point Analysis for Quantitative Review Summarization</title>
      <author><first>An</first><last>Tang</last></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Minh</first><last>Dinh</last></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>10691-10708</pages>
      <abstract>Key Point Analysis (KPA) aims for quantitative summarization that provides key points (KPs) as succinct textual summaries and quantities measuring their prevalence. KPA studies for arguments and reviews have been reported in the literature. A majority of KPA studies for reviews adopt supervised learning to extract short sentences as KPs before matching KPs to review comments for quantification of KP prevalence. Recent abstractive approaches still generate KPs based on sentences, often leading to KPs with overlapping and hallucinated opinions, and inaccurate quantification. In this paper, we propose Prompted Aspect Key Point Analysis (PAKPA) for quantitative review summarization. PAKPA employs aspect sentiment analysis and prompted in-context learning with Large Language Models (LLMs) to generate and quantify KPs grounded in aspects for business entities, which achieves faithful KPs with accurate quantification, and removes the need for large amounts of annotated data for supervised training. Experiments on the popular review dataset Yelp and the aspect-oriented review summarization dataset SPACE show that our framework achieves state-of-the-art performance. Source code and data are available at: https://github.com/antangrocket1312/PAKPA</abstract>
      <url hash="08ed48ae">2024.acl-long.576</url>
      <bibkey>tang-etal-2024-prompted</bibkey>
      <doi>10.18653/v1/2024.acl-long.576</doi>
    </paper>
    <paper id="577">
      <title>Ask Again, Then Fail: Large Language Models’ Vacillations in Judgment</title>
      <author><first>Qiming</first><last>Xie</last></author>
      <author><first>Zengzhi</first><last>Wang</last></author>
      <author><first>Yi</first><last>Feng</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Rui</first><last>Xia</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <pages>10709-10745</pages>
      <abstract>We observe that current large language models often waver in their judgments when faced with follow-up questions, even if the original judgment was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current large language models. Furthermore, to mitigate this issue, we explore various prompting strategies for closed-source models, and develop a training-based framework Unwavering-FQ that teaches large language models to maintain their originally correct judgments through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of large language models.</abstract>
      <url hash="a0ac884f">2024.acl-long.577</url>
      <bibkey>xie-etal-2024-ask</bibkey>
      <doi>10.18653/v1/2024.acl-long.577</doi>
    </paper>
    <paper id="578">
      <title><fixed-case>CLAMBER</fixed-case>: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models</title>
      <author><first>Tong</first><last>Zhang</last></author>
      <author><first>Peixin</first><last>Qin</last></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Chen</first><last>Huang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Junhong</first><last>Liu</last></author>
      <author><first>Dingnan</first><last>Jin</last></author>
      <author><first>Hongru</first><last>Liang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>10746-10766</pages>
      <abstract>Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct 12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs.Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge.In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs.</abstract>
      <url hash="a000e74f">2024.acl-long.578</url>
      <bibkey>zhang-etal-2024-clamber</bibkey>
      <doi>10.18653/v1/2024.acl-long.578</doi>
    </paper>
    <paper id="579">
      <title>Multimodal Reasoning with Multimodal Knowledge Graph</title>
      <author><first>Junlin</first><last>Lee</last></author>
      <author><first>Yequan</first><last>Wang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>10767-10782</pages>
      <abstract>Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, MR-MKG achieves superior performance while training on only a small fraction of parameters, approximately 2.25% of the LLM’s parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR-MKG method outperforms previous state-of-the-art models.</abstract>
      <url hash="a2310e23">2024.acl-long.579</url>
      <bibkey>lee-etal-2024-multimodal</bibkey>
      <doi>10.18653/v1/2024.acl-long.579</doi>
    </paper>
    <paper id="580">
      <title>Confidence is not Timeless: Modeling Temporal Validity for Rule-based Temporal Knowledge Graph Forecasting</title>
      <author><first>Rikui</first><last>Huang</last></author>
      <author><first>Wei</first><last>Wei</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Xiaoye</first><last>Qu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Shengzhe</first><last>Zhang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Dangyang</first><last>Chen</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>10783-10794</pages>
      <abstract>Recently, Temporal Knowledge Graph Forecasting (TKGF) has emerged as a pivotal domain for forecasting future events. Unlike black-box neural network methods, rule-based approaches are lauded for their efficiency and interpretability. For this line of work, it is crucial to correctly estimate the predictive effectiveness of the rules, i.e., the confidence. However, the existing literature lacks in-depth investigation into how confidence evolves with time. Moreover, inaccurate and heuristic confidence estimation limits the performance of rule-based methods. To alleviate such issues, we propose a framework named <b>TempValid</b> to explicitly model the temporal validity of rules for TKGF. Specifically, we design a time function to model the interaction between temporal information with confidence. TempValid conceptualizes confidence and other coefficients as learnable parameters to avoid inaccurate estimation and combinatorial explosion. Furthermore, we introduce a <i>rule-adversarial negative sampling</i> and a <i>time-aware negative sampling</i> strategies to facilitate TempValid learning. Extensive experiments show that TempValid significantly outperforms previous state-of-the-art (SOTA) rule-based methods on six TKGF datasets. Moreover, it exhibits substantial advancements in cross-domain and resource-constrained rule learning scenarios.</abstract>
      <url hash="50ade229">2024.acl-long.580</url>
      <bibkey>huang-etal-2024-confidence</bibkey>
      <doi>10.18653/v1/2024.acl-long.580</doi>
    </paper>
    <paper id="581">
      <title><fixed-case>CARE</fixed-case>: A Clue-guided Assistant for <fixed-case>CSR</fixed-case>s to Read User Manuals</title>
      <author><first>Weihong</first><last>Du</last></author>
      <author><first>Jia</first><last>Liu</last></author>
      <author><first>Zujie</first><last>Wen</last></author>
      <author><first>Dingnan</first><last>Jin</last></author>
      <author><first>Hongru</first><last>Liang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <pages>10795-10811</pages>
      <abstract>It is time-saving to build a reading assistant for customer service representations (CSRs) when reading user manuals, especially information-rich ones. Current solutions don’t fit the online custom service scenarios well due to the lack of attention to user questions and possible responses. Hence, we propose to develop a time-saving and careful reading assistant for CSRs, named CARE. It can help the CSRs quickly find proper responses from the user manuals via explicit clue chains. Specifically, each of the clue chains is formed by inferring over the user manuals, starting from the question clue aligned with the user question and ending at a possible response. To overcome the shortage of supervised data, we adopt the self-supervised strategy for model learning. The offline experiment shows that CARE is efficient in automatically inferring accurate responses from the user manual. The online experiment further demonstrates the superiority of CARE to reduce CSRs’ reading burden and keep high service quality, in particular with &gt;35% decrease in time spent and keeping a &gt;0.75 ICC score.</abstract>
      <url hash="27f77ecd">2024.acl-long.581</url>
      <bibkey>du-etal-2024-care</bibkey>
      <doi>10.18653/v1/2024.acl-long.581</doi>
    </paper>
    <paper id="582">
      <title>Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes</title>
      <author><first>Dingzirui</first><last>Wang</last></author>
      <author><first>Longxu</first><last>Dou</last></author>
      <author><first>Xuanliang</first><last>Zhang</last></author>
      <author><first>Qingfu</first><last>Zhu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>10812-10828</pages>
      <abstract>Numerical reasoning is an essential ability for NLP systems to handle numeric information. Recent research indicates that fine-tuning a small-scale model to learn generating reasoning processes alongside answers can significantly enhance performance. However, current methods have the limitation that most methods generate reasoning processes with large language models (LLMs), which are “unreliable” since such processes could contain information unrelated to the answer. To address this limitation, we introduce enhancing numerical reasoning with reliable processes (Encore), which derives the reliable reasoning process by decomposing the answer formula, ensuring which fully supports the answer. Nevertheless, models could lack enough data to learn the reasoning process generation adequately, since our method generates only one single reasoning process for one formula. To overcome this difficulty, we present a series of pre-training tasks to help models learn the reasoning process generation with synthesized data. The experiments show that Encore yields improvement on all five experimental datasets with an average of 1.8%, proving the effectiveness of our method.</abstract>
      <url hash="98ce91a6">2024.acl-long.582</url>
      <bibkey>wang-etal-2024-enhancing-numerical</bibkey>
      <doi>10.18653/v1/2024.acl-long.582</doi>
    </paper>
    <paper id="583">
      <title><fixed-case>PAGED</fixed-case>: A Benchmark for Procedural Graphs Extraction from Documents</title>
      <author><first>Weihong</first><last>Du</last></author>
      <author><first>Wenrui</first><last>Liao</last></author>
      <author><first>Hongru</first><last>Liang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <pages>10829-10846</pages>
      <abstract>Automatic extraction of procedural graphs from documents creates a low-cost way for users to easily understand a complex procedure by skimming visual graphs. Despite the progress in recent studies, it remains unanswered: whether the existing studies have well solved this task (Q1) and whether the emerging large language models (LLMs) can bring new opportunities to this task (Q2). To this end, we propose a new benchmark PAGED, equipped with a large high-quality dataset and standard evaluations. It investigates five state-of-the-art baselines, revealing that they fail to extract optimal procedural graphs well because of their heavy reliance on hand-written rules and limited available data. We further involve three advanced LLMs in PAGED and enhance them with a novel self-refine strategy. The results point out the advantages of LLMs in identifying textual elements and their gaps in building logical structures. We hope PAGED can serve as a major landmark for automatic procedural graph extraction and the investigations in PAGED can offer insights into the research on logic reasoning among non-sequential elements.</abstract>
      <url hash="5e187e6c">2024.acl-long.583</url>
      <bibkey>du-etal-2024-paged</bibkey>
      <doi>10.18653/v1/2024.acl-long.583</doi>
    </paper>
    <paper id="584">
      <title>Navigating the Shadows: Unveiling Effective Disturbances for <fixed-case>M</fixed-case>odern <fixed-case>AI</fixed-case> Content Detectors</title>
      <author><first>Ying</first><last>Zhou</last></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>10847-10861</pages>
      <abstract>With the launch of ChatGPT, large language models (LLMs) have attracted global attention. In the realm of article writing, LLMs have witnessed extensive utilization, giving rise to concerns related to intellectual property protection, personal privacy, and academic integrity. In response, AI-text detection has emerged to distinguish between human and machine-generated content. However, recent research indicates that these detection systems often lack robustness and struggle to effectively differentiate perturbed texts. Currently, there is a lack of systematic evaluations regarding detection performance in real-world applications, and a comprehensive examination of perturbation techniques and detector robustness is also absent. To bridge this gap, our work simulates real-world scenarios in both informal and professional writing, exploring the out-of-the-box performance of current detectors. Additionally, we have constructed 12 black-box text perturbation methods to assess the robustness of current detection models across various perturbation granularities. Furthermore, through adversarial learning experiments, we investigate the impact of perturbation data augmentation on the robustness of AI-text detectors. We have released our code and data at https://github.com/zhouying20/ai-text-detector-evaluation.</abstract>
      <url hash="982dce29">2024.acl-long.584</url>
      <bibkey>zhou-etal-2024-navigating</bibkey>
      <doi>10.18653/v1/2024.acl-long.584</doi>
    </paper>
    <paper id="585">
      <title><fixed-case>RAGT</fixed-case>ruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models</title>
      <author><first>Cheng</first><last>Niu</last></author>
      <author><first>Yuanhao</first><last>Wu</last><affiliation>Newsbreak</affiliation></author>
      <author><first>Juno</first><last>Zhu</last></author>
      <author><first>Siliang</first><last>Xu</last></author>
      <author><first>KaShun</first><last>Shum</last></author>
      <author><first>Randy</first><last>Zhong</last></author>
      <author><first>Juntong</first><last>Song</last></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>10862-10878</pages>
      <abstract>Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual case and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. We show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive hallucination detection performance when compared to the existing prompt-based approaches using state-of-the-art LLMs such as GPT-4. Furthermore, the finetuned model can effectively mitigate hallucination in LLM responses.</abstract>
      <url hash="1071efb3">2024.acl-long.585</url>
      <bibkey>niu-etal-2024-ragtruth</bibkey>
      <doi>10.18653/v1/2024.acl-long.585</doi>
    </paper>
    <paper id="586">
      <title>The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models</title>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Jie</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ruiyang</first><last>Ren</last></author>
      <author><first>Xiaoxue</first><last>Cheng</last></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jian-Yun</first><last>Nie</last><affiliation>University of Montreal</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>10879-10899</pages>
      <abstract>In the era of large language models (LLMs), hallucination (the tendency to generate factually incorrect content) poses great challenges to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucinations, focused on the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and design a simple yet effective detection method for LLM hallucinations. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucinations. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs.</abstract>
      <url hash="f9d6f684">2024.acl-long.586</url>
      <bibkey>li-etal-2024-dawn</bibkey>
      <doi>10.18653/v1/2024.acl-long.586</doi>
    </paper>
    <paper id="587">
      <title>Revisiting Knowledge Distillation for Autoregressive Language Models</title>
      <author><first>Qihuang</first><last>Zhong</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Li</first><last>Shen</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Juhua</first><last>Liu</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Bo</first><last>Du</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>University of Sydney</affiliation></author>
      <pages>10900-10913</pages>
      <abstract>Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve the student model generalization effectively.</abstract>
      <url hash="01aee1d6">2024.acl-long.587</url>
      <bibkey>zhong-etal-2024-revisiting</bibkey>
      <doi>10.18653/v1/2024.acl-long.587</doi>
    </paper>
    <paper id="588">
      <title>Continual Learning with Semi-supervised Contrastive Distillation for Incremental Neural Machine Translation</title>
      <author><first>Yunlong</first><last>Liang</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jiaan</first><last>Wang</last><affiliation>Soochow University</affiliation></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>10914-10928</pages>
      <abstract>Incrementally expanding the capability of an existing translation model to solve new domain tasks over time is a fundamental and practical problem, which usually suffers from catastrophic forgetting. Generally, multi-domain learning can be seen as a good solution. However, there are two drawbacks: 1) it requires having the training data for all domains available at the same time, which may be unrealistic due to storage or privacy concerns; 2) it requires re-training the model on the data of all domains from scratch when adding a new domain and this is time-consuming and computationally expensive. To address these issues, we present a semi-supervised contrastive distillation framework for incremental neural machine translation. Specifically, to avoid catastrophic forgetting, we propose to exploit unlabeled data from the same distributions of the older domains through knowledge distillation. Further, to ensure the distinct domain characteristics in the model as the number of domains increases, we devise a cross-domain contrastive objective to enhance the distilled knowledge. Extensive experiments on domain translation benchmarks show that our approach, without accessing any previous training data or re-training on all domains from scratch, can significantly prevent the model from forgetting previously learned knowledge while obtaining good performance on the incrementally added domains. The code and data with step-by-step instructions will be released upon acceptance.</abstract>
      <url hash="910c76c8">2024.acl-long.588</url>
      <bibkey>liang-etal-2024-continual</bibkey>
      <doi>10.18653/v1/2024.acl-long.588</doi>
    </paper>
    <paper id="589">
      <title>Make-A-Voice: Revisiting Voice Large Language Models as Scalable Multilingual and Multitask Learners</title>
      <author><first>Rongjie</first><last>Huang</last><affiliation>FAIR</affiliation></author>
      <author><first>Chunlei</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yongqi</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Dongchao</first><last>Yang</last></author>
      <author><first>Jinchuan</first><last>Tian</last></author>
      <author><first>Zhenhui</first><last>Ye</last></author>
      <author><first>Luping</first><last>Liu</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Zehan</first><last>Wang</last></author>
      <author><first>Ziyue</first><last>Jiang</last></author>
      <author><first>Xuankai</first><last>Chang</last></author>
      <author><first>Jiatong</first><last>Shi</last></author>
      <author><first>Chao</first><last>Weng</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>10929-10942</pages>
      <abstract>Large language models (LLMs) have successfully served as a general-purpose interface across multiple tasks and languages, while the adaptation of voice LLMs is mostly designed for specific purposes (either single-task or monolingual), where the advantages of LLMs especially for low-resource language processing and zero-shot task generalization are less exploited in the audio community. To bridge the gap, we introduce Make-A-Voice as a multi-modal voice LLM and conduct a comprehensive study on its capability to deal with multiple tasks/languages. When trained on ~200K hours of 6-language data for 4 voice generation applications, Make-A-Voice emerges notable advantages: 1) as scalable learners to improve performance with end-to-end local and global multiscale transformers; and 2) as multitask learners by adjusting prompts to share common knowledge across modalities (speech/singing) and present in-context learning abilities by generalizing to unseen tasks not explicitly train on; 3) as multilingual learners to alleviate data scarcity of low-resource languages by including rich-resource language training data. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models in monolingual/cross-lingual voice generation. Audio samples are available at https://M-Voice.github.io</abstract>
      <url hash="5cf56a7c">2024.acl-long.589</url>
      <bibkey>huang-etal-2024-make</bibkey>
      <doi>10.18653/v1/2024.acl-long.589</doi>
    </paper>
    <paper id="590">
      <title>Chat Vector: A Simple Approach to Equip <fixed-case>LLM</fixed-case>s with Instruction Following and Model Alignment in New Languages</title>
      <author><first>Shih-Cheng</first><last>Huang</last><affiliation>Appier Inc.</affiliation></author>
      <author><first>Pin-Zu</first><last>Li</last><affiliation>National Applied Research Laboratories</affiliation></author>
      <author><first>Yu-chi</first><last>Hsu</last></author>
      <author><first>Kuang-Ming</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Yu Tung</first><last>Lin</last></author>
      <author><first>Shih-Kai</first><last>Hsiao</last><affiliation>National Central University</affiliation></author>
      <author><first>Richard</first><last>Tsai</last><affiliation>National Central University</affiliation></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <pages>10943-10959</pages>
      <abstract>Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of <tex-math>\textit{chat vector}</tex-math> to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model’s weights, we can endow the model with chat capabilities in new languages without the need for further training.Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our approach, we extend our experiments to encompass various languages, base models, and chat vectors. The results underscore the chat vector’s simplicity, effectiveness, and wide applicability, making it a compelling solution for efficiently enabling conversational capabilities in pre-trained language models. Our code is available at https://github.com/aqweteddy/ChatVector.</abstract>
      <url hash="1abf0605">2024.acl-long.590</url>
      <bibkey>huang-etal-2024-chat</bibkey>
      <doi>10.18653/v1/2024.acl-long.590</doi>
    </paper>
    <paper id="591">
      <title><fixed-case>PRP</fixed-case>: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails</title>
      <author><first>Neal</first><last>Mangaokar</last></author>
      <author><first>Ashish</first><last>Hooda</last><affiliation>Department of Computer Science, University of Wisconsin - Madison</affiliation></author>
      <author><first>Jihye</first><last>Choi</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Shreyas</first><last>Chandrashekaran</last></author>
      <author><first>Kassem</first><last>Fawaz</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Somesh</first><last>Jha</last></author>
      <author><first>Atul</first><last>Prakash</last><affiliation>University of Michigan</affiliation></author>
      <pages>10960-10976</pages>
      <abstract>Large language models (LLMs) are typically aligned to be harmless to humans. Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content. More recent LLMs often incorporate an additional layer of defense, a Guard Model, which is a second LLM that is designed to check and moderate the output response of the primary LLM. Our key contribution is to show a novel attack strategy, PRP, that is successful against several open-source (e.g., Llama 2) and closed-source (e.g., GPT 3.5) implementations of Guard Models. PRP leverages a two step prefix-based attack that operates by (a) constructing a universal adversarial prefix for the Guard Model, and (b) propagating this prefix to the response. We find that this procedure is effective across multiple threat models, including ones in which the adversary has no access to the Guard Model at all. Our work suggests that further advances are required on defenses and Guard Models before they can be considered effective. Code at https://github.com/AshishHoodaIITD/prp-llm-guard-rail-attack.</abstract>
      <url hash="a0c48df1">2024.acl-long.591</url>
      <bibkey>mangaokar-etal-2024-prp</bibkey>
      <doi>10.18653/v1/2024.acl-long.591</doi>
    </paper>
    <paper id="592">
      <title>Hide and Seek in Noise Labels: Noise-Robust Collaborative Active Learning with <fixed-case>LLM</fixed-case>s-Powered Assistance</title>
      <author><first>Bo</first><last>Yuan</last></author>
      <author><first>Yulin</first><last>Chen</last></author>
      <author><first>Yin</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Wei</first><last>Jiang</last></author>
      <pages>10977-11011</pages>
      <abstract>Learning from noisy labels (LNL) is a challenge that arises in many real-world scenarios where collected training data can contain incorrect or corrupted labels. Most existing solutions identify noisy labels and adopt active learning to query human experts on them for denoising. In the era of large language models (LLMs), although we can reduce the human effort to improve these methods, their performances are still subject to accurately separating the clean and noisy samples from noisy data. In this paper, we propose an innovative collaborative learning framework NoiseAL based on active learning to combine LLMs and small models (SMs) for learning from noisy labels. During collaborative training, we first adopt two SMs to form a co-prediction network and propose a dynamic-enhanced threshold strategy to divide the noisy data into different subsets, then select the clean and noisy samples from these subsets to feed the active annotator LLMs to rectify noisy samples. Finally, we employ different optimization objectives to conquer subsets with different degrees of label noises. Extensive experiments on synthetic and real-world noise datasets further demonstrate the superiority of our framework over state-of-the-art baselines.</abstract>
      <url hash="44dc5cf5">2024.acl-long.592</url>
      <bibkey>yuan-etal-2024-hide</bibkey>
      <doi>10.18653/v1/2024.acl-long.592</doi>
    </paper>
    <paper id="593">
      <title><fixed-case>CLOMO</fixed-case>: Counterfactual Logical Modification with Large Language Models</title>
      <author><first>Yinya</first><last>Huang</last></author>
      <author><first>Ruixin</first><last>Hong</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Shao</last></author>
      <author><first>Zhicheng</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Changshui</first><last>Zhang</last><affiliation>Tsinghua University and Department of Computer Science and Technology</affiliation></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <author><first>Linqi</first><last>Song</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>11012-11034</pages>
      <abstract>In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model’s counterfactual capabilities, we propose an innovative evaluation metric, the decomposed Self-Evaluation Score (SES) to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance. Code and data are available at https://github.com/Eleanor-H/CLOMO.</abstract>
      <url hash="c1c1009f">2024.acl-long.593</url>
      <bibkey>huang-etal-2024-clomo</bibkey>
      <doi>10.18653/v1/2024.acl-long.593</doi>
    </paper>
    <paper id="594">
      <title>Exploring Hybrid Question Answering via Program-based Prompting</title>
      <author><first>Qi</first><last>Shi</last></author>
      <author><first>Han</first><last>Cui</last></author>
      <author><first>Haofeng</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Qingfu</first><last>Zhu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>11035-11046</pages>
      <abstract>Question answering over heterogeneous data requires reasoning over diverse sources of data, which is challenging due to the large scale of information and organic coupling of heterogeneous data. Various approaches have been proposed to address these challenges. One approach involves training specialized retrievers to select relevant information, thereby reducing the input length. Another approach is to transform diverse modalities of data into a single modality, simplifying the task difficulty and enabling more straightforward processing. In this paper, we propose HProPro, a novel program-based prompting framework for the hybrid question answering task. HProPro follows the code generation and execution paradigm. In addition, HProPro integrates various functions to tackle the hybrid reasoning scenario. Specifically, HProPro contains function declaration and function implementation to perform hybrid information-seeking over data from various sources and modalities, which enables reasoning over such data without training specialized retrievers or performing modal transformations. Experimental results on two typical hybrid question answering benchmarks HybridQA and MultiModalQA demonstrate the effectiveness of HProPro: it surpasses all baseline systems and achieves the best performances in the few-shot settings on both datasets.</abstract>
      <url hash="262cf62a">2024.acl-long.594</url>
      <bibkey>shi-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.acl-long.594</doi>
    </paper>
    <paper id="595">
      <title><fixed-case>I</fixed-case>ndic<fixed-case>G</fixed-case>en<fixed-case>B</fixed-case>ench: A Multilingual Benchmark to Evaluate Generation Capabilities of <fixed-case>LLM</fixed-case>s on <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Harman</first><last>Singh</last></author>
      <author><first>Nitish</first><last>Gupta</last><affiliation>Google</affiliation></author>
      <author><first>Shikhar</first><last>Bharadwaj</last></author>
      <author><first>Dinesh</first><last>Tewari</last></author>
      <author><first>Partha</first><last>Talukdar</last><affiliation>Google Research and Indian Institute of Science, Bangalore</affiliation></author>
      <pages>11047-11073</pages>
      <abstract>As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench — the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate stateof-the-art LLMs like GPT-3.5, GPT-4, PaLM2, and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench isavailable at www.github.com/google-researchdatasets/indic-gen-bench</abstract>
      <url hash="9da08e56">2024.acl-long.595</url>
      <bibkey>singh-etal-2024-indicgenbench</bibkey>
      <doi>10.18653/v1/2024.acl-long.595</doi>
    </paper>
    <paper id="596">
      <title>Simple but Effective Compound Geometric Operations for Temporal Knowledge Graph Completion</title>
      <author><first>Rui</first><last>Ying</last></author>
      <author><first>Mengting</first><last>Hu</last><affiliation>Nankai University</affiliation></author>
      <author><first>Jianfeng</first><last>Wu</last></author>
      <author><first>Yalan</first><last>Xie</last><affiliation>Nankai University</affiliation></author>
      <author><first>Xiaoyi</first><last>Liu</last></author>
      <author><first>Zhunheng</first><last>Wang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Ming</first><last>Jiang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Hang</first><last>Gao</last><affiliation>Tianjin University of Science and Technology</affiliation></author>
      <author><first>Linlin</first><last>Zhang</last><affiliation>China Automotive Standardization Research Institute, China Automotive Technology and Research Center Co., Ltd.</affiliation></author>
      <author><first>Renhong</first><last>Cheng</last></author>
      <pages>11074-11086</pages>
      <abstract>Temporal knowledge graph completion aims to infer the missing facts in temporal knowledge graphs. Current approaches usually embed factual knowledge into continuous vector space and apply geometric operations to learn potential patterns in temporal knowledge graphs. However, these methods only adopt a single operation, which may have limitations in capturing the complex temporal dynamics present in temporal knowledge graphs. Therefore, we propose a simple but effective method, i.e. TCompoundE, which is specially designed with two geometric operations, including time-specific and relation-specific operations. We provide mathematical proofs to demonstrate the ability of TCompoundE to encode various relation patterns. Experimental results show that our proposed model significantly outperforms existing temporal knowledge graph embedding models. Our code is available at https://github.com/nk-ruiying/TCompoundE.</abstract>
      <url hash="863d0a5c">2024.acl-long.596</url>
      <bibkey>ying-etal-2024-simple</bibkey>
      <doi>10.18653/v1/2024.acl-long.596</doi>
    </paper>
    <paper id="597">
      <title>Uncertainty Aware Learning for Language Model Alignment</title>
      <author><first>Yikun</first><last>Wang</last></author>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>University of Sydney</affiliation></author>
      <pages>11087-11099</pages>
      <abstract>As instruction-tuned large language models (LLMs) evolve, aligning pretrained foundation models presents increasing challenges. Existing alignment strategies, which typically leverage diverse and high-quality data sources, often overlook the intrinsic uncertainty of tasks, learning all data samples equally. This may lead to suboptimal data efficiency and model performance. In response, we propose uncertainty-aware learning (UAL) to improve the model alignment of different task scenarios, by introducing the sample uncertainty (elicited from more capable LLMs). We implement UAL by a simple fashion – adaptively setting the label smoothing value of training according to the uncertainty of individual samples. Analysis shows that our UAL indeed facilitates better token clustering in the feature space, validating our hypothesis. Extensive experiments on widely used benchmarks demonstrate that our UAL significantly and consistently outperforms standard supervised fine-tuning. Notably, LLMs aligned in a mixed scenario have achieved an average improvement of 10.62% on high-entropy tasks (i.e., AlpacaEval leaderboard), and 1.81% on complex low-entropy tasks (i.e., MetaMath and GSM8K).</abstract>
      <url hash="3c982827">2024.acl-long.597</url>
      <bibkey>wang-etal-2024-uncertainty</bibkey>
      <doi>10.18653/v1/2024.acl-long.597</doi>
    </paper>
    <paper id="598">
      <title>Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models</title>
      <author><first>Ying-Chun</first><last>Lin</last><affiliation>Purdue University</affiliation></author>
      <author><first>Jennifer</first><last>Neville</last><affiliation>Purdue University and Purdue University</affiliation></author>
      <author><first>Jack</first><last>Stokes</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Longqi</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tara</first><last>Safavi</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Mengting</first><last>Wan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Scott</first><last>Counts</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Siddharth</first><last>Suri</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Reid</first><last>Andersen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xiaofeng</first><last>Xu</last></author>
      <author><first>Deepak</first><last>Gupta</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sujay Kumar</first><last>Jauhar</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Xia</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <author><first>Georg</first><last>Buscher</last></author>
      <author><first>Saurabh</first><last>Tiwary</last></author>
      <author><first>Brent</first><last>Hecht</last><affiliation>Northwestern University and Northwestern University, Northwestern University</affiliation></author>
      <author><first>Jaime</first><last>Teevan</last><affiliation>University of Washington and Research, Microsoft</affiliation></author>
      <pages>11100-11115</pages>
      <abstract>Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. Our proposed method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.</abstract>
      <url hash="64eb9f28">2024.acl-long.598</url>
      <bibkey>lin-etal-2024-interpretable</bibkey>
      <doi>10.18653/v1/2024.acl-long.598</doi>
    </paper>
    <paper id="599">
      <title>Fundamental Capabilities of Large Language Models and their Applications in Domain Scenarios: A Survey</title>
      <author><first>Jiawei</first><last>Li</last></author>
      <author><first>Yizhe</first><last>Yang</last></author>
      <author><first>Yu</first><last>Bai</last></author>
      <author><first>Xiaofeng</first><last>Zhou</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yinghao</first><last>Li</last></author>
      <author><first>Huashan</first><last>Sun</last></author>
      <author><first>Yuhang</first><last>Liu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Xingpeng</first><last>Si</last></author>
      <author><first>Yuhao</first><last>Ye</last></author>
      <author><first>Yixiao</first><last>Wu</last></author>
      <author><first>林一冠</first><last>林一冠</last></author>
      <author><first>Bin</first><last>Xu</last></author>
      <author><first>Ren</first><last>Bowen</last></author>
      <author><first>Chong</first><last>Feng</last></author>
      <author><first>Yang</first><last>Gao</last></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>11116-11141</pages>
      <abstract>Large Language Models (LLMs) demonstrate significant value in domain-specific applications, benefiting from their fundamental capabilities. Nevertheless, it is still unclear which fundamental capabilities contribute to success in specific domains. Moreover, the existing benchmark-based evaluation cannot effectively reflect the performance of real-world applications. In this survey, we review recent advances of LLMs in domain applications, aiming to summarize the fundamental capabilities and their collaboration. Furthermore, we establish connections between fundamental capabilities and specific domains, evaluating the varying importance of different capabilities. Based on our findings, we propose a reliable strategy for domains to choose more robust backbone LLMs for real-world applications.</abstract>
      <url hash="b2fa6890">2024.acl-long.599</url>
      <bibkey>li-etal-2024-fundamental</bibkey>
      <doi>10.18653/v1/2024.acl-long.599</doi>
    </paper>
    <paper id="600">
      <title>Measuring Political Bias in Large Language Models: What Is Said and How It Is Said</title>
      <author><first>Yejin</first><last>Bang</last></author>
      <author><first>Delong</first><last>Chen</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Pascale</first><last>Fung</last><affiliation>HKUST</affiliation></author>
      <pages>11142-11159</pages>
      <abstract>We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable.</abstract>
      <url hash="1fad8957">2024.acl-long.600</url>
      <bibkey>bang-etal-2024-measuring</bibkey>
      <doi>10.18653/v1/2024.acl-long.600</doi>
    </paper>
    <paper id="601">
      <title>Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use</title>
      <author><first>Yuhan</first><last>Chen</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Ang</first><last>Lv</last></author>
      <author><first>Ting-En</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Changyu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yuchuan</first><last>Wu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>11160-11174</pages>
      <abstract>In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM’s awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchmark, our method elevates a 7B model to achieve state-of-the-art performance, comparable to that of GPT-4. On other benchmarks and some RAG tasks, which also demand a thorough understanding of contextual content, Attention Buckets also exhibited notable enhancements in performance.</abstract>
      <url hash="ef14dd0b">2024.acl-long.601</url>
      <bibkey>chen-etal-2024-fortify</bibkey>
      <doi>10.18653/v1/2024.acl-long.601</doi>
    </paper>
    <paper id="602">
      <title>Layer-Condensed <fixed-case>KV</fixed-case> Cache for Efficient Inference of Large Language Models</title>
      <author><first>Haoyi</first><last>Wu</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>Kewei</first><last>Tu</last><affiliation>ShanghaiTech University</affiliation></author>
      <pages>11175-11188</pages>
      <abstract>Huge memory consumption has been a major bottleneck for deploying high-throughput large language models in real-world applications. In addition to the large number of parameters, the key-value (KV) cache for the attention mechanism in the transformer architecture consumes a significant amount of memory, especially when the number of layers is large for deep language models. In this paper, we propose a novel method that only computes and caches the KVs of a small number of layers, thus significantly saving memory consumption and improving inference throughput. Our experiments on large language models show that our method achieves up to 26<tex-math>\times</tex-math> higher throughput than standard transformers and competitive performance in language modeling and downstream tasks. In addition, our method is orthogonal to existing transformer memory-saving techniques, so it is straightforward to integrate them with our model, achieving further improvement in inference efficiency. Our code is available at https://github.com/whyNLP/LCKV.</abstract>
      <url hash="7ad55a06">2024.acl-long.602</url>
      <bibkey>wu-tu-2024-layer</bibkey>
      <doi>10.18653/v1/2024.acl-long.602</doi>
    </paper>
    <paper id="603">
      <title>Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages</title>
      <author><first>Yuanchi</first><last>Zhang</last></author>
      <author><first>Yile</first><last>Wang</last></author>
      <author><first>Zijun</first><last>Liu</last></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Xiaolong</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Peng</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>11189-11204</pages>
      <abstract>While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on Self-Distillation from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages (English and French) across various comprehension and generation tasks, experimental results demonstrate that SDRRL can significantly enhance multilingual capabilities while minimizing the impact on original performance in resource-rich languages.</abstract>
      <url hash="4f141858">2024.acl-long.603</url>
      <bibkey>zhang-etal-2024-enhancing-multilingual</bibkey>
      <doi>10.18653/v1/2024.acl-long.603</doi>
    </paper>
    <paper id="604">
      <title>Benchmarking <fixed-case>C</fixed-case>hinese Commonsense Reasoning of <fixed-case>LLM</fixed-case>s: From <fixed-case>C</fixed-case>hinese-Specifics to Reasoning-Memorization Correlations</title>
      <author><first>Jiaxing</first><last>Sun</last></author>
      <author><first>Weiquan</first><last>Huang</last></author>
      <author><first>Jiang</first><last>Wu</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Chenya</first><last>Gu</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Hang</first><last>Yan</last><affiliation>AI lab</affiliation></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <pages>11205-11228</pages>
      <abstract>We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs’ reasoning ability, such as Chain-of-Thought. Our findings indicated that the LLM’s language orientation and the task’s domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs’ memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs’ strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM.</abstract>
      <url hash="5df6a926">2024.acl-long.604</url>
      <bibkey>sun-etal-2024-benchmarking-chinese</bibkey>
      <doi>10.18653/v1/2024.acl-long.604</doi>
    </paper>
    <paper id="605">
      <title>Browse and Concentrate: Comprehending Multimodal Content via Prior-<fixed-case>LLM</fixed-case> Context Fusion</title>
      <author><first>Ziyue</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Chi</first><last>Chen</last></author>
      <author><first>Yiqi</first><last>Zhu</last></author>
      <author><first>Fuwen</first><last>Luo</last></author>
      <author><first>Peng</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>11229-11245</pages>
      <abstract>With the bloom of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) that incorporate LLMs with pre-trained vision models have recently demonstrated impressive performance across diverse vision-language tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth multimodal context fusion prior to feeding the features into LLMs. This paradigm initially “browses” through the inputs for essential insights, and then revisits the inputs to “concentrate” on crucial details, guided by these insights, to achieve a more comprehensive understanding of the multimodal inputs. Additionally, we develop training strategies specifically to enhance the understanding of multi-image inputs. Our method markedly boosts the performance on 7 multi-image scenarios, contributing to increments on average accuracy by 2.13% and 7.60% against strong MLLMs baselines with 3B and 11B LLMs, respectively.</abstract>
      <url hash="1f160c59">2024.acl-long.605</url>
      <bibkey>wang-etal-2024-browse</bibkey>
      <doi>10.18653/v1/2024.acl-long.605</doi>
    </paper>
    <paper id="606">
      <title>Model Composition for Multimodal Large Language Models</title>
      <author><first>Chi</first><last>Chen</last></author>
      <author><first>Yiyang</first><last>Du</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zheng</first><last>Fang</last></author>
      <author><first>Ziyue</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Fuwen</first><last>Luo</last></author>
      <author><first>Peng</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>11246-11262</pages>
      <abstract>Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.</abstract>
      <url hash="1e91b7d2">2024.acl-long.606</url>
      <bibkey>chen-etal-2024-model</bibkey>
      <doi>10.18653/v1/2024.acl-long.606</doi>
    </paper>
    <paper id="607">
      <title>Draft

&amp; Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding</title>
      <author><first>Jun</first><last>Zhang</last></author>
      <author><first>Jue</first><last>Wang</last></author>
      <author><first>Huan</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Lidan</first><last>Shou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ke</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Gang</first><last>Chen</last></author>
      <author><first>Sharad</first><last>Mehrotra</last><affiliation>University of California, Irvine</affiliation></author>
      <pages>11263-11282</pages>
      <abstract>We present a novel inference scheme, self-speculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model. This approach is characterized by a two-stage process: drafting and verification. The drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting. Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass. This process ensures the final output remains identical to that produced by the unaltered LLM. Moreover, the proposed method requires no additional neural network training and no extra memory footprint, making it a plug-and-play and cost-effective solution for inference acceleration. Benchmarks with LLaMA-2 and its variants demonstrated a speedup up to 1.99<tex-math>\times</tex-math>.</abstract>
      <url hash="1f6d157a">2024.acl-long.607</url>
      <bibkey>zhang-etal-2024-draft</bibkey>
      <doi>10.18653/v1/2024.acl-long.607</doi>
    </paper>
    <paper id="608">
      <title>Soul-Mix: Enhancing Multimodal Machine Translation with Manifold Mixup</title>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Ziyu</first><last>Yao</last></author>
      <author><first>Yifei</first><last>Xin</last></author>
      <author><first>Hao</first><last>An</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Hongxiang</first><last>Li</last></author>
      <author><first>Yaowei</first><last>Li</last></author>
      <author><first>Yuexian</first><last>Zou</last><affiliation>Peking University</affiliation></author>
      <pages>11283-11294</pages>
      <abstract>Multimodal machine translation (MMT) aims to improve the performance of machine translation with the help of visual information, which has received widespread attention recently. It has been verified that visual information brings greater performance gains when the textual information is limited. However, most previous works ignore to take advantage of the complete textual inputs and the limited textual inputs at the same time, which limits the overall performance. To solve this issue, we propose a mixup method termed Soul-Mix to enhance MMT by using visual information more effectively. We mix the predicted translations of complete textual input and the limited textual inputs. Experimental results on the Multi30K dataset of three translation directions show that our Soul-Mix significantly outperforms existing approaches and achieves new state-of-the-art performance with fewer parameters than some previous models. Besides, the strength of Soul-Mix is more obvious on more challenging MSCOCO dataset which includes more out-of-domain instances with lots of ambiguous verbs.</abstract>
      <url hash="08a296c4">2024.acl-long.608</url>
      <bibkey>cheng-etal-2024-soul</bibkey>
      <doi>10.18653/v1/2024.acl-long.608</doi>
    </paper>
    <paper id="609">
      <title>Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models</title>
      <author><first>Changjiang</first><last>Gao</last><affiliation>nanjing university</affiliation></author>
      <author><first>Jixing</first><last>Li</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <pages>11295-11308</pages>
      <abstract>The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.</abstract>
      <url hash="32879c01">2024.acl-long.609</url>
      <bibkey>gao-etal-2024-measuring</bibkey>
      <doi>10.18653/v1/2024.acl-long.609</doi>
    </paper>
    <paper id="610">
      <title><fixed-case>MIST</fixed-case>: Mutual Information Maximization for Short Text Clustering</title>
      <author><first>Krissanee</first><last>Kamthawee</last></author>
      <author><first>Can</first><last>Udomcharoenchaikit</last><affiliation>Vidyasirimedhi Institute of Science and Technology (VISTEC)</affiliation></author>
      <author><first>Sarana</first><last>Nutanong</last></author>
      <pages>11309-11324</pages>
      <abstract>Short text clustering poses substantial challenges due to the limited amount of information provided by each text sample. Previous efforts based on dense representations are still inadequate as texts are not sufficiently segregated in the embedding space before clustering. Even though the state-of-the-art method utilizes contrastive learning to boost performance, the process of summarizing all local tokens to form a sequence representation for the whole text includes noise that may obscure limited key information. We propose Mutual Information Maximization Framework for Short Text Clustering (MIST), which overcomes the information drown-out by including a mechanism to maximize the mutual information between representations on both sequence and token levels. Experimental results across eight standard short text datasets show that MIST outperforms the state-of-the-art method in terms of Accuracy or Normalized Mutual Information in most cases.</abstract>
      <url hash="82cf5c88">2024.acl-long.610</url>
      <bibkey>kamthawee-etal-2024-mist</bibkey>
      <doi>10.18653/v1/2024.acl-long.610</doi>
    </paper>
    <paper id="611">
      <title>Self-chats from Large Language Models Make Small Emotional Support Chatbot Better</title>
      <author><first>Zhonghua</first><last>Zheng</last></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <pages>11325-11345</pages>
      <abstract>Large Language Models (LLMs) have shown strong generalization abilities to excel in various tasks, including emotion support conversations. However, deploying such LLMs like GPT-3 (175B parameters) is resource-intensive and challenging at scale. In this study, we utilize LLMs as “Counseling Teacher” to enhance smaller models’ emotion support response abilities, significantly reducing the necessity of scaling up model size. To this end, we first introduce an iterative expansion framework, aiming to prompt the large teacher model to curate an expansive emotion support dialogue dataset. This curated dataset, termed ExTES, encompasses a broad spectrum of scenarios and is crafted with meticulous strategies to ensure its quality and comprehensiveness. Based on this, we then devise a Diverse Response Inpainting (DRI) mechanism to harness the teacher model to produce multiple diverse responses by filling in the masked conversation context. This richness and variety serve as instructive examples, providing a robust foundation for fine-tuning smaller student models. Experiments across varied scenarios reveal that the teacher-student scheme with DRI notably improves the response abilities of smaller models, even outperforming the teacher model in some cases. The dataset and codes are available in https://github.com/pandazzh2020/ExTES.</abstract>
      <url hash="50f51419">2024.acl-long.611</url>
      <bibkey>zheng-etal-2024-self</bibkey>
      <doi>10.18653/v1/2024.acl-long.611</doi>
    </paper>
    <paper id="612">
      <title>Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment</title>
      <author><first>Janghwan</first><last>Lee</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Seongmin</first><last>Park</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Sukjin</first><last>Hong</last><affiliation>Hanyang University and Korea Telecom Research</affiliation></author>
      <author><first>Minsoo</first><last>Kim</last></author>
      <author><first>Du-Seong</first><last>Chang</last></author>
      <author><first>Jungwook</first><last>Choi</last><affiliation>Hanyang University</affiliation></author>
      <pages>11346-11364</pages>
      <abstract>The rapid advancement of large language models (LLMs) has facilitated their transformation into conversational chatbots that can grasp contextual nuances and generate pertinent sentences, closely mirroring human values through advanced techniques such as instruction tuning and reinforcement learning from human feedback (RLHF). However, the computational efficiency required for LLMs, achieved through techniques like post-training quantization (PTQ), presents challenges such as token-flipping that can impair chatbot performance. In response, we propose a novel preference alignment approach, quantization-aware direct preference optimization (QDPO), that aligns quantized LLMs with their full-precision counterparts, improving conversational abilities. Evaluated on two instruction-tuned LLMs in various languages, QDPO demonstrated superior performance in improving conversational abilities compared to established PTQ and knowledge-distillation fine-tuning techniques, marking a significant step forward in the development of efficient and effective conversational LLMs.</abstract>
      <url hash="810bab41">2024.acl-long.612</url>
      <bibkey>lee-etal-2024-improving-conversational</bibkey>
      <revision id="1" href="2024.acl-long.612v1" hash="9807f22e"/>
      <revision id="2" href="2024.acl-long.612v2" hash="2dcad80f" date="2024-08-19">Added Acknowledgments.</revision>
      <revision id="3" href="2024.acl-long.612v3" hash="810bab41" date="2024-08-29">Adds the proceedings header and footer stamps.</revision>
      <doi>10.18653/v1/2024.acl-long.612</doi>
    </paper>
    <paper id="613">
      <title>Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs</title>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Zeming</first><last>Chen</last></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Antoine</first><last>Bosselut</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <pages>11365-11384</pages>
      <abstract>Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit contextunderlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense infer-ences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplexCOMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or theeffect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules andlarge language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improve ments in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations</abstract>
      <url hash="35e12999">2024.acl-long.613</url>
      <bibkey>fang-etal-2024-complex</bibkey>
      <doi>10.18653/v1/2024.acl-long.613</doi>
    </paper>
    <paper id="614">
      <title>An Expert is Worth One Token: Synergizing Multiple Expert <fixed-case>LLM</fixed-case>s as Generalist via Expert Token Routing</title>
      <author><first>Ziwei</first><last>Chai</last></author>
      <author><first>Guoyin</first><last>Wang</last><affiliation>Bytedance</affiliation></author>
      <author><first>Jing</first><last>Su</last></author>
      <author><first>Tianjie</first><last>Zhang</last></author>
      <author><first>Xuanwen</first><last>Huang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xuwu</first><last>Wang</last></author>
      <author><first>Jingjing</first><last>Xu</last></author>
      <author><first>Jianbo</first><last>Yuan</last><affiliation>Bytedance</affiliation></author>
      <author><first>Hongxia</first><last>Yang</last></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yang</first><last>Yang</last></author>
      <pages>11385-11396</pages>
      <abstract>We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert LLMs. Our framework represents expert LLMs as special expert tokens within the vocabulary of a meta LLM. The meta LLM can route to an expert LLM like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert LLMs from existing instruction dataset but also allows for dynamic extension of new expert LLMs in a plug-and-play manner. It also conceals the detailed collaboration process from the user’s perspective, facilitating interaction as though it were a singular LLM. Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.</abstract>
      <url hash="fd861f8e">2024.acl-long.614</url>
      <bibkey>chai-etal-2024-expert</bibkey>
      <doi>10.18653/v1/2024.acl-long.614</doi>
    </paper>
    <paper id="615">
      <title>Learning to Plan and Generate Text with Citations</title>
      <author><first>Constanza</first><last>Fierro</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>Reinald Kim</first><last>Amplayo</last><affiliation>Google</affiliation></author>
      <author><first>Fantine</first><last>Huot</last><affiliation>Google</affiliation></author>
      <author><first>Nicola</first><last>De Cao</last><affiliation>Google</affiliation></author>
      <author><first>Joshua</first><last>Maynez</last><affiliation>Google</affiliation></author>
      <author><first>Shashi</first><last>Narayan</last><affiliation>Google</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>11397-11417</pages>
      <abstract>The increasing demand for the deployment of LLMs in information-seeking scenarios has spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. In this paper, we explore the attribution capabilities of plan-based models which have been recently shown to improve the faithfulness, grounding, and controllability of generated text. We conceptualize plans as a sequence of questions which serve as blueprints of the generated content and its organization. We propose two attribution models that utilize different variants of blueprints, an abstractive model where questions are generated from scratch, and an extractive model where questions are copied from the input. Experiments on long-form question-answering show that planning consistently improves attribution quality. Moreover, the citations generated by blueprint models are more accurate compared to those obtained from LLM-based pipelines lacking a planning component.</abstract>
      <url hash="a3304739">2024.acl-long.615</url>
      <bibkey>fierro-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.acl-long.615</doi>
    </paper>
    <paper id="616">
      <title>Exploring Precision and Recall to assess the quality and diversity of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Florian</first><last>Le Bronnec</last></author>
      <author><first>Alexandre</first><last>Verine</last></author>
      <author><first>Benjamin</first><last>Negrevergne</last><affiliation>Univeristé Paris-Dauphine</affiliation></author>
      <author><first>Yann</first><last>Chevaleyre</last></author>
      <author><first>Alexandre</first><last>Allauzen</last><affiliation>Ecole supérieure de physique et chimie and Univeristé Paris-Dauphine</affiliation></author>
      <pages>11418-11441</pages>
      <abstract>We introduce a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on importing Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals new insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned on instruction dataset or with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges that current LLMs face in generating diverse and high-quality text.</abstract>
      <url hash="3ea52298">2024.acl-long.616</url>
      <bibkey>le-bronnec-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.acl-long.616</doi>
    </paper>
    <paper id="617">
      <title>Aligning Large Language Models by On-Policy Self-Judgment</title>
      <author><first>Sangkyu</first><last>Lee</last></author>
      <author><first>Sungdong</first><last>Kim</last><affiliation>KAIST AI and NAVER</affiliation></author>
      <author><first>Ashkan</first><last>Yousefpour</last></author>
      <author><first>Minjoon</first><last>Seo</last><affiliation>Twelve Labs and Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Kang Min</first><last>Yoo</last><affiliation>NAVER</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <pages>11442-11459</pages>
      <abstract>Existing approaches for aligning large language models with human preferences face a trade-off that requires a separate reward model (RM) for on-policy learning. In this paper, we present a novel alignment framework, SELF-JUDGE that (1) does on-policy learning and 2) is parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model to act as both a policy and a judge. Specifically, we view the pairwise judgment task, choosing the better response from a response pair, as a special case of the instruction-following task. The resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines in preference benchmarks. We also show that the rejecting sampling by itself can improve performance further without an additional evaluator.</abstract>
      <url hash="f6b51662">2024.acl-long.617</url>
      <bibkey>lee-etal-2024-aligning</bibkey>
      <doi>10.18653/v1/2024.acl-long.617</doi>
    </paper>
    <paper id="618">
      <title><fixed-case>IL</fixed-case>-<fixed-case>TUR</fixed-case>: Benchmark for <fixed-case>I</fixed-case>ndian Legal Text Understanding and Reasoning</title>
      <author><first>Abhinav</first><last>Joshi</last><affiliation>Indian Institute of Technology, Kanpur</affiliation></author>
      <author><first>Shounak</first><last>Paul</last></author>
      <author><first>Akshat</first><last>Sharma</last></author>
      <author><first>Pawan</first><last>Goyal</last><affiliation>IIT Kharagpur</affiliation></author>
      <author><first>Saptarshi</first><last>Ghosh</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Ashutosh</first><last>Modi</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>11460-11499</pages>
      <abstract>Legal systems worldwide are inundated with exponential growth in cases and documents. There is an imminent need to develop NLP and ML techniques for automatically processing and understanding legal documents to streamline the legal system. However, evaluating and comparing various NLP models designed specifically for the legal domain is challenging. This paper addresses this challenge by proposing : Benchmark for Indian Legal Text Understanding and Reasoning. contains monolingual (English, Hindi) and multi-lingual (9 Indian languages) domain-specific tasks that address different aspects of the legal system from the point of view of understanding and reasoning over Indian legal documents. We present baseline models (including LLM-based) for each task, outlining the gap between models and the ground truth. To foster further research in the legal domain, we create a leaderboard (available at: https://exploration-lab.github.io/IL-TUR/ ) where the research community can upload and compare legal text understanding systems.</abstract>
      <url hash="7fe016c0">2024.acl-long.618</url>
      <bibkey>joshi-etal-2024-il</bibkey>
      <doi>10.18653/v1/2024.acl-long.618</doi>
      <revision id="1" href="2024.acl-long.618v1" hash="7de7ce82"/>
      <revision id="2" href="2024.acl-long.618v2" hash="7fe016c0" date="2024-12-01">Added Acknowledgement Section (at the end of Appendix, on Page 39).</revision>
    </paper>
    <paper id="619">
      <title><fixed-case>J</fixed-case>ump<fixed-case>C</fixed-case>oder: Go Beyond Autoregressive Coder via Online Modification</title>
      <author><first>Mouxiang</first><last>Chen</last></author>
      <author><first>Hao</first><last>Tian</last></author>
      <author><first>Zhongxin</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xiaoxue</first><last>Ren</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jianling</first><last>Sun</last></author>
      <pages>11500-11520</pages>
      <abstract>While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance. We introduce JumpCoder, a novel model-agnostic framework that enables human-like online modification and non-sequential generation to augment code LLMs. The key idea behind JumpCoder is to insert new code into the currently generated code when necessary during generation, which is achieved through an auxiliary infilling model that works in tandem with the code LLM. Since identifying the best infill position beforehand is intractable, we adopt an infill-first, judge-later strategy, which experiments with filling at the <tex-math>k</tex-math> most critical positions following the generation of each line, and uses an Abstract Syntax Tree (AST) parser alongside the Generation Model Scoring to effectively judge the validity of each potential infill. Extensive experiments using six state-of-the-art code LLMs across multiple and multilingual benchmarks consistently indicate significant improvements over all baselines. Our code is available in the uploaded attachment.</abstract>
      <url hash="de68d500">2024.acl-long.619</url>
      <bibkey>chen-etal-2024-jumpcoder</bibkey>
      <doi>10.18653/v1/2024.acl-long.619</doi>
    </paper>
    <paper id="620">
      <title>Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning</title>
      <author><first>Shivalika</first><last>Singh</last></author>
      <author><first>Freddie</first><last>Vargus</last></author>
      <author><first>Daniel</first><last>D’souza</last></author>
      <author><first>Börje</first><last>Karlsson</last><affiliation>Beijing Academy of Artificial Intelligence (BAAI)</affiliation></author>
      <author><first>Abinaya</first><last>Mahendiran</last></author>
      <author><first>Wei-Yin</first><last>Ko</last></author>
      <author><first>Herumb</first><last>Shandilya</last></author>
      <author><first>Jay</first><last>Patel</last></author>
      <author><first>Deividas</first><last>Mataciunas</last></author>
      <author><first>Laura</first><last>O’Mahony</last></author>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Ramith</first><last>Hettiarachchi</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Joseph</first><last>Wilson</last></author>
      <author><first>Marina</first><last>Machado</last></author>
      <author><first>Luisa</first><last>Moura</last><affiliation>Cohere For AI and Cohere</affiliation></author>
      <author><first>Dominik</first><last>Krzemiński</last></author>
      <author><first>Hakimeh</first><last>Fadaei</last><affiliation>Divar company and University of Tehran, University of Tehran</affiliation></author>
      <author><first>Irem</first><last>Ergun</last><affiliation>Cohere</affiliation></author>
      <author><first>Ifeoma</first><last>Okoh</last></author>
      <author><first>Aisha</first><last>Alaagib</last></author>
      <author><first>Oshan</first><last>Mudannayake</last></author>
      <author><first>Zaid</first><last>Alyafeai</last></author>
      <author><first>Vu</first><last>Chien</last></author>
      <author><first>Sebastian</first><last>Ruder</last><affiliation>Cohere and Google</affiliation></author>
      <author><first>Surya</first><last>Guthikonda</last></author>
      <author><first>Emad</first><last>Alghamdi</last><affiliation>King Abdulaziz University</affiliation></author>
      <author><first>Sebastian</first><last>Gehrmann</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Niklas</first><last>Muennighoff</last><affiliation>Contextual AI</affiliation></author>
      <author><first>Max</first><last>Bartolo</last><affiliation>Cohere and University College London</affiliation></author>
      <author><first>Julia</first><last>Kreutzer</last><affiliation>Cohere for AI</affiliation></author>
      <author><first>Ahmet</first><last>Üstün</last><affiliation>Cohere For Ai</affiliation></author>
      <author><first>Marzieh</first><last>Fadaee</last><affiliation>Cohere For AI</affiliation></author>
      <author><first>Sara</first><last>Hooker</last><affiliation>Cohere For AI</affiliation></author>
      <pages>11521-11567</pages>
      <abstract>Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the fine-tuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and augmenting existing datasets across 114 languages. In total, we contribute three key resources: we develop and open-source the Aya Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as an important framework for future research collaborations that aim to bridge gaps in resources.</abstract>
      <url hash="cfcbb1ae">2024.acl-long.620</url>
      <bibkey>singh-etal-2024-aya</bibkey>
      <doi>10.18653/v1/2024.acl-long.620</doi>
    </paper>
    <paper id="621">
      <title>Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks</title>
      <author><first>Anwoy</first><last>Chatterjee</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <author><first>Eshaan</first><last>Tanwar</last></author>
      <author><first>Subhabrata</first><last>Dutta</last><affiliation>Indian Institute of Technology, Delhi and Jadavpur University</affiliation></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>11568-11587</pages>
      <abstract>Large Language Models (LLMs) have transformed NLP with their remarkable In-context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal models excel in zero-shot performance, their computational demands limit widespread use, and smaller language models struggle without context. This paper investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks. Drawing inspiration from biological neurons and the mechanistic interpretation of the Transformer architecture, we explore the potential for information sharing across tasks. We design a cross-task prompting setup with three LLMs and show that LLMs achieve significant performance improvements despite no examples from the target task in the context. Cross-task prompting leads to a remarkable performance boost of 107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 on average over zero-shot prompting, and performs comparable to standard in-context learning. The effectiveness of generating pseudo-labels for in-task examples is demonstrated, and our analyses reveal a strong correlation between the effect of cross-task examples and model activation similarities in source and target input tokens. This paper offers a first-of-its-kind exploration of LLMs’ ability to solve novel tasks based on contextual signals from different task examples.</abstract>
      <url hash="729d0942">2024.acl-long.621</url>
      <bibkey>chatterjee-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.acl-long.621</doi>
    </paper>
    <paper id="622">
      <title>Split and Rephrase with Large Language Models</title>
      <author><first>David</first><last>Ponce</last><affiliation>Vicomtech</affiliation></author>
      <author><first>Thierry</first><last>Etchegoyhen</last><affiliation>Vicomtech</affiliation></author>
      <author><first>Jesús</first><last>Calleja</last><affiliation>Universidad del País Vasco and Vicomtech</affiliation></author>
      <author><first>Harritxu</first><last>Gete</last><affiliation>University of the Basque Country and Vicomtech Foundation</affiliation></author>
      <pages>11588-11607</pages>
      <abstract>The Split and Rephrase (SPRP) task, which consists in splitting complex sentences into a sequence of shorter grammatical sentences, while preserving the original meaning, can facilitate the processing of complex texts for humans and machines alike. It is also a valuable testbed to evaluate natural language processing models, as it requires modelling complex grammatical aspects. In this work, we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance. Results from two human evaluations further support the conclusions drawn from automated metric results. We provide a comprehensive study that includes prompting variants, domain shift, fine-tuned pretrained language models of varying parameter size and training data volumes, contrasted with both zero-shot and few-shot approaches on instruction-tuned language models. Although the latter were markedly outperformed by fine-tuned models, they may constitute a reasonable off-the-shelf alternative. Our results provide a fine-grained analysis of the potential and limitations of large language models for SPRP, with significant improvements achievable using relatively small amounts of training data and model parameters overall, and remaining limitations for all models on the task.</abstract>
      <url hash="b0e00e10">2024.acl-long.622</url>
      <bibkey>ponce-etal-2024-split</bibkey>
      <doi>10.18653/v1/2024.acl-long.622</doi>
    </paper>
    <paper id="623">
      <title><fixed-case>C</fixed-case>hunk<fixed-case>A</fixed-case>ttention: Efficient Self-Attention with Prefix-Aware <fixed-case>KV</fixed-case> Cache and Two-Phase Partition</title>
      <author><first>Lu</first><last>Ye</last></author>
      <author><first>Ze</first><last>Tao</last></author>
      <author><first>Yong</first><last>Huang</last></author>
      <author><first>Yang</first><last>Li</last></author>
      <pages>11608-11620</pages>
      <abstract>Self-attention is an essential component of large language models (LLM) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8<tex-math>\times</tex-math> compared to the start-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096.</abstract>
      <url hash="e0b78c60">2024.acl-long.623</url>
      <bibkey>ye-etal-2024-chunkattention</bibkey>
      <doi>10.18653/v1/2024.acl-long.623</doi>
    </paper>
    <paper id="624">
      <title><fixed-case>A</fixed-case>lign<fixed-case>B</fixed-case>ench: Benchmarking <fixed-case>C</fixed-case>hinese Alignment of Large Language Models</title>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Xuanyu</first><last>Lei</last></author>
      <author><first>Shengyuan</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yue</first><last>Huang</last></author>
      <author><first>Andrew</first><last>Feng</last></author>
      <author><first>Bosi</first><last>Wen</last></author>
      <author><first>Jiale</first><last>Cheng</last></author>
      <author><first>Pei</first><last>Ke</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yifan</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Weng Lam</first><last>Tam</last></author>
      <author><first>Xiaohan</first><last>Zhang</last><affiliation>Beijing Knowledge Atlas Technology Co., Ltd.</affiliation></author>
      <author><first>Lichao</first><last>Sun</last><affiliation>Lehigh University</affiliation></author>
      <author><first>Xiaotao</first><last>Gu</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Hongning</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>11621-11640</pages>
      <abstract>Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, effective evaluation of alignment for emerging Chinese LLMs is still significantly lacking, calling for real-scenario grounded, open-ended, challenging and automatic evaluations tailored for alignment. To fill in this gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark for evaluating LLMs’ alignment in Chinese. We tailor a human-in-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references.To ensure references’ correctness, each knowledge-intensive query is accompanied with evidences collected from reliable webpages (including the url and quotation) by our annotators.For automatic evaluation, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge (CITATION) with Chain-of-Thought to generate explanations and final ratings as evaluations, ensuring high reliability and interpretability.All evaluation codes and data are publicly available at <url>https://github.com/THUDM/AlignBench</url></abstract>
      <url hash="248600a5">2024.acl-long.624</url>
      <bibkey>liu-etal-2024-alignbench</bibkey>
      <doi>10.18653/v1/2024.acl-long.624</doi>
    </paper>
    <paper id="625">
      <title><fixed-case>SAPT</fixed-case>: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models</title>
      <author><first>Weixiang</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Shilong</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yulin</first><last>Hu</last></author>
      <author><first>Yanyan</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xuanyu</first><last>Zhang</last></author>
      <author><first>Qing</first><last>Yang</last></author>
      <author><first>Dongliang</first><last>Xu</last></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>11641-11661</pages>
      <abstract>The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two modules to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning &amp; Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model sizes (from 770M to 13B), different model architectures (T5 and LLaMA-2) and unseen tasks.</abstract>
      <url hash="8c3a9628">2024.acl-long.625</url>
      <bibkey>zhao-etal-2024-sapt</bibkey>
      <doi>10.18653/v1/2024.acl-long.625</doi>
    </paper>
    <paper id="626">
      <title><fixed-case>D</fixed-case>o<fixed-case>RA</fixed-case>: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution</title>
      <author><first>Yulong</first><last>Mao</last></author>
      <author><first>Kaiyu</first><last>Huang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Changhao</first><last>Guan</last></author>
      <author><first>Ganglin</first><last>Bao</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Fengran</first><last>Mo</last></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>11662-11675</pages>
      <abstract>Fine-tuning large-scale pre-trained models is inherently a resource-intensive task. While it can enhance the capabilities of the model, it also incurs substantial computational costs, posing challenges to the practical application of downstream tasks. Existing parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) rely on a bypass framework that ignores the differential parameter budget requirements across weight matrices, which may lead to suboptimal fine-tuning outcomes. To address this issue, we introduce the Dynamic Low-Rank Adaptation (DoRA) method. DoRA decomposes high-rank LoRA layers into structured single-rank components, allowing for dynamic pruning of parameter budget based on their importance to specific tasks during training, which makes the most of the limited parameter budget. Experimental results demonstrate that DoRA can achieve competitive performance compared with LoRA and full model fine-tuning, and outperform various strong baselines with the same storage parameter budget. Our code is available at [github](https://github.com/MIkumikumi0116/DoRA)</abstract>
      <url hash="efbe3c0a">2024.acl-long.626</url>
      <bibkey>mao-etal-2024-dora</bibkey>
      <doi>10.18653/v1/2024.acl-long.626</doi>
    </paper>
    <paper id="627">
      <title>Cross-Lingual Knowledge Editing in Large Language Models</title>
      <author><first>Jiaan</first><last>Wang</last><affiliation>Soochow University</affiliation></author>
      <author><first>Yunlong</first><last>Liang</last></author>
      <author><first>Zengkui</first><last>Sun</last></author>
      <author><first>Yuxuan</first><last>Cao</last></author>
      <author><first>Jiarong</first><last>Xu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <pages>11676-11686</pages>
      <abstract>Knowledge editing aims to change language models’ performance on several special cases (i.e., editing scope) by infusing the corresponding expected knowledge into them. With the recent advancements in large language models (LLMs), knowledge editing has been shown as a promising technique to adapt LLMs to new knowledge without retraining from scratch. However, most of the previous studies neglect the multi-lingual nature of some main-stream LLMs (e.g., LLaMA, ChatGPT and GPT-4), and typically focus on monolingual scenarios, where LLMs are edited and evaluated in the same language. As a result, it is still unknown the effect of source language editing on a different target language. In this paper, we aim to figure out this cross-lingual effect in knowledge editing. Specifically, we first collect a large-scale cross-lingual synthetic dataset by translating ZsRE from English to Chinese. Then, we conduct English editing on various knowledge editing methods covering different paradigms, and evaluate their performance in Chinese, and vice versa. To give deeper analyses of the cross-lingual effect, the evaluation includes four aspects, i.e., reliability, generality, locality and portability. Furthermore, we analyze the inconsistent behaviors of the edited models and discuss their specific challenges.</abstract>
      <url hash="30a51dec">2024.acl-long.627</url>
      <bibkey>wang-etal-2024-cross</bibkey>
      <doi>10.18653/v1/2024.acl-long.627</doi>
    </paper>
    <paper id="628">
      <title>Argument Mining in Data Scarce Settings: Cross-lingual Transfer and Few-shot Techniques</title>
      <author><first>Anar</first><last>Yeginbergen</last></author>
      <author><first>Maite</first><last>Oronoz</last></author>
      <author><first>Rodrigo</first><last>Agerri</last><affiliation>University of the Basque Country</affiliation></author>
      <pages>11687-11699</pages>
      <abstract>Recent research on sequence labelling has been exploring different strategies to mitigate the lack of manually annotated data for the large majority of the world languages. Among others, the most successful approaches have been based on (i) the crosslingual transfer capabilities of multilingual pre-trained language models (model-transfer), (ii) data translation and label projection (data-transfer) and (iii), prompt-based learning by reusing the mask objective to exploit the few-shot capabilities of pre-trained language models (few-shot). Previous work seems to conclude that model-transfer outperform data-transfer methods and that few-shot techniques based on prompting are superior to updating the model’s weights via fine-tuning. In this paper we empirically demonstrate that, for Argument Mining, a sequence labelling task which requires the detection of long and complex discourse structures, previous insights on crosslingual transfer or few-shot learning do not apply. Contrary to previous work, we show that for Argument Mining data-transfer obtains better results than model-transfer and that fine-tuning outperforms few-shot methods. Regarding the former, the domain of the dataset used for data-transfer seems to be a deciding factor, while, for few-shot, the type of task (length and complexity of the sequence spans) and sampling method proves to be crucial.</abstract>
      <url hash="1862994a">2024.acl-long.628</url>
      <bibkey>yeginbergen-etal-2024-argument</bibkey>
      <doi>10.18653/v1/2024.acl-long.628</doi>
    </paper>
    <paper id="629">
      <title>Learning Task Decomposition to Assist Humans in Competitive Programming</title>
      <author><first>Jiaxin</first><last>Wen</last></author>
      <author><first>Ruiqi</first><last>Zhong</last></author>
      <author><first>Pei</first><last>Ke</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhihong</first><last>Shao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Hongning</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>11700-11723</pages>
      <abstract>When using language models (LMs) to solve complex problems, humans might struggle to understand the LM-generated solutions and repair the flawed ones. To assist humans in repairing them, we propose to automatically decompose complex solutions into multiple simpler pieces that correspond to specific subtasks. We introduce a novel objective for learning task decomposition, termed assistive value (AssistV), which measures the feasibility and speed for humans to repair the decomposed solution. We collect a dataset of human repair experiences on different decomposed solutions. Utilizing the collected data as in-context examples, we then learn to critique, refine, and rank decomposed solutions to improve AssistV. We validate our method under competitive programming problems: under 177 hours of human study, our method enables non-experts to solve 33.3% more problems, speeds them up by 3.3x, and empowers them to match unassisted experts.</abstract>
      <url hash="64a9f150">2024.acl-long.629</url>
      <bibkey>wen-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.acl-long.629</doi>
    </paper>
    <paper id="630">
      <title>An Entropy-based Text Watermarking Detection Method</title>
      <author><first>Yijian</first><last>Lu</last></author>
      <author><first>Aiwei</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Dianzhi</first><last>Yu</last><affiliation>Chinese University of Hong Kong</affiliation></author>
      <author><first>Jingjing</first><last>Li</last></author>
      <author><first>Irwin</first><last>King</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>11724-11735</pages>
      <abstract>Text watermarking algorithms for large language models (LLMs) can effectively identify machine-generated texts by embedding and detecting hidden features in the text. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we opine that the influence of token entropy should be fully considered in the watermark detection process, <tex-math>i.e.</tex-math>, the weight of each token during watermark detection should be customized according to its entropy, rather than setting the weights of all tokens to the same value as in previous methods. Specifically, we propose <b>E</b>ntropy-based Text <b>W</b>atermarking <b>D</b>etection (<b>EWD</b>) that gives higher-entropy tokens higher influence weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free and fully automated. From the experiments, we demonstrate that our EWD can achieve better detection performance in low-entropy scenarios, and our method is also general and can be applied to texts with different entropy distributions. Our code and data is available. Additionally, our algorithm could be accessed through MarkLLM (CITATION).</abstract>
      <url hash="aaa42654">2024.acl-long.630</url>
      <bibkey>lu-etal-2024-entropy</bibkey>
      <doi>10.18653/v1/2024.acl-long.630</doi>
    </paper>
    <paper id="631">
      <title>Enhancing Explainable Rating Prediction through Annotated Macro Concepts</title>
      <author><first>Huachi</first><last>Zhou</last></author>
      <author><first>Shuang</first><last>Zhou</last></author>
      <author><first>Hao</first><last>Chen</last></author>
      <author><first>Ninghao</first><last>Liu</last><affiliation>University of Georgia</affiliation></author>
      <author><first>Fan</first><last>Yang</last><affiliation>Wake Forest University</affiliation></author>
      <author><first>Xiao</first><last>Huang</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <pages>11736-11748</pages>
      <abstract>Generating recommendation reasons for recommendation results is a long-standing problem because it is challenging to explain the underlying reasons for recommending an item based on user and item IDs. Existing models usually learn semantic embeddings for each user and item, and generate the reasons according to the embeddings of the user-item pair. However, user and item IDs do not carry inherent semantic meaning, thus the limited number of reviews cannot model users’ preferences and item characteristics effectively, negatively affecting the model generalization for unseen user-item pairs.To tackle the problem, we propose the Concept Enhanced Explainable Recommendation framework (CEER), which utilizes macro concepts as the intermediary to bridge the gap between the user/item embeddings and the recommendation reasons. Specifically, we maximize the information bottleneck to extract macro concepts from user-item reviews. Then, for recommended user-item pairs, we jointly train the concept embeddings with the user and item embeddings, and generate the explanation according to the concepts. Extensive experiments on three datasets verify the superiority of our CEER model.</abstract>
      <url hash="71efae5d">2024.acl-long.631</url>
      <bibkey>zhou-etal-2024-enhancing-explainable</bibkey>
      <doi>10.18653/v1/2024.acl-long.631</doi>
    </paper>
    <paper id="632">
      <title>How to Engage your Readers? Generating Guiding Questions to Promote Active Reading</title>
      <author><first>Peng</first><last>Cui</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Vilém</first><last>Zouhar</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Xiaoyu</first><last>Zhang</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>11749-11765</pages>
      <abstract>Using questions in written text is an effective strategy to enhance readability. However, what makes an active reading question good, what the linguistic role of these questions is, and what is their impact on human reading remains understudied. We introduce GuidingQ, a dataset of 10K in-text questions from textbooks and scientific articles. By analyzing the dataset, we present a comprehensive understanding of the use, distribution, and linguistic characteristics of these questions. Then, we explore various approaches to generate such questions using language models. Our results highlight the importance of capturing inter-question relationships and the challenge of question position identification in generating these questions. Finally, we conduct a human study to understand the implication of such questions on reading comprehension. We find that the generated questions are of high quality and are almost as effective as human-written questions in terms of improving readers’ memorization and comprehension.</abstract>
      <url hash="64a22c22">2024.acl-long.632</url>
      <bibkey>cui-etal-2024-engage</bibkey>
      <doi>10.18653/v1/2024.acl-long.632</doi>
    </paper>
    <paper id="633">
      <title>Less is More: Mitigating Multimodal Hallucination from an <fixed-case>EOS</fixed-case> Decision Perspective</title>
      <author><first>Zihao</first><last>Yue</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Qin</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <pages>11766-11781</pages>
      <abstract>Large Multimodal Models (LMMs) often suffer from multimodal hallucinations, wherein they may create content that is not present in the visual inputs. In this paper, we explore a new angle of this issue: overly detailed training data hinders the model’s ability to timely terminate generation, leading to continued outputs beyond visual perception limits. By investigating how the model decides to terminate generation with EOS, the special end-of-sentence token, we find that the model assesses the completeness of the entire sequence by comparing the generated text with the image. This observation suggests that the model possesses an inherent potential of making proper EOS decisions based on its visual perception to avoid overly lengthy outputs. To take advantage of such potential, we explore two methods to mitigate multimodal hallucinations: a training objective that enables the model to reduce hallucinations by learning from regular instruction data, and a data filtering strategy to prevent harmful training data from exacerbating model hallucinations. Both methods significantly improve the hallucination performance of LMMs, without requiring any additional data or knowledge.</abstract>
      <url hash="5a7ed0da">2024.acl-long.633</url>
      <bibkey>yue-etal-2024-less</bibkey>
      <doi>10.18653/v1/2024.acl-long.633</doi>
    </paper>
    <paper id="634">
      <title>Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation</title>
      <author><first>Xinglin</first><last>Wang</last></author>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Shaoxiong</first><last>Feng</last></author>
      <author><first>Peiwen</first><last>Yuan</last></author>
      <author><first>Boyuan</first><last>Pan</last></author>
      <author><first>Heda</first><last>Wang</last></author>
      <author><first>Yao</first><last>Hu</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>11782-11794</pages>
      <abstract>Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample selection or voting mechanisms to improve output quality. These methods, however, face limitations due to their inability to fully utilize the nuanced consensus knowledge present within multiple candidate samples, often resulting in suboptimal outputs. We propose Fine-Grained Self-Consistency (FSC) to addresses these limitations by extracting and integrating segment-level commonalities from candidate samples, enhancing the performance of LLMs both in open-ended and reasoning tasks. Based on this, we present two additional strategies: candidate filtering, which enhances overall quality by identifying highly similar candidate sets, and merging, which reduces input token requirements by combining similar samples. The effectiveness of FSC is demonstrated through extensive experiments on various tasks, including summarization, code generation, and mathematical reasoning, using GPT-3.5-turbo and GPT-4. The results indicate significant improvements over baseline methods, showcasing the potential of FSC to optimize output quality by effectively synthesizing fine-grained consensus knowledge from multiple samples.</abstract>
      <url hash="21302cc9">2024.acl-long.634</url>
      <bibkey>wang-etal-2024-integrate</bibkey>
      <doi>10.18653/v1/2024.acl-long.634</doi>
    </paper>
    <paper id="635">
      <title>More frequent verbs are associated with more diverse valency frames: Efficient principles at the lexicon-grammar interface</title>
      <author><first>Siyu</first><last>Tao</last></author>
      <author><first>Lucia</first><last>Donatelli</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Michael</first><last>Hahn</last></author>
      <pages>11795-11810</pages>
      <abstract>A substantial body of work has provided evidence that the lexicons of natural languages are organized to support efficient communication. However, existing work has largely focused on word-internal properties, such as Zipf’s observation that more frequent words are optimized in form to minimize communicative cost. Here, we investigate the hypothesis that efficient lexicon organization is also reflected in valency, or the combinations and orders of additional words and phrases a verb selects for in a sentence. We consider two measures of valency diversity for verbs: valency frame count (VFC), the number of distinct frames associated with a verb, and valency frame entropy (VFE), the average information content of frame selection associated with a verb. Using data from 79 languages, we provide evidence that more frequent verbs are associated with a greater diversity of valency frames, suggesting that the organization of valency is consistent with communicative efficiency principles. We discuss our findings in relation to classical findings such as Zipf’s meaning-frequency law and the principle of least effort, as well as implications for theories of valency and communicative efficiency principles.</abstract>
      <url hash="b2230902">2024.acl-long.635</url>
      <bibkey>tao-etal-2024-frequent</bibkey>
      <doi>10.18653/v1/2024.acl-long.635</doi>
    </paper>
    <paper id="636">
      <title>Quantifying Generalizations: Exploring the Divide Between Human and <fixed-case>LLM</fixed-case>s’ Sensitivity to Quantification</title>
      <author><first>Claudia</first><last>Collacciani</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Giulia</first><last>Rambelli</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Marianna</first><last>Bolognesi</last></author>
      <pages>11811-11822</pages>
      <abstract>Generics are expressions used to communicate abstractions about categories. While conveying general truths (e.g., “Birds fly”), generics have the interesting property to admit exceptions (e.g., penguins do not fly). Statements of this type help us organizing our knowledge of the world, and form the basis of how we express it (Hampton, 2012; Leslie, 2014).This study investigates how Large Language Models (LLMs) interpret generics, drawing upon psycholinguistic experimental methodologies. Understanding how LLMs interpret generic statements serves not only as a measure of their ability to abstract but also arguably plays a role in their encoding of stereotypes. Given that generics interpretation necessitates a comparison with explicitly quantified sentences, we explored i.) whether LLMs can correctly associate a quantifier with the generic structure, and ii.) whether the presence of a generic sentence as context influences the outcomes of quantifiers. We evaluated LLMs using both Surprisal distributions and prompting techniques.The findings indicate that models do not exhibit a strong sensitivity to quantification. Nevertheless, they seem to encode a meaning linked with the generic structure, which leads them to adjust their answers accordingly when a generalization is provided as context.</abstract>
      <url hash="3717f50e">2024.acl-long.636</url>
      <bibkey>collacciani-etal-2024-quantifying</bibkey>
      <doi>10.18653/v1/2024.acl-long.636</doi>
    </paper>
    <paper id="637">
      <title>Can Large Language Models Interpret Noun-Noun Compounds? A Linguistically-Motivated Study on Lexicalized and Novel Compounds</title>
      <author><first>Giulia</first><last>Rambelli</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Emmanuele</first><last>Chersoni</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Claudia</first><last>Collacciani</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Marianna</first><last>Bolognesi</last></author>
      <pages>11823-11835</pages>
      <abstract>Noun-noun compounds interpretation is the task where a model is given one of such constructions, and it is asked to provide a paraphrase, making the semantic relation between the nouns explicit, as in carrot cake is “a cake made of carrots.” Such a task requires the ability to understand the implicit structured representation of the compound meaning. In this paper, we test to what extent the recent Large Language Models can interpret the semantic relation between the constituents of lexicalized English compounds and whether they can abstract from such semantic knowledge to predict the semantic relation between the constituents of similar but novel compounds by relying on analogical comparisons (e.g., carrot dessert). We test both Surprisal metrics and prompt-based methods to see whether i.) they can correctly predict the relation between constituents, and ii.) the semantic representation of the relation is robust to paraphrasing. Using a dataset of lexicalized and annotated noun-noun compounds, we find that LLMs can infer some semantic relations better than others (with a preference for compounds involving concrete concepts). When challenged to perform abstractions and transfer their interpretations to semantically similar but novel compounds, LLMs show serious limitations.</abstract>
      <url hash="8c406816">2024.acl-long.637</url>
      <bibkey>rambelli-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.acl-long.637</doi>
    </paper>
    <paper id="638">
      <title><fixed-case>C</fixed-case>haracter<fixed-case>E</fixed-case>val: A <fixed-case>C</fixed-case>hinese Benchmark for Role-Playing Conversational Agent Evaluation</title>
      <author><first>Quan</first><last>Tu</last></author>
      <author><first>Shilong</first><last>Fan</last></author>
      <author><first>Zihang</first><last>Tian</last></author>
      <author><first>Tianhao</first><last>Shen</last></author>
      <author><first>Shuo</first><last>Shang</last></author>
      <author><first>Xin</first><last>Gao</last></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>11836-11850</pages>
      <abstract>Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce <i>CharacterEval</i>, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. <i>CharacterEval</i> employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics in <i>CharacterEval</i>, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments on <i>CharacterEval</i> demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation.</abstract>
      <url hash="d6e884d7">2024.acl-long.638</url>
      <bibkey>tu-etal-2024-charactereval</bibkey>
      <doi>10.18653/v1/2024.acl-long.638</doi>
    </paper>
    <paper id="639">
      <title>Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond</title>
      <author id="yongqi-li-hk"><first>Yongqi</first><last>Li</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Leigang</first><last>Qu</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>11851-11861</pages>
      <abstract>The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to “recall” the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter step teaches the MLLM to generate the corresponding identifier of the target image, given the textual query input. By memorizing images in MLLMs, we introduce a new paradigm to cross-modal retrieval, distinct from previous discriminative approaches. The experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image candidate sets.</abstract>
      <url hash="647205ff">2024.acl-long.639</url>
      <bibkey>li-etal-2024-generative</bibkey>
      <doi>10.18653/v1/2024.acl-long.639</doi>
    </paper>
    <paper id="640">
      <title>Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction</title>
      <author><first>Yice</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Zeng</last></author>
      <author><first>Weiming</first><last>Hu</last></author>
      <author><first>Ziyi</first><last>Wang</last></author>
      <author><first>Shiwei</first><last>Chen</last></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>11862-11875</pages>
      <abstract>Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect term, aspect category, opinion term, sentiment polarity) for a given review, which is the most representative and challenging task in aspect-based sentiment analysis. A key challenge in the ASQP task is the scarcity of labeled data, which limits the performance of existing methods. To tackle this issue, we propose a self-training framework with a pseudo-label scorer, wherein a scorer assesses the match between reviews and their pseudo-labels, aiming to filter out mismatches and thereby enhance the effectiveness of self-training. We highlight two critical aspects to ensure the scorer’s effectiveness and reliability: the quality of the training dataset and its model architecture. To this end, we create a human-annotated comparison dataset and train a generative model on it using ranking-based objectives. Extensive experiments on public ASQP datasets reveal that using our scorer can greatly and consistently improve the effectiveness of self-training. Moreover, we explore the possibility of replacing humans with large language models for comparison dataset annotation, and experiments demonstrate its feasibility. We will release our code and data via GitHub.</abstract>
      <url hash="890bf5ae">2024.acl-long.640</url>
      <bibkey>zhang-etal-2024-self-training</bibkey>
      <doi>10.18653/v1/2024.acl-long.640</doi>
    </paper>
    <paper id="641">
      <title>Learning to Generate Answers with Citations via Factual Consistency Models</title>
      <author><first>Rami</first><last>Aly</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Zhiqiang</first><last>Tang</last><affiliation>AWS</affiliation></author>
      <author><first>Samson</first><last>Tan</last><affiliation>Amazon</affiliation></author>
      <author><first>George</first><last>Karypis</last><affiliation>University of Minnesota, Minneapolis</affiliation></author>
      <pages>11876-11896</pages>
      <abstract>Large Language Models (LLMs) frequently hallucinate, impeding their reliability in mission-critical situations. One approach to address this issue is to provide citations to relevant sources alongside generated content, enhancing the verifiability of generations. However, citing passages accurately in answers remains a substantial challenge. This paper proposes a weakly-supervised fine-tuning method leveraging factual consistency models (FCMs). Our approach alternates between generating texts with citations and supervised fine-tuning with FCM-filtered citation data. Focused learning is integrated into the objective, directing the fine-tuning process to emphasise the factual unit tokens, as measured by an FCM. Results on the ALCE few-shot citation benchmark with various instruction-tuned LLMs demonstrate superior performance compared to in-context learning, vanilla supervised fine-tuning, and state-of-the-art methods, with an average improvement of 34.1, 15.5, and 10.5 citation F<tex-math>_1</tex-math> points, respectively. Moreover, in a domain transfer setting we show that the obtained citation generation ability robustly transfers to unseen datasets. Notably, our citation improvements contribute to the lowest factual error rate across baselines.</abstract>
      <url hash="108c4d29">2024.acl-long.641</url>
      <bibkey>aly-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.acl-long.641</doi>
    </paper>
    <paper id="642">
      <title>Improving Text Embeddings with Large Language Models</title>
      <author><first>Liang</first><last>Wang</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Nan</first><last>Yang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Xiaolong</first><last>Huang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Linjun</first><last>Yang</last></author>
      <author><first>Rangan</first><last>Majumder</last></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>11897-11916</pages>
      <abstract>In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.</abstract>
      <url hash="faf30763">2024.acl-long.642</url>
      <bibkey>wang-etal-2024-improving-text</bibkey>
      <doi>10.18653/v1/2024.acl-long.642</doi>
    </paper>
    <paper id="643">
      <title>Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning</title>
      <author><first>Tianduo</first><last>Wang</last></author>
      <author><first>Shichen</first><last>Li</last></author>
      <author><first>Wei</first><last>Lu</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>11917-11928</pages>
      <abstract>Teaching small-scale language models to perform math reasoning is a valuable yet challenging task. Besides obtaining labeled data from human experts, one of the most common ways to collect high-quality data is by sampling from a larger and more powerful language model. Although previous works have demonstrated the effectiveness of this method, such a knowledge distillation paradigm can be costly and unstable, especially considering that many large language models, such as GPT-4, are closed-sourced, proprietary, and their behaviors are unpredictable. In this work, to avoid relying on outputs from large models, we demonstrate that the reasoning abilities of small-scale language models can be enhanced through self-training, which involves training models with their own outputs. We also show that the vanilla self-training can be further augmented by an alignment algorithm, direct preference optimization (DPO). We empirically found that models trained with the DPO objective are capable of making better generations that largely benefit multi-turn self-training. The experiments show our models outperform the state-of-the-art models with comparable sizes on a series of downstream math reasoning tasks with minimal resource requirements.</abstract>
      <url hash="76ba316c">2024.acl-long.643</url>
      <bibkey>wang-etal-2024-self-training</bibkey>
      <doi>10.18653/v1/2024.acl-long.643</doi>
    </paper>
    <paper id="644">
      <title><fixed-case>U</fixed-case>ltra<fixed-case>L</fixed-case>ink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset</title>
      <author><first>Haoyu</first><last>Wang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Yukun</first><last>Yan</last></author>
      <author><first>Xujia</first><last>Wang</last></author>
      <author><first>Zhiyu</first><last>Yang</last></author>
      <author><first>Yuzhuang</first><last>Xu</last></author>
      <author><first>Zhenghao</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Liner</first><last>Yang</last><affiliation>Beijing Language and Culture University</affiliation></author>
      <author><first>Ning</first><last>Ding</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>11929-11942</pages>
      <abstract>Open-source large language models (LLMs) have gained significant strength across diverse fields. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual abilities.In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset.Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more language-specific knowledge of LLMs, improving their ability to serve users from different countries. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic supervised fine-tuning (SFT) data without any performance degradation, making multilingual SFT more efficient.The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and the proposed data construction method can be easily extended to other languages.UltraLink-LM, which is trained on the UltraLink dataset, outperforms several representative baselines across many tasks.</abstract>
      <url hash="a88c2fec">2024.acl-long.644</url>
      <bibkey>wang-etal-2024-ultralink</bibkey>
      <doi>10.18653/v1/2024.acl-long.644</doi>
    </paper>
    <paper id="645">
      <title>Document-level Claim Extraction and Decontextualisation for Fact-Checking</title>
      <author><first>Zhenyun</first><last>Deng</last></author>
      <author><first>Michael</first><last>Schlichtkrull</last><affiliation>Queen Mary, University of London</affiliation></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <pages>11943-11954</pages>
      <abstract>Selecting which claims to check is a time-consuming task for human fact-checkers, especially from documents consisting of multiple sentences and containing multiple claims. However, existing claim extraction approaches focus more on identifying and extracting claims from individual sentences, e.g., identifying whether a sentence contains a claim or the exact boundaries of the claim within a sentence. In this paper, we propose a method for document-level claim extraction for fact-checking, which aims to extract check-worthy claims from documents and decontextualise them so that they can be understood out of context. Specifically, we first recast claim extraction as extractive summarization in order to identify central sentences from documents, then rewrite them to include necessary context from the originating document through sentence decontextualisation. Evaluation with both automatic metrics and a fact-checking professional shows that our method is able to extract check-worthy claims from documents at a higher rate than previous work, while also improving evidence retrieval.</abstract>
      <url hash="bb9d524f">2024.acl-long.645</url>
      <bibkey>deng-etal-2024-document</bibkey>
      <doi>10.18653/v1/2024.acl-long.645</doi>
    </paper>
    <paper id="646">
      <title><fixed-case>P</fixed-case>air<fixed-case>CFR</fixed-case>: Enhancing Model Training on Paired Counterfactually Augmented Data through Contrastive Learning</title>
      <author><first>Xiaoqi</first><last>Qiu</last></author>
      <author><first>Yongjie</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xu</first><last>Guo</last></author>
      <author><first>Zhiwei</first><last>Zeng</last><affiliation>National Technological University</affiliation></author>
      <author><first>Yu</first><last>Yue</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yuhong</first><last>Feng</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Chunyan</first><last>Miao</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <pages>11955-11971</pages>
      <abstract>Counterfactually Augmented Data (CAD) involves creating new data samples by applying minimal yet sufficient modifications to flip the label of existing data samples to other classes. Training with CAD enhances model robustness against spurious features that happen to correlate with labels by spreading the casual relationships across different classes. Yet, recent research reveals that training with CAD may lead models to overly focus on modified features while ignoring other important contextual information, inadvertently introducing biases that may impair performance on out-of-distribution (OOD) datasets. To mitigate this issue, we employ contrastive learning to promote global feature alignment in addition to learning counterfactual clues. We theoretically prove that contrastive loss can encourage models to leverage a broader range of features beyond those modified ones. Comprehensive experiments on two human-edited CAD datasets demonstrate that our proposed method outperforms the state-of-the-art on OOD datasets.</abstract>
      <url hash="e8d3f0e4">2024.acl-long.646</url>
      <bibkey>qiu-etal-2024-paircfr</bibkey>
      <doi>10.18653/v1/2024.acl-long.646</doi>
    </paper>
    <paper id="647">
      <title><fixed-case>LLM</fixed-case>s Learn Task Heuristics from Demonstrations: A Heuristic-Driven Prompting Strategy for Document-Level Event Argument Extraction</title>
      <author><first>Hanzhang</first><last>Zhou</last></author>
      <author><first>Junlang</first><last>Qian</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zijian</first><last>Feng</last></author>
      <author><first>Lu</first><last>Hui</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zixiao</first><last>Zhu</last></author>
      <author><first>Kezhi</first><last>Mao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>11972-11990</pages>
      <abstract>In this study, we explore in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting tailored for the EAE task. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations in ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a systematic method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL examples. Experiments show that our method outperforms existing prompting methods and few-shot supervised learning methods on document-level EAE datasets. Additionally, the HD-LoA prompting shows effectiveness in other tasks like sentiment analysis and natural language inference, demonstrating its broad adaptability.</abstract>
      <url hash="a046acf0">2024.acl-long.647</url>
      <bibkey>zhou-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.acl-long.647</doi>
    </paper>
    <paper id="648">
      <title>Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models</title>
      <author><first>Weihong</first><last>Zhong</last></author>
      <author><first>Xiaocheng</first><last>Feng</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Liang</first><last>Zhao</last></author>
      <author><first>Qiming</first><last>Li</last></author>
      <author><first>Lei</first><last>Huang</last></author>
      <author><first>Yuxuan</first><last>Gu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Weitao</first><last>Ma</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yuan</first><last>Xu</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>11991-12011</pages>
      <abstract>Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs’ subsequent generation. Thus, we raise a question: <tex-math>\textit{When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists?}</tex-math> To answer this, we propose a framework called <tex-math>\\textit{MMHalSnowball}</tex-math> to evaluate LVLMs’ behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least <tex-math>31\\%</tex-math>, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would not have supported without distractions. We term this <tex-math>\textit{Multimodal Hallucination Snowballing}</tex-math>. To mitigate this issue, we further propose a training-free method called <tex-math>\textit{Residual Visual Decoding},</tex-math> where we revise the output distribution of LVLMs with the one derived from the residual visual input, providing models with direct access to the visual information. Experiments show that our method can mitigate more than <tex-math>24\\%</tex-math> of the snowballed multimodal hallucination while maintaining capabilities.</abstract>
      <url hash="e0e0e93e">2024.acl-long.648</url>
      <bibkey>zhong-etal-2024-investigating</bibkey>
      <doi>10.18653/v1/2024.acl-long.648</doi>
    </paper>
    <paper id="649">
      <title>m<fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>: Multilingual Instruction Tuning for Reasoning Consistency in Language Models</title>
      <author><first>Huiyuan</first><last>Lai</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Malvina</first><last>Nissim</last><affiliation>University of Groningen</affiliation></author>
      <pages>12012-12026</pages>
      <abstract>Large language models (LLMs) with Chain-of-thought (CoT) have recently emerged as a powerful technique for eliciting reasoning to improve various downstream tasks. As most research mainly focuses on English, with few explorations in a multilingual context, the question of how reliable this reasoning capability is in different languages is still open. To address it directly, we study multilingual reasoning consistency across multiple languages, using popular open-source LLMs. First, we compile the first large-scale multilingual math reasoning dataset, *mCoT-MATH*, covering eleven diverse languages. Then, we introduce multilingual CoT instruction tuning to boost reasoning capability across languages, thereby improving model consistency. While existing LLMs show substantial variation across the languages we consider, and especially low performance for lesser resourced languages, our 7B parameter model *mCoT* achieves impressive consistency across languages, and superior or comparable performance to close- and open-source models even of much larger sizes.</abstract>
      <url hash="91847dd6">2024.acl-long.649</url>
      <bibkey>lai-nissim-2024-mcot</bibkey>
      <doi>10.18653/v1/2024.acl-long.649</doi>
    </paper>
    <paper id="650">
      <title><fixed-case>G</fixed-case>un<fixed-case>S</fixed-case>tance: Stance Detection for Gun Control and Gun Regulation</title>
      <author><first>Nikesh</first><last>Gyawali</last></author>
      <author><first>Iustin</first><last>Sirbu</last></author>
      <author><first>Tiberiu</first><last>Sosea</last></author>
      <author><first>Sarthak</first><last>Khanal</last></author>
      <author><first>Doina</first><last>Caragea</last><affiliation>Kansas State University</affiliation></author>
      <author><first>Traian</first><last>Rebedea</last><affiliation>NVIDIA and University Politehnica of Bucharest</affiliation></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>12027-12044</pages>
      <abstract>The debate surrounding gun control and gun regulation in the United States has intensified in the wake of numerous mass shooting events. As perspectives on this matter vary, it becomes increasingly important to comprehend individuals’ positions. Stance detection, the task of determining an author’s position towards a proposition or target, has gained attention for its potential use in understanding public perceptions towards controversial topics and identifying the best strategies to address public concerns. In this paper, we present GunStance, a dataset of tweets pertaining to shooting events, focusing specifically on the controversial topics of “banning guns” versus “regulating guns.” The tweets in the dataset are sourced from discussions on Twitter following various shooting incidents in the United States. Amazon Mechanical Turk was used to manually annotate a subset of the tweets relevant to the targets of interest (“banning guns” and “regulating guns”) into three classes: In-Favor, Against, and Neutral. The remaining unlabeled tweets are included in the dataset to facilitate studies on semi-supervised learning (SSL) approaches that can help address the scarcity of the labeled data in stance detection tasks. Furthermore, we propose a hybrid approach that combines curriculum-based SSL and Large Language Models (LLM), and show that the proposed approach outperforms supervised, semi-supervised, and LLM-based zero-shot models in most experiments on our assembled dataset.</abstract>
      <url hash="319cbc99">2024.acl-long.650</url>
      <bibkey>gyawali-etal-2024-gunstance</bibkey>
      <doi>10.18653/v1/2024.acl-long.650</doi>
    </paper>
    <paper id="651">
      <title>Beyond Traditional Benchmarks: Analyzing Behaviors of Open <fixed-case>LLM</fixed-case>s on Data-to-Text Generation</title>
      <author><first>Zdeněk</first><last>Kasner</last></author>
      <author><first>Ondrej</first><last>Dusek</last><affiliation>Charles University, Prague</affiliation></author>
      <pages>12045-12072</pages>
      <abstract>We analyze the behaviors of open large language models (LLMs) on the task of data-to-text (D2T) generation, i.e., generating coherent and relevant text from structured data. To avoid the issue of LLM training data contamination with standard benchmarks, we design Quintd - a tool for collecting novel structured data records from public APIs. We find that open LLMs (Llama 2, Mistral, and Zephyr) can generate fluent and coherent texts in zero-shot settings from data in common formats collected with Quintd. However, we show that the semantic accuracy of the outputs is a major issue: both according to human annotators and our reference-free metric based on GPT-4, more than 80% of the outputs of open LLMs contain at least one semantic error. We publicly release the code, data, and model outputs.</abstract>
      <url hash="6c81317f">2024.acl-long.651</url>
      <bibkey>kasner-dusek-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.acl-long.651</doi>
    </paper>
    <paper id="652">
      <title>Don’t Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of <fixed-case>LLM</fixed-case>s in Implicit Hate Speech Detection</title>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Jianfeng</first><last>He</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Taoran</first><last>Ji</last><affiliation>Texas A&amp;M University - Corpus Christi</affiliation></author>
      <author><first>Chang-Tien</first><last>Lu</last><affiliation>Virginia Tech</affiliation></author>
      <pages>12073-12086</pages>
      <abstract>The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech and express confidence in their responses. Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs’ confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset’s complexity. Consequently, the calibration performance is heavily reliant on primary classification accuracy. These discoveries unveil new limitations of LLMs, underscoring the need for caution when optimizing models to ensure they do not veer towards extremes. This serves as a reminder to carefully consider sensitivity and confidence in the pursuit of model fairness.</abstract>
      <url hash="9c66bc52">2024.acl-long.652</url>
      <bibkey>zhang-etal-2024-dont-go</bibkey>
      <doi>10.18653/v1/2024.acl-long.652</doi>
    </paper>
    <paper id="653">
      <title>Don’t Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation</title>
      <author><first>Giorgos</first><last>Vernikos</last></author>
      <author><first>Andrei</first><last>Popescu-Belis</last><affiliation>EPFL - EPF Lausanne and HEIG-VD, Switzerland</affiliation></author>
      <pages>12087-12105</pages>
      <abstract>Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method that synthesizes translations using a quality estimation metric (QE), which correlates better with human judgments. QE-fusion leverages a pool of candidates sampled from a model, combining spans from different candidates using a QE metric such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, Mistral, ALMA, and Tower) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generates novel translations in over half of the cases and consistently outperforms other methods across varying numbers of candidates (5–200). Furthermore, we empirically establish that QE-fusion scales linearly with the number of candidates in the pool.</abstract>
      <url hash="0a51fed0">2024.acl-long.653</url>
      <bibkey>vernikos-popescu-belis-2024-dont</bibkey>
      <doi>10.18653/v1/2024.acl-long.653</doi>
    </paper>
    <paper id="654">
      <title>Generating and Evaluating Plausible Explanations for Knowledge Graph Completion</title>
      <author><first>Antonio</first><last>Di Mauro</last><affiliation>Niuma s.r.l</affiliation></author>
      <author><first>Zhao</first><last>Xu</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Wiem</first><last>Ben Rim</last></author>
      <author><first>Timo</first><last>Sztyler</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Carolin</first><last>Lawrence</last><affiliation>NEC Laboratories Europe and NEC Laboratories Europe</affiliation></author>
      <pages>12106-12118</pages>
      <abstract>Explanations for AI should aid human users, yet this ultimate goal remains under-explored. This paper aims to bridge this gap by investigating the specific explanatory needs of human users in the context of Knowledge Graph Completion (KGC) systems. In contrast to the prevailing approaches that primarily focus on mathematical theories, we recognize the potential limitations of explanations that may end up being overly complex or nonsensical for users. Through in-depth user interviews, we gain valuable insights into the types of KGC explanations users seek. Building upon these insights, we introduce GradPath, a novel path-based explanation method designed to meet human-centric explainability constraints and enhance plausibility. Additionally, GradPath harnesses the gradients of the trained KGC model to maintain a certain level of faithfulness. We verify the effectiveness of GradPath through well-designed human-centric evaluations. The results confirm that our method provides explanations that users consider more plausible than previous ones.</abstract>
      <url hash="43149332">2024.acl-long.654</url>
      <bibkey>di-mauro-etal-2024-generating</bibkey>
      <doi>10.18653/v1/2024.acl-long.654</doi>
    </paper>
    <paper id="655">
      <title>One Prompt To Rule Them All: <fixed-case>LLM</fixed-case>s for Opinion Summary Evaluation</title>
      <author><first>Tejpalsingh</first><last>Siledar</last></author>
      <author><first>Swaroop</first><last>Nath</last></author>
      <author><first>Sankara</first><last>Muddu</last></author>
      <author><first>Rupasai</first><last>Rangaraju</last></author>
      <author><first>Swaprava</first><last>Nath</last><affiliation>Computer Science and Engineering, Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Suman</first><last>Banerjee</last><affiliation>Flipkart</affiliation></author>
      <author><first>Amey</first><last>Patil</last></author>
      <author><first>Sudhanshu</first><last>Singh</last></author>
      <author><first>Muthusamy</first><last>Chelliah</last><affiliation>Flipkart</affiliation></author>
      <author><first>Nikesh</first><last>Garera</last></author>
      <pages>12119-12134</pages>
      <abstract>Evaluation of opinion summaries using conventional reference-based metrics often fails to provide a comprehensive assessment and exhibits limited correlation with human judgments. While Large Language Models (LLMs) have shown promise as reference-free metrics for NLG evaluation, their potential remains unexplored for opinion summary evaluation. Furthermore, the absence of sufficient opinion summary evaluation datasets hinders progress in this area. In response, we introduce the SUMMEVAL-OP dataset, encompassing 7 dimensions crucial to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We propose OP-I-PROMPT, a dimension-independent prompt, along with OP-PROMPTS, a dimension-dependent set of prompts for opinion summary evaluation. Our experiments demonstrate that OP-I-PROMPT emerges as a good alternative for evaluating opinion summaries, achieving an average Spearman correlation of 0.70 with human judgments, surpassing prior methodologies. Remarkably, we are the first to explore the efficacy of LLMs as evaluators, both on closed-source and open-source models, in the opinion summary evaluation domain.</abstract>
      <url hash="86baf681">2024.acl-long.655</url>
      <bibkey>siledar-etal-2024-one</bibkey>
      <doi>10.18653/v1/2024.acl-long.655</doi>
    </paper>
    <paper id="656">
      <title><fixed-case>LAND</fixed-case>e<fixed-case>RMT</fixed-case>: Dectecting and Routing Language-Aware Neurons for Selectively Finetuning <fixed-case>LLM</fixed-case>s to Machine Translation</title>
      <author><first>Shaolin</first><last>Zhu</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Leiyu</first><last>Pan</last><affiliation>Tianjin University</affiliation></author>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>Baidu Inc</affiliation></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>12135-12148</pages>
      <abstract>Recent advancements in large language models (LLMs) have shown promising results in multilingual translation even with limited bilingual supervision. The major challenges are catastrophic forgetting and parameter interference for finetuning LLMs when provided parallel training data. To address these challenges, we propose LANDeRMT, a Language-Aware Neuron Detecting and Routing framework that selectively finetunes LLMs to Machine Translation with diverse translation training data. In LANDeRMT, we evaluate the awareness of neurons to MT tasks and categorize them into language-general and language-specific neurons. This categorization enables selective parameter updates during finetuning, mitigating parameter interference and catastrophic forgetting issues. For the detected neurons, we further propose a conditional awareness-based routing mechanism to dynamically adjust language-general and language-specific capacity within LLMs, guided by translation signals. Experimental results demonstrate that the proposed LANDeRMT is very effective in learning translation knowledge, significantly improving translation quality over various strong baselines for multiple language pairs.</abstract>
      <url hash="67de2f44">2024.acl-long.656</url>
      <bibkey>zhu-etal-2024-landermt</bibkey>
      <doi>10.18653/v1/2024.acl-long.656</doi>
    </paper>
    <paper id="657">
      <title>A Joint Coreference-Aware Approach to Document-Level Target Sentiment Analysis</title>
      <author><first>Hongjie</first><last>Cai</last></author>
      <author><first>Heqing</first><last>Ma</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Jianfei</first><last>Yu</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Rui</first><last>Xia</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <pages>12149-12160</pages>
      <abstract>Most existing work on aspect-based sentiment analysis (ABSA) focuses on the sentence level, while research at the document level has not received enough attention. Compared to sentence-level ABSA, the document-level ABSA is not only more practical but also requires holistic document-level understanding capabilities such as coreference resolution. To investigate the impact of coreference information on document-level ABSA, we conduct a three-stage research for the document-level target sentiment analysis (DTSA) task: 1) exploring the effectiveness of coreference information for the DTSA task; 2) reducing the reliance on manually annotated coreference information; 3) alleviating the evaluation bias caused by missing the coreference information of opinion targets. Specifically, we first manually annotate the coreferential opinion targets and propose a multi-task learning framework to jointly model the DTSA task and the coreference resolution task. Then we annotate the coreference information with ChatGPT for joint training. Finally, to address the issue of missing coreference targets, we modify the metrics from strict matching to a loose matching method based on the clusters of targets. The experimental results not only demonstrate the effectiveness of our framework but also reflect the feasibility of using ChatGPT-annotated coreferential entities and the applicability of the modified metrics. Our source code is publicly released at https://github.com/NUSTM/DTSA-Coref.</abstract>
      <url hash="de44b7a2">2024.acl-long.657</url>
      <bibkey>cai-etal-2024-joint</bibkey>
      <doi>10.18653/v1/2024.acl-long.657</doi>
    </paper>
    <paper id="658">
      <title><fixed-case>V</fixed-case>is<fixed-case>D</fixed-case>ia<fixed-case>H</fixed-case>al<fixed-case>B</fixed-case>ench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models</title>
      <author><first>Qingxing</first><last>Cao</last><affiliation>SUN YAT-SEN UNIVERSITY, Tsinghua University</affiliation></author>
      <author><first>Junhao</first><last>Cheng</last></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <author><first>Liang</first><last>Lin</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>12161-12176</pages>
      <abstract>Despite the significant success of large vision-language models (LVLMs), some studies have revealed that LVLMs suffer from the hallucination problem, where the LVLMs’ response contains descriptions of non-existent objects. Although various benchmarks have been proposed to investigate this problem, they mostly focus on single-turn evaluation and overlook the hallucination raised by textual inputs. To investigate the hallucination problem of LVLMs when given long-term misleading textual history, we propose a novel visual dialogue hallucination evaluation benchmark VisDiaHalBench. The benchmark consists of samples with five-turn questions about an edited image and its original version. VisDiaHalBench differs from previous hallucination benchmarks in the following three points: 1) The questions and answers are unambiguously grounded by annotated scene graphs. 2) The images are uncommonly edited to inspect the visual model and common-object hallucination in LLMs. 3) The carefully designed dialogue refers a same object in different turns to assess the image consistency and influence of history for LVLMs. The detailed analysis of several state-of-the-art LVLMs across image consistency, visual understanding, history influence, and other dimensions reveals their substantial performance gap with single-turn VQA tasks. The benchmark is released in: https://github.com/qingxingcao/VisDiaHalBench</abstract>
      <url hash="5dfb5420">2024.acl-long.658</url>
      <bibkey>cao-etal-2024-visdiahalbench</bibkey>
      <doi>10.18653/v1/2024.acl-long.658</doi>
    </paper>
    <paper id="659">
      <title><fixed-case>A</fixed-case>uto<fixed-case>DSL</fixed-case>: Automated domain-specific language design for structural representation of procedures with constraints</title>
      <author><first>Yu-Zhe</first><last>Shi</last><affiliation>PersLab Research</affiliation></author>
      <author><first>Haofei</first><last>Hou</last></author>
      <author><first>Zhangqian</first><last>Bi</last></author>
      <author><first>Fanxu</first><last>Meng</last></author>
      <author><first>Xiang</first><last>Wei</last></author>
      <author><first>Lecheng</first><last>Ruan</last><affiliation>Peking University</affiliation></author>
      <author><first>Qining</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <pages>12177-12214</pages>
      <abstract>Accurate representation of procedures in restricted scenarios, such as non-standardized scientific experiments, requires precise depiction of constraints. Unfortunately, Domain-specific Language (DSL), as an effective tool to express constraints structurally, often requires case-by-case hand-crafting, necessitating customized, labor-intensive efforts. To overcome this challenge, we introduce the AutoDSL framework to automate DSL-based constraint design across various domains. Utilizing domain specified experimental protocol corpora, AutoDSL optimizes syntactic constraints and abstracts semantic constraints. Quantitative and qualitative analyses of the DSLs designed by AutoDSL across five distinct domains highlight its potential as an auxiliary module for language models, aiming to improve procedural planning and execution.</abstract>
      <url hash="0767b395">2024.acl-long.659</url>
      <bibkey>shi-etal-2024-autodsl</bibkey>
      <doi>10.18653/v1/2024.acl-long.659</doi>
    </paper>
    <paper id="660">
      <title>Multipath parsing in the brain</title>
      <author><first>Berta</first><last>Franzluebbers</last></author>
      <author><first>Donald</first><last>Dunagan</last></author>
      <author><first>Miloš</first><last>Stanojević</last><affiliation>University College London, University of London and Google DeepMind</affiliation></author>
      <author><first>Jan</first><last>Buys</last><affiliation>University of Cape Town</affiliation></author>
      <author><first>John</first><last>Hale</last><affiliation>Johns Hopkins University, University of Georgia and DeepMind</affiliation></author>
      <pages>12215-12229</pages>
      <abstract>Humans understand sentences word-by-word, in the order that they hear them. This incrementality entails resolving temporary ambiguities about syntactic relationships. We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook. In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one. This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset. In both English and Chinese data, we find evidence for multipath parsing. Brain regions associated with this multipath effect include bilateral superior temporal gyrus.</abstract>
      <url hash="eecd0da7">2024.acl-long.660</url>
      <bibkey>franzluebbers-etal-2024-multipath</bibkey>
      <doi>10.18653/v1/2024.acl-long.660</doi>
    </paper>
    <paper id="661">
      <title>Search-Adaptor: Embedding Customization for Information Retrieval</title>
      <author><first>Jinsung</first><last>Yoon</last><affiliation>Google</affiliation></author>
      <author><first>Yanfei</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Sercan</first><last>Arik</last><affiliation>Google</affiliation></author>
      <author><first>Tomas</first><last>Pfister</last><affiliation>Google</affiliation></author>
      <pages>12230-12247</pages>
      <abstract>Embeddings extracted by pre-trained Large Language Models (LLMs) have significant potential to improve information retrieval and search. Beyond the zero-shot setup in which they are being conventionally used, being able to take advantage of the information from the relevant query-corpus paired data can further boost the LLM capabilities. In this paper, we propose a novel method, Search-Adaptor, for customizing LLMs for information retrieval in an efficient and robust way. Search-Adaptor modifies the embeddings generated by pre-trained LLMs, and can be integrated with any LLM, including those only available via prediction APIs. On multiple English, multilingual, and multimodal retrieval datasets, we show consistent and significant performance benefits for Search-Adaptor – e.g., more than 5% improvements for Google Embedding APIs in nDCG@10 averaged over 14 BEIR datasets.</abstract>
      <url hash="ffff8842">2024.acl-long.661</url>
      <bibkey>yoon-etal-2024-search</bibkey>
      <doi>10.18653/v1/2024.acl-long.661</doi>
    </paper>
    <paper id="662">
      <title>Back to Basics: Revisiting <fixed-case>REINFORCE</fixed-case>-Style Optimization for Learning from Human Feedback in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Arash</first><last>Ahmadian</last></author>
      <author><first>Chris</first><last>Cremer</last></author>
      <author><first>Matthias</first><last>Gallé</last><affiliation>Cohere</affiliation></author>
      <author><first>Marzieh</first><last>Fadaee</last><affiliation>Cohere For AI</affiliation></author>
      <author><first>Julia</first><last>Kreutzer</last><affiliation>Cohere for AI</affiliation></author>
      <author><first>Olivier</first><last>Pietquin</last><affiliation>Cohere and Earth Species Project</affiliation></author>
      <author><first>Ahmet</first><last>Üstün</last><affiliation>Cohere For Ai</affiliation></author>
      <author><first>Sara</first><last>Hooker</last><affiliation>Cohere For AI</affiliation></author>
      <pages>12248-12267</pages>
      <abstract>AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. Proximal Policy Optimization (PPO) has been installed by the seminal literature as the standard method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit how alignment from human preferences is formulated in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed “RL-free” methods such as DPO and RAFT. Our work suggests that careful adaptation to LLMs alignment characteristics allows benefiting from online RL optimization at low cost.</abstract>
      <url hash="ce7ebef1">2024.acl-long.662</url>
      <bibkey>ahmadian-etal-2024-back</bibkey>
      <doi>10.18653/v1/2024.acl-long.662</doi>
    </paper>
    <paper id="663">
      <title><fixed-case>VIES</fixed-case>core: Towards Explainable Metrics for Conditional Image Synthesis Evaluation</title>
      <author><first>Max</first><last>Ku</last></author>
      <author><first>Dongfu</first><last>Jiang</last></author>
      <author><first>Cong</first><last>Wei</last></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo and Google</affiliation></author>
      <pages>12268-12290</pages>
      <abstract>In the rapidly advancing field of conditional image generation research, challenges such as limited explainability lie in effectively evaluating the performance and capabilities of various models. This paper introduces VIEScore, a Visual Instruction-guided Explainable metric for evaluating any conditional image generation tasks. VIEScore leverages general knowledge from Multimodal Large Language Models (MLLMs) as the backbone and does not require training or fine-tuning. We evaluate VIEScore on seven prominent tasks in conditional image tasks and found: (1) VIEScore (GPT4-o) achieves a high Spearman correlation of 0.4 with human evaluations, while the human-to-human correlation is 0.45. (2) VIEScore (with open-source MLLM) is significantly weaker than GPT-4o and GPT-4v in evaluating synthetic images. (3) VIEScore achieves a correlation on par with human ratings in the generation tasks but struggles in editing tasks. With these results, we believe VIEScore shows its great potential to replace human judges in evaluating image synthesis tasks.</abstract>
      <url hash="362dfa2c">2024.acl-long.663</url>
      <bibkey>ku-etal-2024-viescore</bibkey>
      <doi>10.18653/v1/2024.acl-long.663</doi>
    </paper>
    <paper id="664">
      <title>Tree Transformer’s Disambiguation Ability of Prepositional Phrase Attachment and Garden Path Effects</title>
      <author><first>Lingling</first><last>Zhou</last></author>
      <author><first>Suzan</first><last>Verberne</last><affiliation>Universiteit Leiden</affiliation></author>
      <author><first>Gijs</first><last>Wijnholds</last><affiliation>Leiden University, Leiden University</affiliation></author>
      <pages>12291-12301</pages>
      <abstract>This work studies two types of ambiguity in natural language: prepositional phrase (PP) attachment ambiguity, and garden path constructions. Due to the different nature of these ambiguities – one being structural, the other incremental in nature – we pretrain and evaluate the Tree Transformer of Wang et al. (2019), an unsupervised Transformer model that induces tree representations internally. To assess PP attachment ambiguity we inspect the model’s induced parse trees against a newly prepared dataset derived from the PP attachment corpus (Ratnaparkhi et al., 1994). Measuring garden path effects is done by considering surprisal rates of the underlying language model on a number of dedicated test suites, following Futrell et al. (2019). For comparison we evaluate a pretrained supervised BiLSTM-based model trained on constituency parsing as sequence labelling (Gómez-Rodríguez and Vilares, 2018). Results show that the unsupervised Tree Transformer does exhibit garden path effects, but its parsing ability is far inferior to the supervised BiLSTM, and it is not as sensitive to lexical cues as other large LSTM models, suggesting that supervised parsers based on a pre-Transformer architecture may be the better choice in the presence of ambiguity.</abstract>
      <url hash="14b92935">2024.acl-long.664</url>
      <bibkey>zhou-etal-2024-tree</bibkey>
      <doi>10.18653/v1/2024.acl-long.664</doi>
    </paper>
    <paper id="665">
      <title>Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs</title>
      <author><first>Elan</first><last>Markowitz</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Anil</first><last>Ramakrishna</last><affiliation>Amazon</affiliation></author>
      <author><first>Jwala</first><last>Dhamala</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Charith</first><last>Peris</last><affiliation>Amazon</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California, University of Southern California, University of Southern California and Amazon Alexa</affiliation></author>
      <pages>12302-12319</pages>
      <abstract>Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing reliable, structured, domain-specific, and up-to-date external knowledge. However, KGs and LLMs are often developed separately and must be integrated after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning algorithm that enables augmentation of black-box LLMs with one or more KGs. The algorithm equips a LLM with actions for interfacing a KG and enables the LLM to perform tree search over possible thoughts and actions to find high confidence reasoning paths. Tree-of-Traversals significantly improves performance on question answering and KG question answering tasks. Code is available at https://github.com/amazon-science/tree-of-traversals</abstract>
      <url hash="cf1c01b0">2024.acl-long.665</url>
      <bibkey>markowitz-etal-2024-tree</bibkey>
      <doi>10.18653/v1/2024.acl-long.665</doi>
    </paper>
    <paper id="666">
      <title>Structured Tree Alignment for Evaluation of (Speech) Constituency Parsing</title>
      <author><first>Freda</first><last>Shi</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Kevin</first><last>Gimpel</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Karen</first><last>Livescu</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <pages>12320-12332</pages>
      <abstract>We present the structured average intersection-over-union ratio (STRUCT-IOU), an evaluation metric that compares a constituency parse tree over automatically recognized spoken word boundaries with the ground-truth parse tree over written words. To compute the metric, we (1) project the ground-truth parse tree to the speech domain by forced alignment, (2) align the projected ground-truth constituents with the predicted ones under certain structured constraints, and (3) calculate the average IOU score across all aligned constituent pairs. STRUCT-IOU takes word boundaries into account and overcomes the challenge that the predicted words and ground truth may not have perfect one-to-one correspondence. Extending to the evaluation of text constituency parsing, we demonstrate that STRUCT-IOU shows higher tolerance to syntactically plausible parses than PARSEVAL (Black et al., 1991).</abstract>
      <url hash="05a589ff">2024.acl-long.666</url>
      <bibkey>shi-etal-2024-structured</bibkey>
      <doi>10.18653/v1/2024.acl-long.666</doi>
    </paper>
    <paper id="667">
      <title><fixed-case>V</fixed-case>i<fixed-case>SAG</fixed-case>e: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation</title>
      <author><first>Akshita</first><last>Jha</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last><affiliation>Google</affiliation></author>
      <author><first>Remi</first><last>Denton</last><affiliation>Google</affiliation></author>
      <author><first>Sarah</first><last>Laszlo</last><affiliation>Research, Google</affiliation></author>
      <author><first>Shachi</first><last>Dave</last><affiliation>Research, Google</affiliation></author>
      <author><first>Rida</first><last>Qadri</last><affiliation>Google</affiliation></author>
      <author><first>Chandan</first><last>Reddy</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Sunipa</first><last>Dev</last><affiliation>Google</affiliation></author>
      <pages>12333-12347</pages>
      <abstract>Recent studies have shown that Text-to-Image (T2I) model generations can reflect social stereotypes present in the real world. However, existing approaches for evaluating stereotypes have a noticeable lack of coverage of global identity groups and their associated stereotypes. To address this gap, we introduce the ViSAGe (Visual Stereotypes Around the Globe) dataset to enable the evaluation of known nationality-based stereotypes in T2I models, across 135 nationalities. We enrich an existing textual stereotype resource by distinguishing between stereotypical associations that are more likely to have visual depictions, such as ‘sombrero’, from those that are less visually concrete, such as ‘attractive’. We demonstrate ViSAGe’s utility through a multi-faceted evaluation of T2I generations. First, we show that stereotypical attributes in ViSAGe are thrice as likely to be present in generated images of corresponding identities as compared to other attributes, and that the offensiveness of these depictions is especially higher for identities from Africa, South America, and South East Asia. Second, we assess the ‘stereotypical pull’ of visual depictions of identity groups, which reveals how the ‘default’ representations of all identity groups in ViSAGe have a pull towards stereotypical depictions, and that this pull is even more prominent for identity groups from the Global South. CONTENT WARNING: Some examples contain offensive stereotypes.</abstract>
      <url hash="0f279a9e">2024.acl-long.667</url>
      <bibkey>jha-etal-2024-visage</bibkey>
      <doi>10.18653/v1/2024.acl-long.667</doi>
    </paper>
    <paper id="668">
      <title>Transferable and Efficient Non-Factual Content Detection via Probe Training with Offline Consistency Checking</title>
      <author><first>Xiaokang</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zijun</first><last>Yao</last></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Kaifeng</first><last>Yun</last></author>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>12348-12364</pages>
      <abstract>This paper proposes PiNose, which trains a probing model on offline self-consistency checking results, thereby circumventing the need for human-annotated data and achieving transferability across diverse data distributions. As the consistency check process is offline, PiNose reduces the computational burden of generating multiple responses by online consistency verification. Additionally, it examines various aspects of internal states prior to response decoding, contributing to more effective detection of factual inaccuracies. Experiment results on both factuality detection and question answering benchmarks show that PiNose achieves surpassing results than existing factuality detection methods.</abstract>
      <url hash="639ba67f">2024.acl-long.668</url>
      <bibkey>zhang-etal-2024-transferable</bibkey>
      <doi>10.18653/v1/2024.acl-long.668</doi>
    </paper>
    <paper id="669">
      <title>What Do Language Models Learn in Context? The Structured Task Hypothesis.</title>
      <author><first>Jiaoda</first><last>Li</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Yifan</first><last>Hou</last><affiliation>Department of Computer Science, Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>12365-12379</pages>
      <abstract>Large language models (LLMs) exhibit an intriguing ability to learn a novel task from in-context examples presented in a demonstration, termed in-context learning (ICL). Understandably, a swath of research has been dedicated to uncovering the theories underpinning ICL. One popular hypothesis explains ICL by task selection. LLMs identify the task based on the demonstration and generalize it to the prompt. Another popular hypothesis is that ICL is a form of meta-learning, i.e., the models learn a learning algorithm at pre-training time and apply it to the demonstration. Finally, a third hypothesis argues that LLMs use the demonstration to select a composition of tasks learned during pre-training to perform ICL. In this paper, we empirically explore these three hypotheses that explain LLMs’ ability to learn in context with a suite of experiments derived from common text classification tasks. We invalidate the first two hypotheses with counterexamples and provide evidence in support of the last hypothesis. Our results suggest an LLM could learn a novel task in context via composing tasks learned during pre-training.</abstract>
      <url hash="947f82bf">2024.acl-long.669</url>
      <bibkey>li-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.acl-long.669</doi>
    </paper>
    <paper id="670">
      <title>Agent Lumos: Unified and Modular Training for Open-Source Language Agents</title>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Faeze</first><last>Brahman</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Abhilasha</first><last>Ravichander</last><affiliation>Allen Institute for Artificial Intelligence and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Khyathi</first><last>Chandu</last></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <pages>12380-12403</pages>
      <abstract>Closed-source agents suffer from several issues such as a lack of affordability, transparency, and reproducibility, particularly on complex interactive tasks. This motivates the development of open-source alternatives. We introduce Lumos, one of the first frameworks for training open-source LLM-based agents. Lumos features a learnable, unified and modular architecture with a planning module that learns high-level subgoal generation, and a grounding module trained to translate these into the actions using various tools in the execution module. The design allows for modular upgrades and wider applicability to diverse interactive tasks. To foster generalizable agent learning, we collect large-scale, unified, and high-quality training annotations derived from diverse ground-truth reasoning rationales across various complex interactive tasks. On 9 datasets, Lumos exhibits several key advantages: (1) Lumos excels multiple larger open-source agents on the held-out datasets (unused for training) for each task type. Lumos even surpasses GPT agents on QA and web tasks; (2) Lumos outperforms open-source agents produced by chain-of-thoughts and unmodularized integrated training; and (3) Lumos effectively generalizes to unseen tasks, outperforming 33B-scale agents and domain-specific agents. Code and data will be released.</abstract>
      <url hash="9087c8f4">2024.acl-long.670</url>
      <bibkey>yin-etal-2024-agent</bibkey>
      <doi>10.18653/v1/2024.acl-long.670</doi>
    </paper>
    <paper id="671">
      <title>Investigating Cultural Alignment of Large Language Models</title>
      <author><first>Badr</first><last>AlKhamissi</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Muhammad</first><last>ElNokrashy</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mai</first><last>Alkhamissi</last></author>
      <author><first>Mona</first><last>Diab</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>12404-12422</pages>
      <abstract>The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions—firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.</abstract>
      <url hash="eda78b62">2024.acl-long.671</url>
      <bibkey>alkhamissi-etal-2024-investigating</bibkey>
      <doi>10.18653/v1/2024.acl-long.671</doi>
    </paper>
    <paper id="672">
      <title>More Victories, Less Cooperation: Assessing Cicero’s Diplomacy Play</title>
      <author><first>Wichayaporn</first><last>Wongkamjan</last><affiliation>Department of Computer Science, University of Maryland, College Park</affiliation></author>
      <author><first>Feng</first><last>Gu</last></author>
      <author><first>Yanze</first><last>Wang</last></author>
      <author><first>Ulf</first><last>Hermjakob</last><affiliation>University of Southern California Information Sciences Institute</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <author><first>Brandon</first><last>Stewart</last></author>
      <author><first>Jonathan</first><last>Kummerfeld</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Denis</first><last>Peskoff</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>12423-12441</pages>
      <abstract>The boardgame Diplomacy is a challenging setting for communicative and cooperative artificial intelligence. The most prominent communicative Diplomacy AI, Cicero, has excellent strategic abilities, exceeding human players. However, the best Diplomacy players master communication, not just tactics, which is why the game has received attention as an AI challenge. This work seeks to understand the degree to which Cicero succeeds at communication. First, we annotate in-game communication with abstract meaning representation to separate in-game tactics from general language. Second, we run two dozen games with humans and Cicero, totaling over 200 human-player hours of competition. While AI can consistently outplay human players, AI-Human communication is still limited because of AI’s difficulty with deception and persuasion. This shows that Cicero relies on strategy and has not yet reached the full promise of communicative and cooperative AI.</abstract>
      <url hash="928477b4">2024.acl-long.672</url>
      <bibkey>wongkamjan-etal-2024-victories</bibkey>
      <doi>10.18653/v1/2024.acl-long.672</doi>
    </paper>
    <paper id="673">
      <title><fixed-case>V</fixed-case>oice<fixed-case>C</fixed-case>raft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</title>
      <author><first>Puyuan</first><last>Peng</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Po-Yao</first><last>Huang</last><affiliation>Meta</affiliation></author>
      <author><first>Shang-Wen</first><last>Li</last><affiliation>Facebook</affiliation></author>
      <author><first>Abdelrahman</first><last>Mohamed</last><affiliation>Rembrand Inc</affiliation></author>
      <author><first>David</first><last>Harwath</last><affiliation>University of Texas, Austin</affiliation></author>
      <pages>12442-12462</pages>
      <abstract>We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALL-E and the popular commercial model XTTS v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named . We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web. Data, code, and model weights are available at https://github.com/jasonppy/VoiceCraft</abstract>
      <url hash="49ce59ae">2024.acl-long.673</url>
      <bibkey>peng-etal-2024-voicecraft</bibkey>
      <doi>10.18653/v1/2024.acl-long.673</doi>
    </paper>
    <paper id="674">
      <title><fixed-case>RAID</fixed-case>: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors</title>
      <author><first>Liam</first><last>Dugan</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Alyssa</first><last>Hwang</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Filip</first><last>Trhlík</last></author>
      <author><first>Andrew</first><last>Zhu</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Josh Magnus</first><last>Ludan</last></author>
      <author><first>Hainiu</first><last>Xu</last></author>
      <author><first>Daphne</first><last>Ippolito</last></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>Allen Institute for Artificial Intelligence and University of Pennsylvania</affiliation></author>
      <pages>12463-12492</pages>
      <abstract>Many commercial and open-source models claim to detect machine-generated text with extremely high accuracy (99% or more). However, very few of these detectors are evaluated on shared benchmark datasets and even when they are, the datasets used for evaluation are insufficiently challenging—lacking variations in sampling strategy, adversarial attacks, and open-source generative models. In this work we present RAID: the largest and most challenging benchmark dataset for machine-generated text detection. RAID includes over 6 million generations spanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding strategies. Using RAID, we evaluate the out-of-domain and adversarial robustness of 8 open- and 4 closed-source detectors and find that current detectors are easily fooled by adversarial attacks, variations in sampling strategies, repetition penalties, and unseen generative models. We release our data along with a leaderboard to encourage future research.</abstract>
      <url hash="9246fd21">2024.acl-long.674</url>
      <bibkey>dugan-etal-2024-raid</bibkey>
      <doi>10.18653/v1/2024.acl-long.674</doi>
    </paper>
    <paper id="675">
      <title>Silent Signals, Loud Impact: <fixed-case>LLM</fixed-case>s for Word-Sense Disambiguation of Coded Dog Whistles</title>
      <author><first>Julia</first><last>Kruk</last></author>
      <author><first>Michela</first><last>Marchini</last></author>
      <author><first>Rijul</first><last>Magu</last></author>
      <author><first>Caleb</first><last>Ziems</last></author>
      <author><first>David</first><last>Muchlinski</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>12493-12509</pages>
      <abstract>A dog whistle is a form of coded communication that carries a secondary meaning to specific audiences and is often weaponized for racial and socioeconomic discrimination. Dog whistling historically originated from United States politics, but in recent years has taken root in social media as a means of evading hate speech detection systems and maintaining plausible deniability. In this paper, we present an approach for word-sense disambiguation of dog whistles from standard speech using Large Language Models (LLMs), and leverage this technique to create a dataset of 16,550 high-confidence coded examples of dog whistles used in formal and informal communication. Silent Signals is the largest dataset of disambiguated dog whistle usage, created for applications in hate speech detection, neology, and political science.</abstract>
      <url hash="8fb8a39f">2024.acl-long.675</url>
      <bibkey>kruk-etal-2024-silent</bibkey>
      <doi>10.18653/v1/2024.acl-long.675</doi>
    </paper>
    <paper id="676">
      <title>On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning</title>
      <author><first>Franz</first><last>Nowak</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Anej</first><last>Svete</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Alexandra</first><last>Butoi</last></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>12510-12548</pages>
      <abstract>The performance of modern language models (LMs) has been improved by chain-of-thought (CoT) reasoning, i.e., the process of generating intermediate results that guide the model towards a final answer. A possible explanation for this improvement is that CoT reasoning extends an LM’s computational power, as RNNs and transformers with additional scratch space are known to be Turing complete. Comparing LMs to Turing machines, however, introduces a category error—Turing machines decide language membership, whereas LMs define distributions over strings. To bridge this gap, we formalize CoT reasoning in a probabilistic setting. We present several results on the representational capacity of recurrent and transformer LMs with CoT reasoning, showing that they can represent the same family of distributions over strings as probabilistic Turing machines.</abstract>
      <url hash="95c807c3">2024.acl-long.676</url>
      <bibkey>nowak-etal-2024-representational</bibkey>
      <doi>10.18653/v1/2024.acl-long.676</doi>
    </paper>
    <paper id="677">
      <title>Analyzing <fixed-case>LLM</fixed-case> Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends</title>
      <author><first>Sanjana</first><last>Ramprasad</last></author>
      <author><first>Elisa</first><last>Ferracane</last><affiliation>Abridge AI</affiliation></author>
      <author><first>Zachary</first><last>Lipton</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>12549-12561</pages>
      <abstract>Recent advancements in large language models (LLMs) have significantly advanced the capabilities of summarization systems.However, they continue to face a persistent challenge: hallucination. While prior work has extensively examined LLMs in news domains, evaluation of dialogue summarization has primarily focused on BART-based models, resulting in a notable gap in understanding LLM effectiveness.Our work seeks to address this gap by benchmarking LLMs for dialogue summarization faithfulness using human annotations,focusing on identifying and categorizing span-level inconsistencies.Specifically, we evaluate two prominent LLMs: GPT-4 and Alpaca-13B.Our evaluation reveals that LLMs often generate plausible, but not fully supported inferences based on conversation contextual cues, a trait absent in older models. As a result, we propose a refined taxonomy of errors, introducing a novel category termed “Contextual Inference” to address this aspect of LLM behavior. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models. Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors effectively. To address this, we introduce two prompt-based approaches for fine-grained error detection. Our methods outperform existing metrics, particularly in identifying the novel “Contextual Inference” error type.</abstract>
      <url hash="f1468ef3">2024.acl-long.677</url>
      <bibkey>ramprasad-etal-2024-analyzing</bibkey>
      <doi>10.18653/v1/2024.acl-long.677</doi>
    </paper>
    <paper id="678">
      <title><fixed-case>LLM</fixed-case> in a flash: Efficient Large Language Model Inference with Limited Memory</title>
      <author><first>Keivan</first><last>Alizadeh</last><affiliation>Apple</affiliation></author>
      <author><first>Seyed Iman</first><last>Mirzadeh</last><affiliation>Apple</affiliation></author>
      <author><first>Dmitry</first><last>Belenko</last><affiliation>Apple and Depthwise LLC</affiliation></author>
      <author><first>S.</first><last>Khatamifard</last><affiliation>Apple</affiliation></author>
      <author><first>Minsik</first><last>Cho</last></author>
      <author><first>Carlo C</first><last>Del Mundo</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Mohammad</first><last>Rastegari</last><affiliation>Apple and Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Mehrdad</first><last>Farajtabar</last><affiliation>Apple</affiliation></author>
      <pages>12562-12584</pages>
      <abstract>Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, “windowing” strategically reduces data transfer by reusing previously activated neurons, and second, “row-column bundling”, tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.</abstract>
      <url hash="cf04d63a">2024.acl-long.678</url>
      <bibkey>alizadeh-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.acl-long.678</doi>
    </paper>
    <paper id="679">
      <title>Video-<fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>: Towards Detailed Video Understanding via Large Vision and Language Models</title>
      <author><first>Muhammad</first><last>Maaz</last></author>
      <author><first>Hanoona</first><last>Rasheed</last></author>
      <author><first>Salman</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Australian National University</affiliation></author>
      <author><first>Fahad</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Linköping University</affiliation></author>
      <pages>12585-12602</pages>
      <abstract>Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of <i>video-based conversation</i> by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Code: https://github.com/mbzuai-oryx/Video-ChatGPT.</abstract>
      <url hash="b2b57f09">2024.acl-long.679</url>
      <bibkey>maaz-etal-2024-video</bibkey>
      <doi>10.18653/v1/2024.acl-long.679</doi>
    </paper>
    <paper id="680">
      <title>To Distill or Not to Distill? On the Robustness of Robust Knowledge Distillation</title>
      <author><first>Abdul</first><last>Waheed</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Karima</first><last>Kadaoui</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>12603-12621</pages>
      <abstract>Arabic is known to present unique challengesfor Automatic Speech Recognition (ASR). Onone hand, its rich linguistic diversity andwide range of dialects complicate the de-velopment of robust, inclusive models. Onthe other, current multilingual ASR modelsare compute-intensive and lack proper com-prehensive evaluations. In light of thesechallenges, we distill knowledge from largeteacher models into smaller student variantsthat more efficient. We also introduce a novelhuman-annotated dataset covering five under-represented Arabic dialects for evaluation. Wefurther evaluate both our models and existingSoTA multilingual models on both standardavailable benchmarks and our new dialectaldata. Our best-distilled model’s overall perfor-mance (45.0% WER) surpasses that of a SoTAmodel twice its size (SeamlessM4T-large-v2,WER=47.0%) and its teacher model (Whisper-large-v2, WER=55.1%), and its average perfor-mance on our new dialectal data (56.9% WER)outperforms all other models. To gain more in-sight into the poor performance of these modelson dialectal data, we conduct an error analysisand report the main types of errors the differentmodels tend to make. The GitHub repositoryfor the project is available at https://github.com/UBC-NLP/distill-whisper-ar.</abstract>
      <url hash="dab5b64b">2024.acl-long.680</url>
      <bibkey>waheed-etal-2024-distill</bibkey>
      <doi>10.18653/v1/2024.acl-long.680</doi>
    </paper>
    <paper id="681">
      <title><fixed-case>L</fixed-case>ayer<fixed-case>S</fixed-case>kip: Enabling Early Exit Inference and Self-Speculative Decoding</title>
      <author><first>Mostafa</first><last>Elhoushi</last><affiliation>Meta</affiliation></author>
      <author><first>Akshat</first><last>Shrivastava</last></author>
      <author><first>Diana</first><last>Liskovich</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Basil</first><last>Hosmer</last></author>
      <author><first>Bram</first><last>Wasti</last><affiliation>Research, Facebook</affiliation></author>
      <author><first>Liangzhen</first><last>Lai</last><affiliation>Facebook</affiliation></author>
      <author><first>Anas</first><last>Mahmoud</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Bilge</first><last>Acun</last><affiliation>Facebook AI Research</affiliation></author>
      <author><first>Saurabh</first><last>Agarwal</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Ahmed</first><last>Roman</last><affiliation>Broad Institute, Dana Farber Cancer Institute and Harvard Medical School</affiliation></author>
      <author><first>Ahmed</first><last>Aly</last><affiliation>Facebook</affiliation></author>
      <author><first>Beidi</first><last>Chen</last><affiliation>CMU, Carnegie Mellon University and Facebook</affiliation></author>
      <author><first>Carole-Jean</first><last>Wu</last><affiliation>Meta</affiliation></author>
      <pages>12622-12642</pages>
      <abstract>We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task. We open source our code at https://github.com/facebookresearch/LayerSkip.</abstract>
      <url hash="e06440a9">2024.acl-long.681</url>
      <bibkey>elhoushi-etal-2024-layerskip</bibkey>
      <doi>10.18653/v1/2024.acl-long.681</doi>
    </paper>
    <paper id="682">
      <title>Classist Tools: Social Class Correlates with Performance in <fixed-case>NLP</fixed-case></title>
      <author><first>Amanda</first><last>Cercas Curry</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Giuseppe</first><last>Attanasio</last><affiliation>Instituto de Telecomunicações</affiliation></author>
      <author><first>Zeerak</first><last>Talat</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>12643-12655</pages>
      <abstract>The field of sociolinguistics has studied factors affecting language use for the last century. Labov (1964) and Bernstein (1960) showed that socioeconomic class strongly influences our accents, syntax and lexicon. However, despite growing concerns surrounding fairness and bias in Natural Language Processing (NLP), there is a dearth of studies delving into the effects it may have on NLP systems. We show empirically that NLP systems’ performance is affected by speakers’ SES, potentially disadvantaging less-privileged socioeconomic groups. We annotate a corpus of 95K utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, automatic speech recognition, and grammar error correction. We find significant performance disparities that can be attributed to socioeconomic status as well as ethnicity and geographical differences. With NLP technologies becoming ever more ubiquitous and quotidian, they must accommodate all language varieties to avoid disadvantaging already marginalised groups. We argue for the inclusion of socioeconomic class in future language technologies.</abstract>
      <url hash="f5fbe45f">2024.acl-long.682</url>
      <bibkey>curry-etal-2024-classist</bibkey>
      <doi>10.18653/v1/2024.acl-long.682</doi>
    </paper>
    <paper id="683">
      <title><fixed-case>A</fixed-case>ction<fixed-case>IE</fixed-case>: Action Extraction from Scientific Literature with Programming Languages</title>
      <author><first>Xianrui</first><last>Zhong</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Yufeng</first><last>Du</last></author>
      <author><first>Siru</first><last>Ouyang</last><affiliation>University of Illinois Urbana-Champaign Champaign</affiliation></author>
      <author><first>Ming</first><last>Zhong</last></author>
      <author><first>Tingfeng</first><last>Luo</last></author>
      <author><first>Qirong</first><last>Ho</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Petuum, Inc.</affiliation></author>
      <author><first>Hao</first><last>Peng</last><affiliation>Department of Computer Science, University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>12656-12671</pages>
      <abstract>Extraction of experimental procedures from human language in scientific literature and patents into actionable sequences in robotics language holds immense significance in scientific domains. Such an action extraction task is particularly challenging given the intricate details and context-dependent nature of the instructions, especially in fields like chemistry where reproducibility is paramount. In this paper, we introduce ActionIE, a method that leverages Large Language Models (LLMs) to bridge this divide by converting actions written in natural language into executable Python code. This enables us to capture the entities of interest, and the relationship between each action, given the features of Programming Languages. Utilizing linguistic cues identified by frequent patterns, ActionIE provides an improved mechanism to discern entities of interest. While our method is broadly applicable, we exemplify its power in the domain of chemical literature, wherein we focus on extracting experimental procedures for chemical synthesis. The code generated by our method can be easily transformed into robotics language which is in high demand in scientific fields. Comprehensive experiments demonstrate the superiority of our method. In addition, we propose a graph-based metric to more accurately reflect the precision of extraction. We also develop a dataset to address the scarcity of scientific literature occurred in existing datasets.</abstract>
      <url hash="44670052">2024.acl-long.683</url>
      <bibkey>zhong-etal-2024-actionie</bibkey>
      <doi>10.18653/v1/2024.acl-long.683</doi>
    </paper>
    <paper id="684">
      <title>A Community-Centric Perspective for Characterizing and Detecting Anti-<fixed-case>A</fixed-case>sian Violence-Provoking Speech</title>
      <author><first>Gaurav</first><last>Verma</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Rynaa</first><last>Grover</last></author>
      <author><first>Jiawei</first><last>Zhou</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Binny</first><last>Mathew</last></author>
      <author><first>Jordan</first><last>Kraemer</last></author>
      <author><first>Munmun</first><last>Choudhury</last></author>
      <author><first>Srijan</first><last>Kumar</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>12672-12684</pages>
      <abstract>Violence-provoking speech – speech that implicitly or explicitly promotes violence against the members of the targeted community, contributed to a massive surge in anti-Asian crimes during the COVID-19 pandemic. While previous works have characterized and built tools for detecting other forms of harmful speech, like fear speech and hate speech, our work takes a community-centric approach to studying anti-Asian violence-provoking speech. Using data from ~420k Twitter posts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we develop a codebook to characterize anti-Asian violence-provoking speech and collect a community-crowdsourced dataset to facilitate its large-scale detection using state-of-the-art classifiers. We contrast the capabilities of natural language processing classifiers, ranging from BERT-based to LLM-based classifiers, in detecting violence-provoking speech with their capabilities to detect anti-Asian hateful speech. In contrast to prior work that has demonstrated the effectiveness of such classifiers in detecting hateful speech (<tex-math>F_1</tex-math> = 0.89), our work shows that accurate and reliable detection of violence-provoking speech is a challenging task (<tex-math>F_1</tex-math> = 0.69). We discuss the implications of our findings, particularly the need for proactive interventions to support Asian communities during public health crises.</abstract>
      <url hash="b9bec907">2024.acl-long.684</url>
      <bibkey>verma-etal-2024-community</bibkey>
      <doi>10.18653/v1/2024.acl-long.684</doi>
    </paper>
    <paper id="685">
      <title>Retaining Key Information under High Compression Ratios: Query-Guided Compressor for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zhiwei</first><last>Cao</last></author>
      <author><first>Qian</first><last>Cao</last></author>
      <author><first>Yu</first><last>Lu</last></author>
      <author><first>Ningxin</first><last>Peng</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Luyang</first><last>Huang</last><affiliation>Bytedance AI Lab</affiliation></author>
      <author><first>Shanbo</first><last>Cheng</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <pages>12685-12695</pages>
      <abstract>The growing popularity of Large Language Models has sparked interest in context compression for Large Language Models (LLMs). However, the performance of previous methods degrades dramatically as compression ratios increase, sometimes even falling to the closed-book level. This decline can be attributed to the loss of key information during the compression process. Our preliminary study supports this hypothesis, emphasizing the significance of retaining key information to maintain model performance under high compression ratios. As a result, we introduce Query-Guided Compressor (QGC), which leverages queries to guide the context compression process, effectively preserving key information within the compressed context. Additionally, we employ a dynamic compression strategy. We validate the effectiveness of our proposed QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets. Experimental results show that QGC can consistently perform well even at high compression ratios, which also offers significant benefits in terms of inference cost and throughput.</abstract>
      <url hash="92a12cf1">2024.acl-long.685</url>
      <bibkey>cao-etal-2024-retaining</bibkey>
      <doi>10.18653/v1/2024.acl-long.685</doi>
    </paper>
    <paper id="686">
      <title><fixed-case>COSMIC</fixed-case>: Mutual Information for Task-Agnostic Summarization Evaluation</title>
      <author><first>Maxime</first><last>Darrin</last></author>
      <author><first>Philippe</first><last>Formont</last><affiliation>École de technologie supérieure, Université du Québec and Université Paris-Saclay</affiliation></author>
      <author><first>Jackie</first><last>Cheung</last><affiliation>McGill University, Mila Research Institute and Microsoft</affiliation></author>
      <author><first>Pablo</first><last>Piantanida</last><affiliation>Mila - Quebec AI Institute and Université Paris-Saclay, CNRS</affiliation></author>
      <pages>12696-12717</pages>
      <abstract>Assessing the quality of summarizers poses significant challenges—gold summaries are hard to obtain and their suitability depends on the use context of the summarization system. Who is the user of the system, and what do they intend to do with the summary? In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries while preserving task outcomes. We theoretically establish both a lower and upper bound on the expected error rate of these tasks, which depends on the mutual information between source texts and generated summaries. We introduce COSMIC, a practical implementation of this metric, and demonstrate its strong correlation with human judgment-based metrics, as well as its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like BERTScore and ROUGE highlight the competitive performance of COSMIC.</abstract>
      <url hash="561012d3">2024.acl-long.686</url>
      <bibkey>darrin-etal-2024-cosmic</bibkey>
      <doi>10.18653/v1/2024.acl-long.686</doi>
    </paper>
    <paper id="687">
      <title><fixed-case>EUROPA</fixed-case>: A Legal Multilingual Keyphrase Generation Dataset</title>
      <author><first>Olivier</first><last>Salaün</last><affiliation>Département d’informatique et de recherche opérationnelle (DIRO), Université de Montreal</affiliation></author>
      <author><first>Frédéric</first><last>Piedboeuf</last><affiliation>CTA</affiliation></author>
      <author><first>Guillaume</first><last>Le Berre</last><affiliation>Université de Montréal, Université de Lorraine and University of Lorraine</affiliation></author>
      <author><first>David</first><last>Alfonso-Hermelo</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Philippe</first><last>Langlais</last><affiliation>Université de Montréal</affiliation></author>
      <pages>12718-12736</pages>
      <abstract>Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present EUROPA, a novel dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.</abstract>
      <url hash="e1f580a4">2024.acl-long.687</url>
      <bibkey>salaun-etal-2024-europa</bibkey>
      <doi>10.18653/v1/2024.acl-long.687</doi>
    </paper>
    <paper id="688">
      <title><fixed-case>GLIMPSE</fixed-case>: Pragmatically Informative Multi-Document Summarization for Scholarly Reviews</title>
      <author><first>Maxime</first><last>Darrin</last></author>
      <author><first>Ines</first><last>Arous</last></author>
      <author><first>Pablo</first><last>Piantanida</last><affiliation>Mila - Quebec AI Institute and Université Paris-Saclay, CNRS</affiliation></author>
      <author><first>Jackie</first><last>Cheung</last><affiliation>McGill University, Mila Research Institute and Microsoft</affiliation></author>
      <pages>12737-12752</pages>
      <abstract>Scientific peer review is essential for the quality of academic publications. However, the increasing number of paper submissions to conferences has strained the reviewing process. This surge poses a burden on area chairs who have to carefully read an ever-growing volume of reviews and discern each reviewer’s main arguments as part of their decision process. In this paper, we introduce , a summarization method designed to offer a concise yet comprehensive overview of scholarly reviews. Unlike traditional consensus-based methods, extracts both common and unique opinions from the reviews. We introduce novel uniqueness scores based on the Rational Speech Act framework to identify relevant sentences in the reviews. Our method aims to provide a pragmatic glimpse into all reviews, offering a balanced perspective on their opinions. Our experimental results with both automatic metrics and human evaluation show that generates more discriminative summaries than baseline methods in terms of human evaluation while achieving comparable performance with these methods in terms of automatic metrics.</abstract>
      <url hash="e99edd1a">2024.acl-long.688</url>
      <bibkey>darrin-etal-2024-glimpse</bibkey>
      <doi>10.18653/v1/2024.acl-long.688</doi>
    </paper>
    <paper id="689">
      <title>Peacock: A Family of <fixed-case>A</fixed-case>rabic Multimodal Large Language Models and Benchmarks</title>
      <author><first>Fakhraddin</first><last>Alwajih</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Gagan</first><last>Bhatia</last></author>
      <author><first>Abdelrahman</first><last>Mohamed</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>12753-12776</pages>
      <abstract>Multimodal large language models (MLLMs) have proven effective in a wide range of tasks that require complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, the success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, even those with large speaker populations, such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed *Peacock*, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce *Henna*, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic culture, setting the first stone for culturally-aware Arabic MLLMs. The GitHub repository for the *Peacock* project is available at [https://github.com/UBC-NLP/peacock](https://github.com/UBC-NLP/peacock).</abstract>
      <url hash="08d5a4f5">2024.acl-long.689</url>
      <bibkey>alwajih-etal-2024-peacock</bibkey>
      <doi>10.18653/v1/2024.acl-long.689</doi>
    </paper>
    <paper id="690">
      <title>Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks</title>
      <author><first>João</first><last>Bordalo</last><affiliation>NOVA School of Science and Technology</affiliation></author>
      <author><first>Vasco</first><last>Ramos</last></author>
      <author><first>Rodrigo</first><last>Valério</last></author>
      <author><first>Diogo</first><last>Glória-Silva</last><affiliation>Universidade NOVA de Lisboa</affiliation></author>
      <author><first>Yonatan</first><last>Bitton</last><affiliation>Google</affiliation></author>
      <author><first>Michal</first><last>Yarom</last><affiliation>Research, Google</affiliation></author>
      <author><first>Idan</first><last>Szpektor</last><affiliation>Google</affiliation></author>
      <author><first>Joao</first><last>Magalhaes</last><affiliation>Universidade Nova de Lisboa</affiliation></author>
      <pages>12777-12797</pages>
      <abstract>Multistep instructions, such as recipes and how-to guides, greatly benefit from visual aids, such as a series of images that accompany the instruction steps. While Large Language Models (LLMs) have become adept at generating coherent textual steps, Large Vision/Language Models (LVLMs) are less capable of generating accompanying image sequences. The most challenging aspect is that each generated image needs to adhere to the relevant textual step instruction, as well as be visually consistent with earlier images in the sequence. To address this problem, we propose an approach for generating consistent image sequences, which integrates a Latent Diffusion Model (LDM) with an LLM to transform the sequence into a caption to maintain the semantic coherence of the sequence. In addition, to maintain the visual coherence of the image sequence, we introduce a copy mechanism to initialise reverse diffusion processes with a latent vector iteration from a previously generated image from a relevant step. Both strategies will condition the reverse diffusion process on the sequence of instruction steps and tie the contents of the current image to previous instruction steps and corresponding images. Experiments show that the proposed approach is preferred by humans in 46.6% of the cases against 26.6% for the second best method. In addition, automatic metrics showed that the proposed method maintains semantic coherence and visual consistency across steps in both domains.</abstract>
      <url hash="e014645d">2024.acl-long.690</url>
      <bibkey>bordalo-etal-2024-generating</bibkey>
      <doi>10.18653/v1/2024.acl-long.690</doi>
    </paper>
    <paper id="691">
      <title>Cheetah: Natural Language Generation for 517 <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Ife</first><last>Adebara</last></author>
      <author><first>AbdelRahim</first><last>Elmadany</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>12798-12823</pages>
      <abstract>Low-resource African languages pose unique challenges for natural language processing (NLP) tasks, including natural language generation (NLG). In this paper, we develop Cheetah, a massively multilingual NLG language model for African languages. Cheetah supports 517 African languages and language varieties, allowing us to address the scarcity of NLG resources and provide a solution to foster linguistic diversity. We demonstrate the effectiveness of Cheetah through comprehensive evaluations across six generation downstream tasks. In five of the six tasks, Cheetah significantly outperforms other models, showcasing its remarkable performance for generating coherent and contextually appropriate text in a wide range of African languages. We additionally conduct a detailed human evaluation to delve deeper into the linguistic capabilities of Cheetah. The findings of this study contribute to advancing NLP research in low-resource settings, enabling greater accessibility and inclusion for African languages in a rapidly expanding digital landscape. We will publicly release our models for research.</abstract>
      <url hash="9a966f33">2024.acl-long.691</url>
      <bibkey>adebara-etal-2024-cheetah</bibkey>
      <doi>10.18653/v1/2024.acl-long.691</doi>
    </paper>
    <paper id="692">
      <title><fixed-case>T</fixed-case>a<fixed-case>PERA</fixed-case>: Enhancing Faithfulness and Interpretability in Long-Form Table <fixed-case>QA</fixed-case> by Content Planning and Execution-based Reasoning</title>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Lyuhao</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>New York University Shanghai</affiliation></author>
      <pages>12824-12840</pages>
      <abstract>Long-form Table Question Answering (LFTQA) requires systems to generate paragraph long and complex answers to questions over tabular data. While Large language models based systems have made significant progress, it often hallucinates, especially when the task involves complex reasoning over tables. To tackle this issue, we propose a new LLM-based framework, TaPERA, for LFTQA tasks. Our framework uses a modular approach that decomposes the whole process into three sub-modules: 1) QA-based Content Planner that iteratively decomposes the input question into sub-questions; 2) Execution-based Table Reasoner that produces executable Python program for each sub-question; and 3) Answer Generator that generates long-form answer grounded on the program output. Human evaluation results on the FeTaQA and QTSumm datasets indicate that our framework significantly improves strong baselines on both accuracy and truthfulness, as our modular framework is better at table reasoning, and the long-form answer is always consistent with the program output. Our modular design further provides transparency as users are able to interact with our framework by manually changing the content plans.</abstract>
      <url hash="121ab2b1">2024.acl-long.692</url>
      <bibkey>zhao-etal-2024-tapera</bibkey>
      <doi>10.18653/v1/2024.acl-long.692</doi>
    </paper>
    <paper id="693">
      <title>FinanceMATH: Knowledge-Intensive Math Reasoning in Finance Domains</title>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Hongjun</first><last>Liu</last></author>
      <author><first>Yitao</first><last>Long</last><affiliation>New York University</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>New York University Shanghai</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>12841-12858</pages>
      <abstract>We introduce FinanceMath, a novel benchmark designed to evaluate LLMs' capabilities in solving knowledge-intensive math reasoning problems. Compared to prior works, this study features three core advancements. First, FinanceMath includes 1,200 problems with a hybrid of textual and tabular content. These problems require college-level knowledge in the finance domain for effective resolution. Second, we provide expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment. We also construct a finance-domain knowledge bank and investigate various knowledge integration strategies. Finally, we evaluate a wide spectrum of 44 LLMs with both Chain-of-Thought and Program-of-Thought prompting methods. Our experimental results reveal that the current best-performing system (i.e., GPT-4o) achieves only 60.9% accuracy using CoT prompting, leaving substantial room for improvement. Moreover, while augmenting LLMs with external knowledge can improve model performance (e.g., from 47.5% to 54.5% for Gemini-1.5-Pro), their accuracy remains significantly lower than the estimated human expert performance of 92%. We believe that FinanceMath can advance future research in the area of domain-specific knowledge retrieval and integration, particularly within the context of solving reasoning-intensive tasks.</abstract>
      <url hash="ba7e748c">2024.acl-long.693</url>
      <bibkey>zhao-etal-2024-knowledgefmath</bibkey>
      <revision id="1" href="2024.acl-long.693v1" hash="e2e21860"/>
      <revision id="2" href="2024.acl-long.693v2" hash="ba7e748c" date="2024-09-17">Revised the dataset name.</revision>
      <doi>10.18653/v1/2024.acl-long.693</doi>
    </paper>
    <paper id="694">
      <title><fixed-case>API</fixed-case>-<fixed-case>BLEND</fixed-case>: A Comprehensive Corpora for Training and Benchmarking <fixed-case>API</fixed-case> <fixed-case>LLM</fixed-case>s</title>
      <author><first>Kinjal</first><last>Basu</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ibrahim</first><last>Abdelaziz</last><affiliation>International Business Machines (IBM)</affiliation></author>
      <author><first>Subhajit</first><last>Chaudhury</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Soham</first><last>Dan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Maxwell</first><last>Crouse</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Asim</first><last>Munawar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Vernon</first><last>Austel</last></author>
      <author><first>Sadhana</first><last>Kumaravel</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Vinod</first><last>Muthusamy</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Pavan</first><last>Kapanipathi</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Luis</first><last>Lastras</last></author>
      <pages>12859-12870</pages>
      <abstract>There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrate the utility of the API-BLEND dataset for both training and benchmarking purposes.</abstract>
      <url hash="8d5afb8a">2024.acl-long.694</url>
      <bibkey>basu-etal-2024-api</bibkey>
      <doi>10.18653/v1/2024.acl-long.694</doi>
    </paper>
    <paper id="695">
      <title><fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>-Flow: Dynamic <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Fusion for Large Language Models in Generative Tasks</title>
      <author><first>Hanqing</first><last>Wang</last></author>
      <author><first>Bowen</first><last>Ping</last></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yun</first><last>Chen</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>12871-12882</pages>
      <abstract>LoRA employs lightweight modules to customize large language models (LLMs) for each downstream task or domain, where different learned additional modules represent diverse skills. Combining existing LoRAs to address new tasks can enhance the reusability of learned LoRAs, particularly beneficial for tasks with limited annotated data. Most prior works on LoRA combination primarily rely on task-level weights for each involved LoRA, making different examples and tokens share the same LoRA weights. However, in generative tasks, different tokens may necessitate diverse skills to manage. Taking the Chinese math task as an example, understanding the problem description may depend more on the Chinese LoRA, while the calculation part may rely more on the math LoRA. To this end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the impact of different LoRAs. The weights at each step are determined by a fusion gate with extremely few parameters, which can be learned with only 200 training examples. Experiments across six generative tasks demonstrate that our method consistently outperforms baselines with task-level fusion weights. This underscores the necessity of introducing dynamic fusion weights for LoRA combination.</abstract>
      <url hash="bbc502b2">2024.acl-long.695</url>
      <bibkey>wang-etal-2024-lora-flow</bibkey>
      <doi>10.18653/v1/2024.acl-long.695</doi>
    </paper>
    <paper id="696">
      <title>Harder Task Needs More Experts: Dynamic Routing in <fixed-case>M</fixed-case>o<fixed-case>E</fixed-case> Models</title>
      <author><first>Quzhe</first><last>Huang</last></author>
      <author><first>Zhenwei</first><last>An</last></author>
      <author><first>Nan</first><last>Zhuang</last></author>
      <author><first>Mingxu</first><last>Tao</last></author>
      <author><first>Chen</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yang</first><last>Jin</last><affiliation>Peking University</affiliation></author>
      <author><first>Kun</first><last>Xu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Kun</first><last>Xu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Liwei</first><last>Chen</last></author>
      <author><first>Songfang</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <pages>12883-12895</pages>
      <abstract>In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike existing MoE approaches that rely on fixed TopK Routing, which activates a predetermined number of experts regardless of the input’s complexity, our method dynamically allocates experts based on the confidence level in expert selection for each input. This allows for more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over Top2 Routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model dispatches more experts to tasks requiring complex reasoning skills, like BBH, confirming its ability to dynamically allocate computational resources in alignment with the input’s complexity.Our findings also highlight a variation in the number of experts needed across different layers of the transformer model, offering insights into the potential for designing heterogeneous MoE frameworks. The code and models are available at https://github.com/ZhenweiAn/Dynamic_MoE.</abstract>
      <url hash="53020846">2024.acl-long.696</url>
      <bibkey>huang-etal-2024-harder</bibkey>
      <doi>10.18653/v1/2024.acl-long.696</doi>
    </paper>
    <paper id="697">
      <title><fixed-case>XLAVS</fixed-case>-<fixed-case>R</fixed-case>: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception</title>
      <author><first>HyoJung</first><last>Han</last><affiliation>Department of Computer Science, University of Maryland, College Park</affiliation></author>
      <author><first>Mohamed</first><last>Anwar</last></author>
      <author><first>Juan</first><last>Pino</last><affiliation>Meta</affiliation></author>
      <author><first>Wei-Ning</first><last>Hsu</last><affiliation>Facebook</affiliation></author>
      <author><first>Marine</first><last>Carpuat</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Bowen</first><last>Shi</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Changhan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <pages>12896-12911</pages>
      <abstract>Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources.To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability with audio-only fine-tuning.</abstract>
      <url hash="bedca060">2024.acl-long.697</url>
      <bibkey>han-etal-2024-xlavs</bibkey>
      <doi>10.18653/v1/2024.acl-long.697</doi>
    </paper>
    <paper id="698">
      <title><fixed-case>SOTOPIA</fixed-case>-π: Interactive Learning of Socially Intelligent Language Agents</title>
      <author><first>Ruiyi</first><last>Wang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Haofei</first><last>Yu</last></author>
      <author><first>Wenxin</first><last>Zhang</last></author>
      <author><first>Zhengyang</first><last>Qi</last></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yonatan</first><last>Bisk</last><affiliation>Meta and Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Hao</first><last>Zhu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>12912-12940</pages>
      <abstract>Humans learn social skills through both imitation and social interaction. This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA-π, that improves the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement based training on filtered social interaction data according to large language model (LLM) rating. We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent) without the loss of more generic abilities, such as the ability to answer knowledge-based questions. We also demonstrate that this training paradigm uncovers some weaknesses in standard evaluation and safety training paradigms that (1) LLM-based evaluation of social intelligence overestimates the abilities of the language agents trained specifically for social interaction, and that (2) despite not training for better safety or question answering (QA) ability, our methods improve the safety of language agents and maintain general QA ability on the MMLU benchmark.</abstract>
      <url hash="3ff82ff8">2024.acl-long.698</url>
      <bibkey>wang-etal-2024-sotopia</bibkey>
      <doi>10.18653/v1/2024.acl-long.698</doi>
    </paper>
    <paper id="699">
      <title><fixed-case>XFT</fixed-case>: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts</title>
      <author><first>Yifeng</first><last>Ding</last><affiliation>Amazon and Department of Computer Science, University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Jiawei</first><last>Liu</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Yuxiang</first><last>Wei</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Lingming</first><last>Zhang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <pages>12941-12955</pages>
      <abstract>We introduce XFT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs). While vanilla sparse upcycling fails to improve instruction tuning, XFT introduces a shared expert mechanism with a novel routing weight normalization strategy into sparse upcycling, which significantly boosts instruction tuning. After fine-tuning the upcycled MoE model, XFT introduces a learnable model merging mechanism to compile the upcycled MoE model back to a dense model, achieving upcycled MoE-level performance with only dense-model compute. By applying XFT to a 1.3B model, we create a new state-of-the-art tiny code LLM with 67.1 and 64.6 pass@1 on HumanEval and HumanEval+ respectively. With the same data and model architecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+, along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and DS-1000, demonstrating its generalizability. XFT is fully orthogonal to existing techniques such as Evol-Instruct and OSS-Instruct, opening a new dimension for improving code instruction tuning. Codes are available at https://github.com/ise-uiuc/xft.</abstract>
      <url hash="18560bfc">2024.acl-long.699</url>
      <bibkey>ding-etal-2024-mathcal</bibkey>
      <doi>10.18653/v1/2024.acl-long.699</doi>
    </paper>
    <paper id="700">
      <title>Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning</title>
      <author><first>Tuc</first><last>Nguyen</last></author>
      <author><first>Thai</first><last>Le</last><affiliation>Indiana University</affiliation></author>
      <pages>12956-12973</pages>
      <abstract>Several parameter-efficient fine-tuning methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of them at once. Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via model weight averaging to optimize performance on novel, unseen domains with excellent computational efficiency. However, the essential generalizability of this emerging weight-space adapter mixing mechanism on <i>unseen, in-domain examples</i> remains unexplored. Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation. We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis on the negative correlation between their fraction of weight sign difference and their mixtures’ generalizability. The code is available at Github.</abstract>
      <url hash="0d316307">2024.acl-long.700</url>
      <bibkey>nguyen-le-2024-generalizability</bibkey>
      <doi>10.18653/v1/2024.acl-long.700</doi>
    </paper>
    <paper id="701">
      <title>Learning to Decode Collaboratively with Multiple Language Models</title>
      <author><first>Zejiang</first><last>Shen</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Hunter</first><last>Lang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Bailin</first><last>Wang</last></author>
      <author><first>Yoon</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>David</first><last>Sontag</last><affiliation>Massachusetts Institute of Technology and Massachusetts Institute of Technology</affiliation></author>
      <pages>12974-12990</pages>
      <abstract>We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level. We model the decision of which LLM generates the next token as a latent variable. By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the “assistant” language models to generate, all without direct supervision. Token-level collaboration during decoding allows for a fusion of each model’s expertise in a manner tailored to the specific task at hand. Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models. On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models. Through qualitative analysis, we show models trained with our method exhibit several interesting collaboration patterns, e.g., template-filling, by visualizing the learned latent decisions.</abstract>
      <url hash="8eede54e">2024.acl-long.701</url>
      <bibkey>shen-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.acl-long.701</doi>
    </paper>
    <paper id="702">
      <title><fixed-case>DRAGIN</fixed-case>: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models</title>
      <author><first>Weihang</first><last>Su</last></author>
      <author><first>Yichen</first><last>Tang</last></author>
      <author><first>Qingyao</first><last>Ai</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhijing</first><last>Wu</last></author>
      <author><first>Yiqun</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>12991-13013</pages>
      <abstract>Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs).There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve).However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM’s most recent sentence or the last few tokens, while the LLM’s information needs may span across the entire context.To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the Information Needs of LLMs. Our framework is specifically designed to make decisions on when and what to retrieve based on the LLM’s information needs during the text generation process.We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method.</abstract>
      <url hash="e25dfa71">2024.acl-long.702</url>
      <bibkey>su-etal-2024-dragin</bibkey>
      <doi>10.18653/v1/2024.acl-long.702</doi>
    </paper>
    <paper id="703">
      <title>Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?</title>
      <author><first>Zhaochen</first><last>Su</last></author>
      <author><first>Juntao</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Jun</first><last>Zhang</last></author>
      <author><first>Tong</first><last>Zhu</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Xiaoye</first><last>Qu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Pan</first><last>Zhou</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Yan</first><last>Bowen</last></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>13014-13033</pages>
      <abstract>Temporal reasoning is fundamental for large language models (LLMs) to comprehend the world. Current temporal reasoning datasets are limited to questions about single or isolated events, falling short in mirroring the realistic temporal characteristics involving concurrent nature and intricate temporal interconnections. In this paper, we introduce CoTempQA, a comprehensive co-temporal Question Answering (QA) benchmark containing four co-temporal scenarios (Equal, Overlap, During, Mix) with 4,748 samples for evaluating the co-temporal comprehension and reasoning abilities of LLMs. Our extensive experiments reveal a significant gap between the performance of current LLMs and human-level reasoning on CoTempQA tasks. Even when enhanced with Chain of Thought (CoT) methodologies, models consistently struggle with our task. In our preliminary exploration, we discovered that mathematical reasoning plays a significant role in handling co-temporal events and proposed a strategy to boost LLMs’ co-temporal reasoning from a mathematical perspective. We hope that our CoTempQA datasets will encourage further advancements in improving the co-temporal reasoning capabilities of LLMs.</abstract>
      <url hash="2d82d9e4">2024.acl-long.703</url>
      <bibkey>su-etal-2024-living</bibkey>
      <doi>10.18653/v1/2024.acl-long.703</doi>
    </paper>
    <paper id="704">
      <title><fixed-case>C</fixed-case>ritique<fixed-case>LLM</fixed-case>: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation</title>
      <author><first>Pei</first><last>Ke</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Bosi</first><last>Wen</last></author>
      <author><first>Andrew</first><last>Feng</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Xuanyu</first><last>Lei</last></author>
      <author><first>Jiale</first><last>Cheng</last></author>
      <author><first>Shengyuan</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Aohan</first><last>Zeng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Hongning</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>13034-13054</pages>
      <abstract>Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4’s direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.</abstract>
      <url hash="f1ad2ba0">2024.acl-long.704</url>
      <bibkey>ke-etal-2024-critiquellm</bibkey>
      <doi>10.18653/v1/2024.acl-long.704</doi>
    </paper>
    <paper id="705">
      <title><fixed-case>LLMA</fixed-case>rena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments</title>
      <author><first>Junzhe</first><last>Chen</last></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Shuodi</first><last>Liu</last></author>
      <author><first>Shiyu</first><last>Huang</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Wei-Wei</first><last>Tu</last><affiliation>4Paradigm Inc.</affiliation></author>
      <author><first>Zhaofeng</first><last>He</last><affiliation>Beijing University of Post and Telecommunication</affiliation></author>
      <author><first>Lijie</first><last>Wen</last><affiliation>School of Software, Tsinghua University</affiliation></author>
      <pages>13055-13077</pages>
      <abstract>Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings.</abstract>
      <url hash="07ec0a4f">2024.acl-long.705</url>
      <bibkey>chen-etal-2024-llmarena</bibkey>
      <doi>10.18653/v1/2024.acl-long.705</doi>
    </paper>
    <paper id="706">
      <title>Small But Funny: A Feedback-Driven Approach to Humor Distillation</title>
      <author><first>Sahithya</first><last>Ravi</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Patrick</first><last>Huber</last><affiliation>Facebook</affiliation></author>
      <author><first>Akshat</first><last>Shrivastava</last></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <author><first>Arash</first><last>Einolghozati</last><affiliation>Facebook</affiliation></author>
      <pages>13078-13090</pages>
      <abstract>The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a “teacher” generating data, as well as a “critic” evaluating the student’s performance. Our experiments on humor generation reveal that the incorporation of feedback significantly narrows the performance gap between SLMs and their larger counterparts compared to merely relying on imitation. As a result, our research highlights the potential of using feedback as an additional dimension to data when transferring complex language abilities via distillation.</abstract>
      <url hash="7195cb7b">2024.acl-long.706</url>
      <bibkey>ravi-etal-2024-small</bibkey>
      <doi>10.18653/v1/2024.acl-long.706</doi>
    </paper>
    <paper id="707">
      <title>Symbol-<fixed-case>LLM</fixed-case>: Towards Foundational Symbol-centric Interface For Large Language Models</title>
      <author><first>Fangzhi</first><last>Xu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Zhiyong</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Qiushi</first><last>Sun</last><affiliation>University of Hong Kong and Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Siyu</first><last>Ren</last></author>
      <author><first>Fei</first><last>Yuan</last></author>
      <author><first>Shuai</first><last>Yuan</last></author>
      <author><first>Qika</first><last>Lin</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Jun</first><last>Liu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>13091-13116</pages>
      <abstract>Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language(e.g., chemical molecular formula). Injecting a collection of symbolic data directly into the training of LLMs can be problematic, as it disregards the synergies among different symbolic families and overlooks the need for a balanced mixture of natural and symbolic data. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we curated a data collection consisting of 34 tasks and incorporating 20 distinct symbolic families, intending to capture the interrelations and foster synergies between symbols. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss of the generality ability. Extensive experiments on both symbol- and NL-centric tasks demonstrate the balanced and superior performances of Symbol-LLM series models.</abstract>
      <url hash="ed2a9839">2024.acl-long.707</url>
      <bibkey>xu-etal-2024-symbol</bibkey>
      <doi>10.18653/v1/2024.acl-long.707</doi>
    </paper>
    <paper id="708">
      <title>From Sights to Insights: Towards Summarization of Multimodal Clinical Documents</title>
      <author><first>Akash</first><last>Ghosh</last><affiliation>Indian Institute of Technology, Patna</affiliation></author>
      <author><first>Mohit</first><last>Tomar</last></author>
      <author><first>Abhisek</first><last>Tiwari</last></author>
      <author><first>Sriparna</first><last>Saha</last><affiliation>Indian Institute of Technology Patna, India</affiliation></author>
      <author><first>Jatin</first><last>Salve</last></author>
      <author><first>Setu</first><last>Sinha</last></author>
      <pages>13117-13129</pages>
      <abstract>The advancement of Artificial Intelligence is pivotal in reshaping healthcare, enhancing diagnostic precision, and facilitating personalized treatment strategies. One major challenge for healthcare professionals is quickly navigating through long clinical documents to provide timely and effective solutions. Doctors often struggle to draw quick conclusions from these extensive documents. To address this issue and save time for healthcare professionals, an effective summarization model is essential. Most current models assume the data is only text-based. However, patients often include images of their medical conditions in clinical documents. To effectively summarize these multimodal documents, we introduce <b>
          <i>EDI-Summ</i></b>, an innovative Image-Guided Encoder-Decoder Model. This model uses modality-aware contextual attention on the encoder and an image cross-attention mechanism on the decoder, enhancing the BART base model to create detailed visual-guided summaries. We have tested our model extensively on three multimodal clinical benchmarks involving multimodal question and dialogue summarization tasks. Our analysis demonstrates that <b>
          <i>EDI-Summ</i></b> outperforms state-of-the-art large language and vision-aware models in these summarization tasks. <b>Disclaimer</b>: The work includes vivid medical illustrations, depicting the essential aspects of the subject matter.</abstract>
      <url hash="638f8c08">2024.acl-long.708</url>
      <bibkey>ghosh-etal-2024-sights</bibkey>
      <doi>10.18653/v1/2024.acl-long.708</doi>
    </paper>
    <paper id="709">
      <title>When Phrases Meet Probabilities: Enabling Open Relation Extraction with Cooperating Large Language Models</title>
      <author><first>Jiaxin</first><last>Wang</last><affiliation>Xi’an jiaotong University</affiliation></author>
      <author><first>Lingling</first><last>Zhang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Wee Sun</first><last>Lee</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yujie</first><last>Zhong</last></author>
      <author><first>Liwei</first><last>Kang</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Jun</first><last>Liu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>13130-13147</pages>
      <abstract>Current clustering-based open relation extraction (OpenRE) methods usually apply clustering algorithms on top of pre-trained language models. However, this practice has three drawbacks. First, embeddings from language models are high-dimensional and anisotropic, so using simple metrics to calculate distances between these embeddings may not accurately reflect the relational similarity. Second, there exists a gap between the pre-trained language models and downstream clustering for their different objective forms. Third, clustering with embeddings deviates from the primary aim of relation extraction, as it does not directly obtain relations. In this work, we propose a new idea for OpenRE in the era of LLMs, that is, extracting relational phrases and directly exploiting the knowledge in LLMs to assess the semantic similarity between phrases without relying on any additional metrics. Based on this idea, we developed a framework, oreLLM, that makes two LLMs work collaboratively to achieve clustering and address the above issues. Experimental results on different datasets show that oreLLM outperforms current baselines by <tex-math>1.4\%\sim 3.13\%</tex-math> in terms of clustering accuracy.</abstract>
      <url hash="dcb49cfc">2024.acl-long.709</url>
      <bibkey>wang-etal-2024-phrases</bibkey>
      <doi>10.18653/v1/2024.acl-long.709</doi>
    </paper>
    <paper id="710">
      <title>Effects of diversity incentives on sample diversity and downstream model performance in <fixed-case>LLM</fixed-case>-based text augmentation</title>
      <author><first>Jan</first><last>Cegin</last><affiliation>Brno University of Technology</affiliation></author>
      <author><first>Branislav</first><last>Pecher</last><affiliation>Kempelen Institute of Intelligent Technologies, Brno University of Technology and Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Jakub</first><last>Simko</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Ivan</first><last>Srba</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Maria</first><last>Bielikova</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Peter</first><last>Brusilovsky</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>13148-13171</pages>
      <abstract>The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune downstream models. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts’ lexical diversity and downstream model performance. We compare the effects over 5 different LLMs, 6 datasets and 2 downstream models. We show that diversity is most increased by taboo words, but downstream model performance is highest with hints.</abstract>
      <url hash="c8913014">2024.acl-long.710</url>
      <bibkey>cegin-etal-2024-effects</bibkey>
      <doi>10.18653/v1/2024.acl-long.710</doi>
    </paper>
    <paper id="711">
      <title>Beyond Orthography: Automatic Recovery of Short Vowels and Dialectal Sounds in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Yassine</first><last>El Kheir</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Ahmed</first><last>Ali</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Shammur</first><last>Chowdhury</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>13172-13184</pages>
      <abstract>This paper presents a novel Dialectal Sound and Vowelization Recovery framework, designed to recognize borrowed and dialectal sounds within phonologically diverse and dialect-rich languages, that extends beyond its standard orthographic sound sets. The proposed framework utilized quantized sequence of input with(out) continuous pretrained self-supervised representation. We show the efficacy of the pipeline using limited data for Arabic, a dialect-rich language containing more than 22 major dialects. Phonetically correct transcribed speech resources for dialectal Arabic is scare. Therefore, we introduce ArabVoice15, a first of its kind, curated test set featuring 5 hours of dialectal speech across 15 Arab countries, with phonetically accurate transcriptions, including borrowed and dialect-specific sounds. We described in detail the annotation guideline along with the analysis of the dialectal confusion pairs. Our extensive evaluation includes both subjective – human perception tests and objective measures. Our empirical results, reported with three test sets, show that with only one and half hours of training data, our model improve character error rate by ≈7% in ArabVoice15 compared to the baseline.</abstract>
      <url hash="d7fbfa6b">2024.acl-long.711</url>
      <bibkey>el-kheir-etal-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.acl-long.711</doi>
    </paper>
    <paper id="712">
      <title>Document-Level Machine Translation with Large-Scale Public Parallel Corpora</title>
      <author><first>Proyag</first><last>Pal</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Kenneth</first><last>Heafield</last><affiliation>Facebook</affiliation></author>
      <pages>13185-13197</pages>
      <abstract>Despite the fact that document-level machine translation has inherent advantages over sentence-level machine translation due to additional information available to a model from document context, most translation systems continue to operate at a sentence level. This is primarily due to the severe lack of publicly available large-scale parallel corpora at the document level. We release a large-scale open parallel corpus with document context extracted from ParaCrawl in five language pairs, along with code to compile document-level datasets for any language pair supported by ParaCrawl. We train context-aware models on these datasets and find improvements in terms of overall translation quality and targeted document-level phenomena. We also analyse how much long-range information is useful to model some of these discourse phenomena and find models are able to utilise context from several preceding sentences.</abstract>
      <url hash="f97d2543">2024.acl-long.712</url>
      <bibkey>pal-etal-2024-document</bibkey>
      <doi>10.18653/v1/2024.acl-long.712</doi>
    </paper>
    <paper id="713">
      <title>Bridging the Empirical-Theoretical Gap in Neural Network Formal Language Learning Using Minimum Description Length</title>
      <author><first>Nur</first><last>Lan</last></author>
      <author><first>Emmanuel</first><last>Chemla</last><affiliation>CNRS</affiliation></author>
      <author><first>Roni</first><last>Katzir</last><affiliation>Tel Aviv University</affiliation></author>
      <pages>13198-13210</pages>
      <abstract>Neural networks offer good approximation to many tasks but consistently fail to reach perfect generalization, even when theoretical work shows that such perfect solutions can be expressed by certain architectures. Using the task of formal language learning, we focus on one simple formal language and show that the theoretically correct solution is in fact not an optimum of commonly used objectives — even with regularization techniques that according to common wisdom should lead to simple weights and good generalization (L1, L2) or other meta-heuristics (early-stopping, dropout). On the other hand, replacing standard targets with the Minimum Description Length objective (MDL) results in the correct solution being an optimum.</abstract>
      <url hash="df76fba5">2024.acl-long.713</url>
      <bibkey>lan-etal-2024-bridging</bibkey>
      <doi>10.18653/v1/2024.acl-long.713</doi>
    </paper>
    <paper id="714">
      <title>Context versus Prior Knowledge in Language Models</title>
      <author><first>Kevin</first><last>Du</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich and Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Vésteinn</first><last>Snæbjarnarson</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Niklas</first><last>Stoehr</last></author>
      <author><first>Jennifer</first><last>White</last></author>
      <author><first>Aaron</first><last>Schein</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>13211-13235</pages>
      <abstract>To answer a question, language models often need to integrate prior knowledge learned during pretraining and new information presented in context. We hypothesize that models perform this integration in a predictable way across different questions and contexts: models will rely more on prior knowledge for questions about entities (e.g., persons, places, etc.) that they are more familiar with due to higher exposure in the training corpus, and be more easily persuaded by some contexts than others. To formalize this problem, we propose two mutual information-based metrics to measure a model’s dependency on a context and on its prior about an entity: first, the persuasion score of a given context represents how much a model depends on the context in its decision, and second, the susceptibility score of a given entity represents how much the model can be swayed away from its original answer distribution about an entity. We empirically test our metrics for their validity and reliability. Finally, we explore and find a relationship between the scores and the model’s expected familiarity with an entity, and provide two use cases to illustrate their benefits.</abstract>
      <url hash="7bda1035">2024.acl-long.714</url>
      <bibkey>du-etal-2024-context</bibkey>
      <doi>10.18653/v1/2024.acl-long.714</doi>
    </paper>
    <paper id="715">
      <title>Word Matters: What Influences Domain Adaptation in Summarization?</title>
      <author><first>Yinghao</first><last>Li</last></author>
      <author><first>Siyu</first><last>Miao</last></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Gao</last></author>
      <pages>13236-13249</pages>
      <abstract>Domain adaptation aims to enable Large Language Models (LLMs) to generalize domain datasets unseen effectively during the training phase. However, factors such as the size of the model parameters and the scale of training data are general influencers and do not reflect the nuances of domain adaptation performance. This paper investigates the fine-grained factors affecting domain adaptation performance, analyzing the specific impact of ‘words’ in training data on summarization tasks. We propose quantifying dataset learning difficulty as the learning difficulty of generative summarization, which is determined by two indicators: word-based compression rate and abstraction level. Our experiments conclude that, when considering dataset learning difficulty, the cross-domain overlap and the performance gain in summarization tasks exhibit an approximate linear relationship, which is not directly related to the number of words. Based on this finding, predicting a model’s performance on unknown domain datasets is possible without undergoing training. Source code and scripts are available at https://github.com/li-aolong/Word-Matters.</abstract>
      <url hash="d7e7fab7">2024.acl-long.715</url>
      <bibkey>li-etal-2024-word</bibkey>
      <doi>10.18653/v1/2024.acl-long.715</doi>
    </paper>
    <paper id="716">
      <title>Visualization Recommendation with Prompt-based Reprogramming of Large Language Models</title>
      <author><first>Xinhang</first><last>Li</last></author>
      <author><first>Jingbo</first><last>Zhou</last><affiliation>Baidu Research</affiliation></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Derong</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Tong</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Enhong</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>13250-13262</pages>
      <abstract>Visualization recommendations, which aim to automatically match proper visual charts for specific data tables, can significantly simplify the data analysis process. Traditional approaches in this domain have primarily relied on rule-based or machine learning-based methodologies. These methods often demand extensive manual maintenance and yet fail to fully comprehend the tabular data, leading to unsatisfactory performance. Recently, Large Language Models (LLMs) have emerged as powerful tools, exhibiting strong reasoning capabilities. This advancement suggests their substantial promise in addressing visualization recommendation challenges. However, effectively harnessing LLMs to discern and rationalize patterns in tabular data, and consequently deduce the essential information for chart generation, remains an unresolved challenge. To this end, we introduce a novel Hierarchical Table Prompt-based reprogramming framework, named HTP. This framework aims to integrate multi-dimensional tabular data into LLMs through a strategically crafted prompt learning method while keeping the LLMs’ backbone and weights unaltered. The HTP framework uniquely incorporates a four-level prompt structure, encompassing general, instance, cluster, and column levels. This multi-level approach is engineered to provide a comprehensive understanding of both general distribution and multifaceted fine-grained features of tabular data, before inputting the tabular data into the frozen LLM. Our empirical studies confirm that the HTP framework achieves state-of-the-art performance, marking an advancement in the field of data visualization and analysis. The code and data will be made publicly available upon acceptance.</abstract>
      <url hash="f9949ca9">2024.acl-long.716</url>
      <bibkey>li-etal-2024-visualization</bibkey>
      <doi>10.18653/v1/2024.acl-long.716</doi>
    </paper>
    <paper id="717">
      <title><fixed-case>HOLMES</fixed-case>: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Pranoy</first><last>Panda</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Ankush</first><last>Agarwal</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Chaitanya</first><last>Devaguptapu</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Manohar</first><last>Kaul</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Prathosh</first><last>Ap</last><affiliation>Indian Institute of Technology Delhi</affiliation></author>
      <pages>13263-13282</pages>
      <abstract>Given unstructured text, Large Language Models (LLMs) are adept at answering simple (single-hop) questions. However, as the complexity of the questions increase, the performance of LLMs degrade. We believe this is due to the overhead associated with understanding the complex question followed by filtering and aggregating unstructured information in the raw text. Recent methods try to reduce this burden by integrating structured knowledge triples into the raw text, aiming to provide a structured overview that simplifies information processing. However, this simplistic approach is query-agnostic and the extracted facts are ambiguous as they lack context. To address these drawbacks and to enable LLMs to answer complex (multi-hop) questions with ease, we propose to use a knowledge graph (KG) that is context-aware and is distilled to contain query-relevant information. The use of our compressed distilled KG as input to the LLM results in our method utilizing up to 67% fewer tokens to represent the query relevant information present in the supporting documents, compared to the state-of-the-art (SoTA) method.Our experiments show consistent improvements over the SoTA across several metrics (EM, F1, BERTScore, and Human Eval) on two popular benchmark datasets (HotpotQA and MuSiQue).</abstract>
      <url hash="609e08b7">2024.acl-long.717</url>
      <bibkey>panda-etal-2024-holmes</bibkey>
      <doi>10.18653/v1/2024.acl-long.717</doi>
    </paper>
    <paper id="718">
      <title>Toward In-Context Teaching: Adapting Examples to Students’ Misconceptions</title>
      <author><first>Alexis</first><last>Ross</last><affiliation>Massachusetts Institute of Technology and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology and Microsoft</affiliation></author>
      <pages>13283-13310</pages>
      <abstract>When a teacher provides examples for a student to study, these examples must be informative, enabling a student to progress from their current state toward a target concept or skill. Good teachers must therefore simultaneously infer what students already know and adapt their teaching to students’ changing state of knowledge. There is increasing interest in using computational models, particularly large language models, as pedagogical tools. As students, language models in particular have shown a remarkable ability to adapt to new tasks given small numbers of examples. But how effectively can these models adapt as teachers to students of different types? To study this question, we introduce a suite of models and evaluation methods we call AdapT. AdapT has two components: (1) a collection of simulated Bayesian student models that can be used for evaluation of automated teaching methods; (2) a platform for evaluation with human students, to characterize the real-world effectiveness of these methods. We additionally introduce (3) AToM, a new probabilistic method for adaptive teaching that jointly infers students’ past beliefs and optimizes for the correctness of future beliefs. In evaluations of simulated students across three learning domains (fraction arithmetic, English morphology, function learning), AToM systematically outperforms LLM-based and standard Bayesian teaching methods. In human experiments, both AToM and LLMs outperform non-adaptive random example selection. Our results highlight both the difficulty of the adaptive teaching task and the potential of learned adaptive methods for solving it.</abstract>
      <url hash="8af9dba6">2024.acl-long.718</url>
      <bibkey>ross-andreas-2024-toward</bibkey>
      <doi>10.18653/v1/2024.acl-long.718</doi>
    </paper>
    <paper id="719">
      <title>Bridging Word-Pair and Token-Level Metaphor Detection with Explainable Domain Mining</title>
      <author><first>Yuan</first><last>Tian</last></author>
      <author><first>Ruike</first><last>Zhang</last></author>
      <author><first>Nan</first><last>Xu</last></author>
      <author><first>Wenji</first><last>Mao</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>13311-13325</pages>
      <abstract>Metaphor detection aims to identify whether a linguistic expression in text is metaphorical or literal. Most existing research tackles this problem either using word-pair or token-level information as input, and thus treats word-pair and token-level metaphor detection as distinct subtasks. Benefited from the simplified structure of word pairs, recent methods for word-pair metaphor detection can provide intermediate explainable clues for the detection results, which remains a challenging issue for token-level metaphor detection. To mitigate this issue in token-level metaphor detection and take advantage of word pairs, in this paper, we make the first attempt to bridge word-pair and token-level metaphor detection via modeling word pairs within a sentence as explainable intermediate information. As the central role of verb in metaphorical expressions, we focus on token-level verb metaphor detection and propose a novel explainable Word Pair based Domain Mining (WPDM) method. Our work is inspired by conceptual metaphor theory (CMT). We first devise an approach for conceptual domain mining utilizing semantic role mapping and resources at cognitive, commonsense and lexical levels. We then leverage the inconsistency between source and target domains for core word pair modeling to facilitate the explainability. Experiments on four datasets verify the effectiveness of our method and demonstrate its capability to provide the core word pair and corresponding conceptual domains as explainable clues for metaphor detection.</abstract>
      <url hash="21874caa">2024.acl-long.719</url>
      <bibkey>tian-etal-2024-bridging</bibkey>
      <doi>10.18653/v1/2024.acl-long.719</doi>
    </paper>
    <paper id="720">
      <title>Faithful Logical Reasoning via Symbolic Chain-of-Thought</title>
      <author><first>Jundong</first><last>Xu</last></author>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Qian</first><last>Liu</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Mong-Li</first><last>Lee</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Wynne</first><last>Hsu</last><affiliation>National University of Singapore</affiliation></author>
      <pages>13326-13365</pages>
      <abstract>While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first attempt at combining symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at https://github.com/Aiden0526/SymbCoT.</abstract>
      <url hash="13b7c099">2024.acl-long.720</url>
      <bibkey>xu-etal-2024-faithful</bibkey>
      <doi>10.18653/v1/2024.acl-long.720</doi>
    </paper>
    <paper id="721">
      <title><fixed-case>S</fixed-case><tex-math>^2</tex-math><fixed-case>GSL</fixed-case>: Incorporating Segment to Syntactic Enhanced Graph Structure Learning for Aspect-based Sentiment Analysis</title>
      <author><first>Bingfeng</first><last>Chen</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Qihan</first><last>Ouyang</last></author>
      <author><first>Yongqi</first><last>Luo</last></author>
      <author><first>Boyan</first><last>Xu</last></author>
      <author><first>Ruichu</first><last>Cai</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Zhifeng</first><last>Hao</last><affiliation>Shantou University</affiliation></author>
      <pages>13366-13379</pages>
      <abstract>Previous graph-based approaches in Aspect-based Sentiment Analysis(ABSA) have demonstrated impressive performance by utilizing graph neural networks and attention mechanisms to learn structures of static dependency trees and dynamic latent trees. However, incorporating both semantic and syntactic information simultaneously within complex global structures can introduce irrelevant contexts and syntactic dependencies during the process of graph structure learning, potentially resulting in inaccurate predictions. In order to address the issues above, we propose S<tex-math>^2</tex-math>GSL, incorporating Segment to Syntactic enhanced Graph Structure Learning for ABSA. Specifically, S<tex-math>^2</tex-math>GSL is featured with a segment-aware semantic graph learning and a syntax-based latent graph learning enabling the removal of irrelevant contexts and dependencies, respectively. We further propose a self-adaptive aggregation network that facilitates the fusion of two graph learning branches, thereby achieving complementarity across diverse structures. Experimental results on four benchmarks demonstrate the effectiveness of our framework.</abstract>
      <url hash="7ea2811f">2024.acl-long.721</url>
      <bibkey>chen-etal-2024-s2gsl</bibkey>
      <doi>10.18653/v1/2024.acl-long.721</doi>
    </paper>
    <paper id="722">
      <title>Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends</title>
      <author><first>Giuliano</first><last>Martinelli</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Edoardo</first><last>Barba</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>13380-13394</pages>
      <abstract>Large autoregressive generative models have emerged as the cornerstone for achieving the highest performance across several Natural Language Processing tasks. However, the urge to attain superior results has, at times, led to the premature replacement of carefully designed task-specific approaches without exhaustive experimentation. The Coreference Resolution task is no exception; all recent state-of-the-art solutions adopt large generative autoregressive models that outperform encoder-based discriminative systems. In this work, we challenge this recent trend by introducing Maverick, a carefully designed – yet simple – pipeline, which enables running a state-of-the-art Coreference Resolution system within the constraints of an academic budget, outperforming models with up to 13 billion parameters with as few as 500 million parameters. Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark, training with up to 0.006x the memory resources and obtaining a 170x faster inference compared to previous state-of-the-art systems. We extensively validate the robustness of the Maverick framework with an array of diverse experiments, reporting improvements over prior systems in data-scarce, long-document, and out-of-domain settings. We release our code and models for research purposes at https://github.com/SapienzaNLP/maverick-coref.</abstract>
      <url hash="7993ce4f">2024.acl-long.722</url>
      <bibkey>martinelli-etal-2024-maverick</bibkey>
      <doi>10.18653/v1/2024.acl-long.722</doi>
    </paper>
    <paper id="723">
      <title><fixed-case>ESC</fixed-case>o<fixed-case>T</fixed-case>: Towards Interpretable Emotional Support Dialogue Systems</title>
      <author><first>Tenggan</first><last>Zhang</last></author>
      <author><first>Xinjie</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jinming</first><last>Zhao</last></author>
      <author><first>Li</first><last>Zhou</last></author>
      <author><first>Qin</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <pages>13395-13412</pages>
      <abstract>Understanding the reason for emotional support response is crucial for establishing connections between users and emotional support dialogue systems. Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. To empower the system with better interpretability, we propose an emotional support response generation scheme, named <tex-math>\textbf{E}</tex-math>motion-Focused and <tex-math>\textbf{S}</tex-math>trategy-Driven <tex-math>\textbf{C}</tex-math>hain-<tex-math>\textbf{o}</tex-math>f-<tex-math>\textbf{T}</tex-math>hought (<tex-math>\textbf{ESCoT}</tex-math>), mimicking the process of <tex-math>\textit{identifying}</tex-math>, <tex-math>\textit{understanding}</tex-math>, and <tex-math>\textit{regulating}</tex-math> emotions. Specially, we construct a new dataset with ESCoT in two steps: (1) <tex-math>\textit{Dialogue Generation}</tex-math> where we first generate diverse conversation situations, then enhance dialogue generation using richer emotional support strategies based on these situations; (2) <tex-math>\textit{Chain Supplement}</tex-math> where we focus on supplementing selected dialogues with elements such as emotion, stimuli, appraisal, and strategy reason, forming the manually verified chains. Additionally, we further develop a model to generate dialogue responses with better interpretability. We also conduct extensive experiments and human evaluations to validate the effectiveness of the proposed ESCoT and generated dialogue responses. Our dataset, code, and model will be released.</abstract>
      <url hash="c82f8b6c">2024.acl-long.723</url>
      <bibkey>zhang-etal-2024-escot</bibkey>
      <doi>10.18653/v1/2024.acl-long.723</doi>
    </paper>
    <paper id="724">
      <title><fixed-case>P</fixed-case>ath<fixed-case>R</fixed-case>easoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering</title>
      <author><first>Fangzhi</first><last>Xu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Qika</first><last>Lin</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Tianzhe</first><last>Zhao</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>JiaweiHan</first><last>JiaweiHan</last></author>
      <author><first>Jun</first><last>Liu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>13413-13429</pages>
      <abstract>Logical reasoning task has attracted great interest since it was proposed. Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMs struggle in logical consistency modeling and logical structure perception. To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture PathReasoner. It addresses the task from the views of both data and model. To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths. From the model perspective, we design a stack of transformer-style blocks. In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy. Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities.</abstract>
      <url hash="546dd17e">2024.acl-long.724</url>
      <bibkey>xu-etal-2024-pathreasoner</bibkey>
      <doi>10.18653/v1/2024.acl-long.724</doi>
    </paper>
    <paper id="725">
      <title><fixed-case>WARDEN</fixed-case>: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection</title>
      <author><first>Anudeex</first><last>Shetty</last></author>
      <author><first>Yue</first><last>Teng</last></author>
      <author><first>Ke</first><last>He</last></author>
      <author><first>Qiongkai</first><last>Xu</last><affiliation>Macquarie University</affiliation></author>
      <pages>13430-13444</pages>
      <abstract>Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the stealthiness of watermarks and has been empirically shown to be effective against CSE attack.</abstract>
      <url hash="f53725dc">2024.acl-long.725</url>
      <bibkey>shetty-etal-2024-warden</bibkey>
      <doi>10.18653/v1/2024.acl-long.725</doi>
    </paper>
    <paper id="726">
      <title>Advancing Parameter Efficiency in Fine-tuning via Representation Editing</title>
      <author><first>Muling</first><last>Wu</last></author>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Xiaohua</first><last>Wang</last></author>
      <author><first>Tianlong</first><last>Li</last></author>
      <author><first>Changze</first><last>Lv</last></author>
      <author><first>Zixuan</first><last>Ling</last></author>
      <author><first>Zhu</first><last>JianHao</last></author>
      <author><first>Cenyuan</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xiaoqing</first><last>Zheng</last></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>13445-13464</pages>
      <abstract>Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of 25,700 compared to full parameter fine-tuning, and by a factor of 32 compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, including RoBERTa, GPT-2, T5, and Llama-2, and the results demonstrate the efficiency and efficacy of RED, positioning it as a promising PEFT approach for large neural models.</abstract>
      <url hash="ffaa3b12">2024.acl-long.726</url>
      <bibkey>wu-etal-2024-advancing</bibkey>
      <doi>10.18653/v1/2024.acl-long.726</doi>
    </paper>
    <paper id="727">
      <title>Context Consistency between Training and Inference in Simultaneous Machine Translation</title>
      <author><first>Meizhi</first><last>Zhong</last></author>
      <author><first>Lemao</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Mingming</first><last>Yang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>13465-13476</pages>
      <abstract>Simultaneous Machine Translation (SiMT) aims to yield a real-time partial translation with a monotonically growing source-side context.However, there is a counterintuitive phenomenon about the context usage between training and inference: *e.g.*, in wait-<tex-math>k</tex-math> inference, model consistently trained with wait-<tex-math>k</tex-math> is much worse than that model inconsistently trained with wait-<tex-math>k'</tex-math> (<tex-math>k'\neq k</tex-math>) in terms of translation quality. To this end, we first investigate the underlying reasons behind this phenomenon and uncover the following two factors: 1) the limited correlation between translation quality and training loss; 2) exposure bias between training and inference. Based on both reasons, we then propose an effective training approach called context consistency training accordingly, which encourages consistent context usage between training and inference by optimizing translation quality and latency as bi-objectives and exposing the predictions to the model during the training. The experiments on three language pairs demonstrate that our SiMT system encouraging context consistency outperforms existing SiMT systems with context inconsistency for the first time.</abstract>
      <url hash="3a7fd319">2024.acl-long.727</url>
      <bibkey>zhong-etal-2024-context</bibkey>
      <doi>10.18653/v1/2024.acl-long.727</doi>
    </paper>
    <paper id="728">
      <title>Using Natural Language Explanations to Improve Robustness of In-context Learning</title>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Yuxiang</first><last>Wu</last></author>
      <author><first>Oana-Maria</first><last>Camburu</last><affiliation>Department of Computer Science, University College London, University of London</affiliation></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Pontus</first><last>Stenetorp</last><affiliation>University College London</affiliation></author>
      <pages>13477-13499</pages>
      <abstract>Recent studies demonstrated that large language models (LLMs) can excel in many tasks via in-context learning (ICL). However, recentworks show that ICL-prompted models tend to produce inaccurate results when presented with adversarial inputs. In this work, we investigate whether augmenting ICL with natural language explanations (NLEs) improves the robustness of LLMs on adversarial datasets covering natural language inference and paraphrasing identification. We prompt LLMs with a small set of human-generated NLEs to produce further NLEs, yielding more accurate results than both a zero-shot-ICL setting and using only human-generated NLEs. Our results on five popular LLMs (GPT3.5-turbo, Llama2, Vicuna, Zephyr, and Mistral) show that our approach yields over 6% improvement over baseline approaches for eight adversarial datasets: HANS, ISCS, NaN, ST, PICD, PISP, ANLI, and PAWS. Furthermore, previous studies have demonstrated that prompt selection strategies significantly enhance ICL on in-distribution test sets. However, our findings reveal that these strategies do not match the efficacy of our approach for robustness evaluations, resulting in an accuracy drop of 8% compared to the proposed approach.</abstract>
      <url hash="f5727aeb">2024.acl-long.728</url>
      <bibkey>he-etal-2024-using</bibkey>
      <doi>10.18653/v1/2024.acl-long.728</doi>
    </paper>
    <paper id="729">
      <title>Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers</title>
      <author><first>Jiawen</first><last>Xie</last></author>
      <author><first>Pengyu</first><last>Cheng</last><affiliation>Tencent</affiliation></author>
      <author><first>Xiao</first><last>Liang</last></author>
      <author><first>Yong</first><last>Dai</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Nan</first><last>Du</last></author>
      <pages>13500-13519</pages>
      <abstract>Although dominant in natural language processing, transformer-based models still struggle with long-sequence processing, due to the computational costs of their self-attention operations, which increase exponentially as the length of the input sequence grows. To address this challenge, we propose a **Sim**ple framework to enhance the long-content processing of off-the-shelf pre-trained transformers via three steps: **C**hunk, **A**lign, and **S**elect (SimCAS). More specifically, we first divide each long-sequence input into a batch of chunks, then align the inter-chunk information during the encoding steps, and finally, select the most representative hidden states from the encoder for the decoding process. With our SimCAS, the computation and memory costs can be reduced to linear complexity. In experiments, we demonstrate the effectiveness of the proposed method on various real-world long-text summarization and reading comprehension tasks, in which SimCAS significantly outperforms prior long-sequence processing baselines. The code is at [https://github.com/xjw-nlp/SimCAS](https://github.com/xjw-nlp/SimCAS).</abstract>
      <url hash="83c67b46">2024.acl-long.729</url>
      <bibkey>xie-etal-2024-chunk</bibkey>
      <doi>10.18653/v1/2024.acl-long.729</doi>
    </paper>
    <paper id="730">
      <title><fixed-case>A</fixed-case>rch<fixed-case>C</fixed-case>ode: Incorporating Software Requirements in Code Generation with Large Language Models</title>
      <author><first>Hojae</first><last>Han</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jaejin</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jaeseok</first><last>Yoo</last><affiliation>Seoul National University and Samsung</affiliation></author>
      <author><first>Youngwon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>13520-13552</pages>
      <abstract>This paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (i.e. achieving expected behavior for inputs) and non-functional (e.g., time/space performance, robustness, maintainability) requirements. However, textual descriptions can either express requirements verbosely or may even omit some of them. We introduce ARCHCODE, a novel framework that leverages in-context learning to organize requirements observed in descriptions and to extrapolate unexpressed requirements from them. ARCHCODE generates requirements from given descriptions, conditioning them to produce code snippets and test cases. Each test case is tailored to one of the requirements, allowing for the ranking of code snippets based on the compliance of their execution results with the requirements. Public benchmarks show that ARCHCODE enhances to satisfy functional requirements, significantly improving Pass@k scores.Furthermore, we introduce HumanEval-NFR, the first evaluation of LLMs’ non-functional requirements in code generation, demonstrating ARCHCODE’s superiority over baseline methods. The implementation of ARCHCODE and the HumanEval-NFR benchmark are both publicly accessible.</abstract>
      <url hash="59777601">2024.acl-long.730</url>
      <bibkey>han-etal-2024-archcode</bibkey>
      <doi>10.18653/v1/2024.acl-long.730</doi>
    </paper>
    <paper id="731">
      <title>Combining Supervised Learning and Reinforcement Learning for Multi-Label Classification Tasks with Partial Labels</title>
      <author><first>Zixia</first><last>Jia</last></author>
      <author><first>Junpeng</first><last>Li</last></author>
      <author><first>Shichuan</first><last>Zhang</last></author>
      <author><first>Anji</first><last>Liu</last></author>
      <author><first>Zilong</first><last>Zheng</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <pages>13553-13569</pages>
      <abstract>Traditional supervised learning heavily relies on human-annotated datasets, especially in data-hungry neural approaches. However, various tasks, especially multi-label tasks like document-level relation extraction, pose challenges in fully manual annotation due to the specific domain knowledge and large class sets. Therefore, we address the multi-label positive-unlabelled learning (MLPUL) problem, where only a subset of positive classes is annotated. We propose Mixture Learner for Partially Annotated Classification (MLPAC), an RL-based framework combining the exploration ability of reinforcement learning and the exploitation ability of supervised learning. Experimental results across various tasks, including document-level relation extraction, multi-label image classification, and binary PU learning, demonstrate the generalization and effectiveness of our framework.</abstract>
      <url hash="5d1ef4df">2024.acl-long.731</url>
      <bibkey>jia-etal-2024-combining</bibkey>
      <revision id="1" href="2024.acl-long.731v1" hash="db8c6db1"/>
      <revision id="2" href="2024.acl-long.731v2" hash="5d1ef4df" date="2024-08-19">This reversion corrects typos in author affiliation.</revision>
      <doi>10.18653/v1/2024.acl-long.731</doi>
    </paper>
    <paper id="732">
      <title><fixed-case>MULFE</fixed-case>: A Multi-Level Benchmark for Free Text Model Editing</title>
      <author><first>Chenhao</first><last>Wang</last></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Daojian</first><last>Zeng</last><affiliation>Hunan Normal University</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>13570-13587</pages>
      <abstract>Adjusting the outdated behaviors of large langugae models (LLMs) after deployment remains a significant challenge. It motivates the model editing research, which is however mainly explored in a restricted task form with triple-based edit requests. Recent works have initiated a transition to a more practical and unified editing task that takes free-form text as edit requests. However, there are gaps in nuanced benchmark designs and re-evaluation of existing methods. To bridge the gaps, we introduce a multi-level benchmark for free text model editing (MULFE). The benchmark categorizes probe queries into three levels of generalization, ranging from basic literal memory to deeper understanding and reasoning. Based on the benchmark, we conduct extensive experiments across various base models, edit sizes, and editing methods, including adaptations of mainstream locate-and-edit and hypernetwork methods. The results highlight the inconsistent behaviors of edited models on different generalization levels. Higher-level generalization remains a significant challenge. Based on the findings, we propose SIDE, a simple yet effective method based on in-context distillation to enhance the generalization performance. The benchmark dataset and evaluation scripts are publicly available at http://github.com/wchrepo/mulfe.</abstract>
      <url hash="69d5cfeb">2024.acl-long.732</url>
      <bibkey>wang-etal-2024-mulfe</bibkey>
      <doi>10.18653/v1/2024.acl-long.732</doi>
    </paper>
    <paper id="733">
      <title><fixed-case>M</fixed-case>obile<fixed-case>S</fixed-case>peech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech</title>
      <author><first>Shengpeng</first><last>Ji</last></author>
      <author><first>Ziyue</first><last>Jiang</last></author>
      <author><first>Hanting</first><last>Wang</last></author>
      <author><first>Jialong</first><last>Zuo</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>13588-13600</pages>
      <abstract>Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts. However, all previous work has been developed for cloud-based systems. Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness. Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at https://mobilespeech.github.io/</abstract>
      <url hash="0875acc7">2024.acl-long.733</url>
      <bibkey>ji-etal-2024-mobilespeech</bibkey>
      <doi>10.18653/v1/2024.acl-long.733</doi>
    </paper>
    <paper id="734">
      <title>Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation</title>
      <author><first>Muraleekrishna</first><last>Gopinathan</last></author>
      <author><first>Martin</first><last>Masek</last><affiliation>Edith Cowan University</affiliation></author>
      <author><first>Jumana</first><last>Abu-Khalaf</last><affiliation>Edith Cowan University and German Jordanian University</affiliation></author>
      <author><first>David</first><last>Suter</last><affiliation>Edith Cowan University</affiliation></author>
      <pages>13601-13614</pages>
      <abstract>Embodied AI aims to develop robots that can <i>understand</i> and execute human language instructions, as well as communicate in natural languages. On this front, we study the task of generating highly detailed navigational instructions for the embodied robots to follow. Although recent studies have demonstrated significant leaps in the generation of step-by-step instructions from sequences of images, the generated instructions lack variety in terms of their referral to objects and landmarks. Existing speaker models learn strategies to evade the evaluation metrics and obtain higher scores even for low-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker), an instruction generator or <i>Speaker</i> model that utilises both structural and semantic knowledge of the environment to produce richer instructions. For training, we employ a reward learning method in an adversarial setting to avoid systematic bias introduced by language evaluation metrics. Empirically, our method outperforms existing instruction generation models, evaluated using standard metrics. Our code is available at https://github.com/gmuraleekrishna/SAS.</abstract>
      <url hash="d3c019ba">2024.acl-long.734</url>
      <bibkey>gopinathan-etal-2024-spatially</bibkey>
      <doi>10.18653/v1/2024.acl-long.734</doi>
    </paper>
    <paper id="735">
      <title><fixed-case>H</fixed-case>i<fixed-case>R</fixed-case>o<fixed-case>PE</fixed-case>: Length Extrapolation for Code Models Using Hierarchical Position</title>
      <author><first>Kechi</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Ge</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Huangzhao</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University and Peking University</affiliation></author>
      <pages>13615-13627</pages>
      <abstract>Addressing the limitation of context length in large language models for code-related tasks is the primary focus of this paper. Existing LLMs are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences. Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code. HiRoPE offers easy integration into existing LLMs without extra training costs. Our method is extensively evaluated with various LLMs, demonstrating stable performance in tasks such as language modeling and long code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. Theoretically and experimentally, we find that HiRoPE also addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length.</abstract>
      <url hash="789a02b1">2024.acl-long.735</url>
      <bibkey>zhang-etal-2024-hirope</bibkey>
      <doi>10.18653/v1/2024.acl-long.735</doi>
    </paper>
    <paper id="736">
      <title>Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training</title>
      <author><first>Junqing</first><last>He</last><affiliation>International Digital Econemy Academy</affiliation></author>
      <author><first>Kunhao</first><last>Pan</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Xiaoqun</first><last>Dong</last></author>
      <author><first>Zhuoyang</first><last>Song</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>LiuYiBo</first><last>LiuYiBo</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Qianguosun</first><last>Qianguosun</last></author>
      <author><first>Yuxin</first><last>Liang</last></author>
      <author><first>Hao</first><last>Wang</last><affiliation>IDEA</affiliation></author>
      <author><first>Enming</first><last>Zhang</last></author>
      <author><first>Jiaxing</first><last>Zhang</last><affiliation>IDEA</affiliation></author>
      <pages>13628-13642</pages>
      <abstract>While large language models (LLMs) are equipped with longer text input capabilities than before, they are struggling to seek correct information in long contexts. The “lost in the middle” problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called Position-Agnostic Multi-step QA (PAM QA). Trained in this task, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7% absolute gain in shuffled settings, by 21.5% in passage retrieval task. We release our model and code to promote related research in the community.</abstract>
      <url hash="02cdd699">2024.acl-long.736</url>
      <bibkey>he-etal-2024-never</bibkey>
      <doi>10.18653/v1/2024.acl-long.736</doi>
    </paper>
    <paper id="737">
      <title><fixed-case>C</fixed-case>ode<fixed-case>A</fixed-case>gent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges</title>
      <author><first>Kechi</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Jia</first><last>Li</last></author>
      <author><first>Ge</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Xianjie</first><last>Shi</last></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University and Peking University</affiliation></author>
      <pages>13643-13658</pages>
      <abstract>Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. However, real-world software development often involves complex code repositories with complex dependencies and extensive documentation. To enable LLMs to handle these realworld repo-level code generation, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code implementation, and code testing. We implement four agent strategies to optimize these tools’ usage. To the best of our knowledge, CodeAgent is the first agent tool framework specifically for repo-level code generation. In order to measure the effectiveness of our method at the repository level, we have introduced a benchmark dataset CodAgentBench. The performance on this dataset shows a significant improvement brought by our method, with improvements of pass rate ranging from 2.0 to 15.8. Further tests on the HumanEval benchmark confirm CodeAgent’s adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent’s robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.</abstract>
      <url hash="6e7f6eea">2024.acl-long.737</url>
      <bibkey>zhang-etal-2024-codeagent</bibkey>
      <doi>10.18653/v1/2024.acl-long.737</doi>
    </paper>
    <paper id="738">
      <title>When is Tree Search Useful for <fixed-case>LLM</fixed-case> Planning? It Depends on the Discriminator</title>
      <author><first>Ziru</first><last>Chen</last></author>
      <author><first>Michael</first><last>White</last><affiliation>Ohio State University</affiliation></author>
      <author><first>Ray</first><last>Mooney</last><affiliation>, University of Texas, Austin</affiliation></author>
      <author><first>Ali</first><last>Payani</last><affiliation>Cisco</affiliation></author>
      <author><first>Yu</first><last>Su</last><affiliation>Ohio State University and Microsoft</affiliation></author>
      <author><first>Huan</first><last>Sun</last><affiliation>The Ohio State University, Columbus</affiliation></author>
      <pages>13659-13678</pages>
      <abstract>In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs’ discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10–20 times slower but leads to negligible performance gains, which hinders its real-world applications.</abstract>
      <url hash="8a52202d">2024.acl-long.738</url>
      <bibkey>chen-etal-2024-tree</bibkey>
      <doi>10.18653/v1/2024.acl-long.738</doi>
    </paper>
    <paper id="739">
      <title><fixed-case>L</fixed-case>ogic<fixed-case>B</fixed-case>ench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models</title>
      <author><first>Mihir</first><last>Parmar</last></author>
      <author><first>Nisarg</first><last>Patel</last></author>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Mutsumi</first><last>Nakamura</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Man</first><last>Luo</last><affiliation>Intel</affiliation></author>
      <author><first>Santosh</first><last>Mashetty</last></author>
      <author><first>Arindam</first><last>Mitra</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <pages>13679-13707</pages>
      <abstract>Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really “reason” over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to ‘logical reasoning’ has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes tend to prioritize parametric knowledge over contextual information and overlook the correct reasoning chain. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs.</abstract>
      <url hash="9591e7b7">2024.acl-long.739</url>
      <bibkey>parmar-etal-2024-logicbench</bibkey>
      <doi>10.18653/v1/2024.acl-long.739</doi>
    </paper>
    <paper id="740">
      <title>Meta-Tuning <fixed-case>LLM</fixed-case>s to Leverage Lexical Knowledge for Generalizable Language Style Understanding</title>
      <author><first>Ruohao</first><last>Guo</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Alan</first><last>Ritter</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>13708-13731</pages>
      <abstract>Language style is often used by writers to convey their intentions, identities, and mastery of language. In this paper, we show that current large language models struggle to capture some language styles without fine-tuning. To address this challenge, we investigate whether LLMs can be meta-trained based on representative lexicons to recognize new styles they have not been fine-tuned on. Experiments on 13 established style classification tasks, as well as 63 novel tasks generated using LLMs, demonstrate that meta-training with style lexicons consistently improves zero-shot transfer across styles. We release the code and data at https://github.com/octaviaguo/Style-LLM.</abstract>
      <url hash="d5978e4c">2024.acl-long.740</url>
      <bibkey>guo-etal-2024-meta</bibkey>
      <doi>10.18653/v1/2024.acl-long.740</doi>
    </paper>
    <paper id="741">
      <title>Reducing Privacy Risks in Online Self-Disclosures with Language Models</title>
      <author><first>Yao</first><last>Dou</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Isadora</first><last>Krsek</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Tarek</first><last>Naous</last></author>
      <author><first>Anubha</first><last>Kabra</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Sauvik</first><last>Das</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Alan</first><last>Ritter</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>13732-13754</pages>
      <abstract>Self-disclosure, while being common and rewarding in social media interaction, also poses privacy risks. In this paper, we take the initiative to protect the user-side privacy associated with online self-disclosure through detection and abstraction. We develop a taxonomy of 19 self-disclosure categories and curate a large corpus consisting of 4.8K annotated disclosure spans. We then fine-tune a language model for detection, achieving over 65% partial span F<tex-math>_1</tex-math>. We further conduct an HCI user study, with 82% of participants viewing the model positively, highlighting its real-world applicability. Motivated by the user feedback, we introduce the task of self-disclosure abstraction, which is rephrasing disclosures into less specific terms while preserving their utility, e.g., “Im 16F” to “I’m a teenage girl”. We explore various fine-tuning strategies, and our best model can generate diverse abstractions that moderately reduce privacy risks while maintaining high utility according to human evaluation. To help users in deciding which disclosures to abstract, we present a task of rating their importance for context understanding. Our fine-tuned model achieves 80% accuracy, on-par with GPT-3.5. Given safety and privacy considerations, we will only release our corpus and models to researcher who agree to the ethical guidelines outlined in Ethics Statement.</abstract>
      <url hash="fda4d03a">2024.acl-long.741</url>
      <bibkey>dou-etal-2024-reducing</bibkey>
      <doi>10.18653/v1/2024.acl-long.741</doi>
    </paper>
    <paper id="742">
      <title>Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models</title>
      <author><first>Zihao</first><last>Lin</last></author>
      <author><first>Mohammad</first><last>Beigi</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Hongxuan</first><last>Li</last></author>
      <author><first>Yufan</first><last>Zhou</last><affiliation>Adobe</affiliation></author>
      <author><first>Yuxiang</first><last>Zhang</last></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Wenpeng</first><last>Yin</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>Virginia Tech</affiliation></author>
      <pages>13755-13772</pages>
      <abstract>Memory Editing (ME) has emerged as an efficient method to modify erroneous facts or inject new facts into Large Language Models (LLMs). Two mainstream ME methods exist: parameter-modifying ME and parameter-preserving ME (integrating extra modules while preserving original parameters). Regrettably, previous studies on ME evaluation have two critical limitations: (i) evaluating LLMs with single edit only, neglecting the need for continuous editing, and (ii) evaluations focusing solely on basic factual triples, overlooking broader LLM capabilities like logical reasoning and reading understanding. This study addresses these limitations with contributions threefold: (i) We explore how ME affects a wide range of fundamental capabilities of LLMs under sequential editing. Experimental results reveal an intriguing phenomenon: Most parameter-modifying ME consistently degrade performance across all tasks after a few sequential edits. In contrast, parameter-preserving ME effectively maintains LLMs’ fundamental capabilities but struggles to accurately recall edited knowledge presented in a different format. (ii) We extend our evaluation to different editing settings, such as layers to edit, model size, instruction tuning, etc. Experimental findings indicate several strategies that can potentially mitigate the adverse effects of ME. (iii) We further explain why parameter-modifying damages LLMs from three dimensions: parameter changes after editing, language modeling capability, and the in-context learning capability. Our in-depth study advocates more careful use of ME in real-world scenarios.</abstract>
      <url hash="28115d48">2024.acl-long.742</url>
      <bibkey>lin-etal-2024-navigating</bibkey>
      <doi>10.18653/v1/2024.acl-long.742</doi>
    </paper>
    <paper id="743">
      <title><fixed-case>REFINESUMM</fixed-case>: Self-Refining <fixed-case>MLLM</fixed-case> for Generating a Multimodal Summarization Dataset</title>
      <author><first>Vaidehi</first><last>Patil</last></author>
      <author><first>Leonardo</first><last>Ribeiro</last><affiliation>Amazon</affiliation></author>
      <author><first>Mengwen</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Markus</first><last>Dreyer</last><affiliation>Amazon</affiliation></author>
      <pages>13773-13786</pages>
      <abstract>Multimodal Large Language Models (MLLMs) excel at synthesizing key information from diverse sources. However, generating accurate and faithful multimodal summaries is challenging, primarily due to the lack of appropriate multimodal datasets for fine-tuning that meaningfully integrate textual and visual modalities. To address this gap, we present a new dataset designed specifically for image-text multimodal summarization, harnessing the capabilities of state-of-the-art MLLMs. We generate summaries from Wikipedia sections and corresponding images and evaluate them across text-based, visual and multimodal dimensions, employing reference-free metrics. To refine the dataset, we: (1) Filter the MLLM-generated summaries by training a critic model on human annotations and using its predictions to remove low-quality summaries; (2) Fine-tune the MLLM with the filtered high-quality summaries; (3) Use the fine-tuned model in turn to regenerate the summaries. This self-refinement process significantly improves summary quality, as measured by human judgements and automatic multimodal metrics, resulting in a valuable dataset for multimodal summarization research. The dataset is publicly available at https://github.com/amazon-science/refinesumm.</abstract>
      <url hash="ff4d8f5a">2024.acl-long.743</url>
      <bibkey>patil-etal-2024-refinesumm</bibkey>
      <doi>10.18653/v1/2024.acl-long.743</doi>
    </paper>
    <paper id="744">
      <title>When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards</title>
      <author><first>Norah</first><last>Alzahrani</last><affiliation>Saudi Data and AI Authority, Saudi Data and AI Authority</affiliation></author>
      <author><first>Hisham</first><last>Alyahya</last><affiliation>Saudi Data and AI Authority, Saudi Data and AI Authority</affiliation></author>
      <author><first>Yazeed</first><last>Alnumay</last><affiliation>Saudi Data and AI Authority, Saudi Data and AI Authority</affiliation></author>
      <author><first>Sultan</first><last>AlRashed</last><affiliation>King Abdullah University of Science and Technology and Saudi Data &amp; Artificial Intelligence Authority</affiliation></author>
      <author><first>Shaykhah</first><last>Alsubaie</last><affiliation>Saudi Data and AI Authority, Saudi Data and AI Authority</affiliation></author>
      <author><first>Yousef</first><last>Almushayqih</last><affiliation>Saudi Data and AI Authority, Saudi Data and AI Authority</affiliation></author>
      <author><first>Faisal</first><last>Mirza</last></author>
      <author><first>Nouf</first><last>Alotaibi</last><affiliation>Saudi Data and AI Authority</affiliation></author>
      <author><first>Nora</first><last>Al-Twairesh</last><affiliation>King Saud University</affiliation></author>
      <author><first>Areeb</first><last>Alowisheq</last></author>
      <author><first>M Saiful</first><last>Bari</last><affiliation>National Centre of Artificial Intelligence, Saudi Data and AI Authority</affiliation></author>
      <author><first>Haidar</first><last>Khan</last><affiliation>Amazon</affiliation></author>
      <pages>13787-13805</pages>
      <abstract>Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value — we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a *hybrid* scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at [https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness](https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness).</abstract>
      <url hash="3bf49343">2024.acl-long.744</url>
      <bibkey>alzahrani-etal-2024-benchmarks</bibkey>
      <doi>10.18653/v1/2024.acl-long.744</doi>
    </paper>
    <paper id="745">
      <title><fixed-case>LLM</fixed-case>-Rubric: A Multidimensional, Calibrated Approach to Automated Evaluation of Natural Language Texts</title>
      <author><first>Helia</first><last>Hashemi</last></author>
      <author><first>Jason</first><last>Eisner</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Corby</first><last>Rosset</last></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Chris</first><last>Kedzie</last><affiliation>Rasa Technologies, Inc.</affiliation></author>
      <pages>13806-13834</pages>
      <abstract>This paper introduces a framework for the automated evaluation of natural language texts. A manually constructed rubric describes how to assess multiple dimensions of interest. To evaluate a text, a large language model (LLM) is prompted with each rubric question and produces a distribution over potential responses. The LLM predictions often fail to agree well with human judges—indeed, the humans do not fully agree with one another. However, the multiple LLM distributions can be _combined_ to _predict_ each human judge’s annotations on all questions, including a summary question that assesses overall quality or relevance. LLM-Rubric accomplishes this by training a small feed-forward neural network that includes both judge-specific and judge-independent parameters. When evaluating dialogue systems in a human-AI information-seeking task, we find that LLM-Rubric with 9 questions (assessing dimensions such as naturalness, conciseness, and citation quality) predicts human judges’ assessment of overall user satisfaction, on a scale of 1–4, with RMS error &lt; 0.5, a 2× improvement over the uncalibrated baseline.</abstract>
      <url hash="6434e925">2024.acl-long.745</url>
      <bibkey>hashemi-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.acl-long.745</doi>
    </paper>
    <paper id="746">
      <title><fixed-case>LIEDER</fixed-case>: Linguistically-Informed Evaluation for Discourse Entity Recognition</title>
      <author><first>Xiaomeng</first><last>Zhu</last><affiliation>Yale University</affiliation></author>
      <author><first>Robert</first><last>Frank</last><affiliation>Yale University</affiliation></author>
      <pages>13835-13850</pages>
      <abstract>Discourse Entity (DE) recognition is the task of identifying novel and known entities introduced within a text. While previous work has found that large language models have basic, if imperfect, DE recognition abilities (Schuster and Linzen, 2022), it remains largely unassessed which of the fundamental semantic properties that govern the introduction and subsequent reference to DEs they have knowledge of. We propose the Linguistically-Informed Evaluation for Discourse Entity Recognition (LIEDER) dataset that allows for a detailed examination of language models’ knowledge of four crucial semantic properties: existence, uniqueness, plurality, and novelty. We find evidence that state-of-the-art large language models exhibit sensitivity to all of these properties except novelty, which demonstrates that they have yet to reach human-level language understanding abilities.</abstract>
      <url hash="c17bef25">2024.acl-long.746</url>
      <bibkey>zhu-frank-2024-lieder</bibkey>
      <doi>10.18653/v1/2024.acl-long.746</doi>
    </paper>
    <paper id="747">
      <title>Evaluating Very Long-Term Conversational Memory of <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Adyasha</first><last>Maharana</last></author>
      <author><first>Dong-Ho</first><last>Lee</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Sergey</first><last>Tulyakov</last><affiliation>Snap Inc.</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Francesco</first><last>Barbieri</last><affiliation>Snap Inc.</affiliation></author>
      <author><first>Yuwei</first><last>Fang</last><affiliation>Snap Inc.</affiliation></author>
      <pages>13851-13870</pages>
      <abstract>Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 600 turns and 16K tokens on avg., over up to 32 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.</abstract>
      <url hash="8684826f">2024.acl-long.747</url>
      <bibkey>maharana-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.acl-long.747</doi>
    </paper>
    <paper id="748">
      <title>Prototypical Reward Network for Data-Efficient RLHF</title>
      <author><first>Jinghan</first><last>Zhang</last><affiliation>Portland State University</affiliation></author>
      <author><first>Xiting</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yiqiao</first><last>Jin</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Changyu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xinhao</first><last>Zhang</last><affiliation>Portland State University</affiliation></author>
      <author><first>Kunpeng</first><last>Liu</last><affiliation>Portland State University</affiliation></author>
      <pages>13871-13884</pages>
      <abstract>The reward model for Reinforcement Learning from Human Feedback (RLHF) has proven effective in fine-tuning Large Language Models (LLMs). Notably, collecting human feedback for RLHF can be resource-intensive and lead to scalability issues for LLMs and complex tasks. Our proposed framework Proto-RM leverages prototypical networks to enhance reward models under limited human feedback. By enabling stable and reliable structural learning from fewer samples, Proto-RM significantly enhances LLMs' adaptability and accuracy in interpreting human preferences. Extensive experiments on various datasets demonstrate that Proto-RM significantly improves the performance of reward models and LLMs in human feedback tasks, achieving comparable and usually better results than traditional methods, while requiring significantly less data in data-limited scenarios. This research offers a promising direction for enhancing the efficiency of reward models and optimizing the fine-tuning of language models under restricted feedback conditions.</abstract>
      <url hash="f45d9850">2024.acl-long.748</url>
      <bibkey>zhang-etal-2024-prototypical</bibkey>
      <doi>10.18653/v1/2024.acl-long.748</doi>
    </paper>
    <paper id="749">
      <title><fixed-case>NEO</fixed-case>-<fixed-case>BENCH</fixed-case>: Evaluating Robustness of Large Language Models with Neologisms</title>
      <author><first>Jonathan</first><last>Zheng</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Alan</first><last>Ritter</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>13885-13906</pages>
      <abstract>The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms – new word forms – over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs’ ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments.</abstract>
      <url hash="31453dbb">2024.acl-long.749</url>
      <bibkey>zheng-etal-2024-neo</bibkey>
      <doi>10.18653/v1/2024.acl-long.749</doi>
    </paper>
    <paper id="750">
      <title>Impacts of Misspelled Queries on Translation and Product Search</title>
      <author><first>Greg</first><last>Hanneman</last><affiliation>Amazon</affiliation></author>
      <author><first>Natawut</first><last>Monaikul</last><affiliation>Capital One and University of Illinois at Chicago</affiliation></author>
      <author><first>Taichi</first><last>Nakatani</last><affiliation>Amazon</affiliation></author>
      <pages>13907-13920</pages>
      <abstract>Machine translation is used in e-commerce to translate second-language queries into the primary language of the store, to be matched by the search system against the product catalog. However, many queries contain spelling mistakes. We first present an analysis of the spelling-robustness of a population of MT systems, quantifying how spelling variations affect MT output, the list of returned products, and ultimately user behavior. We then present two sets of practical experiments illustrating how spelling-robustness may be specifically improved. For MT, reducing the number of BPE operations significantly improves spelling-robustness in six language pairs. In end-to-end e-commerce, the inclusion of a dedicated spelling correction model, and the augmentation of that model’s training data with language-relevant phenomena, each improve robustness and consistency of search results.</abstract>
      <url hash="7ff2bdde">2024.acl-long.750</url>
      <bibkey>hanneman-etal-2024-impacts</bibkey>
      <doi>10.18653/v1/2024.acl-long.750</doi>
    </paper>
    <paper id="751">
      <title>Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Bilgehan</first><last>Sel</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Priya</first><last>Shanmugasundaram</last></author>
      <author><first>Mohammad</first><last>Kachuee</last><affiliation>Amazon</affiliation></author>
      <author><first>Kun</first><last>Zhou</last></author>
      <author><first>Ruoxi</first><last>Jia</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Ming</first><last>Jin</last><affiliation>Virginia Tech</affiliation></author>
      <pages>13921-13959</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable capabilities in tasks such as summarization, arithmetic reasoning, and question answering. However, they encounter significant challenges in the domain of moral reasoning and ethical decision-making, especially in complex scenarios with multiple stakeholders. This paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing moral reasoning in LLMs by exploring decisions’ consequences from multiple stakeholder perspectives. The core components of the framework consist of simulating accountability for decisions, conducting empathy exercises on different stakeholders, and evaluating the risks associated with the impacts of potential actions. We study SKIG’s performance across various moral reasoning benchmarks with proprietary and open-source LLMs, and investigate its crucial components through extensive ablation analyses. Our framework exhibits marked improvements in performance compared to baselines across different language models and benchmarks.</abstract>
      <url hash="fd30d126">2024.acl-long.751</url>
      <bibkey>sel-etal-2024-skin</bibkey>
      <doi>10.18653/v1/2024.acl-long.751</doi>
    </paper>
    <paper id="752">
      <title>The <fixed-case>MERSA</fixed-case> Dataset and a Transformer-Based Approach for Speech Emotion Recognition</title>
      <author><first>Enshi</first><last>Zhang</last><affiliation>Florida International University</affiliation></author>
      <author><first>Rafael</first><last>Trujillo</last><affiliation>Florida International University</affiliation></author>
      <author><first>Christian</first><last>Poellabauer</last><affiliation>Florida International University</affiliation></author>
      <pages>13960-13970</pages>
      <abstract>Research in the field of speech emotion recognition (SER) relies on the availability of comprehensive datasets to make it possible to design accurate emotion detection models. This study introduces the Multimodal Emotion Recognition and Sentiment Analysis (MERSA) dataset, which includes both natural and scripted speech recordings, transcribed text, physiological data, and self-reported emotional surveys from 150 participants collected over a two-week period. This work also presents a novel emotion recognition approach that uses a transformer-based model, integrating pre-trained wav2vec 2.0 and BERT for feature extractions and additional LSTM layers to learn hidden representations from fused representations from speech and text. Our model predicts emotions on dimensions of arousal, valence, and dominance. We trained and evaluated the model on the MSP-PODCAST dataset and achieved competitive results from the best-performing model regarding the concordance correlation coefficient (CCC). Further, this paper demonstrates the effectiveness of this model through cross-domain evaluations on both IEMOCAP and MERSA datasets.</abstract>
      <url hash="9fe1afcf">2024.acl-long.752</url>
      <bibkey>zhang-etal-2024-mersa</bibkey>
      <doi>10.18653/v1/2024.acl-long.752</doi>
    </paper>
    <paper id="753">
      <title>Transparent and Scrutable Recommendations Using Natural Language User Profiles</title>
      <author><first>Jerome</first><last>Ramos</last></author>
      <author><first>Hossein A.</first><last>Rahmani</last></author>
      <author><first>Xi</first><last>Wang</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Xiao</first><last>Fu</last></author>
      <author><first>Aldo</first><last>Lipani</last><affiliation>University College London, University of London</affiliation></author>
      <pages>13971-13984</pages>
      <abstract>Recent state-of-the-art recommender systems predominantly rely on either implicit or explicit feedback from users to suggest new items. While effective in recommending novel options, many recommender systems often use uninterpretable embeddings to represent user preferences. This lack of transparency not only limits user understanding of why certain items are suggested but also reduces the user’s ability to scrutinize and modify their preferences, thereby affecting their ability to receive a list of preferred recommendations. Given the recent advances in Large Language Models (LLMs), we investigate how a properly crafted prompt can be used to summarize a user’s preferences from past reviews and recommend items based only on language-based preferences. In particular, we study how LLMs can be prompted to generate a natural language (NL) user profile that holistically describe a user’s preferences. These NL profiles can then be leveraged to fine-tune a LLM using only NL profiles to make transparent and scrutable recommendations. Furthermore, we validate the scrutability of our user profile-based recommender by investigating the impact on recommendation changes after editing NL user profiles. According to our evaluations of the model’s rating prediction performance on two benchmarking rating prediction datasets, we observe that this novel approach maintains a performance level on par with established recommender systems in a warm-start setting. With a systematic analysis into the effect of updating user profiles and system prompts, we show the advantage of our approach in easier adjustment of user preferences and a greater autonomy over users’ received recommendations.</abstract>
      <url hash="df91fd80">2024.acl-long.753</url>
      <bibkey>ramos-etal-2024-transparent</bibkey>
      <doi>10.18653/v1/2024.acl-long.753</doi>
    </paper>
    <paper id="754">
      <title>Fora: A corpus and framework for the study of facilitated dialogue</title>
      <author><first>Hope</first><last>Schroeder</last></author>
      <author><first>Deb</first><last>Roy</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jad</first><last>Kabbara</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>13985-14001</pages>
      <abstract>Facilitated dialogue is increasingly popular as a method of civic engagement and as a method for gathering social insight, but resources for its study are scant. We present Fora, a unique collection of annotated facilitated dialogues. We compile 262 facilitated conversations that were hosted with partner organizations seeking to engage their members and surface insights regarding issues like education, elections, and public health, primarily through the sharing of personal experience. Alongside this corpus of 39,911 speaker turns, we present a framework for the analysis of facilitated dialogue. We taxonomize key personal sharing behaviors and facilitation strategies in the corpus, annotate a 25% sample (10,000+ speaker turns) of the data accordingly, and evaluate and establish baselines on a number of tasks essential to the identification of these phenomena in dialogue. We describe the data, and relate facilitator behavior to turn-taking and participant sharing. We outline how this research can inform future work in understanding and improving facilitated dialogue, parsing spoken conversation, and improving the behavior of dialogue agents.</abstract>
      <url hash="f628e964">2024.acl-long.754</url>
      <bibkey>schroeder-etal-2024-fora</bibkey>
      <doi>10.18653/v1/2024.acl-long.754</doi>
    </paper>
    <paper id="755">
      <title>Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning</title>
      <author><first>Yue</first><last>Yu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Jiaming</first><last>Shen</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Tianqi</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>Zhen</first><last>Qin</last><affiliation>Google</affiliation></author>
      <author><first>Jing Nathan</first><last>Yan</last></author>
      <author><first>Jialu</first><last>Liu</last><affiliation>Google Research</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <pages>14002-14024</pages>
      <abstract>Large language models (LLMs) have shown remarkable capabilities in various natural language understanding tasks with a few demonstration examples via in-context learning. Common strategies to boost such “in-context” learning ability are to ensemble multiple model decoded results and require the model to generate an explanation along with the prediction. However, these models often treat different class predictions equally and neglect the potential discrepancy between the explanations and predictions. To fully unleash the power of explanations, we propose EASE, an <i>Explanation-Aware Soft Ensemble</i> framework to empower in-context learning with LLMs. We design two techniques, explanation-guided ensemble, and soft probability aggregation, to mitigate the effect of unreliable explanations and improve the consistency between explanations and final predictions. Experiments on seven natural language understanding tasks and four varying-size LLMs demonstrate the effectiveness of our proposed framework.</abstract>
      <url hash="557d2ad7">2024.acl-long.755</url>
      <bibkey>yu-etal-2024-explanation</bibkey>
      <doi>10.18653/v1/2024.acl-long.755</doi>
    </paper>
    <paper id="756">
      <title>What is the Best Way for <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> to Translate Poetry?</title>
      <author><first>Shanshan</first><last>Wang</last><affiliation>University of Macau</affiliation></author>
      <author><first>Derek</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Jingming</first><last>Yao</last><affiliation>University of Macau</affiliation></author>
      <author><first>Lidia</first><last>Chao</last></author>
      <pages>14025-14043</pages>
      <abstract>Machine translation (MT) has historically faced significant challenges when applied to literary works, particularly in the domain of poetry translation. The advent of Large Language Models such as ChatGPT holds potential for innovation in this field. This study examines ChatGPT’s capabilities in English-Chinese poetry translation tasks, utilizing targeted prompts and small sample scenarios to ascertain optimal performance. Despite promising outcomes, our analysis reveals persistent issues in the translations generated by ChatGPT that warrant attention. To address these shortcomings, we propose an Explanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages monolingual poetry explanation as a guiding information for the translation process. Furthermore, we refine existing evaluation criteria to better suit the nuances of modern poetry translation. We engaged a panel of professional poets for assessments, complemented evaluations by using GPT-4. The results from both human and machine evaluations demonstrate that our EAPMT method outperforms traditional translation methods of ChatGPT and the existing online systems. This paper validates the efficacy of our method and contributes a novel perspective to machine-assisted literary translation.</abstract>
      <url hash="10b0cdd0">2024.acl-long.756</url>
      <bibkey>wang-etal-2024-best</bibkey>
      <doi>10.18653/v1/2024.acl-long.756</doi>
    </paper>
    <paper id="757">
      <title>Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling</title>
      <author><first>Pratyush</first><last>Maini</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Skyler</first><last>Seto</last><affiliation>Apple</affiliation></author>
      <author><first>Richard</first><last>Bai</last><affiliation>Apple</affiliation></author>
      <author><first>David</first><last>Grangier</last><affiliation>Apple</affiliation></author>
      <author><first>Yizhe</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Navdeep</first><last>Jaitly</last><affiliation>Apple</affiliation></author>
      <pages>14044-14072</pages>
      <abstract>Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training (WRAP) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as “like Wikipedia” or in “question-answer format” to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by ~3x. At the same pre-training compute budget, it improves perplexity by more than 50% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher ‘quality’ than web-scraped data.</abstract>
      <url hash="185cbf8e">2024.acl-long.757</url>
      <bibkey>maini-etal-2024-rephrasing</bibkey>
      <doi>10.18653/v1/2024.acl-long.757</doi>
    </paper>
    <paper id="758">
      <title><fixed-case>D</fixed-case>e<fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention</title>
      <author><first>Junda</first><last>Wu</last></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Xiang</first><last>Chen</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Haoliang</first><last>Wang</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Ryan</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Sungchul</first><last>Kim</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Anup</first><last>Rao</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <pages>14073-14087</pages>
      <abstract>Large language models (LLMs) often require task-relevant knowledge to augment their internal knowledge through prompts. However, simply injecting external knowledge into prompts does not guarantee that LLMs can identify and use relevant information in the prompts to conduct chain-of-thought reasoning, especially when the LLM’s internal knowledge is derived from biased information on the pretraining data. In this paper, we propose a novel causal view to formally explain the internal knowledge bias of LLMs via a Structural Causal Model (SCM). We review the chain-of-thought (CoT) prompting from a causal perspective and discover that the biased information from pretrained models can impair LLMs’ reasoning abilities. When the CoT reasoning paths are misled by irrelevant information from prompts and are logically incorrect, simply editing factual information is insufficient to reach the correct answer. To estimate the confounding effect on CoT reasoning in LLMs, we use external knowledge as an instrumental variable. We further introduce CoT as a mediator to conduct front-door adjustment and generate logically correct CoTs where the spurious correlation between LLMs’ pretrained knowledge and task queries is reduced. With extensive experiments, we validate that our approach enables more accurate CoT reasoning and enhances LLM generation on knowledge-intensive tasks.</abstract>
      <url hash="badbdbb5">2024.acl-long.758</url>
      <bibkey>wu-etal-2024-decot</bibkey>
      <doi>10.18653/v1/2024.acl-long.758</doi>
    </paper>
    <paper id="759">
      <title>Representation Learning with Conditional Information Flow Maximization</title>
      <author><first>Dou</first><last>Hu</last></author>
      <author><first>Lingwei</first><last>Wei</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>14088-14103</pages>
      <abstract>This paper proposes an information-theoretic representation learning framework, named conditional information flow maximization, to extract noise-invariant sufficient representations for the input data and target task. It promotes the learned representations have good feature uniformity and sufficient predictive ability, which can enhance the generalization of pre-trained language models (PLMs) for the target task. Firstly, an information flow maximization principle is proposed to learn more sufficient representations for the input and target by simultaneously maximizing both input-representation and representation-label mutual information. Unlike the information bottleneck, we handle the input-representation information in an opposite way to avoid the over-compression issue of latent representations. Besides, to mitigate the negative effect of potential redundant features from the input, we design a conditional information minimization principle to eliminate negative redundant features while preserve noise-invariant features. Experiments on 13 language understanding benchmarks demonstrate that our method effectively improves the performance of PLMs for classification and regression. Extensive experiments show that the learned representations are more sufficient, robust and transferable.</abstract>
      <url hash="caff7a4a">2024.acl-long.759</url>
      <bibkey>hu-etal-2024-representation</bibkey>
      <revision id="1" href="2024.acl-long.759v1" hash="264833a2"/>
      <revision id="2" href="2024.acl-long.759v2" hash="adb10dc6" date="2024-08-17">Corrected typos.</revision>
      <doi>10.18653/v1/2024.acl-long.759</doi>
      <revision id="3" href="2024.acl-long.759v3" hash="caff7a4a" date="2024-10-11">Add the proceedings footer stamps and page numbers in the revised PDF.</revision>
    </paper>
    <paper id="760">
      <title><fixed-case>GPT</fixed-case> is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction</title>
      <author><first>Virginia</first><last>Felkner</last><affiliation>USC/ISI</affiliation></author>
      <author><first>Jennifer</first><last>Thompson</last><affiliation>California State University, Northridge</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>14104-14115</pages>
      <abstract>Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.</abstract>
      <url hash="70662acb">2024.acl-long.760</url>
      <bibkey>felkner-etal-2024-gpt</bibkey>
      <doi>10.18653/v1/2024.acl-long.760</doi>
    </paper>
    <paper id="761">
      <title>Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models</title>
      <author><first>Martin</first><last>Riddell</last></author>
      <author><first>Ansong</first><last>Ni</last><affiliation>Yale University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>14116-14137</pages>
      <abstract>While large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. While recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of LLMs in programming contexts. In this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform significantly better on the subset of the benchmarks where similar solutions are seen during training. We also conduct extensive analysis on the factors that affect model memorization and generalization, such as model size, problem difficulty, and question length. We release all resulting files from our matching pipeline for future research.</abstract>
      <url hash="10b5288a">2024.acl-long.761</url>
      <bibkey>riddell-etal-2024-quantifying</bibkey>
      <doi>10.18653/v1/2024.acl-long.761</doi>
    </paper>
    <paper id="762">
      <title>Language Models are <fixed-case>H</fixed-case>omer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic</title>
      <author><first>Rishabh</first><last>Bhardwaj</last></author>
      <author><first>Duc Anh</first><last>Do</last></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>14138-14149</pages>
      <abstract>We propose RESTA to perform LLM realignment towards safety, which gets compromised due to downstream task fine-tuning. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in parameter-efficient and full fine-tuning, respectively, while maintaining most of the model’s performance on the task. We release the source codes at: https://github.com/declare-lab/resta.</abstract>
      <url hash="3c34c1aa">2024.acl-long.762</url>
      <bibkey>bhardwaj-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.acl-long.762</doi>
    </paper>
    <paper id="763">
      <title>Tracking the Newsworthiness of Public Documents</title>
      <author><first>Alexander</first><last>Spangher</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Serdar</first><last>Tumgoren</last><affiliation>Stanford University</affiliation></author>
      <author><first>Ben</first><last>Welsh</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Emilio</first><last>Ferrara</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>14150-14168</pages>
      <abstract>Journalists regularly make decisions on whether or not to report stories, based on “news values”. In this work, we wish to explicitly model these decisions to explore _when_ and _why_ certain stories get press attention. This is challenging because very few labelled links between source documents and news articles exist and language use between corpora is very different. We address this problem by implementing a novel _probabilistic relational modeling_ framework, which we show is a low-annotation linking methodology that outperforms other, more state-of-the-art retrieval-based baselines. Next, we define a new task: __newsworthiness prediction__, to predict if a policy item will get covered. We focus on news coverage of local public policy in the San Francisco Bay Area by the _San Francisco Chronicle_. We gather 15k policies discussed across 10 years of public policy meetings, and transcribe over 3,200 hours of public discussion. In general, we find limited impact of public discussion on newsworthiness prediction accuracy, suggesting that some of the most important stories barely get discussed in public.Finally, we show that newsworthiness predictions can be a useful assistive tool for journalists seeking to keep abreast of local government. We perform human evaluation with expert journalists and show our systems identify policies they consider newsworthy with 68% F1 and our coverage recommendations are helpful with an 84% win-rate against baseline. We release all code and data to our work here: https://github.com/alex2awesome/newsworthiness-public.</abstract>
      <url hash="4f280702">2024.acl-long.763</url>
      <bibkey>spangher-etal-2024-tracking</bibkey>
      <doi>10.18653/v1/2024.acl-long.763</doi>
    </paper>
    <paper id="764">
      <title><fixed-case>EWEK</fixed-case>-<fixed-case>QA</fixed-case> : Enhanced Web and Efficient Knowledge Graph Retrieval for Citation-based Question Answering Systems</title>
      <author><first>Mohammad</first><last>Dehghan</last></author>
      <author><first>Mohammad</first><last>Alomrani</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Sunyam</first><last>Bagga</last></author>
      <author><first>David</first><last>Alfonso-Hermelo</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Khalil</first><last>Bibi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Abbas</first><last>Ghaddar</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yingxue</first><last>Zhang</last></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Jianye</first><last>Hao</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Boxing</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Prasanna</first><last>Parthasarathi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mahdi</first><last>Biparva</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <pages>14169-14187</pages>
      <abstract>The emerging citation-based QA systems are gaining more attention especially in generative AI search applications. The importance of extracted knowledge provided to these systems is vital from both accuracy (completeness of information) and efficiency (extracting the information in a timely manner). In this regard, citation-based QA systems are suffering from two shortcomings. First, they usually rely only on web as a source of extracted knowledge and adding other external knowledge sources can hamper the efficiency of the system. Second, web-retrieved contents are usually obtained by some simple heuristics such as fixed length or breakpoints which might lead to splitting information into pieces. To mitigate these issues, we propose our enhanced web and efficient knowledge graph (KG) retrieval solution (EWEK-QA) to enrich the content of the extracted knowledge fed to the system. This has been done through designing an adaptive web retriever and incorporating KGs triples in an efficient manner. We demonstrate the effectiveness of over the open-source state-of-the-art (SoTA) web-based and KG baseline models using a comprehensive set of quantitative and human evaluation experiments. Our model is able to: first, improve the web-retriever baseline in terms of extracting more relevant passages (&gt;20%), the coverage of answer span (&gt;25%) and self containment (&gt;35%); second, obtain and integrate KG triples into its pipeline very efficiently (by avoiding any LLM calls) to outperform the web-only and KG-only SoTA baselines significantly in 7 quantitative QA tasks and our human evaluation.</abstract>
      <url hash="6d00dbf5">2024.acl-long.764</url>
      <bibkey>dehghan-etal-2024-ewek</bibkey>
      <doi>10.18653/v1/2024.acl-long.764</doi>
    </paper>
    <paper id="765">
      <title>Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models</title>
      <author><first>Shengzhi</first><last>Li</last><affiliation>TIFIN</affiliation></author>
      <author><first>Rongyu</first><last>Lin</last></author>
      <author><first>Shichao</first><last>Pei</last><affiliation>University of Massachusetts Boston</affiliation></author>
      <pages>14188-14200</pages>
      <abstract>Multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities in production. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets with which the underlying language model was trained. To address this degradation, we first collect a lightweight, 5k-sample VQA preference dataset where answers were annotated by Gemini for five quality metrics in a granular fashion and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO) and SteerLM algorithms. Our findings indicate that with DPO, we can surpass the instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna’s 6.57 and LLaVA’s 5.99. This enhancement in textual instruction-following capability correlates with boosted visual instruction performance (+4.9% on MM-Vet, +6% on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks compared to the previous RLHF approach. In conclusion, we propose a distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that restores and boosts MLLM’s language capability after visual instruction tuning.</abstract>
      <url hash="8e5c32bf">2024.acl-long.765</url>
      <bibkey>li-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.acl-long.765</doi>
    </paper>
    <paper id="766">
      <title>Multistage Collaborative Knowledge Distillation from a Large Language Model for Semi-Supervised Sequence Generation</title>
      <author><first>Jiachen</first><last>Zhao</last></author>
      <author><first>Wenlong</first><last>Zhao</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Andrew</first><last>Drozdov</last><affiliation>Databricks</affiliation></author>
      <author><first>Benjamin</first><last>Rozonoyer</last></author>
      <author><first>Md Arafat</first><last>Sultan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Jay-Yoon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Andrew</first><last>McCallum</last><affiliation>University of Massachusetts Amherst and University of Massachusetts Amherst</affiliation></author>
      <pages>14201-14214</pages>
      <abstract>We study semi-supervised sequence generation tasks, where the few labeled examples are too scarce to finetune a model, and meanwhile, few-shot prompted large language models (LLMs) exhibit room for improvement. In this paper, we present the discovery that a student model distilled from a few-shot prompted LLM can commonly generalize better than its teacher to unseen examples on such tasks. We find that the student is able to learn a general pattern from the high-quality pseudolabels produced by the teacher during knowledge distillation (KD), and favorably not a general pattern from the low-quality pseudolabels. Leveraging this discovery, we propose a new method, Multistage Collaborative Knowledge Distillation from an LLM (MCKD), for these tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. Then at each stage of an iterative KD process, a new pair of students is trained on disjoint partitions of the pseudolabeled data, and produces new and improved pseudolabels for their unseen partitions. We conduct extensive experiments on four syntactic and semantic parsing datasets and show the effectiveness of MCKD for low-resource semi-supervised sequence generation. On CRAFT biomedical parsing, for example, 3-stage MCKD with 50 labeled examples outperforms an LLM teacher and vanilla KD by 7.5% and 3.7% parsing F1, respectively, and matches the performance of supervised finetuning with 500 labeled examples.</abstract>
      <url hash="2d24013e">2024.acl-long.766</url>
      <bibkey>zhao-etal-2024-multistage</bibkey>
      <doi>10.18653/v1/2024.acl-long.766</doi>
    </paper>
    <paper id="767">
      <title>Controlled Text Generation for Black-box Language Models via Score-based Progressive Editor</title>
      <author><first>Sangwon</first><last>Yu</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Changmin</first><last>Lee</last><affiliation>Kakao Corporation</affiliation></author>
      <author><first>Hojin</first><last>Lee</last><affiliation>Kakao brain</affiliation></author>
      <author><first>Sungroh</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <pages>14215-14237</pages>
      <abstract>Controlled text generation, aiming to ensure that language models produce text containing only the desired domain or corpus attributes, is immensely crucial in the practical application of language models. Existing methods, however, are inapplicable to black-box models or suffer a significant trade-off between control and fluency in text generation. This paper introduces the Score-based Progressive Editor (ScoPE), a novel approach designed to overcome these issues. ScoPE modifies the context at the token level during the generation process of a backbone language model. This modification guides the subsequent text to naturally include the target attributes. To facilitate this process, ScoPE employs a training objective that maximizes a target score, comprehensively considering both control and fluency. Experimental results on diverse controlled generation tasks demonstrate that ScoPE can effectively regulate the attributes of the generated text while effectively utilizing the capability of the backbone large language models.</abstract>
      <url hash="7904cc9d">2024.acl-long.767</url>
      <bibkey>yu-etal-2024-controlled</bibkey>
      <doi>10.18653/v1/2024.acl-long.767</doi>
    </paper>
    <paper id="768">
      <title><fixed-case>L</fixed-case>ogogram<fixed-case>NLP</fixed-case>: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for <fixed-case>NLP</fixed-case></title>
      <author><first>Danlu</first><last>Chen</last></author>
      <author><first>Freda</first><last>Shi</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Aditi</first><last>Agarwal</last></author>
      <author><first>Jacobo</first><last>Myerston</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>14238-14254</pages>
      <abstract>Standard natural language processing (NLP) pipelines operate on symbolic representations of language, which typically consist of sequences of discrete tokens. However, creating an analogous representation for ancient logographic writing systems is an extremely labor-intensive process that requires expert knowledge. At present, a large portion of logographic data persists in a purely visual form due to the absence of transcription—this issue poses a bottleneck for researchers seeking to apply NLP toolkits to study ancient logographic languages: most of the relevant data are images of writing. This paper investigates whether direct processing of visual representations of language offers a potential solution. We introduce LogogramNLP, the first benchmark enabling NLP analysis of ancient logographic languages, featuring both transcribed and visual datasetsfor four writing systems along with annotations for tasks like classification, translation, and parsing. Our experiments compare systems thatemploy recent visual and text encoding strategies as backbones. The results demonstrate that visual representations outperform textual representations for some investigated tasks, suggesting that visual processing pipelines may unlock a large amount of cultural heritage data of logographic languages for NLP-based analyses. Data and code are available at https: //logogramNLP.github.io/.</abstract>
      <url hash="209aea77">2024.acl-long.768</url>
      <bibkey>chen-etal-2024-logogramnlp</bibkey>
      <doi>10.18653/v1/2024.acl-long.768</doi>
    </paper>
    <paper id="769">
      <title>Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning</title>
      <author><first>Ming</first><last>Li</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Yong</first><last>Zhang</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Shwai</first><last>He</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Zhitao</first><last>Li</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Hongyu</first><last>Zhao</last></author>
      <author><first>Jianzong</first><last>Wang</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Ning</first><last>Cheng</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>14255-14273</pages>
      <abstract>Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of our approach.</abstract>
      <url hash="ee929545">2024.acl-long.769</url>
      <bibkey>li-etal-2024-superfiltering</bibkey>
      <doi>10.18653/v1/2024.acl-long.769</doi>
    </paper>
    <paper id="770">
      <title>Confabulation: The Surprising Value of Large Language Model Hallucinations</title>
      <author><first>Peiqi</first><last>Sui</last></author>
      <author><first>Eamon</first><last>Duede</last><affiliation>Harvard University</affiliation></author>
      <author><first>Sophie</first><last>Wu</last></author>
      <author><first>Richard</first><last>So</last></author>
      <pages>14274-14284</pages>
      <abstract>This paper presents a systematic defense of large language model (LLM) hallucinations or ‘confabulations’ as a potential resource instead of a categorically negative pitfall. The standard view is that confabulations are inherently problematic and AI research should eliminate this flaw. In this paper, we argue and empirically demonstrate that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication. In other words, it has potential value. Specifically, we analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs. This finding reveals a tension in our usually dismissive understandings of confabulation. It suggests, counter-intuitively, that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation.</abstract>
      <url hash="80c52618">2024.acl-long.770</url>
      <bibkey>sui-etal-2024-confabulation</bibkey>
      <doi>10.18653/v1/2024.acl-long.770</doi>
    </paper>
    <paper id="771">
      <title><fixed-case>IAPT</fixed-case>: Instance-Aware Prompt Tuning for Large Language Models</title>
      <author><first>Wei</first><last>Zhu</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Aaron</first><last>Tian</last></author>
      <author><first>Congrui</first><last>Yin</last></author>
      <author><first>Yuan</first><last>Ni</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Xiaoling</first><last>Wang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Guotong</first><last>Xie</last><affiliation>Pingan Technology</affiliation></author>
      <pages>14285-14304</pages>
      <abstract>Soft prompt tuning is a widely studied parameter-efficient fine-tuning method. However, it has a clear drawback: many soft tokens must be inserted into the input sequences to guarantee downstream performance. As a result, soft prompt tuning is less considered than Low-rank adaptation (LoRA) in the large language modeling (LLM) era. In this work, we propose a novel prompt tuning method, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft tokens. First, we install a parameter-efficient soft prompt generator at each Transformer layer to generate idiosyncratic soft prompts for each input instruction. The generated soft prompts can be seen as a semantic summary of the input instructions and can effectively guide the output generation. Second, the soft prompt generators are modules with a bottleneck architecture consisting of a self-attention pooling operation, two linear projections, and an activation function. Pilot experiments show that prompt generators at different Transformer layers require different activation functions. Thus, we propose to learn the idiosyncratic activation functions for prompt generators automatically with the help of rational functions. We have conducted experiments on various tasks, and the experimental results demonstrate that (a) our IAPT method can outperform the recent baselines with comparable tunable parameters. (b) Our IAPT method is more efficient than LoRA under the single-backbone multi-tenant setting.</abstract>
      <url hash="92338376">2024.acl-long.771</url>
      <bibkey>zhu-etal-2024-iapt</bibkey>
      <doi>10.18653/v1/2024.acl-long.771</doi>
    </paper>
    <paper id="772">
      <title><fixed-case>D</fixed-case>e<fixed-case>VA</fixed-case>n: Dense Video Annotation for Video-Language Models</title>
      <author><first>Tingkai</first><last>Liu</last><affiliation>Cold Spring Harbor Laboratory</affiliation></author>
      <author><first>Yunzhe</first><last>Tao</last><affiliation>ByteDance</affiliation></author>
      <author><first>Haogeng</first><last>Liu</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Qihang</first><last>Fang</last><affiliation>Institute of automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ding</first><last>Zhou</last><affiliation>Bytedance</affiliation></author>
      <author><first>Huaibo</first><last>Huang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ran</first><last>He</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongxia</first><last>Yang</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>14305-14321</pages>
      <abstract>We present a novel human annotated dataset for evaluating the ability for visual-language models to generate both short and long descriptions for real-world video clips, termed <tex-math>{\bf DeVAn}</tex-math> (Dense Video Annotation). The dataset contains 8.5K YouTube video clips of 20-60 seconds in duration and covers a wide range of topics and interests. Each video clip is independently annotated by 5 human annotators, producing both captions (1 sentence) and summaries (3-10 sentences). Given any video selected from the dataset and its corresponding ASR information, we evaluate visual-language models on either caption or summary generation that is grounded in both the visual and auditory content of the video. Additionally, models are also evaluated on caption- and summary-based retrieval tasks, where the summary-based retrieval task requires the identification of a target video given <tex-math>\textit{excerpts}</tex-math> of a given summary. Given the novel nature of the paragraph-length video summarization task, we compared different existing evaluation metrics and their alignment with human preferences and found that model-based evaluation metrics provide more semantically-oriented and human-aligned evaluation. Finally, we benchmarked a wide range of current video-language models on DeVAn, and we aim for DeVAn to serve as a useful evaluation set in the age of large language models and complex multi-modal tasks. Code is available at https://github.com/TK-21st/DeVAn.</abstract>
      <url hash="e738f259">2024.acl-long.772</url>
      <bibkey>liu-etal-2024-devan</bibkey>
      <doi>10.18653/v1/2024.acl-long.772</doi>
    </paper>
    <paper id="773">
      <title>How Johnny Can Persuade <fixed-case>LLM</fixed-case>s to Jailbreak Them: Rethinking Persuasion to Challenge <fixed-case>AI</fixed-case> Safety by Humanizing <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yi</first><last>Zeng</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Hongpeng</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jingwen</first><last>Zhang</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Ruoxi</first><last>Jia</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Weiyan</first><last>Shi</last><affiliation>Stanford University</affiliation></author>
      <pages>14322-14350</pages>
      <abstract>Most traditional AI safety research views models as machines and centers on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. Observing this, we shift the perspective, by treating LLMs as human-like communicators to examine the interplay between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak risk across all risk categories: PAP consistently achieves an attack success rate of over 92% on Llama-2-7b-Chat, GPT-3.5, and GPT-4 in 10 trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for more fundamental solutions for AI safety.</abstract>
      <url hash="44944142">2024.acl-long.773</url>
      <bibkey>zeng-etal-2024-johnny</bibkey>
      <doi>10.18653/v1/2024.acl-long.773</doi>
    </paper>
    <paper id="774">
      <title>The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models</title>
      <author><first>Adithya</first><last>Bhaskar</last><affiliation>Princeton University</affiliation></author>
      <author><first>Dan</first><last>Friedman</last><affiliation>Princeton University</affiliation></author>
      <author><first>Danqi</first><last>Chen</last><affiliation>Department of Computer Science, Princeton University</affiliation></author>
      <pages>14351-14368</pages>
      <abstract>Prior work has found that pretrained language models (LMs) fine-tuned with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization. In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently. To better understand these phenomena, we investigate if they can be understood in terms of “competing subnetworks”: the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one. This explanation has been used to account for generalization in simple algorithmic tasks (“grokking”). Instead of finding competing subnetworks, we find that all subnetworks—whether they generalize or not—share a set of attention heads, which we refer to as the _heuristic core_. Further analysis suggests that these attention heads emerge early in training and compute shallow, non-generalizing features. The model learns to generalize by incorporating additional attention heads, which depend on the outputs of the “heuristic” heads to compute higher-level features. Overall, our results offer a more detailed picture of the mechanisms for syntactic generalization in pre-trained LMs.</abstract>
      <url hash="d33d3752">2024.acl-long.774</url>
      <bibkey>bhaskar-etal-2024-heuristic</bibkey>
      <doi>10.18653/v1/2024.acl-long.774</doi>
    </paper>
    <paper id="775">
      <title>Multimodal <fixed-case>A</fixed-case>r<fixed-case>X</fixed-case>iv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models</title>
      <author><first>Lei</first><last>Li</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Yuqi</first><last>Wang</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Runxin</first><last>Xu</last><affiliation>Peking University</affiliation></author>
      <author><first>Peiyi</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiachong</first><last>Feng</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Hong Kong</affiliation></author>
      <pages>14369-14387</pages>
      <abstract>Large vision-language models (LVLMs) excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains.To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension.ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions, sourced from 572K ArXiv papers spanning various scientific domains.Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances open-sourced LVLMs’ mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark.Furthermore, employing ArXivCap, we devise four vision-to-text tasks for benchmarking LVLMs.Evaluation results with state-of-the-art LVLMs underscore their struggle with the nuanced semantics of academic figures, while domain-specific training yields substantial performance gains.Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.</abstract>
      <url hash="92fee889">2024.acl-long.775</url>
      <bibkey>li-etal-2024-multimodal-arxiv</bibkey>
      <doi>10.18653/v1/2024.acl-long.775</doi>
    </paper>
    <paper id="776">
      <title><fixed-case>L</fixed-case>-Eval: Instituting Standardized Evaluation for Long Context Language Models</title>
      <author><first>Chenxin</first><last>An</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Shansan</first><last>Gong</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Ming</first><last>Zhong</last><affiliation>University of Illinois Urbana Champaign</affiliation></author>
      <author><first>Xingjian</first><last>Zhao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Mukai</first><last>Li</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Jun</first><last>Zhang</last><affiliation>ByteDance</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>14388-14411</pages>
      <abstract>Recently, there has been growing interest in long-context scaling of large language models (LLMs). To facilitate research in this field, we propose L-Eval to institute a more standardized evaluation for Long-Context Language Models (LCLMs) addressing two key aspects: dataset construction and evaluation metrics. On the one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long documents, and more than 2,000 human-labeled query-response pairs including diverse task types, domains, and input length (3k~200k tokens). On the other hand, we investigate the effectiveness of evaluation metrics for LCLMs and we show that Length-instruction-enhanced (LIE) evaluation and LLM judges can better correlate with human judgments. We conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source counterparts using the L-Eval benchmark. Our empirical findings offer useful insights into the study of LCLMs and lay the groundwork for the development of a more principled evaluation of these models.</abstract>
      <url hash="1a4d3fdb">2024.acl-long.776</url>
      <bibkey>an-etal-2024-l</bibkey>
      <doi>10.18653/v1/2024.acl-long.776</doi>
    </paper>
    <paper id="777">
      <title><fixed-case>DIALECTBENCH</fixed-case>: An <fixed-case>NLP</fixed-case> Benchmark for Dialects, Varieties, and Closely-Related Languages</title>
      <author><first>Fahim</first><last>Faisal</last><affiliation>, George Mason University</affiliation></author>
      <author><first>Orevaoghene</first><last>Ahia</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Aarohi</first><last>Srivastava</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Kabir</first><last>Ahuja</last><affiliation>Microsoft</affiliation></author>
      <author><first>David</first><last>Chiang</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center</affiliation></author>
      <pages>14412-14454</pages>
      <abstract>Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP benchmarks are limited to standard language varieties. To fill this gap, we propose DIALECTBENCH, the first-ever large-scale benchmark for NLP on varieties, which aggregates an extensive set of task-varied varieties datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different varieties. We provide substantial proof of performance disparities between standard and non-standard language varieties, and we also identify language clusters with larger performance divergence across tasks.We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for varieties and one step towards advancing it further.</abstract>
      <url hash="9249c4b1">2024.acl-long.777</url>
      <bibkey>faisal-etal-2024-dialectbench</bibkey>
      <doi>10.18653/v1/2024.acl-long.777</doi>
    </paper>
    <paper id="778">
      <title>Causal-Guided Active Learning for Debiasing Large Language Models</title>
      <author><first>Zhouhao</first><last>Sun</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Li</first><last>Du</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xiao</first><last>Ding</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yixuan</first><last>Ma</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Kaitao</first><last>Qiu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>14455-14469</pages>
      <abstract>Although achieving promising performance, recent analyses show that current generative large language models (LLMs) may still capture dataset biases and utilize them for generation, leading to poor generalizability and harmfulness of LLMs. However, due to the diversity of dataset biases and the over-optimization problem, previous prior-knowledge-based debiasing methods and fine-tuning-based debiasing methods may not be suitable for current LLMs.To address this issue, we explore combining active learning with the causal mechanisms and propose a casual-guided active learning (CAL) framework, which utilizes LLMs itself to automatically and autonomously identify informative biased samples and induce the bias patterns. Then a cost-effective and efficient in-context learning based method is employed to prevent LLMs from utilizing dataset biases during generation.Experimental results show that CAL can effectively recognize typical biased instances and induce various bias patterns for debiasing LLMs.</abstract>
      <url hash="b2bec36b">2024.acl-long.778</url>
      <bibkey>sun-etal-2024-causal</bibkey>
      <doi>10.18653/v1/2024.acl-long.778</doi>
    </paper>
    <paper id="779">
      <title><fixed-case>P</fixed-case>sycho<fixed-case>GAT</fixed-case>: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Qisen</first><last>Yang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zekun</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Honghui</first><last>Chen</last><affiliation>Central South University</affiliation></author>
      <author><first>Shenzhi</first><last>Wang</last><affiliation>Department of Automation, Tsinghua University</affiliation></author>
      <author><first>Yifan</first><last>Pu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xin</first><last>Gao</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Wenhao</first><last>Huang</last><affiliation>BAAI</affiliation></author>
      <author><first>Shiji</first><last>Song</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Gao</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>14470-14505</pages>
      <abstract>Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT’s enhancements in content coherence, interactivity, interest, immersion, and satisfaction.</abstract>
      <url hash="66523d08">2024.acl-long.779</url>
      <bibkey>yang-etal-2024-psychogat</bibkey>
      <doi>10.18653/v1/2024.acl-long.779</doi>
    </paper>
    <paper id="780">
      <title>Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient</title>
      <author><first>Mingxin</first><last>Li</last><affiliation>Beihang University</affiliation></author>
      <author><first>Richong</first><last>Zhang</last><affiliation>Beihang University</affiliation></author>
      <author><first>Zhijie</first><last>Nie</last><affiliation>The Insititute of Advanced Computing Technology, Beihang University</affiliation></author>
      <pages>14506-14521</pages>
      <abstract>Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, many studies have investigated the similarities between contrastive and non-contrastive SSL from a theoretical perspective. Such similarities can be verified in classification tasks, where the two approaches achieve comparable performance. But in ranking tasks (i.e., Semantic Textual Similarity (STS) in SRL), contrastive SSL significantly outperforms non-contrastive SSL. Therefore, two questions arise: First, *what commonalities enable various contrastive losses to achieve superior performance in STS?* Second, *how can we make non-contrastive SSL also effective in STS?* To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, which depends on three components: the **Gradient Dissipation**, the **Weight**, and the **Ratio**. Then, we conduct an in-depth analysis of the roles these components play in optimization and experimentally demonstrate their significance for model performance. Finally, by adjusting these components, we enable non-contrastive SSL to achieve outstanding performance in STS.</abstract>
      <url hash="2477a6a0">2024.acl-long.780</url>
      <bibkey>li-etal-2024-towards-better</bibkey>
      <doi>10.18653/v1/2024.acl-long.780</doi>
    </paper>
    <paper id="781">
      <title>Emergent Word Order Universals from Cognitively-Motivated Language Models</title>
      <author><first>Tatsuki</first><last>Kuribayashi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ryo</first><last>Ueda</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Ryo</first><last>Yoshida</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <author><first>Ted</first><last>Briscoe</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>14522-14543</pages>
      <abstract>The world’s languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) languages typically use postpositions. Explaining the source of such biases is a key goal of linguistics.We study word-order universals through a computational simulation with language models (LMs).Our experiments show that typologically-typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of cognitive biases and predictability (perplexity) can explain many aspects of word-order universals.It also showcases the advantage of cognitively-motivated LMs, typically employed in cognitive modeling, in the simulation of language universals.</abstract>
      <url hash="2f42351c">2024.acl-long.781</url>
      <bibkey>kuribayashi-etal-2024-emergent</bibkey>
      <doi>10.18653/v1/2024.acl-long.781</doi>
    </paper>
    <paper id="782">
      <title>Exploring Collaboration Mechanisms for <fixed-case>LLM</fixed-case> Agents: A Social Psychology View</title>
      <author><first>Jintian</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xin</first><last>Xu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ruibo</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Shumin</first><last>Deng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>14544-14607</pages>
      <abstract>As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: *Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)?* This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique ‘societies’ comprised of LLM agents, where each agent is characterized by a specific ‘trait’ (easy-going or overconfident) and engages in collaboration with a distinct ‘thinking pattern’ (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets, hoping to catalyze further research in this promising avenue.</abstract>
      <url hash="cb5006e9">2024.acl-long.782</url>
      <bibkey>zhang-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.acl-long.782</doi>
    </paper>
    <paper id="783">
      <title><fixed-case>MARVEL</fixed-case>: Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin</title>
      <author><first>Tianshuo</first><last>Zhou</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Sen</first><last>Mei</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xinze</first><last>Li</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Zhenghao</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Chenyan</first><last>Xiong</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yu</first><last>Gu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Ge</first><last>Yu</last><affiliation>Northeastern University, China</affiliation></author>
      <pages>14608-14624</pages>
      <abstract>This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL), which learns an embedding space for queries and multi-modal documents to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts. Specifically, we enable the image understanding ability of the well-trained dense retriever, T5-ANCE, by incorporating the visual module’s encoded image features as its inputs. To facilitate the multi-modal retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which regards anchor texts as queries, and extracts the related text and image documents from anchor-linked web pages. Our experiments show that MARVEL significantly outperforms the state-of-the-art methods on the multi-modal retrieval dataset WebQA and ClueWeb22-MM. MARVEL provides an opportunity to broaden the advantages of text retrieval to the multi-modal scenario. Besides, we also illustrate that the language model has the ability to extract image semantics and partly map the image features to the input word embedding space. All codes are available at https://github.com/OpenMatch/MARVEL.</abstract>
      <url hash="999e7a91">2024.acl-long.783</url>
      <bibkey>zhou-etal-2024-marvel</bibkey>
      <doi>10.18653/v1/2024.acl-long.783</doi>
    </paper>
    <paper id="784">
      <title>Distributional Inclusion Hypothesis and Quantifications: Probing for Hypernymy in Functional Distributional Semantics</title>
      <author><first>Chun Hei</first><last>Lo</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Hong</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Guy</first><last>Emerson</last><affiliation>University of Cambridge</affiliation></author>
      <pages>14625-14637</pages>
      <abstract>Functional Distributional Semantics (FDS) models the meaning of words by truth-conditional functions. This provides a natural representation for hypernymy but no guarantee that it can be learnt when FDS models are trained on a corpus. In this paper, we probe into FDS models and study the representations learnt, drawing connections between quantifications, the Distributional Inclusion Hypothesis (DIH), and the variational-autoencoding objective of FDS model training. Using synthetic data sets, we reveal that FDS models learn hypernymy on a restricted class of corpus that strictly follows the DIH. We further introduce a training objective that both enables hypernymy learning under the reverse of the DIH and improves hypernymy detection from real corpora.</abstract>
      <url hash="78b16997">2024.acl-long.784</url>
      <bibkey>lo-etal-2024-distributional</bibkey>
      <doi>10.18653/v1/2024.acl-long.784</doi>
    </paper>
    <paper id="785">
      <title><fixed-case>C</fixed-case>ausal<fixed-case>G</fixed-case>ym: Benchmarking causal interpretability methods on linguistic tasks</title>
      <author><first>Aryaman</first><last>Arora</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <author><first>Christopher</first><last>Potts</last><affiliation>Stanford University</affiliation></author>
      <pages>14638-14663</pages>
      <abstract>Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M–6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler–gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.</abstract>
      <url hash="e1b07a7a">2024.acl-long.785</url>
      <bibkey>arora-etal-2024-causalgym</bibkey>
      <doi>10.18653/v1/2024.acl-long.785</doi>
    </paper>
    <paper id="786">
      <title>Don’t Hallucinate, Abstain: Identifying <fixed-case>LLM</fixed-case> Knowledge Gaps via Multi-<fixed-case>LLM</fixed-case> Collaboration</title>
      <author><first>Shangbin</first><last>Feng</last><affiliation>University of Washington</affiliation></author>
      <author><first>Weijia</first><last>Shi</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Yike</first><last>Wang</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Wenxuan</first><last>Ding</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Vidhisha</first><last>Balachandran</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>14664-14690</pages>
      <abstract>Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps—missing or outdated information in LLMs—might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our abstention methods pinpoint failure cases in retrieval augmentation and knowledge gaps in multi-hop reasoning.</abstract>
      <url hash="36f2f0eb">2024.acl-long.786</url>
      <bibkey>feng-etal-2024-dont</bibkey>
      <doi>10.18653/v1/2024.acl-long.786</doi>
    </paper>
    <paper id="787">
      <title>Mission: Impossible Language Models</title>
      <author><first>Julie</first><last>Kallini</last><affiliation>Stanford University</affiliation></author>
      <author><first>Isabel</first><last>Papadimitriou</last><affiliation>Stanford University</affiliation></author>
      <author><first>Richard</first><last>Futrell</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Kyle</first><last>Mahowald</last><affiliation>The University of Texas at Austin</affiliation></author>
      <author><first>Christopher</first><last>Potts</last><affiliation>Stanford University</affiliation></author>
      <pages>14691-14714</pages>
      <abstract>Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations.</abstract>
      <url hash="f4fc476b">2024.acl-long.787</url>
      <bibkey>kallini-etal-2024-mission</bibkey>
      <doi>10.18653/v1/2024.acl-long.787</doi>
    </paper>
    <paper id="788">
      <title>Semisupervised Neural Proto-Language Reconstruction</title>
      <author><first>Liang</first><last>Lu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Peirong</first><last>Xie</last><affiliation>University of Southern California</affiliation></author>
      <author><first>David</first><last>Mortensen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>14715-14759</pages>
      <abstract>Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). We propose a neural architecture for comparative reconstruction (DPD-BiReconstructor) incorporating an essential insight from linguists’ comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. We show that this architecture is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task.</abstract>
      <url hash="0c211f0e">2024.acl-long.788</url>
      <bibkey>lu-etal-2024-semisupervised</bibkey>
      <doi>10.18653/v1/2024.acl-long.788</doi>
    </paper>
    <paper id="789">
      <title>Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?</title>
      <author><first>Marco</first><last>Gaido</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Sara</first><last>Papi</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Luisa</first><last>Bentivogli</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>14760-14778</pages>
      <abstract>The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice. Lastly, we outline recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST.</abstract>
      <url hash="5159d9cd">2024.acl-long.789</url>
      <bibkey>gaido-etal-2024-speech</bibkey>
      <doi>10.18653/v1/2024.acl-long.789</doi>
    </paper>
    <paper id="790">
      <title>Speech vs. Transcript: Does It Matter for Human Annotators in Speech Summarization?</title>
      <author><first>Roshan</first><last>Sharma</last><affiliation>Google</affiliation></author>
      <author><first>Suwon</first><last>Shon</last><affiliation>ASAPP</affiliation></author>
      <author><first>Mark</first><last>Lindsey</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Hira</first><last>Dhamyal</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Bhiksha</first><last>Raj</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>14779-14797</pages>
      <abstract>Reference summaries for abstractive speech summarization require human annotation, which can be performed by listening to an audio recording or by reading textual transcripts of the recording. In this paper, we examine whether summaries based on annotators listening to the recordings differ from those based on annotators reading transcripts. Using existing intrinsic evaluation based on human evaluation, automatic metrics, LLM-based evaluation, and a retrieval-based reference-free method, we find that summaries are indeed different based on the source modality, and that speech-based summaries are more factually consistent and information-selective than transcript-based summaries. Transcript-based summaries are impacted by recognition errors in the source, and expert-written summaries are more informative and reliable. We make all the collected data and analysis code public to facilitate the reproduction of our work and advance research in this area.</abstract>
      <url hash="07b4ce4e">2024.acl-long.790</url>
      <bibkey>sharma-etal-2024-speech</bibkey>
      <doi>10.18653/v1/2024.acl-long.790</doi>
    </paper>
    <paper id="791">
      <title><fixed-case>D</fixed-case>2<fixed-case>LLM</fixed-case>: Decomposed and Distilled Large Language Models for Semantic Search</title>
      <author><first>Zihan</first><last>Liao</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Hang</first><last>Yu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Jianguo</first><last>Li</last><affiliation>Ant Group</affiliation></author>
      <author><first>Jun</first><last>Wang</last><affiliation>Columbia University, Columbia University</affiliation></author>
      <author><first>Wei</first><last>Zhang</last><affiliation>East China Normal University</affiliation></author>
      <pages>14798-14814</pages>
      <abstract>The key challenge in semantic search is to create models that are both accurate and efficient in pinpointing relevant sentences for queries. While BERT-style bi-encoders excel in efficiency with pre-computed embeddings, they often miss subtle nuances in search tasks. Conversely, GPT-style LLMs with cross-encoder designs capture these nuances but are computationally intensive, hindering real-time applications. In this paper, we present D2LLMs—Decomposed and Distilled LLMs for semantic search—that combines the best of both worlds. We decompose a cross-encoder into an efficient bi-encoder integrated with Pooling by Multihead Attention and an Interaction Emulation Module, achieving nuanced understanding and pre-computability. Knowledge from the LLM is distilled into this model using contrastive, rank, and feature imitation techniques. Our experiments show that D2LLM surpasses five leading baselines in terms of all metrics across three tasks, particularly improving NLI task performance by at least 6.45%</abstract>
      <url hash="cb31f721">2024.acl-long.791</url>
      <bibkey>liao-etal-2024-d2llm</bibkey>
      <doi>10.18653/v1/2024.acl-long.791</doi>
    </paper>
    <paper id="792">
      <title><fixed-case>A</fixed-case>rabic Diacritics in the Wild: Exploiting Opportunities for Improved Diacritization</title>
      <author><first>Salman</first><last>Elgamal</last><affiliation>New York University, Abu Dhabi</affiliation></author>
      <author><first>Ossama</first><last>Obeid</last><affiliation>New York University</affiliation></author>
      <author><first>Mhd</first><last>Kabbani</last><affiliation>New York University, Abu Dhabi</affiliation></author>
      <author><first>Go</first><last>Inoue</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <pages>14815-14829</pages>
      <abstract>The widespread absence of diacritical marks in Arabic text poses a significant challenge for Arabic natural language processing (NLP). This paper explores instances of naturally occurring diacritics, referred to as “diacritics in the wild,” to unveil patterns and latent information across six diverse genres: news articles, novels, children’s books, poetry, political documents, and ChatGPT outputs. We present a new annotated dataset that maps real-world partially diacritized words to their maximal full diacritization in context. Additionally, we propose extensions to the analyze-and-disambiguate approach in Arabic NLP to leverage these diacritics, resulting in notable improvements. Our contributions encompass a thorough analysis, valuable datasets, and an extended diacritization algorithm. We release our code and datasets as open source.</abstract>
      <url hash="0b397bd2">2024.acl-long.792</url>
      <bibkey>elgamal-etal-2024-arabic</bibkey>
      <doi>10.18653/v1/2024.acl-long.792</doi>
    </paper>
    <paper id="793">
      <title>Disinformation Capabilities of Large Language Models</title>
      <author><first>Ivan</first><last>Vykopal</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Matúš</first><last>Pikuliak</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Ivan</first><last>Srba</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Robert</first><last>Moro</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Dominik</first><last>Macko</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Maria</first><last>Bielikova</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <pages>14830-14847</pages>
      <abstract>Automated disinformation generation is often listed as one of the risks of large language models (LLMs). The theoretical ability to flood the information space with disinformation content might have dramatic consequences for democratic societies around the world. This paper presents a comprehensive study of the disinformation capabilities of the current generation of LLMs to generate false news articles in English language. In our study, we evaluated the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated several aspects of the LLMs: how well they are at generating news articles, how strongly they tend to agree or disagree with the disinformation narratives, how often they generate safety warnings, etc. We also evaluated the abilities of detection models to detect these articles as LLM-generated. We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation narratives.</abstract>
      <url hash="7094caba">2024.acl-long.793</url>
      <bibkey>vykopal-etal-2024-disinformation</bibkey>
      <doi>10.18653/v1/2024.acl-long.793</doi>
    </paper>
    <paper id="794">
      <title>Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models</title>
      <author><first>Junhao</first><last>Zheng</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Shengjie</first><last>Qiu</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Qianli</first><last>Ma</last><affiliation>South China University of Technology</affiliation></author>
      <pages>14848-14877</pages>
      <abstract>Incremental Learning (IL) has been a long-standing problem in both vision and Natural Language Processing (NLP) communities.In recent years, as Pre-trained Language Models (PLMs) have achieved remarkable progress in various NLP downstream tasks, utilizing PLMs as backbones has become a common practice in recent research of IL in NLP.Most assume that catastrophic forgetting is the biggest obstacle to achieving superior IL performance and propose various techniques to overcome this issue.However, we find that this assumption is problematic.Specifically, we revisit more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) and reveal that most of them severely underestimate the inherent anti-forgetting ability of PLMs.Based on the observation, we propose a frustratingly easy method called SEQ* for IL with PLMs.The results show that SEQ* has competitive or superior performance compared with state-of-the-art (SOTA) IL methods yet requires considerably less trainable parameters and training time.These findings urge us to revisit the IL with PLMs and encourage future studies to have a fundamental understanding of the catastrophic forgetting in PLMs.</abstract>
      <url hash="221a2b6f">2024.acl-long.794</url>
      <bibkey>zheng-etal-2024-learn</bibkey>
      <doi>10.18653/v1/2024.acl-long.794</doi>
    </paper>
    <paper id="795">
      <title>How to Handle Different Types of Out-of-Distribution Scenarios in Computational Argumentation? A Comprehensive and Fine-Grained Field Study</title>
      <author><first>Andreas</first><last>Waldis</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Yufang</first><last>Hou</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>14878-14898</pages>
      <abstract>The advent of pre-trained Language Models (LMs) has markedly advanced natural language processing, but their efficacy in out-of-distribution (OOD) scenarios remains a significant challenge. Computational argumentation (CA), modeling human argumentation processes, is a field notably impacted by these challenges because complex annotation schemes and high annotation costs naturally lead to resources barely covering the multiplicity of available text sources and topics. Due to this data scarcity, generalization to data from uncovered covariant distributions is a common challenge for CA tasks like stance detection or argument classification. This work systematically assesses LMs’ capabilities for such OOD scenarios. While previous work targets specific OOD types like topic shifts or OOD uniformly, we address three prevalent OOD scenarios in CA: topic shift, domain shift, and language shift. Our findings challenge the previously asserted general superiority of in-context learning (ICL) for OOD. We find that the efficacy of such learning paradigms varies with the type of OOD. Specifically, while ICL excels for domain shifts, prompt-based fine-tuning surpasses for topic shifts. To sum up, we navigate the heterogeneity of OOD scenarios in CA and empirically underscore the potential of base-sized LMs in overcoming these challenges.</abstract>
      <url hash="6132b33f">2024.acl-long.795</url>
      <bibkey>waldis-etal-2024-handle</bibkey>
      <doi>10.18653/v1/2024.acl-long.795</doi>
    </paper>
    <paper id="796">
      <title>Cendol: Open Instruction-tuned Generative Large Language Models for <fixed-case>I</fixed-case>ndonesian Languages</title>
      <author><first>Samuel</first><last>Cahyawijaya</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Holy</first><last>Lovenia</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Rifki</first><last>Putri</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Wawan</first><last>Cenggoro</last><affiliation>Binus University</affiliation></author>
      <author><first>Jhonson</first><last>Lee</last><affiliation>Tokopedia</affiliation></author>
      <author><first>Salsabil</first><last>Akbar</last><affiliation>Institut Teknologi Bandung</affiliation></author>
      <author><first>Emmanuel</first><last>Dave</last><affiliation>Binus University</affiliation></author>
      <author><first>Nuurshadieq</first><last>Nuurshadieq</last><affiliation>Universitas Telkom</affiliation></author>
      <author><first>Muhammad</first><last>Mahendra</last><affiliation>Universitas Telkom</affiliation></author>
      <author><first>Rr</first><last>Putri</last><affiliation>Universitas Indonesia</affiliation></author>
      <author><first>Bryan</first><last>Wilie</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Genta</first><last>Winata</last><affiliation>Capital One AI Foundations</affiliation></author>
      <author><first>Alham</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ayu</first><last>Purwarianti</last><affiliation>Institut Teknologi Bandung</affiliation></author>
      <author><first>Pascale</first><last>Fung</last><affiliation>HKUST</affiliation></author>
      <pages>14899-14914</pages>
      <abstract>Large language models (LLMs) show remarkable human-like capability in various domains and languages. To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes. We highlight Cendol’s effectiveness across a diverse array of tasks, attaining ~20% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia. Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia. In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation. Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning.</abstract>
      <url hash="82eb89df">2024.acl-long.796</url>
      <bibkey>cahyawijaya-etal-2024-cendol</bibkey>
      <doi>10.18653/v1/2024.acl-long.796</doi>
    </paper>
    <paper id="797">
      <title>Must <fixed-case>NLP</fixed-case> be Extractive?</title>
      <author><first>Steven</first><last>Bird</last><affiliation>Charles Darwin University</affiliation></author>
      <pages>14915-14929</pages>
      <abstract>How do we roll out language technologies across a world with 7,000 languages? In one story, we scale the successes of NLP further into ‘low-resource’ languages, doing ever more with less. However, this approach does not recognise the fact that, beyond the 500 institutional languages, the remaining languages are oral vernaculars spoken by communities who use a language of wider communication to interact with the outside world. I argue that such ‘contact languages’ are the appropriate target for technologies like machine translation, and that the 6,500 oral languages must be approached differently. I share a story from an Indigenous community, where local people reshaped an extractive agenda to align with their relational agenda. I describe the emerging paradigm of relational NLP and explain how it opens the way to non-extractive methods and to solutions that enhance human agency.</abstract>
      <url hash="d620024e">2024.acl-long.797</url>
      <bibkey>bird-2024-must</bibkey>
      <doi>10.18653/v1/2024.acl-long.797</doi>
    </paper>
    <paper id="798">
      <title>Spiral of Silence: How is Large Language Model Killing Information Retrieval?—<fixed-case>A</fixed-case> Case Study on Open Domain Question Answering</title>
      <author><first>Xiaoyang</first><last>Chen</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Ben</first><last>He</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Tianshu</first><last>Wang</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Boxi</first><last>Cao</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yingfei</first><last>Sun</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <pages>14930-14951</pages>
      <abstract>The practice of Retrieval-Augmented Generation (RAG), which integrates Large Language Models (LLMs) with retrieval systems, has become increasingly prevalent. However, the repercussions of LLM-derived content infiltrating the web and influencing the retrieval-generation feedback loop are largely uncharted territories. In this study, we construct and iteratively run a simulation pipeline to deeply investigate the short-term and long-term effects of LLM text on RAG systems. Taking the trending Open Domain Question Answering (ODQA) task as a point of entry, our findings reveal a potential digital “Spiral of Silence” effect, with LLM-generated text consistently outperforming human-authored content in search rankings, thereby diminishing the presence and impact of human contributions online. This trend risks creating an imbalanced information ecosystem, where the unchecked proliferation of erroneous LLM-generated content may result in the marginalization of accurate information. We urge the academic community to take heed of this potential issue, ensuring a diverse and authentic digital information landscape.</abstract>
      <url hash="b2af5e27">2024.acl-long.798</url>
      <bibkey>chen-etal-2024-spiral</bibkey>
      <doi>10.18653/v1/2024.acl-long.798</doi>
    </paper>
    <paper id="799">
      <title>Latxa: An Open Language Model and Evaluation Suite for <fixed-case>B</fixed-case>asque</title>
      <author><first>Julen</first><last>Etxaniz</last><affiliation>HiTZ Center, University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Oscar</first><last>Sainz</last><affiliation>University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Naiara</first><last>Miguel</last><affiliation>University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Itziar</first><last>Aldabe</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>German</first><last>Rigau</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Eneko</first><last>Agirre</last><affiliation>University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Aitor</first><last>Ormazabal</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Mikel</first><last>Artetxe</last><affiliation>Reka AI</affiliation></author>
      <author><first>Aitor</first><last>Soroa</last><affiliation>University of the Basque Country. UPV/EHU.</affiliation></author>
      <pages>14952-14972</pages>
      <abstract>We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,046 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses. Our suite enables reproducible research on methods to build LLMs for low-resource languages.</abstract>
      <url hash="879cf118">2024.acl-long.799</url>
      <bibkey>etxaniz-etal-2024-latxa</bibkey>
      <doi>10.18653/v1/2024.acl-long.799</doi>
    </paper>
    <paper id="800">
      <title>Why are Sensitive Functions Hard for Transformers?</title>
      <author><first>Michael</first><last>Hahn</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Mark</first><last>Rofin</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>14973-15008</pages>
      <abstract>Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalization for PARITY. This shows that understanding transformers’ inductive biases requires studying not just their in-principle expressivity, but also their loss landscape.</abstract>
      <url hash="a461b288">2024.acl-long.800</url>
      <bibkey>hahn-rofin-2024-sensitive</bibkey>
      <doi>10.18653/v1/2024.acl-long.800</doi>
    </paper>
    <paper id="801">
      <title>Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction</title>
      <author><first>Haoqiu</first><last>Yan</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yongxin</first><last>Zhu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Kai</first><last>Zheng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Bing</first><last>Liu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Haoyu</first><last>Cao</last><affiliation>Tencent Youtu Lab</affiliation></author>
      <author><first>Deqiang</first><last>Jiang</last><affiliation>Tencent YouTu Lab</affiliation></author>
      <author><first>Linli</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>15009-15022</pages>
      <abstract>Large Language Model (LLM)-enhanced agents become increasingly prevalent in Human-AI communication, offering vast potential from entertainment to professional domains. However, current multi-modal dialogue systems overlook the acoustic information present in speech, which is crucial for understanding human communication nuances. This oversight can lead to misinterpretations of speakers’ intentions, resulting in inconsistent or even contradictory responses within dialogues. To bridge this gap, in this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system designed to discern deeper or more subtle meanings beyond the literal interpretations of words through the integration of speech modality perception. Employing LLMs as a cognitive core, PerceptiveAgent perceives acoustic information from input speech and generates empathetic responses based on speaking styles described in natural language. Experimental results indicate that PerceptiveAgent excels in contextual understanding by accurately discerning the speakers’ true intentions in scenarios where the linguistic meaning is either contrary to or inconsistent with the speaker’s true feelings, producing more nuanced and expressive spoken dialogues. Code is publicly available at: https://github.com/Haoqiu-Yan/PerceptiveAgent.</abstract>
      <url hash="fdf3cfdb">2024.acl-long.801</url>
      <bibkey>yan-etal-2024-talk</bibkey>
      <doi>10.18653/v1/2024.acl-long.801</doi>
    </paper>
    <paper id="802">
      <title><fixed-case>IRC</fixed-case>oder: Intermediate Representations Make Language Models Robust Multilingual Code Generators</title>
      <author><first>Indraneil</first><last>Paul</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>15023-15041</pages>
      <abstract>Code generation has fast become one of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs, such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside the exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations (IR)—shared across programming languages—to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer. To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled with their respective intermediate representations. Next, starting from various base Code-LMs (ranging from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages. Our resulting models, dubbed IRCoder, display sizeable and consistent gains across various code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.</abstract>
      <url hash="07f90cb7">2024.acl-long.802</url>
      <bibkey>paul-etal-2024-ircoder</bibkey>
      <doi>10.18653/v1/2024.acl-long.802</doi>
    </paper>
    <paper id="803">
      <title>The Echoes of Multilinguality: Tracing Cultural Value Shifts during Language Model Fine-tuning</title>
      <author><first>Rochelle</first><last>Choenni</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>15042-15058</pages>
      <abstract>Texts written in different languages reflect different culturally-dependent beliefs of their writers. Thus, we expect multilingual LMs (MLMs), that are jointly trained on a concatenation of text in multiple languages, to encode different cultural values for each language. Yet, as the ‘multilinguality’ of these LMs is driven by cross-lingual sharing, we also have reason to belief that cultural values bleed over from one language into another. This limits the use of MLMs in practice, as apart from being proficient in generating text in multiple languages, creating language technology that can serve a community also requires the output of LMs to be sensitive to their biases (Naous et al. 2023). Yet, little is known about how cultural values emerge and evolve in MLMs (Hershcovich et al. 2022). We are the first to study how languages can exert influence on the cultural values encoded for different test languages, by studying how such values are revised during fine-tuning. Focusing on the fine-tuning stage allows us to study the interplay between value shifts when exposed to new linguistic experience from different data sources and languages. Lastly, we use a training data attribution method to find patterns in the fine-tuning examples, and the languages that they come from, that tend to instigate value shifts.</abstract>
      <url hash="1ad86843">2024.acl-long.803</url>
      <bibkey>choenni-etal-2024-echoes</bibkey>
      <doi>10.18653/v1/2024.acl-long.803</doi>
    </paper>
    <paper id="804">
      <title><fixed-case>MYTE</fixed-case>: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling</title>
      <author><first>Tomasz</first><last>Limisiewicz</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>Terra</first><last>Blevins</last><affiliation>University of Washington</affiliation></author>
      <author><first>Hila</first><last>Gonen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Orevaoghene</first><last>Ahia</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Luke</first><last>Zettlemoyer</last><affiliation>University of Washington</affiliation></author>
      <pages>15059-15076</pages>
      <abstract>A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts.Although contemporary text encoding methods cover most of the world’s writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and diminishes the perplexity gap throughout diverse languages.</abstract>
      <url hash="e1ae23a1">2024.acl-long.804</url>
      <bibkey>limisiewicz-etal-2024-myte</bibkey>
      <doi>10.18653/v1/2024.acl-long.804</doi>
    </paper>
    <paper id="805">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>L</fixed-case>egal<fixed-case>P</fixed-case>ile: A 689<fixed-case>GB</fixed-case> Multilingual Legal Corpus</title>
      <author><first>Joel</first><last>Niklaus</last><affiliation>University of Bern, Universität Bern</affiliation></author>
      <author><first>Veton</first><last>Matoshi</last><affiliation>BFH - Bern University of Applied Sciences</affiliation></author>
      <author><first>Matthias</first><last>Stürmer</last><affiliation>BFH - Bern University of Applied Sciences</affiliation></author>
      <author><first>Ilias</first><last>Chalkidis</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Daniel</first><last>Ho</last><affiliation>Stanford University</affiliation></author>
      <pages>15077-15094</pages>
      <abstract>Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, few datasets are available for specialized critical domains such as law and the available ones are often small and only in English. To fill this gap, we curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. MultiLegalPile includes diverse legal data sources and allows for pretraining NLP models under fair use, with most of the dataset licensed very permissively. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, trained models, and all code under the most open licenses possible.</abstract>
      <url hash="5f8fff77">2024.acl-long.805</url>
      <bibkey>niklaus-etal-2024-multilegalpile</bibkey>
      <doi>10.18653/v1/2024.acl-long.805</doi>
    </paper>
    <paper id="806">
      <title><fixed-case>W</fixed-case>eb<fixed-case>C</fixed-case>ite<fixed-case>S</fixed-case>: Attributed Query-Focused Summarization on <fixed-case>C</fixed-case>hinese Web Search Results with Citations</title>
      <author><first>Haolin</first><last>Deng</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Chang</first><last>Wang</last><affiliation>Tencent Inc.</affiliation></author>
      <author><first>Li</first><last>Xin</last><affiliation>Tencent QB search</affiliation></author>
      <author><first>Dezhang</first><last>Yuan</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Junlang</first><last>Zhan</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Tian</first><last>Zhou</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Jin</first><last>Ma</last><affiliation>Tencent PCG</affiliation></author>
      <author><first>Jun</first><last>Gao</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>15095-15114</pages>
      <abstract>Enhancing the attribution in large language models (LLMs) is a crucial task. One feasible approach is to enable LLMs to cite external sources that support their generations. However, existing datasets and evaluation methods in this domain still exhibit notable limitations. In this work, we formulate the task of attributed query-focused summarization (AQFS) and present WebCiteS, a Chinese dataset featuring 7k human-annotated summaries with citations. WebCiteS derives from real-world user queries and web search results, offering a valuable resource for model training and evaluation. Prior works in attribution evaluation do not differentiate between groundedness errors and citation errors. They also fall short in automatically verifying sentences that draw partial support from multiple sources. We tackle these issues by developing detailed metrics and enabling the automatic evaluator to decompose the sentences into sub-claims for fine-grained verification. Our comprehensive evaluation of both open-source and proprietary models on WebCiteS highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement. The dataset and code will be open-sourced to facilitate further research in this crucial field.</abstract>
      <url hash="21582154">2024.acl-long.806</url>
      <bibkey>deng-etal-2024-webcites</bibkey>
      <doi>10.18653/v1/2024.acl-long.806</doi>
    </paper>
    <paper id="807">
      <title>What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages</title>
      <author><first>Nadav</first><last>Borenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Anej</first><last>Svete</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Robin</first><last>Chan</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Josef</first><last>Valvoda</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Franz</first><last>Nowak</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Eleanor</first><last>Chodroff</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>15115-15134</pages>
      <abstract>What can large language models learn? By definition, language models (LM) are distributionsover strings. Therefore, an intuitive way of addressing the above question is to formalize it as a matter of learnability of classes of distributions over strings. While prior work in this direction focused on assessing the theoretical limits, in contrast, we seek to understand the empirical learnability. Unlike prior empirical work, we evaluate neural LMs on their home turf—learning probabilistic languages—rather than as classifiers of formal languages. In particular, we investigate the learnability of regular LMs (RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs as a function of various complexity parameters of the RLM and the hidden state size of the neural LM. We find that the RLM rank, which corresponds to the size of linear space spanned by the logits of its conditional distributions, and the expected length of sampled strings are strong and significant predictors of learnability for both RNNs and Transformers. Several other predictors also reach significance, but with differing patterns between RNNs and Transformers.</abstract>
      <url hash="2fbdc5ca">2024.acl-long.807</url>
      <bibkey>borenstein-etal-2024-languages</bibkey>
      <doi>10.18653/v1/2024.acl-long.807</doi>
    </paper>
    <paper id="808">
      <title>Tree-Averaging Algorithms for Ensemble-Based Unsupervised Discontinuous Constituency Parsing</title>
      <author><first>Behzad</first><last>Shayegh</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Yuqiao</first><last>Wen</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Lili</first><last>Mou</last><affiliation>University of Alberta</affiliation></author>
      <pages>15135-15156</pages>
      <abstract>We address unsupervised discontinuous constituency parsing, where we observe a high variance in the performance of the only previous model in the literature. We propose to build an ensemble of different runs of the existing discontinuous parser by averaging the predicted trees, to stabilize and boost performance. To begin with, we provide comprehensive computational complexity analysis (in terms of P and NP-complete) for tree averaging under different setups of binarity and continuity. We then develop an efficient exact algorithm to tackle the task, which runs in a reasonable time for all samples in our experiments. Results on three datasets show our method outperforms all baselines in all metrics; we also provide in-depth analyses of our approach.</abstract>
      <url hash="f09b06d5">2024.acl-long.808</url>
      <bibkey>shayegh-etal-2024-tree</bibkey>
      <doi>10.18653/v1/2024.acl-long.808</doi>
    </paper>
    <paper id="809">
      <title><fixed-case>A</fixed-case>rt<fixed-case>P</fixed-case>rompt: <fixed-case>ASCII</fixed-case> Art-based Jailbreak Attacks against Aligned <fixed-case>LLM</fixed-case>s</title>
      <author><first>Fengqing</first><last>Jiang</last><affiliation>University of Washington</affiliation></author>
      <author><first>Zhangchen</first><last>Xu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Luyao</first><last>Niu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Zhen</first><last>Xiang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Bhaskar</first><last>Ramasubramanian</last><affiliation>Western Washington University</affiliation></author>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Radha</first><last>Poovendran</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>15157-15173</pages>
      <abstract>Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we develop the jailbreak attack ArtPrompt, which leverages the poor performance of LLMs in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from LLMs. ArtPrompt only requires black-box access to the victim LLMs, making it a practical attack. We evaluate ArtPrompt on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five LLMs.</abstract>
      <url hash="77cb8504">2024.acl-long.809</url>
      <bibkey>jiang-etal-2024-artprompt</bibkey>
      <doi>10.18653/v1/2024.acl-long.809</doi>
    </paper>
    <paper id="810">
      <title><fixed-case>C</fixed-case>hat<fixed-case>D</fixed-case>ev: Communicative Agents for Software Development</title>
      <author><first>Chen</first><last>Qian</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Wei</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Hongzhang</first><last>Liu</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Nuo</first><last>Chen</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yufan</first><last>Dang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Jiahao</first><last>Li</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Cheng</first><last>Yang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Weize</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yusheng</first><last>Su</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xin</first><last>Cong</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juyuan</first><last>Xu</last><affiliation>Brown University</affiliation></author>
      <author><first>Dahai</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>15174-15186</pages>
      <abstract>Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev.</abstract>
      <url hash="5388e3da">2024.acl-long.810</url>
      <bibkey>qian-etal-2024-chatdev</bibkey>
      <doi>10.18653/v1/2024.acl-long.810</doi>
    </paper>
    <paper id="811">
      <title>Disentangled Learning with Synthetic Parallel Data for Text Style Transfer</title>
      <author><first>Jingxuan</first><last>Han</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Quan</first><last>Wang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zikang</first><last>Guo</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Benfeng</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Licheng</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>15187-15201</pages>
      <abstract>Text style transfer (TST) is an important task in natural language generation, which aims to transfer the text style (e.g., sentiment) while keeping its semantic information. Due to the absence of parallel datasets for supervision, most existing studies have been conducted in an unsupervised manner, where the generated sentences often suffer from high semantic divergence and thus low semantic preservation. In this paper, we propose a novel disentanglement-based framework for TST named DisenTrans, where disentanglement means that we separate the attribute and content components in the natural language corpus and consider this task from these two perspectives. Concretely, we first create a disentangled Chain-of-Thought prompting procedure to synthesize parallel data and corresponding attribute components for supervision. Then we develop a disentanglement learning method with synthetic data, where two losses are designed to enhance the focus on attribute properties and constrain the semantic space, thereby benefiting style control and semantic preservation respectively. Instructed by the disentanglement concept, our framework creates valuable supervised information and utilizes it effectively in TST tasks. Extensive experiments on mainstream datasets present that our framework achieves significant performance with great sample efficiency.</abstract>
      <url hash="35810a16">2024.acl-long.811</url>
      <bibkey>han-etal-2024-disentangled</bibkey>
      <doi>10.18653/v1/2024.acl-long.811</doi>
    </paper>
    <paper id="812">
      <title><fixed-case>P</fixed-case>sy<fixed-case>S</fixed-case>afe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety</title>
      <author><first>Zaibin</first><last>Zhang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Yongting</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Lijun</first><last>Li</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jing</first><last>Shao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Hongzhi</first><last>Gao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>Shanghai Aritifcal Intelligence Laboratory</affiliation></author>
      <author><first>Lijun</first><last>Wang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Huchuan</first><last>Lu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Feng</first><last>Zhao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>15202-15231</pages>
      <abstract>Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety.To tackle these concerns, we propose a comprehensive framework (PsySafe) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks.Our experiments reveal several intriguing phenomena, such as the collective dangerous behaviors among agents, agents’ self-reflection when engaging in dangerous behavior, and the correlation between agents’ psychological assessments and dangerous behaviors. We anticipate that our framework and observations will provide valuable insights for further research into the safety of multi-agent systems. We make our data and code publicly accessible at https://github.com/AI4Good24/PsySafe.</abstract>
      <url hash="c5da5232">2024.acl-long.812</url>
      <bibkey>zhang-etal-2024-psysafe</bibkey>
      <doi>10.18653/v1/2024.acl-long.812</doi>
    </paper>
    <paper id="813">
      <title>Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation</title>
      <author><first>Dongjin</first><last>Kang</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Sunghwan</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Taeyoon</first><last>Kwon</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Seungjun</first><last>Moon</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Hyunsouk</first><last>Cho</last><affiliation>Ajou University</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <pages>15232-15261</pages>
      <abstract>Emotional Support Conversation (ESC) is a task aimed at alleviating individuals’ emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) existing LLMs alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs.</abstract>
      <url hash="5a117051">2024.acl-long.813</url>
      <bibkey>kang-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.acl-long.813</doi>
    </paper>
    <paper id="814">
      <title><tex-math>\infty</tex-math><fixed-case>B</fixed-case>ench: Extending Long Context Evaluation Beyond 100<fixed-case>K</fixed-case> Tokens</title>
      <author><first>Xinrong</first><last>Zhang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yingfa</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Shengding</first><last>Hu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zihang</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Junhao</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Moo</first><last>Hao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhen</first><last>Thai</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Shuo</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>15262-15277</pages>
      <abstract>Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose , the first LLM benchmark featuring an average data length surpassing 100K tokens. comprises synthetic and realistic tasks spanning diverse domains in English and Chinese. The tasks in are designed to require an understanding of long dependencies in contexts and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. Based on , we evaluate several state-of-the-art LLMs tailored for processing long contexts. The experimental results indicate that existing long-context LLMs still require significant advancements to process 100K+ contexts effectively. Furthermore, we present three intriguing analyses regarding the behavior of LLMs processing long context. Our code and data is released.</abstract>
      <url hash="035ca15f">2024.acl-long.814</url>
      <bibkey>zhang-etal-2024-bench</bibkey>
      <doi>10.18653/v1/2024.acl-long.814</doi>
    </paper>
    <paper id="815">
      <title>Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models</title>
      <author><first>Tharindu</first><last>Madusanka</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Ian</first><last>Pratt-Hartmann</last><affiliation>University of Opole</affiliation></author>
      <author><first>Riza</first><last>Batista-Navarro</last><affiliation>University of Manchester</affiliation></author>
      <pages>15278-15294</pages>
      <abstract>Efforts to apply transformer-based language models (TLMs) to the problem of reasoning in natural language have enjoyed ever-increasing success in recent years. The most fundamental task in this area to which nearly all others can be reduced is that of determining satisfiability. However, from a logical point of view, satisfiability problems vary along various dimensions, which may affect TLMs’ ability to learn how to solve them. The problem instances of satisfiability in natural language can belong to different computational complexity classes depending on the language fragment in which they are expressed. Although prior research has explored the problem of natural language satisfiability, the above-mentioned point has not been discussed adequately. Hence, we investigate how problem instances from varying computational complexity classes and having different grammatical constructs impact TLMs’ ability to learn rules of inference. Furthermore, to faithfully evaluate TLMs, we conduct an empirical study to explore the distribution of satisfiability problems.</abstract>
      <url hash="4322fb76">2024.acl-long.815</url>
      <bibkey>madusanka-etal-2024-natural</bibkey>
      <doi>10.18653/v1/2024.acl-long.815</doi>
    </paper>
    <paper id="816">
      <title>Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models</title>
      <author><first>Paul</first><last>Röttger</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Valentin</first><last>Hofmann</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Valentina</first><last>Pyatkin</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Musashi</first><last>Hinck</last><affiliation>Intel</affiliation></author>
      <author><first>Hannah</first><last>Kirk</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>15295-15311</pages>
      <abstract>Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing *constrained* evaluation paradigm for values and opinions in LLMs and explore more realistic *unconstrained* evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT *forces models to comply with the PCT’s multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.</abstract>
      <url hash="545e06ab">2024.acl-long.816</url>
      <bibkey>rottger-etal-2024-political</bibkey>
      <doi>10.18653/v1/2024.acl-long.816</doi>
    </paper>
    <paper id="817">
      <title><fixed-case>AI</fixed-case> ‘News’ Content Farms Are Easy to Make and Hard to Detect: A Case Study in <fixed-case>I</fixed-case>talian</title>
      <author><first>Giovanni</first><last>Puccetti</last><affiliation>CNR</affiliation></author>
      <author><first>Anna</first><last>Rogers</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Chiara</first><last>Alzetta</last><affiliation>Istituto di Linguistica Computazionale “A.Zampolli”, CNR</affiliation></author>
      <author><first>Felice</first><last>Dell’Orletta</last><affiliation>Istituto di Linguistica Computazionale “A. Zampolli” (ILC)</affiliation></author>
      <author><first>Andrea</first><last>Esuli</last><affiliation>CNR</affiliation></author>
      <pages>15312-15338</pages>
      <abstract>Large Language Models (LLMs) are increasingly used as ‘content farm’ models (CFMs), to generate synthetic text that could pass for real news articles. This is already happening even for languages that do not have high-quality monolingual LLMs. We show that fine-tuning Llama (v1), mostly trained on English, on as little as 40K Italian news articles, is sufficient for producing news-like texts that native speakers of Italian struggle to identify as synthetic.We investigate three LLMs and three methods of detecting synthetic texts (log-likelihood, DetectGPT, and supervised classification), finding that they all perform better than human raters, but they are all impractical in the real world (requiring either access to token likelihood information or a large dataset of CFM texts). We also explore the possibility of creating a proxy CFM: an LLM fine-tuned on a similar dataset to one used by the real ‘content farm’. We find that even a small amount of fine-tuning data suffices for creating a successful detector, but we need to know which base LLM is used, which is a major challenge.Our results suggest that there are currently no practical methods for detecting synthetic news-like texts ‘in the wild’, while generating them is too easy. We highlight the urgency of more NLP research on this problem.</abstract>
      <url hash="c9390b27">2024.acl-long.817</url>
      <bibkey>puccetti-etal-2024-ai</bibkey>
      <doi>10.18653/v1/2024.acl-long.817</doi>
    </paper>
    <paper id="818">
      <title>Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models</title>
      <author><first>Mosh</first><last>Levy</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Alon</first><last>Jacoby</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Yoav</first><last>Goldberg</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>15339-15353</pages>
      <abstract>This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs’ reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities.Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs’ on our reasoning dataset. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.</abstract>
      <url hash="c7648846">2024.acl-long.818</url>
      <bibkey>levy-etal-2024-task</bibkey>
      <doi>10.18653/v1/2024.acl-long.818</doi>
    </paper>
    <paper id="819">
      <title>Disambiguate Words like Composing Them: A Morphology-Informed Approach to Enhance <fixed-case>C</fixed-case>hinese Word Sense Disambiguation</title>
      <author><first>Yue</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Qiliang</first><last>Liang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yaqi</first><last>Yin</last><affiliation>Peking University</affiliation></author>
      <author><first>Hansi</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <pages>15354-15365</pages>
      <abstract>In parataxis languages like Chinese, word meanings are highly correlated with morphological knowledge, which can help to disambiguate word senses. However, in-depth exploration of morphological knowledge in previous word sense disambiguation (WSD) methods is still lacking due to the absence of publicly available resources. In this paper, we are motivated to enhance Chinese WSD with full morphological knowledge, including both word-formations and morphemes. We first construct the largest and releasable Chinese WSD resources, including the lexico-semantic inventories MorInv and WrdInv, a Chinese WSD dataset MiCLS, and an out-of-volcabulary (OOV) test set. Then, we propose a model, MorBERT, to fully leverage this morphology-informed knowledge for Chinese WSD and achieve a SOTA F1 of 92.18% in the task. Finally, we demonstrated the model’s robustness in low-resource settings and generalizability to OOV senses. These resources and methods may bring new insights into and solutions for various downstream tasks in both computational and humanistic fields.</abstract>
      <url hash="7664f3fa">2024.acl-long.819</url>
      <bibkey>wang-etal-2024-disambiguate</bibkey>
      <doi>10.18653/v1/2024.acl-long.819</doi>
    </paper>
    <paper id="820">
      <title>Do Llamas Work in <fixed-case>E</fixed-case>nglish? On the Latent Language of Multilingual Transformers</title>
      <author><first>Chris</first><last>Wendler</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Veniamin</first><last>Veselovsky</last><affiliation>Princeton University</affiliation></author>
      <author><first>Giovanni</first><last>Monea</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Robert</first><last>West</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <pages>15366-15394</pages>
      <abstract>We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language—-a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study is based on carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already in middle layers allow for decoding a semantically correct next token, but giving higher probability to its version in English than in the input language; (3) move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in ”input space”, ”concept space”, and ”output space”, respectively. Crucially, our evidence suggests that the abstract ”concept space” lies closer to English than to other input languages, which may have important consequences regarding the biases embodied by multilingual language models.</abstract>
      <url hash="4acfb7e2">2024.acl-long.820</url>
      <bibkey>wendler-etal-2024-llamas</bibkey>
      <doi>10.18653/v1/2024.acl-long.820</doi>
    </paper>
    <paper id="821">
      <title><fixed-case>G</fixed-case>-<fixed-case>DIG</fixed-case>: Towards Gradient-based <fixed-case>DI</fixed-case>verse and hi<fixed-case>G</fixed-case>h-quality Instruction Data Selection for Machine Translation</title>
      <author><first>Xingyuan</first><last>Pan</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Luyang</first><last>Huang</last><affiliation>Bytedance AI Lab</affiliation></author>
      <author><first>Liyan</first><last>Kang</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Zhicheng</first><last>Liu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yu</first><last>Lu</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Shanbo</first><last>Cheng</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>15395-15406</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable abilities in general scenarios. Instruction finetuning empowers them to align with humans in various tasks. Nevertheless, the <i>Diversity</i> and <i>Quality</i> of the instruction data remain two main challenges for instruction finetuning. With regard to this, in this paper, we propose a novel gradient-based method to automatically select high-quality and diverse instruction finetuning data for machine translation. Our key innovation centers around analyzing how individual training examples influence the model during training. Specifically, we select training examples that exert beneficial influences on the model as high-quality ones by means of Influence Function plus a small high-quality seed dataset. Moreover, to enhance the diversity of the training data we maximize the variety of influences they have on the model by clustering on their gradients and resampling. Extensive experiments on WMT22 and FLORES translation tasks demonstrate the superiority of our methods, and in-depth analysis further validates their effectiveness and generalization.</abstract>
      <url hash="22a00126">2024.acl-long.821</url>
      <bibkey>pan-etal-2024-g</bibkey>
      <doi>10.18653/v1/2024.acl-long.821</doi>
    </paper>
    <paper id="822">
      <title>Media Framing: A typology and Survey of Computational Approaches Across Disciplines</title>
      <author><first>Yulia</first><last>Otmakhova</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Shima</first><last>Khanehzar</last><affiliation>CSIRO</affiliation></author>
      <author><first>Lea</first><last>Frermann</last><affiliation>University of Melbourne</affiliation></author>
      <pages>15407-15428</pages>
      <abstract>Framing studies how individuals and societies make sense of the world, by communicating or representing complex issues through schema of interpretation. The framing of information in the mass media influences our interpretation of facts and corresponding decisions, so detecting and analysing it is essential to understand biases in the information we consume. Despite that, framing is still mostly examined manually, on a case-by-case basis, while existing large-scale automatic analyses using NLP methods are not mature enough to solve this task. In this survey we show that despite the growing interest to framing in NLP its current approaches do not capture those aspects which allow to frame, rather than simply convey, the message. To this end, we bring together definitions of frames and framing adopted in different disciplines; examine cognitive, linguistic, and communicative aspects a frame contains beyond its topical content. We survey recent work on computational frame detection, and discuss how framing aspects and frame definitions are (or should) be reflected in NLP approaches.</abstract>
      <url hash="45a7d323">2024.acl-long.822</url>
      <bibkey>otmakhova-etal-2024-media</bibkey>
      <doi>10.18653/v1/2024.acl-long.822</doi>
    </paper>
    <paper id="823">
      <title><fixed-case>SPZ</fixed-case>: A Semantic Perturbation-based Data Augmentation Method with Zonal-Mixing for <fixed-case>A</fixed-case>lzheimer’s Disease Detection</title>
      <author><first>FangFang</first><last>Li</last><affiliation>Central South University</affiliation></author>
      <author><first>Cheng</first><last>Huang</last><affiliation>Central South University</affiliation></author>
      <author><first>PuZhen</first><last>Su</last><affiliation>Central South University</affiliation></author>
      <author><first>Jie</first><last>Yin</last><affiliation>The University of Sydney</affiliation></author>
      <pages>15429-15439</pages>
      <abstract>Alzheimer’s Disease (AD), characterized by significant cognitive and functional impairment, necessitates the development of early detection techniques. Traditional diagnostic practices, such as cognitive assessments and biomarker analysis, are often invasive and costly. Deep learning-based approaches for non-invasive AD detection have been explored in recent studies, but the lack of accessible data hinders further improvements in detection performance. To address these challenges, we propose a novel semantic perturbation-based data augmentation method that essentially differs from existing techniques, which primarily rely on explicit data engineering. Our approach generates controlled semantic perturbations to enhance textual representations, aiding the model in identifying AD-specific linguistic patterns, particularly in scenarios with limited data availability. It learns contextual information and dynamically adjusts the perturbation degree for different linguistic features. This enhances the model’s sensitivity to AD-specific linguistic features and its robustness against natural language noise. Experimental results on the ADReSS challenge dataset demonstrate that our approach outperforms other strong and competitive deep learning methods.</abstract>
      <url hash="756be8d4">2024.acl-long.823</url>
      <bibkey>li-etal-2024-spz</bibkey>
      <doi>10.18653/v1/2024.acl-long.823</doi>
    </paper>
    <paper id="824">
      <title>Calibrating Large Language Models Using Their Generations Only</title>
      <author><first>Dennis</first><last>Ulmer</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Martin</first><last>Gubri</last><affiliation>Parameter Lab</affiliation></author>
      <author><first>Hwaran</first><last>Lee</last><affiliation>NAVER AI Lab</affiliation></author>
      <author><first>Sangdoo</first><last>Yun</last><affiliation>NAVER</affiliation></author>
      <author><first>Seong</first><last>Oh</last><affiliation>Parameter Lab</affiliation></author>
      <pages>15440-15459</pages>
      <abstract>As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model’s confidence in its prediction becomes even more important. However, finding effective ways to calibrate LLMs—especially when the only interface to the models is their generated text—remains a challenge. We propose APRICOT (Auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM’s confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or using it to re-prompting the LLM to accurately reflecting its uncertainty. We show how our approach performs competitively in terms of calibration error for white-box and black-box LLMs on closed-book question-answering to detect incorrect LLM answers.</abstract>
      <url hash="a69a1df4">2024.acl-long.824</url>
      <bibkey>ulmer-etal-2024-calibrating</bibkey>
      <doi>10.18653/v1/2024.acl-long.824</doi>
    </paper>
    <paper id="825">
      <title>Iterative Forward Tuning Boosts In-Context Learning in Language Models</title>
      <author><first>Jiaxi</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Binyuan</first><last>Hui</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Bailin</first><last>Wang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Bowen</first><last>Li</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Binhua</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>15460-15473</pages>
      <abstract>Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.</abstract>
      <url hash="df3dca34">2024.acl-long.825</url>
      <bibkey>yang-etal-2024-iterative-forward</bibkey>
      <doi>10.18653/v1/2024.acl-long.825</doi>
    </paper>
    <paper id="826">
      <title>Pride and Prejudice: <fixed-case>LLM</fixed-case> Amplifies Self-Bias in Self-Refinement</title>
      <author><first>Wenda</first><last>Xu</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Guanglei</first><last>Zhu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Xuandong</first><last>Zhao</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>William</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>15474-15492</pages>
      <abstract>Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM’s bias in evaluating their own output. In this paper, we formally define LLM’s self-bias – the tendency to favor its own generation – using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks. The code and data are released at https://github.com/xu1998hz/llm_self_bias.</abstract>
      <url hash="a383b55b">2024.acl-long.826</url>
      <bibkey>xu-etal-2024-pride</bibkey>
      <doi>10.18653/v1/2024.acl-long.826</doi>
    </paper>
    <paper id="827">
      <title>Language Complexity and Speech Recognition Accuracy: Orthographic Complexity Hurts, Phonological Complexity Doesn’t</title>
      <author><first>Chihiro</first><last>Taguchi</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>David</first><last>Chiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>15493-15503</pages>
      <abstract>We investigate what linguistic factors affect the performance of Automatic Speech Recognition (ASR) models. We hypothesize that orthographic and phonological complexities both degrade accuracy. To examine this, we fine-tune the multilingual self-supervised pretrained model Wav2Vec2-XLSR-53 on 25 languages with 15 writing systems, and we compare their ASR accuracy, number of graphemes, unigram grapheme entropy, logographicity (how much word/morpheme-level information is encoded in the writing system), and number of phonemes. The results demonstrate that a high logographicity correlates with low ASR accuracy, while phonological complexity has no significant effect.</abstract>
      <url hash="3ddef82d">2024.acl-long.827</url>
      <bibkey>taguchi-chiang-2024-language</bibkey>
      <doi>10.18653/v1/2024.acl-long.827</doi>
    </paper>
    <paper id="828">
      <title>Steering Llama 2 via Contrastive Activation Addition</title>
      <author><first>Nina</first><last>Rimsky</last><affiliation>Anthropic</affiliation></author>
      <author><first>Nick</first><last>Gabrieli</last><affiliation>Harvard University</affiliation></author>
      <author><first>Julian</first><last>Schulz</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Meg</first><last>Tong</last><affiliation>Anthropic</affiliation></author>
      <author><first>Evan</first><last>Hubinger</last><affiliation>Machine Intelligence Research Institute</affiliation></author>
      <author><first>Alexander</first><last>Turner</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>15504-15522</pages>
      <abstract>We introduce Contrastive Activation Addition (CAA), a method for steering language models by modifying their activations during forward passes. CAA computes “steering vectors” by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user’s prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA’s effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA’s mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).</abstract>
      <url hash="27d4268d">2024.acl-long.828</url>
      <bibkey>rimsky-etal-2024-steering</bibkey>
      <doi>10.18653/v1/2024.acl-long.828</doi>
    </paper>
    <paper id="829">
      <title><fixed-case>E</fixed-case>con<fixed-case>A</fixed-case>gent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities</title>
      <author><first>Nian</first><last>Li</last><affiliation>Shenzhen International Graduate School, Tsinghua University</affiliation></author>
      <author><first>Chen</first><last>Gao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Mingyu</first><last>Li</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yong</first><last>Li</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Qingmin</first><last>Liao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>15523-15536</pages>
      <abstract>The advent of artificial intelligence has led to a growing emphasis on data-driven modeling in macroeconomics, with agent-based modeling (ABM) emerging as a prominent bottom-up simulation paradigm. In ABM, agents (*e.g.*, households, firms) interact within a macroeconomic environment, collectively generating market dynamics. Existing agent modeling typically employs predetermined rules or learning-based neural networks for decision-making. However, customizing each agent presents significant challenges, complicating the modeling of agent heterogeneity. Additionally, the influence of multi-period market dynamics and multifaceted macroeconomic factors are often overlooked in decision-making processes.In this work, we introduce **EconAgent**, a large language model-empowered agent with human-like characteristics for macroeconomic simulation. We first construct a simulation environment that incorporates various market dynamics driven by agents’ decisions regarding work and consumption. Through the perception module, we create heterogeneous agents with distinct decision-making mechanisms. Furthermore, we model the impact of macroeconomic trends using a memory module, which allows agents to reflect on past individual experiences and market dynamics.Simulation experiments show that EconAgent can make realistic decisions, leading to more reasonable macroeconomic phenomena compared to existing rule-based or learning-based agents. Our codes are released at https://github.com/tsinghua-fib-lab/ACL24-EconAgent.</abstract>
      <url hash="c996513d">2024.acl-long.829</url>
      <bibkey>li-etal-2024-econagent</bibkey>
      <doi>10.18653/v1/2024.acl-long.829</doi>
    </paper>
    <paper id="830">
      <title><fixed-case>S</fixed-case>afety<fixed-case>B</fixed-case>ench: Evaluating the Safety of Large Language Models</title>
      <author><first>Zhexin</first><last>Zhang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Leqi</first><last>Lei</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Lindong</first><last>Wu</last><affiliation>Northwest Minzu University</affiliation></author>
      <author><first>Rui</first><last>Sun</last><affiliation>Peking University</affiliation></author>
      <author><first>Yongkang</first><last>Huang</last><affiliation>Northwest Minzu University</affiliation></author>
      <author><first>Chong</first><last>Long</last><affiliation>China Mobile Research Institute</affiliation></author>
      <author><first>Xiao</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xuanyu</first><last>Lei</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>15537-15553</pages>
      <abstract>With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We also demonstrate that the measured safety understanding abilities in SafetyBench are correlated with safety generation abilities. Data and evaluation guidelines are available at https://github.com/thu-coai/SafetyBench. Submission entrance and leaderboard are available at https://llmbench.ai/safety.</abstract>
      <url hash="6ddf2139">2024.acl-long.830</url>
      <bibkey>zhang-etal-2024-safetybench</bibkey>
      <doi>10.18653/v1/2024.acl-long.830</doi>
    </paper>
    <paper id="831">
      <title>Deciphering Oracle Bone Language with Diffusion Models</title>
      <author><first>Haisu</first><last>Guan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Huanxin</first><last>Yang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Xinyu</first><last>Wang</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Shengwei</first><last>Han</last><affiliation>Anyang Normal University</affiliation></author>
      <author><first>Yongge</first><last>Liu</last><affiliation>Anyang Normal University</affiliation></author>
      <author><first>Lianwen</first><last>Jin</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Xiang</first><last>Bai</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Yuliang</first><last>Liu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>15554-15567</pages>
      <abstract>Originating from China’s Shang Dynasty approximately 3,000 years ago, the Oracle Bone Script (OBS) is a cornerstone in the annals of linguistic history, predating many established writing systems. Despite the discovery of thousands of inscriptions, a vast expanse of OBS remains undeciphered, casting a veil of mystery over this ancient language. The emergence of modern AI technologies presents a novel frontier for OBS decipherment, challenging traditional NLP methods that rely heavily on large textual corpora, a luxury not afforded by historical languages. This paper introduces a novel approach by adopting image generation techniques, specifically through the development of Oracle Bone Script Decipher (OBSD). Utilizing a conditional diffusion-based strategy, OBSD generates vital clues for decipherment, charting a new course for AI-assisted analysis of ancient languages. To validate its efficacy, extensive experiments were conducted on an oracle bone script dataset, with quantitative results demonstrating the effectiveness of OBSD.</abstract>
      <url hash="77c78647">2024.acl-long.831</url>
      <bibkey>guan-etal-2024-deciphering</bibkey>
      <doi>10.18653/v1/2024.acl-long.831</doi>
    </paper>
    <paper id="832">
      <title><fixed-case>M</fixed-case>4<fixed-case>LE</fixed-case>: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models</title>
      <author><first>Wai-Chung</first><last>Kwan</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xingshan</first><last>Zeng</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yufei</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yusen</first><last>Sun</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Liangyou</first><last>Li</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Yuxin</first><last>Jiang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>15568-15592</pages>
      <abstract>Managing long sequences has become an important and necessary feature for large language models (LLMs). However, assessing their ability to handle long contexts remains a challenge. This paper introduces M<tex-math>^4</tex-math>LE, a <tex-math>\textbf{M}</tex-math>ulti-ability, <tex-math>\textbf{M}</tex-math>ulti-range, <tex-math>\textbf{M}</tex-math>ulti-task, <tex-math>\textbf{M}</tex-math>ulti-domain benchmark for <tex-math>\textbf{L}</tex-math>ong-context <tex-math>\textbf{E}</tex-math>valuation. It encompasses 36 NLP datasets, covering 11 types of tasks and 12 domains, providing a comprehensive test bed. To address the lack of tasks featuring naturally long sequences, we propose an automatic approach to convert short-sequence tasks into long-sequence scenarios. These scenarios evaluate LLMs’ long-context understanding across five key abilities: understanding of single or multiple relevant spans in long contexts based on explicit or semantic hints, and global context understanding. This automatic approach allows us to create instances evenly distributed from 1k to 8k input length. Our evaluation of 11 prominent LLMs reveals that 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area.</abstract>
      <url hash="4a924d47">2024.acl-long.832</url>
      <bibkey>kwan-etal-2024-m4le</bibkey>
      <doi>10.18653/v1/2024.acl-long.832</doi>
    </paper>
    <paper id="833">
      <title><fixed-case>R</fixed-case>oman<fixed-case>S</fixed-case>etu: Efficiently unlocking multilingual capabilities of Large Language Models via <fixed-case>R</fixed-case>omanization</title>
      <author><first>Jaavid</first><last>J</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Aswanth</first><last>M</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Jay</first><last>Gala</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Thanmay</first><last>Jayakumar</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Ratish</first><last>Puduppully</last><affiliation>A*STAR</affiliation></author>
      <author><first>Anoop</first><last>Kunchukuttan</last><affiliation>Microsoft</affiliation></author>
      <pages>15593-15615</pages>
      <abstract>This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Roman scripts. We propose an approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Our approach involve the continual pretraining of a English LLM like Llama 2 on romanized text of non-English, non-Roman script languages, followed by instruction tuning on romanized data. The results indicate that romanized text not only reduces token fertility by 2x-4x but also matches if not outperforms native script representation across various NLU, NLG and MT tasks. Moreover, the embeddings computed on romanized text exhibit closer alignment with their English translations than those from the native script. Our approach presents a promising direction for leveraging the power of English LLMs in languages traditionally underrepresented in NLP research.</abstract>
      <url hash="46d066d5">2024.acl-long.833</url>
      <bibkey>j-etal-2024-romansetu</bibkey>
      <doi>10.18653/v1/2024.acl-long.833</doi>
    </paper>
    <paper id="834">
      <title>Causal Estimation of Memorisation Profiles</title>
      <author><first>Pietro</first><last>Lesci</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Clara</first><last>Meister</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Thomas</first><last>Hofmann</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Tiago</first><last>Pimentel</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <pages>15616-15635</pages>
      <abstract>Understanding memorisation in language models has practical and societal implications, e.g., studying models’ training dynamics or preventing copyright infringements.Prior work defines memorisation as the causal effect of training with an instance on the model’s ability to predict that instance. This definition relies on a counterfactual: the ability to observe what would have happened had the model not seen that instance.Existing methods struggle to provide computationally efficient and accurate estimates of this counterfactual. Further, they often estimate memorisation for a model architecture rather than for a specific model instance. This paper fills an important gap in the literature, proposing a new, principled, and efficient method to estimate memorisation based on the difference-in-differences design from econometrics. Using this method, we characterise a model’s memorisation profile–its memorisation trends across training–by only observing its behaviour on a small set of instances throughout training.In experiments with the Pythia model suite, we find that memorisation (i) is stronger and more persistent in larger models, (ii) is determined by data order and learning rate, and (iii) has stable trends across model sizes, thus making memorisation in larger models predictable from smaller ones.</abstract>
      <url hash="de809071">2024.acl-long.834</url>
      <bibkey>lesci-etal-2024-causal</bibkey>
      <doi>10.18653/v1/2024.acl-long.834</doi>
    </paper>
    <paper id="835">
      <title><fixed-case>CHECKWHY</fixed-case>: Causal Fact Verification via Argument Structure</title>
      <author><first>Jiasheng</first><last>Si</last><affiliation>Qilu University of Technology (Shandong Academy of Sciences)</affiliation></author>
      <author><first>Yibo</first><last>Zhao</last><affiliation>Southeast University</affiliation></author>
      <author><first>Yingjie</first><last>Zhu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Haiyang</first><last>Zhu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Wenpeng</first><last>Lu</last><affiliation>Qilu University of Technology</affiliation></author>
      <author><first>Deyu</first><last>Zhou</last><affiliation>Southeast University</affiliation></author>
      <pages>15636-15659</pages>
      <abstract>With the growing complexity of fact verification tasks, the concern with “thoughtful” reasoning capabilities is increasing. However, recent fact verification benchmarks mainly focus on checking a narrow scope of semantic factoids within claims and lack an explicit logical reasoning process. In this paper, we introduce CHECKWHY, a challenging dataset tailored to a novel causal fact verification task: checking the truthfulness of the causal relation within claims through rigorous reasoning steps. CHECKWHY consists of over 19K “why” claim-evidence- argument structure triplets with supports, refutes, and not enough info labels. Each argument structure is composed of connected evidence, representing the reasoning process that begins with foundational evidence and progresses toward claim establishment. Through extensive experiments on state-of-the-art models, we validate the importance of incorporating the argument structure for causal fact verification. Moreover, the automated and human evaluation of argument structure generation reveals the difficulty in producing satisfying argument structure by fine-tuned models or Chain-of-Thought prompted LLMs, leaving considerable room for future improvements.</abstract>
      <url hash="e49fd60c">2024.acl-long.835</url>
      <bibkey>si-etal-2024-checkwhy</bibkey>
      <doi>10.18653/v1/2024.acl-long.835</doi>
    </paper>
    <paper id="836">
      <title>Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model</title>
      <author><first>Christian</first><last>Tomani</last><affiliation>Technical University Munich</affiliation></author>
      <author><first>David</first><last>Vilar</last><affiliation>Google</affiliation></author>
      <author><first>Markus</first><last>Freitag</last><affiliation>Google</affiliation></author>
      <author><first>Colin</first><last>Cherry</last><affiliation>Google</affiliation></author>
      <author><first>Subhajit</first><last>Naskar</last><affiliation>Google</affiliation></author>
      <author><first>Mara</first><last>Finkelstein</last><affiliation>Google</affiliation></author>
      <author><first>Xavier</first><last>Garcia</last><affiliation>Google</affiliation></author>
      <author><first>Daniel</first><last>Cremers</last><affiliation>Technical University Munich</affiliation></author>
      <pages>15660-15679</pages>
      <abstract>Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy for neural machine translation (NMT) models. The underlying assumption is that model probability correlates well with human judgment, with better translations getting assigned a higher score by the model. However, research has shown that this assumption does not always hold, and generation quality can be improved by decoding to optimize a utility function backed by a metric or quality-estimation signal, as is done by Minimum Bayes Risk (MBR) or Quality-Aware decoding. The main disadvantage of these approaches is that they require an additional model to calculate the utility function during decoding, significantly increasing the computational cost. In this paper, we propose to make the NMT models themselves quality-aware by training them to estimate the quality of their own output. Using this approach for MBR decoding we can drastically reduce the size of the candidate list, resulting in a speed-up of two-orders of magnitude. When applying our method to MAP decoding we obtain quality gains similar or even superior to quality reranking approaches, but with the efficiency of single pass decoding.</abstract>
      <url hash="fbc8356f">2024.acl-long.836</url>
      <bibkey>tomani-etal-2024-quality</bibkey>
      <doi>10.18653/v1/2024.acl-long.836</doi>
    </paper>
    <paper id="837">
      <title>On Efficient and Statistical Quality Estimation for Data Annotation</title>
      <author><first>Jan-Christoph</first><last>Klie</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Juan</first><last>Haladjian</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Marc</first><last>Kirchner</last><affiliation>Apple</affiliation></author>
      <author><first>Rahul</first><last>Nair</last><affiliation>Heidelberg University, Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <pages>15680-15696</pages>
      <abstract>Annotated datasets are an essential ingredient to train, evaluate, compare and productionalize supervised machine learning models. It is therefore imperative that annotations are of high quality. For their creation, good quality management and thereby reliable quality estimates are needed. Then, if quality is insufficient during the annotation process, rectifying measures can be taken to improve it. Quality estimation is often performed by having experts manually label instances as correct or incorrect. But checking all annotated instances tends to be expensive. Therefore, in practice, usually only subsets are inspected; sizes are chosen mostly without justification or regard to statistical power and more often than not, are relatively small. Basing estimates on small sample sizes, however, can lead to imprecise values for the error rate. Using unnecessarily large sample sizes costs money that could be better spent, for instance on more annotations. Therefore, we first describe in detail how to use confidence intervals for finding the minimal sample size needed to estimate the annotation error rate. Then, we propose applying acceptance sampling as an alternative to error rate estimation We show that acceptance sampling can reduce the required sample sizes up to 50% while providing the same statistical guarantees.</abstract>
      <url hash="2f8b77e2">2024.acl-long.837</url>
      <bibkey>klie-etal-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.acl-long.837</doi>
    </paper>
    <paper id="838">
      <title><fixed-case>EZ</fixed-case>-<fixed-case>STANCE</fixed-case>: A Large Dataset for <fixed-case>E</fixed-case>nglish Zero-Shot Stance Detection</title>
      <author><first>Chenye</first><last>Zhao</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>15697-15714</pages>
      <abstract>Zero-shot stance detection (ZSSD) aims to determine whether the author of a text is in favor, against, or neutral toward a target that is unseen during training. In this paper, we present EZ-STANCE, a large English ZSSD dataset with 47,316 annotated text-target pairs. In contrast to VAST, which is the only other large existing ZSSD dataset for English, EZ-STANCE is 2.5 times larger, includes both noun-phrase targets and claim targets that cover a wide range of domains, provides two challenging subtasks for ZSSD: target-based ZSSD and domain-based ZSSD, and contains much harder examples for the neutral class. We evaluate EZ-STANCE using state-of-the-art deep learning models. Furthermore, we propose to transform ZSSD into the NLI task by applying simple yet effective prompts to noun-phrase targets. Our experimental results show that EZ-STANCE is a challenging new benchmark, which provides significant research opportunities on English ZSSD. We publicly release our dataset and code at https://github.com/chenyez/EZ-STANCE.</abstract>
      <url hash="9c27fff1">2024.acl-long.838</url>
      <bibkey>zhao-caragea-2024-ez</bibkey>
      <doi>10.18653/v1/2024.acl-long.838</doi>
    </paper>
    <paper id="839">
      <title><fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Handshapes Reflect Pressures for Communicative Efficiency</title>
      <author><first>Kayo</first><last>Yin</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Terry</first><last>Regier</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Dan</first><last>Klein</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>15715-15724</pages>
      <abstract>Communicative efficiency is a key topic in linguistics and cognitive psychology, with many studies demonstrating how the pressure to communicate with minimal effort guides the form of natural language. However, this phenomenon is rarely explored in signed languages. This paper shows how handshapes in American Sign Language (ASL) reflect these efficiency pressures and provides new evidence of communicative efficiency in the visual-gestural modality.We focus on hand configurations in native ASL signs and signs borrowed from English to compare efficiency pressures from both ASL and English usage. First, we develop new methodologies to quantify the articulatory effort needed to produce handshapes and the perceptual effort required to recognize them. Then, we analyze correlations between communicative effort and usage statistics in ASL or English. Our findings reveal that frequent ASL handshapes are easier to produce and that pressures for communicative efficiency mostly come from ASL usage, rather than from English lexical borrowing.</abstract>
      <url hash="81e2224a">2024.acl-long.839</url>
      <bibkey>yin-etal-2024-american</bibkey>
      <doi>10.18653/v1/2024.acl-long.839</doi>
    </paper>
    <paper id="840">
      <title>Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research</title>
      <author><first>Luca</first><last>Soldaini</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Rodney</first><last>Kinney</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Akshita</first><last>Bhagia</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Dustin</first><last>Schwenk</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>David</first><last>Atkinson</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Russell</first><last>Authur</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Ben</first><last>Bogin</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Khyathi</first><last>Chandu</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Jennifer</first><last>Dumas</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yanai</first><last>Elazar</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Valentin</first><last>Hofmann</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Ananya</first><last>Jha</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Sachin</first><last>Kumar</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Li</first><last>Lucy</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Xinxi</first><last>Lyu</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Nathan</first><last>Lambert</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Ian</first><last>Magnusson</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Jacob</first><last>Morrison</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Niklas</first><last>Muennighoff</last><affiliation>Contextual AI</affiliation></author>
      <author><first>Aakanksha</first><last>Naik</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Crystal</first><last>Nam</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Matthew</first><last>Peters</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Abhilasha</first><last>Ravichander</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Kyle</first><last>Richardson</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Zejiang</first><last>Shen</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Emma</first><last>Strubell</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Nishant</first><last>Subramani</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Oyvind</first><last>Tafjord</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Evan</first><last>Walsh</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Luke</first><last>Zettlemoyer</last><affiliation>University of Washington</affiliation></author>
      <author><first>Noah</first><last>Smith</last><affiliation>University of Washington</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last><affiliation>University of Washington</affiliation></author>
      <author><first>Iz</first><last>Beltagy</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Dirk</first><last>Groeneveld</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Jesse</first><last>Dodge</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Kyle</first><last>Lo</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>15725-15788</pages>
      <abstract>Information about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.</abstract>
      <url hash="e793cf4a">2024.acl-long.840</url>
      <bibkey>soldaini-etal-2024-dolma</bibkey>
      <doi>10.18653/v1/2024.acl-long.840</doi>
    </paper>
    <paper id="841">
      <title><fixed-case>OLM</fixed-case>o: Accelerating the Science of Language Models</title>
      <author><first>Dirk</first><last>Groeneveld</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Iz</first><last>Beltagy</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Evan</first><last>Walsh</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Akshita</first><last>Bhagia</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Rodney</first><last>Kinney</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Oyvind</first><last>Tafjord</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Ananya</first><last>Jha</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Hamish</first><last>Ivison</last><affiliation>University of Washington</affiliation></author>
      <author><first>Ian</first><last>Magnusson</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yizhong</first><last>Wang</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Shane</first><last>Arora</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>David</first><last>Atkinson</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Russell</first><last>Authur</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Khyathi</first><last>Chandu</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University</affiliation></author>
      <author><first>Jennifer</first><last>Dumas</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yanai</first><last>Elazar</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yuling</first><last>Gu</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Jack</first><last>Hessel</last><affiliation>Samaya AI</affiliation></author>
      <author><first>Tushar</first><last>Khot</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>William</first><last>Merrill</last><affiliation>New York University</affiliation></author>
      <author><first>Jacob</first><last>Morrison</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Niklas</first><last>Muennighoff</last><affiliation>Contextual AI</affiliation></author>
      <author><first>Aakanksha</first><last>Naik</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Crystal</first><last>Nam</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Matthew</first><last>Peters</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Valentina</first><last>Pyatkin</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Abhilasha</first><last>Ravichander</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Dustin</first><last>Schwenk</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Saurabh</first><last>Shah</last><affiliation>Apple</affiliation></author>
      <author><first>William</first><last>Smith</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Emma</first><last>Strubell</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Nishant</first><last>Subramani</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Mitchell</first><last>Wortsman</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Pradeep</first><last>Dasigi</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Nathan</first><last>Lambert</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Kyle</first><last>Richardson</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Luke</first><last>Zettlemoyer</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jesse</first><last>Dodge</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Kyle</first><last>Lo</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Luca</first><last>Soldaini</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Noah</first><last>Smith</last><affiliation>University of Washington</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last><affiliation>University of Washington</affiliation></author>
      <pages>15789-15809</pages>
      <abstract>Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.</abstract>
      <url hash="f4155bea">2024.acl-long.841</url>
      <bibkey>groeneveld-etal-2024-olmo</bibkey>
      <doi>10.18653/v1/2024.acl-long.841</doi>
    </paper>
    <paper id="842">
      <title>Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!</title>
      <author><first>Zhanhui</first><last>Zhou</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jie</first><last>Liu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zhichen</first><last>Dong</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chao</first><last>Yang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Wanli</first><last>Ouyang</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>Shanghai Aritifcal Intelligence Laboratory</affiliation></author>
      <pages>15810-15830</pages>
      <abstract>Large language models (LLMs) undergo safety alignment to ensure safe conversations with humans. However, this paper introduces a training-free attack method capable of reversing safety alignment, converting the outcomes of stronger alignment into greater potential for harm by accessing only LLM output token distributions. Specifically, our method achieves this reversal by contrasting the output token distribution of a safety-aligned language model (e.g., Llama-2-chat) against its pre-trained version (e.g., Llama-2), so that the token predictions are shifted towards the opposite direction of safety alignment.We name this method <i>emulated disalignment</i> (ED) because sampling from this contrastive distribution provably emulates the result of fine-tuning to minimize a safety reward.Our experiments with ED across three evaluation datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rates in 43 out of 48 evaluation subsets by a large margin.Eventually, given ED’s reliance on language model output token distributions, which particularly compromises open-source models, our findings highlight the need to reassess the open accessibility of language models, even if they have been safety-aligned.Code is available at https://github.com/ZHZisZZ/emulated-disalignment.</abstract>
      <url hash="83cc5751">2024.acl-long.842</url>
      <bibkey>zhou-etal-2024-emulated</bibkey>
      <doi>10.18653/v1/2024.acl-long.842</doi>
    </paper>
    <paper id="843">
      <title><fixed-case>I</fixed-case>ndic<fixed-case>LLMS</fixed-case>uite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Mohammed Safi Ur Rahman</first><last>Khan</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Priyam</first><last>Mehta</last><affiliation>Gujarat Technological University Ahmedabad</affiliation></author>
      <author><first>Ananth</first><last>Sankar</last><affiliation>Annamalai University</affiliation></author>
      <author><first>Umashankar</first><last>Kumaravelan</last><affiliation>AI4Bharat</affiliation></author>
      <author><first>Sumanth</first><last>Doddapaneni</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Suriyaprasaad</first><last>B</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Varun</first><last>G</last><affiliation>Indian Institute of Information Technology, Design and Manufacturing, Kancheepuram</affiliation></author>
      <author><first>Sparsh</first><last>Jain</last><affiliation>Guru Gobind Singh Indraprastha University, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Anoop</first><last>Kunchukuttan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Pratyush</first><last>Kumar</last><affiliation>Indian Institute of Technology Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Mitesh M.</first><last>Khapra</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>15831-15879</pages>
      <abstract>Despite the considerable advancements in English LLMs, the progress in building comparable models for other languages has been hindered due to the scarcity of tailored resources. Our work aims to bridge this divide by introducing an expansive suite of resources specifically designed for the development of Indic LLMs, covering 22 languages, containing a total of 251B tokens and 74.8M instruction-response pairs. Recognizing the importance of both data quality and quantity, our approach combines highly curated manually verified data, unverified yet valuable data, and synthetic data. We build a clean, open-source pipeline for curating pre-training data from diverse sources, including websites, PDFs, and videos, incorporating best practices for crawling, cleaning, flagging, and deduplication. For instruction-fine tuning, we amalgamate existing Indic datasets, translate/transliterate English datasets into Indian languages, and utilize LLaMa2 and Mixtral models to create conversations grounded in articles from Indian Wikipedia and Wikihow. Additionally, we address toxicity alignment by generating toxic prompts for multiple scenarios and then generate non-toxic responses by feeding these toxic prompts to an aligned LLaMa2 model. We hope that the datasets, tools, and resources released as a part of this work will not only propel the research and development of Indic LLMs but also establish an open-source blueprint for extending such efforts to other languages.</abstract>
      <url hash="28f6a48c">2024.acl-long.843</url>
      <bibkey>khan-etal-2024-indicllmsuite</bibkey>
      <doi>10.18653/v1/2024.acl-long.843</doi>
    </paper>
    <paper id="844">
      <title>Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models</title>
      <author><first>Xiaolong</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yile</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yuanchi</first><last>Zhang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Fuwen</first><last>Luo</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Peng</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>15880-15893</pages>
      <abstract>Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that RiC can yield significant improvement compared with various baselines.</abstract>
      <url hash="8ff47454">2024.acl-long.844</url>
      <bibkey>wang-etal-2024-reasoning</bibkey>
      <doi>10.18653/v1/2024.acl-long.844</doi>
    </paper>
    <paper id="845">
      <title>Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model</title>
      <author><first>Ahmet</first><last>Üstün</last><affiliation>Cohere For Ai</affiliation></author>
      <author><first>Viraat</first><last>Aryabumi</last><affiliation>Cohere For AI</affiliation></author>
      <author><first>Zheng</first><last>Yong</last><affiliation>Brown University</affiliation></author>
      <author><first>Wei-Yin</first><last>Ko</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Daniel</first><last>D’souza</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Gbemileke</first><last>Onilude</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Neel</first><last>Bhandari</last><affiliation>R V College of Engineering</affiliation></author>
      <author><first>Shivalika</first><last>Singh</last><affiliation>Christ University</affiliation></author>
      <author><first>Hui-Lee</first><last>Ooi</last><affiliation>École Polytechnique de Montréal, Université de Montréal</affiliation></author>
      <author><first>Amr</first><last>Kayid</last><affiliation>Technical University Munich</affiliation></author>
      <author><first>Freddie</first><last>Vargus</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Phil</first><last>Blunsom</last><affiliation>Google</affiliation></author>
      <author><first>Shayne</first><last>Longpre</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Niklas</first><last>Muennighoff</last><affiliation>Contextual AI</affiliation></author>
      <author><first>Marzieh</first><last>Fadaee</last><affiliation>Cohere For AI</affiliation></author>
      <author><first>Julia</first><last>Kreutzer</last><affiliation>Cohere for AI</affiliation></author>
      <author><first>Sara</first><last>Hooker</last><affiliation>Cohere For AI</affiliation></author>
      <pages>15894-15939</pages>
      <abstract>Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages —— including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.</abstract>
      <url hash="2f16d266">2024.acl-long.845</url>
      <bibkey>ustun-etal-2024-aya</bibkey>
      <doi>10.18653/v1/2024.acl-long.845</doi>
    </paper>
    <paper id="846">
      <title><fixed-case>B</fixed-case>atch<fixed-case>E</fixed-case>val: Towards Human-like Text Evaluation</title>
      <author><first>Peiwen</first><last>Yuan</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Shaoxiong</first><last>Feng</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yiwei</first><last>Li</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Xinglin</first><last>Wang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Boyuan</first><last>Pan</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Heda</first><last>Wang</last><affiliation>xiaohongshu</affiliation></author>
      <author><first>Yao</first><last>Hu</last><affiliation>Zhejiang University, Tsinghua University</affiliation></author>
      <author><first>Kan</first><last>Li</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>15940-15958</pages>
      <abstract>Significant progress has been made in automatic text evaluation with the introduction of large language models (LLMs) as evaluators. However, current sample-wise evaluation paradigm suffers from the following issues: (1) Sensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble performance with static reference. Inspired by the fact that humans treat both criterion definition and inter sample comparison as references for evaluation, we propose BatchEval, a paradigm that conducts batch-wise evaluation iteratively to alleviate the above problems. We explore variants under this paradigm and confirm the optimal settings are two stage procedure with heterogeneous batch composition strategy and decimal scoring format. Comprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate that BatchEval outperforms state-of-the-art methods by 10.5% on Pearson correlations with only 64% API cost on average. Further analyses have been conducted to verify the robustness, generalization, and working mechanism of BatchEval.</abstract>
      <url hash="eeb82278">2024.acl-long.846</url>
      <bibkey>yuan-etal-2024-batcheval</bibkey>
      <doi>10.18653/v1/2024.acl-long.846</doi>
    </paper>
    <paper id="847">
      <title><fixed-case>T</fixed-case>o<fixed-case>MB</fixed-case>ench: Benchmarking Theory of Mind in Large Language Models</title>
      <author><first>Zhuang</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Jincenzi</first><last>Wu</last><affiliation>New York University</affiliation></author>
      <author><first>Jinfeng</first><last>Zhou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Bosi</first><last>Wen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Guanqun</first><last>Bi</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Gongyao</first><last>Jiang</last><affiliation>The Hongkong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Yaru</first><last>Cao</last><affiliation>Northwest Minzu University</affiliation></author>
      <author><first>Mengting</first><last>Hu</last><affiliation>Nankai University</affiliation></author>
      <author><first>Yunghwei</first><last>Lai</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Zexuan</first><last>Xiong</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>15959-15983</pages>
      <abstract>Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet. Our aim with ToMBench is to enable an efficient and effective evaluation of LLMs’ ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence.</abstract>
      <url hash="a767cf33">2024.acl-long.847</url>
      <bibkey>chen-etal-2024-tombench</bibkey>
      <doi>10.18653/v1/2024.acl-long.847</doi>
    </paper>
    <paper id="848">
      <title><fixed-case>COKE</fixed-case>: A Cognitive Knowledge Graph for Machine Theory of Mind</title>
      <author><first>Jincenzi</first><last>Wu</last><affiliation>New York University</affiliation></author>
      <author><first>Zhuang</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Jiawen</first><last>Deng</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Sahand</first><last>Sabour</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Helen</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>15984-16007</pages>
      <abstract>Theory of mind (ToM) refers to humans’ ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans’ social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. In addition, we further generalize COKE using LLMs and build a powerful generation model COLM tailored for cognitive reasoning. Experimental results in both automatic and human evaluation demonstrate the high quality of COKE, the superior ToM ability of COLM, and its potential to significantly enhance social applications.</abstract>
      <url hash="42e802a7">2024.acl-long.848</url>
      <bibkey>wu-etal-2024-coke</bibkey>
      <doi>10.18653/v1/2024.acl-long.848</doi>
    </paper>
    <paper id="849">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>PIC</fixed-case>o: Multilingual Perspectivist Irony Corpus</title>
      <author><first>Silvia</first><last>Casola</last><affiliation>University of Turin</affiliation></author>
      <author><first>Simona</first><last>Frenda</last><affiliation>University of Turin</affiliation></author>
      <author><first>Soda Marem</first><last>Lo</last><affiliation>University of Turin</affiliation></author>
      <author><first>Erhan</first><last>Sezerer</last><affiliation>Amazon</affiliation></author>
      <author><first>Antonio</first><last>Uva</last><affiliation>Amazon</affiliation></author>
      <author><first>Valerio</first><last>Basile</last><affiliation>University of Turin</affiliation></author>
      <author><first>Cristina</first><last>Bosco</last><affiliation>University of Turin</affiliation></author>
      <author><first>Alessandro</first><last>Pedrani</last><affiliation>Amazon</affiliation></author>
      <author><first>Chiara</first><last>Rubagotti</last><affiliation>Catholic University of the Sacred Heart</affiliation></author>
      <author><first>Viviana</first><last>Patti</last><affiliation>University of Turin, Computer Science Department</affiliation></author>
      <author><first>Davide</first><last>Bernardi</last><affiliation>Amazon</affiliation></author>
      <pages>16008-16021</pages>
      <abstract>Recently, several scholars have contributed to the growth of a new theoretical framework in NLP called perspectivism. This approach aimsto leverage data annotated by different individuals to model diverse perspectives that affect their opinions on subjective phenomena such as irony. In this context, we propose MultiPICo, a multilingual perspectivist corpus of ironic short conversations in different languages andlinguistic varieties extracted from Twitter and Reddit. The corpus includes sociodemographic information about its annotators. Our analysis of the annotated corpus shows how different demographic cohorts may significantly disagree on their annotation of irony and how certain cultural factors influence the perception of the phenomenon and the agreement on the annotation. Moreover, we show how disaggregated annotations and rich annotator metadata can be exploited to benchmark the ability of large language models to recognize irony, their positionality with respect to sociodemographic groups, and the efficacy of perspective-taking prompting for irony detection in multiple languages.</abstract>
      <url hash="2b988cdb">2024.acl-long.849</url>
      <bibkey>casola-etal-2024-multipico</bibkey>
      <doi>10.18653/v1/2024.acl-long.849</doi>
    </paper>
    <paper id="850">
      <title><fixed-case>A</fixed-case>pp<fixed-case>W</fixed-case>orld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents</title>
      <author><first>Harsh</first><last>Trivedi</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Tushar</first><last>Khot</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Mareike</first><last>Hartmann</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Ruskin</first><last>Manku</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Vinty</first><last>Dong</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Edward</first><last>Li</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Shashank</first><last>Gupta</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Ashish</first><last>Sabharwal</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <pages>16022-16076</pages>
      <abstract>Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only ~49% of our ‘normal’ tasks and ~30% of ‘challenge’ tasks, while other models solve at least 16% fewer. This highlights the benchmark’s difficulty and AppWorld’s potential to push the frontiers of interactive coding agents.</abstract>
      <url hash="19cefe81">2024.acl-long.850</url>
      <bibkey>trivedi-etal-2024-appworld</bibkey>
      <doi>10.18653/v1/2024.acl-long.850</doi>
    </paper>
    <paper id="851">
      <title><fixed-case>MMT</fixed-case>o<fixed-case>M</fixed-case>-<fixed-case>QA</fixed-case>: Multimodal Theory of Mind Question Answering</title>
      <author><first>Chuanyang</first><last>Jin</last><affiliation>New York University</affiliation></author>
      <author><first>Yutong</first><last>Wu</last><affiliation>Harvard University</affiliation></author>
      <author><first>Jing</first><last>Cao</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jiannan</first><last>Xiang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Yen-Ling</first><last>Kuo</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Zhiting</first><last>Hu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Tomer</first><last>Ullman</last><affiliation>Harvard University</affiliation></author>
      <author><first>Antonio</first><last>Torralba</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Joshua</first><last>Tenenbaum</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Tianmin</first><last>Shu</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>16077-16102</pages>
      <abstract>Theory of Mind (ToM), the ability to understand people’s mental states, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets – either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person’s mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person’s activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models.</abstract>
      <url hash="6f98d25d">2024.acl-long.851</url>
      <bibkey>jin-etal-2024-mmtom</bibkey>
      <doi>10.18653/v1/2024.acl-long.851</doi>
    </paper>
    <paper id="852">
      <title><fixed-case>D</fixed-case>oc<fixed-case>M</fixed-case>ath-Eval: Evaluating Math Reasoning Capabilities of <fixed-case>LLM</fixed-case>s in Understanding Long and Specialized Documents</title>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Yitao</first><last>Long</last><affiliation>New York University</affiliation></author>
      <author><first>Hongjun</first><last>Liu</last><affiliation>College of Computer Science and Technology, Zhejiang University</affiliation></author>
      <author><first>Ryo</first><last>Kamoi</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Linyong</first><last>Nan</last><affiliation>Yale University</affiliation></author>
      <author><first>Lyuhao</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Xiangru</first><last>Tang</last><affiliation>Yale University</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University</affiliation></author>
      <pages>16103-16120</pages>
      <abstract>Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing specialized documents containing both text and tables. We conduct an extensive evaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting methods, aiming to comprehensively assess the capabilities and limitations of existing LLMs in DocMath-Eval. We found that even the current best-performing system (i.e., GPT-4o) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe that DocMath-Eval can serve as a valuable benchmark for evaluating LLMs' capabilities in solving challenging numerical reasoning problems within expert domains.</abstract>
      <url hash="0f0bea3b">2024.acl-long.852</url>
      <bibkey>zhao-etal-2024-docmath</bibkey>
      <revision id="1" href="2024.acl-long.852v1" hash="de5c6157"/>
      <revision id="2" href="2024.acl-long.852v2" hash="0f0bea3b" date="2024-09-17">Included experimental results.</revision>
      <doi>10.18653/v1/2024.acl-long.852</doi>
    </paper>
    <paper id="853">
      <title>Unintended Impacts of <fixed-case>LLM</fixed-case> Alignment on Global Representation</title>
      <author><first>Michael J</first><last>Ryan</last><affiliation>Stanford University</affiliation></author>
      <author><first>William</first><last>Held</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>16121-16140</pages>
      <abstract>Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable preference tuning. We make our code and data publicly available on Github.</abstract>
      <url hash="b30fe558">2024.acl-long.853</url>
      <bibkey>ryan-etal-2024-unintended</bibkey>
      <doi>10.18653/v1/2024.acl-long.853</doi>
    </paper>
    <paper id="854">
      <title><fixed-case>ICLEF</fixed-case>: In-Context Learning with Expert Feedback for Explainable Style Transfer</title>
      <author><first>Arkadiy</first><last>Saakyan</last><affiliation>Columbia University</affiliation></author>
      <author><first>Smaranda</first><last>Muresan</last><affiliation>Amazon</affiliation></author>
      <pages>16141-16163</pages>
      <abstract>While state-of-the-art large language models (LLMs) can excel at adapting text from one style to another, current work does not address the explainability of style transfer models. Recent work has explored generating textual explanations from larger teacher models and distilling them into smaller student models. One challenge with such approach is that LLM outputs may contain errors that require expertise to correct, but gathering and incorporating expert feedback is difficult due to cost and availability. To address this challenge, we propose ICLEF, a novel human-AI collaboration approach to model distillation that incorporates scarce expert human feedback by combining in-context learning and model self-critique. We show that our method leads to generation of high-quality synthetic explainable style transfer datasets for formality (E-GYAFC) and subjective bias (E-WNC). Via automatic and human evaluation, we show that specialized student models fine-tuned on our datasets outperform generalist teacher models on the explainable style transfer task in one-shot settings, and perform competitively compared to few-shot teacher models, highlighting the quality of the data and the role of expert feedback. In an extrinsic task of authorship attribution, we show that explanations generated by smaller models fine-tuned on E-GYAFC are more predictive of authorship than explanations generated by few-shot teacher models.</abstract>
      <url hash="109bdf1f">2024.acl-long.854</url>
      <bibkey>saakyan-muresan-2024-iclef</bibkey>
      <doi>10.18653/v1/2024.acl-long.854</doi>
    </paper>
    <paper id="855">
      <title><fixed-case>MAP</fixed-case>’s not dead yet: Uncovering true language model modes by conditioning away degeneracy</title>
      <author><first>Davis</first><last>Yoshida</last><affiliation>Amazon</affiliation></author>
      <author><first>Kartik</first><last>Goyal</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Kevin</first><last>Gimpel</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <pages>16164-16215</pages>
      <abstract>It has been widely observed that exact or approximate MAP (mode-seeking) decoding from natural language generation (NLG) models consistently leads to degenerate outputs (Holtzman et al., 2019; Stahlberg and Byrne, 2019). Prior work has attributed this behavior to either a fundamental and unavoidable inadequacy of modes in probabilistic models or weaknesses in language modeling. Contrastingly, we argue that degenerate modes can even occur in the absence of any modeling error, due to contamination of the training data. Specifically, we argue that mixing even a tiny amount of low-entropy noise with a population text distribution can cause the data distribution’s mode to become degenerate. We therefore propose to apply MAP decoding to the model’s true conditional distribution where the conditioning variable explicitly avoids specific degenerate behavior. Using exact search, we empirically verify that the length-conditional modes of machine translation models and language models are indeed more fluent and topical than their unconditional modes. For the first time, we also share many examples of exact modal sequences from these models, and from several variants of the LLaMA-7B model. Notably, we observethat various kinds of degenerate modes persist, even at the scale of LLaMA-7B. Although we cannot tractably address these degeneracieswith exact search, we perform a classifier-based approximate search on LLaMA-7B, a model which was not trained for instruction following, and find that we are able to elicit reasonable outputs without any finetuning.</abstract>
      <url hash="0ff91cee">2024.acl-long.855</url>
      <bibkey>yoshida-etal-2024-maps</bibkey>
      <doi>10.18653/v1/2024.acl-long.855</doi>
    </paper>
    <paper id="856">
      <title>Guardians of the Machine Translation Meta-Evaluation: Sentinel Metrics Fall In!</title>
      <author><first>Stefano</first><last>Perrella</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Lorenzo</first><last>Proietti</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Alessandro</first><last>Scirè</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Edoardo</first><last>Barba</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>16216-16244</pages>
      <abstract>Annually, at the Conference of Machine Translation (WMT), the Metrics Shared Task organizers conduct the meta-evaluation of Machine Translation (MT) metrics, ranking them according to their correlation with human judgments. Their results guide researchers toward enhancing the next generation of metrics and MT systems. With the recent introduction of neural metrics, the field has witnessed notable advancements. Nevertheless, the inherent opacity of these metrics has posed substantial challenges to the meta-evaluation process. This work highlights two issues with the meta-evaluation framework currently employed in WMT, and assesses their impact on the metrics rankings. To do this, we introduce the concept of sentinel metrics, which are designed explicitly to scrutinize the meta-evaluation process’s accuracy, robustness, and fairness. By employing sentinel metrics, we aim to validate our findings, and shed light on and monitor the potential biases or inconsistencies in the rankings. We discover that the present meta-evaluation framework favors two categories of metrics: i) those explicitly trained to mimic human quality assessments, and ii) continuous metrics. Finally, we raise concerns regarding the evaluation capabilities of state-of-the-art metrics, emphasizing that they might be basing their assessments on spurious correlations found in their training data.</abstract>
      <url hash="a69fefe6">2024.acl-long.856</url>
      <bibkey>perrella-etal-2024-guardians</bibkey>
      <doi>10.18653/v1/2024.acl-long.856</doi>
    </paper>
    <paper id="857">
      <title><fixed-case>N</fixed-case>oun<fixed-case>A</fixed-case>tlas: Filling the Gap in Nominal Semantic Role Labeling</title>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <author><first>Marco</first><last>Lo Pinto</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Pasquale</first><last>Silvestri</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Dennis</first><last>Rotondi</last><affiliation>Universität Stuttgart</affiliation></author>
      <author><first>Simone</first><last>Ciciliano</last><affiliation>Free University of Bozen</affiliation></author>
      <author><first>Alessandro</first><last>Scirè</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <pages>16245-16258</pages>
      <abstract>Despite significant advances in Semantic Role Labeling (SRL), much work in this field has been carried out with a focus on verbal predicates, with the research on nominal SRL lagging behind. In many contexts, however, nominal predicates are often as informative as verbal ones, thus needing proper treatment. In this paper we aim to fill this gap and make nominal SRL a first-class citizen. We introduce a novel approach to create the first large-scale, high-quality inventory of nominal predicates and organize them into semantically-coherent frames. Although automatically created, NounAtlas – our frame inventory – is subsequently fully validated. We then put forward a technique to generate silver training data for nominal SRL and show that a state-of-the-art SRL model can achieve good performance. Interestingly, thanks to our design choices which enable seamless integration of our predicate inventory with its verbal counterpart, we can mix verbal and nominal data and perform robust SRL on both types of predicates.</abstract>
      <url hash="4d537122">2024.acl-long.857</url>
      <bibkey>navigli-etal-2024-nounatlas</bibkey>
      <doi>10.18653/v1/2024.acl-long.857</doi>
    </paper>
    <paper id="858">
      <title>The Earth is Flat because...: Investigating <fixed-case>LLM</fixed-case>s’ Belief towards Misinformation via Persuasive Conversation</title>
      <author><first>Rongwu</first><last>Xu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Brian</first><last>Lin</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Shujian</first><last>Yang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Tianqi</first><last>Zhang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Weiyan</first><last>Shi</last><affiliation>Stanford University</affiliation></author>
      <author><first>Tianwei</first><last>Zhang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zhixuan</first><last>Fang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Han</first><last>Qiu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>16259-16303</pages>
      <abstract>Large language models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs’ susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs’ belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs’ correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies.</abstract>
      <url hash="2a64e2c8">2024.acl-long.858</url>
      <bibkey>xu-etal-2024-earth</bibkey>
      <doi>10.18653/v1/2024.acl-long.858</doi>
    </paper>
    <paper id="859">
      <title><fixed-case>L</fixed-case>oo<fixed-case>GLE</fixed-case>: Can Long-Context Language Models Understand Long Contexts?</title>
      <author><first>Jiaqi</first><last>Li</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Mengmeng</first><last>Wang</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Zilong</first><last>Zheng</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Muhan</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>16304-16333</pages>
      <abstract>Large language models (LLMs) are typically limited to processing texts within context window size, which has spurred significant research efforts into enhancing LLMs’ long-context understanding as well as developing high-quality benchmarks to evaluate the ability. However, prior datasets suffer from short comings like short length compared to the context window of modern LLMs; outdated documents that might have data leakage problems; and an emphasis on short dependency tasks only. In this paper, we present LooGLE , a Long Context Generic Language Evaluation benchmark. It features documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning varying dependency ranges in diverse domains. Human annotators meticulously crafted over 1,100 high-quality question-answer (QA) pairs with thorough cross-validation for a most precise assessment of LLMs’ long dependency capabilities. We conduct a comprehensive evaluation of representative LLMs on LooGLE . The results indicate that most LLMs have shockingly bad long context ability and fail to capture long dependencies in the context, even when their context window size is enough to fit the entire document. Our results shed light on enhancing the “true long-context understanding” ability of LLMs instead of merely enlarging their context window.</abstract>
      <url hash="4da9bf1d">2024.acl-long.859</url>
      <bibkey>li-etal-2024-loogle</bibkey>
      <doi>10.18653/v1/2024.acl-long.859</doi>
    </paper>
    <paper id="860">
      <title>Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation</title>
      <author><first>Se</first><last>Park</last><affiliation>KAIST</affiliation></author>
      <author><first>Chae</first><last>Kim</last><affiliation>KAIST</affiliation></author>
      <author><first>Hyeongseop</first><last>Rha</last><affiliation>KAIST, Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Minsu</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Joanna</first><last>Hong</last><affiliation>Meta</affiliation></author>
      <author><first>Jeonghun</first><last>Yeo</last><affiliation>KAIST</affiliation></author>
      <author><first>Yong</first><last>Ro</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>16334-16348</pages>
      <abstract>In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e, audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. Through extensive experiments, we validate the effectiveness of our model in facilitating a face-to-face conversation. Demo and data are available at https://multidialog.github.io and https://huggingface.co/datasets/IVLLab/MultiDialog, respectively.</abstract>
      <url hash="6f7336bc">2024.acl-long.860</url>
      <bibkey>park-etal-2024-lets</bibkey>
      <doi>10.18653/v1/2024.acl-long.860</doi>
    </paper>
    <paper id="861">
      <title><fixed-case>ECBD</fixed-case>: Evidence-Centered Benchmark Design for <fixed-case>NLP</fixed-case></title>
      <author><first>Yu Lu</first><last>Liu</last><affiliation>McGill University, McGill University</affiliation></author>
      <author><first>Su Lin</first><last>Blodgett</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jackie</first><last>Cheung</last><affiliation>McGill University</affiliation></author>
      <author><first>Q. Vera</first><last>Liao</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Alexandra</first><last>Olteanu</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Ziang</first><last>Xiao</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <pages>16349-16365</pages>
      <abstract>Benchmarking is seen as critical to assessing progress in NLP. However, creating a benchmark involves many design decisions (e.g., which datasets to include, which metrics to use) that often rely on tacit, untested assumptions about what the benchmark is intended to measure or is actually measuring. There is currently no principled way of analyzing these decisions and how they impact the validity of the benchmark’s measurements. To address this gap, we draw on evidence-centered design in educational assessments and propose Evidence-Centered Benchmark Design (ECBD), a framework which formalizes the benchmark design process into five modules. ECBD specifies the role each module plays in helping practitioners collect evidence about capabilities of interest. Specifically, each module requires benchmark designers to describe, justify, and support benchmark design choices—e.g., clearly specifying the capabilities the benchmark aims to measure or how evidence about those capabilities is collected from model responses. To demonstrate the use of ECBD, we conduct case studies with three benchmarks: BoolQ, SuperGLUE, and HELM. Our analysis reveals common trends in benchmark design and documentation that could threaten the validity of benchmarks’ measurements.</abstract>
      <url hash="bf1a4b4f">2024.acl-long.861</url>
      <bibkey>liu-etal-2024-ecbd</bibkey>
      <doi>10.18653/v1/2024.acl-long.861</doi>
    </paper>
    <paper id="862">
      <title>Having Beer after Prayer? Measuring Cultural Bias in Large Language Models</title>
      <author><first>Tarek</first><last>Naous</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Michael J</first><last>Ryan</last><affiliation>Stanford University</affiliation></author>
      <author><first>Alan</first><last>Ritter</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>16366-16393</pages>
      <abstract>As the reach of large language models (LMs) expands globally, their ability to cater to diverse cultural contexts becomes crucial. Despite advancements in multilingual capabilities, models are not designed with appropriate cultural nuances. In this paper, we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture. We introduce CAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities spanning eight types that contrast Arab and Western cultures. CAMeL provides a foundation for measuring cultural biases in LMs through both extrinsic and intrinsic evaluations. Using CAMeL, we examine the cross-cultural performance in Arabic of 16 different LMs on tasks such as story generation, NER, and sentiment analysis, where we find concerning cases of stereotyping and cultural unfairness. We further test their text-infilling performance, revealing the incapability of appropriate adaptation to Arab cultural contexts. Finally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware LMs, if used as they are without adjustment. We will make CAMeL publicly available at: https://github.com/tareknaous/camel</abstract>
      <url hash="94ae0173">2024.acl-long.862</url>
      <bibkey>naous-etal-2024-beer</bibkey>
      <doi>10.18653/v1/2024.acl-long.862</doi>
    </paper>
    <paper id="863">
      <title>Explicating the Implicit: Argument Detection Beyond Sentence Boundaries</title>
      <author><first>Paul</first><last>Roit</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Aviv</first><last>Slobodkin</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Eran</first><last>Hirsch</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Arie</first><last>Cattan</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Ayal</first><last>Klein</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Valentina</first><last>Pyatkin</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Ido</first><last>Dagan</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>16394-16409</pages>
      <abstract>Detecting semantic arguments of a predicate word has been conventionally modeled as a sentence-level task. The typical reader, however, perfectly interprets predicate-argument relations in a much wider context than just the sentence where the predicate was evoked. In this work, we reformulate the problem of argument detection through textual entailment to capture semantic relations across sentence boundaries. We propose a method that tests whether some semantic relation can be inferred from a full passage by first encoding it into a simple and standalone proposition and then testing for entailment against the passage. Our method does not require direct supervision, which is generally absent due to dataset scarcity, but instead builds on existing NLI and sentence-level SRL resources. Such a method can potentially explicate pragmatically understood relations into a set of explicit sentences. We demonstrate it on a recent document-level benchmark, outperforming some supervised methods and contemporary language models.</abstract>
      <url hash="dfbd1499">2024.acl-long.863</url>
      <bibkey>roit-etal-2024-explicating</bibkey>
      <doi>10.18653/v1/2024.acl-long.863</doi>
    </paper>
    <paper id="864">
      <title>Word Embeddings Are Steers for Language Models</title>
      <author><first>Chi</first><last>Han</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Jialiang</first><last>Xu</last><affiliation>Stanford University</affiliation></author>
      <author><first>Manling</first><last>Li</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Yi</first><last>Fung</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Chenkai</first><last>Sun</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Nan</first><last>Jiang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Tarek</first><last>Abdelzaher</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>16410-16430</pages>
      <abstract>Language models (LMs) automatically learn word embeddings during pre-training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain underexplored. In this work, we theoretically and empirically revisit output word embeddings and find that their linear transformations are equivalent to steering language model generation styles. We name such steers LM-Steers and find them existing in LMs of all sizes. It requires learning parameters equal to 0.2% of the original LMs’ size for steering each style. On tasks such as language model detoxification and sentiment control, LM-Steers can achieve comparable or superior performance compared with state-of-the-art controlled generation methods while maintaining a better balance with generation quality. The learned LM-Steer serves as a lens in text styles: it reveals that word embeddings are interpretable when associated with language model generations and can highlight text spans that most indicate the style differences. An LM-Steer is transferrable between different language models by an explicit form calculation. One can also continuously steer LMs simply by scaling the LM-Steer or compose multiple LM-Steers by adding their transformations. Our codes are publicly available at https://github.com/Glaciohound/LM-Steer.</abstract>
      <url hash="843312e5">2024.acl-long.864</url>
      <bibkey>han-etal-2024-word</bibkey>
      <doi>10.18653/v1/2024.acl-long.864</doi>
    </paper>
  </volume>
  <volume id="short" ingest-date="2024-08-12" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</booktitle>
      <editor><first>Lun-Wei</first><last>Ku</last><affiliation>Academia Sinica</affiliation></editor>
      <editor><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico / Instituto de Telecomunicações / Unbabel</affiliation></editor>
      <editor><first>Vivek</first><last>Srikumar</last><affiliation>University of Utah</affiliation></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="0ae59515">2024.acl-short</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="f5cd3dde">2024.acl-short.0</url>
      <bibkey>acl-2024-short</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Can Language Models Serve as Text-Based World Simulators?</title>
      <author><first>Ruoyao</first><last>Wang</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Graham</first><last>Todd</last></author>
      <author><first>Ziang</first><last>Xiao</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Xingdi</first><last>Yuan</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Marc-Alexandre</first><last>Côté</last><affiliation>Microsoft</affiliation></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Peter</first><last>Jansen</last><affiliation>University of Arizona</affiliation></author>
      <pages>1-17</pages>
      <abstract>Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM’s capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.</abstract>
      <url hash="60668c3b">2024.acl-short.1</url>
      <bibkey>wang-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.acl-short.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>F</fixed-case>an<fixed-case>O</fixed-case>ut<fixed-case>QA</fixed-case>: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models</title>
      <author><first>Andrew</first><last>Zhu</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Alyssa</first><last>Hwang</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Liam</first><last>Dugan</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>Allen Institute for Artificial Intelligence and University of Pennsylvania</affiliation></author>
      <pages>18-37</pages>
      <abstract>One type of question that is commonly found in day-to-day scenarios is “fan-out” questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset, along with open-source tools to run models to encourage evaluation.</abstract>
      <url hash="1d5622b5">2024.acl-short.2</url>
      <bibkey>zhu-etal-2024-fanoutqa</bibkey>
      <doi>10.18653/v1/2024.acl-short.2</doi>
    </paper>
    <paper id="3">
      <title>Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance</title>
      <author><first>Yewei</first><last>Song</last></author>
      <author><first>Cedric</first><last>Lothritz</last><affiliation>Luxembourg Institute of Science and Technology</affiliation></author>
      <author><first>Xunzhu</first><last>Tang</last></author>
      <author><first>Tegawendé</first><last>Bissyandé</last><affiliation>University of Luxemburg</affiliation></author>
      <author><first>Jacques</first><last>Klein</last><affiliation>University of Luxemburg</affiliation></author>
      <pages>38-46</pages>
      <abstract>This paper revisits recent code similarity evaluation metrics, particularly focusing on the application of Abstract Syntax Tree (AST) editing distance in diverse programming languages. In particular, we explore the usefulness of these metrics and compare them to traditional sequence similarity metrics. Our experiments showcase the effectiveness of AST editing distance in capturing intricate code structures, revealing a high correlation with established metrics. Furthermore, we explore the strengths and weaknesses of AST editing distance and prompt-based GPT similarity scores in comparison to BLEU score, execution match, and Jaccard Similarity. We propose, optimize, and publish an adaptable metric that demonstrates effectiveness across all tested languages, representing an enhanced version of Tree Similarity of Edit Distance (TSED).</abstract>
      <url hash="6ae3de1d">2024.acl-short.3</url>
      <bibkey>song-etal-2024-revisiting</bibkey>
      <doi>10.18653/v1/2024.acl-short.3</doi>
    </paper>
    <paper id="4">
      <title>Resisting the Lure of the Skyline: Grounding Practices in Active Learning for Morphological Inflection</title>
      <author><first>Saliha</first><last>Muradoglu</last></author>
      <author><first>Michael</first><last>Ginn</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Miikka</first><last>Silfverberg</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Mans</first><last>Hulden</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>47-55</pages>
      <abstract>Active learning (AL) aims to lower the demand of annotation by selecting informative unannotated samples for the model building. In this paper, we explore the importance of conscious experimental design in the language documentation and description setting, particularly the distribution of the unannotated sample pool. We focus on the task of morphological inflection using a Transformer model. We propose context motivated benchmarks: a baseline and skyline. The baseline describes the frequency weighted distribution encountered in natural speech. We simulate this using Wikipedia texts. The skyline defines the more common approach, uniform sampling from a large, balanced corpus (UniMorph, in our case), which often yields mixed results. We note the unrealistic nature of this unannotated pool. When these factors are considered, our results show a clear benefit to targeted sampling.</abstract>
      <url hash="b60a8dd9">2024.acl-short.4</url>
      <bibkey>muradoglu-etal-2024-resisting</bibkey>
      <doi>10.18653/v1/2024.acl-short.4</doi>
    </paper>
    <paper id="5">
      <title>Speculative Contrastive Decoding</title>
      <author><first>Hongyi</first><last>Yuan</last></author>
      <author><first>Keming</first><last>Lu</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chang</first><last>Zhou</last></author>
      <pages>56-64</pages>
      <abstract>Large language models (LLMs) exhibit exceptional performance in language tasks, yet their auto-regressive inference is limited due to high computational requirements and is sub-optimal due to the exposure bias. Inspired by speculative decoding and contrastive decoding, we introduce Speculative Contrastive Decoding (SCD), a straightforward yet powerful decoding approach that leverages predictions from smaller language models (LMs) to achieve both decoding acceleration and quality improvement. Extensive evaluations and analyses on four diverse language tasks demonstrate the effectiveness of SCD, showing that decoding efficiency and quality can compatibly benefit from one smaller LM.</abstract>
      <url hash="d93e4b83">2024.acl-short.5</url>
      <bibkey>yuan-etal-2024-speculative</bibkey>
      <doi>10.18653/v1/2024.acl-short.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>RDR</fixed-case>ec: Rationale Distillation for <fixed-case>LLM</fixed-case>-based Recommendation</title>
      <author><first>Xinfeng</first><last>Wang</last></author>
      <author><first>Jin</first><last>Cui</last></author>
      <author><first>Yoshimi</first><last>Suzuki</last><affiliation>Yamanashi University</affiliation></author>
      <author><first>Fumiyo</first><last>Fukumoto</last><affiliation>Yamanashi University</affiliation></author>
      <pages>65-74</pages>
      <abstract>Large language model (LLM)-based recommender models that bridge users and items through textual prompts for effective semantic reasoning have gained considerable attention. However, few methods consider the underlying rationales behind interactions, such as user preferences and item attributes, limiting the reasoning ability of LLMs for recommendations. This paper proposes a rationale distillation recommender (RDRec), a compact model designed to learn rationales generated by a larger language model (LM). By leveraging rationales from reviews related to users and items, RDRec remarkably specifies their profiles for recommendations. Experiments show that RDRec achieves state-of-the-art (SOTA) performance in both top-N and sequential recommendations. Our code is available online.</abstract>
      <url hash="7a319980">2024.acl-short.6</url>
      <bibkey>wang-etal-2024-rdrec</bibkey>
      <doi>10.18653/v1/2024.acl-short.6</doi>
    </paper>
    <paper id="7">
      <title>Isotropy, Clusters, and Classifiers</title>
      <author><first>Timothee</first><last>Mickus</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Stig-Arne</first><last>Grönroos</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Joseph</first><last>Attieh</last><affiliation>University of Helsinki</affiliation></author>
      <pages>75-84</pages>
      <abstract>Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters—which also negatively impacts linear classification objectives. We demonstrate this fact both empirically and mathematically and use it to shed light on previous results from the literature.</abstract>
      <url hash="08dcd840">2024.acl-short.7</url>
      <bibkey>mickus-etal-2024-isotropy</bibkey>
      <doi>10.18653/v1/2024.acl-short.7</doi>
    </paper>
    <paper id="8">
      <title>Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy Arithmetic Tasks</title>
      <author><first>Andrew</first><last>Gambardella</last><affiliation>The University of Tokyo, Tokyo University</affiliation></author>
      <author><first>Yusuke</first><last>Iwasawa</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yutaka</first><last>Matsuo</last><affiliation>The University of Tokyo and The University of Tokyo</affiliation></author>
      <pages>85-91</pages>
      <abstract>The ability (and inability) of large language models (LLMs) to perform arithmetic tasks has been the subject of much theoretical and practical debate. We show that LLMs are frequently able to correctly and confidently predict the first digit of <tex-math>n</tex-math>-digit by <tex-math>m</tex-math>-digit multiplication tasks without using chain of thought reasoning, despite these tasks require compounding operations to solve. Simultaneously, LLMs in practice often fail to correctly or confidently predict the last digit of an <tex-math>n</tex-math>-digit by <tex-math>m</tex-math>-digit multiplication, a task equivalent to 1-digit by 1-digit multiplication which can be easily learned or memorized. We show that the latter task can be solved more robustly when the LLM is conditioned on all of the correct higher-order digits, which on average increases the confidence of the correct last digit on 5-digit by 5-digit multiplication tasks using Llama 2-13B by over 230% (0.13→0.43) and Mistral-7B by 150% (0.22→0.55).</abstract>
      <url hash="05e02ec4">2024.acl-short.8</url>
      <bibkey>gambardella-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.acl-short.8</doi>
    </paper>
    <paper id="9">
      <title>Simpson’s Paradox and the Accuracy-Fluency Tradeoff in Translation</title>
      <author><first>Zheng Wei</first><last>Lim</last></author>
      <author><first>Ekaterina</first><last>Vylomova</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Trevor</first><last>Cohn</last><affiliation>Google and The University of Melbourne</affiliation></author>
      <author><first>Charles</first><last>Kemp</last><affiliation>University of Melbourne</affiliation></author>
      <pages>92-103</pages>
      <abstract>A good translation should be faithful to the source and should respect the norms of the target language. We address a theoretical puzzle about the relationship between these objectives. On one hand, intuition and some prior work suggest that accuracy and fluency should trade off against each other, and that capturing every detail of the source can only be achieved at the cost of fluency. On the other hand, quality assessment researchers often suggest that accuracy and fluency are highly correlated and difficult for human raters to distinguish (Callison-Burch et al., 2007). We show that the tension between these views is an instance of Simpson’s paradox, and that accuracy and fluency are positively correlated at the level of the corpus but trade off at the level of individual source segments. We further suggest that the relationship between accuracy and fluency is best evaluated at the segment (or sentence) level, and that the trade off between these dimensions has implications both for assessing translation quality and developing improved MT systems.</abstract>
      <url hash="d0413145">2024.acl-short.9</url>
      <bibkey>lim-etal-2024-simpsons</bibkey>
      <doi>10.18653/v1/2024.acl-short.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>U</fixed-case>ltra<fixed-case>S</fixed-case>parse<fixed-case>BERT</fixed-case>: 99% Conditionally Sparse Language Modelling</title>
      <author><first>Peter</first><last>Belcak</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Roger</first><last>Wattenhofer</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>104-108</pages>
      <abstract>We present UltraSparseBERT, a BERT variant that uses 0.3% of its neurons during inference while performing on par with similar BERT models. UltraSparseBERT selectively engages just 12 out of 4095 neurons for each layer inference. This is achieved by reorganizing feedforward networks into fast feedforward networks (FFFs).To showcase but one benefit of high sparsity, we provide an Intel MKL implementation achieving 78x speedup over the optimized feedforward baseline on CPUs, and an OpenAI Triton implementation performing forward passes 4.1x faster than the corresponding native GPU implementation. The training and benchmarking code is enclosed.</abstract>
      <url hash="73882686">2024.acl-short.10</url>
      <bibkey>belcak-wattenhofer-2024-ultrasparsebert</bibkey>
      <doi>10.18653/v1/2024.acl-short.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>S</fixed-case>ce<fixed-case>MQA</fixed-case>: A Scientific College Entrance Level Multimodal Question Answering Benchmark</title>
      <author><first>Zhenwen</first><last>Liang</last></author>
      <author><first>Kehan</first><last>Guo</last></author>
      <author><first>Gang</first><last>Liu</last></author>
      <author><first>Taicheng</first><last>Guo</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Yujun</first><last>Zhou</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Tianyu</first><last>Yang</last></author>
      <author><first>Jiajun</first><last>Jiao</last></author>
      <author><first>Renjie</first><last>Pi</last></author>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Xiangliang</first><last>Zhang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>109-119</pages>
      <abstract>The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI models’ abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developing more capable MLLM, as highlighted by only 50% to 60% accuracy achieved by the strongest models.</abstract>
      <url hash="23e72866">2024.acl-short.11</url>
      <bibkey>liang-etal-2024-scemqa</bibkey>
      <doi>10.18653/v1/2024.acl-short.11</doi>
    </paper>
    <paper id="12">
      <title>On the Role of Long-tail Knowledge in Retrieval Augmented Large Language Models</title>
      <author><first>Dongyang</first><last>Li</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Junbing</first><last>Yan</last></author>
      <author><first>Taolin</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chengyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaofeng</first><last>He</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Longtao</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hui</first><last>Xue’</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <pages>120-126</pages>
      <abstract>Retrieval augmented generation (RAG) exhibits outstanding performance in promoting the knowledge capabilities of large language models (LLMs) with retrieved documents related to user queries. However, RAG only focuses on improving the response quality of LLMs via enhancing queries indiscriminately with retrieved information, paying little attention to what type of knowledge LLMs really need to answer original queries more accurately. In this paper, we suggest that long-tail knowledge is crucial for RAG as LLMs have already remembered common world knowledge during large-scale pre-training. Based on our observation, we propose a simple but effective long-tail knowledge detection method for LLMs. Specifically, the novel Generative Expected Calibration Error (GECE) metric is derived to measure the “long-tailness” of knowledge based on both statistics and semantics. Hence, we retrieve relevant documents and infuse them into the model for patching knowledge loopholes only when the input query relates to long-tail knowledge. Experiments show that, compared to existing RAG pipelines, our method achieves over 4x speedup in average inference time and consistent performance improvement in downstream tasks.</abstract>
      <url hash="0be5c7c7">2024.acl-short.12</url>
      <bibkey>li-etal-2024-role-long</bibkey>
      <doi>10.18653/v1/2024.acl-short.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>IEP</fixed-case>ile: Unearthing Large Scale Schema-Conditioned Information Extraction Corpus</title>
      <author><first>Honghao</first><last>Gui</last></author>
      <author><first>Lin</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hongbin</first><last>Ye</last></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Mengshu</first><last>Sun</last></author>
      <author><first>Lei</first><last>Liang</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>127-146</pages>
      <abstract>Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimentally, IEPile enhance the performance of LLMs for IE, with notable improvements in zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.</abstract>
      <url hash="3b6b9654">2024.acl-short.13</url>
      <bibkey>gui-etal-2024-iepile</bibkey>
      <doi>10.18653/v1/2024.acl-short.13</doi>
    </paper>
    <paper id="14">
      <title>Bi-Directional Multi-Granularity Generation Framework for Knowledge Graph-to-Text with Large Language Model</title>
      <author><first>Haowei</first><last>Du</last><affiliation>Peking University</affiliation></author>
      <author><first>Chen</first><last>Li</last><affiliation>HPC-AI Tech</affiliation></author>
      <author><first>Dinghao</first><last>Zhang</last></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <pages>147-152</pages>
      <abstract>The knowledge graph-to-text (KG-to-text) generation task aims to synthesize coherent and engaging sentences that accurately convey the complex information derived from an input knowledge graph. Existing methods generate the whole target text based on all KG triples at once and may incorporate incorrect KG triples for each sentence. To this end, we propose the bi-directional multi-granularity generation framework. Instead of generating the whole text at a time, we construct the sentence level generation based on the corresponding triples and generate the graph-level text as a result. Moreover, we design a backward relation extraction task to enhance the correctness of relational information. Our method achieves the new state-of-the-art in benchmark dataset WebNLG and further analysis shows the efficiency of different modules.</abstract>
      <url hash="0a71c295">2024.acl-short.14</url>
      <bibkey>du-etal-2024-bi</bibkey>
      <doi>10.18653/v1/2024.acl-short.14</doi>
    </paper>
    <paper id="15">
      <title>Code-Switching Can be Better Aligners: Advancing Cross-Lingual <fixed-case>SLU</fixed-case> through Representation-Level and Prediction-Level Alignment</title>
      <author><first>Zhihong</first><last>Zhu</last><affiliation>Tencent</affiliation></author>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Zhanpeng</first><last>Chen</last></author>
      <author><first>Xianwei</first><last>Zhuang</last></author>
      <author><first>Zhiqi</first><last>Huang</last><affiliation>Tencent Game</affiliation></author>
      <author><first>Yuexian</first><last>Zou</last><affiliation>Peking University</affiliation></author>
      <pages>153-160</pages>
      <abstract>Zero-shot cross-lingual spoken language understanding (SLU) can promote the globalization application of dialog systems, which has attracted increasing attention. While current code-switching based cross-lingual SLU frameworks have shown promising results, they (i) predominantly utilize contrastive objectives to model hard alignment, which may disrupt the inherent structure within sentences of each language; and (ii) focus optimization objectives solely on the original sentences, neglecting the relation between original sentences and code-switched sentences, which may hinder contextualized embeddings from further alignment. In this paper, we propose a novel framework dubbed REPE (short for Representation-Level and Prediction-Level Alignment), which leverages both code-switched and original sentences to achieve multi-level alignment. Specifically, REPE introduces optimal transport to facilitate soft alignment between the representations of code-switched and original sentences, thereby preserving structural integrity as much as possible. Moreover, REPE adopts multi-view learning to enforce consistency regularization between the prediction of the two sentences, aligning them into a more refined language-invariant space. Based on this, we further incorporate a self-distillation layer to boost the robustness of REPE. Extensive experiments on two benchmarks across ten languages demonstrate the superiority of the proposed REPE framework.</abstract>
      <url hash="3af4e41d">2024.acl-short.15</url>
      <bibkey>zhu-etal-2024-code</bibkey>
      <doi>10.18653/v1/2024.acl-short.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>AFL</fixed-case>o<fixed-case>RA</fixed-case>: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models</title>
      <author><first>Zeyu</first><last>Liu</last></author>
      <author><first>Souvik</first><last>Kundu</last><affiliation>Intel</affiliation></author>
      <author><first>Anni</first><last>Li</last></author>
      <author><first>Junrui</first><last>Wan</last></author>
      <author><first>Lianghao</first><last>Jiang</last></author>
      <author><first>Peter</first><last>Beerel</last><affiliation>University of Southern California</affiliation></author>
      <pages>161-167</pages>
      <abstract>We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as <tex-math>\textit{Adaptive Freezing of Low-Rank Adaptation}</tex-math> (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel <i>freezing score</i>, we incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to 0.85% as evaluated on the GLUE benchmark while yielding up to <tex-math>9.5\times</tex-math> fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to <tex-math>1.86\times</tex-math> improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the trainability requirements of LoRA paths at different modules and the freezing schedule for the different projection matrices.</abstract>
      <url hash="de2bf327">2024.acl-short.16</url>
      <bibkey>liu-etal-2024-aflora</bibkey>
      <doi>10.18653/v1/2024.acl-short.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>DDP</fixed-case>rompt: Differential Diversity Prompting in Large Language Models</title>
      <author><first>Lin</first><last>Mu</last><affiliation>Anhui University</affiliation></author>
      <author><first>Wenhao</first><last>Zhang</last></author>
      <author><first>Yiwen</first><last>Zhang</last><affiliation>Anhui University</affiliation></author>
      <author><first>Peiquan</first><last>Jin</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>168-174</pages>
      <abstract>Large Language Models (LLMs) have shown that their reasoning ability could be enhanced through approaches like Chain-of-Thought (CoT) prompting. However, these methods use single prompts for different types of questions and do not design appropriate prompts for questions with different characteristics. In this paper, we aim to explore a methodology that generates differentially diverse reasoning paths for different types of questions. To achieve this, we propose a novel prompting strategy called Differential Diversity Prompting (DDPrompt). Firstly, we generate the optimal prompts collection based on question characteristics. Then, we use this optimal prompts collection to generate multiple answers for a question and choose the final answer by voting. We evaluated DDPrompt on twelve reasoning benchmarks and significant improvement in the performance of LLMs on complex reasoning tasks (e.g., GSM8K 75%-&gt;84%, Tracking Shuffled Objects (68.8%-&gt;83.9%))</abstract>
      <url hash="58ff129d">2024.acl-short.17</url>
      <bibkey>mu-etal-2024-ddprompt</bibkey>
      <revision id="1" href="2024.acl-short.17v1" hash="8e414551"/>
      <revision id="2" href="2024.acl-short.17v2" hash="58ff129d" date="2024-08-20">Minor update.</revision>
      <doi>10.18653/v1/2024.acl-short.17</doi>
    </paper>
    <paper id="18">
      <title>Monotonic Representation of Numeric Attributes in Language Models</title>
      <author><first>Benjamin</first><last>Heinzerling</last><affiliation>RIKEN and Tohoku University</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence, RIKEN and Tohoku University</affiliation></author>
      <pages>175-195</pages>
      <abstract>Language models (LMs) can express factual knowledge involving numeric properties such as Karl Popper was born in 1902. However, how this information is encoded in the model’s internal representations is not understood well. Here, we introduce a method for finding and editing representations of numeric properties such as an entity’s birth year. We find directions that encode numeric properties monotonically, in an interpretable fashion. When editing representations along these directions, LM output changes accordingly. For example, by patching activations along a “birthyear” direction we can make the LM express an increasingly late birthyear. Property-encoding directions exist across several numeric properties in all models under consideration, suggesting the possibility that monotonic representation of numeric properties consistently emerges during LM pretraining.Code: https://github.com/bheinzerling/numeric-property-reprA long version of this short paper is available at: https://arxiv.org/abs/2403.10381</abstract>
      <url hash="e7e6ede2">2024.acl-short.18</url>
      <bibkey>heinzerling-inui-2024-monotonic</bibkey>
      <doi>10.18653/v1/2024.acl-short.18</doi>
    </paper>
    <paper id="19">
      <title>Two Issues with <fixed-case>C</fixed-case>hinese Spelling Correction and A Refinement Solution</title>
      <author><first>Changxuan</first><last>Sun</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Linlin</first><last>She</last></author>
      <author><first>Xuesong</first><last>Lu</last><affiliation>East China Normal University</affiliation></author>
      <pages>196-204</pages>
      <abstract>The Chinese Spelling Correction (CSC) task aims to detect and correct misspelled characters in Chinese text, and has received lots of attention in the past few years. Most recent studies adopt a Transformer-based model and leverage different features of characters such as pronunciation, glyph and contextual information to enhance the model’s ability to complete the task. Despite their state-of-the-art performance, we observe two issues that should be addressed to further advance the CSC task. First, the widely-used benchmark datasets SIGHAN13, SIGHAN14 and SIGHAN15, contain many mistakes. Hence the performance of existing models is not accurate and should be re-evaluated. Second, existing models seem to have reached a performance bottleneck, where the improvements on the SIGHAN’s testing sets are increasingly smaller and unstable. To deal with the two issues, we make two contributions: (1) we manually fix the SIGHAN datasets and re-evaluate four representative CSC models using the fixed datasets; (2) we analyze the new results to identify the spelling errors that none of the four models successfully corrects, based on which we propose a simple yet effective refinement solution. Experimental results show that our solution improves the four models in all metrics by notable margins.</abstract>
      <url hash="aa4d49e1">2024.acl-short.19</url>
      <bibkey>sun-etal-2024-two</bibkey>
      <doi>10.18653/v1/2024.acl-short.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>D</fixed-case>yna<fixed-case>S</fixed-case>emble: Dynamic Ensembling of Textual and Structure-Based Models for Knowledge Graph Completion</title>
      <author><first>Ananjan</first><last>Nandi</last><affiliation>Stanford University</affiliation></author>
      <author><first>Navdeep</first><last>Kaur</last><affiliation>Alan Turing Institute</affiliation></author>
      <author><first>Parag</first><last>Singla</last><affiliation>IIT Delhi</affiliation></author>
      <author><first>Mausam</first><last>.</last><affiliation>Indian Institute of Technology Delhi</affiliation></author>
      <pages>205-216</pages>
      <abstract>We consider two popular approaches to KnowledgeGraph Completion (KGC): textual modelsthat rely on textual entity descriptions, andstructure-based models that exploit the connectivitystructure of the Knowledge Graph(KG). Preliminary experiments show that theseapproaches have complementary strengths:structure-based models perform exceptionallywell when the gold answer is easily reachablefrom the query head in the KG, while textualmodels exploit descriptions to give goodperformance even when the gold answer isnot easily reachable. In response, we proposeDynaSemble, a novel method for learningquery-dependent ensemble weights to combinethese approaches by using the distributions ofscores assigned by the models in the ensembleto all candidate entities. DynaSemble achievesstate-of-the-art results on three standard KGCdatasets, with up to 6.8 pt MRR and 8.3 ptHits@1 gains over the best baseline model forthe WN18RR dataset.</abstract>
      <url hash="58a068a9">2024.acl-short.20</url>
      <bibkey>nandi-etal-2024-dynasemble</bibkey>
      <doi>10.18653/v1/2024.acl-short.20</doi>
    </paper>
    <paper id="21">
      <title>Fine-Tuning Pre-Trained Language Models with Gaze Supervision</title>
      <author><first>Shuwen</first><last>Deng</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>Paul</first><last>Prasse</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>David</first><last>Reich</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>Tobias</first><last>Scheffer</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>Lena</first><last>Jäger</last><affiliation>University of Zurich and Universität Potsdam</affiliation></author>
      <pages>217-224</pages>
      <abstract>Human gaze data provide cognitive information that reflect human language comprehension and has been effectively integrated into a variety of natural language processing (NLP) tasks, demonstrating improved performance over corresponding plain text-based models. In this work, we propose to integrate a gaze module into pre-trained language models (LMs) at the fine-tuning stage to improve their capabilities to learn representations that are grounded in human language processing. This is done by extending the conventional purely text-based fine-tuning objective with an auxiliary loss to exploit cognitive signals. The gaze module is only included during training, retaining compatibility with existing pre-trained LM-based pipelines. We evaluate the proposed approach using two distinct pre-trained LMs on the GLUE benchmark and observe that the proposed model improves performance compared to both standard fine-tuning and traditional text augmentation baselines.</abstract>
      <url hash="8bf8378a">2024.acl-short.21</url>
      <bibkey>deng-etal-2024-fine</bibkey>
      <doi>10.18653/v1/2024.acl-short.21</doi>
    </paper>
    <paper id="22">
      <title>Growing Trees on Sounds: Assessing Strategies for End-to-End Dependency Parsing of Speech</title>
      <author><first>Adrien</first><last>Pupier</last><affiliation>Université Grenoble Alpes</affiliation></author>
      <author><first>Maximin</first><last>Coavoux</last><affiliation>CNRS</affiliation></author>
      <author><first>Jérôme</first><last>Goulian</last><affiliation>Université Grenoble Alpes</affiliation></author>
      <author><first>Benjamin</first><last>Lecouteux</last><affiliation>University of Grenoble-Alpes</affiliation></author>
      <pages>225-233</pages>
      <abstract>Direct dependency parsing of the speech signal –as opposed to parsing speech transcriptions– has recently been proposed as a task (Pupier et al. 2022), as a way of incorporating prosodic information in the parsing system and bypassing the limitations of a pipeline approach that would consist of using first an Automatic Speech Recognition (ASR) system and then a syntactic parser. In this article, we report on a set of experiments aiming at assessing the performance of two parsing paradigms (graph-based parsing and sequence labeling based parsing) on speech parsing. We perform this evaluation on a large treebank of spoken French, featuring realistic spontaneous conversations. Our findings show that (i) the graph based approach obtain better results across the board (ii) parsing directly from speech outperforms a pipeline approach, despite having 30% fewer parameters.</abstract>
      <url hash="ffe95104">2024.acl-short.22</url>
      <bibkey>pupier-etal-2024-growing</bibkey>
      <doi>10.18653/v1/2024.acl-short.22</doi>
    </paper>
    <paper id="23">
      <title>Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access</title>
      <author><first>Saibo</first><last>Geng</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Berkay</first><last>Döner</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Chris</first><last>Wendler</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Martin</first><last>Josifoski</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Robert</first><last>West</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <pages>234-245</pages>
      <abstract>Constrained decoding, a technique for enforcing constraints on language model outputs, offers a way to control text generation without retraining or architectural modifications. Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs). This paper introduces sketch-guided constrained decoding (SketchGCD), a novel approach to constrained decoding for blackbox LLMs, which operates without access to the logits of the blackbox LLM. SketchGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a “sketch” for further elaboration. This approach is complementary to traditional logit-based techniques and enables the application of constrained decoding in settings where full model transparency is unavailable. We demonstrate the efficacy of SketchGCD through experiments in closed information extraction and constituency parsing, showing how it enhances the utility and flexibility of blackbox LLMs for complex NLP tasks.</abstract>
      <url hash="6bc1433b">2024.acl-short.23</url>
      <bibkey>geng-etal-2024-sketch</bibkey>
      <doi>10.18653/v1/2024.acl-short.23</doi>
    </paper>
    <paper id="24">
      <title>On the Semantic Latent Space of Diffusion-Based Text-To-Speech Models</title>
      <author><first>Miri</first><last>Varshavsky-Hassid</last><affiliation>Verily Life Sciences</affiliation></author>
      <author><first>Roy</first><last>Hirsch</last></author>
      <author><first>Regev</first><last>Cohen</last><affiliation>Google</affiliation></author>
      <author><first>Tomer</first><last>Golany</last><affiliation>Google</affiliation></author>
      <author><first>Daniel</first><last>Freedman</last><affiliation>Verily</affiliation></author>
      <author><first>Ehud</first><last>Rivlin</last><affiliation>Technion, Technion</affiliation></author>
      <pages>246-255</pages>
      <abstract>The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech (TTS) domain is rising, providing great value in synthesizing high quality speech. Although they exhibit impressive audio quality, the extent of their semantic capabilities is unknown, and controlling their synthesized speech’s vocal properties remains a challenge. Inspired by recent advances in image synthesis, we explore the latent space of frozen TTS models, which is composed of the latent bottleneck activations of the DDM’s denoiser. We identify that this space contains rich semantic information, and outline several novel methods for finding semantic directions within it, both supervised and unsupervised. We then demonstrate how these enable off-the-shelf audio editing, without any further training, architectural changes or data requirements. We present evidence of the semantic and acoustic qualities of the edited audio, and provide supplemental samples: https://latent-analysis-grad-tts.github.io/speech-samples/.</abstract>
      <url hash="c6117392">2024.acl-short.24</url>
      <bibkey>varshavsky-hassid-etal-2024-semantic</bibkey>
      <doi>10.18653/v1/2024.acl-short.24</doi>
    </paper>
    <paper id="25">
      <title>Learnable Privacy Neurons Localization in Language Models</title>
      <author><first>Ruizhe</first><last>Chen</last></author>
      <author><first>Tianxiang</first><last>Hu</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>256-264</pages>
      <abstract>Concerns regarding Large Language Models (LLMs) to memorize and disclose private information, particularly Personally Identifiable Information (PII), become prominent within the community. Many efforts have been made to mitigate the privacy risks.However, the mechanism through which LLMs memorize PII remains poorly understood. To bridge this gap, we introduce a pioneering method for pinpointing PII-sensitive neurons (privacy neurons) within LLMs. Our method employs learnable binary weight masks to localize specific neurons that account for the memorization of PII in LLMs through adversarial training. Our investigations discover that PII is memorized by a small subset of neurons across all layers, which shows the property of PII specificity. Furthermore, we propose to validate the potential in PII risk mitigation by deactivating the localized privacy neurons. Both quantitative and qualitative experiments demonstrate the effectiveness of our neuron localization algorithm.</abstract>
      <url hash="243a1f1a">2024.acl-short.25</url>
      <bibkey>chen-etal-2024-learnable</bibkey>
      <doi>10.18653/v1/2024.acl-short.25</doi>
    </paper>
    <paper id="26">
      <title>Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Non-Literal Intent Resolution in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Akhila</first><last>Yerukola</last></author>
      <author><first>Saujas</first><last>Vaduguru</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Daniel</first><last>Fried</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>265-275</pages>
      <abstract>Humans often express their communicative intents indirectly or non-literally, which requires their interlocutors—human or AI—to understand beyond the literal meaning of words. While most existing work has focused on discriminative evaluations, we present a new approach to generatively evaluate large language models’ (LLMs’) intention understanding by examining their responses to non-literal utterances. Ideally, an LLM should respond in line with the true intention of a non-literal utterance, not its literal interpretation. Our findings show that LLMs struggle to generate contextually relevant responses to non-literal language. We also find that providing oracle intentions substantially improves response appropriateness, but using chain-of-thought to make models spell out intentions before responding improves much less. These findings suggest that LLMs are not yet pragmatic interlocutors, and that explicitly modeling intention could improve LLM responses to non-literal language.</abstract>
      <url hash="e2ee04d7">2024.acl-short.26</url>
      <bibkey>yerukola-etal-2024-pope</bibkey>
      <doi>10.18653/v1/2024.acl-short.26</doi>
    </paper>
    <paper id="27">
      <title>Generating Harder Cross-document Event Coreference Resolution Datasets using Metaphoric Paraphrasing</title>
      <author><first>Shafiuddin Rehan</first><last>Ahmed</last></author>
      <author><first>Zhiyong</first><last>Wang</last></author>
      <author><first>George</first><last>Baker</last></author>
      <author><first>Kevin</first><last>Stowe</last></author>
      <author><first>James</first><last>Martin</last></author>
      <pages>276-286</pages>
      <abstract>The most popular Cross-Document Event Coreference Resolution (CDEC) datasets fail to convey the true difficulty of the task, due to the lack of lexical diversity between coreferring event triggers (words or phrases that refer to an event). Furthermore, there is a dearth of event datasets for figurative language, limiting a crucial avenue of research in event comprehension. We address these two issues by introducing ECB+META, a lexically rich variant of Event Coref Bank Plus (ECB+) for CDEC on symbolic and metaphoric language. We use ChatGPT as a tool for the metaphoric transformation of sentences in the documents of ECB+, then tag the original event triggers in the transformed sentences in a semi-automated manner. In this way, we avoid the re-annotation of expensive coreference links. We present results that show existing methods that work well on ECB+ struggle with ECB+META, thereby paving the way for CDEC research on a much more challenging dataset. Code/data: https://github.com/ahmeshaf/llms_coref</abstract>
      <url hash="17d81066">2024.acl-short.27</url>
      <bibkey>ahmed-etal-2024-generating</bibkey>
      <doi>10.18653/v1/2024.acl-short.27</doi>
    </paper>
    <paper id="28">
      <title>Soft Self-Consistency Improves Language Models Agents</title>
      <author><first>Han</first><last>Wang</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Archiki</first><last>Prasad</last></author>
      <author><first>Elias</first><last>Stengel-Eskin</last></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>287-301</pages>
      <abstract>Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current “sample and select” methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (SOFT-SC), which replaces SC’s discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. SOFT-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, SOFT-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that SOFT-SC can be applied to both open-source and black-box models.</abstract>
      <url hash="448433e3">2024.acl-short.28</url>
      <bibkey>wang-etal-2024-soft</bibkey>
      <doi>10.18653/v1/2024.acl-short.28</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>R</fixed-case>ec<fixed-case>GPT</fixed-case>: Generative Pre-training for Text-based Recommendation</title>
      <author><first>Hoang</first><last>Ngo</last><affiliation>VinAI Research</affiliation></author>
      <author><first>Dat Quoc</first><last>Nguyen</last><affiliation>VinAI Research, Vietnam</affiliation></author>
      <pages>302-313</pages>
      <abstract>We present the first domain-adapted and fully-trained large language model, RecGPT-7B, and its instruction-following variant, RecGPT-7B-Instruct, for text-based recommendation. Experimental results on rating prediction and sequential recommendation tasks show that our model, RecGPT-7B-Instruct, outperforms previous strong baselines. We are releasing our RecGPT models as well as their pre-training and fine-tuning datasets to facilitate future research and downstream applications in text-based recommendation. Public “huggingface” links to our RecGPT models and datasets are available at: https://github.com/VinAIResearch/RecGPT</abstract>
      <url hash="d4c4f5f4">2024.acl-short.29</url>
      <bibkey>ngo-nguyen-2024-recgpt</bibkey>
      <doi>10.18653/v1/2024.acl-short.29</doi>
    </paper>
    <paper id="30">
      <title><fixed-case>MTP</fixed-case>: A Dataset for Multi-Modal Turning Points in Casual Conversations</title>
      <author><first>Gia-Bao</first><last>Ho</last><affiliation>VinUniversity</affiliation></author>
      <author><first>Chang</first><last>Tan</last><affiliation>Monash University</affiliation></author>
      <author><first>Zahra</first><last>Darban</last></author>
      <author><first>Mahsa</first><last>Salehi</last><affiliation>Monash University</affiliation></author>
      <author><first>Reza</first><last>Haf</last><affiliation>Monash University</affiliation></author>
      <author><first>Wray</first><last>Buntine</last><affiliation>VinUniversity</affiliation></author>
      <pages>314-326</pages>
      <abstract>Detecting critical moments, such as emotional outbursts or changes in decisions during conversations, is crucial for understanding shifts in human behavior and their consequences. Our work introduces a novel problem setting focusing on these moments as turning points (TPs), accompanied by a meticulously curated, high-consensus, human-annotated multi-modal dataset. We provide precise timestamps, descriptions, and visual-textual evidence high-lighting changes in emotions, behaviors, perspectives, and decisions at these turning points. We also propose a framework, TPMaven, utilizing state-of-the-art vision-language models to construct a narrative from the videos and large language models to classify and detect turning points in our multi-modal dataset. Evaluation results show that TPMaven achieves an F1-score of 0.88 in classification and 0.61 in detection, with additional explanations aligning with human expectations.</abstract>
      <url hash="e6e6a73e">2024.acl-short.30</url>
      <bibkey>ho-etal-2024-mtp</bibkey>
      <doi>10.18653/v1/2024.acl-short.30</doi>
    </paper>
    <paper id="31">
      <title>What Does Parameter-free Probing Really Uncover?</title>
      <author><first>Tommi</first><last>Buder-Gröndahl</last></author>
      <pages>327-336</pages>
      <abstract>Supervised approaches to probing large language models (LLMs) have been criticized of using pre-defined theory-laden target labels. As an alternative, parameter-free probing constructs structural representations bottom-up via information derived from the LLM alone. This has been suggested to capture a genuine “LLM-internal grammar”. However, its relation to familiar linguistic formalisms remains unclear. I extend prior work on a parameter-free probing technique called perturbed masking applied to BERT, by comparing its results to the Universal Dependencies (UD) formalism for English. The results highlight several major discrepancies between BERT and UD, which lack correlates in linguistic theory. This raises the question of whether human grammar is the correct analogy to interpret BERT in the first place.</abstract>
      <url hash="3d247358">2024.acl-short.31</url>
      <bibkey>buder-grondahl-2024-parameter</bibkey>
      <doi>10.18653/v1/2024.acl-short.31</doi>
    </paper>
    <paper id="32">
      <title><fixed-case>ATLAS</fixed-case>: Improving Lay Summarisation with Attribute-based Control</title>
      <author><first>Zhihao</first><last>Zhang</last><affiliation>Beijing University of Technology</affiliation></author>
      <author><first>Tomas</first><last>Goldsack</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Carolina</first><last>Scarton</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <pages>337-345</pages>
      <abstract>Lay summarisation aims to produce summaries of scientific articles that are comprehensible to non-expert audiences. However, previous work assumes a one-size-fits-all approach, where the content and style of the produced summary are entirely dependent on the data used to train the model. In practice, audiences with different levels of expertise will have specific needs, impacting what content should appear in a lay summary and how it should be presented. Aiming to address this, we propose ATLAS, a novel abstractive summarisation approach that can control various properties that contribute to the overall “layness” of the generated summary using targeted control attributes. We evaluate ATLAS on a combination of biomedical lay summarisation datasets, where it outperforms state-of-the-art baselines using mainstream summarisation metrics.Additional analyses provided on the discriminatory power and emergent influence of our selected controllable attributes further attest to the effectiveness of our approach.</abstract>
      <url hash="c6f1cbda">2024.acl-short.32</url>
      <bibkey>zhang-etal-2024-atlas</bibkey>
      <doi>10.18653/v1/2024.acl-short.32</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>E</fixed-case>mb<fixed-case>S</fixed-case>patial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models</title>
      <author><first>Mengfei</first><last>Du</last></author>
      <author><first>Binhao</first><last>Wu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zejun</first><last>Li</last></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>346-355</pages>
      <abstract>The recent rapid development of Large Vision-Language Models (LVLMs) has indicated their potential for embodied tasks. However, the critical skill of spatial understanding in embodied environments has not been thoroughly evaluated, leaving the gap between current LVLMs and qualified embodied intelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs. The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective. Experiments expose the insufficient capacity of current LVLMs (even GPT-4V). We further present EmbSpatial-SFT, an instruction-tuning dataset designed to improve LVLMs’ embodied spatial understanding.</abstract>
      <url hash="cfeb9fed">2024.acl-short.33</url>
      <bibkey>du-etal-2024-embspatial</bibkey>
      <doi>10.18653/v1/2024.acl-short.33</doi>
    </paper>
    <paper id="34">
      <title>Understanding the Effects of Noise in Text-to-<fixed-case>SQL</fixed-case>: An Examination of the <fixed-case>BIRD</fixed-case>-Bench Benchmark</title>
      <author><first>Niklas</first><last>Wretblad</last></author>
      <author><first>Fredrik</first><last>Riseby</last></author>
      <author><first>Rahul</first><last>Biswas</last></author>
      <author><first>Amin</first><last>Ahmadi</last></author>
      <author><first>Oskar</first><last>Holmström</last></author>
      <pages>356-369</pages>
      <abstract>Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of noise, such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRD-Bench benchmark and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold SQL queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the benchmark’s reliability. Surprisingly, when evaluating models on corrected SQL queries, zero-shot baselines surpassed the performance of state-of-the-art prompting methods. We conclude that informative noise labels and reliable benchmarks are crucial to developing new Text-to-SQL methods that can handle varying types of noise.</abstract>
      <url hash="8a547cd4">2024.acl-short.34</url>
      <bibkey>wretblad-etal-2024-understanding</bibkey>
      <doi>10.18653/v1/2024.acl-short.34</doi>
    </paper>
    <paper id="35">
      <title>Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval</title>
      <author><first>João</first><last>Coelho</last></author>
      <author><first>Bruno</first><last>Martins</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Joao</first><last>Magalhaes</last><affiliation>Universidade Nova de Lisboa</affiliation></author>
      <author><first>Jamie</first><last>Callan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Chenyan</first><last>Xiong</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>370-377</pages>
      <abstract>This study investigates the existence of positional biases in Transformer-based language models for text representation learning, particularly in the context of web document retrieval. We build on previous research that demonstrated loss of information in the middle of input sequences for causal language models, extending it to the domain of embedding learning. We examine positional biases at multiple stages of the training pipeline for an encoder-decoder neural retrieval model, namely language model pre-training, contrastive pre-training, and contrastive fine-tuning. Experiments with the MS-MARCO document collection reveal that after contrastive pre-training the model already generates embeddings that better capture the beginning of the input content, with fine-tuning further aggravating this effect.</abstract>
      <url hash="fc8476e8">2024.acl-short.35</url>
      <bibkey>coelho-etal-2024-dwell</bibkey>
      <doi>10.18653/v1/2024.acl-short.35</doi>
    </paper>
    <paper id="36">
      <title>That’s Optional: A Contemporary Exploration of “that” Omission in <fixed-case>E</fixed-case>nglish Subordinate Clauses</title>
      <author><first>Ella</first><last>Rabinovich</last><affiliation>International Business Machines</affiliation></author>
      <pages>378-385</pages>
      <abstract>The Uniform Information Density (UID) hypothesis posits that speakers optimize the communicative properties of their utterances by avoiding spikes in information, thereby maintaining a relatively uniform information profile over time. This paper investigates the impact of UID principles on syntactic reduction, specifically focusing on the optional omission of the connector “that” in English subordinate clauses. Building upon previous research, we extend our investigation to a larger corpus of written English, utilize contemporary large language models (LLMs) and extend the information-uniformity principles by the notion of entropy, to estimate the UID manifestations in the usecase of syntactic reduction choices.</abstract>
      <url hash="dc8f90ce">2024.acl-short.36</url>
      <bibkey>rabinovich-2024-thats</bibkey>
      <doi>10.18653/v1/2024.acl-short.36</doi>
    </paper>
    <paper id="37">
      <title>Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?</title>
      <author><first>Haozhe</first><last>An</last><affiliation>Google and University of Maryland, College Park</affiliation></author>
      <author><first>Christabel</first><last>Acquaye</last></author>
      <author><first>Colin</first><last>Wang</last></author>
      <author><first>Zongxia</first><last>Li</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Rachel</first><last>Rudinger</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>386-397</pages>
      <abstract>We examine whether large language models (LLMs) exhibit race- and gender-based name discrimination in hiring decisions, similar to classic findings in the social sciences (Bertrand and Mullainathan, 2004). We design a series of templatic prompts to LLMs to write an email to a named job applicant informing them of a hiring decision. By manipulating the applicant’s first name, we measure the effect of perceived race, ethnicity, and gender on the probability that the LLM generates an acceptance or rejection email. We find that the hiring decisions of LLMs in many settings are more likely to favor White applicants over Hispanic applicants. In aggregate, the groups with the highest and lowest acceptance rates respectively are masculine White names and masculine Hispanic names. However, the comparative acceptance rates by group vary under different templatic settings, suggesting that LLMs’ race- and gender-sensitivity may be idiosyncratic and prompt-sensitive.</abstract>
      <url hash="7319c6fc">2024.acl-short.37</url>
      <bibkey>an-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.acl-short.37</doi>
    </paper>
    <paper id="38">
      <title>Explainability and Hate Speech: Structured Explanations Make Social Media Moderators Faster</title>
      <author><first>Agostina</first><last>Calabrese</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Leonardo</first><last>Neves</last></author>
      <author><first>Neil</first><last>Shah</last><affiliation>Snap Inc.</affiliation></author>
      <author><first>Maarten</first><last>Bos</last><affiliation>Snap Inc.</affiliation></author>
      <author><first>Björn</first><last>Ross</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Francesco</first><last>Barbieri</last><affiliation>Snap Inc.</affiliation></author>
      <pages>398-408</pages>
      <abstract>Content moderators play a key role in keeping the conversation on social media healthy. While the high volume of content they need to judge represents a bottleneck to the moderation pipeline, no studies have explored how models could support them to make faster decisions. There is, by now, a vast body of research into detecting hate speech, sometimes explicitly motivated by a desire to help improve content moderation, but published research using real content moderators is scarce. In this work we investigate the effect of explanations on the speed of real-world moderators. Our experiments show that while generic explanations do not affect their speed and are often ignored, structured explanations lower moderators’ decision making time by 7.4%.</abstract>
      <url hash="008bd31c">2024.acl-short.38</url>
      <bibkey>calabrese-etal-2024-explainability</bibkey>
      <doi>10.18653/v1/2024.acl-short.38</doi>
    </paper>
    <paper id="39">
      <title>Born Differently Makes a Difference: Counterfactual Study of Bias in Biography Generation from a Data-to-Text Perspective</title>
      <author><first>Biaoyan</first><last>Fang</last><affiliation>CSIRO</affiliation></author>
      <author><first>Ritvik</first><last>Dinesh</last><affiliation>University of Sydney, University of Sydney</affiliation></author>
      <author><first>Xiang</first><last>Dai</last><affiliation>CSIRO</affiliation></author>
      <author><first>Sarvnaz</first><last>Karimi</last><affiliation>CSIRO</affiliation></author>
      <pages>409-424</pages>
      <abstract>How do personal attributes affect biography generation? Addressing this question requires an identical pair of biographies where only the personal attributes of interest are different. However, it is rare in the real world. To address this, we propose a counterfactual methodology from a data-to-text perspective, manipulating the personal attributes of interest while keeping the co-occurring attributes unchanged. We first validate that the fine-tuned Flan-T5 model generates the biographies based on the given attributes. This work expands the analysis of gender-centered bias in text generation. Our results confirm the well-known bias in gender and also show the bias in regions, in both individual and its related co-occurring attributes in semantic machining and sentiment.</abstract>
      <url hash="b475b636">2024.acl-short.39</url>
      <bibkey>fang-etal-2024-born</bibkey>
      <doi>10.18653/v1/2024.acl-short.39</doi>
    </paper>
    <paper id="40">
      <title>Sign Language Translation with Sentence Embedding Supervision</title>
      <author><first>Yasser</first><last>Hamidullah</last></author>
      <author><first>Josef</first><last>van Genabith</last><affiliation>German Research Center for AI and Universität des Saarlandes</affiliation></author>
      <author><first>Cristina</first><last>España-Bonet</last><affiliation>German Research Center for AI</affiliation></author>
      <pages>425-434</pages>
      <abstract>State-of-the-art sign language translation (SLT) systems facilitate the learning process through gloss annotations, either in an end2end manner or by involving an intermediate step. Unfortunately, gloss labelled sign language data is usually not available at scale and, when available, gloss annotations widely differ from dataset to dataset. We present a novel approach using sentence embeddings of the target sentences at training time that take the role of glosses. The new kind of supervision does not need any manual annotation but it is learned on raw textual data. As our approach easily facilitates multilinguality, we evaluate it on datasets covering German (PHOENIX-2014T) and American (How2Sign) sign languages and experiment with mono- and multilingual sentence embeddings and translation systems. Our approach significantly outperforms other gloss-free approaches, setting the new state-of-the-art for data sets where glosses are not available and when no additional SLT datasets are used for pretraining, diminishing the gap between gloss-free and gloss-dependent systems.</abstract>
      <url hash="237e48a7">2024.acl-short.40</url>
      <bibkey>hamidullah-etal-2024-sign</bibkey>
      <doi>10.18653/v1/2024.acl-short.40</doi>
    </paper>
    <paper id="41">
      <title><fixed-case>STREAM</fixed-case>: Simplified Topic Retrieval, Exploration, and Analysis Module</title>
      <author><first>Anton</first><last>Thielmann</last><affiliation>Technische Universität Clausthal and Georg-August Universität Göttingen</affiliation></author>
      <author><first>Arik</first><last>Reuter</last><affiliation>Technische Universität Clausthal and Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Christoph</first><last>Weisser</last><affiliation>BASF</affiliation></author>
      <author><first>Gillian</first><last>Kant</last></author>
      <author><first>Manish</first><last>Kumar</last><affiliation>BASF</affiliation></author>
      <author><first>Benjamin</first><last>Säfken</last><affiliation>Technische Universität Clausthal</affiliation></author>
      <pages>435-444</pages>
      <abstract>Topic modeling is a widely used technique to analyze large document corpora. With the ever-growing emergence of scientific contributions in the field, non-technical users may often use the simplest available software module, independent of whether there are potentially better models available. We present a Simplified Topic Retrieval, Exploration, and Analysis Module (STREAM) for user-friendly topic modelling and especially subsequent interactive topic visualization and analysis. For better topic analysis, we implement multiple intruder-word based topic evaluation metrics. Additionally, we publicize multiple new datasets that can extend the so far very limited number of publicly available benchmark datasets in topic modeling. We integrate downstream interpretable analysis modules to enable users to easily analyse the created topics in downstream tasks together with additional tabular information.The code is available at the following link: https://github.com/AnFreTh/STREAM</abstract>
      <url hash="3c35666c">2024.acl-short.41</url>
      <bibkey>thielmann-etal-2024-stream</bibkey>
      <doi>10.18653/v1/2024.acl-short.41</doi>
    </paper>
    <paper id="42">
      <title><fixed-case>D</fixed-case>oc<fixed-case>F</fixed-case>in<fixed-case>QA</fixed-case>: A Long-Context Financial Reasoning Dataset</title>
      <author><first>Varshini</first><last>Reddy</last></author>
      <author><first>Rik</first><last>Koncel-Kedziorski</last><affiliation>Apple</affiliation></author>
      <author><first>Viet Dac</first><last>Lai</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Michael</first><last>Krumdick</last><affiliation>Kensho</affiliation></author>
      <author><first>Charles</first><last>Lovering</last><affiliation>Kensho</affiliation></author>
      <author><first>Chris</first><last>Tanner</last><affiliation>Massachusetts Institute of Technology and Kensho</affiliation></author>
      <pages>445-458</pages>
      <abstract>For large language models (LLMs) to be effective in the financial domain – where each decision can have a significant impact – it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents spanning hundreds of pages, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with full-document context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. Based on our experiments, DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case study on a subset of the longest documents in DocFinQA and find that models particularly struggle with these documents. Addressing these challenges may have a wide-reaching impact across applications where specificity and long-range contexts are critical, like gene sequences and legal document contract analysis. DocFinQA dataset is publicly accessible.</abstract>
      <url hash="6aef5161">2024.acl-short.42</url>
      <bibkey>reddy-etal-2024-docfinqa</bibkey>
      <doi>10.18653/v1/2024.acl-short.42</doi>
    </paper>
    <paper id="43">
      <title><fixed-case>M</fixed-case>ask<fixed-case>LID</fixed-case>: Code-Switching Language Identification through Iterative Masking</title>
      <author><first>Amir Hossein</first><last>Kargaran</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>ISIR, Sorbonne Université &amp; CNRS</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>459-469</pages>
      <abstract>We present MaskLID, a simple, yet effective, code-switching (CS) language identification (LID) method. MaskLID does not require any training and is designed to complement current high-performance sentence-level LIDs. Sentence-level LIDs are classifiers trained on monolingual texts to provide single labels, typically using a softmax layer to turn scores into probabilities. However, in cases where a sentence is composed in both L1 and L2 languages, the LID classifier often only returns the dominant label L1. To address this limitation, MaskLID employs a strategy to mask text features associated with L1, allowing the LID to classify the text as L2 in the next round. This method uses the LID itself to identify the features that require masking and does not rely on any external resource. In this work, we explore the use of MaskLID for two open-source LIDs (GlotLID and OpenLID), that are both based on the FastText architecture. Code and demo are available at https://github.com/cisnlp/MaskLID.</abstract>
      <url hash="a0202fe5">2024.acl-short.43</url>
      <bibkey>kargaran-etal-2024-masklid</bibkey>
      <doi>10.18653/v1/2024.acl-short.43</doi>
    </paper>
    <paper id="44">
      <title>An Empirical Analysis on Large Language Models in Debate Evaluation</title>
      <author><first>Xinyi</first><last>Liu</last></author>
      <author><first>Pinxin</first><last>Liu</last></author>
      <author><first>Hangfeng</first><last>He</last><affiliation>University of Rochester</affiliation></author>
      <pages>470-487</pages>
      <abstract>In this study, we investigate the capabilities and inherent biases of advanced large language models (LLMs) such as GPT-3.5 and GPT-4 in the context of debate evaluation. We discover that LLM’s performance exceeds humans and surpasses the performance of state-of-the-art methods fine-tuned on extensive datasets. We additionally explore and analyze biases present in LLMs, including positional bias, lexical bias, order bias, which may affect their evaluative judgments. Our findings reveal a consistent bias in both GPT-3.5 and GPT-4 towards the second candidate response presented, attributed to prompt design. We also uncover a lexical bias in both GPT-3.5 and GPT-4, especially when label sets carry connotations such as numerical or sequential, highlighting the critical need for careful label verbalizer selection in prompt design. Additionally, our analysis indicates a tendency of both models to favor the debate’s concluding side as the winner, suggesting an end-of-discussion bias.</abstract>
      <url hash="541c7a41">2024.acl-short.44</url>
      <bibkey>liu-etal-2024-empirical</bibkey>
      <doi>10.18653/v1/2024.acl-short.44</doi>
    </paper>
    <paper id="45">
      <title>Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains</title>
      <author><first>Vilém</first><last>Zouhar</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Shuoyang</first><last>Ding</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Anna</first><last>Currey</last><affiliation>Amazon</affiliation></author>
      <author><first>Tatyana</first><last>Badeka</last><affiliation>Amazon</affiliation></author>
      <author><first>Jenyuan</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Brian</first><last>Thompson</last><affiliation>Amazon</affiliation></author>
      <pages>488-500</pages>
      <abstract>We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain. We use this dataset to investigate whether machine translation (MT) metrics which are fine-tuned on human-generated MT quality judgements are robust to domain shifts between training and inference. We find that fine-tuned metrics exhibit a substantial performance drop in the unseen domain scenario relative to both metrics that rely on the surface form and pre-trained metrics that are not fine-tuned on MT quality judgments.</abstract>
      <url hash="d4d0d79d">2024.acl-short.45</url>
      <bibkey>zouhar-etal-2024-fine</bibkey>
      <doi>10.18653/v1/2024.acl-short.45</doi>
    </paper>
    <paper id="46">
      <title><fixed-case>I</fixed-case>ndic<fixed-case>IRS</fixed-case>uite: Multilingual Dataset and Neural Information Models for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Saiful</first><last>Haq</last></author>
      <author><first>Ashutosh</first><last>Sharma</last></author>
      <author><first>Omar</first><last>Khattab</last></author>
      <author><first>Niyati</first><last>Chhaya</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>501-509</pages>
      <abstract>In this paper, we introduce Neural Information Retrieval resources for 11 widely spoken Indian Languages (Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu) from two major Indian language families (Indo-Aryan and Dravidian). These resources include (a) INDIC-MARCO, a multilingual version of the MS MARCO dataset in 11 Indian Languages created using Machine Translation, and (b) Indic-ColBERT, a collection of 11 distinct Monolingual Neural Information Retrieval models, each trained on one of the 11 languages in the INDIC-MARCO dataset. To the best of our knowledge, IndicIRSuite is the first attempt at building large-scale Neural Information Retrieval resources for a large number of Indian languages, and we hope that it will help accelerate research in Neural IR for Indian Languages. Experiments demonstrate that Indic-ColBERT achieves 47.47% improvement in the MRR@10 score averaged over the INDIC-MARCO baselines for all 11 Indian languages except Oriya, 12.26% improvement in the NDCG@10 score averaged over the MIRACL Bengali and Hindi Language baselines, and 20% improvement in the MRR@100 Score over the Mr. Tydi Bengali Language baseline.</abstract>
      <url hash="aedd76fb">2024.acl-short.46</url>
      <bibkey>haq-etal-2024-indicirsuite</bibkey>
      <doi>10.18653/v1/2024.acl-short.46</doi>
    </paper>
    <paper id="47">
      <title><fixed-case>AGR</fixed-case>: Reinforced Causal Agent-Guided Self-explaining Rationalization</title>
      <author><first>Yunxiao</first><last>Zhao</last></author>
      <author><first>Zhiqiang</first><last>Wang</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Xiaoli</first><last>Li</last></author>
      <author><first>Jiye</first><last>Liang</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Ru</first><last>Li</last><affiliation>Shanxi University</affiliation></author>
      <pages>510-518</pages>
      <abstract>Most existing rationalization approaches are susceptible to degeneration accumulation due to a lack of effective control over the learning direction of the model during training. To address this issue, we propose a novel approach AGR (<b>A</b>gent-<b>G</b>uided <b>R</b>ationalization), guiding the next action of the model based on its current training state. Specifically, we introduce causal intervention calculus to quantify the causal effects inherent during rationale training, and utilize reinforcement learning process to refine the learning bias of them. Furthermore, we pretrain an agent within this reinforced causal environment to guide the next step of the model. We <i>theoretically</i> demonstrate that a good model needs the desired guidance, and <i>empirically</i> show the effectiveness of our approach, outperforming existing state-of-the-art methods on BeerAdvocate and HotelReview datasets.</abstract>
      <url hash="8f28dfe4">2024.acl-short.47</url>
      <bibkey>zhao-etal-2024-agr</bibkey>
      <doi>10.18653/v1/2024.acl-short.47</doi>
    </paper>
    <paper id="48">
      <title>Shoulders of Giants: A Look at the Degree and Utility of Openness in <fixed-case>NLP</fixed-case> Research</title>
      <author><first>Surangika</first><last>Ranathunga</last><affiliation>Massey University</affiliation></author>
      <author><first>Nisansa</first><last>De Silva</last><affiliation>University of Moratuwa</affiliation></author>
      <author><first>Dilith</first><last>Jayakody</last></author>
      <author><first>Aloka</first><last>Fernando</last><affiliation>University of Moratuwa</affiliation></author>
      <pages>519-529</pages>
      <abstract>We analysed a sample of NLP research papers archived in ACL Anthology as an attempt to quantify the degree of openness and the benefit of such an open culture in the NLP community. We observe that papers published in different NLP venues show different patterns related to artefact reuse. We also note that more than 30% of the papers we analysed do not release their artefacts publicly. Further, we observe a wide language-wise disparity in publicly available NLP-related artefacts.</abstract>
      <url hash="f0b36660">2024.acl-short.48</url>
      <bibkey>ranathunga-etal-2024-shoulders</bibkey>
      <doi>10.18653/v1/2024.acl-short.48</doi>
    </paper>
    <paper id="49">
      <title>The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models</title>
      <author><first>Noah</first><last>Siegel</last><affiliation>University College London, University of London and Google DeepMind</affiliation></author>
      <author><first>Oana-Maria</first><last>Camburu</last><affiliation>Department of Computer Science, University College London, University of London</affiliation></author>
      <author><first>Nicolas</first><last>Heess</last><affiliation>Google</affiliation></author>
      <author><first>Maria</first><last>Perez-Ortiz</last><affiliation>University College London, University of London</affiliation></author>
      <pages>530-546</pages>
      <abstract>In order to oversee advanced AI systems, it is important to understand their reasons for generating a given output. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are truly capturing the factors responsible for the model’s predictions: the most “human-like” explanation may be different from the one that is most faithful to the model’s true decision making process. In this work, we introduce the correlational counterfactual test (CCT), a faithfulness metric based on counterfactual input edits that takes into account not just the binary label change, but the total shift in the model’s predicted label distribution. We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama-2 family on three NLP tasks. We find that these explanations are indeed more likely to mention factors when they are impactful to the model’s prediction, with the degree of association increasing with model size but varying significantly by task.</abstract>
      <url hash="506da454">2024.acl-short.49</url>
      <bibkey>siegel-etal-2024-probabilities</bibkey>
      <doi>10.18653/v1/2024.acl-short.49</doi>
    </paper>
    <paper id="50">
      <title>Naming, Describing, and Quantifying Visual Objects in Humans and <fixed-case>LLM</fixed-case>s</title>
      <author><first>Alberto</first><last>Testoni</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Juell</first><last>Sprott</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Sandro</first><last>Pezzelle</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>547-557</pages>
      <abstract>While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision &amp; Language Large Language Models (VLLMs) can mimic this crucial feature of language use is an open question. This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy. Furthermore, similar patterns of variation are observed among human speakers for highly context-sensitive expressions, such as the quantifiers ‘few’ or ‘most’. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work. Our results reveal mixed evidence on the ability of VLLMs to capture human naming preferences at generation time: while some models are good at mimicking human distributions for nouns and attributes, all of them fail to assign quantifiers, a task that requires more accurate, high-level reasoning.</abstract>
      <url hash="08146afa">2024.acl-short.50</url>
      <bibkey>testoni-etal-2024-naming</bibkey>
      <doi>10.18653/v1/2024.acl-short.50</doi>
    </paper>
    <paper id="51">
      <title>Are <fixed-case>LLM</fixed-case>s classical or nonmonotonic reasoners? Lessons from generics</title>
      <author><first>Alina</first><last>Leidinger</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Robert</first><last>Van Rooij</last></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>558-573</pages>
      <abstract>Recent scholarship on reasoning in LLMs has supplied evidence of impressive performance and flexible adaptation to machine generated or human critique. Nonmonotonic reasoning, crucial to human cognition for navigating the real world, remains a challenging, yet understudied task. In this work, we study nonmonotonic reasoning capabilities of seven state-of-the-art LLMs in one abstract and one commonsense reasoning task featuring generics, such as ‘Birds fly’, and exceptions, ‘Penguins don’t fly’ (see Fig. 1). While LLMs exhibit reasoning patterns in accordance with human nonmonotonic reasoning abilities, they fail to maintain stable beliefs on truth conditions of generics at the addition of supporting examples (‘Owls fly’) or unrelated information (‘Lions have manes’).Our findings highlight pitfalls in attributing human reasoning behaviours to LLMs as long as consistent reasoning remains elusive.</abstract>
      <url hash="b41c56fe">2024.acl-short.51</url>
      <bibkey>leidinger-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.acl-short.51</doi>
    </paper>
    <paper id="52">
      <title><fixed-case>C</fixed-case>onstitutional<fixed-case>E</fixed-case>xperts: Training a Mixture of Principle-based Prompts</title>
      <author><first>Savvas</first><last>Petridis</last><affiliation>Google</affiliation></author>
      <author><first>Ben</first><last>Wedin</last><affiliation>Google</affiliation></author>
      <author><first>Ann</first><last>Yuan</last><affiliation>Google</affiliation></author>
      <author><first>James</first><last>Wexler</last><affiliation>Google</affiliation></author>
      <author><first>Nithum</first><last>Thain</last><affiliation>Google</affiliation></author>
      <pages>574-582</pages>
      <abstract>Large language models (LLMs) are highly capable at a variety of tasks given the right prompt, but writing one is still a difficult and tedious process. In this work, we introduce ConstitutionalExperts, a method for learning a prompt consisting of constitutional principles (i.e. rules), given a training dataset. Unlike prior methods that optimize the prompt as a single entity, our method incrementally improves the prompt by surgically editing individual principles. We also show that we can improve overall performance by learning unique prompts for different semantic regions of the training data and using a mixture-of-experts (MoE) architecture to route inputs at inference time. We compare our method to other state of the art prompt-optimization techniques across six benchmark datasets. We also investigate whether MoE improves these other techniques. Our results suggest that ConstitutionalExperts outperforms other prompt optimization techniques by 10.9% (F1) and that mixture-of-experts improves all techniques, suggesting its broad applicability.</abstract>
      <url hash="7c4a62a2">2024.acl-short.52</url>
      <bibkey>petridis-etal-2024-constitutionalexperts</bibkey>
      <doi>10.18653/v1/2024.acl-short.52</doi>
    </paper>
    <paper id="53">
      <title>Time Sensitive Knowledge Editing through Efficient Finetuning</title>
      <author><first>Xiou</first><last>Ge</last><affiliation>Apple</affiliation></author>
      <author><first>Ali</first><last>Mousavi</last><affiliation>Apple</affiliation></author>
      <author><first>Edouard</first><last>Grave</last><affiliation>Facebook</affiliation></author>
      <author><first>Armand</first><last>Joulin</last><affiliation>Facebook</affiliation></author>
      <author><first>Kun</first><last>Qian</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Benjamin</first><last>Han</last><affiliation>Apple</affiliation></author>
      <author><first>Mostafa</first><last>Arefiyan</last><affiliation>Apple</affiliation></author>
      <author><first>Yunyao</first><last>Li</last><affiliation>Adobe Systems</affiliation></author>
      <pages>583-593</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive capability in different tasks and are bringing transformative changes to many domains. However, keeping the knowledge in LLMs up-to-date remains a challenge once pretraining is complete. It is thus essential to design effective methods to both update obsolete knowledge and induce new knowledge into LLMs. Existing locate-and-edit knowledge editing (KE) method suffers from two limitations. First, the post-edit LLMs by such methods generally have poor capability in answering complex queries that require multi-hop reasoning. Second, the long run-time of such locate-and-edit methods to perform knowledge edits make it infeasible for large scale KE in practice. In this paper, we explore Parameter-Efficient Fine-Tuning (PEFT) techniques as an alternative for KE. We curate a more comprehensive temporal KE dataset with both knowledge update and knowledge injection examples for KE performance benchmarking. We further probe the effect of fine-tuning on a range of layers in an LLM for the multi-hop QA task. We find that PEFT performs better than locate-and-edit techniques for time-sensitive knowledge edits.</abstract>
      <url hash="76ced689">2024.acl-short.53</url>
      <bibkey>ge-etal-2024-time</bibkey>
      <doi>10.18653/v1/2024.acl-short.53</doi>
    </paper>
    <paper id="54">
      <title><fixed-case>PR</fixed-case>ewrite: Prompt Rewriting with Reinforcement Learning</title>
      <author><first>Weize</first><last>Kong</last><affiliation>Google</affiliation></author>
      <author><first>Spurthi</first><last>Hombaiah</last><affiliation>Google Research</affiliation></author>
      <author><first>Mingyang</first><last>Zhang</last><affiliation>Google</affiliation></author>
      <author><first>Qiaozhu</first><last>Mei</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <pages>594-601</pages>
      <abstract>Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a “trial and error” fashion that can be time consuming, ineffective, and sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?To address these problems, we investigate automated prompt engineering in this paper. Specifically, we propose PRewrite, an automated method to rewrite an under-optimized prompt to a more effective prompt. We instantiate the prompt rewriter using an LLM. The rewriter LLM is trained using reinforcement learning to optimize the performance on a given downstream task. We conduct experiments on diverse benchmark datasets, which demonstrates the effectiveness of PRewrite.</abstract>
      <url hash="10563f3a">2024.acl-short.54</url>
      <bibkey>kong-etal-2024-prewrite</bibkey>
      <doi>10.18653/v1/2024.acl-short.54</doi>
    </paper>
    <paper id="55">
      <title>Paraphrasing in Affirmative Terms Improves Negation Understanding</title>
      <author><first>MohammadHossein</first><last>Rezaei</last></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <pages>602-615</pages>
      <abstract>Negation is a common linguistic phenomenon. Yet language models face challenges with negation in many natural language understanding tasks such as question answering and natural language inference. In this paper, we experiment with seamless strategies that incorporate affirmative interpretations (i.e., paraphrases without negation) to make models more robust against negation. Crucially, our affirmative interpretations are obtained automatically. We show improvements with CondaQA, a large corpus requiring reasoning with negation, and five natural language understanding tasks.</abstract>
      <url hash="96fde3d3">2024.acl-short.55</url>
      <bibkey>rezaei-blanco-2024-paraphrasing</bibkey>
      <doi>10.18653/v1/2024.acl-short.55</doi>
    </paper>
    <paper id="56">
      <title>Exploring Conditional Variational Mechanism to <fixed-case>P</fixed-case>inyin Input Method for Addressing One-to-Many Mappings in Low-Resource Scenarios</title>
      <author><first>Bin</first><last>Sun</last></author>
      <author><first>Jianfeng</first><last>Li</last><affiliation>Wechat AI</affiliation></author>
      <author><first>Hao</first><last>Zhou</last><affiliation>Tencent, Wechat AI</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Kan</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>616-629</pages>
      <abstract>Pinyin input method engine (IME) refers to the transformation tool from pinyin sequence to Chinese characters, which is widely used on mobile phone applications. Due to the homophones, Pinyin IME suffers from the one-to-many mapping problem in the process of pinyin sequences to Chinese characters. To solve the above issue, this paper makes the first exploration to leverage an effective conditional variational mechanism (CVM) for pinyin IME. However, to ensure the stable and smooth operation of Pinyin IME under low-resource conditions (e.g., on offline mobile devices), we should balance diversity, accuracy, and efficiency with CVM, which is still challenging. To this end, we employ a novel strategy that simplifies the complexity of semantic encoding by facilitating the interaction between pinyin and the Chinese character information during the construction of continuous latent variables. Concurrently, the accuracy of the outcomes is enhanced by capitalizing on the discrete latent variables. Experimental results demonstrate the superior performance of our method.</abstract>
      <url hash="5c19e8b4">2024.acl-short.56</url>
      <bibkey>sun-etal-2024-exploring-conditional</bibkey>
      <doi>10.18653/v1/2024.acl-short.56</doi>
    </paper>
    <paper id="57">
      <title>Consistency Training by Synthetic Question Generation for Conversational Question Answering</title>
      <author><first>Hamed</first><last>Hematian Hemati</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Hamid</first><last>Beigy</last></author>
      <pages>630-639</pages>
      <abstract>Efficiently modeling historical information is a critical component in addressing user queries within a conversational question-answering (QA) context, as historical context plays a vital role in clarifying the user’s questions. However, irrelevant history induces noise in the reasoning process, especially for those questions with a considerable historical context. In our novel model-agnostic approach, referred to as **CoTaH** (**Co**nsistency-**T**rained **a**ugmented **H**istory), we augment the historical information with synthetic questions and subsequently employ consistency training to train a model that utilizes both real and augmented historical data to implicitly make the reasoning robust to irrelevant history. To the best of our knowledge, this is the first instance of research using synthetic question generation as a form of data augmentation to model conversational QA settings. By citing a common modeling error prevalent in previous research, we introduce a new baseline and compare our model’s performance against it, demonstrating an improvement in results, particularly in later turns of the conversation, when dealing with questions that include a large historical context.</abstract>
      <url hash="872b22ca">2024.acl-short.57</url>
      <bibkey>hemati-beigy-2024-consistency</bibkey>
      <doi>10.18653/v1/2024.acl-short.57</doi>
    </paper>
    <paper id="58">
      <title>How Good is Zero-Shot <fixed-case>MT</fixed-case> Evaluation for Low Resource <fixed-case>I</fixed-case>ndian Languages?</title>
      <author><first>Anushka</first><last>Singh</last></author>
      <author><first>Ananya</first><last>Sai</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Ratish</first><last>Puduppully</last><affiliation>A*STAR</affiliation></author>
      <author><first>Anoop</first><last>Kunchukuttan</last><affiliation>Microsoft and Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Mitesh</first><last>Khapra</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>640-649</pages>
      <abstract>While machine translation evaluation has been studied primarily for high-resource languages, there has been a recent interest in evaluation for low-resource languages due to the increasing availability of data and models. In this paper, we focus on a zero-shot evaluation setting focusing on low-resource Indian languages, namely Assamese, Kannada, Maithili, and Punjabi. We collect sufficient Multi-Dimensional Quality Metrics (MQM) and Direct Assessment (DA) annotations to create test sets and meta-evaluate a plethora of automatic evaluation metrics. We observe that even for learned metrics, which are known to exhibit zero-shot performance, the Kendall Tau and Pearson correlations with human annotations are only as high as 0.32 and 0.45. Synthetic data approaches show mixed results and overall do not help close the gap by much for these languages. This indicates that there is still a long way to go for low-resource evaluation.</abstract>
      <url hash="e379e93c">2024.acl-short.58</url>
      <bibkey>singh-etal-2024-good</bibkey>
      <doi>10.18653/v1/2024.acl-short.58</doi>
    </paper>
    <paper id="59">
      <title>Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages</title>
      <author><first>Mofetoluwa</first><last>Adeyemi</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Akintunde</first><last>Oladipo</last></author>
      <author><first>Ronak</first><last>Pradeep</last></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>650-656</pages>
      <abstract>Large language models (LLMs) as listwise rerankers have shown impressive zero-shot capabilities in various passage ranking tasks. Despite their success, there is still a gap in existing literature on their effectiveness in reranking low-resource languages. To address this, we investigate how LLMs function as listwise rerankers in cross-lingual information retrieval (CLIR) systems with queries in English and passages in four African languages: Hausa, Somali, Swahili, and Yoruba. We analyze and compare the effectiveness of monolingual reranking using either query or document translations. We also evaluate the effectiveness of LLMs when leveraging their own generated translations. To grasp the general picture, we examine the effectiveness of multiple LLMs — the proprietary models RankGPT-4 and RankGPT-3.5, along with the open-source model RankZephyr. While the document translation setting, i.e., both queries and documents are in English, leads to the best reranking effectiveness, our results indicate that for specific LLMs, reranking in the African language setting achieves competitive effectiveness with the cross-lingual setting, and even performs better when using the LLM’s own translations.</abstract>
      <url hash="f58e4b63">2024.acl-short.59</url>
      <bibkey>adeyemi-etal-2024-zero</bibkey>
      <doi>10.18653/v1/2024.acl-short.59</doi>
    </paper>
    <paper id="60">
      <title>Cross-Modal Projection in Multimodal <fixed-case>LLM</fixed-case>s Doesn’t Really Project Visual Attributes to Textual Space</title>
      <author><first>Gaurav</first><last>Verma</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Minje</first><last>Choi</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Kartik</first><last>Sharma</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Jamelle</first><last>Watson-Daniels</last><affiliation>Harvard University</affiliation></author>
      <author><first>Sejoon</first><last>Oh</last></author>
      <author><first>Srijan</first><last>Kumar</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>657-664</pages>
      <abstract>Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general-purpose conversations about images with the language modality. As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications. The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a large language model. It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models. To this end, via experiments on 4 datasets and under 2 fine-tuning settings, we find that as the MLLM is fine-tuned, it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual attributes. Our results indicate that the domain-specific visual attributes are modeled by the LLM, even when only the projection is fine-tuned. Through this study, we offer a potential reinterpretation of the role of cross-modal projections in MLLM architectures.</abstract>
      <url hash="250ca0ef">2024.acl-short.60</url>
      <bibkey>verma-etal-2024-cross</bibkey>
      <doi>10.18653/v1/2024.acl-short.60</doi>
    </paper>
    <paper id="61">
      <title>Guidance-Based Prompt Data Augmentation in Specialized Domains for Named Entity Recognition</title>
      <author><first>Hyeonseok</first><last>Kang</last><affiliation>Chungnam National University</affiliation></author>
      <author><first>Hyein</first><last>Seo</last><affiliation>Chungnam National University</affiliation></author>
      <author><first>Jeesu</first><last>Jung</last></author>
      <author><first>Sangkeun</first><last>Jung</last></author>
      <author><first>Du-Seong</first><last>Chang</last></author>
      <author><first>Riwoo</first><last>Chung</last></author>
      <pages>665-672</pages>
      <abstract>While the abundance of rich and vast datasets across numerous fields has facilitated the advancement of natural language processing, sectors in need of specialized data types continue to struggle with the challenge of finding quality data. Our study introduces a novel guidance data augmentation technique utilizing abstracted context and sentence structures to produce varied sentences while maintaining context-entity relationships, addressing data scarcity challenges. By fostering a closer relationship between context, sentence structure, and role of entities, our method enhances data augmentation’s effectiveness. Consequently, by showcasing diversification in both entity-related vocabulary and overall sentence structure, and simultaneously improving the training performance of named entity recognition task.</abstract>
      <url hash="215355ca">2024.acl-short.61</url>
      <bibkey>kang-etal-2024-guidance</bibkey>
      <doi>10.18653/v1/2024.acl-short.61</doi>
    </paper>
    <paper id="62">
      <title>Aligning Large Language Models via Fine-grained Supervision</title>
      <author><first>Dehong</first><last>Xu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Liang</first><last>Qiu</last><affiliation>Amazon</affiliation></author>
      <author><first>Minseok</first><last>Kim</last><affiliation>Amazon</affiliation></author>
      <author><first>Faisal</first><last>Ladhak</last><affiliation>Columbia University</affiliation></author>
      <author><first>Jaeyoung</first><last>Do</last></author>
      <pages>673-680</pages>
      <abstract>Pre-trained large-scale language models (LLMs) excel at producing coherent articles, yet their outputs may be untruthful, toxic, or fail to align with user expectations. Current approaches focus on using reinforcement learning with human feedback (RLHF) to improve model alignment, which works by transforming coarse human preferences of LLM outputs into a feedback signal that guides the model learning process. However, because this approach operates on sequence-level feedback, it lacks the precision to identify the exact parts of the output affecting user preferences. To address this gap, we propose a method to enhance LLM alignment through fine-grained token-level supervision. Specifically, we ask annotators to minimally edit less preferred responses within the standard reward modeling dataset to make them more favorable, ensuring changes are made only where necessary while retaining most of the original content. The refined dataset is used to train a token-level reward model, which is then used for training our fine-grained Proximal Policy Optimization (PPO) model. Our experiment results demonstrate that this approach can improve LLM performance by up to 5.1% in terms of win rate against the reference model, compared with the traditional PPO model.</abstract>
      <url hash="d5cb2813">2024.acl-short.62</url>
      <bibkey>xu-etal-2024-aligning</bibkey>
      <doi>10.18653/v1/2024.acl-short.62</doi>
    </paper>
    <paper id="63">
      <title>Annotating <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et via Structure-Conditioned Language Generation</title>
      <author><first>Xinyue</first><last>Cui</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Swabha</first><last>Swayamdipta</last><affiliation>University of Southern California</affiliation></author>
      <pages>681-692</pages>
      <abstract>Despite the remarkable generative capabilities of language models in producing naturalistic language, their effectiveness on explicit manipulation and generation of linguistic structures remain understudied. In this paper, we investigate the task of generating new sentences preserving a given semantic structure, following the FrameNet formalism. We propose a framework to produce novel frame-semantically annotated sentences following an overgenerate-and-filter approach. Our results show that conditioning on rich, explicit semantic information tends to produce generations with high human acceptance, under both prompting and finetuning. Our generated frame-semantic structured annotations are effective at training data augmentation for frame-semantic role labeling in low-resource settings; however, we do not see benefits under higher resource settings. Our study concludes that while generating high-quality, semantically rich data might be within reach, the downstream utility of such generations remains to be seen, highlighting the outstanding challenges with automating linguistic annotation tasks.</abstract>
      <url hash="b4cd6f0c">2024.acl-short.63</url>
      <bibkey>cui-swayamdipta-2024-annotating</bibkey>
      <doi>10.18653/v1/2024.acl-short.63</doi>
    </paper>
    <paper id="64">
      <title><fixed-case>DUAL</fixed-case>-<fixed-case>REFLECT</fixed-case>: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms</title>
      <author><first>Andong</first><last>Chen</last></author>
      <author><first>Lianzhang</first><last>Lou</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Xuefeng</first><last>Bai</last></author>
      <author><first>Yang</first><last>Xiang</last></author>
      <author><first>Muyun</first><last>Yang</last></author>
      <author><first>Tiejun</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>693-704</pages>
      <abstract>Recently, large language models (LLMs) enhanced by self-reflection have achieved promising performance on machine transla004 tion. The key idea is guiding LLMs to generate translation with human-like feedback. However, existing self-reflection methods lack effective feedback information, limiting the translation performance. To address this, we introduce a DUAL-REFLECT framework, leveraging the dual learning of translation tasks to provide effective feedback, thereby enhancing the models’ self-reflective abilities and improving translation performance. The application of this method across various translation tasks has proven its effectiveness in improving translation accuracy and eliminating ambiguities, especially in translation tasks with low-resource language pairs.</abstract>
      <url hash="28737103">2024.acl-short.64</url>
      <bibkey>chen-etal-2024-dual</bibkey>
      <doi>10.18653/v1/2024.acl-short.64</doi>
    </paper>
    <paper id="65">
      <title>Towards Artwork Explanation in Large-scale Vision Language Models</title>
      <author><first>Kazuki</first><last>Hayashi</last></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Division of Information Science, Nara Institute of Science and Technology</affiliation></author>
      <author><first>Katsuhiko</first><last>Hayashi</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>705-729</pages>
      <abstract>Large-scale Vision-Language Models (LVLMs) output text from images and instructions, demonstrating advanced capabilities in text generation and comprehension. However, it has not been clarified to what extent LVLMs understand the knowledge necessary for explaining images, the complex relationships between various pieces of knowledge, and how they integrate these understandings into their explanations. To address this issue, we propose a new task: the artwork explanation generation task, along with its evaluation dataset and metric for quantitatively assessing the understanding and utilization of knowledge about artworks. This task is apt for image description based on the premise that LVLMs are expected to have pre-existing knowledge of artworks, which are often subjects of wide recognition and documented information.It consists of two parts: generating explanations from both images and titles of artworks, and generating explanations using only images, thus evaluating the LVLMs’ language-based and vision-based knowledge.Alongside, we release a training dataset for LVLMs to learn explanations that incorporate knowledge about artworks.Our findings indicate that LVLMs not only struggle with integrating language and visual information but also exhibit a more pronounced limitation in acquiring knowledge from images alone. The datasets ExpArt=Explain Artworks are available at https://huggingface.co/datasets/naist-nlp/ExpArt</abstract>
      <url hash="fd6a04cd">2024.acl-short.65</url>
      <bibkey>hayashi-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.acl-short.65</doi>
    </paper>
    <paper id="66">
      <title>On the Hallucination in Simultaneous Machine Translation</title>
      <author><first>Meizhi</first><last>Zhong</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Zhengshan</first><last>Xue</last><affiliation>Tianjin University and OPPO</affiliation></author>
      <author><first>Lemao</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Mingming</first><last>Yang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>730-742</pages>
      <abstract>It is widely known that hallucination is a critical issue in Simultaneous Machine Translation (SiMT) due to the absence of source-side information. While many efforts have been made to enhance performance for SiMT, few of them attempt to understand and analyze hallucination in SiMT.Therefore, we conduct a comprehensive analysis of hallucination in SiMT from two perspectives: understanding the distribution of hallucination words and the target-side context usage of them.Intensive experiments demonstrate some valuable findings and particularly show that it is possible to alleviate hallucination by decreasing the over usage of target-side information for SiMT.</abstract>
      <url hash="e056260d">2024.acl-short.66</url>
      <bibkey>zhong-etal-2024-hallucination</bibkey>
      <doi>10.18653/v1/2024.acl-short.66</doi>
    </paper>
    <paper id="67">
      <title>Self-Augmented In-Context Learning for Unsupervised Word Translation</title>
      <author><first>Yaoyiran</first><last>Li</last></author>
      <author><first>Anna</first><last>Korhonen</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Ivan</first><last>Vulić</last><affiliation>University of Cambridge and PolyAI Limited</affiliation></author>
      <pages>743-753</pages>
      <abstract>Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of ‘traditional’ mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised BLI performance, we also conduct comprehensive analyses on SAIL and discuss its limitations.</abstract>
      <url hash="90e8d9b0">2024.acl-short.67</url>
      <bibkey>li-etal-2024-self-augmented</bibkey>
      <doi>10.18653/v1/2024.acl-short.67</doi>
    </paper>
    <paper id="68">
      <title><fixed-case>RAM</fixed-case>-<fixed-case>EHR</fixed-case>: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records</title>
      <author><first>Ran</first><last>Xu</last><affiliation>Emory University</affiliation></author>
      <author><first>Wenqi</first><last>Shi</last><affiliation>University of Texas Southwestern Medical Center</affiliation></author>
      <author><first>Yue</first><last>Yu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yuchen</first><last>Zhuang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Bowen</first><last>Jin</last></author>
      <author><first>May Dongmei</first><last>Wang</last></author>
      <author><first>Joyce</first><last>Ho</last><affiliation>Emory University</affiliation></author>
      <author><first>Carl</first><last>Yang</last><affiliation>Emory University</affiliation></author>
      <pages>754-765</pages>
      <abstract>We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks.</abstract>
      <url hash="0272dc8b">2024.acl-short.68</url>
      <bibkey>xu-etal-2024-ram</bibkey>
      <doi>10.18653/v1/2024.acl-short.68</doi>
    </paper>
    <paper id="69">
      <title>Estimating the Level of Dialectness Predicts Inter-annotator Agreement in Multi-dialect <fixed-case>A</fixed-case>rabic Datasets</title>
      <author><first>Amr</first><last>Keleg</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Walid</first><last>Magdy</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Sharon</first><last>Goldwater</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>766-777</pages>
      <abstract>On annotating multi-dialect Arabic datasets, it is common to randomly assign the samples across a pool of native Arabic speakers. Recent analyses recommended routing dialectal samples to native speakers of their respective dialects to build higher-quality datasets. However, automatically identifying the dialect of samples is hard. Moreover, the pool of annotators who are native speakers of specific Arabic dialects might be scarce. Arabic Level of Dialectness (ALDi) was recently introduced as a quantitative variable that measures how sentences diverge from Standard Arabic. On randomly assigning samples to annotators, we hypothesize that samples of higher ALDi scores are harder to label especially if they are written in dialects that the annotators do not speak. We test this by analyzing the relation between ALDi scores and the annotators’ agreement, on 15 public datasets having raw individual sample annotations for various sentence-classification tasks. We find strong evidence supporting our hypothesis for 11 of them. Consequently, we recommend prioritizing routing samples of high ALDi scores to native speakers of each sample’s dialect, for which the dialect could be automatically identified at higher accuracies.</abstract>
      <url hash="82a30d81">2024.acl-short.69</url>
      <bibkey>keleg-etal-2024-estimating</bibkey>
      <doi>10.18653/v1/2024.acl-short.69</doi>
    </paper>
    <paper id="70">
      <title>Estimating the Level of Dialectness Predicts Inter-annotator Agreement in Multi-dialect <fixed-case>A</fixed-case>rabic Datasets</title>
      <author><first>Amr</first><last>Keleg</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Walid</first><last>Magdy</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Sharon</first><last>Goldwater</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>778-789</pages>
      <abstract>On annotating multi-dialect Arabic datasets, it is common to randomly assign the samples across a pool of native Arabic speakers. Recent analyses recommended routing dialectal samples to native speakers of their respective dialects to build higher-quality datasets. However, automatically identifying the dialect of samples is hard. Moreover, the pool of annotators who are native speakers of specific Arabic dialects might be scarce. Arabic Level of Dialectness (ALDi) was recently introduced as a quantitative variable that measures how sentences diverge from Standard Arabic. On randomly assigning samples to annotators, we hypothesize that samples of higher ALDi scores are harder to label especially if they are written in dialects that the annotators do not speak. We test this by analyzing the relation between ALDi scores and the annotators’ agreement, on 15 public datasets having raw individual sample annotations for various sentence-classification tasks. We find strong evidence supporting our hypothesis for 11 of them. Consequently, we recommend prioritizing routing samples of high ALDi scores to native speakers of each sample’s dialect, for which the dialect could be automatically identified at higher accuracies.</abstract>
      <url hash="82a30d81">2024.acl-short.70</url>
      <bibkey>keleg-etal-2024-estimating-level</bibkey>
      <doi>10.18653/v1/2024.acl-short.70</doi>
    </paper>
    <paper id="71">
      <title>Linear-time Minimum <fixed-case>B</fixed-case>ayes Risk Decoding with Reference Aggregation</title>
      <author><first>Jannis</first><last>Vamvas</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Rico</first><last>Sennrich</last><affiliation>University of Zurich</affiliation></author>
      <pages>790-801</pages>
      <abstract>Minimum Bayes Risk (MBR) decoding is a text generation technique that has been shown to improve the quality of machine translations, but is expensive, even if a sampling-based approximation is used. Besides requiring a large number of sampled sequences, it requires the pairwise calculation of a utility metric, which has quadratic complexity. In this paper, we propose to approximate pairwise metric scores with scores calculated against aggregated reference representations. This changes the complexity of utility estimation from <tex-math>O(n^2)</tex-math> to <tex-math>O(n)</tex-math>, while empirically preserving most of the quality gains of MBR decoding. We release our source code.</abstract>
      <url hash="1749cfbd">2024.acl-short.71</url>
      <bibkey>vamvas-sennrich-2024-linear</bibkey>
      <doi>10.18653/v1/2024.acl-short.71</doi>
    </paper>
    <paper id="72">
      <title>Cleaner Pretraining Corpus Curation with Neural Web Scraping</title>
      <author><first>Zhipeng</first><last>Xu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Zhenghao</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yukun</first><last>Yan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ge</first><last>Yu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Chenyan</first><last>Xiong</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>802-812</pages>
      <abstract>The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https://github.com/OpenMatch/NeuScraper.</abstract>
      <url hash="0207af1c">2024.acl-short.72</url>
      <bibkey>xu-etal-2024-cleaner</bibkey>
      <doi>10.18653/v1/2024.acl-short.72</doi>
    </paper>
    <paper id="73">
      <title>Greed is All You Need: An Evaluation of Tokenizer Inference Methods</title>
      <author><first>Omri</first><last>Uzan</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <author><first>Craig W.</first><last>Schmidt</last><affiliation>Kensho</affiliation></author>
      <author><first>Chris</first><last>Tanner</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yuval</first><last>Pinter</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <pages>813-822</pages>
      <abstract>While subword tokenizers such as BPE and WordPiece are typically used to build vocabularies for NLP models, the method of decoding text into a sequence of tokens from these vocabularies is often left unspecified, or ill-suited to the method in which they were constructed. We provide a controlled analysis of seven tokenizer inference methods across four different algorithms and three vocabulary sizes, performed on a novel intrinsic evaluation suite we curated for English, combining measures rooted in morphology, cognition, and information theory. We show that for the most commonly used tokenizers, greedy inference performs surprisingly well; and that SaGe, a recently-introduced contextually-informed tokenizer, outperforms all others on morphological alignment.</abstract>
      <url hash="5f2fb137">2024.acl-short.73</url>
      <bibkey>uzan-etal-2024-greed</bibkey>
      <doi>10.18653/v1/2024.acl-short.73</doi>
    </paper>
    <paper id="74">
      <title>What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for <fixed-case>G</fixed-case>erman Dialects</title>
      <author><first>Verena</first><last>Blaschke</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Christoph</first><last>Purschke</last><affiliation>University of Luxemburg</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>823-841</pages>
      <abstract>Natural language processing (NLP) has largely focused on modelling standardized languages. More recently, attention has increasingly shifted to local, non-standardized languages and dialects. However, the relevant speaker populations’ needs and wishes with respect to NLP tools are largely unknown. In this paper, we focus on dialects and regional languages related to German – a group of varieties that is heterogeneous in terms of prestige and standardization. We survey speakers of these varieties (N=327) and present their opinions on hypothetical language technologies for their dialects. Although attitudes vary among subgroups of our respondents, we find that respondents are especially in favour of potential NLP tools that work with dialectal input (especially audio input) such as virtual assistants, and less so for applications that produce dialectal output such as machine translation or spellcheckers.</abstract>
      <url hash="5c2986f9">2024.acl-short.74</url>
      <bibkey>blaschke-etal-2024-dialect</bibkey>
      <doi>10.18653/v1/2024.acl-short.74</doi>
    </paper>
    <paper id="75">
      <title><fixed-case>S</fixed-case>ee<fixed-case>GULL</fixed-case> Multilingual: a Dataset of Geo-Culturally Situated Stereotypes</title>
      <author><first>Mukul</first><last>Bhutani</last><affiliation>Google</affiliation></author>
      <author><first>Kevin</first><last>Robinson</last><affiliation>Google Research</affiliation></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last><affiliation>Google</affiliation></author>
      <author><first>Shachi</first><last>Dave</last><affiliation>Research, Google</affiliation></author>
      <author><first>Sunipa</first><last>Dev</last><affiliation>Google</affiliation></author>
      <pages>842-854</pages>
      <abstract>While generative multilingual models are rapidly being deployed, their safety and fairness evaluations are largely limited to resources collected in English. This is especially problematic for evaluations targeting inherently socio-cultural phenomena such as stereotyping, where it is important to build multilingual resources that reflect the stereotypes prevalent in respective language communities. However, gathering these resources, at scale, in varied languages and regions pose a significant challenge as it requires broad socio-cultural knowledge and can also be prohibitively expensive. To overcome this critical gap, we employ a recently introduced approach that couples LLM generations for scale with culturally situated validations for reliability, and build SeeGULL Multilingual, a global-scale multilingual dataset of social stereotypes, containing over 25K stereotypes, spanning 23 pairs of languages and regions they are common in, with human annotations, and demonstrate its utility in identifying gaps in model evaluations.</abstract>
      <url hash="805ace3b">2024.acl-short.75</url>
      <bibkey>bhutani-etal-2024-seegull</bibkey>
      <doi>10.18653/v1/2024.acl-short.75</doi>
    </paper>
    <paper id="76">
      <title>Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models</title>
      <author><first>Zachary</first><last>Horvitz</last><affiliation>Columbia University</affiliation></author>
      <author><first>Jingru</first><last>Chen</last><affiliation>Columbia University</affiliation></author>
      <author><first>Rahul</first><last>Aditya</last><affiliation>Columbia University</affiliation></author>
      <author><first>Harshvardhan</first><last>Srivastava</last><affiliation>Columbia University</affiliation></author>
      <author><first>Robert</first><last>West</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Kathleen</first><last>McKeown</last><affiliation>Columbia University</affiliation></author>
      <pages>855-869</pages>
      <abstract>Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. We investigate whether large language models (LLMs) can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to “unfun” jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset where we find that GPT-4’s synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.</abstract>
      <url hash="9b372b05">2024.acl-short.76</url>
      <bibkey>horvitz-etal-2024-getting</bibkey>
      <doi>10.18653/v1/2024.acl-short.76</doi>
    </paper>
    <paper id="77">
      <title>Don’t Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models</title>
      <author><first>Anna</first><last>Bavaresco</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Alberto</first><last>Testoni</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Raquel</first><last>Fernández</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>870-879</pages>
      <abstract>Image-based advertisements are complex multimodal stimuli that often contain unusual visual elements and figurative language. Previous research on automatic ad understanding has reported impressive zero-shot accuracy of contrastive vision-and-language models (VLMs) on an ad-explanation retrieval task. Here, we examine the original task setup and show that contrastive VLMs can solve it by exploiting grounding heuristics. To control for this confound, we introduce TRADE, a new evaluation test set with adversarial grounded explanations. While these explanations look implausible to humans, we show that they “fool” four different contrastive VLMs. Our findings highlight the need for an improved operationalisation of automatic ad understanding that truly evaluates VLMs’ multimodal reasoning abilities. We make our code and TRADE available at https://github.com/dmg-illc/trade.</abstract>
      <url hash="41cd2a31">2024.acl-short.77</url>
      <bibkey>bavaresco-etal-2024-dont</bibkey>
      <doi>10.18653/v1/2024.acl-short.77</doi>
    </paper>
  </volume>
  <volume id="demos" ingest-date="2024-08-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)</booktitle>
      <editor><first>Yixin</first><last>Cao</last><affiliation>Singapore Management University</affiliation></editor>
      <editor><first>Yang</first><last>Feng</last><affiliation>Chinese Academy of Science</affiliation></editor>
      <editor><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="b3e6880b">2024.acl-demos</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="3a2bc2c7">2024.acl-demos.0</url>
      <bibkey>acl-2024-demos</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>PAI</fixed-case>-Diffusion: Constructing and Serving a Family of Open <fixed-case>C</fixed-case>hinese Diffusion Models for Text-to-image Synthesis on the Cloud</title>
      <author><first>Chengyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhongjie</first><last>Duan</last></author>
      <author><first>Bingyan</first><last>Liu</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Xinyi</first><last>Zou</last></author>
      <author><first>Cen</first><last>Chen</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Kui</first><last>Jia</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Jun</first><last>Huang</last></author>
      <pages>1-8</pages>
      <abstract>Text-to-image synthesis for the Chinese language poses unique challenges due to its large vocabulary size, and intricate character relationships. While existing diffusion models have shown promise in generating images from textual descriptions, they often neglect domain-specific contexts and lack robustness in handling the Chinese language. This paper introduces PAI-Diffusion, a comprehensive framework that addresses these limitations. PAI-Diffusion incorporates both general and domain-specific Chinese diffusion models, enabling the generation of contextually relevant images. It explores the potential of using LoRA and ControlNet for fine-grained image style transfer and image editing, empowering users with enhanced control over image generation. Moreover, PAI-Diffusion seamlessly integrates with Alibaba Cloud’s Platform for AI, providing accessible and scalable solutions. All the Chinese diffusion model checkpoints, LoRAs, and ControlNets, including domain-specific ones, are publicly available. A user-friendly Chinese WebUI and the diffusers-api elastic inference toolkit, also open-sourced, further facilitate the easy deployment of PAI-Diffusion models in various local and cloud environments, making it a valuable resource for Chinese text-to-image synthesis.</abstract>
      <url hash="986d43cd">2024.acl-demos.1</url>
      <bibkey>wang-etal-2024-pai</bibkey>
      <doi>10.18653/v1/2024.acl-demos.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>O</fixed-case>pen<fixed-case>VNA</fixed-case>: A Framework for Analyzing the Behavior of Multimodal Language Understanding System under Noisy Scenarios</title>
      <author><first>Ziqi</first><last>Yuan</last></author>
      <author><first>Baozheng</first><last>Zhang</last></author>
      <author><first>Hua</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyun</first><last>Liang</last></author>
      <author><first>Kai</first><last>Gao</last></author>
      <pages>9-18</pages>
      <abstract>We present OpenVNA, an open-source framework designed for analyzing the behavior of multimodal language understanding systems under noisy conditions. OpenVNA serves as an intuitive toolkit tailored for researchers, facilitating convenience batch-level robustness evaluation and on-the-fly instance-level demonstration. It primarily features a benchmark Python library for assessing global model robustness, offering high flexibility and extensibility, thereby enabling customization with user-defined noise types and models. Additionally, a GUI-based interface has been developed to intuitively analyze local model behavior. In this paper, we delineate the design principles and utilization of the created library and GUI-based web platform. Currently, OpenVNA is publicly accessible at <url>https://github.com/thuiar/OpenVNA</url>, with a demonstration video available at <url>https://youtu.be/0Z9cW7RGct4</url>.</abstract>
      <url hash="39fbc8c0">2024.acl-demos.2</url>
      <bibkey>yuan-etal-2024-openvna</bibkey>
      <doi>10.18653/v1/2024.acl-demos.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>XNLP</fixed-case>: An Interactive Demonstration System for Universal Structured <fixed-case>NLP</fixed-case></title>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China and Tianjin University, China</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>19-30</pages>
      <abstract>Structured Natural Language Processing (XNLP) is an important subset of NLP that entails understanding the underlying semantic or syntactic structure of texts, which serves as a foundational component for many downstream applications. Despite certain recent efforts to explore universal solutions for specific categories of XNLP tasks, a comprehensive and effective approach for unifying all XNLP tasks long remains underdeveloped. Meanwhile, while XNLP demonstration systems are vital for researchers exploring various XNLP tasks, existing platforms can be limited to, e.g., supporting few XNLP tasks, lacking interactivity and universalness. To this end, we propose an advanced XNLP demonstration system, where we leverage LLM to achieve universal XNLP, with one model for all with high generalizability. Overall, our system advances in multiple aspects, including universal XNLP modeling, high performance, interpretability, scalability, and interactivity, offering a unified platform for exploring diverse XNLP tasks in the community.</abstract>
      <url hash="9fe5bb3e">2024.acl-demos.3</url>
      <bibkey>fei-etal-2024-xnlp</bibkey>
      <doi>10.18653/v1/2024.acl-demos.3</doi>
    </paper>
    <paper id="4">
      <title>Towards the <fixed-case>T</fixed-case>op<fixed-case>M</fixed-case>ost: A Topic Modeling System Toolkit</title>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Fengjun</first><last>Pan</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>31-41</pages>
      <abstract>Topic models have a rich history with various applications and have recently been reinvigorated by neural topic modeling. However, these numerous topic models adopt totally distinct datasets, implementations, and evaluations. This impedes quick utilization and fair comparisons, and thereby hinders their research progress and applications. To tackle this challenge, we in this paper propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by supporting more extensive features. It covers a broader spectrum of topic modeling scenarios with their complete lifecycles, including datasets, preprocessing, models, training, and evaluations. Thanks to its highly cohesive and decoupled modular design, TopMost enables rapid utilization, fair comparisons, and flexible extensions of diverse cutting-edge topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.</abstract>
      <url hash="09f44af9">2024.acl-demos.4</url>
      <bibkey>wu-etal-2024-towards-topmost</bibkey>
      <doi>10.18653/v1/2024.acl-demos.4</doi>
    </paper>
    <paper id="5">
      <title>Wordflow: Social Prompt Engineering for Large Language Models</title>
      <author><first>Zijie</first><last>Wang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Aishwarya</first><last>Chakravarthy</last></author>
      <author><first>David</first><last>Munechika</last></author>
      <author><first>Duen Horng</first><last>Chau</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>42-50</pages>
      <abstract>Large language models (LLMs) require well-crafted prompts for effective use. Prompt engineering, the process of designing prompts, is challenging, particularly for non-experts who are less familiar with AI technologies. While researchers have proposed techniques and tools to assist LLM users in prompt design, these works primarily target AI application developers rather than non-experts. To address this research gap, we propose social prompt engineering, a novel paradigm that leverages social computing techniques to facilitate collaborative prompt design. To investigate social prompt engineering, we introduce Wordflow, an open-source and social text editor that enables everyday users to easily create, run, share, and discover LLM prompts. Additionally, by leveraging modern web technologies, Wordflow allows users to run LLMs locally and privately in their browsers. Two usage scenarios highlight how social prompt engineering and our tool can enhance laypeople’s interaction with LLMs. Wordflow is publicly accessible at https://poloclub.github.io/wordflow.</abstract>
      <url hash="96ad5bcb">2024.acl-demos.5</url>
      <bibkey>wang-etal-2024-wordflow</bibkey>
      <doi>10.18653/v1/2024.acl-demos.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>LM</fixed-case> Transparency Tool: Interactive Tool for Analyzing Transformer Language Models</title>
      <author><first>Igor</first><last>Tufanov</last><affiliation>Facebook</affiliation></author>
      <author><first>Karen</first><last>Hambardzumyan</last><affiliation>Facebook and University College London, University of London</affiliation></author>
      <author><first>Javier</first><last>Ferrando</last></author>
      <author><first>Elena</first><last>Voita</last><affiliation>FAIR at Meta AI and University of Amsterdam</affiliation></author>
      <pages>51-60</pages>
      <abstract>We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models. Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model. Specifically, it (i) shows the important part of the whole input-to-output information flow, (ii) allows attributing any changes done by a model block to individual attention heads and feed-forward neurons, (iii) allows interpreting the functions of those heads or neurons. A crucial part of this pipeline is showing the importance of specific model components at each step. As a result, we are able to look at the roles of model components only in cases where they are important for a prediction. Since knowing which components should be inspected is key for analyzing large models where the number of these components is extremely high, we believe our tool will greatly support the interpretability community both in research settings and in practical applications.</abstract>
      <url hash="cd66af42">2024.acl-demos.6</url>
      <bibkey>tufanov-etal-2024-lm</bibkey>
      <doi>10.18653/v1/2024.acl-demos.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>E</fixed-case>mpathy<fixed-case>E</fixed-case>ar: An Open-source Avatar Multimodal Empathetic Chatbot</title>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Han</first><last>Zhang</last><affiliation>Xidian University</affiliation></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Qian</first><last>Liu</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>61-71</pages>
      <abstract>This paper introduces EmpathyEar, a pioneering open-source, avatar-based multimodal empathetic chatbot, to fill the gap in traditional text-only empathetic response generation (ERG) systems. Leveraging the advancements of a large language model, combined with multimodal encoders and generators, EmpathyEar supports user inputs in any combination of text, sound, and vision, and produces multimodal empathetic responses, offering users, not just textual responses but also digital avatars with talking faces and synchronized speeches. A series of emotion-aware instruction-tuning is performed for comprehensive emotional understanding and generation capabilities. In this way, EmpathyEar provides users with responses that achieve a deeper emotional resonance, closely emulating human-like empathy. The system paves the way for the next emotional intelligence, for which we open-source the code for public access.</abstract>
      <url hash="a7c26a34">2024.acl-demos.7</url>
      <bibkey>fei-etal-2024-empathyear</bibkey>
      <doi>10.18653/v1/2024.acl-demos.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>O</fixed-case>pen<fixed-case>W</fixed-case>eb<fixed-case>A</fixed-case>gent: An Open Toolkit to Enable Web Agents on Large Language Models</title>
      <author><first>Iat Long</first><last>Iong</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Yuxuan</first><last>Chen</last></author>
      <author><first>Hanyu</first><last>Lai</last></author>
      <author><first>Shuntian</first><last>Yao</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Pengbo</first><last>Shen</last></author>
      <author><first>Hao</first><last>Yu</last></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>72-81</pages>
      <abstract>We introduce OpenWebAgent, an open toolkit designed to optimize web automation by integrating both large language models (LLMs) and large multimodal models (LMMs). This toolkit focuses on enhancing human-computer interactions on the web, simplifying complex tasks through an advanced HTML parser, a rapid action generation module, and an intuitive user interface. At the core of OpenWebAgent is an innovative web agent framework that uses a modular design to allow developers to seamlessly integrate a variety of models and tools to process web information and automate tasks on the web. This enables the development of powerful, task-oriented web agents, significantly enhancing user experience and operational efficiency on the web. The OpenWebAgent framework, Chrome plugin, and demo video are available at https://github.com/THUDM/OpenWebAgent/.</abstract>
      <url hash="68ef5c4a">2024.acl-demos.8</url>
      <bibkey>iong-etal-2024-openwebagent</bibkey>
      <doi>10.18653/v1/2024.acl-demos.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>E</fixed-case>asy<fixed-case>E</fixed-case>dit: An Easy-to-use Knowledge Editing Framework for Large Language Models</title>
      <author><first>Peng</first><last>Wang</last></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Bozhong</first><last>Tian</last></author>
      <author><first>Zekun</first><last>Xi</last></author>
      <author><first>Yunzhi</first><last>Yao</last></author>
      <author><first>Ziwen</first><last>Xu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Mengru</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Shengyu</first><last>Mao</last></author>
      <author><first>Xiaohan</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Siyuan</first><last>Cheng</last></author>
      <author><first>Kangwei</first><last>Liu</last></author>
      <author><first>Yuansheng</first><last>Ni</last></author>
      <author><first>Guozhou</first><last>Zheng</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>82-93</pages>
      <abstract>Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged – aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub, along with Google Colab tutorials and comprehensive documentation for beginners to get started. Besides, we present an online system for real-time knowledge editing, and a demo video.</abstract>
      <url hash="0fbd3741">2024.acl-demos.9</url>
      <bibkey>wang-etal-2024-easyedit</bibkey>
      <doi>10.18653/v1/2024.acl-demos.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>E</fixed-case>asy<fixed-case>I</fixed-case>nstruct: An Easy-to-use Instruction Processing Framework for Large Language Models</title>
      <author><first>Yixin</first><last>Ou</last></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Honghao</first><last>Gui</last></author>
      <author><first>Ziwen</first><last>Xu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Shuofei</first><last>Qiao</last></author>
      <author><first>Runnan</first><last>Fang</last></author>
      <author><first>Lei</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Zhen</first><last>Bi</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Guozhou</first><last>Zheng</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>94-106</pages>
      <abstract>In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at Github, along with an online demo app and a demo video for quick-start, calling for broader research centered on instruction data and synthetic data.</abstract>
      <url hash="ca4e6d15">2024.acl-demos.10</url>
      <bibkey>ou-etal-2024-easyinstruct</bibkey>
      <doi>10.18653/v1/2024.acl-demos.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>B</fixed-case>ot<fixed-case>E</fixed-case>val: Facilitating Interactive Human Evaluation</title>
      <author><first>Hyundong</first><last>Cho</last><affiliation>USC/ISI</affiliation></author>
      <author><first>Thamme</first><last>Gowda</last><affiliation>Microsoft Translator</affiliation></author>
      <author><first>Yuyang</first><last>Huang</last></author>
      <author><first>Zixun</first><last>Lu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Tianli</first><last>Tong</last></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>107-116</pages>
      <abstract>Following the rapid progress in natural language processing (NLP) models, language models are applied to increasingly more complex interactive tasks such as negotiations and conversation moderations. Having human evaluators directly interact with these NLP models is essential for adequately evaluating the performance on such interactive tasks. We develop BotEval, an easily customizable, open-source, evaluation toolkit that focuses on enabling human-bot interactions as part of the evaluation process, as opposed to human evaluators making judgements for a static input. BotEval balances flexibility for customization and user-friendliness by providing templates for common use cases that span various degrees of complexity and built-in compatibility with popular crowdsourcing platforms.We showcase the numerous useful features of BotEval through a study that evaluates the performance of various chatbots on their effectiveness for conversational moderation and discuss how BotEval differs from other annotation tools.</abstract>
      <url hash="b1b93542">2024.acl-demos.11</url>
      <bibkey>cho-etal-2024-boteval</bibkey>
      <doi>10.18653/v1/2024.acl-demos.11</doi>
    </paper>
    <paper id="12">
      <title><fixed-case>G</fixed-case>en<fixed-case>GO</fixed-case>: <fixed-case>ACL</fixed-case> Paper Explorer with Semantic Features</title>
      <author><first>Sotaro</first><last>Takeshita</last><affiliation>Universit�t Mannheim</affiliation></author>
      <author><first>Simone</first><last>Ponzetto</last><affiliation>University of Mannheim</affiliation></author>
      <author><first>Kai</first><last>Eckert</last><affiliation>Mannheim University of Applied Sciences</affiliation></author>
      <pages>117-126</pages>
      <abstract>We present GenGO, a system for exploring papers published in ACL conferences. Paper data stored in our database is enriched with multi-aspect summaries, extracted named entities, a field of study label, and text embeddings by our data processing pipeline. These metadata are used in our web-based user interface to enable researchers to quickly find papers relevant to their interests, and grasp an overview of papers without reading full-text of papers. To make GenGO to be available online as long as possible, we design GenGO to be simple and efficient to reduce maintenance and financial costs. In addition, the modularity of our data processing pipeline lets developers easily extend it to add new features. We make our code available to foster open development and transparency: https://gengo.sotaro.io.</abstract>
      <url hash="360aa349">2024.acl-demos.12</url>
      <bibkey>takeshita-etal-2024-gengo</bibkey>
      <doi>10.18653/v1/2024.acl-demos.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>NLP</fixed-case>-<fixed-case>KG</fixed-case>: A System for Exploratory Search of Scientific Literature in Natural Language Processing</title>
      <author><first>Tim</first><last>Schopf</last></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universit�t M�nchen</affiliation></author>
      <pages>127-135</pages>
      <abstract>Scientific literature searches are often exploratory, whereby users are not yet familiar with a particular field or concept but are interested in learning more about it. However, existing systems for scientific literature search are typically tailored to keyword-based lookup searches, limiting the possibilities for exploration. We propose NLP-KG, a feature-rich system designed to support the exploration of research literature in unfamiliar natural language processing (NLP) fields. In addition to a semantic search, NLP-KG allows users to easily find survey papers that provide a quick introduction to a field of interest. Further, a Fields of Study hierarchy graph enables users to familiarize themselves with a field and its related areas. Finally, a chat interface allows users to ask questions about unfamiliar concepts or specific articles in NLP and obtain answers grounded in knowledge retrieved from scientific publications. Our system provides users with comprehensive exploration possibilities, supporting them in investigating the relationships between different fields, understanding unfamiliar concepts in NLP, and finding relevant research literature. Demo, video, and code are available at: https://github.com/NLP-Knowledge-Graph/NLP-KG-WebApp.</abstract>
      <url hash="5a87cd68">2024.acl-demos.13</url>
      <bibkey>schopf-matthes-2024-nlp</bibkey>
      <doi>10.18653/v1/2024.acl-demos.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>L</fixed-case>ocal<fixed-case>RQA</fixed-case>: From Generating Data to Locally Training, Testing, and Deploying Retrieval-Augmented <fixed-case>QA</fixed-case> Systems</title>
      <author><first>Xiao</first><last>Yu</last></author>
      <author><first>Yunan</first><last>Lu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>136-151</pages>
      <abstract>Retrieval-augmented question-answering systems combine retrieval techniques with large language models to provide answers that are more accurate and informative. Many existing toolkits allow users to quickly build such systems using off-the-shelf models, but they fall short in supporting researchers and developers to customize the *model training, testing, and deployment process*. We propose LocalRQA, an open-source toolkit that features a wide selection of model training algorithms, evaluation methods, and deployment tools curated from the latest research. As a showcase, we build QA systems using online documentation obtained from Databricks and Faire’s websites. We find 7B-models trained and deployed using LocalRQA reach a similar performance compared to using OpenAI’s text-ada-002 and GPT-4-turbo.</abstract>
      <url hash="b2cde0c9">2024.acl-demos.14</url>
      <bibkey>yu-etal-2024-localrqa</bibkey>
      <doi>10.18653/v1/2024.acl-demos.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>JORA</fixed-case>: <fixed-case>JAX</fixed-case> Tensor-Parallel <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Library for Retrieval Augmented Fine-Tuning</title>
      <author><first>Anique</first><last>Tahir</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Lu</first><last>Cheng</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Huan</first><last>Liu</last><affiliation>Arizona State University</affiliation></author>
      <pages>152-159</pages>
      <abstract>The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences. Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of GPT models, leveraging distributed training. Our framework uniquely utilizes JAX’s just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources. Our experiments show more than 12x improvement in runtime compared to Hugging Face/DeepSpeed implementation with four GPUs while consuming less than half the VRAM per GPU.</abstract>
      <url hash="fbc13051">2024.acl-demos.15</url>
      <bibkey>tahir-etal-2024-jora</bibkey>
      <doi>10.18653/v1/2024.acl-demos.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>L</fixed-case>ingua<fixed-case>L</fixed-case>inked: Distributed Large Language Model Inference on Mobile Devices</title>
      <author><first>Junchen</first><last>Zhao</last></author>
      <author><first>Yurun</first><last>Song</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Simenl3@uci.edu</first><last>Simenl3@uci.edu</last><affiliation>NA</affiliation></author>
      <author><first>Ian</first><last>Harris</last><affiliation>University of California-Irvine</affiliation></author>
      <author><first>Sangeetha</first><last>Abdu Jyothi</last><affiliation>University of California, Irvine</affiliation></author>
      <pages>160-171</pages>
      <abstract>Deploying Large Language Models (LLMs) locally on mobile devices presents a significant challenge due to their extensive memory requirements. In this paper, we introduce LinguaLinked, a system for decentralized, distributed LLM inference on mobile devices. LinguaLinked enables collaborative execution of the inference task across multiple trusted devices and ensures data privacy by processing information locally. LinguaLinked uses three key strategies. First, an optimized model assignment technique segments LLMs and uses linear optimization to align segments with each device�s capabilities. Second, an optimized data transmission mechanism ensures efficient and structured data flow between model segments while also maintaining the integrity of the original model structure. Finally, LinguaLinked incorporates a runtime load balancer that actively monitors and redistributes tasks among mobile devices to prevent bottlenecks, enhancing the system�s overall efficiency and responsiveness. We demonstrate that LinguaLinked facilitates efficient LLM inference while maintaining consistent throughput and minimal latency through extensive testing across various mobile devices, from high-end to low-end Android devices.</abstract>
      <url hash="40859d08">2024.acl-demos.16</url>
      <bibkey>zhao-etal-2024-lingualinked</bibkey>
      <doi>10.18653/v1/2024.acl-demos.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>IMGTB</fixed-case>: A Framework for Machine-Generated Text Detection Benchmarking</title>
      <author><first>Michal</first><last>Spiegel</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Dominik</first><last>Macko</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <pages>172-179</pages>
      <abstract>In the era of large language models generating high quality texts, it is a necessity to develop methods for detection of machine-generated text to avoid their harmful use or simply for annotation purposes. It is, however, also important to properly evaluate and compare such developed methods. Recently, a few benchmarks have been proposed for this purpose; however, integration of newest detection methods is rather challenging, since new methods appear each month and provide slightly different evaluation pipelines.In this paper, we present the IMGTB framework, which simplifies the benchmarking of machine-generated text detection methods by easy integration of custom (new) methods and evaluation datasets. In comparison to existing frameworks, it enables to objectively compare statistical metric-based zero-shot detectors with classification-based detectors and with differently fine-tuned detectors. Its configurability and flexibility makes research and development of new detection methods easier, especially their comparison to the existing state-of-the-art detectors. The default set of analyses, metrics and visualizations offered by the tool follows the established practices of machine-generated text detection benchmarking found in state-of-the-art literature.</abstract>
      <url hash="09bd2a5d">2024.acl-demos.17</url>
      <bibkey>spiegel-macko-2024-imgtb</bibkey>
      <doi>10.18653/v1/2024.acl-demos.17</doi>
    </paper>
    <paper id="18">
      <title><fixed-case>D</fixed-case>rug<fixed-case>W</fixed-case>atch: A Comprehensive Multi-Source Data Visualisation Platform for Drug Safety Information</title>
      <author><first>Artem</first><last>Bobrov</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Domantas</first><last>Saltenis</last></author>
      <author><first>Zhaoyue</first><last>Sun</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Gabriele</first><last>Pergola</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>180-189</pages>
      <abstract>Drug safety research is crucial for maintaining public health, often requiring comprehensive data support. However, the resources currently available to the public are limited and fail to provide a comprehensive understanding of the relationship between drugs and their side effects. This paper introduces “DrugWatch”, an easy-to-use and interactive multi-source information visualisation platform for drug safety study. It allows users to understand common side effects of drugs and their statistical information, flexibly retrieve relevant medical reports, or annotate their own medical texts with our automated annotation tool. Supported by NLP technology and enriched with interactive visual components, we are committed to providing researchers and practitioners with a one-stop information analysis, retrieval, and annotation service. The demonstration video is available at https://www.youtube.com/watch?v=RTqDgxzETjw. We also deployed an online demonstration system at https://drugwatch.net/.</abstract>
      <url hash="675c8430">2024.acl-demos.18</url>
      <bibkey>bobrov-etal-2024-drugwatch</bibkey>
      <doi>10.18653/v1/2024.acl-demos.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>O</fixed-case>pen<fixed-case>E</fixed-case>val: Benchmarking <fixed-case>C</fixed-case>hinese <fixed-case>LLM</fixed-case>s across Capability, Alignment and Safety</title>
      <author><first>Chuang</first><last>Liu</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Linhao</first><last>Yu</last></author>
      <author><first>Jiaxuan</first><last>Li</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Renren</first><last>Jin</last></author>
      <author><first>Yufei</first><last>Huang</last></author>
      <author><first>Ling</first><last>Shi</last></author>
      <author><first>Junhui</first><last>Zhang</last></author>
      <author><first>Xinmeng</first><last>Ji</last></author>
      <author><first>Tingting</first><last>Cui</last></author>
      <author><first>Liutao</first><last>Liutao</last></author>
      <author><first>Jinwang</first><last>Song</last></author>
      <author><first>Hongying</first><last>Zan</last><affiliation>Zhengzhou University</affiliation></author>
      <author><first>Sun</first><last>Li</last><affiliation>China Academy of Information and Communications Technology</affiliation></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>190-210</pages>
      <abstract>The rapid development of Chinese large language models (LLMs) poses big challenges for efficient LLM evaluation. While current initiatives have introduced new benchmarks or evaluation platforms for assessing Chinese LLMs, many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues. To address this gap, we introduce OpenEval, an evaluation testbed that benchmarks Chinese LLMs across capability, alignment and safety. For capability assessment, we include 12 benchmark datasets to evaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge, commonsense reasoning and mathematical reasoning. For alignment assessment, OpenEval contains 7 datasets that examines the bias, offensiveness and illegalness in the outputs yielded by Chinese LLMs. To evaluate safety, especially anticipated risks (e.g., power-seeking, self-awareness) of advanced LLMs, we include 6 datasets. In addition to these benchmarks, we have implemented a phased public evaluation and benchmark update strategy to ensure that OpenEval is in line with the development of Chinese LLMs or even able to provide cutting-edge benchmark datasets to guide the development of Chinese LLMs. In our first public evaluation, we have tested a range of Chinese LLMs, spanning from 7B to 72B parameters, including both open-source and proprietary models. Evaluation results indicate that while Chinese LLMs have shown impressive performance in certain tasks, more attention should be directed towards broader aspects such as commonsense reasoning, alignment, and safety.</abstract>
      <url hash="438f45ce">2024.acl-demos.19</url>
      <bibkey>liu-etal-2024-openeval</bibkey>
      <doi>10.18653/v1/2024.acl-demos.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>A</fixed-case>uto<fixed-case>RE</fixed-case>: Document-Level Relation Extraction with Large Language Models</title>
      <author><first>Lilong</first><last>Xue</last></author>
      <author><first>Dan</first><last>Zhang</last></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>211-220</pages>
      <abstract>Large Language Models (LLMs) have demonstrated exceptional abilities in comprehending and generating text, motivating numerous researchers to utilize them for Information Extraction (IE) purposes, including Relation Extraction (RE). Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple relations and triplet facts distributed across a given document, posing distinct challenges. To overcome these limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel RE extraction paradigm named RHF (Relation-Head-Facts). Unlike existing approaches, AutoRE does not rely on the assumption of known relation options, making it more reflective of real-world scenarios. Additionally, we have developed an easily extensible RE framework using a Parameters Efficient Fine Tuning (PEFT) algorithm (QLoRA). Our experiments on the RE-DocRED dataset showcase AutoRE’s best performance, achieving state-of-the-art results, surpassing TAG by 10.03% and 9.03% respectively on the dev and test set. The code is available and the demonstration video is provided.</abstract>
      <url hash="d4d18b21">2024.acl-demos.20</url>
      <bibkey>xue-etal-2024-autore</bibkey>
      <doi>10.18653/v1/2024.acl-demos.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>L</fixed-case>ink<fixed-case>T</fixed-case>ransformer: A Unified Package for Record Linkage with Transformer Language Models</title>
      <author><first>Abhishek</first><last>Arora</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <author><first>Melissa</first><last>Dell</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <pages>221-231</pages>
      <abstract>Many computational analyses require linking information across noisy text datasets. While large language models (LLMs) offer significant promise, approximate string matching packages in popular statistical softwares such as R and Stata remain predominant in academic applications. These packages have simple interfaces and can be easily extended to a diversity of languages and settings, and for academic applications, ease-of-use and extensibility are essential. In contrast, packages for record linkage with LLMs require significant familiarity with deep learning frameworks and often focus on specialized applications of commercial value in English. The open-source package LinkTransformer aims to bridge this gap by providing an end-to-end software for performing record linkage and other data cleaning tasks with transformer LLMs, treating linkage as a text retrieval problem. At its core is an off-the-shelf toolkit for applying transformer models to record linkage. LinkTransformer contains a rich repository of pre-trained models for multiple languages and supports easy integration of any transformer language model from Hugging Face or OpenAI, providing the extensibility required for many scholarly applications. Its APIs also perform common data processing tasks, e.g., aggregation, noisy de-duplication, and translation-free cross-lingual linkage. LinkTransformer contains comprehensive tools for efficient model tuning, allowing for highly customized applications, and users can easily contribute their custom-trained models to its model hub to ensure reproducibility. Using a novel benchmark dataset geared towards academic applications, we show that LinkTransformer - with both custom models and Hugging Face or OpenAI models off-the-shelf - outperforms string matching by a wide margin. By combining transformer LMs with intuitive APIs, LinkTransformer aims to democratize these performance gains for those who lack familiarity with deep learning frameworks.</abstract>
      <url hash="64889ce9">2024.acl-demos.21</url>
      <bibkey>arora-dell-2024-linktransformer</bibkey>
      <doi>10.18653/v1/2024.acl-demos.21</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>D</fixed-case>oc<fixed-case>P</fixed-case>ilot: Copilot for Automating <fixed-case>PDF</fixed-case> Edit Workflows in Documents</title>
      <author><first>Puneet</first><last>Mathur</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Alexa</first><last>Siu</last><affiliation>Adobe</affiliation></author>
      <author><first>Varun</first><last>Manjunatha</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Tong</first><last>Sun</last><affiliation>Adobe Systems</affiliation></author>
      <pages>232-246</pages>
      <abstract>Digital documents, such as PDFs, are vital in business workflows, enabling communication, documentation, and collaboration. Handling PDFs can involve navigating complex workflows and numerous tools (e.g., comprehension, annotation, editing), which can be tedious and time-consuming for users. We introduce DocPilot, an AI-assisted document workflow Copilot system capable of understanding user intent and executing tasks accordingly to help users streamline their workflows. DocPilot undertakes intelligent orchestration of various tools through LLM prompting in four steps: (1) Task plan generation, (2) Task plan verification and self-correction, (3) Multi-turn User Feedback, and (4) Task Plan Execution via Code Generation and Error log-based Code Self-Revision. The primary goal of this system is to free the user from the intricacies of document editing, enabling them to focus on the creative aspects and enrich their document management experience.</abstract>
      <url hash="f7a4608f">2024.acl-demos.22</url>
      <bibkey>mathur-etal-2024-docpilot</bibkey>
      <doi>10.18653/v1/2024.acl-demos.22</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>U</fixed-case>ltra<fixed-case>E</fixed-case>val: A Lightweight Platform for Flexible and Comprehensive Evaluation for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Chaoqun</first><last>He</last></author>
      <author><first>Renjie</first><last>Luo</last></author>
      <author><first>Shengding</first><last>Hu</last></author>
      <author><first>Ranchi</first><last>Zhao</last><affiliation>ModelBest</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Hanghao</first><last>Wu</last></author>
      <author><first>Jiajie</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>247-257</pages>
      <abstract>Evaluation is pivotal for honing Large Language Models (LLMs), pinpointing their capabilities and guiding enhancements. The rapid development of LLMs calls for a lightweight and easy-to-use framework for swift evaluation deployment. However, due to the various implementation details to consider, developing a comprehensive evaluation platform is never easy. Existing platforms are often complex and poorly modularized, hindering seamless incorporation into researcher’s workflows. This paper introduces UltraEval, a user-friendly evaluation framework characterized by lightweight, comprehensiveness, modularity, and efficiency. We identify and reimplement three core components of model evaluation (models, data, and metrics). The resulting composability allows for the free combination of different models, tasks, prompts, and metrics within a unified evaluation workflow. Additionally, UltraEval supports diverse models owing to a unified HTTP service and provides sufficient inference acceleration.</abstract>
      <url hash="c02f6336">2024.acl-demos.23</url>
      <bibkey>he-etal-2024-ultraeval</bibkey>
      <doi>10.18653/v1/2024.acl-demos.23</doi>
    </paper>
    <paper id="24">
      <title><fixed-case>P</fixed-case>y<fixed-case>F</fixed-case>oma: a Python finite-state compiler module</title>
      <author><first>Mans</first><last>Hulden</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Michael</first><last>Ginn</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Miikka</first><last>Silfverberg</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Michael</first><last>Hammond</last><affiliation>University of Arizona</affiliation></author>
      <pages>258-265</pages>
      <abstract>We describe PyFoma, an open-source Python module for constructing weighted and unweighted finite-state transducers and automata from regular expressions, string rewriting rules, right-linear grammars, or low-level state/transition manipulation. A large variety of standard algorithms for working with finite-state machines is included, with a particular focus on the needs of linguistic and NLP applications. The data structures and code in the module are designed for legibility to allow for potential use in teaching the theory and algorithms associated with finite-state machines.</abstract>
      <url hash="d256ff2d">2024.acl-demos.24</url>
      <bibkey>hulden-etal-2024-pyfoma</bibkey>
      <doi>10.18653/v1/2024.acl-demos.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>V</fixed-case>era<fixed-case>CT</fixed-case> Scan: Retrieval-Augmented Fake News Detection with Justifiable Reasoning</title>
      <author><first>Cheng</first><last>Niu</last></author>
      <author><first>Yang</first><last>Guan</last></author>
      <author><first>Yuanhao</first><last>Wu</last><affiliation>Newsbreak</affiliation></author>
      <author><first>Juno</first><last>Zhu</last></author>
      <author><first>Juntong</first><last>Song</last></author>
      <author><first>Randy</first><last>Zhong</last></author>
      <author><first>Kaihua</first><last>Zhu</last></author>
      <author><first>Siliang</first><last>Xu</last></author>
      <author><first>Shizhe</first><last>Diao</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>266-277</pages>
      <abstract>The proliferation of fake news poses a significant threat not only by disseminating misleading information but also by undermining the very foundations of democracy. The recent advance of generative artificial intelligence has further exacerbated the challenge of distinguishing genuine news from fabricated stories. In response to this challenge, we introduce VeraCT Scan, a novel retrieval-augmented system for fake news detection. This system operates by extracting the core facts from a given piece of news and subsequently conducting an internet-wide search to identify corroborating or conflicting reports. Then sources’ credibility is leveraged for information verification. Besides determining the veracity of news, we also provide transparent evidence and reasoning to support its conclusions, resulting in the interpretability and trust in the results. In addition to GPT-4 Turbo, Llama-2 13B is also fine-tuned for news content understanding, information verification, and reasoning. Both implementations have demonstrated state-of-the-art accuracy in the realm of fake news detection.</abstract>
      <url hash="5e3db6b5">2024.acl-demos.25</url>
      <bibkey>niu-etal-2024-veract</bibkey>
      <doi>10.18653/v1/2024.acl-demos.25</doi>
    </paper>
    <paper id="26">
      <title>string2string: A Modern Python Library for String-to-String Algorithms</title>
      <author><first>Mirac</first><last>Suzgun</last><affiliation>Stanford University</affiliation></author>
      <author><first>Stuart</first><last>Shieber</last><affiliation>Harvard University</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <pages>278-285</pages>
      <abstract>We introduce **string2string**, an open-source library that offers a comprehensive suite of efficient algorithms for a broad range of string-to-string problems. It includes traditional algorithmic solutions as well as recent advanced neural approaches to tackle various problems in string alignment, distance measurement, lexical and semantic search, and similarity analysis�along with several helpful visualization tools and metrics to facilitate the interpretation and analysis of these methods. Notable algorithms featured in the library include the Smith-Waterman algorithm for pairwise local alignment, the Hirschberg algorithm for global alignment, the Wagner-Fischer algorithm for edit distance, BARTScore and BERTScore for similarity analysis, the Knuth-Morris-Pratt algorithm for lexical search, and Faiss for semantic search. In addition, it wraps existing efficient and widely-used implementations of certain frameworks and metrics, such as sacreBLEU and ROUGE. Overall, the library aims to provide extensive coverage and increased flexibility in comparison to existing libraries for strings. It can be used for many downstream applications, tasks, and problems in natural-language processing, bioinformatics, and computational social sciences. It is implemented in Python, easily installable via pip, and accessible through a simple API. Source code, documentation, and tutorials are all available on our GitHub page: https://github.com/stanfordnlp/string2string* Documentation: https://string2string.readthedocs.io/en/latest/* GitHub page: https://github.com/stanfordnlp/string2string* Short video: https://drive.google.com/file/d/1IT-pBACDVUoEHewk__5Pz5mU5oAMq5k_/view?usp=sharing</abstract>
      <url hash="79bb351d">2024.acl-demos.26</url>
      <bibkey>suzgun-etal-2024-string2string</bibkey>
      <doi>10.18653/v1/2024.acl-demos.26</doi>
    </paper>
    <paper id="27">
      <title>Proofread: Fixes All Errors with One Tap</title>
      <author><first>Renjie</first><last>Liu</last></author>
      <author><first>Yanxiang</first><last>Zhang</last></author>
      <author><first>Yun</first><last>Zhu</last><affiliation>Google</affiliation></author>
      <author><first>Haicheng</first><last>Sun</last></author>
      <author><first>Yuanbo</first><last>Zhang</last></author>
      <author><first>Michael</first><last>Huang</last><affiliation>Google</affiliation></author>
      <author><first>Shanqing</first><last>Cai</last><affiliation>Google</affiliation></author>
      <author><first>Lei</first><last>Meng</last></author>
      <author><first>Shumin</first><last>Zhai</last><affiliation>Google</affiliation></author>
      <pages>286-293</pages>
      <abstract>The impressive capabilities in Large Language Models (LLMs) provide a powerful approach to reimagine users’ typing experience. This paper demonstrates the Proofread feature in Gboard, a virtual keyboard running on mobile phones. Proofread enables seamless sentence-level and paragraph-level corrections with a single tap. We describe the complete system in this paper, from data generation, metrics design to model tuning and deployment. To obtain models with sufficient quality, we implement a careful data synthetic pipeline tailored to online use cases, design multifaceted metrics, employ a two-stage tuning approach to acquire the dedicated LLM for the feature: the Supervised Fine Tuning (SFT) for foundational quality, followed by the Reinforcement Learning (RL) tuning approach for targeted refinement. Specifically, we find sequential tuning on Rewrite and proofread tasks yields the best quality in SFT stage, and propose global and direct rewards in the RL tuning stage to seek further improvement. Extensive experiments on a human-labeled golden set showed our tuned PaLM2-XS model achieved 85.56% good ratio. We launched the feature to Pixel 8 devices by serving the model on TPU v5 in Google Cloud, with thousands of daily active users. Serving latency was significantly reduced by quantization, bucket inference, text segmentation, and speculative decoding. Our demo could be seen in Youtube.</abstract>
      <url hash="196f46d9">2024.acl-demos.27</url>
      <bibkey>liu-etal-2024-proofread</bibkey>
      <doi>10.18653/v1/2024.acl-demos.27</doi>
    </paper>
    <paper id="28">
      <title><fixed-case>S</fixed-case>ea<fixed-case>LLM</fixed-case>s - Large Language Models for <fixed-case>S</fixed-case>outheast <fixed-case>A</fixed-case>sia</title>
      <author><first>Xuan-Phi</first><last>Nguyen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Xin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Mahani</first><last>Aljunied</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhiqiang</first><last>Hu</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Chenhui</first><last>Shen</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yew Ken</first><last>Chia</last></author>
      <author><first>Xingxuan</first><last>Li</last></author>
      <author><first>Jianyu</first><last>Wang</last><affiliation>Alibaba DAMO Academy</affiliation></author>
      <author><first>Qingyu</first><last>Tan</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Liying</first><last>Cheng</last></author>
      <author><first>Guanzheng</first><last>Chen</last></author>
      <author><first>Yue</first><last>Deng</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Sen</first><last>Yang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Chaoqun</first><last>Liu</last></author>
      <author><first>Hang</first><last>Zhang</last></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Alibaba Group</affiliation></author>
      <pages>294-304</pages>
      <abstract>Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon popular English-centric models through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate.</abstract>
      <url hash="9ac522a5">2024.acl-demos.28</url>
      <bibkey>nguyen-etal-2024-seallms</bibkey>
      <doi>10.18653/v1/2024.acl-demos.28</doi>
    </paper>
    <paper id="29">
      <title>Fundus: A Simple-to-Use News Scraper Optimized for High Quality Extractions</title>
      <author><first>Max</first><last>Dallabetta</last><affiliation>Department of Computer Science, Humboldt University Berlin, Humboldt Universit�t Berlin</affiliation></author>
      <author><first>Conrad</first><last>Dobberstein</last><affiliation>Technische Universit�t Berlin</affiliation></author>
      <author><first>Adrian</first><last>Breiding</last></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universit�t Berlin</affiliation></author>
      <pages>305-314</pages>
      <abstract>This paper introduces Fundus, a user-friendly news scraper that enables users to obtain millions of high-quality news articles with just a few lines of code. Unlike existing news scrapers, we use manually crafted, bespoke content extractors that are specifically tailored to the formatting guidelines of each supported online newspaper. This allows us to optimize our scraping for quality such that retrieved news articles are textually complete and without HTML artifacts. Further, our framework combines both crawling (retrieving HTML from the web or large web archives) and content extraction into a single pipeline. By providing a unified interface for a predefined collection of newspapers, we aim to make Fundus broadly usable even for non-technical users. This paper gives an overview of the framework, discusses our design choices, and presents a comparative evaluation against other popular news scrapers. Our evaluation shows that Fundus yields significantly higher quality extractions (complete and artifact-free news articles) than prior work.The framework is available on GitHub under https://github.com/flairNLP/fundus and can be simply installed using pip.</abstract>
      <url hash="ad816694">2024.acl-demos.29</url>
      <bibkey>dallabetta-etal-2024-fundus</bibkey>
      <doi>10.18653/v1/2024.acl-demos.29</doi>
    </paper>
    <paper id="30">
      <title><fixed-case>C</fixed-case>har<fixed-case>P</fixed-case>oet: A <fixed-case>C</fixed-case>hinese Classical Poetry Generation System Based on Token-free <fixed-case>LLM</fixed-case></title>
      <author><first>Chengyue</first><last>Yu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Lei</first><last>Zang</last></author>
      <author><first>Jiaotuan</first><last>Wang</last></author>
      <author><first>Chenyi</first><last>Zhuang</last></author>
      <author><first>Jinjie</first><last>Gu</last></author>
      <pages>315-325</pages>
      <abstract>Automatic Chinese classical poetry generation has attracted much research interest, but achieving effective control over format and content simultaneously remains challenging. Traditional systems usually accept keywords as user inputs, resulting in limited control over content. Large language models (LLMs) improve content control by allowing unrestricted user instructions, but the token-by-token generation process frequently makes format errors. Motivated by this, we propose CharPoet, a Chinese classical poetry generation system based on token-free LLM, which provides effective control over both format and content. Our token-free architecture generates in a character-by-character manner, enabling precise control over the number of characters. Pruned from existing token-based LLMs, CharPoet inherits their pretrained capabilities and can generate poetry following instructions like �Write me a poem for my mother’s birthday.� CharPoet achieves format accuracy above 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of content quality, CharPoet surpasses traditional systems including Jiuge, and is comparable to other LLMs. Our system is open source and available at https://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of CharPoet is available at https://youtu.be/voZ25qEp3Dc.</abstract>
      <url hash="d8d7a6d3">2024.acl-demos.30</url>
      <bibkey>yu-etal-2024-charpoet</bibkey>
      <doi>10.18653/v1/2024.acl-demos.30</doi>
    </paper>
    <paper id="31">
      <title><fixed-case>ITAKE</fixed-case>: Interactive Unstructured Text Annotation and Knowledge Extraction System with <fixed-case>LLM</fixed-case>s and <fixed-case>M</fixed-case>odel<fixed-case>O</fixed-case>ps</title>
      <author><first>Jiahe</first><last>Song</last></author>
      <author><first>Hongxin</first><last>Ding</last></author>
      <author><first>Zhiyuan</first><last>Wang</last></author>
      <author><first>Yongxin</first><last>Xu</last></author>
      <author><first>Yasha</first><last>Wang</last></author>
      <author><first>Junfeng</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <pages>326-334</pages>
      <abstract>Extracting structured knowledge from unstructured text data has a wide range of application prospects, and a pervasive trend is to develop text annotation tools to help extraction. However, they often encounter issues such as single scenario usage, lack of effective human-machine collaboration, insufficient model supervision, and suboptimal utilization of Large Language Models (LLMs). We introduces an interactive unstructured text annotation and knowledge extraction system that synergistically integrates LLMs and ModelOps to alleviate these issues. The system leverages LLMs for enhanced performance in low-resource contexts, employs a ModelOps platform to monitor models throughout their lifecycle, and amalgamates interactive annotation methods with online machine learning and active learning. The demo video and website are now publicly available.</abstract>
      <url hash="ecb4212e">2024.acl-demos.31</url>
      <bibkey>song-etal-2024-itake</bibkey>
      <doi>10.18653/v1/2024.acl-demos.31</doi>
    </paper>
    <paper id="32">
      <title><fixed-case>LEGENT</fixed-case>: Open Platform for Embodied Agents</title>
      <author><first>Zhili</first><last>Cheng</last></author>
      <author><first>Zhitong</first><last>Wang</last></author>
      <author><first>Jinyi</first><last>Hu</last></author>
      <author><first>Shengding</first><last>Hu</last></author>
      <author><first>An</first><last>Liu</last></author>
      <author><first>Yuge</first><last>Tu</last></author>
      <author><first>Pengkai</first><last>Li</last><affiliation>Central South University</affiliation></author>
      <author><first>Lei</first><last>Shi</last></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>335-345</pages>
      <abstract>Despite advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), their integration into language-grounded, human-like embodied agents remains incomplete, hindering complex real-life task performance in 3D environments. Existing integrations often feature limited open-sourcing, challenging collective progress in this field. We introduce LEGENT, an open, scalable platform for developing embodied agents using LLMs and LMMs. LEGENT offers a dual approach: a rich 3D environment with interactive, communicable, and actionable agents, paired with a user-friendly interface, and a sophisticated data generation pipeline utilizing advanced algorithms to exploit supervision from simulated worlds at scale. In our experiments, an embryonic vision-language-action model trained on LEGENT-generated data surpasses GPT-4V in embodied tasks, showcasing promising generalization capabilities. The demo video is available at the following link https://video.legent.ai.</abstract>
      <url hash="3fec9d8b">2024.acl-demos.32</url>
      <bibkey>cheng-etal-2024-legent</bibkey>
      <doi>10.18653/v1/2024.acl-demos.32</doi>
    </paper>
    <paper id="33">
      <title>Variationist: Exploring Multifaceted Variation and Bias in Written Language Data</title>
      <author><first>Alan</first><last>Ramponi</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Camilla</first><last>Casula</last><affiliation>University of Trento and Fondazione Bruno Kessler</affiliation></author>
      <author><first>Stefano</first><last>Menini</last></author>
      <pages>346-354</pages>
      <abstract>Exploring and understanding language data is a fundamental stage in all areas dealing with human language. It allows NLP practitioners to uncover quality concerns and harmful biases in data before training, and helps linguists and social scientists to gain insight into language use and human behavior. Yet, there is currently a lack of a unified, customizable tool to seamlessly inspect and visualize language variation and bias across multiple variables, language units, and diverse metrics that go beyond descriptive statistics. In this paper, we introduce Variationist, a highly-modular, extensible, and task-agnostic tool that fills this gap. Variationist handles at once a potentially unlimited combination of variable types and semantics across diversity and association metrics with regards to the language unit of choice, and orchestrates the creation of up to five-dimensional interactive charts for over 30 variable type-semantics combinations. Through our case studies on computational dialectology, human label variation, and text generation, we show how Variationist enables researchers from different disciplines to effortlessly answer specific research questions or unveil undesired associations in language data. A Python library, code, documentation, and tutorials are made publicly available to the research community.</abstract>
      <url hash="3e034427">2024.acl-demos.33</url>
      <bibkey>ramponi-etal-2024-variationist</bibkey>
      <doi>10.18653/v1/2024.acl-demos.33</doi>
    </paper>
    <paper id="34">
      <title>An <fixed-case>LLM</fixed-case>-based Knowledge Synthesis and Scientific Reasoning Framework for Biomedical Discovery</title>
      <author><first>Oskar</first><last>Wysocki</last></author>
      <author><first>Magdalena.wysocka@cruk.manchester.ac.uk</first><last>Magdalena.wysocka@cruk.manchester.ac.uk</last><affiliation>NA</affiliation></author>
      <author><first>Danilo</first><last>Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Alex</first><last>Bogatu</last></author>
      <author><first>Danilo.miranda@idiap.ch</first><last>Danilo.miranda@idiap.ch</last><affiliation>NA</affiliation></author>
      <author><first>Maxime.delmas@idiap.ch</first><last>Maxime.delmas@idiap.ch</last><affiliation>NA</affiliation></author>
      <author><first>Harriet.unsworth@cruk.manchester.ac.uk</first><last>Harriet.unsworth@cruk.manchester.ac.uk</last><affiliation>NA</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>355-364</pages>
      <abstract>We present BioLunar, developed using the Lunar framework, as a tool for supporting biological analyses, with a particular emphasis on molecular-level evidence enrichment for biomarker discovery in oncology. The platform integrates Large Language Models (LLMs) to facilitate complex scientific reasoning across distributed evidence spaces, enhancing the capability for harmonizing and reasoning over heterogeneous data sources. Demonstrating its utility in cancer research, BioLunar leverages modular design, reusable data access and data analysis components, and a low-code user interface, enabling researchers of all programming levels to construct LLM-enabled scientific workflows. By facilitating automatic scientific discovery and inference from heterogeneous evidence, BioLunar exemplifies the potential of the integration between LLMs, specialised databases and biomedical tools to support expert-level knowledge synthesis and discovery.</abstract>
      <url hash="7bd84691">2024.acl-demos.34</url>
      <bibkey>wysocki-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.acl-demos.34</doi>
    </paper>
    <paper id="35">
      <title><fixed-case>C</fixed-case>og<fixed-case>MG</fixed-case>: Collaborative Augmentation Between Large Language Model and Knowledge Graph</title>
      <author><first>Tong</first><last>Zhou</last></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>365-373</pages>
      <abstract>Large language models have become integral to question-answering applications despite their propensity for generating hallucinations and factually inaccurate content. Querying knowledge graphs to reduce hallucinations in LLM meets the challenge of incomplete knowledge coverage in knowledge graphs. On the other hand, updating knowledge graphs by information extraction and knowledge graph completion faces the knowledge update misalignment issue. In this work, we introduce a collaborative augmentation framework, CogMG, leveraging knowledge graphs to address the limitations of LLMs in QA scenarios, explicitly targeting the problems of incomplete knowledge coverage and knowledge update misalignment. The LLMs identify and decompose required knowledge triples that are not present in the KG, enriching them and aligning updates with real-world demands. We demonstrate the efficacy of this approach through a supervised fine-tuned LLM within an agent framework, showing significant improvements in reducing hallucinations and enhancing factual accuracy in QA responses. Our code and video are publicly available.</abstract>
      <url hash="b2b195e3">2024.acl-demos.35</url>
      <bibkey>zhou-etal-2024-cogmg</bibkey>
      <doi>10.18653/v1/2024.acl-demos.35</doi>
    </paper>
    <paper id="36">
      <title><fixed-case>ELLA</fixed-case>: Empowering <fixed-case>LLM</fixed-case>s for Interpretable, Accurate and Informative Legal Advice</title>
      <author><first>Yutong</first><last>Hu</last></author>
      <author><first>Kangcheng</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <pages>374-387</pages>
      <abstract>Despite remarkable performance in legal consultation exhibited by legal Large Language Models(LLMs) combined with legal article retrieval components, there are still cases when the advice given is incorrect or baseless. To alleviate these problems, we propose <b>ELLA</b>, a tool for <b>E</b>mpowering <b>L</b>LMs for interpretable, accurate, and informative <b>L</b>egal <b>A</b>dvice. ELLA visually presents the correlation between legal articles and LLM’s response by calculating their similarities, providing users with an intuitive legal basis for the responses. Besides, based on the users’ queries, ELLA retrieves relevant legal articles and displays them to users. Users can interactively select legal articles for LLM to generate more accurate responses. ELLA also retrieves relevant legal cases for user reference. Our user study shows that presenting the legal basis for the response helps users understand better. The accuracy of LLM’s responses also improves when users intervene in selecting legal articles for LLM. Providing relevant legal cases also aids individuals in obtaining comprehensive information. Our github repo is: <url>https://github.com/Huyt00/ELLA</url>.</abstract>
      <url hash="07abd0c6">2024.acl-demos.36</url>
      <bibkey>hu-etal-2024-ella</bibkey>
      <doi>10.18653/v1/2024.acl-demos.36</doi>
    </paper>
    <paper id="37">
      <title><fixed-case>LLMB</fixed-case>ox: A Comprehensive Library for Large Language Models</title>
      <author><first>Tianyi</first><last>Tang</last></author>
      <author><first>Hu</first><last>Yiwen</last></author>
      <author><first>Bingqian</first><last>Li</last></author>
      <author><first>Wenyang</first><last>Luo</last></author>
      <author><first>ZiJing</first><last>Qin</last></author>
      <author><first>Haoxiang</first><last>Sun</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jiapeng</first><last>Wang</last></author>
      <author><first>Shiyi</first><last>Xu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xiaoxue</first><last>Cheng</last></author>
      <author><first>Geyang</first><last>Guo</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Han</first><last>Peng</last></author>
      <author><first>Bowen</first><last>Zheng</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yiru</first><last>Tang</last></author>
      <author><first>Yingqian</first><last>Min</last></author>
      <author><first>Yushuo</first><last>Chen</last></author>
      <author><first>Jie</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ranchi</first><last>Zhao</last><affiliation>ModelBest</affiliation></author>
      <author><first>Luran</first><last>Ding</last></author>
      <author><first>Yuhao</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zican</first><last>Dong</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xia</first><last>Chunxuan</last></author>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Kun</first><last>Zhou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>388-399</pages>
      <abstract>To facilitate the research on large language models (LLMs), this paper presents a comprehensive and unified library, LLMBox, to ease the development, use, and evaluation of LLMs. This library is featured with three main merits: (1) a unified data interface that supports the flexible implementation of various training strategies, (2) a comprehensive evaluation that covers extensive tasks, datasets, and models, and (3) more practical consideration, especially on user-friendliness and efficiency. With our library, users can easily reproduce existing methods, train new models, and conduct comprehensive performance comparisons. To rigorously test LLMBox, we conduct extensive experiments in a diverse coverage of evaluation settings, and experimental results demonstrate the effectiveness and efficiency of our library in supporting various implementations related to LLMs. The detailed introduction and usage guidance can be found at <url>https://github.com/RUCAIBox/LLMBox</url>.</abstract>
      <url hash="efb8c69b">2024.acl-demos.37</url>
      <bibkey>tang-etal-2024-llmbox</bibkey>
      <doi>10.18653/v1/2024.acl-demos.37</doi>
    </paper>
    <paper id="38">
      <title><fixed-case>L</fixed-case>lama<fixed-case>F</fixed-case>actory: Unified Efficient Fine-Tuning of 100+ Language Models</title>
      <author><first>Yaowei</first><last>Zheng</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Junhao</first><last>Zhang</last></author>
      <author><first>Yanhan</first><last>Ye</last></author>
      <author><first>Zheyan</first><last>Luo</last></author>
      <pages>400-410</pages>
      <abstract>Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and 3,000 forks.</abstract>
      <url hash="75692ad3">2024.acl-demos.38</url>
      <bibkey>zheng-etal-2024-llamafactory</bibkey>
      <doi>10.18653/v1/2024.acl-demos.38</doi>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2024-08-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)</booktitle>
      <editor><first>Xiyan</first><last>Fu</last><affiliation>Heidelberg University</affiliation></editor>
      <editor><first>Eve</first><last>Fleisig</last><affiliation>UC Berkeley</affiliation></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="cde5b670">2024.acl-srw</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="fc18831c">2024.acl-srw.0</url>
      <bibkey>acl-2024-srw</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Feriji: A <fixed-case>F</fixed-case>rench-<fixed-case>Z</fixed-case>arma Parallel Corpus, Glossary &amp; Translator</title>
      <author><first>Mamadou</first><last>Keita</last></author>
      <author><first>Elysabhete</first><last>Ibrahim</last></author>
      <author><first>Habibatou</first><last>Alfari</last></author>
      <author><first>Christopher</first><last>Homan</last></author>
      <pages>1-9</pages>
      <abstract>Machine translation (MT) is a rapidly expanding field that has experienced significant advancements in recent years with the development of models capable of translating multiple languages with remarkable accuracy. However, the representation of African languages in this field still needs improvement due to linguistic complexities and limited resources. This applies to the Zarma language, a dialect of Songhay (of the Nilo-Saharan language family) spoken by over 5 million people across Niger and neighboring countries (Lewis et al., 2016). This paper introduces Feriji, the first robust French-Zarma parallel corpus and glossary designed for MT. The corpus, containing 61,085 sentences in Zarma and 42,789 in French, and a glossary of 4,062 words represents a significant step in addressing the need for more resources for Zarma. We fine-tune three large language models on our dataset, obtaining a BLEU score of 30.06 on the best-performing model. We further evaluate the models on human judgments of fluency, comprehension, and readability and the importance and impact of the corpus and models. Our contributions help to bridge a significant language gap and promote an essential and overlooked indigenous African language.</abstract>
      <url hash="cd755fd9">2024.acl-srw.1</url>
      <bibkey>keita-etal-2024-feriji</bibkey>
      <doi>10.18653/v1/2024.acl-srw.1</doi>
    </paper>
    <paper id="2">
      <title>Pragmatic inference of scalar implicature by <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ye-eun</first><last>Cho</last><affiliation>Sungkyunkwan University, South Korea</affiliation></author>
      <author><first>Seong mook</first><last>Kim</last><affiliation>Sungkyunkwan University, South Korea</affiliation></author>
      <pages>10-20</pages>
      <abstract>This study investigates how Large Language Models (LLMs), particularly BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic inference of scalar implicature, such as some. Two sets of experiments were conducted using cosine similarity and next sentence/token prediction as experimental methods. The results in experiment 1 showed that, both models interpret some as pragmatic implicature not all in the absence of context, aligning with human language processing. In experiment 2, in which Question Under Discussion (QUD) was presented as a contextual cue, BERT showed consistent performance regardless of types of QUDs, while GPT-2 encountered processing difficulties since a certain type of QUD required pragmatic inference for implicature. The findings revealed that, in terms of theoretical approaches, BERT inherently incorporates pragmatic implicature not all within the term some, adhering to Default model (Levinson, 2000). In contrast, GPT-2 seems to encounter processing difficulties in inferring pragmatic implicature within context, consistent with Context-driven model (Sperber and Wilson, 2002).</abstract>
      <url hash="49c975e0">2024.acl-srw.2</url>
      <bibkey>cho-ismkim99-skku-edu-2024-pragmatic</bibkey>
      <doi>10.18653/v1/2024.acl-srw.2</doi>
    </paper>
    <paper id="3">
      <title>Topic Modeling for Short Texts with Large Language Models</title>
      <author><first>Tomoki</first><last>Doi</last></author>
      <author><first>Masaru</first><last>Isonuma</last></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <pages>21-33</pages>
      <abstract>As conventional topic models rely on word co-occurrence to infer latent topics, topic modeling for short texts has been a long-standing challenge. Large Language Models (LLMs) can potentially overcome this challenge by contextually learning the meanings of words via pretraining. In this paper, we study two approaches to using LLMs for topic modeling: parallel prompting and sequential prompting. Input length limitations prevent LLMs from processing many texts at once. However, an arbitrary number of texts can be handled by LLMs by splitting the texts into smaller subsets and processing them in parallel or sequentially. Our experimental results demonstrate that our methods can identify more coherent topics than existing ones while maintaining the diversity of the induced topics. Furthermore, we found that the inferred topics cover the input texts to some extent, while hallucinated topics are hardly generated.</abstract>
      <url hash="eb8faa52">2024.acl-srw.3</url>
      <bibkey>doi-etal-2024-topic</bibkey>
      <doi>10.18653/v1/2024.acl-srw.3</doi>
    </paper>
    <paper id="5">
      <title>Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer</title>
      <author><first>Yongqi</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Bai</first><last>Jionghao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Rongjie</first><last>Huang</last><affiliation>FAIR</affiliation></author>
      <author><first>Ruiqi</first><last>Li</last></author>
      <author><first>Zhiqing</first><last>Hong</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>34-41</pages>
      <abstract>Direct speech-to-speech translation (S2ST) with discrete self-supervised representations has achieved remarkable accuracy, but is unable to preserve the speaker timbre of the source speech. Meanwhile, the scarcity of high-quality speaker-parallel data poses a challenge for learning style transfer during translation. We design an S2ST pipeline with style-transfer capability on the basis of discrete self-supervised speech representations and codec units. The acoustic language model we introduce for style transfer leverages self-supervised in-context learning, acquiring style transfer ability without relying on any speaker-parallel data, thereby overcoming data scarcity. By using extensive training data, our model achieves zero-shot cross-lingual style transfer on previously unseen source languages. Experiments show that our model generates translated speeches with high fidelity and speaker similarity. Audio samples are available at http://stylelm.github.io/ .</abstract>
      <url hash="1e867503">2024.acl-srw.5</url>
      <bibkey>wang-etal-2024-speech</bibkey>
      <doi>10.18653/v1/2024.acl-srw.5</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>B</fixed-case>ias<fixed-case>DPO</fixed-case>: Mitigating Bias in Language Models through Direct Preference Optimization</title>
      <author><first>Ahmed</first><last>Allam</last></author>
      <pages>42-50</pages>
      <abstract>Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns. This paper introduces a new framework employing Direct Preference Optimization (DPO) to mitigate gender, racial, and religious biases in LLM-generated English text. By developing a loss function that favors less biased over biased completions, our approach cultivates a preference for respectful and non-discriminatory language in LLMs. We also contribute a manually designed dataset for training LLMs to recognize and correct biases. This dataset encompasses a diverse range of prompts paired with both biased and unbiased completions. Implementing this approach on the Microsoft Phi-2 model, we demonstrate substantial reductions in biased outputs as our model outperforms the baseline model on almost all bias benchmarks. Our model also achieves better performance compared to other open-source models on most benchmarks. By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.</abstract>
      <url hash="1ffa5346">2024.acl-srw.7</url>
      <bibkey>allam-2024-biasdpo</bibkey>
      <doi>10.18653/v1/2024.acl-srw.7</doi>
    </paper>
    <paper id="10">
      <title>Document Alignment based on Overlapping Fixed-Length Segments</title>
      <author><first>Xiaotian</first><last>Wang</last></author>
      <author><first>Takehito</first><last>Utsuro</last><affiliation>University of Tsukuba</affiliation></author>
      <author><first>Masaaki</first><last>Nagata</last><affiliation>NTT Corporation</affiliation></author>
      <pages>51-61</pages>
      <abstract>Acquiring large-scale parallel corpora is crucial for NLP tasks such asNeural Machine Translation, and web crawling has become a popularmethodology for this purpose. Previous studies have been conductedbased on sentence-based segmentation (SBS) when aligning documents invarious languages which are obtained through web crawling. Among them,the TK-PERT method (Thompson and Koehn, 2020) achieved state-of-the-artresults and addressed the boilerplate text in web crawling data wellthrough a down-weighting approach. However, there remains a problemwith how to handle long-text encoding better. Thus, we introduce thestrategy of Overlapping Fixed-Length Segmentation (OFLS) in place ofSBS, and observe a pronounced enhancement when performing the sameapproach for document alignment. In this paper, we compare the SBS andOFLS using three previous methods, Mean-Pool, TK-PERT (Thompson andKoehn, 2020), and Optimal Transport (Clark et al., 2019; El- Kishky andGuzman, 2020), on the WMT16 document alignment shared task forFrench-English, as well as on our self-established Japanese-Englishdataset MnRN. As a result, for the WMT16 task, various SBS basedmethods showed an increase in recall by 1% to 10% after reproductionwith OFLS. For MnRN data, OFLS demonstrated notable accuracyimprovements and exhibited faster document embedding speed.</abstract>
      <url hash="992f5355">2024.acl-srw.10</url>
      <bibkey>wang-etal-2024-document</bibkey>
      <doi>10.18653/v1/2024.acl-srw.10</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>R</fixed-case>e<fixed-case>MAG</fixed-case>-<fixed-case>KR</fixed-case>: Retrieval and Medically Assisted Generation with Knowledge Reduction for Medical Question Answering</title>
      <author><first>Sidhaarth</first><last>Murali</last></author>
      <author><first>Sowmya</first><last>S.</last><affiliation>National Institute of Technology Karnataka</affiliation></author>
      <author><first>Supreetha</first><last>R</last></author>
      <pages>62-67</pages>
      <abstract>Large Language Models (LLMs) have significant potential for facilitating intelligent end-user applications in healthcare. However, hallucinations remain an inherent problem with LLMs, making it crucial to address this issue with extensive medical knowledge and data. In this work, we propose a Retrieve-and-Medically-Augmented-Generation with Knowledge Reduction (ReMAG-KR) pipeline, employing a carefully curated knowledge base using cross-encoder re-ranking strategies. The pipeline is tested on medical MCQ-based QA datasets as well as general QA datasets. It was observed that when the knowledge base is reduced, the model’s performance decreases by 2-8%, while the inference time improves by 47%.</abstract>
      <url hash="0fa3d62d">2024.acl-srw.13</url>
      <bibkey>murali-etal-2024-remag</bibkey>
      <doi>10.18653/v1/2024.acl-srw.13</doi>
    </paper>
    <paper id="15">
      <title>Demystifying Instruction Mixing for Fine-tuning Large Language Models</title>
      <author><first>Renxi</first><last>Wang</last></author>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Xudong</first><last>Han</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Chiyu</first><last>Zhang</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>68-75</pages>
      <abstract>Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures, laying the foundations for future research.</abstract>
      <url hash="1be37de3">2024.acl-srw.15</url>
      <bibkey>wang-etal-2024-demystifying</bibkey>
      <doi>10.18653/v1/2024.acl-srw.15</doi>
    </paper>
    <paper id="16">
      <title>Fine-Tuning <fixed-case>ASR</fixed-case> models for Very Low-Resource Languages: A Study on Mvskoke</title>
      <author><first>Julia</first><last>Mainzinger</last></author>
      <author><first>Gina-Anne</first><last>Levow</last><affiliation>University of Washington and University of Washington</affiliation></author>
      <pages>76-82</pages>
      <abstract>Recent advancements in multilingual models for automatic speech recognition (ASR) have been able to achieve a high accuracy for languages with extremely limited resources. This study examines ASR modeling for the Mvskoke language, an indigenous language of America. The parameter efficiency of adapter training is contrasted with training entire models, and it is demonstrated how performance varies with different amounts of data. Additionally, the models are evaluated with trigram language model decoding, and the outputs are compared across different types of speech recordings. Results show that training an adapter is both parameter efficient and gives higher accuracy for a relatively small amount of data.</abstract>
      <url hash="918ba88b">2024.acl-srw.16</url>
      <bibkey>mainzinger-levow-2024-fine</bibkey>
      <doi>10.18653/v1/2024.acl-srw.16</doi>
    </paper>
    <paper id="17">
      <title>Automating Qualitative Data Analysis with Large Language Models</title>
      <author><first>Angelina</first><last>Parfenova</last><affiliation>Universität Grenoble Alpes</affiliation></author>
      <author><first>Alexander</first><last>Denzler</last><affiliation>Lucerne University of Applied Sciences and Arts</affiliation></author>
      <author><first>Jörgen</first><last>Pfeffer</last><affiliation>Technische Universität München</affiliation></author>
      <pages>83-91</pages>
      <abstract>This PhD proposal aims to investigate ways of automating qualitative data analysis, specifically the thematic coding of texts. Despite existing methods vastly covered in literature, they mainly use Topic Modeling and other quantitative approaches which are far from resembling a human’s analysis outcome. This proposal examines the limitations of current research in the field. It proposes a novel methodology based on Large Language Models to tackle automated coding and make it as close as possible to the results of human researchers. This paper covers studies already done in this field and their limitations, existing software, the problem of duplicating the researcher bias, and the proposed methodology.</abstract>
      <url hash="a9c1ea15">2024.acl-srw.17</url>
      <bibkey>parfenova-etal-2024-automating</bibkey>
      <doi>10.18653/v1/2024.acl-srw.17</doi>
    </paper>
    <paper id="18">
      <title><fixed-case>ANHALTEN</fixed-case>: Cross-Lingual Transfer for <fixed-case>G</fixed-case>erman Token-Level Reference-Free Hallucination Detection</title>
      <author><first>Janek</first><last>Herrlein</last><affiliation>Bayerische Julius-Maximilians-Universit�t W�rzburg</affiliation></author>
      <author><first>Chia-Chien</first><last>Hung</last><affiliation>NEC Laboratories Europe and Universit�t Mannheim</affiliation></author>
      <author><first>Goran</first><last>Glava�</last><affiliation>Julius-Maximilians-Universit�t W�rzburg</affiliation></author>
      <pages>92-100</pages>
      <abstract>Research on token-level reference-free hallucination detection has predominantly focused on English, primarily due to the scarcity of robust datasets in other languages. This has hindered systematic investigations into the effectiveness of cross-lingual transfer for this important NLP application. To address this gap, we introduce ANHALTEN, a new evaluation dataset that extends the English hallucination detection dataset to German. To the best of our knowledge, this is the first work that explores cross-lingual transfer for token-level reference-free hallucination detection. ANHALTEN contains gold annotations in German that are parallel (i.e., directly comparable to the original English instances). We benchmark several prominent cross-lingual transfer approaches, demonstrating that larger context length leads to better hallucination detection in German, even without succeeding context. Importantly, we show that the sample-efficient few-shot transfer is the most effective approach in most setups. This highlights the practical benefits of minimal annotation effort in the target language for reference-free hallucination detection. Aiming to catalyze future research on cross-lingual token-level reference-free hallucination detection, we make ANHALTEN publicly available: https://github.com/janekh24/anhalten</abstract>
      <url hash="132c929f">2024.acl-srw.18</url>
      <bibkey>herrlein-etal-2024-anhalten</bibkey>
      <doi>10.18653/v1/2024.acl-srw.18</doi>
    </paper>
    <paper id="19">
      <title>Label-Aware Automatic Verbalizer for Few-Shot Text Classification in Mid-To-Low Resource Languages</title>
      <author><first>Thanakorn</first><last>Thaminkaew</last><affiliation>Chulalongkorn University</affiliation></author>
      <author><first>Piyawat</first><last>Lertvittayakumjorn</last><affiliation>Google</affiliation></author>
      <author><first>Peerapon</first><last>Vateekul</last><affiliation>Chulalongkorn University</affiliation></author>
      <pages>101-109</pages>
      <abstract>Prompt-based learning has shown its effectiveness in few-shot text classification. A key factor in its success is a verbalizer, which translates output from a language model into a predicted class. Notably, the simplest and widely acknowledged verbalizer employs manual labels to represent the classes. However, manual selection may not yield the optimal words for a given language model, potentially leading to subpar classification performance, especially in mid-to-low resource languages with weaker language models. Therefore, we propose Label-Aware Automatic Verbalizer (LAAV), effectively augmenting manual labels for improved few-shot classification results. Specifically, we utilize the label name along with the conjunction “and” to induce the model to generate more effective words for the verbalizer. Experimental results on four mid-to-low resource Southeast Asian languages demonstrate that LAAV significantly outperforms existing verbalizers.</abstract>
      <url hash="bd759d3b">2024.acl-srw.19</url>
      <bibkey>thaminkaew-etal-2024-label</bibkey>
      <doi>10.18653/v1/2024.acl-srw.19</doi>
    </paper>
    <paper id="20">
      <title>Vector Spaces for Quantifying Disparity of Multiword Expressions in Annotated Text</title>
      <author><first>Louis</first><last>Estève</last></author>
      <author><first>Agata</first><last>Savary</last><affiliation>Université Paris-Saclay</affiliation></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <pages>110-130</pages>
      <abstract>Multiword Expressions (MWEs) make a goodcase study for linguistic diversity due to theiridiosyncratic nature. Defining MWE canonicalforms as types, diversity may be measurednotably through disparity, based on pairwisedistances between types. To this aim, wetrain static MWE-aware word embeddings forverbal MWEs in 14 languages, and we showinteresting properties of these vector spaces.We use these vector spaces to implement theso-called functional diversity measure. Weapply this measure to the results of severalMWE identification systems. We find that,although MWE vector spaces are meaningful ata local scale, the disparity measure aggregatingthem at a global scale strongly correlateswith the number of types, which questions itsusefulness in presence of simpler diversitymetrics such as variety. We make the vectorspaces we generated available.</abstract>
      <url hash="6321ffb4">2024.acl-srw.20</url>
      <bibkey>estve-etal-2024-vector</bibkey>
      <doi>10.18653/v1/2024.acl-srw.20</doi>
    </paper>
    <paper id="21">
      <title>Narratives at Conflict: Computational Analysis of News Framing in Multilingual Disinformation Campaigns</title>
      <author><first>Antonina</first><last>Sinelnik</last></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>131-143</pages>
      <abstract>Any report frames issues to favor a particular interpretation by highlighting or excluding certain aspects of a story. Despite the widespread use of framing in disinformation, framing properties and detection methods remain underexplored outside the English-speaking world. We explore how multilingual framing of the same issue differs systematically. We use eight years of Russia-backed disinformation campaigns, spanning 8k news articles in 4 languages targeting 15 countries. We find that disinformation campaigns consistently and intentionally favor specific framing, depending on the target language of the audience. We further discover how Russian-language articles consistently highlight selected frames depending on the region of the media coverage. We find that the two most prominent models for automatic frame analysis underperform and show high disagreement, highlighting the need for further research.</abstract>
      <url hash="5d5a1c27">2024.acl-srw.21</url>
      <bibkey>sinelnik-hovy-2024-narratives</bibkey>
      <doi>10.18653/v1/2024.acl-srw.21</doi>
    </paper>
    <paper id="22">
      <title>Assessing In-context Learning and Fine-tuning for Topic Classification of <fixed-case>G</fixed-case>erman Web Data</title>
      <author><first>Julian</first><last>Schelb</last><affiliation>Universität Konstanz</affiliation></author>
      <author><first>Roberto</first><last>Ulloa</last></author>
      <author><first>Andreas</first><last>Spitz</last><affiliation>Universität Konstanz</affiliation></author>
      <pages>144-158</pages>
      <abstract>Researchers in the political and social sciences often rely on classification models to analyze trends in information consumption by examining browsing histories of millions of webpages. Automated scalable methods are necessary due to the impracticality of manual labeling. In this paper, we model the detection of topic-related content as a binary classification task and compare the accuracy of fine-tuned pre-trained encoder models against in-context learning strategies. Using only a few hundred annotated data points per topic, we detect content related to three German policies in a database of scraped webpages. We compare multilingual and monolingual models, as well as zero and few-shot approaches, and investigate the impact of negative sampling strategies and the combination of URL &amp; content-based features. Our results show that a small sample of annotated data is sufficient to train an effective classifier. Fine-tuning encoder-based models yields better results than in-context learning. Classifiers using both URL &amp; content-based features perform best, while using URLs alone provides adequate results when content is unavailable.</abstract>
      <url hash="81f4b458">2024.acl-srw.22</url>
      <bibkey>schelb-etal-2024-assessing</bibkey>
      <doi>10.18653/v1/2024.acl-srw.22</doi>
    </paper>
    <paper id="23">
      <title>Knowledge Editing of Large Language Models Unconstrained by Word Order</title>
      <author><first>Ryoma</first><last>Ishigaki</last></author>
      <author><first>Jundai</first><last>Suzuki</last><affiliation>Tokyo Denki University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Masaki</first><last>Shuzo</last><affiliation>Tokyo Denki University</affiliation></author>
      <author><first>Eisaku</first><last>Maeda</last><affiliation>Tokyo Denki University</affiliation></author>
      <pages>159-169</pages>
      <abstract>Large Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the LLMs themselves. To address this issue, a method called local modification-based knowledge editing has been developed. This method identifies the knowledge neurons that encode the target knowledge and adjusts the parameters associated with these neurons to update the knowledge. Knowledge neurons are identified by masking the <tex-math>\it{o}</tex-math> part from sentences representing relational triplets (<tex-math>\it{s, r, o}</tex-math>), having the LLM predict the masked part, and observing the LLM's activation during the prediction. When the architecture is decoder-based, the predicted <tex-math>\it{o}</tex-math> needs to be located at the end of the sentence. Previous local modification-based knowledge editing methods for decoder-based models have assumed SVO languages and faced challenges when applied to SOV languages such as Japanese. In this study, we propose a knowledge editing method that eliminates the need for word order constraints by converting the input for identifying knowledge neurons into a question where <tex-math>\it{o}</tex-math> is the answer. We conducted validation experiments on 500 examples and confirmed that the proposed method is effective for Japanese, a non-SVO language. We also applied this method to English, an SVO language, and demonstrated that it outperforms conventional methods.</abstract>
      <url hash="115421f1">2024.acl-srw.23</url>
      <bibkey>ishigaki-etal-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.acl-srw.23</doi>
    </paper>
    <paper id="24">
      <title>Exploring the Effectiveness and Consistency of Task Selection in Intermediate-Task Transfer Learning</title>
      <author><first>Pin-Jie</first><last>Lin</last></author>
      <author><first>Miaoran</first><last>Zhang</last><affiliation>Saarland University</affiliation></author>
      <author><first>Marius</first><last>Mosbach</last><affiliation>McGill University and Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last><affiliation>Saarland University</affiliation></author>
      <pages>170-185</pages>
      <abstract>Identifying beneficial tasks to transfer from is a critical step toward successful intermediate-task transfer learning. In this work, we experiment with 130 source-target task combinations and demonstrate that the transfer performance exhibits severe variance across different source tasks and training seeds, highlighting the crucial role of intermediate-task selection in a broader context. We compare four representative task selection methods in a unified setup, focusing on their effectiveness and consistency. Compared to embedding-free methods and text embeddings, task embeddings constructed from fine-tuned weights can better estimate task transferability by improving task prediction scores from 2.59% to 3.96%. Despite their strong performance, we observe that the task embeddings do not consistently demonstrate superiority for tasks requiring reasoning abilities. Furthermore, we introduce a novel method that measures pairwise token similarity using maximum inner product search, leading to the highest performance in task prediction. Our findings suggest that token-wise similarity is better predictive for predicting transferability compared to averaging weights.</abstract>
      <url hash="a0631551">2024.acl-srw.24</url>
      <bibkey>lin-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.acl-srw.24</doi>
    </paper>
    <paper id="25">
      <title>Does the structure of textual content have an impact on language models for automatic summarization?</title>
      <author><first>Eve</first><last>Sauvage</last></author>
      <author><first>Sabrina</first><last>Campano</last><affiliation>EDF R&amp;D</affiliation></author>
      <author><first>Lydia</first><last>Ouali</last></author>
      <author><first>Cyril</first><last>Grouin</last><affiliation>CNRS</affiliation></author>
      <pages>186-191</pages>
      <abstract>The processing of long sequences with models remains a subject in its own right, including automatic summary, despite recent improvements. In this work, we present experiments on the automatic summarization of scientific articles using BART models, taking into account textual information coming from distinct passages from the long texts to be summarized. We demonstrate that taking into account document structure improves the performance of state-of-the-art models and approaches the performance of LongFormer on English.</abstract>
      <url hash="b378c57e">2024.acl-srw.25</url>
      <bibkey>sauvage-etal-2024-structure</bibkey>
      <doi>10.18653/v1/2024.acl-srw.25</doi>
    </paper>
    <paper id="26">
      <title>Action Inference for Destination Prediction in Vision-and-Language Navigation</title>
      <author><first>Anirudh</first><last>Kondapally</last><affiliation>Honda R&amp;D Co., Ltd.</affiliation></author>
      <author><first>Kentaro</first><last>Yamada</last><affiliation>Honda R&amp;D Co., Ltd.</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <pages>192-199</pages>
      <abstract>Vision-and-Language Navigation (VLN) encompasses interacting with autonomous vehicles using language and visual input from the perspective of mobility.Most of the previous work in this field focuses on spatial reasoning and the semantic grounding of visual information.However, reasoning based on the actions of pedestrians in the scene is not much considered.In this study, we provide a VLN dataset for destination prediction with action inference to investigate the extent to which current VLN models perform action inference.We introduce a crowd-sourcing process to construct a dataset for this task in two steps: (1) collecting beliefs about the next action for a pedestrian and (2) annotating the destination considering the pedestrian’s next action.Our benchmarking results of the models on destination prediction lead us to believe that the models can learn to reason about the effect of the action and the next action on the destination to a certain extent.However, there is still much scope for improvement.</abstract>
      <url hash="0a4c0c54">2024.acl-srw.26</url>
      <bibkey>kondapally-etal-2024-action</bibkey>
      <doi>10.18653/v1/2024.acl-srw.26</doi>
    </paper>
    <paper id="27">
      <title>A Computational Analysis and Exploration of Linguistic Borrowings in <fixed-case>F</fixed-case>rench Rap Lyrics</title>
      <author><first>Lucas</first><last>Zurbuchen</last></author>
      <author><first>Rob</first><last>Voigt</last><affiliation>Northwestern University</affiliation></author>
      <pages>200-208</pages>
      <abstract>In France, linguistic borrowings in the relatively conservative French language are an important site of cultural debate, and rap in particular is a hotspot for borrowings. In this work, we use computational methods to understand the factors that affect the prominence and prevalence of a borrowing. To do so, we manually annotate a lexicon of over 700 borrowings occurring in this context (including key aspects for each borrowing such as origin and semantic class). We analyze the prevalence of these borrowings in a newly collected corpus of over 8000 French rap song lyrics and find that there are increases in the proportion of linguistic borrowings, interjections, and Niger-Congo borrowings while terms related to the arts are decreasing in prevalence. We release our code and data to facilitate further research in this area and discuss potential future directions.</abstract>
      <url hash="d6346cb3">2024.acl-srw.27</url>
      <bibkey>zurbuchen-voigt-2024-computational</bibkey>
      <doi>10.18653/v1/2024.acl-srw.27</doi>
    </paper>
    <paper id="28">
      <title>On Improving Repository-Level Code <fixed-case>QA</fixed-case> for Large Language Models</title>
      <author><first>Jan</first><last>Strich</last></author>
      <author><first>Florian</first><last>Schneider</last><affiliation>Universit�t Hamburg</affiliation></author>
      <author><first>Irina</first><last>Nikishina</last></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>U Hamburg</affiliation></author>
      <pages>209-244</pages>
      <abstract>Large Language Models (LLMs) such as ChatGPT, GitHub Copilot, Llama, or Mistral assist programmers as copilots and knowledge sources to make the coding process faster and more efficient. This paper aims to improve the copilot performance by implementing different self-alignment processes and retrieval-augmented generation (RAG) pipelines, as well as their combination. To test the effectiveness of all approaches, we create a dataset and apply a model-based evaluation, using LLM as a judge. It is designed to check the model’s abilities to understand the source code semantics, the dependency between files, and the overall meta-information about the repository. We also compare our approach with other existing solutions, e.g. ChatGPT-3.5, and evaluate on the existing benchmarks. Code and dataset are available online (https://anonymous.4open.science/r/ma_llm-382D).</abstract>
      <url hash="3dba2e84">2024.acl-srw.28</url>
      <bibkey>strich-etal-2024-improving</bibkey>
      <doi>10.18653/v1/2024.acl-srw.28</doi>
    </paper>
    <paper id="29">
      <title>Compromesso! <fixed-case>I</fixed-case>talian Many-Shot Jailbreaks undermine the safety of Large Language Models</title>
      <author><first>Fabio</first><last>Pernisi</last></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Paul</first><last>R�ttger</last><affiliation>Bocconi University</affiliation></author>
      <pages>245-251</pages>
      <abstract>As diverse linguistic communities and users adopt Large Language Models (LLMs), assessing their safety across languages becomes critical. Despite ongoing efforts to align these models with safe and ethical guidelines, they can still be induced into unsafe behavior with jailbreaking, a technique in which models are prompted to act outside their operational guidelines. What research has been conducted on these vulnerabilities was predominantly on English, limiting the understanding of LLM behavior in other languages. We address this gap by investigating Many-Shot Jailbreaking (MSJ) in Italian, underscoring the importance of understanding LLM behavior in different languages. We base our analysis on a newly created Italian dataset to identify unique safety vulnerabilities in 4 families of open-source LLMs.We find that the models exhibit unsafe behaviors even with minimal exposure to harmful prompts, and–more alarmingly–this tendency rapidly escalates with more demonstrations.</abstract>
      <url hash="5b5c8ec0">2024.acl-srw.29</url>
      <bibkey>pernisi-etal-2024-compromesso</bibkey>
      <doi>10.18653/v1/2024.acl-srw.29</doi>
    </paper>
    <paper id="31">
      <title><fixed-case>V</fixed-case>i<fixed-case>M</fixed-case>ed<fixed-case>AQA</fixed-case>: A <fixed-case>V</fixed-case>ietnamese Medical Abstractive Question-Answering Dataset and Findings of Large Language Model</title>
      <author><first>Minh-Nam</first><last>Tran</last></author>
      <author><first>Phu-Vinh</first><last>Nguyen</last></author>
      <author><first>Long</first><last>Nguyen</last><affiliation>Ho Chi Minh city University of Science, Vietnam National University</affiliation></author>
      <author><first>Dien</first><last>Dinh</last></author>
      <pages>252-260</pages>
      <abstract>Question answering involves creating answers to questions. With the growth of large language models, the ability of question-answering systems has dramatically improved. However, there is a lack of Vietnamese abstractive question-answering datasets, especially in the medical domain. Therefore, this research aims to mitigate this gap by introducing ViMedAQA. This **Vi**etnamese **Med**ical **A**bstractive **Q**uestion-**A**nswering dataset covers four topics in the Vietnamese medical domain, including body parts, disease, drugs and medicine. Additionally, the empirical results on the proposed dataset examine the capability of the large language models in the Vietnamese medical domain, including reasoning, memorizing and awareness of essential information.</abstract>
      <url hash="8ffe4f78">2024.acl-srw.31</url>
      <bibkey>tran-etal-2024-vimedaqa</bibkey>
      <doi>10.18653/v1/2024.acl-srw.31</doi>
    </paper>
    <paper id="32">
      <title>Rescue: Ranking <fixed-case>LLM</fixed-case> Responses with Partial Ordering to Improve Response Generation</title>
      <author><first>Yikun</first><last>Wang</last></author>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Haoming</first><last>Li</last></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author id="fei-liu"><first>Fei</first><last>Liu</last><affiliation>Emory University</affiliation></author>
      <pages>261-272</pages>
      <abstract>Customizing LLMs for a specific task involves separating high-quality responses from lower-quality ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining a large volume of expert-annotated data is costly for most tasks. In this paper, we explore a novel method to optimize LLMs using ranking metrics. This method trains the model to prioritize the best responses from a pool of candidates created for a particular task. Rather than a traditional full ordering, we advocate for a partial ordering, as achieving consensus on the perfect order of candidate responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be achieved with limited human annotations or through heuristic methods. We test our system’s improved response generation ability using benchmark datasets, including textual entailment and multi-document question answering. We conduct ablation studies to understand crucial factors, such as how to gather candidate responses for a specific task, determine their most suitable order, and balance supervised fine-tuning with ranking metrics. Our approach, named RESCUE, offers a promising avenue for enhancing the response generation and task accuracy of LLMs.</abstract>
      <url hash="cc5abbb7">2024.acl-srw.32</url>
      <bibkey>wang-etal-2024-rescue</bibkey>
      <doi>10.18653/v1/2024.acl-srw.32</doi>
    </paper>
    <paper id="33">
      <title>Basreh or Basra? Geoparsing Historical Locations in the Svoboda Diaries</title>
      <author><first>Jolie</first><last>Zhou</last><affiliation>University of Washington</affiliation></author>
      <author><first>Camille</first><last>Cole</last><affiliation>Illinois State University</affiliation></author>
      <author><first>Annie</first><last>Chen</last><affiliation>University of Washington</affiliation></author>
      <pages>273-286</pages>
      <abstract>Geoparsing, the task of assigning coordinates to locations extracted from free text, is invaluable in enabling us to place locations in time and space. In the historical domain, many geoparsing corpora are from large news collections. We examine the Svoboda Diaries, a small historical corpus written primarily in English, with many location names in transliterated Arabic. We develop a pipeline employing named entity recognition for geotagging, and a map-based generate-and-rank approach incorporating candidate name augmentation and clustering of location context words for geocoding. Our system outperforms existing map-based geoparsers in terms of accuracy, lowest mean distance error, and number of locations correctly identified. As location names may vary from those in knowledge bases, we find that augmented candidate generation is instrumental in the system’s performance. Among our candidate generation methods, the generation of transliterated names contributed the most to increased location matches in the knowledge base. Our main contribution is proposing an integrated pipeline for geoparsing of historical corpora using augmented candidate location name generation and clustering methods – an approach that can be generalized to other texts with foreign or non-standard spellings.</abstract>
      <url hash="22cb71e6">2024.acl-srw.33</url>
      <bibkey>zhou-etal-2024-basreh</bibkey>
      <doi>10.18653/v1/2024.acl-srw.33</doi>
    </paper>
    <paper id="34">
      <title><fixed-case>H</fixed-case>omophone2<fixed-case>V</fixed-case>ec: Embedding Space Analysis for Empirical Evaluation of Phonological and Semantic Similarity</title>
      <author><first>Sophie</first><last>Wu</last><affiliation>McGill University</affiliation></author>
      <author><first>Anita</first><last>Zheng</last><affiliation>McGill University</affiliation></author>
      <author><first>Joey</first><last>Chuang</last><affiliation>McGill University</affiliation></author>
      <pages>287-292</pages>
      <abstract>This paper introduces a novel method for empirically evaluating the relationship between the phonological and semantic similarity of linguistic units using embedding spaces. Chinese character homophones are used as a proof-of-concept. We employ cosine similarity as a proxy for semantic similarity between characters, and compare relationships between phonologically-related characters and baseline characters (chosen as similar-frequency characters). We show there is a strongly statistically significant positive semantic relationship among different Chinese characters at varying levels of sound-sharing. We also perform some basic probing using t-SNE and UMAP visualizations, and indicate directions for future applications of this method.</abstract>
      <url hash="ccee2b3e">2024.acl-srw.34</url>
      <bibkey>wu-etal-2024-homophone2vec</bibkey>
      <doi>10.18653/v1/2024.acl-srw.34</doi>
    </paper>
    <paper id="35">
      <title>Trace-of-Thought Prompting: Investigating Prompt-Based Knowledge Distillation Through Question Decomposition</title>
      <author><first>Tyler</first><last>McDonald</last><affiliation>Brock University</affiliation></author>
      <author><first>Ali</first><last>Emami</last><affiliation>Brock University</affiliation></author>
      <pages>293-306</pages>
      <abstract>Knowledge distillation allows smaller neural networks to emulate the performance of larger, teacher models with reduced computational demands. Traditional methods for Large Language Models (LLMs) often necessitate extensive fine-tuning, which limits their accessibility. To address this, we introduce Trace-of-Thought Prompting, a novel framework designed to distill critical reasoning capabilities from large-scale teacher models (over 8 billion parameters) to small-scale student models (up to 8 billion parameters). This approach leverages problem decomposition to enhance interpretability and facilitate human-in-the-loop interventions. Empirical evaluations on the GSM8K and MATH datasets show that student models achieve accuracy gains of up to 113% on GSM8K and 20% on MATH, with significant improvements particularly notable in smaller models like Llama 2 and Zephyr. Our results suggest a promising pathway for open-source, small-scale models to eventually serve as both students and teachers, potentially reducing our reliance on large-scale, proprietary models. Our code, featuring data analytics and testing scripts, is provided here: https://github.com/traceofthought/trace-of-thought-prompting/tree/main.</abstract>
      <url hash="c6c9ea7c">2024.acl-srw.35</url>
      <bibkey>mcdonald-emami-2024-trace</bibkey>
      <doi>10.18653/v1/2024.acl-srw.35</doi>
    </paper>
    <paper id="36">
      <title>Can <fixed-case>LLM</fixed-case>s Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges</title>
      <author><first>Vinay</first><last>Samuel</last></author>
      <author><first>Houda</first><last>Aynaou</last></author>
      <author><first>Arijit</first><last>Chowdhury</last><affiliation>Amazon</affiliation></author>
      <author><first>Karthik</first><last>Venkat Ramanan</last></author>
      <author><first>Aman</first><last>Chadha</last><affiliation>Amazon</affiliation></author>
      <pages>307-317</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive zero-shot performance on a wide range of NLP tasks, demonstrating the ability to reason and apply common sense. A relevant application is to use them for creating high-quality synthetic datasets for downstream tasks. In this work, we probe whether GPT-4 can be used to augment existing extractive reading comprehension datasets. Automating data annotation processes has the potential to save large amounts of time, money, and effort that goes into manually labeling datasets. In this paper, we evaluate the performance of GPT-4 as a replacement for human annotators for low-resource reading comprehension tasks, by comparing performance after fine-tuning, and the cost associated with annotation. This work serves to be the first analysis of LLMs as synthetic data augmenters for QA systems, highlighting the unique opportunities and challenges. Additionally, we release augmented versions of low-resource datasets, that will allow the research community to create further benchmarks for evaluation of generated datasets. Github available at https://github.com/vsamuel2003/qa-gpt4</abstract>
      <url hash="b89c25ab">2024.acl-srw.36</url>
      <bibkey>samuel-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.acl-srw.36</doi>
    </paper>
    <paper id="37">
      <title>Automatic Derivation of Semantic Representations for <fixed-case>T</fixed-case>hai Serial Verb Constructions: A Grammar-Based Approach</title>
      <author><first>Vipasha</first><last>Bansal</last></author>
      <pages>318-333</pages>
      <abstract>Deep semantic representations are useful for many NLU tasks (Droganova and Zeman 2019; Schuster and Manning-2016). Manual annotation to build these representations is time-consuming, and so automatic approaches are preferred (Droganova and Zeman 2019; Bender et al. 2015). This paper demonstrates how rich semantic representations can be automatically derived for Thai Serial Verb Constructions (SVCs), where the semantic relationship between component verbs is not immediately clear from the surface forms. I present the first fully-implemented HPSG analysis for Thai SVCs, deriving appropriate semantic representations (MRS; Copestake et al. 2005) from syntactic features, implemented within a DELPH-IN computational grammar (Slayden 2009). This analysis increases verified coverage of SVCs by 73% and decreases ambiguity by 46%. The final grammar can be found at: https://github.com/VipashaB94/ThaiGrammar</abstract>
      <url hash="47b0652a">2024.acl-srw.37</url>
      <bibkey>bansal-2024-automatic</bibkey>
      <doi>10.18653/v1/2024.acl-srw.37</doi>
    </paper>
    <paper id="39">
      <title>Bridging Distribution Gap via Semantic Rewriting with <fixed-case>LLM</fixed-case>s to Enhance <fixed-case>OOD</fixed-case> Robustness</title>
      <author><first>Manas</first><last>Madine</last></author>
      <pages>334-344</pages>
      <abstract>This paper investigates the robustness of Large Language Models (LLMs) against Out-Of-Distribution (OOD) data within the context of sentiment analysis. Traditional fine-tuning approaches often fail to generalize effectively across different data distributions, limiting the practical deployment of LLMs in dynamic real-world scenarios. To address this challenge, we introduce a novel method called “Semantic Rewriting,” which leverages the inherent flexibility of LLMs to align both in-distribution (ID) and OOD data with the LLMs distributions. By semantically transforming sentences to minimize linguistic discrepancies, our approach helps to standardize features across datasets, thus enhancing model robustness. We conduct extensive experiments with several benchmark datasets and LLMs to validate the efficacy of our method. The results demonstrate that Semantic Rewriting significantly improves the performance of models on OOD tasks, outperforming traditional methods in both robustness and generalization capabilities. Our findings suggest that Semantic Rewriting is a promising technique for developing more reliable and versatile NLP systems capable of performing robustly across diverse operational environments.</abstract>
      <url hash="01f7a2c0">2024.acl-srw.39</url>
      <bibkey>madine-2024-bridging</bibkey>
      <doi>10.18653/v1/2024.acl-srw.39</doi>
    </paper>
    <paper id="40">
      <title><fixed-case>C</fixed-case>o<fixed-case>V</fixed-case>o<fixed-case>S</fixed-case>witch: Machine Translation of Synthetic Code-Switched Text Based on Intonation Units</title>
      <author><first>Yeeun</first><last>Kang</last></author>
      <pages>345-357</pages>
      <abstract>Multilingual code-switching research is often hindered by the lack and linguistically biased status of available datasets. To expand language representation, we synthesize code-switching data by replacing intonation units detected through PSST, a speech segmentation model fine-tuned from OpenAI’s Whisper, using a speech-to-text translation dataset, CoVoST 2. With our dataset, CoVoSwitch, spanning 13 languages, we evaluate the code-switching translation performance of two multilingual translation models, M2M-100 418M and NLLB-200 600M. We reveal that the inclusion of code-switching units results in higher translation performance than monolingual settings and that models are better at code-switching translation into English than non-English. Further, low-resource languages gain most from integration of code-switched units when translating into English but much less when translating into non-English. Translations into low-resource languages also perform worse than even raw code-switched inputs. We find that systems excel at copying English tokens but struggle with non-English tokens, that the off-target problem in monolingual settings is also relevant in code-switching settings, and that models hallucinate in code-switching translation by introducing words absent in both of the original source sentences. CoVoSwitch and code are available at https://github.com/sophiayk20/covoswitch.</abstract>
      <url hash="b87b975f">2024.acl-srw.40</url>
      <bibkey>kang-2024-covoswitch</bibkey>
      <doi>10.18653/v1/2024.acl-srw.40</doi>
    </paper>
    <paper id="42">
      <title>Beyond Abstracts: A New Dataset, Prompt Design Strategy and Method for Biomedical Synthesis Generation</title>
      <author><first>James</first><last>O’Doherty</last></author>
      <author><first>Cian</first><last>Nolan</last></author>
      <author><first>Yufang</first><last>Hou</last><affiliation>Technische Universit�t Darmstadt and IBM Research Ireland</affiliation></author>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University</affiliation></author>
      <pages>358-377</pages>
      <abstract>The biomedical field relies on cost and time intensive systematic reviews of papers to enable practitioners to keep up to date with research. Impressive recent advances in large language models (LLMs) have made the task of automating at least part of the systematic review process feasible, but progress is slow. This paper identifies some factors that may have been holding research back, and proposes a new, enhanced dataset and prompting-based method for automatic synthesis generation, the most challenging step for automation. We test different models and types of information from and about biomedical studies for their usefulness in obtaining high-quality results.We find that, surprisingly, inclusion of paper abstracts can worsens results. Instead, study summary information, and system instructions informed by domain knowledge, are key to producing high-quality syntheses.</abstract>
      <url hash="4129859d">2024.acl-srw.42</url>
      <bibkey>odoherty-etal-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.acl-srw.42</doi>
    </paper>
    <paper id="43">
      <title>Improving Sentence Embeddings with Automatic Generation of Training Data Using Few-shot Examples</title>
      <author><first>Soma</first><last>Sato</last></author>
      <author><first>Hayato</first><last>Tsukagoshi</last></author>
      <author><first>Ryohei</first><last>Sasano</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Koichi</first><last>Takeda</last><affiliation>Nagoya University</affiliation></author>
      <pages>378-389</pages>
      <abstract>Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL requires a manually annotated natural language inference (NLI) dataset for fine-tuning.We aim to improve sentence embeddings without using large manually annotated datasets by automatically generating an NLI dataset with an LLM and using it for fine-tuning of PromptEOL. To achieve this, we explore methods of data generation suitable for sentence embedding learning in this study. Specifically, we will focus on automatic dataset generation through few-shot learning and explore the appropriate methods to leverage few-shot examples. Experimental results on the STS tasks demonstrate that our approach outperforms existing models in settings without large manually annotated datasets.</abstract>
      <url hash="f55354dd">2024.acl-srw.43</url>
      <bibkey>sato-etal-2024-improving</bibkey>
      <doi>10.18653/v1/2024.acl-srw.43</doi>
    </paper>
    <paper id="44">
      <title>Curriculum Learning for Small Code Language Models</title>
      <author><first>Marwa</first><last>Naïr</last><affiliation>New York University, Abu Dhabi and Ecole Nationale Supérieure d’Informatique</affiliation></author>
      <author><first>Kamel</first><last>Yamani</last><affiliation>New York University, Abu Dhabi and Ecole Nationale Supérieure d’Informatique (ESI)</affiliation></author>
      <author><first>Lynda</first><last>Lhadj</last><affiliation>ESI</affiliation></author>
      <author><first>Riyadh</first><last>Baghdadi</last><affiliation>New York University</affiliation></author>
      <pages>390-401</pages>
      <abstract>Code language models have emerged as useful tools for various programming tasks, yet they often struggle when it comes to complex ones. In this paper, we explore the potential of curriculum learning in enhancing the performance of these models. While prior research has suggested that curriculum learning does not necessarily help in improving the performance of language models, our results surprisingly show that this may not be the case for code language models. We demonstrate that a well-designed curriculum learning approach significantly improves the accuracy of small decoder-only code language models on the task of code execution, while its effect on code completion is less significant. To explore the potential of curriculum learning, we train multiple GPT models with 1 million parameters each to predict the next token and evaluate them on code completion and execution tasks. Our contributions include proposing a novel code difficulty assessment metric by combining software code measures, investigating the effectiveness of Curriculum Learning for code language models, and introducing a Novel Curriculum Learning schedule that enhances the performance of small decoder-only language models in code execution tasks. The results of this paper open the door for more research on the use of curriculum learning for code language models.</abstract>
      <url hash="1afd1526">2024.acl-srw.44</url>
      <bibkey>nair-etal-2024-curriculum</bibkey>
      <doi>10.18653/v1/2024.acl-srw.44</doi>
    </paper>
    <paper id="45">
      <title>Question-Analysis Prompting Improves <fixed-case>LLM</fixed-case> Performance in Reasoning Tasks</title>
      <author><first>Dharunish</first><last>Yugeswardeenoo</last><affiliation>Algoverse</affiliation></author>
      <author><first>Kevin</first><last>Zhu</last><affiliation>Algoverse AI Research</affiliation></author>
      <author><first>Sean</first><last>O’Brien</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>402-413</pages>
      <abstract>Although LLMs have the potential to transform many fields, they still underperform humans in reasoning tasks. Existing methods induce the model to produce step-by-step calculations, but this research explores the question: Does making the LLM analyze the question improve its performance? We propose a novel prompting strategy called Question Analysis Prompting (QAP), in which the model is prompted to explain the question in ’n’ words before solving. The value of ’n’ influences the length of response generated by the model. QAP is evaluated on GPT-3.5 Turbo and GPT-4 Turbo on arithmetic datasets GSM8K, AQuA, and SAT and commonsense dataset StrategyQA. QAP is compared with other state-of-the-art prompts including chain-of-thought (CoT), Plan and Solve Prompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all state-of-the-art prompts on AQuA and SAT datasets on both GPT-3.5 and GPT-4. QAP consistently ranks among the top-2 prompts on 75% of the tests. A key factor of QAP performance can be attributed to response length, where detailed responses are beneficial when answering harder questions, but can negatively affect easy questions.</abstract>
      <url hash="28db8c22">2024.acl-srw.45</url>
      <bibkey>yugeswardeenoo-etal-2024-question</bibkey>
      <doi>10.18653/v1/2024.acl-srw.45</doi>
    </paper>
    <paper id="48">
      <title><fixed-case>C</fixed-case>heckers<fixed-case>GPT</fixed-case>: Learning World Models through Language Modeling</title>
      <author><first>Abhinav</first><last>Joshi</last><affiliation>Indian Institute of Technology, Kanpur</affiliation></author>
      <author><first>Vaibhav</first><last>Sharma</last></author>
      <author><first>Ashutosh</first><last>Modi</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>414-426</pages>
      <abstract>Although Large Language Models (LLMs) have been trained using just the next token prediction objective, these have shown impressive performance on various tasks. Consequently, it has attracted research interests in this regard. While one line of work in the past has suggested that LLMs learn surface-level statistics from the dataset, another line of work emphasizes that the learned representations are effective for simulating the underlying world model, considering the causal relationship for the next token prediction. This phenomenon is often referred to as the emergence of a world model in sequence prediction tasks. Recent work has demonstrated this phenomenon in a simulated setting of board games like Othello and Chess. In this paper, we analyze the game of Checkers to find out the emergence of a world model in a language model. By training a GPT-style autoregressive language model using only the next character prediction objective, we find that the model does show a hint of learning a world model representation of the board positions. We perform our analysis on two datasets: 1) synthetic dataset, which comes from the checkers game tree, and 2) human gameplay dataset. With multiple models trained with different layer sizes, we find that increasing the parameter size does help learn better world model representation decoded by linear probes.</abstract>
      <url hash="93839814">2024.acl-srw.48</url>
      <bibkey>joshi-etal-2024-checkersgpt</bibkey>
      <doi>10.18653/v1/2024.acl-srw.48</doi>
    </paper>
    <paper id="49">
      <title>In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery</title>
      <author><first>Matteo</first><last>Merler</last></author>
      <author><first>Katsiaryna</first><last>Haitsiukevich</last><affiliation>Aalto University</affiliation></author>
      <author><first>Nicola</first><last>Dainese</last><affiliation>Aalto University</affiliation></author>
      <author><first>Pekka</first><last>Marttinen</last><affiliation>Aalto University</affiliation></author>
      <pages>427-444</pages>
      <abstract>State of the art Symbolic Regression (SR) methods currently build specialized models, while the application of Large Language Models (LLMs) remains largely unexplored. In this work, we introduce the first comprehensive framework that utilizes LLMs for the task of SR.We propose In-Context Symbolic Regression (ICSR), an SR method which iteratively refines a functional form with an LLM and determines its coefficients with an external optimizer. ICSR leverages LLMs’ strong mathematical prior both to propose an initial set of possible functions given the observations and to refine them based on their errors.Our findings reveal that LLMs are able to successfully find symbolic equations that fit the given data, matching or outperforming the overall performance of the best SR baselines on four popular benchmarks, while yielding simpler equations with better out of distribution generalization.</abstract>
      <url hash="4321195c">2024.acl-srw.49</url>
      <bibkey>merler-etal-2024-context</bibkey>
      <doi>10.18653/v1/2024.acl-srw.49</doi>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2024-08-11" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 5: Tutorial Abstracts)</booktitle>
      <editor><first>Luis</first><last>Chiruzzo</last><affiliation>Universidad de la República</affiliation></editor>
      <editor><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></editor>
      <editor><first>Leonardo</first><last>Ribeiro</last><affiliation>Amazon Alexa Seattle</affiliation></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="7c0a93a8">2024.acl-tutorials</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="612d13b1">2024.acl-tutorials.0</url>
      <bibkey>acl-2024-tutorials</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Computational Linguistics for Brain Encoding and Decoding: Principles, Practices and Beyond</title>
      <author><first>Jingyuan</first><last>Sun</last></author>
      <author><first>Shaonan</first><last>Wang</last></author>
      <author><first>Zijiao</first><last>Chen</last></author>
      <author><first>Jixing</first><last>Li</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>1-2</pages>
      <abstract>Computational linguistics (CL) has witnessed tremendous advancements in recent years, with models such as large language models demonstrating exceptional performance in various natural language processing tasks. These advancements highlight their potential to help understand brain language processing, especially through the lens of brain encoding and decoding. Brain encoding involves the mapping of linguistic stimuli to brain activity, while brain decoding is the process of reconstructing linguistic stimuli from observed brain activities. CL models that excel at capturing and manipulating linguistic features are crucial for mapping linguistic stimuli to brain activities and vice versa. Brain encoding and decoding have vast applications, from enhancing human-computer interaction to developing assistive technologies for individuals with communication impairments. This tutorial will focus on elucidating how computational linguistics can facilitate brain encoding and decoding. We will delve into the principles and practices of using computational linguistics methods for brain encoding and decoding. We will also discuss the challenges and future directions of brain encoding and decoding. Through this tutorial, we aim to provide a comprehensive and informative overview of the intersection between computational linguistics and cognitive neuroscience, inspiring future research in this exciting and rapidly evolving field.</abstract>
      <url hash="9dda3d29">2024.acl-tutorials.1</url>
      <bibkey>sun-etal-2024-computational</bibkey>
      <doi>10.18653/v1/2024.acl-tutorials.1</doi>
    </paper>
    <paper id="2">
      <title>Automatic and Human-<fixed-case>AI</fixed-case> Interactive Text Generation (with a focus on Text Simplification and Revision)</title>
      <author><first>Yao</first><last>Dou</last></author>
      <author><first>Philippe</first><last>Laban</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <pages>3-4</pages>
      <abstract>In this tutorial, we focus on text-to-text generation, a class of natural language generation (NLG) tasks, that takes a piece of text as input and then generates a revision that is improved according to some specific criteria (e.g., readability or linguistic styles), while largely retaining the original meaning and the length of the text. This includes many useful applications, such as text simplification, paraphrase generation, style transfer, etc. In contrast to text summarization and open-ended text completion (e.g., story), the text-to-text generation tasks we discuss in this tutorial are more constrained in terms of semantic consistency and targeted language styles. This level of control makes these tasks ideal testbeds for studying the ability of models to generate text that is both semantically adequate and stylistically appropriate. Moreover, these tasks are interesting from a technical standpoint, as they require complex combinations of lexical and syntactical transformations, stylistic control, and adherence to factual knowledge, – all at once. With a special focus on text simplification and revision, this tutorial aims to provide an overview of the state-of-the-art natural language generation research from four major aspects – Data, Models, Human-AI Collaboration, and Evaluation – and to discuss and showcase a few significant and recent advances: (1) the use of non-retrogressive approaches; (2) the shift from fine-tuning to prompting with large language models; (3) the development of new learnable metric and fine-grained human evaluation framework; (4) a growing body of studies and datasets on non-English languages; (5) the rise of HCI+NLP+Accessibility interdisciplinary research to create real-world writing assistant systems.</abstract>
      <url hash="4a3a0a71">2024.acl-tutorials.2</url>
      <bibkey>dou-etal-2024-automatic</bibkey>
      <doi>10.18653/v1/2024.acl-tutorials.2</doi>
    </paper>
    <paper id="3">
      <title>Computational Expressivity of Neural Language Models</title>
      <author><first>Alexandra</first><last>Butoi</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Anej</first><last>Svete</last></author>
      <pages>5-5</pages>
      <abstract>Language models (LMs) are currently at the forefront of NLP research due to their remarkable versatility across diverse tasks. However, a large gap exists between their observed capabilities and the explanations proposed by established formal machinery. To motivate a better theoretical characterization of LMs’ abilities and limitations, this tutorial aims to provide a comprehensive introduction to a specific framework for formal analysis of modern LMs using tools from formal language theory (FLT). We present how tools from FLT can be useful in understanding the inner workings and predicting the capabilities of modern neural LM architectures. We will cover recent results using FLT to make precise and practically relevant statements about LMs based on recurrent neural networks and transformers by relating them to formal devices such as finite-state automata, Turing machines, and analog circuits. Altogether, the results covered in this tutorial will allow us to make precise statements and explanations about the observed as well as predicted behaviors of LMs, as well as provide theoretically motivated suggestions on the aspects of the architectures that could be improved.</abstract>
      <url hash="89e4331d">2024.acl-tutorials.3</url>
      <bibkey>butoi-etal-2024-computational</bibkey>
      <doi>10.18653/v1/2024.acl-tutorials.3</doi>
      <revision id="1" href="2024.acl-tutorials.3v1" hash="adedb189"/>
      <revision id="2" href="2024.acl-tutorials.3v2" hash="7dc00d4e" date="2024-10-11">Minor update.</revision>
    </paper>
    <paper id="4">
      <title>Presentation Matters: How to Communicate Science in the <fixed-case>NLP</fixed-case> Venues and in the Wild</title>
      <author><first>Sarvnaz</first><last>Karimi</last></author>
      <author><first>Cecile</first><last>Paris</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>6-7</pages>
      <abstract>Each year a large number of early career researchers join the NLP/Computational Linguistics community, with most starting by presenting their research in the *ACL conferences and workshops. While writing a paper that has made it to these venues is one important step, what comes with communicating the outcome is equally important and sets the path to impact of a research outcome. In addition, not all PhD candidates get the chance of being trained for their presentation skills. Research methods courses are not all of the same quality and may not cover scientific communications, and certainly not all are tailored to the NLP community. We are proposing an introductory tutorial that covers a range of different communication skills, including writing, oral presentation (posters and demos), and social media presence. This is to fill in the gap for the researchers who may not have access to research methods courses or other mentors who could help them acquire such skills. The interactive nature of such a tutorial would allow attendees to ask questions and clarifications which would not be possible from reading materials alone.</abstract>
      <url hash="c30f2e87">2024.acl-tutorials.4</url>
      <bibkey>karimi-etal-2024-presentation</bibkey>
      <doi>10.18653/v1/2024.acl-tutorials.4</doi>
      <revision id="1" href="2024.acl-tutorials.4v1" hash="cb503c09"/>
      <revision id="2" href="2024.acl-tutorials.4v2" hash="e3e00c95" date="2024-10-05">Updated to correct pdf.</revision>
    </paper>
    <paper id="5">
      <title>Vulnerabilities of Large Language Models to Adversarial Attacks</title>
      <author><first>Yu</first><last>Fu</last></author>
      <author><first>Erfan</first><last>Shayegan</last></author>
      <author><first>Md. Mamun Al</first><last>Abdullah</last></author>
      <author><first>Pedram</first><last>Zaree</last></author>
      <author><first>Nael</first><last>Abu-Ghazaleh</last></author>
      <author><first>Yue</first><last>Dong</last></author>
      <pages>8-9</pages>
      <abstract>This tutorial serves as a comprehensive guide on the vulnerabilities of Large Language Models (LLMs) to adversarial attacks, an interdisciplinary field that blends perspectives from Natural Language Processing (NLP) and Cybersecurity. As LLMs become more complex and integrated into various systems, understanding their security attributes is crucial. However, current research indicates that even safety-aligned models are not impervious to adversarial attacks that can result in incorrect or harmful outputs. The tutorial first lays the foundation by explaining safety-aligned LLMs and concepts in cybersecurity. It then categorizes existing research based on different types of learning architectures and attack methods. We highlight the existing vulnerabilities of unimodal LLMs, multi-modal LLMs, and systems that integrate LLMs, focusing on adversarial attacks designed to exploit weaknesses and mislead AI systems. Finally, the tutorial delves into the potential causes of these vulnerabilities and discusses potential defense mechanisms.</abstract>
      <url hash="53564850">2024.acl-tutorials.5</url>
      <bibkey>fu-etal-2024-vulnerabilities</bibkey>
      <doi>10.18653/v1/2024.acl-tutorials.5</doi>
    </paper>
    <paper id="6">
      <title>Watermarking for Large Language Models</title>
      <author><first>Xuandong</first><last>Zhao</last></author>
      <author><first>Yu-Xiang</first><last>Wang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>10-11</pages>
      <abstract>As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial in both the computational linguistics and machine learning communities. In this tutorial, we aim to provide an in-depth exploration of text watermarking, a subfield of linguistic steganography with the goal of embedding a hidden message (the watermark) within a text passage. We will introduce the fundamentals of text watermarking, discuss the main challenges in identifying AI-generated text, and delve into the current watermarking methods, assessing their strengths and weaknesses. Moreover, we will explore other possible applications of text watermarking and discuss future directions for this field. Each section will be supplemented with examples and key takeaways.</abstract>
      <url hash="d0e779a1">2024.acl-tutorials.6</url>
      <bibkey>zhao-etal-2024-watermarking</bibkey>
      <doi>10.18653/v1/2024.acl-tutorials.6</doi>
    </paper>
  </volume>
  <event id="acl-2024">
    <colocated>
      <volume-id>2024.findings-acl</volume-id>
      <volume-id>2024.alvr-1</volume-id>
      <volume-id>2024.arabicnlp-1</volume-id>
      <volume-id>2024.argmining-1</volume-id>
      <volume-id>2024.bionlp-1</volume-id>
      <volume-id>2024.c3nlp-1</volume-id>
      <volume-id>2024.climatenlp-1</volume-id>
      <volume-id>2024.cmcl-1</volume-id>
      <volume-id>2024.conda-1</volume-id>
      <volume-id>2024.fieldmatters-1</volume-id>
      <volume-id>2024.gebnlp-1</volume-id>
      <volume-id>2024.hucllm-1</volume-id>
      <volume-id>2024.iwslt-1</volume-id>
      <volume-id>2024.kallm-1</volume-id>
      <volume-id>2024.knowledgenlp-1</volume-id>
      <volume-id>2024.knowllm-1</volume-id>
      <volume-id>2024.langmol-1</volume-id>
      <volume-id>2024.lchange-1</volume-id>
      <volume-id>2024.loresmt-1</volume-id>
      <volume-id>2024.ml4al-1</volume-id>
      <volume-id>2024.nlp4convai-1</volume-id>
      <volume-id>2024.nlrse-1</volume-id>
      <volume-id>2024.privatenlp-1</volume-id>
      <volume-id>2024.repl4nlp-1</volume-id>
      <volume-id>2024.sdp-1</volume-id>
      <volume-id>2024.sighan-1</volume-id>
      <volume-id>2024.sigturk-1</volume-id>
      <volume-id>2024.smm4h-1</volume-id>
      <volume-id>2024.splurobonlp-1</volume-id>
      <volume-id>2024.teachingnlp-1</volume-id>
      <volume-id>2024.textgraphs-1</volume-id>
      <volume-id>2024.wassa-1</volume-id>
    </colocated>
  </event>
</collection>
