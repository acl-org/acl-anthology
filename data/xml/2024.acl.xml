<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.acl">
  <volume id="srw" ingest-date="2024-08-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)</booktitle>
      <editor><first>Xiyan</first><last>Fu</last><affiliation>Heidelberg University</affiliation></editor>
      <editor><first>Eve</first><last>Fleisig</last><affiliation>UC Berkeley</affiliation></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand and virtual meeting</address>
      <month>August</month>
      <year>2024</year>
      <url hash="bed3d6fa">2024.acl-srw</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="e9c7245d">2024.acl-srw.0</url>
    </frontmatter>
    <paper id="1">
      <title>Feriji: A <fixed-case>F</fixed-case>rench-<fixed-case>Z</fixed-case>arma Parallel Corpus, Glossary &amp; Translator</title>
      <author><first>Mamadou</first><last>Keita</last></author>
      <author><first>Elysabhete</first><last>Ibrahim</last></author>
      <author><first>Habibatou</first><last>Alfari</last></author>
      <author><first>Christopher</first><last>Homan</last></author>
      <pages>1-9</pages>
      <abstract>Machine translation (MT) is a rapidly expanding field that has experienced significant advancements in recent years with the development of models capable of translating multiple languages with remarkable accuracy. However, the representation of African languages in this field still needs improvement due to linguistic complexities and limited resources. This applies to the Zarma language, a dialect of Songhay (of the Nilo-Saharan language family) spoken by over 5 million people across Niger and neighboring countries (Lewis et al., 2016). This paper introduces Feriji, the first robust French-Zarma parallel corpus and glossary designed for MT. The corpus, containing 61,085 sentences in Zarma and 42,789 in French, and a glossary of 4,062 words represents a significant step in addressing the need for more resources for Zarma. We fine-tune three large language models on our dataset, obtaining a BLEU score of 30.06 on the best-performing model. We further evaluate the models on human judgments of fluency, comprehension, and readability and the importance and impact of the corpus and models. Our contributions help to bridge a significant language gap and promote an essential and overlooked indigenous African language.</abstract>
      <url hash="64cb8c14">2024.acl-srw.1</url>
    </paper>
    <paper id="2">
      <title>Pragmatic inference of scalar implicature by <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ye-eun</first><last>Cho</last></author>
      <author><first>Ismkim99@skku.edu</first><last>Ismkim99@skku.edu</last><affiliation>NA</affiliation></author>
      <pages>10-20</pages>
      <abstract>This study investigates how Large Language Models (LLMs), particularly BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic inference of scalar implicature, such as some. Two sets of experiments were conducted using cosine similarity and next sentence/token prediction as experimental methods. The results in experiment 1 showed that, both models interpret some as pragmatic implicature not all in the absence of context, aligning with human language processing. In experiment 2, in which Question Under Discussion (QUD) was presented as a contextual cue, BERT showed consistent performance regardless of types of QUDs, while GPT-2 encountered processing difficulties since a certain type of QUD required pragmatic inference for implicature. The findings revealed that, in terms of theoretical approaches, BERT inherently incorporates pragmatic implicature not all within the term some, adhering to Default model (Levinson, 2000). In contrast, GPT-2 seems to encounter processing difficulties in inferring pragmatic implicature within context, consistent with Context-driven model (Sperber and Wilson, 2002).</abstract>
      <url hash="b056f7d3">2024.acl-srw.2</url>
    </paper>
    <paper id="3">
      <title>Topic Modeling for Short Texts with Large Language Models</title>
      <author><first>Tomoki</first><last>Doi</last></author>
      <author><first>Masaru</first><last>Isonuma</last></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <pages>21-33</pages>
      <abstract>As conventional topic models rely on word co-occurrence to infer latent topics, topic modeling for short texts has been a long-standing challenge. Large Language Models (LLMs) can potentially overcome this challenge by contextually learning the meanings of words via pretraining. In this paper, we study two approaches to using LLMs for topic modeling: parallel prompting and sequential prompting. Input length limitations prevent LLMs from processing many texts at once. However, an arbitrary number of texts can be handled by LLMs by splitting the texts into smaller subsets and processing them in parallel or sequentially. Our experimental results demonstrate that our methods can identify more coherent topics than existing ones while maintaining the diversity of the induced topics. Furthermore, we found that the inferred topics cover the input texts to some extent, while hallucinated topics are hardly generated.</abstract>
      <url hash="49f3cdbb">2024.acl-srw.3</url>
    </paper>
    <paper id="4">
      <title>Can <fixed-case>LLM</fixed-case>s substitute <fixed-case>SQL</fixed-case>? Comparing Resource Utilization of Querying <fixed-case>LLM</fixed-case>s versus Traditional Relational Databases</title>
      <author><first>Xiang</first><last>Zhang</last><affiliation>Metropolitan College, Boston University</affiliation></author>
      <author><first>Khatoon</first><last>Khedri</last></author>
      <author><first>Reza</first><last>Rawassizadeh</last></author>
      <pages>34-41</pages>
      <abstract>Large Language Models (LLMs) can automate or substitute different types of tasks in the software engineering process. This study evaluates the resource utilization and accuracy of LLM in interpreting and executing natural language queries against traditional SQL within relational database management systems. We empirically examine the resource utilization and accuracy of nine LLMs varying from 7 to 34 Billion parameters, including Llama2 7B, Llama2 13B, Mistral, Mixtral, Optimus-7B, SUS-chat-34B, platypus-yi-34b, NeuralHermes-2.5-Mistral-7B and Starling-LM-7B-alpha, using a small transaction dataset. Our findings indicate that using LLMs for database queries incurs significant energy overhead (even small and quantized models), making it an environmentally unfriendly approach. Therefore, we advise against replacing relational databases with LLMs due to their substantial resource utilization.</abstract>
      <url hash="5a5d8056">2024.acl-srw.4</url>
    </paper>
    <paper id="5">
      <title>Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer</title>
      <author><first>Yongqi</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Bai</first><last>Jionghao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Rongjie</first><last>Huang</last><affiliation>FAIR</affiliation></author>
      <author><first>Ruiqi</first><last>Li</last></author>
      <author><first>Zhiqing</first><last>Hong</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>42-49</pages>
      <abstract>Direct speech-to-speech translation (S2ST) with discrete self-supervised representations has achieved remarkable accuracy, but is unable to preserve the speaker timbre of the source speech. Meanwhile, the scarcity of high-quality speaker-parallel data poses a challenge for learning style transfer during translation. We design an S2ST pipeline with style-transfer capability on the basis of discrete self-supervised speech representations and codec units. The acoustic language model we introduce for style transfer leverages self-supervised in-context learning, acquiring style transfer ability without relying on any speaker-parallel data, thereby overcoming data scarcity. By using extensive training data, our model achieves zero-shot cross-lingual style transfer on previously unseen source languages. Experiments show that our model generates translated speeches with high fidelity and speaker similarity. Audio samples are available at http://stylelm.github.io/ .</abstract>
      <url hash="42b083ff">2024.acl-srw.5</url>
    </paper>
    <paper id="6">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>C</fixed-case>oder: Instruction Tuning Large Language Models for Code Editing</title>
      <author><first>Kaixin</first><last>Li</last></author>
      <author><first>Qisheng</first><last>Hu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>James</first><last>Zhao</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Hui</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yuxi</first><last>Xie</last></author>
      <author><first>Tiedong</first><last>Liu</last></author>
      <author><first>Michael</first><last>Shieh</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Junxian</first><last>He</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>50-70</pages>
      <abstract>Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of Large Language Models (LLMs) to edit code based on user instructions. Evaluated on a novel human-written execution-based benchmark dubbed EditEval, we found current models often struggle to fulfill the instructions. In light of this, we contribute InstructCoder, the first instruction-tuning dataset designed to adapt LLMs for general-purpose code editing, containing high-diversity code-editing tasks such as comment insertion, code optimization, and code refactoring. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The collection process starts with filtered commit data sourced from GitHub Python repositories as seeds. Subsequently, the dataset is systematically expanded through an iterative process, where both seed and generated tasks are used to prompt ChatGPT for more data. Our findings reveal that open-source LLMs fine-tuned on InstructCoder can significantly enhance the accuracy of code edits, exhibiting superior code-editing performance matching advanced proprietary LLMs. The datasets and the source code are publicly available.</abstract>
      <url hash="f674712d">2024.acl-srw.6</url>
    </paper>
    <paper id="7">
      <title><fixed-case>B</fixed-case>ias<fixed-case>DPO</fixed-case>: Mitigating Bias in Language Models through Direct Preference Optimization</title>
      <author><first>Ahmed</first><last>Allam</last></author>
      <pages>71-79</pages>
      <abstract>Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns. This paper introduces a new framework employing Direct Preference Optimization (DPO) to mitigate gender, racial, and religious biases in LLM-generated English text. By developing a loss function that favors less biased over biased completions, our approach cultivates a preference for respectful and non-discriminatory language in LLMs. We also contribute a manually designed dataset for training LLMs to recognize and correct biases. This dataset encompasses a diverse range of prompts paired with both biased and unbiased completions. Implementing this approach on the Microsoft Phi-2 model, we demonstrate substantial reductions in biased outputs as our model outperforms the baseline model on almost all bias benchmarks. Our model also achieves better performance compared to other open-source models on most benchmarks. By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.</abstract>
      <url hash="8a3968a2">2024.acl-srw.7</url>
    </paper>
    <paper id="8">
      <title><fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>xtend: Tuning New Experts for Modality and Task Extension</title>
      <author><first>Shanshan</first><last>Zhong</last></author>
      <author><first>Shanghua</first><last>Gao</last><affiliation>Harvard University</affiliation></author>
      <author><first>Zhongzhan</first><last>Huang</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Wushao</first><last>Wen</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Marinka</first><last>Zitnik</last><affiliation>Harvard University</affiliation></author>
      <author><first>Pan</first><last>Zhou</last><affiliation>Singapore Management University</affiliation></author>
      <pages>80-91</pages>
      <abstract>Large language models (LLMs) excel in various tasks but are primarily trained on text data, limiting their application scope. Expanding LLM capabilities to include vision-language understanding is vital, yet training them on multimodal data from scratch is challenging and costly. Existing instruction tuning methods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs via fully fine-tuning LLMs to bridge the modality gap. However, full fine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous knowledge, and high training costs particularly in the era of increasing tasks and modalities. To solve this issue, we introduce MoExtend, an effective framework designed to streamline the modality adaptation and extension of Mixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts into pre-trained MoE models, endowing them with novel knowledge without the need to tune pretrained models such as MoE and vision encoders. This approach enables rapid adaptation and extension to new modal data or tasks, effectively addressing the challenge of accommodating new modalities within LLMs. Furthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk of catastrophic forgetting. Experimental results demonstrate the efficacy and efficiency of MoExtend in enhancing the multimodal capabilities of LLMs, contributing to advancements in multimodal AI research.</abstract>
      <url hash="2c73b595">2024.acl-srw.8</url>
    </paper>
    <paper id="9">
      <title>On the Interpretability of Deep Learning Models for Collaborative Argumentation Analysis in Classrooms</title>
      <author><first>Deliang</first><last>Wang</last></author>
      <author><first>Gaowei</first><last>Chen</last><affiliation>University of Hong Kong</affiliation></author>
      <pages>92-102</pages>
      <abstract>Collaborative argumentation holds significant potential for enhancing students’ learning outcomes within classroom settings. Consequently, researchers have explored the application of artificial intelligence (AI) to automatically analyze argumentation in these contexts. Despite the remarkable performance of deep learning models in this task, their lack of interpretability poses a critical challenge, leading to teachers’ skepticism and limited utilization. To cultivate trust among teachers, this PhD thesis proposal aims to leverage explainable AI techniques to provide explanations for these deep learning models. Specifically, the study develops two deep learning models for automated analysis of argument moves (claim, evidence, and warrant) and specificity levels (low, medium, and high) within collaborative argumentation. To address the interpretability issue, four explainable AI methods are proposed: gradient sensitivity, gradient input, integrated gradient, and LIME. Computational experiments demonstrate the efficacy of these methods in elucidating model predictions by computing word contributions, with LIME delivering exceptional performance. Moreover, a quasi-experiment is designed to evaluate the impact of model explanations on user trust and knowledge, serving as a future study of this PhD proposal. By tackling the challenges of interpretability and trust, this PhD thesis proposal aims to contribute to fostering user trust in AI and facilitating the practical implementation of AI in educational contexts.</abstract>
      <url hash="6121b687">2024.acl-srw.9</url>
    </paper>
    <paper id="10">
      <title>Document Alignment based on Overlapping Fixed-Length Segments</title>
      <author><first>Xiaotian</first><last>Wang</last></author>
      <author><first>Takehito</first><last>Utsuro</last><affiliation>University of Tsukuba</affiliation></author>
      <author><first>Masaaki</first><last>Nagata</last><affiliation>NTT Corporation</affiliation></author>
      <pages>103-113</pages>
      <abstract>Acquiring large-scale parallel corpora is crucial for NLP tasks such asNeural Machine Translation, and web crawling has become a popularmethodology for this purpose. Previous studies have been conductedbased on sentence-based segmentation (SBS) when aligning documents invarious languages which are obtained through web crawling. Among them,the TK-PERT method (Thompson and Koehn, 2020) achieved state-of-the-artresults and addressed the boilerplate text in web crawling data wellthrough a down-weighting approach. However, there remains a problemwith how to handle long-text encoding better. Thus, we introduce thestrategy of Overlapping Fixed-Length Segmentation (OFLS) in place ofSBS, and observe a pronounced enhancement when performing the sameapproach for document alignment. In this paper, we compare the SBS andOFLS using three previous methods, Mean-Pool, TK-PERT (Thompson andKoehn, 2020), and Optimal Transport (Clark et al., 2019; El- Kishky andGuzman, 2020), on the WMT16 document alignment shared task forFrench-English, as well as on our self-established Japanese-Englishdataset MnRN. As a result, for the WMT16 task, various SBS basedmethods showed an increase in recall by 1% to 10% after reproductionwith OFLS. For MnRN data, OFLS demonstrated notable accuracyimprovements and exhibited faster document embedding speed.</abstract>
      <url hash="5bb0d7cc">2024.acl-srw.10</url>
    </paper>
    <paper id="11">
      <title>Automatically Suggesting Diverse Example Sentences for <fixed-case>L</fixed-case>2 <fixed-case>J</fixed-case>apanese Learners Using Pre-Trained Language Models</title>
      <author><first>Enrico</first><last>Benedetti</last></author>
      <author><first>Akiko</first><last>Aizawa</last><affiliation>NII, Tokyo Institute of Technology</affiliation></author>
      <author><first>Florian</first><last>Boudin</last><affiliation>University of Nantes</affiliation></author>
      <pages>114-131</pages>
      <abstract>Providing example sentences that are diverse and aligned with learners’ proficiency levels is essential for fostering effective language acquisition.This study examines the use of Pre-trained Language Models (PLMs) to produce example sentences targeting L2 Japanese learners.We utilize PLMs in two ways: as quality scoring components in a retrieval system that draws from a newly curated corpus of Japanese sentences, and as direct sentence generators using zero-shot learning.We evaluate the quality of sentences by considering multiple aspects such as difficulty, diversity, and naturalness, with a panel of raters consisting of learners of Japanese, native speakers – and GPT-4.Our findings suggest that there is inherent disagreement among participants on the ratings of sentence qualities, except for difficulty. Despite that, the retrieval approach was preferred by all evaluators, especially for beginner and advanced target proficiency, while the generative approaches received lower scores on average.Even so, our experiments highlight the potential for using PLMs to enhance the adaptability of sentence suggestion systems and therefore improve the language learning journey.</abstract>
      <url hash="bffa5386">2024.acl-srw.11</url>
    </paper>
    <paper id="12">
      <title><fixed-case>Z</fixed-case>-coref: <fixed-case>T</fixed-case>hai Coreference and Zero Pronoun Resolution</title>
      <author><first>Poomphob</first><last>Suwannapichat</last><affiliation>King Mongkut’s University of Technology Thonburi</affiliation></author>
      <author><first>Sansiri.tarn@kmutt.ac.th</first><last>Sansiri.tarn@kmutt.ac.th</last><affiliation>NA</affiliation></author>
      <author><first>Santitham.pro@kmutt.ac.th</first><last>Santitham.pro@kmutt.ac.th</last><affiliation>NA</affiliation></author>
      <pages>132-139</pages>
      <abstract>Coreference Resolution (CR) and Zero Pronoun Resolution (ZPR) are vital for extracting meaningful information from text. However, limited research and datasets pose significant challenges in Thai language. To address this, we developed an annotated joint CR and ZPR dataset. Additionally, we introduced the Z-coref model, capable of simultaneously handling CR and ZPR tasks by adjusting the span definition of a prior CR architecture to include token gaps. The proposed model trained on our dataset outperformed the state-of-the-art in resolving both coreference resolution and zero-pronoun resolution, while taking less time to train.</abstract>
      <url hash="3734a9dd">2024.acl-srw.12</url>
    </paper>
    <paper id="13">
      <title><fixed-case>R</fixed-case>e<fixed-case>MAG</fixed-case>-<fixed-case>KR</fixed-case>: Retrieval and Medically Assisted Generation with Knowledge Reduction for Medical Question Answering</title>
      <author><first>Sidhaarth</first><last>Murali</last></author>
      <author><first>Sowmya</first><last>S.</last><affiliation>National Institute of Technology Karnataka</affiliation></author>
      <author><first>Supreetha</first><last>R</last></author>
      <pages>140-145</pages>
      <abstract>Large Language Models (LLMs) have significant potential for facilitating intelligent end-user applications in healthcare. However, hallucinations remain an inherent problem with LLMs, making it crucial to address this issue with extensive medical knowledge and data. In this work, we propose a Retrieve-and-Medically-Augmented-Generation with Knowledge Reduction (ReMAG-KR) pipeline, employing a carefully curated knowledge base using cross-encoder re-ranking strategies. The pipeline is tested on medical MCQ-based QA datasets as well as general QA datasets. It was observed that when the knowledge base is reduced, the model’s performance decreases by 2-8%, while the inference time improves by 47%.</abstract>
      <url hash="3ae8d443">2024.acl-srw.13</url>
    </paper>
    <paper id="14">
      <title>Plot Retrieval as an Assessment of Abstract Semantic Association</title>
      <author><first>Shicheng</first><last>Xu</last></author>
      <author><first>Liang</first><last>Pang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiangnan</first><last>Li</last><affiliation>WeChat, Tencent Inc.</affiliation></author>
      <author><first>Mo</first><last>Yu</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>146-161</pages>
      <abstract>Retrieving relevant plots from the book for a query is a critical task, which can improve the reading experience and efficiency of readers. Readers usually only give an abstract and vague description as the query based on their own understanding, summaries, or speculations of the plot, which requires the retrieval model to have a strong ability to estimate the abstract semantic associations between the query and candidate plots. However, existing information retrieval (IR) datasets cannot reflect this ability well. In this paper, we propose PlotRetrieval, a labeled dataset to train and evaluate the performance of IR models on the novel task Plot Retrieval. Text pairs in PlotRetrieval have less word overlap and more abstract semantic association, which can reflect the ability of the IR models to estimate the abstract semantic association, rather than just traditional lexical or semantic matching. Extensive experiments across various lexical retrieval, sparse retrieval, dense retrieval, and cross-encoder methods compared with human studies on PlotRetrieval show current IR models still struggle in capturing abstract semantic association between texts. PlotRetrieval can be the benchmark for further research on the semantic association modeling ability of IR models.</abstract>
      <url hash="e20cd3fd">2024.acl-srw.14</url>
    </paper>
    <paper id="15">
      <title>Demystifying Instruction Mixing for Fine-tuning Large Language Models</title>
      <author><first>Renxi</first><last>Wang</last></author>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Xudong</first><last>Han</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Chiyu</first><last>Zhang</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>162-169</pages>
      <abstract>Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures, laying the foundations for future research.</abstract>
      <url hash="85592cb9">2024.acl-srw.15</url>
    </paper>
    <paper id="16">
      <title>Fine-Tuning <fixed-case>ASR</fixed-case> models for Very Low-Resource Languages: A Study on Mvskoke</title>
      <author><first>Julia</first><last>Mainzinger</last></author>
      <author><first>Gina-Anne</first><last>Levow</last><affiliation>University of Washington and University of Washington</affiliation></author>
      <pages>170-176</pages>
      <abstract>Recent advancements in multilingual models for automatic speech recognition (ASR) have been able to achieve a high accuracy for languages with extremely limited resources. This study examines ASR modeling for the Mvskoke language, an indigenous language of America. The parameter efficiency of adapter training is contrasted with training entire models, and it is demonstrated how performance varies with different amounts of data. Additionally, the models are evaluated with trigram language model decoding, and the outputs are compared across different types of speech recordings. Results show that training an adapter is both parameter efficient and gives higher accuracy for a relatively small amount of data.</abstract>
      <url hash="452f498f">2024.acl-srw.16</url>
    </paper>
    <paper id="17">
      <title>Automating Qualitative Data Analysis with Large Language Models</title>
      <author><first>Angelina</first><last>Parfenova</last><affiliation>Universit� Grenoble Alpes</affiliation></author>
      <author><first>Alexander.denzler@hslu.ch</first><last>Alexander.denzler@hslu.ch</last><affiliation>NA</affiliation></author>
      <author><first>J�rgen</first><last>Pfeffer</last><affiliation>Technische Universit�t M�nchen</affiliation></author>
      <pages>177-185</pages>
      <abstract>This PhD proposal aims to investigate ways of automating qualitative data analysis, specifically the thematic coding of texts. Despite existing methods vastly covered in literature, they mainly use Topic Modeling and other quantitative approaches which are far from resembling a human’s analysis outcome. This proposal examines the limitations of current research in the field. It proposes a novel methodology based on Large Language Models to tackle automated coding and make it as close as possible to the results of human researchers. This paper covers studies already done in this field and their limitations, existing software, the problem of duplicating the researcher bias, and the proposed methodology.</abstract>
      <url hash="c0e0eb97">2024.acl-srw.17</url>
    </paper>
    <paper id="18">
      <title><fixed-case>ANHALTEN</fixed-case>: Cross-Lingual Transfer for <fixed-case>G</fixed-case>erman Token-Level Reference-Free Hallucination Detection</title>
      <author><first>Janek</first><last>Herrlein</last><affiliation>Bayerische Julius-Maximilians-Universit�t W�rzburg</affiliation></author>
      <author><first>Chia-Chien</first><last>Hung</last><affiliation>NEC Laboratories Europe and Universit�t Mannheim</affiliation></author>
      <author><first>Goran</first><last>Glava�</last><affiliation>Julius-Maximilians-Universit�t W�rzburg</affiliation></author>
      <pages>186-194</pages>
      <abstract>Research on token-level reference-free hallucination detection has predominantly focused on English, primarily due to the scarcity of robust datasets in other languages. This has hindered systematic investigations into the effectiveness of cross-lingual transfer for this important NLP application. To address this gap, we introduce ANHALTEN, a new evaluation dataset that extends the English hallucination detection dataset to German. To the best of our knowledge, this is the first work that explores cross-lingual transfer for token-level reference-free hallucination detection. ANHALTEN contains gold annotations in German that are parallel (i.e., directly comparable to the original English instances). We benchmark several prominent cross-lingual transfer approaches, demonstrating that larger context length leads to better hallucination detection in German, even without succeeding context. Importantly, we show that the sample-efficient few-shot transfer is the most effective approach in most setups. This highlights the practical benefits of minimal annotation effort in the target language for reference-free hallucination detection. Aiming to catalyze future research on cross-lingual token-level reference-free hallucination detection, we make ANHALTEN publicly available: https://github.com/janekh24/anhalten</abstract>
      <url hash="25cae645">2024.acl-srw.18</url>
    </paper>
    <paper id="19">
      <title>Label-Aware Automatic Verbalizer for Few-Shot Text Classification in Mid-To-Low Resource Languages</title>
      <author><first>Thanakorn</first><last>Thaminkaew</last><affiliation>Chulalongkorn University</affiliation></author>
      <author><first>Piyawat</first><last>Lertvittayakumjorn</last><affiliation>Google</affiliation></author>
      <author><first>Peerapon</first><last>Vateekul</last><affiliation>Chulalongkorn University</affiliation></author>
      <pages>195-203</pages>
      <abstract>Prompt-based learning has shown its effectiveness in few-shot text classification. A key factor in its success is a verbalizer, which translates output from a language model into a predicted class. Notably, the simplest and widely acknowledged verbalizer employs manual labels to represent the classes. However, manual selection may not yield the optimal words for a given language model, potentially leading to subpar classification performance, especially in mid-to-low resource languages with weaker language models. Therefore, we propose Label-Aware Automatic Verbalizer (LAAV), effectively augmenting manual labels for improved few-shot classification results. Specifically, we utilize the label name along with the conjunction “and” to induce the model to generate more effective words for the verbalizer. Experimental results on four mid-to-low resource Southeast Asian languages demonstrate that LAAV significantly outperforms existing verbalizers.</abstract>
      <url hash="65f0271d">2024.acl-srw.19</url>
    </paper>
    <paper id="20">
      <title>Vector Spaces for Quantifying Disparity of Multiword Expressions in Annotated Text</title>
      <author><first>Louis</first><last>Est�ve</last></author>
      <author><first>Agata</first><last>Savary</last><affiliation>Universit� Paris-Saclay</affiliation></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <pages>204-224</pages>
      <abstract>Multiword Expressions (MWEs) make a goodcase study for linguistic diversity due to theiridiosyncratic nature. Defining MWE canonicalforms as types, diversity may be measurednotably through disparity, based on pairwisedistances between types. To this aim, wetrain static MWE-aware word embeddings forverbal MWEs in 14 languages, and we showinteresting properties of these vector spaces.We use these vector spaces to implement theso-called functional diversity measure. Weapply this measure to the results of severalMWE identification systems. We find that,although MWE vector spaces are meaningful ata local scale, the disparity measure aggregatingthem at a global scale strongly correlateswith the number of types, which questions itsusefulness in presence of simpler diversitymetrics such as variety. We make the vectorspaces we generated available.</abstract>
      <url hash="3bb4baaf">2024.acl-srw.20</url>
    </paper>
    <paper id="21">
      <title>Narratives at Conflict: Computational Analysis of News Framing in Multilingual Disinformation Campaigns</title>
      <author><first>Antonina</first><last>Sinelnik</last></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>225-237</pages>
      <abstract>Any report frames issues to favor a particular interpretation by highlighting or excluding certain aspects of a story. Despite the widespread use of framing in disinformation, framing properties and detection methods remain underexplored outside the English-speaking world. We explore how multilingual framing of the same issue differs systematically. We use eight years of Russia-backed disinformation campaigns, spanning 8k news articles in 4 languages targeting 15 countries. We find that disinformation campaigns consistently and intentionally favor specific framing, depending on the target language of the audience. We further discover how Russian-language articles consistently highlight selected frames depending on the region of the media coverage. We find that the two most prominent models for automatic frame analysis underperform and show high disagreement, highlighting the need for further research.</abstract>
      <url hash="6efed0d5">2024.acl-srw.21</url>
    </paper>
    <paper id="22">
      <title>Assessing In-context Learning and Fine-tuning for Topic Classification of <fixed-case>G</fixed-case>erman Web Data</title>
      <author><first>Julian</first><last>Schelb</last><affiliation>Universit�t Konstanz</affiliation></author>
      <author><first>Andreas</first><last>Spitz</last><affiliation>Universit�t Konstanz</affiliation></author>
      <author><first>Roberto</first><last>Ulloa</last></author>
      <pages>238-252</pages>
      <abstract>Researchers in the political and social sciences often rely on classification models to analyze trends in information consumption by examining browsing histories of millions of webpages. Automated scalable methods are necessary due to the impracticality of manual labeling. In this paper, we model the detection of topic-related content as a binary classification task and compare the accuracy of fine-tuned pre-trained encoder models against in-context learning strategies. Using only a few hundred annotated data points per topic, we detect content related to three German policies in a database of scraped webpages. We compare multilingual and monolingual models, as well as zero and few-shot approaches, and investigate the impact of negative sampling strategies and the combination of URL &amp; content-based features. Our results show that a small sample of annotated data is sufficient to train an effective classifier. Fine-tuning encoder-based models yields better results than in-context learning. Classifiers using both URL &amp; content-based features perform best, while using URLs alone provides adequate results when content is unavailable.</abstract>
      <url hash="58f2f8eb">2024.acl-srw.22</url>
    </paper>
    <paper id="23">
      <title>Knowledge Editing of Large Language Models Unconstrained by Word Order</title>
      <author><first>Ryoma</first><last>Ishigaki</last></author>
      <author><first>Jundai</first><last>Suzuki</last><affiliation>Tokyo Denki University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Masaki</first><last>Shuzo</last><affiliation>Tokyo Denki University</affiliation></author>
      <author><first>Eisaku</first><last>Maeda</last><affiliation>Tokyo Denki University</affiliation></author>
      <pages>253-263</pages>
      <abstract>Large Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the LLMs themselves. To address this issue, a method called local modification-based knowledge editing has been developed. This method identifies the knowledge neurons that encode the target knowledge and adjusts the parameters associated with these neurons to update the knowledge. Knowledge neurons are identified by masking the <tex-math>\it{o}</tex-math> part from sentences representing relational triplets (<tex-math>\it{s, r, o}</tex-math>), having the LLM predict the masked part, and observing the LLM�s activation during the prediction. When the architecture is decoder-based, the predicted <tex-math>\it{o}</tex-math> needs to be located at the end of the sentence. Previous local modification-based knowledge editing methods for decoder-based models have assumed SVO languages and faced challenges when applied to SOV languages such as Japanese. In this study, we propose a knowledge editing method that eliminates the need for word order constraints by converting the input for identifying knowledge neurons into a question where <tex-math>\it{o}</tex-math> is the answer. We conducted validation experiments on 500 examples and confirmed that the proposed method is effective for Japanese, a non-SVO language. We also applied this method to English, an SVO language, and demonstrated that it outperforms conventional methods.</abstract>
      <url hash="f90117c0">2024.acl-srw.23</url>
    </paper>
    <paper id="24">
      <title>Exploring the Effectiveness and Consistency of Task Selection in Intermediate-Task Transfer Learning</title>
      <author><first>Pin-Jie</first><last>Lin</last></author>
      <author><first>Miaoran</first><last>Zhang</last><affiliation>Saarland University</affiliation></author>
      <author><first>Marius</first><last>Mosbach</last><affiliation>McGill University and Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last><affiliation>Saarland University</affiliation></author>
      <pages>264-279</pages>
      <abstract>Identifying beneficial tasks to transfer from is a critical step toward successful intermediate-task transfer learning. In this work, we experiment with 130 source-target task combinations and demonstrate that the transfer performance exhibits severe variance across different source tasks and training seeds, highlighting the crucial role of intermediate-task selection in a broader context. We compare four representative task selection methods in a unified setup, focusing on their effectiveness and consistency. Compared to embedding-free methods and text embeddings, task embeddings constructed from fine-tuned weights can better estimate task transferability by improving task prediction scores from 2.59% to 3.96%. Despite their strong performance, we observe that the task embeddings do not consistently demonstrate superiority for tasks requiring reasoning abilities. Furthermore, we introduce a novel method that measures pairwise token similarity using maximum inner product search, leading to the highest performance in task prediction. Our findings suggest that token-wise similarity is better predictive for predicting transferability compared to averaging weights.</abstract>
      <url hash="0139ec25">2024.acl-srw.24</url>
    </paper>
    <paper id="25">
      <title>Does the structure of textual content have an impact on language models for automatic summarization?</title>
      <author><first>Eve</first><last>Sauvage</last></author>
      <author><first>Sabrina</first><last>Campano</last><affiliation>EDF R&amp;D</affiliation></author>
      <author><first>Lydia</first><last>Ouali</last></author>
      <author><first>Cyril</first><last>Grouin</last><affiliation>CNRS</affiliation></author>
      <pages>280-285</pages>
      <abstract>The processing of long sequences with models remains a subject in its own right, including automatic summary, despite recent improvements. In this work, we present experiments on the automatic summarization of scientific articles using BART models, taking into account textual information coming from distinct passages from the long texts to be summarized. We demonstrate that taking into account document structure improves the performance of state-of-the-art models and approaches the performance of LongFormer on English.</abstract>
      <url hash="52fa8d53">2024.acl-srw.25</url>
    </paper>
    <paper id="26">
      <title>Action Inference for Destination Prediction in Vision-and-Language Navigation</title>
      <author><first>Anirudh</first><last>Kondapally</last><affiliation>Honda R&amp;D Co., Ltd.</affiliation></author>
      <author><first>Kentaro_yamada@jp.honda</first><last>Kentaro_yamada@jp.honda</last><affiliation>NA</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <pages>286-293</pages>
      <abstract>Vision-and-Language Navigation (VLN) encompasses interacting with autonomous vehicles using language and visual input from the perspective of mobility.Most of the previous work in this field focuses on spatial reasoning and the semantic grounding of visual information.However, reasoning based on the actions of pedestrians in the scene is not much considered.In this study, we provide a VLN dataset for destination prediction with action inference to investigate the extent to which current VLN models perform action inference.We introduce a crowd-sourcing process to construct a dataset for this task in two steps: (1) collecting beliefs about the next action for a pedestrian and (2) annotating the destination considering the pedestrian’s next action.Our benchmarking results of the models on destination prediction lead us to believe that the models can learn to reason about the effect of the action and the next action on the destination to a certain extent.However, there is still much scope for improvement.</abstract>
      <url hash="ba6af917">2024.acl-srw.26</url>
    </paper>
    <paper id="27">
      <title>A Computational Analysis and Exploration of Linguistic Borrowings in <fixed-case>F</fixed-case>rench Rap Lyrics</title>
      <author><first>Lucas</first><last>Zurbuchen</last></author>
      <author><first>Rob</first><last>Voigt</last><affiliation>Northwestern University</affiliation></author>
      <pages>294-302</pages>
      <abstract>In France, linguistic borrowings in the relatively conservative French language are an important site of cultural debate, and rap in particular is a hotspot for borrowings. In this work, we use computational methods to understand the factors that affect the prominence and prevalence of a borrowing. To do so, we manually annotate a lexicon of over 700 borrowings occurring in this context (including key aspects for each borrowing such as origin and semantic class). We analyze the prevalence of these borrowings in a newly collected corpus of over 8000 French rap song lyrics and find that there are increases in the proportion of linguistic borrowings, interjections, and Niger-Congo borrowings while terms related to the arts are decreasing in prevalence. We release our code and data to facilitate further research in this area and discuss potential future directions.</abstract>
      <url hash="1193dd7b">2024.acl-srw.27</url>
    </paper>
    <paper id="28">
      <title>On Improving Repository-Level Code <fixed-case>QA</fixed-case> for Large Language Models</title>
      <author><first>Jan</first><last>Strich</last></author>
      <author><first>Florian</first><last>Schneider</last><affiliation>Universit�t Hamburg</affiliation></author>
      <author><first>Irina</first><last>Nikishina</last></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>U Hamburg</affiliation></author>
      <pages>303-338</pages>
      <abstract>Large Language Models (LLMs) such as ChatGPT, GitHub Copilot, Llama, or Mistral assist programmers as copilots and knowledge sources to make the coding process faster and more efficient. This paper aims to improve the copilot performance by implementing different self-alignment processes and retrieval-augmented generation (RAG) pipelines, as well as their combination. To test the effectiveness of all approaches, we create a dataset and apply a model-based evaluation, using LLM as a judge. It is designed to check the model’s abilities to understand the source code semantics, the dependency between files, and the overall meta-information about the repository. We also compare our approach with other existing solutions, e.g. ChatGPT-3.5, and evaluate on the existing benchmarks. Code and dataset are available online (https://anonymous.4open.science/r/ma_llm-382D).</abstract>
      <url hash="55a69700">2024.acl-srw.28</url>
    </paper>
    <paper id="29">
      <title>Compromesso! <fixed-case>I</fixed-case>talian Many-Shot Jailbreaks undermine the safety of Large Language Models</title>
      <author><first>Fabio</first><last>Pernisi</last></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Paul</first><last>R�ttger</last><affiliation>Bocconi University</affiliation></author>
      <pages>339-345</pages>
      <abstract>As diverse linguistic communities and users adopt Large Language Models (LLMs), assessing their safety across languages becomes critical. Despite ongoing efforts to align these models with safe and ethical guidelines, they can still be induced into unsafe behavior with jailbreaking, a technique in which models are prompted to act outside their operational guidelines. What research has been conducted on these vulnerabilities was predominantly on English, limiting the understanding of LLM behavior in other languages. We address this gap by investigating Many-Shot Jailbreaking (MSJ) in Italian, underscoring the importance of understanding LLM behavior in different languages. We base our analysis on a newly created Italian dataset to identify unique safety vulnerabilities in 4 families of open-source LLMs.We find that the models exhibit unsafe behaviors even with minimal exposure to harmful prompts, and–more alarmingly–this tendency rapidly escalates with more demonstrations.</abstract>
      <url hash="9255c313">2024.acl-srw.29</url>
    </paper>
    <paper id="30">
      <title>Foundation Model for Biomedical Graphs: Integrating Knowledge Graphs and Protein Structures to Large Language Models</title>
      <author><first>Yunsoo</first><last>Kim</last></author>
      <pages>346-355</pages>
      <abstract>Transformer model has been a de-facto standard in natural language processing. Its adaptations in other fields such as computer vision showed promising results that this architecture is a powerful neural network in representation learning regardless of the data type. This recent success has led to research in multimodal Large Language Model (LLM), which enabled us to new types of tasks and applications with multiple data types. However, multimodal LLM in the biomedical domain is primarily limited to images, text, and/or sequence data. Here I propose to work on multimodal LLM architecture for biomedical graphs such as protein structure and chemical molecules. The research hypothesis is based on the fact that clinicians and researchers in computational biology and clinical research take advantage of various information for their decision-making process. Therefore, an AI model being able to handle multiple data types should boost its ability to use diverse knowledge for improved performances in clinical applications.</abstract>
      <url hash="76544871">2024.acl-srw.30</url>
    </paper>
    <paper id="31">
      <title><fixed-case>V</fixed-case>i<fixed-case>M</fixed-case>ed<fixed-case>AQA</fixed-case>: A <fixed-case>V</fixed-case>ietnamese Medical Abstractive Question-Answering Dataset and Findings of Large Language Model</title>
      <author><first>Minh-Nam</first><last>Tran</last></author>
      <author><first>Phu-Vinh</first><last>Nguyen</last></author>
      <author><first>Long</first><last>Nguyen</last><affiliation>Ho Chi Minh city University of Science, Vietnam National University</affiliation></author>
      <author><first>Dien</first><last>Dinh</last></author>
      <pages>356-364</pages>
      <abstract>Question answering involves creating answers to questions. With the growth of large language models, the ability of question-answering systems has dramatically improved. However, there is a lack of Vietnamese abstractive question-answering datasets, especially in the medical domain. Therefore, this research aims to mitigate this gap by introducing ViMedAQA. This **Vi**etnamese **Med**ical **A**bstractive **Q**uestion-**A**nswering dataset covers four topics in the Vietnamese medical domain, including body parts, disease, drugs and medicine. Additionally, the empirical results on the proposed dataset examine the capability of the large language models in the Vietnamese medical domain, including reasoning, memorizing and awareness of essential information.</abstract>
      <url hash="700922aa">2024.acl-srw.31</url>
    </paper>
    <paper id="32">
      <title>Rescue: Ranking <fixed-case>LLM</fixed-case> Responses with Partial Ordering to Improve Response Generation</title>
      <author><first>Yikun</first><last>Wang</last></author>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Haoming</first><last>Li</last></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author id="fei-liu"><first>Fei</first><last>Liu</last><affiliation>Emory University</affiliation></author>
      <pages>365-376</pages>
      <abstract>Customizing LLMs for a specific task involves separating high-quality responses from lower-quality ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining a large volume of expert-annotated data is costly for most tasks. In this paper, we explore a novel method to optimize LLMs using ranking metrics. This method trains the model to prioritize the best responses from a pool of candidates created for a particular task. Rather than a traditional full ordering, we advocate for a partial ordering, as achieving consensus on the perfect order of candidate responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be achieved with limited human annotations or through heuristic methods. We test our system’s improved response generation ability using benchmark datasets, including textual entailment and multi-document question answering. We conduct ablation studies to understand crucial factors, such as how to gather candidate responses for a specific task, determine their most suitable order, and balance supervised fine-tuning with ranking metrics. Our approach, named RESCUE, offers a promising avenue for enhancing the response generation and task accuracy of LLMs.</abstract>
      <url hash="8770faa4">2024.acl-srw.32</url>
    </paper>
    <paper id="33">
      <title>Basreh or Basra? Geoparsing Historical Locations in the Svoboda Diaries</title>
      <author><first>Jolie</first><last>Zhou</last><affiliation>University of Washington</affiliation></author>
      <author><first>Camille</first><last>Cole</last><affiliation>Illinois State University</affiliation></author>
      <author><first>Annie</first><last>Chen</last><affiliation>University of Washington</affiliation></author>
      <pages>377-390</pages>
      <abstract>Geoparsing, the task of assigning coordinates to locations extracted from free text, is invaluable in enabling us to place locations in time and space. In the historical domain, many geoparsing corpora are from large news collections. We examine the Svoboda Diaries, a small historical corpus written primarily in English, with many location names in transliterated Arabic. We develop a pipeline employing named entity recognition for geotagging, and a map-based generate-and-rank approach incorporating candidate name augmentation and clustering of location context words for geocoding. Our system outperforms existing map-based geoparsers in terms of accuracy, lowest mean distance error, and number of locations correctly identified. As location names may vary from those in knowledge bases, we find that augmented candidate generation is instrumental in the system’s performance. Among our candidate generation methods, the generation of transliterated names contributed the most to increased location matches in the knowledge base. Our main contribution is proposing an integrated pipeline for geoparsing of historical corpora using augmented candidate location name generation and clustering methods – an approach that can be generalized to other texts with foreign or non-standard spellings.</abstract>
      <url hash="2305199c">2024.acl-srw.33</url>
    </paper>
    <paper id="34">
      <title><fixed-case>H</fixed-case>omophone2<fixed-case>V</fixed-case>ec: Embedding Space Analysis for Empirical Evaluation of Phonological and Semantic Similarity</title>
      <author><first>Sophie</first><last>Wu</last></author>
      <author><first>Anita</first><last>Zheng</last></author>
      <author><first>Ching-i-chuang@mail.mcgill.ca</first><last>Ching-i-chuang@mail.mcgill.ca</last><affiliation>NA</affiliation></author>
      <pages>391-396</pages>
      <abstract>This paper introduces a novel method for empirically evaluating the relationship between the phonological and semantic similarity of linguistic units using embedding spaces. Chinese character homophones are used as a proof-of-concept. We employ cosine similarity as a proxy for semantic similarity between characters, and compare relationships between phonologically-related characters and baseline characters (chosen as similar-frequency characters). We show there is a strongly statistically significant positive semantic relationship among different Chinese characters at varying levels of sound-sharing. We also perform some basic probing using t-SNE and UMAP visualizations, and indicate directions for future applications of this method.</abstract>
      <url hash="7a4b426d">2024.acl-srw.34</url>
    </paper>
    <paper id="35">
      <title>Trace-of-Thought Prompting: Investigating Prompt-Based Knowledge Distillation Through Question Decomposition</title>
      <author><first>Tyler</first><last>McDonald</last><affiliation>Brock University</affiliation></author>
      <author><first>Ali</first><last>Emami</last><affiliation>Brock University</affiliation></author>
      <pages>397-410</pages>
      <abstract>Knowledge distillation allows smaller neural networks to emulate the performance of larger, teacher models with reduced computational demands. Traditional methods for Large Language Models (LLMs) often necessitate extensive fine-tuning, which limits their accessibility. To address this, we introduce Trace-of-Thought Prompting, a novel framework designed to distill critical reasoning capabilities from large-scale teacher models (over 8 billion parameters) to small-scale student models (up to 8 billion parameters). This approach leverages problem decomposition to enhance interpretability and facilitate human-in-the-loop interventions. Empirical evaluations on the GSM8K and MATH datasets show that student models achieve accuracy gains of up to 113% on GSM8K and 20% on MATH, with significant improvements particularly notable in smaller models like Llama 2 and Zephyr. Our results suggest a promising pathway for open-source, small-scale models to eventually serve as both students and teachers, potentially reducing our reliance on large-scale, proprietary models. Our code, featuring data analytics and testing scripts, is provided here: https://github.com/traceofthought/trace-of-thought-prompting/tree/main.</abstract>
      <url hash="e14b0a5a">2024.acl-srw.35</url>
    </paper>
    <paper id="36">
      <title>Can <fixed-case>LLM</fixed-case>s Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges</title>
      <author><first>Vinay</first><last>Samuel</last></author>
      <author><first>Houda</first><last>Aynaou</last></author>
      <author><first>Arijit</first><last>Chowdhury</last><affiliation>Amazon</affiliation></author>
      <author><first>Karthik</first><last>Venkat Ramanan</last></author>
      <author><first>Aman</first><last>Chadha</last><affiliation>Amazon</affiliation></author>
      <pages>411-421</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive zero-shot performance on a wide range of NLP tasks, demonstrating the ability to reason and apply common sense. A relevant application is to use them for creating high-quality synthetic datasets for downstream tasks. In this work, we probe whether GPT-4 can be used to augment existing extractive reading comprehension datasets. Automating data annotation processes has the potential to save large amounts of time, money, and effort that goes into manually labeling datasets. In this paper, we evaluate the performance of GPT-4 as a replacement for human annotators for low-resource reading comprehension tasks, by comparing performance after fine-tuning, and the cost associated with annotation. This work serves to be the first analysis of LLMs as synthetic data augmenters for QA systems, highlighting the unique opportunities and challenges. Additionally, we release augmented versions of low-resource datasets, that will allow the research community to create further benchmarks for evaluation of generated datasets. Github available at https://github.com/vsamuel2003/qa-gpt4</abstract>
      <url hash="80b13500">2024.acl-srw.36</url>
    </paper>
    <paper id="37">
      <title>Automatic Derivation of Semantic Representations for <fixed-case>T</fixed-case>hai Serial Verb Constructions: A Grammar-Based Approach</title>
      <author><first>Vipasha</first><last>Bansal</last></author>
      <pages>422-437</pages>
      <abstract>Deep semantic representations are useful for many NLU tasks (Droganova and Zeman 2019; Schuster and Manning-2016). Manual annotation to build these representations is time-consuming, and so automatic approaches are preferred (Droganova and Zeman 2019; Bender et al. 2015). This paper demonstrates how rich semantic representations can be automatically derived for Thai Serial Verb Constructions (SVCs), where the semantic relationship between component verbs is not immediately clear from the surface forms. I present the first fully-implemented HPSG analysis for Thai SVCs, deriving appropriate semantic representations (MRS; Copestake et al. 2005) from syntactic features, implemented within a DELPH-IN computational grammar (Slayden 2009). This analysis increases verified coverage of SVCs by 73% and decreases ambiguity by 46%. The final grammar can be found at: https://github.com/VipashaB94/ThaiGrammar</abstract>
      <url hash="bd7b0824">2024.acl-srw.37</url>
    </paper>
    <paper id="38">
      <title>Seed-Free Synthetic Data Generation Framework for Instruction-Tuning <fixed-case>LLM</fixed-case>s: A Case Study in <fixed-case>T</fixed-case>hai</title>
      <author><first>Parinthapat</first><last>Pengpun</last></author>
      <author><first>Can</first><last>Udomcharoenchaikit</last><affiliation>Vidyasirimedhi Institute of Science and Technology (VISTEC)</affiliation></author>
      <author><first>Weerayut</first><last>Buaphet</last></author>
      <author><first>Peerat</first><last>Limkonchotiwat</last></author>
      <pages>438-457</pages>
      <abstract>We present a synthetic data approach for instruction-tuning large language models (LLMs) for low-resource languages in a data-efficient manner, specifically focusing on Thai. We identify three key properties that contribute to the effectiveness of instruction-tuning datasets: fluency, diversity, and cultural context. We propose a seed-data-free framework for generating synthetic instruction-tuning data that incorporates these essential properties. Our framework employs an LLM to generate diverse topics, retrieve relevant contexts from Wikipedia, and create instructions for various tasks, such as question answering, summarization, and conversation. The experimental results show that our best-performing synthetic dataset, which incorporates all three key properties, achieves competitive performance using only 5,000 instructions when compared to state-of-the-art Thai LLMs trained on hundreds of thousands of instructions. Our code and dataset are publicly available at https://github.com/parinzee/seed-free-synthetic-instruct.</abstract>
      <url hash="80fc5d98">2024.acl-srw.38</url>
    </paper>
    <paper id="39">
      <title>Bridging Distribution Gap via Semantic Rewriting with <fixed-case>LLM</fixed-case>s to Enhance <fixed-case>OOD</fixed-case> Robustness</title>
      <author><first>Manas</first><last>Madine</last></author>
      <pages>458-468</pages>
      <abstract>This paper investigates the robustness of Large Language Models (LLMs) against Out-Of-Distribution (OOD) data within the context of sentiment analysis. Traditional fine-tuning approaches often fail to generalize effectively across different data distributions, limiting the practical deployment of LLMs in dynamic real-world scenarios. To address this challenge, we introduce a novel method called “Semantic Rewriting,” which leverages the inherent flexibility of LLMs to align both in-distribution (ID) and OOD data with the LLMs distributions. By semantically transforming sentences to minimize linguistic discrepancies, our approach helps to standardize features across datasets, thus enhancing model robustness. We conduct extensive experiments with several benchmark datasets and LLMs to validate the efficacy of our method. The results demonstrate that Semantic Rewriting significantly improves the performance of models on OOD tasks, outperforming traditional methods in both robustness and generalization capabilities. Our findings suggest that Semantic Rewriting is a promising technique for developing more reliable and versatile NLP systems capable of performing robustly across diverse operational environments.</abstract>
      <url hash="da3dcd7d">2024.acl-srw.39</url>
    </paper>
    <paper id="40">
      <title><fixed-case>C</fixed-case>o<fixed-case>V</fixed-case>o<fixed-case>S</fixed-case>witch: Machine Translation of Synthetic Code-Switched Text Based on Intonation Units</title>
      <author><first>Yeeun</first><last>Kang</last></author>
      <pages>469-481</pages>
      <abstract>Multilingual code-switching research is often hindered by the lack and linguistically biased status of available datasets. To expand language representation, we synthesize code-switching data by replacing intonation units detected through PSST, a speech segmentation model fine-tuned from OpenAI’s Whisper, using a speech-to-text translation dataset, CoVoST 2. With our dataset, CoVoSwitch, spanning 13 languages, we evaluate the code-switching translation performance of two multilingual translation models, M2M-100 418M and NLLB-200 600M. We reveal that the inclusion of code-switching units results in higher translation performance than monolingual settings and that models are better at code-switching translation into English than non-English. Further, low-resource languages gain most from integration of code-switched units when translating into English but much less when translating into non-English. Translations into low-resource languages also perform worse than even raw code-switched inputs. We find that systems excel at copying English tokens but struggle with non-English tokens, that the off-target problem in monolingual settings is also relevant in code-switching settings, and that models hallucinate in code-switching translation by introducing words absent in both of the original source sentences. CoVoSwitch and code are available at https://github.com/sophiayk20/covoswitch.</abstract>
      <url hash="a36aa248">2024.acl-srw.40</url>
    </paper>
    <paper id="41">
      <title>An Analysis under a Unified Formulation of Learning Algorithms with Output Constraints</title>
      <author><first>Mooho</first><last>Song</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jay-Yoon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <pages>482-498</pages>
      <abstract>Neural networks (NN) perform well in diverse tasks, but sometimes produce nonsensical results to humans. Most NN models “solely” learn from (input, output) pairs, occasionally conflicting with human knowledge. Many studies indicate injecting human knowledge by reducing output constraints during training can improve model performance and reduce constraint violations.While there have been several attempts to compare different existing algorithms under the same programming framework, nonetheless, there has been no previous work that categorizes learning algorithms with output constraints in a unified manner. Our contributions are as follows: (1) We categorize the previous studies based on three axes: type of constraint loss used (e.g. probabilistic soft logic, REINFORCE), exploration strategy of constraint-violating examples, and integration mechanism of learning signals from main task and constraint.(2) We propose new algorithms to integrate the information of main task and constraint injection, inspired by continual-learning algorithms.(3) Furthermore, we propose the <tex-math>H\beta</tex-math>-score as a metric for considering the main task metric and constraint violation simultaneously.To provide a thorough analysis, we examine all the algorithms on three NLP tasks: natural language inference (NLI), synthetic transduction examples (STE), and semantic role labeling (SRL). We explore and reveal the key factors of various algorithms associated with achieving high <tex-math>H\beta</tex-math>-scores.</abstract>
      <url hash="20a0fa1a">2024.acl-srw.41</url>
    </paper>
    <paper id="42">
      <title>Beyond Abstracts: A New Dataset, Prompt Design Strategy and Method for Biomedical Synthesis Generation</title>
      <author><first>James</first><last>O’Doherty</last></author>
      <author><first>Cian</first><last>Nolan</last></author>
      <author><first>Yufang</first><last>Hou</last><affiliation>Technische Universit�t Darmstadt and IBM Research Ireland</affiliation></author>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University</affiliation></author>
      <pages>499-518</pages>
      <abstract>The biomedical field relies on cost and time intensive systematic reviews of papers to enable practitioners to keep up to date with research. Impressive recent advances in large language models (LLMs) have made the task of automating at least part of the systematic review process feasible, but progress is slow. This paper identifies some factors that may have been holding research back, and proposes a new, enhanced dataset and prompting-based method for automatic synthesis generation, the most challenging step for automation. We test different models and types of information from and about biomedical studies for their usefulness in obtaining high-quality results.We find that, surprisingly, inclusion of paper abstracts can worsens results. Instead, study summary information, and system instructions informed by domain knowledge, are key to producing high-quality syntheses.</abstract>
      <url hash="9cb6897d">2024.acl-srw.42</url>
    </paper>
    <paper id="43">
      <title>Improving Sentence Embeddings with Automatic Generation of Training Data Using Few-shot Examples</title>
      <author><first>Soma</first><last>Sato</last></author>
      <author><first>Hayato</first><last>Tsukagoshi</last></author>
      <author><first>Ryohei</first><last>Sasano</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Koichi</first><last>Takeda</last><affiliation>Nagoya University</affiliation></author>
      <pages>519-530</pages>
      <abstract>Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL requires a manually annotated natural language inference (NLI) dataset for fine-tuning.We aim to improve sentence embeddings without using large manually annotated datasets by automatically generating an NLI dataset with an LLM and using it for fine-tuning of PromptEOL. To achieve this, we explore methods of data generation suitable for sentence embedding learning in this study. Specifically, we will focus on automatic dataset generation through few-shot learning and explore the appropriate methods to leverage few-shot examples. Experimental results on the STS tasks demonstrate that our approach outperforms existing models in settings without large manually annotated datasets.</abstract>
      <url hash="1a718125">2024.acl-srw.43</url>
    </paper>
    <paper id="44">
      <title>Curriculum Learning for Small Code Language Models</title>
      <author><first>Marwa</first><last>Na�r</last><affiliation>New York University, Abu Dhabi and �cole Nationale Sup�rieure d’Informatique</affiliation></author>
      <author><first>Kamel</first><last>Yamani</last><affiliation>New York University, Abu Dhabi and Ecole Nationale Sup�rieure d’Informatique (ESI)</affiliation></author>
      <author><first>Lynda</first><last>Lhadj</last><affiliation>ESI</affiliation></author>
      <author><first>Riyadh</first><last>Baghdadi</last><affiliation>New York University</affiliation></author>
      <pages>531-542</pages>
      <abstract>Code language models have emerged as useful tools for various programming tasks, yet they often struggle when it comes to complex ones. In this paper, we explore the potential of curriculum learning in enhancing the performance of these models. While prior research has suggested that curriculum learning does not necessarily help in improving the performance of language models, our results surprisingly show that this may not be the case for code language models. We demonstrate that a well-designed curriculum learning approach significantly improves the accuracy of small decoder-only code language models on the task of code execution, while its effect on code completion is less significant. To explore the potential of curriculum learning, we train multiple GPT models with 1 million parameters each to predict the next token and evaluate them on code completion and execution tasks. Our contributions include proposing a novel code difficulty assessment metric by combining software code measures, investigating the effectiveness of Curriculum Learning for code language models, and introducing a Novel Curriculum Learning schedule that enhances the performance of small decoder-only language models in code execution tasks. The results of this paper open the door for more research on the use of curriculum learning for code language models.</abstract>
      <url hash="f702f70f">2024.acl-srw.44</url>
    </paper>
    <paper id="45">
      <title>Question-Analysis Prompting Improves <fixed-case>LLM</fixed-case> Performance in Reasoning Tasks</title>
      <author><first>Dharunish</first><last>Yugeswardeenoo</last><affiliation>Algoverse</affiliation></author>
      <author><first>Kevin</first><last>Zhu</last><affiliation>Algoverse AI Research</affiliation></author>
      <author><first>Sean</first><last>O’Brien</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>543-554</pages>
      <abstract>Although LLMs have the potential to transform many fields, they still underperform humans in reasoning tasks. Existing methods induce the model to produce step-by-step calculations, but this research explores the question: Does making the LLM analyze the question improve its performance? We propose a novel prompting strategy called Question Analysis Prompting (QAP), in which the model is prompted to explain the question in ’n’ words before solving. The value of ’n’ influences the length of response generated by the model. QAP is evaluated on GPT-3.5 Turbo and GPT-4 Turbo on arithmetic datasets GSM8K, AQuA, and SAT and commonsense dataset StrategyQA. QAP is compared with other state-of-the-art prompts including chain-of-thought (CoT), Plan and Solve Prompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all state-of-the-art prompts on AQuA and SAT datasets on both GPT-3.5 and GPT-4. QAP consistently ranks among the top-2 prompts on 75% of the tests. A key factor of QAP performance can be attributed to response length, where detailed responses are beneficial when answering harder questions, but can negatively affect easy questions.</abstract>
      <url hash="561d051a">2024.acl-srw.45</url>
    </paper>
    <paper id="46">
      <title>An Individualized News Affective Response Dataset</title>
      <author><first>Tiancheng</first><last>Hu</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Nigel</first><last>Collier</last><affiliation>University of Cambridge</affiliation></author>
      <pages>555-563</pages>
      <abstract>The rise of sensationalism in news reporting, driven by market saturation and online competition, has compromised news quality and trust. At the core of sensationalism is the evocation of affective responses in the readers. Current NLP approaches to emotion detection often overlook the subjective differences in groups and individuals, relying on aggregation techniques that can obscure nuanced reactions. We introduce a novel large-scale dataset capturing subjective affective responses to news headlines. The dataset includes Facebook post screenshots from popular UK media outlets and uses a comprehensive annotation scheme. Annotators report their affective responses, provide discrete emotion labels, assess relevance to current events, and indicate sharing likelihood. Additionally, we collect demographic, personality, and media consumption data. This ongoing dataset aims to enable more accurate models of affective response by considering individual and contextual factors. This work is ongoing and we highly appreciate any feedback.</abstract>
      <url hash="958f6d67">2024.acl-srw.46</url>
    </paper>
    <paper id="47">
      <title>How Well Do Vision Models Encode Diagram Attributes?</title>
      <author><first>Haruto</first><last>Yoshida</last></author>
      <author><first>Keito</first><last>Kudo</last></author>
      <author><first>Yoichi</first><last>Aoki</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Ryota</first><last>Tanaka</last><affiliation>NTT</affiliation></author>
      <author><first>Itsumi</first><last>Saito</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Keisuke</first><last>Sakaguchi</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence, RIKEN and Tohoku University</affiliation></author>
      <pages>564-575</pages>
      <abstract>Research on understanding and generating diagrams has used vision models such as CLIP. However, it remains unclear whether these models accurately identify diagram attributes, such as node colors and shapes, along with edge colors and connection patterns. This study evaluates how well vision models recognize the diagram attributes by probing the model and retrieving diagrams using text queries. Experimental results showed that while vision models can recognize differences in node colors, shapes, and edge colors, they struggle to identify differences in edge connection patterns that play a pivotal role in the semantics of diagrams. Moreover, we revealed inadequate alignment between diagram attributes and language representations in the embedding space.</abstract>
      <url hash="8d86833f">2024.acl-srw.47</url>
    </paper>
    <paper id="48">
      <title><fixed-case>C</fixed-case>heckers<fixed-case>GPT</fixed-case>: Learning World Models through Language Modeling</title>
      <author><first>Abhinav</first><last>Joshi</last><affiliation>Indian Institute of Technology, Kanpur</affiliation></author>
      <author><first>Vaibhav</first><last>Sharma</last></author>
      <author><first>Ashutosh</first><last>Modi</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>576-588</pages>
      <abstract>Although Large Language Models (LLMs) have been trained using just the next token prediction objective, these have shown impressive performance on various tasks. Consequently, it has attracted research interests in this regard. While one line of work in the past has suggested that LLMs learn surface-level statistics from the dataset, another line of work emphasizes that the learned representations are effective for simulating the underlying world model, considering the causal relationship for the next token prediction. This phenomenon is often referred to as the emergence of a world model in sequence prediction tasks. Recent work has demonstrated this phenomenon in a simulated setting of board games like Othello and Chess. In this paper, we analyze the game of Checkers to find out the emergence of a world model in a language model. By training a GPT-style autoregressive language model using only the next character prediction objective, we find that the model does show a hint of learning a world model representation of the board positions. We perform our analysis on two datasets: 1) synthetic dataset, which comes from the checkers game tree, and 2) human gameplay dataset. With multiple models trained with different layer sizes, we find that increasing the parameter size does help learn better world model representation decoded by linear probes.</abstract>
      <url hash="0604ec50">2024.acl-srw.48</url>
    </paper>
    <paper id="49">
      <title>In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery</title>
      <author><first>Matteo</first><last>Merler</last></author>
      <author><first>Katsiaryna</first><last>Haitsiukevich</last><affiliation>Aalto University</affiliation></author>
      <author><first>Nicola</first><last>Dainese</last><affiliation>Aalto University</affiliation></author>
      <author><first>Pekka</first><last>Marttinen</last><affiliation>Aalto University</affiliation></author>
      <pages>589-606</pages>
      <abstract>State of the art Symbolic Regression (SR) methods currently build specialized models, while the application of Large Language Models (LLMs) remains largely unexplored. In this work, we introduce the first comprehensive framework that utilizes LLMs for the task of SR.We propose In-Context Symbolic Regression (ICSR), an SR method which iteratively refines a functional form with an LLM and determines its coefficients with an external optimizer. ICSR leverages LLMs’ strong mathematical prior both to propose an initial set of possible functions given the observations and to refine them based on their errors.Our findings reveal that LLMs are able to successfully find symbolic equations that fit the given data, matching or outperforming the overall performance of the best SR baselines on four popular benchmarks, while yielding simpler equations with better out of distribution generalization.</abstract>
      <url hash="21389a3b">2024.acl-srw.49</url>
    </paper>
    <paper id="50">
      <title><fixed-case>STEP</fixed-case>: Staged Parameter-Efficient Pre-training for Large Language Models</title>
      <author><first>Kazuki</first><last>Yano</last></author>
      <author><first>Takumi</first><last>Ito</last><affiliation>Langsmith Inc., Tohoku University and Machine Learning Solutions</affiliation></author>
      <author><first>Jun</first><last>Suzuki</last><affiliation>Tohoku University</affiliation></author>
      <pages>607-614</pages>
      <abstract>Pre-training large language models faces significant memory challenges due to the large size of model weights.We propose STaged parameter-Efficient Pre-training (STEP), which combines ideas from parameter-efficient tuning and staged training. We conduct experiments on pre-training models of various sizes and demonstrate that STEP can achieve up to a 40.4% reduction in maximum memory requirement compared to vanilla pre-training while maintaining comparable performance.</abstract>
      <url hash="f05b9673">2024.acl-srw.50</url>
    </paper>
  </volume>
  <volume id="demos" ingest-date="2024-08-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)</booktitle>
      <editor><first>Yixin</first><last>Cao</last><affiliation>Singapore Management University</affiliation></editor>
      <editor><first>Yang</first><last>Feng</last><affiliation>Chinese Academy of Science</affiliation></editor>
      <editor><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand and virtual meeting</address>
      <month>August</month>
      <year>2024</year>
      <url hash="7988e48f">2024.acl-demos</url>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="f23682e8">2024.acl-demos.0</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>PAI</fixed-case>-Diffusion: Constructing and Serving a Family of Open <fixed-case>C</fixed-case>hinese Diffusion Models for Text-to-image Synthesis on the Cloud</title>
      <author><first>Chengyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhongjie</first><last>Duan</last></author>
      <author><first>Bingyan</first><last>Liu</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Xinyi</first><last>Zou</last></author>
      <author><first>Cen</first><last>Chen</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Kui</first><last>Jia</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Jun</first><last>Huang</last></author>
      <pages>1-8</pages>
      <abstract>Text-to-image synthesis for the Chinese language poses unique challenges due to its large vocabulary size, and intricate character relationships. While existing diffusion models have shown promise in generating images from textual descriptions, they often neglect domain-specific contexts and lack robustness in handling the Chinese language. This paper introduces PAI-Diffusion, a comprehensive framework that addresses these limitations. PAI-Diffusion incorporates both general and domain-specific Chinese diffusion models, enabling the generation of contextually relevant images. It explores the potential of using LoRA and ControlNet for fine-grained image style transfer and image editing, empowering users with enhanced control over image generation. Moreover, PAI-Diffusion seamlessly integrates with Alibaba Cloud’s Platform for AI, providing accessible and scalable solutions. All the Chinese diffusion model checkpoints, LoRAs, and ControlNets, including domain-specific ones, are publicly available. A user-friendly Chinese WebUI and the diffusers-api elastic inference toolkit, also open-sourced, further facilitate the easy deployment of PAI-Diffusion models in various local and cloud environments, making it a valuable resource for Chinese text-to-image synthesis.</abstract>
      <url hash="0dfd3046">2024.acl-demos.1</url>
    </paper>
    <paper id="2">
      <title><fixed-case>O</fixed-case>pen<fixed-case>VNA</fixed-case>: A Framework for Analyzing the Behavior of Multimodal Language Understanding System under Noisy Scenarios</title>
      <author><first>Ziqi</first><last>Yuan</last></author>
      <author><first>Baozheng</first><last>Zhang</last></author>
      <author><first>Hua</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyun</first><last>Liang</last></author>
      <author><first>Kai</first><last>Gao</last></author>
      <pages>9-18</pages>
      <abstract>We present OpenVNA, an open-source framework designed for analyzing the behavior of multimodal language understanding systems under noisy conditions. OpenVNA serves as an intuitive toolkit tailored for researchers, facilitating convenience batch-level robustness evaluation and on-the-fly instance-level demonstration. It primarily features a benchmark Python library for assessing global model robustness, offering high flexibility and extensibility, thereby enabling customization with user-defined noise types and models. Additionally, a GUI-based interface has been developed to intuitively analyze local model behavior. In this paper, we delineate the design principles and utilization of the created library and GUI-based web platform. Currently, OpenVNA is publicly accessible at <url>https://github.com/thuiar/OpenVNA</url>, with a demonstration video available at <url>https://youtu.be/0Z9cW7RGct4</url>.</abstract>
      <url hash="519feae3">2024.acl-demos.2</url>
    </paper>
    <paper id="3">
      <title><fixed-case>XNLP</fixed-case>: An Interactive Demonstration System for Universal Structured <fixed-case>NLP</fixed-case></title>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China and Tianjin University, China</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>19-30</pages>
      <abstract>Structured Natural Language Processing (XNLP) is an important subset of NLP that entails understanding the underlying semantic or syntactic structure of texts, which serves as a foundational component for many downstream applications. Despite certain recent efforts to explore universal solutions for specific categories of XNLP tasks, a comprehensive and effective approach for unifying all XNLP tasks long remains underdeveloped. Meanwhile, while XNLP demonstration systems are vital for researchers exploring various XNLP tasks, existing platforms can be limited to, e.g., supporting few XNLP tasks, lacking interactivity and universalness. To this end, we propose an advanced XNLP demonstration system, where we leverage LLM to achieve universal XNLP, with one model for all with high generalizability. Overall, our system advances in multiple aspects, including universal XNLP modeling, high performance, interpretability, scalability, and interactivity, offering a unified platform for exploring diverse XNLP tasks in the community.</abstract>
      <url hash="b5bdf322">2024.acl-demos.3</url>
    </paper>
    <paper id="4">
      <title>Towards the <fixed-case>T</fixed-case>op<fixed-case>M</fixed-case>ost: A Topic Modeling System Toolkit</title>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Fengjun</first><last>Pan</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>31-41</pages>
      <abstract>Topic models have a rich history with various applications and have recently been reinvigorated by neural topic modeling. However, these numerous topic models adopt totally distinct datasets, implementations, and evaluations. This impedes quick utilization and fair comparisons, and thereby hinders their research progress and applications. To tackle this challenge, we in this paper propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by supporting more extensive features. It covers a broader spectrum of topic modeling scenarios with their complete lifecycles, including datasets, preprocessing, models, training, and evaluations. Thanks to its highly cohesive and decoupled modular design, TopMost enables rapid utilization, fair comparisons, and flexible extensions of diverse cutting-edge topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.</abstract>
      <url hash="77a9397c">2024.acl-demos.4</url>
    </paper>
    <paper id="5">
      <title>Wordflow: Social Prompt Engineering for Large Language Models</title>
      <author><first>Zijie</first><last>Wang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Aishwarya</first><last>Chakravarthy</last></author>
      <author><first>David</first><last>Munechika</last></author>
      <author><first>Duen Horng</first><last>Chau</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>42-50</pages>
      <abstract>Large language models (LLMs) require well-crafted prompts for effective use. Prompt engineering, the process of designing prompts, is challenging, particularly for non-experts who are less familiar with AI technologies. While researchers have proposed techniques and tools to assist LLM users in prompt design, these works primarily target AI application developers rather than non-experts. To address this research gap, we propose social prompt engineering, a novel paradigm that leverages social computing techniques to facilitate collaborative prompt design. To investigate social prompt engineering, we introduce Wordflow, an open-source and social text editor that enables everyday users to easily create, run, share, and discover LLM prompts. Additionally, by leveraging modern web technologies, Wordflow allows users to run LLMs locally and privately in their browsers. Two usage scenarios highlight how social prompt engineering and our tool can enhance laypeople’s interaction with LLMs. Wordflow is publicly accessible at https://poloclub.github.io/wordflow.</abstract>
      <url hash="c4225137">2024.acl-demos.5</url>
    </paper>
    <paper id="6">
      <title><fixed-case>LM</fixed-case> Transparency Tool: Interactive Tool for Analyzing Transformer Language Models</title>
      <author><first>Igor</first><last>Tufanov</last><affiliation>Facebook</affiliation></author>
      <author><first>Karen</first><last>Hambardzumyan</last><affiliation>Facebook and University College London, University of London</affiliation></author>
      <author><first>Javier</first><last>Ferrando</last></author>
      <author><first>Elena</first><last>Voita</last><affiliation>FAIR at Meta AI and University of Amsterdam</affiliation></author>
      <pages>51-60</pages>
      <abstract>We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models. Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model. Specifically, it (i) shows the important part of the whole input-to-output information flow, (ii) allows attributing any changes done by a model block to individual attention heads and feed-forward neurons, (iii) allows interpreting the functions of those heads or neurons. A crucial part of this pipeline is showing the importance of specific model components at each step. As a result, we are able to look at the roles of model components only in cases where they are important for a prediction. Since knowing which components should be inspected is key for analyzing large models where the number of these components is extremely high, we believe our tool will greatly support the interpretability community both in research settings and in practical applications.</abstract>
      <url hash="4bf23d7a">2024.acl-demos.6</url>
    </paper>
    <paper id="7">
      <title><fixed-case>E</fixed-case>mpathy<fixed-case>E</fixed-case>ar: An Open-source Avatar Multimodal Empathetic Chatbot</title>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Han</first><last>Zhang</last><affiliation>Xidian University</affiliation></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Qian</first><last>Liu</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>61-71</pages>
      <abstract>This paper introduces EmpathyEar, a pioneering open-source, avatar-based multimodal empathetic chatbot, to fill the gap in traditional text-only empathetic response generation (ERG) systems. Leveraging the advancements of a large language model, combined with multimodal encoders and generators, EmpathyEar supports user inputs in any combination of text, sound, and vision, and produces multimodal empathetic responses, offering users, not just textual responses but also digital avatars with talking faces and synchronized speeches. A series of emotion-aware instruction-tuning is performed for comprehensive emotional understanding and generation capabilities. In this way, EmpathyEar provides users with responses that achieve a deeper emotional resonance, closely emulating human-like empathy. The system paves the way for the next emotional intelligence, for which we open-source the code for public access.</abstract>
      <url hash="5575f300">2024.acl-demos.7</url>
    </paper>
    <paper id="8">
      <title><fixed-case>O</fixed-case>pen<fixed-case>W</fixed-case>eb<fixed-case>A</fixed-case>gent: An Open Toolkit to Enable Web Agents on Large Language Models</title>
      <author><first>Iat Long</first><last>Iong</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Yuxuan</first><last>Chen</last></author>
      <author><first>Hanyu</first><last>Lai</last></author>
      <author><first>Shuntian</first><last>Yao</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Pengbo</first><last>Shen</last></author>
      <author><first>Hao</first><last>Yu</last></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>72-81</pages>
      <abstract>We introduce OpenWebAgent, an open toolkit designed to optimize web automation by integrating both large language models (LLMs) and large multimodal models (LMMs). This toolkit focuses on enhancing human-computer interactions on the web, simplifying complex tasks through an advanced HTML parser, a rapid action generation module, and an intuitive user interface. At the core of OpenWebAgent is an innovative web agent framework that uses a modular design to allow developers to seamlessly integrate a variety of models and tools to process web information and automate tasks on the web. This enables the development of powerful, task-oriented web agents, significantly enhancing user experience and operational efficiency on the web. The OpenWebAgent framework, Chrome plugin, and demo video are available at https://github.com/THUDM/OpenWebAgent/.</abstract>
      <url hash="693211b1">2024.acl-demos.8</url>
    </paper>
    <paper id="9">
      <title><fixed-case>E</fixed-case>asy<fixed-case>E</fixed-case>dit: An Easy-to-use Knowledge Editing Framework for Large Language Models</title>
      <author><first>Peng</first><last>Wang</last></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Bozhong</first><last>Tian</last></author>
      <author><first>Zekun</first><last>Xi</last></author>
      <author><first>Yunzhi</first><last>Yao</last></author>
      <author><first>Ziwen</first><last>Xu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Mengru</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Shengyu</first><last>Mao</last></author>
      <author><first>Xiaohan</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Siyuan</first><last>Cheng</last></author>
      <author><first>Kangwei</first><last>Liu</last></author>
      <author><first>Yuansheng</first><last>Ni</last></author>
      <author><first>Guozhou</first><last>Zheng</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>82-93</pages>
      <abstract>Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged – aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub, along with Google Colab tutorials and comprehensive documentation for beginners to get started. Besides, we present an online system for real-time knowledge editing, and a demo video.</abstract>
      <url hash="00639017">2024.acl-demos.9</url>
    </paper>
    <paper id="10">
      <title><fixed-case>E</fixed-case>asy<fixed-case>I</fixed-case>nstruct: An Easy-to-use Instruction Processing Framework for Large Language Models</title>
      <author><first>Yixin</first><last>Ou</last></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Honghao</first><last>Gui</last></author>
      <author><first>Ziwen</first><last>Xu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Shuofei</first><last>Qiao</last></author>
      <author><first>Runnan</first><last>Fang</last></author>
      <author><first>Lei</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Zhen</first><last>Bi</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Guozhou</first><last>Zheng</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>94-106</pages>
      <abstract>In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at Github, along with an online demo app and a demo video for quick-start, calling for broader research centered on instruction data and synthetic data.</abstract>
      <url hash="3b8296e3">2024.acl-demos.10</url>
    </paper>
    <paper id="11">
      <title><fixed-case>B</fixed-case>ot<fixed-case>E</fixed-case>val: Facilitating Interactive Human Evaluation</title>
      <author><first>Hyundong</first><last>Cho</last><affiliation>USC/ISI</affiliation></author>
      <author><first>Thamme</first><last>Gowda</last><affiliation>Microsoft Translator</affiliation></author>
      <author><first>Yuyang</first><last>Huang</last></author>
      <author><first>Zixun</first><last>Lu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Tianli</first><last>Tong</last></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>107-116</pages>
      <abstract>Following the rapid progress in natural language processing (NLP) models, language models are applied to increasingly more complex interactive tasks such as negotiations and conversation moderations. Having human evaluators directly interact with these NLP models is essential for adequately evaluating the performance on such interactive tasks. We develop BotEval, an easily customizable, open-source, evaluation toolkit that focuses on enabling human-bot interactions as part of the evaluation process, as opposed to human evaluators making judgements for a static input. BotEval balances flexibility for customization and user-friendliness by providing templates for common use cases that span various degrees of complexity and built-in compatibility with popular crowdsourcing platforms.We showcase the numerous useful features of BotEval through a study that evaluates the performance of various chatbots on their effectiveness for conversational moderation and discuss how BotEval differs from other annotation tools.</abstract>
      <url hash="b97fdfcb">2024.acl-demos.11</url>
    </paper>
    <paper id="12">
      <title><fixed-case>G</fixed-case>en<fixed-case>GO</fixed-case>: <fixed-case>ACL</fixed-case> Paper Explorer with Semantic Features</title>
      <author><first>Sotaro</first><last>Takeshita</last><affiliation>Universit�t Mannheim</affiliation></author>
      <author><first>Simone</first><last>Ponzetto</last><affiliation>University of Mannheim</affiliation></author>
      <author><first>Kai</first><last>Eckert</last><affiliation>Mannheim University of Applied Sciences</affiliation></author>
      <pages>117-126</pages>
      <abstract>We present GenGO, a system for exploring papers published in ACL conferences. Paper data stored in our database is enriched with multi-aspect summaries, extracted named entities, a field of study label, and text embeddings by our data processing pipeline. These metadata are used in our web-based user interface to enable researchers to quickly find papers relevant to their interests, and grasp an overview of papers without reading full-text of papers. To make GenGO to be available online as long as possible, we design GenGO to be simple and efficient to reduce maintenance and financial costs. In addition, the modularity of our data processing pipeline lets developers easily extend it to add new features. We make our code available to foster open development and transparency: https://gengo.sotaro.io.</abstract>
      <url hash="bf16a5a6">2024.acl-demos.12</url>
    </paper>
    <paper id="13">
      <title><fixed-case>NLP</fixed-case>-<fixed-case>KG</fixed-case>: A System for Exploratory Search of Scientific Literature in Natural Language Processing</title>
      <author><first>Tim</first><last>Schopf</last></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universit�t M�nchen</affiliation></author>
      <pages>127-135</pages>
      <abstract>Scientific literature searches are often exploratory, whereby users are not yet familiar with a particular field or concept but are interested in learning more about it. However, existing systems for scientific literature search are typically tailored to keyword-based lookup searches, limiting the possibilities for exploration. We propose NLP-KG, a feature-rich system designed to support the exploration of research literature in unfamiliar natural language processing (NLP) fields. In addition to a semantic search, NLP-KG allows users to easily find survey papers that provide a quick introduction to a field of interest. Further, a Fields of Study hierarchy graph enables users to familiarize themselves with a field and its related areas. Finally, a chat interface allows users to ask questions about unfamiliar concepts or specific articles in NLP and obtain answers grounded in knowledge retrieved from scientific publications. Our system provides users with comprehensive exploration possibilities, supporting them in investigating the relationships between different fields, understanding unfamiliar concepts in NLP, and finding relevant research literature. Demo, video, and code are available at: https://github.com/NLP-Knowledge-Graph/NLP-KG-WebApp.</abstract>
      <url hash="adc7117f">2024.acl-demos.13</url>
    </paper>
    <paper id="14">
      <title><fixed-case>L</fixed-case>ocal<fixed-case>RQA</fixed-case>: From Generating Data to Locally Training, Testing, and Deploying Retrieval-Augmented <fixed-case>QA</fixed-case> Systems</title>
      <author><first>Xiao</first><last>Yu</last></author>
      <author><first>Yunan</first><last>Lu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>136-151</pages>
      <abstract>Retrieval-augmented question-answering systems combine retrieval techniques with large language models to provide answers that are more accurate and informative. Many existing toolkits allow users to quickly build such systems using off-the-shelf models, but they fall short in supporting researchers and developers to customize the *model training, testing, and deployment process*. We propose LocalRQA, an open-source toolkit that features a wide selection of model training algorithms, evaluation methods, and deployment tools curated from the latest research. As a showcase, we build QA systems using online documentation obtained from Databricks and Faire’s websites. We find 7B-models trained and deployed using LocalRQA reach a similar performance compared to using OpenAI’s text-ada-002 and GPT-4-turbo.</abstract>
      <url hash="d1973c8c">2024.acl-demos.14</url>
    </paper>
    <paper id="15">
      <title><fixed-case>JORA</fixed-case>: <fixed-case>JAX</fixed-case> Tensor-Parallel <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Library for Retrieval Augmented Fine-Tuning</title>
      <author><first>Anique</first><last>Tahir</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Lu</first><last>Cheng</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Huan</first><last>Liu</last><affiliation>Arizona State University</affiliation></author>
      <pages>152-159</pages>
      <abstract>The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences. Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of GPT models, leveraging distributed training. Our framework uniquely utilizes JAX’s just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources. Our experiments show more than 12x improvement in runtime compared to Hugging Face/DeepSpeed implementation with four GPUs while consuming less than half the VRAM per GPU.</abstract>
      <url hash="d6ea1a95">2024.acl-demos.15</url>
    </paper>
    <paper id="16">
      <title><fixed-case>L</fixed-case>ingua<fixed-case>L</fixed-case>inked: Distributed Large Language Model Inference on Mobile Devices</title>
      <author><first>Junchen</first><last>Zhao</last></author>
      <author><first>Yurun</first><last>Song</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Simenl3@uci.edu</first><last>Simenl3@uci.edu</last><affiliation>NA</affiliation></author>
      <author><first>Ian</first><last>Harris</last><affiliation>University of California-Irvine</affiliation></author>
      <author><first>Sangeetha</first><last>Abdu Jyothi</last><affiliation>University of California, Irvine</affiliation></author>
      <pages>160-171</pages>
      <abstract>Deploying Large Language Models (LLMs) locally on mobile devices presents a significant challenge due to their extensive memory requirements. In this paper, we introduce LinguaLinked, a system for decentralized, distributed LLM inference on mobile devices. LinguaLinked enables collaborative execution of the inference task across multiple trusted devices and ensures data privacy by processing information locally. LinguaLinked uses three key strategies. First, an optimized model assignment technique segments LLMs and uses linear optimization to align segments with each device�s capabilities. Second, an optimized data transmission mechanism ensures efficient and structured data flow between model segments while also maintaining the integrity of the original model structure. Finally, LinguaLinked incorporates a runtime load balancer that actively monitors and redistributes tasks among mobile devices to prevent bottlenecks, enhancing the system�s overall efficiency and responsiveness. We demonstrate that LinguaLinked facilitates efficient LLM inference while maintaining consistent throughput and minimal latency through extensive testing across various mobile devices, from high-end to low-end Android devices.</abstract>
      <url hash="e0f790bd">2024.acl-demos.16</url>
    </paper>
    <paper id="17">
      <title><fixed-case>IMGTB</fixed-case>: A Framework for Machine-Generated Text Detection Benchmarking</title>
      <author><first>Michal</first><last>Spiegel</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Dominik</first><last>Macko</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <pages>172-179</pages>
      <abstract>In the era of large language models generating high quality texts, it is a necessity to develop methods for detection of machine-generated text to avoid their harmful use or simply for annotation purposes. It is, however, also important to properly evaluate and compare such developed methods. Recently, a few benchmarks have been proposed for this purpose; however, integration of newest detection methods is rather challenging, since new methods appear each month and provide slightly different evaluation pipelines.In this paper, we present the IMGTB framework, which simplifies the benchmarking of machine-generated text detection methods by easy integration of custom (new) methods and evaluation datasets. In comparison to existing frameworks, it enables to objectively compare statistical metric-based zero-shot detectors with classification-based detectors and with differently fine-tuned detectors. Its configurability and flexibility makes research and development of new detection methods easier, especially their comparison to the existing state-of-the-art detectors. The default set of analyses, metrics and visualizations offered by the tool follows the established practices of machine-generated text detection benchmarking found in state-of-the-art literature.</abstract>
      <url hash="abb49eaf">2024.acl-demos.17</url>
    </paper>
    <paper id="18">
      <title><fixed-case>D</fixed-case>rug<fixed-case>W</fixed-case>atch: A Comprehensive Multi-Source Data Visualisation Platform for Drug Safety Information</title>
      <author><first>Artem</first><last>Bobrov</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Domantas</first><last>Saltenis</last></author>
      <author><first>Zhaoyue</first><last>Sun</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Gabriele</first><last>Pergola</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>180-189</pages>
      <abstract>Drug safety research is crucial for maintaining public health, often requiring comprehensive data support. However, the resources currently available to the public are limited and fail to provide a comprehensive understanding of the relationship between drugs and their side effects. This paper introduces “DrugWatch”, an easy-to-use and interactive multi-source information visualisation platform for drug safety study. It allows users to understand common side effects of drugs and their statistical information, flexibly retrieve relevant medical reports, or annotate their own medical texts with our automated annotation tool. Supported by NLP technology and enriched with interactive visual components, we are committed to providing researchers and practitioners with a one-stop information analysis, retrieval, and annotation service. The demonstration video is available at https://www.youtube.com/watch?v=RTqDgxzETjw. We also deployed an online demonstration system at https://drugwatch.net/.</abstract>
      <url hash="bfb6ca29">2024.acl-demos.18</url>
    </paper>
    <paper id="19">
      <title><fixed-case>O</fixed-case>pen<fixed-case>E</fixed-case>val: Benchmarking <fixed-case>C</fixed-case>hinese <fixed-case>LLM</fixed-case>s across Capability, Alignment and Safety</title>
      <author><first>Chuang</first><last>Liu</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Linhao</first><last>Yu</last></author>
      <author><first>Jiaxuan</first><last>Li</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Renren</first><last>Jin</last></author>
      <author><first>Yufei</first><last>Huang</last></author>
      <author><first>Ling</first><last>Shi</last></author>
      <author><first>Junhui</first><last>Zhang</last></author>
      <author><first>Xinmeng</first><last>Ji</last></author>
      <author><first>Tingting</first><last>Cui</last></author>
      <author><first>Liutao</first><last>Liutao</last></author>
      <author><first>Jinwang</first><last>Song</last></author>
      <author><first>Hongying</first><last>Zan</last><affiliation>Zhengzhou University</affiliation></author>
      <author><first>Sun</first><last>Li</last><affiliation>China Academy of Information and Communications Technology</affiliation></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>190-210</pages>
      <abstract>The rapid development of Chinese large language models (LLMs) poses big challenges for efficient LLM evaluation. While current initiatives have introduced new benchmarks or evaluation platforms for assessing Chinese LLMs, many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues. To address this gap, we introduce OpenEval, an evaluation testbed that benchmarks Chinese LLMs across capability, alignment and safety. For capability assessment, we include 12 benchmark datasets to evaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge, commonsense reasoning and mathematical reasoning. For alignment assessment, OpenEval contains 7 datasets that examines the bias, offensiveness and illegalness in the outputs yielded by Chinese LLMs. To evaluate safety, especially anticipated risks (e.g., power-seeking, self-awareness) of advanced LLMs, we include 6 datasets. In addition to these benchmarks, we have implemented a phased public evaluation and benchmark update strategy to ensure that OpenEval is in line with the development of Chinese LLMs or even able to provide cutting-edge benchmark datasets to guide the development of Chinese LLMs. In our first public evaluation, we have tested a range of Chinese LLMs, spanning from 7B to 72B parameters, including both open-source and proprietary models. Evaluation results indicate that while Chinese LLMs have shown impressive performance in certain tasks, more attention should be directed towards broader aspects such as commonsense reasoning, alignment, and safety.</abstract>
      <url hash="c19a1ae0">2024.acl-demos.19</url>
    </paper>
    <paper id="20">
      <title><fixed-case>A</fixed-case>uto<fixed-case>RE</fixed-case>: Document-Level Relation Extraction with Large Language Models</title>
      <author><first>Lilong</first><last>Xue</last></author>
      <author><first>Dan</first><last>Zhang</last></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>211-220</pages>
      <abstract>Large Language Models (LLMs) have demonstrated exceptional abilities in comprehending and generating text, motivating numerous researchers to utilize them for Information Extraction (IE) purposes, including Relation Extraction (RE). Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple relations and triplet facts distributed across a given document, posing distinct challenges. To overcome these limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel RE extraction paradigm named RHF (Relation-Head-Facts). Unlike existing approaches, AutoRE does not rely on the assumption of known relation options, making it more reflective of real-world scenarios. Additionally, we have developed an easily extensible RE framework using a Parameters Efficient Fine Tuning (PEFT) algorithm (QLoRA). Our experiments on the RE-DocRED dataset showcase AutoRE’s best performance, achieving state-of-the-art results, surpassing TAG by 10.03% and 9.03% respectively on the dev and test set. The code is available and the demonstration video is provided.</abstract>
      <url hash="8f8994d2">2024.acl-demos.20</url>
    </paper>
    <paper id="21">
      <title><fixed-case>L</fixed-case>ink<fixed-case>T</fixed-case>ransformer: A Unified Package for Record Linkage with Transformer Language Models</title>
      <author><first>Abhishek</first><last>Arora</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <author><first>Melissa</first><last>Dell</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <pages>221-231</pages>
      <abstract>Many computational analyses require linking information across noisy text datasets. While large language models (LLMs) offer significant promise, approximate string matching packages in popular statistical softwares such as R and Stata remain predominant in academic applications. These packages have simple interfaces and can be easily extended to a diversity of languages and settings, and for academic applications, ease-of-use and extensibility are essential. In contrast, packages for record linkage with LLMs require significant familiarity with deep learning frameworks and often focus on specialized applications of commercial value in English. The open-source package LinkTransformer aims to bridge this gap by providing an end-to-end software for performing record linkage and other data cleaning tasks with transformer LLMs, treating linkage as a text retrieval problem. At its core is an off-the-shelf toolkit for applying transformer models to record linkage. LinkTransformer contains a rich repository of pre-trained models for multiple languages and supports easy integration of any transformer language model from Hugging Face or OpenAI, providing the extensibility required for many scholarly applications. Its APIs also perform common data processing tasks, e.g., aggregation, noisy de-duplication, and translation-free cross-lingual linkage. LinkTransformer contains comprehensive tools for efficient model tuning, allowing for highly customized applications, and users can easily contribute their custom-trained models to its model hub to ensure reproducibility. Using a novel benchmark dataset geared towards academic applications, we show that LinkTransformer - with both custom models and Hugging Face or OpenAI models off-the-shelf - outperforms string matching by a wide margin. By combining transformer LMs with intuitive APIs, LinkTransformer aims to democratize these performance gains for those who lack familiarity with deep learning frameworks.</abstract>
      <url hash="a5ac53d4">2024.acl-demos.21</url>
    </paper>
    <paper id="22">
      <title><fixed-case>D</fixed-case>oc<fixed-case>P</fixed-case>ilot: Copilot for Automating <fixed-case>PDF</fixed-case> Edit Workflows in Documents</title>
      <author><first>Puneet</first><last>Mathur</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Alexa</first><last>Siu</last><affiliation>Adobe</affiliation></author>
      <author><first>Varun</first><last>Manjunatha</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Tong</first><last>Sun</last><affiliation>Adobe Systems</affiliation></author>
      <pages>232-246</pages>
      <abstract>Digital documents, such as PDFs, are vital in business workflows, enabling communication, documentation, and collaboration. Handling PDFs can involve navigating complex workflows and numerous tools (e.g., comprehension, annotation, editing), which can be tedious and time-consuming for users. We introduce DocPilot, an AI-assisted document workflow Copilot system capable of understanding user intent and executing tasks accordingly to help users streamline their workflows. DocPilot undertakes intelligent orchestration of various tools through LLM prompting in four steps: (1) Task plan generation, (2) Task plan verification and self-correction, (3) Multi-turn User Feedback, and (4) Task Plan Execution via Code Generation and Error log-based Code Self-Revision. The primary goal of this system is to free the user from the intricacies of document editing, enabling them to focus on the creative aspects and enrich their document management experience.</abstract>
      <url hash="ebd97356">2024.acl-demos.22</url>
    </paper>
    <paper id="23">
      <title><fixed-case>U</fixed-case>ltra<fixed-case>E</fixed-case>val: A Lightweight Platform for Flexible and Comprehensive Evaluation for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Chaoqun</first><last>He</last></author>
      <author><first>Renjie</first><last>Luo</last></author>
      <author><first>Shengding</first><last>Hu</last></author>
      <author><first>Ranchi</first><last>Zhao</last><affiliation>ModelBest</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Hanghao</first><last>Wu</last></author>
      <author><first>Jiajie</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>247-257</pages>
      <abstract>Evaluation is pivotal for honing Large Language Models (LLMs), pinpointing their capabilities and guiding enhancements. The rapid development of LLMs calls for a lightweight and easy-to-use framework for swift evaluation deployment. However, due to the various implementation details to consider, developing a comprehensive evaluation platform is never easy. Existing platforms are often complex and poorly modularized, hindering seamless incorporation into researcher’s workflows. This paper introduces UltraEval, a user-friendly evaluation framework characterized by lightweight, comprehensiveness, modularity, and efficiency. We identify and reimplement three core components of model evaluation (models, data, and metrics). The resulting composability allows for the free combination of different models, tasks, prompts, and metrics within a unified evaluation workflow. Additionally, UltraEval supports diverse models owing to a unified HTTP service and provides sufficient inference acceleration.</abstract>
      <url hash="0296efd2">2024.acl-demos.23</url>
    </paper>
    <paper id="24">
      <title><fixed-case>P</fixed-case>y<fixed-case>F</fixed-case>oma: a Python finite-state compiler module</title>
      <author><first>Mans</first><last>Hulden</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Michael</first><last>Ginn</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Miikka</first><last>Silfverberg</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Michael</first><last>Hammond</last><affiliation>University of Arizona</affiliation></author>
      <pages>258-265</pages>
      <abstract>We describe PyFoma, an open-source Python module for constructing weighted and unweighted finite-state transducers and automata from regular expressions, string rewriting rules, right-linear grammars, or low-level state/transition manipulation. A large variety of standard algorithms for working with finite-state machines is included, with a particular focus on the needs of linguistic and NLP applications. The data structures and code in the module are designed for legibility to allow for potential use in teaching the theory and algorithms associated with finite-state machines.</abstract>
      <url hash="e953c5d9">2024.acl-demos.24</url>
    </paper>
    <paper id="25">
      <title><fixed-case>V</fixed-case>era<fixed-case>CT</fixed-case> Scan: Retrieval-Augmented Fake News Detection with Justifiable Reasoning</title>
      <author><first>Cheng</first><last>Niu</last></author>
      <author><first>Yang</first><last>Guan</last></author>
      <author><first>Yuanhao</first><last>Wu</last><affiliation>Newsbreak</affiliation></author>
      <author><first>Juno</first><last>Zhu</last></author>
      <author><first>Juntong</first><last>Song</last></author>
      <author><first>Randy</first><last>Zhong</last></author>
      <author><first>Kaihua</first><last>Zhu</last></author>
      <author><first>Siliang</first><last>Xu</last></author>
      <author><first>Shizhe</first><last>Diao</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>266-277</pages>
      <abstract>The proliferation of fake news poses a significant threat not only by disseminating misleading information but also by undermining the very foundations of democracy. The recent advance of generative artificial intelligence has further exacerbated the challenge of distinguishing genuine news from fabricated stories. In response to this challenge, we introduce VeraCT Scan, a novel retrieval-augmented system for fake news detection. This system operates by extracting the core facts from a given piece of news and subsequently conducting an internet-wide search to identify corroborating or conflicting reports. Then sources’ credibility is leveraged for information verification. Besides determining the veracity of news, we also provide transparent evidence and reasoning to support its conclusions, resulting in the interpretability and trust in the results. In addition to GPT-4 Turbo, Llama-2 13B is also fine-tuned for news content understanding, information verification, and reasoning. Both implementations have demonstrated state-of-the-art accuracy in the realm of fake news detection.</abstract>
      <url hash="5772bcbb">2024.acl-demos.25</url>
    </paper>
    <paper id="26">
      <title>string2string: A Modern Python Library for String-to-String Algorithms</title>
      <author><first>Mirac</first><last>Suzgun</last><affiliation>Stanford University</affiliation></author>
      <author><first>Stuart</first><last>Shieber</last><affiliation>Harvard University</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <pages>278-285</pages>
      <abstract>We introduce **string2string**, an open-source library that offers a comprehensive suite of efficient algorithms for a broad range of string-to-string problems. It includes traditional algorithmic solutions as well as recent advanced neural approaches to tackle various problems in string alignment, distance measurement, lexical and semantic search, and similarity analysis�along with several helpful visualization tools and metrics to facilitate the interpretation and analysis of these methods. Notable algorithms featured in the library include the Smith-Waterman algorithm for pairwise local alignment, the Hirschberg algorithm for global alignment, the Wagner-Fischer algorithm for edit distance, BARTScore and BERTScore for similarity analysis, the Knuth-Morris-Pratt algorithm for lexical search, and Faiss for semantic search. In addition, it wraps existing efficient and widely-used implementations of certain frameworks and metrics, such as sacreBLEU and ROUGE. Overall, the library aims to provide extensive coverage and increased flexibility in comparison to existing libraries for strings. It can be used for many downstream applications, tasks, and problems in natural-language processing, bioinformatics, and computational social sciences. It is implemented in Python, easily installable via pip, and accessible through a simple API. Source code, documentation, and tutorials are all available on our GitHub page: https://github.com/stanfordnlp/string2string* Documentation: https://string2string.readthedocs.io/en/latest/* GitHub page: https://github.com/stanfordnlp/string2string* Short video: https://drive.google.com/file/d/1IT-pBACDVUoEHewk__5Pz5mU5oAMq5k_/view?usp=sharing</abstract>
      <url hash="dfc43a85">2024.acl-demos.26</url>
    </paper>
    <paper id="27">
      <title>Proofread: Fixes All Errors with One Tap</title>
      <author><first>Renjie</first><last>Liu</last></author>
      <author><first>Yanxiang</first><last>Zhang</last></author>
      <author><first>Yun</first><last>Zhu</last><affiliation>Google</affiliation></author>
      <author><first>Haicheng</first><last>Sun</last></author>
      <author><first>Yuanbo</first><last>Zhang</last></author>
      <author><first>Michael</first><last>Huang</last><affiliation>Google</affiliation></author>
      <author><first>Shanqing</first><last>Cai</last><affiliation>Google</affiliation></author>
      <author><first>Lei</first><last>Meng</last></author>
      <author><first>Shumin</first><last>Zhai</last><affiliation>Google</affiliation></author>
      <pages>286-293</pages>
      <abstract>The impressive capabilities in Large Language Models (LLMs) provide a powerful approach to reimagine users’ typing experience. This paper demonstrates the Proofread feature in Gboard, a virtual keyboard running on mobile phones. Proofread enables seamless sentence-level and paragraph-level corrections with a single tap. We describe the complete system in this paper, from data generation, metrics design to model tuning and deployment. To obtain models with sufficient quality, we implement a careful data synthetic pipeline tailored to online use cases, design multifaceted metrics, employ a two-stage tuning approach to acquire the dedicated LLM for the feature: the Supervised Fine Tuning (SFT) for foundational quality, followed by the Reinforcement Learning (RL) tuning approach for targeted refinement. Specifically, we find sequential tuning on Rewrite and proofread tasks yields the best quality in SFT stage, and propose global and direct rewards in the RL tuning stage to seek further improvement. Extensive experiments on a human-labeled golden set showed our tuned PaLM2-XS model achieved 85.56% good ratio. We launched the feature to Pixel 8 devices by serving the model on TPU v5 in Google Cloud, with thousands of daily active users. Serving latency was significantly reduced by quantization, bucket inference, text segmentation, and speculative decoding. Our demo could be seen in Youtube.</abstract>
      <url hash="a4223881">2024.acl-demos.27</url>
    </paper>
    <paper id="28">
      <title><fixed-case>S</fixed-case>ea<fixed-case>LLM</fixed-case>s - Large Language Models for <fixed-case>S</fixed-case>outheast <fixed-case>A</fixed-case>sia</title>
      <author><first>Xuan-Phi</first><last>Nguyen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Xin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Mahani</first><last>Aljunied</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhiqiang</first><last>Hu</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Chenhui</first><last>Shen</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yew Ken</first><last>Chia</last></author>
      <author><first>Xingxuan</first><last>Li</last></author>
      <author><first>Jianyu</first><last>Wang</last><affiliation>Alibaba DAMO Academy</affiliation></author>
      <author><first>Qingyu</first><last>Tan</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Liying</first><last>Cheng</last></author>
      <author><first>Guanzheng</first><last>Chen</last></author>
      <author><first>Yue</first><last>Deng</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Sen</first><last>Yang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Chaoqun</first><last>Liu</last></author>
      <author><first>Hang</first><last>Zhang</last></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Alibaba Group</affiliation></author>
      <pages>294-304</pages>
      <abstract>Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon popular English-centric models through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate.</abstract>
      <url hash="b2b6086c">2024.acl-demos.28</url>
    </paper>
    <paper id="29">
      <title>Fundus: A Simple-to-Use News Scraper Optimized for High Quality Extractions</title>
      <author><first>Max</first><last>Dallabetta</last><affiliation>Department of Computer Science, Humboldt University Berlin, Humboldt Universit�t Berlin</affiliation></author>
      <author><first>Conrad</first><last>Dobberstein</last><affiliation>Technische Universit�t Berlin</affiliation></author>
      <author><first>Adrian</first><last>Breiding</last></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universit�t Berlin</affiliation></author>
      <pages>305-314</pages>
      <abstract>This paper introduces Fundus, a user-friendly news scraper that enables users to obtain millions of high-quality news articles with just a few lines of code. Unlike existing news scrapers, we use manually crafted, bespoke content extractors that are specifically tailored to the formatting guidelines of each supported online newspaper. This allows us to optimize our scraping for quality such that retrieved news articles are textually complete and without HTML artifacts. Further, our framework combines both crawling (retrieving HTML from the web or large web archives) and content extraction into a single pipeline. By providing a unified interface for a predefined collection of newspapers, we aim to make Fundus broadly usable even for non-technical users. This paper gives an overview of the framework, discusses our design choices, and presents a comparative evaluation against other popular news scrapers. Our evaluation shows that Fundus yields significantly higher quality extractions (complete and artifact-free news articles) than prior work.The framework is available on GitHub under https://github.com/flairNLP/fundus and can be simply installed using pip.</abstract>
      <url hash="356e496a">2024.acl-demos.29</url>
    </paper>
    <paper id="30">
      <title><fixed-case>C</fixed-case>har<fixed-case>P</fixed-case>oet: A <fixed-case>C</fixed-case>hinese Classical Poetry Generation System Based on Token-free <fixed-case>LLM</fixed-case></title>
      <author><first>Chengyue</first><last>Yu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Lei</first><last>Zang</last></author>
      <author><first>Jiaotuan</first><last>Wang</last></author>
      <author><first>Chenyi</first><last>Zhuang</last></author>
      <author><first>Jinjie</first><last>Gu</last></author>
      <pages>315-325</pages>
      <abstract>Automatic Chinese classical poetry generation has attracted much research interest, but achieving effective control over format and content simultaneously remains challenging. Traditional systems usually accept keywords as user inputs, resulting in limited control over content. Large language models (LLMs) improve content control by allowing unrestricted user instructions, but the token-by-token generation process frequently makes format errors. Motivated by this, we propose CharPoet, a Chinese classical poetry generation system based on token-free LLM, which provides effective control over both format and content. Our token-free architecture generates in a character-by-character manner, enabling precise control over the number of characters. Pruned from existing token-based LLMs, CharPoet inherits their pretrained capabilities and can generate poetry following instructions like �Write me a poem for my mother’s birthday.� CharPoet achieves format accuracy above 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of content quality, CharPoet surpasses traditional systems including Jiuge, and is comparable to other LLMs. Our system is open source and available at https://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of CharPoet is available at https://youtu.be/voZ25qEp3Dc.</abstract>
      <url hash="e359bfda">2024.acl-demos.30</url>
    </paper>
    <paper id="31">
      <title><fixed-case>ITAKE</fixed-case>: Interactive Unstructured Text Annotation and Knowledge Extraction System with <fixed-case>LLM</fixed-case>s and <fixed-case>M</fixed-case>odel<fixed-case>O</fixed-case>ps</title>
      <author><first>Jiahe</first><last>Song</last></author>
      <author><first>Hongxin</first><last>Ding</last></author>
      <author><first>Zhiyuan</first><last>Wang</last></author>
      <author><first>Yongxin</first><last>Xu</last></author>
      <author><first>Yasha</first><last>Wang</last></author>
      <author><first>Junfeng</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <pages>326-334</pages>
      <abstract>Extracting structured knowledge from unstructured text data has a wide range of application prospects, and a pervasive trend is to develop text annotation tools to help extraction. However, they often encounter issues such as single scenario usage, lack of effective human-machine collaboration, insufficient model supervision, and suboptimal utilization of Large Language Models (LLMs). We introduces an interactive unstructured text annotation and knowledge extraction system that synergistically integrates LLMs and ModelOps to alleviate these issues. The system leverages LLMs for enhanced performance in low-resource contexts, employs a ModelOps platform to monitor models throughout their lifecycle, and amalgamates interactive annotation methods with online machine learning and active learning. The demo video and website are now publicly available.</abstract>
      <url hash="b2a6b68f">2024.acl-demos.31</url>
    </paper>
    <paper id="32">
      <title><fixed-case>LEGENT</fixed-case>: Open Platform for Embodied Agents</title>
      <author><first>Zhili</first><last>Cheng</last></author>
      <author><first>Zhitong</first><last>Wang</last></author>
      <author><first>Jinyi</first><last>Hu</last></author>
      <author><first>Shengding</first><last>Hu</last></author>
      <author><first>An</first><last>Liu</last></author>
      <author><first>Yuge</first><last>Tu</last></author>
      <author><first>Pengkai</first><last>Li</last><affiliation>Central South University</affiliation></author>
      <author><first>Lei</first><last>Shi</last></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>335-345</pages>
      <abstract>Despite advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), their integration into language-grounded, human-like embodied agents remains incomplete, hindering complex real-life task performance in 3D environments. Existing integrations often feature limited open-sourcing, challenging collective progress in this field. We introduce LEGENT, an open, scalable platform for developing embodied agents using LLMs and LMMs. LEGENT offers a dual approach: a rich 3D environment with interactive, communicable, and actionable agents, paired with a user-friendly interface, and a sophisticated data generation pipeline utilizing advanced algorithms to exploit supervision from simulated worlds at scale. In our experiments, an embryonic vision-language-action model trained on LEGENT-generated data surpasses GPT-4V in embodied tasks, showcasing promising generalization capabilities. The demo video is available at the following link https://video.legent.ai.</abstract>
      <url hash="3189f1e9">2024.acl-demos.32</url>
    </paper>
    <paper id="33">
      <title>Variationist: Exploring Multifaceted Variation and Bias in Written Language Data</title>
      <author><first>Alan</first><last>Ramponi</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Camilla</first><last>Casula</last><affiliation>University of Trento and Fondazione Bruno Kessler</affiliation></author>
      <author><first>Stefano</first><last>Menini</last></author>
      <pages>346-354</pages>
      <abstract>Exploring and understanding language data is a fundamental stage in all areas dealing with human language. It allows NLP practitioners to uncover quality concerns and harmful biases in data before training, and helps linguists and social scientists to gain insight into language use and human behavior. Yet, there is currently a lack of a unified, customizable tool to seamlessly inspect and visualize language variation and bias across multiple variables, language units, and diverse metrics that go beyond descriptive statistics. In this paper, we introduce Variationist, a highly-modular, extensible, and task-agnostic tool that fills this gap. Variationist handles at once a potentially unlimited combination of variable types and semantics across diversity and association metrics with regards to the language unit of choice, and orchestrates the creation of up to five-dimensional interactive charts for over 30 variable type-semantics combinations. Through our case studies on computational dialectology, human label variation, and text generation, we show how Variationist enables researchers from different disciplines to effortlessly answer specific research questions or unveil undesired associations in language data. A Python library, code, documentation, and tutorials are made publicly available to the research community.</abstract>
      <url hash="8ed90a72">2024.acl-demos.33</url>
    </paper>
    <paper id="34">
      <title>An <fixed-case>LLM</fixed-case>-based Knowledge Synthesis and Scientific Reasoning Framework for Biomedical Discovery</title>
      <author><first>Oskar</first><last>Wysocki</last></author>
      <author><first>Magdalena.wysocka@cruk.manchester.ac.uk</first><last>Magdalena.wysocka@cruk.manchester.ac.uk</last><affiliation>NA</affiliation></author>
      <author><first>Danilo</first><last>Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Alex</first><last>Bogatu</last></author>
      <author><first>Danilo.miranda@idiap.ch</first><last>Danilo.miranda@idiap.ch</last><affiliation>NA</affiliation></author>
      <author><first>Maxime.delmas@idiap.ch</first><last>Maxime.delmas@idiap.ch</last><affiliation>NA</affiliation></author>
      <author><first>Harriet.unsworth@cruk.manchester.ac.uk</first><last>Harriet.unsworth@cruk.manchester.ac.uk</last><affiliation>NA</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>355-364</pages>
      <abstract>We present BioLunar, developed using the Lunar framework, as a tool for supporting biological analyses, with a particular emphasis on molecular-level evidence enrichment for biomarker discovery in oncology. The platform integrates Large Language Models (LLMs) to facilitate complex scientific reasoning across distributed evidence spaces, enhancing the capability for harmonizing and reasoning over heterogeneous data sources. Demonstrating its utility in cancer research, BioLunar leverages modular design, reusable data access and data analysis components, and a low-code user interface, enabling researchers of all programming levels to construct LLM-enabled scientific workflows. By facilitating automatic scientific discovery and inference from heterogeneous evidence, BioLunar exemplifies the potential of the integration between LLMs, specialised databases and biomedical tools to support expert-level knowledge synthesis and discovery.</abstract>
      <url hash="d0f121d9">2024.acl-demos.34</url>
    </paper>
    <paper id="35">
      <title><fixed-case>C</fixed-case>og<fixed-case>MG</fixed-case>: Collaborative Augmentation Between Large Language Model and Knowledge Graph</title>
      <author><first>Tong</first><last>Zhou</last></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>365-373</pages>
      <abstract>Large language models have become integral to question-answering applications despite their propensity for generating hallucinations and factually inaccurate content. Querying knowledge graphs to reduce hallucinations in LLM meets the challenge of incomplete knowledge coverage in knowledge graphs. On the other hand, updating knowledge graphs by information extraction and knowledge graph completion faces the knowledge update misalignment issue. In this work, we introduce a collaborative augmentation framework, CogMG, leveraging knowledge graphs to address the limitations of LLMs in QA scenarios, explicitly targeting the problems of incomplete knowledge coverage and knowledge update misalignment. The LLMs identify and decompose required knowledge triples that are not present in the KG, enriching them and aligning updates with real-world demands. We demonstrate the efficacy of this approach through a supervised fine-tuned LLM within an agent framework, showing significant improvements in reducing hallucinations and enhancing factual accuracy in QA responses. Our code and video are publicly available.</abstract>
      <url hash="cc0e3625">2024.acl-demos.35</url>
    </paper>
    <paper id="36">
      <title><fixed-case>ELLA</fixed-case>: Empowering <fixed-case>LLM</fixed-case>s for Interpretable, Accurate and Informative Legal Advice</title>
      <author><first>Yutong</first><last>Hu</last></author>
      <author><first>Kangcheng</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <pages>374-387</pages>
      <abstract>Despite remarkable performance in legal consultation exhibited by legal Large Language Models(LLMs) combined with legal article retrieval components, there are still cases when the advice given is incorrect or baseless. To alleviate these problems, we propose <b>ELLA</b>, a tool for <b>E</b>mpowering <b>L</b>LMs for interpretable, accurate, and informative <b>L</b>egal <b>A</b>dvice. ELLA visually presents the correlation between legal articles and LLM’s response by calculating their similarities, providing users with an intuitive legal basis for the responses. Besides, based on the users’ queries, ELLA retrieves relevant legal articles and displays them to users. Users can interactively select legal articles for LLM to generate more accurate responses. ELLA also retrieves relevant legal cases for user reference. Our user study shows that presenting the legal basis for the response helps users understand better. The accuracy of LLM’s responses also improves when users intervene in selecting legal articles for LLM. Providing relevant legal cases also aids individuals in obtaining comprehensive information. Our github repo is: <url>https://github.com/Huyt00/ELLA</url>.</abstract>
      <url hash="b2d5717d">2024.acl-demos.36</url>
    </paper>
    <paper id="37">
      <title><fixed-case>LLMB</fixed-case>ox: A Comprehensive Library for Large Language Models</title>
      <author><first>Tianyi</first><last>Tang</last></author>
      <author><first>Hu</first><last>Yiwen</last></author>
      <author><first>Bingqian</first><last>Li</last></author>
      <author><first>Wenyang</first><last>Luo</last></author>
      <author><first>ZiJing</first><last>Qin</last></author>
      <author><first>Haoxiang</first><last>Sun</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jiapeng</first><last>Wang</last></author>
      <author><first>Shiyi</first><last>Xu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xiaoxue</first><last>Cheng</last></author>
      <author><first>Geyang</first><last>Guo</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Han</first><last>Peng</last></author>
      <author><first>Bowen</first><last>Zheng</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yiru</first><last>Tang</last></author>
      <author><first>Yingqian</first><last>Min</last></author>
      <author><first>Yushuo</first><last>Chen</last></author>
      <author><first>Jie</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ranchi</first><last>Zhao</last><affiliation>ModelBest</affiliation></author>
      <author><first>Luran</first><last>Ding</last></author>
      <author><first>Yuhao</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zican</first><last>Dong</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xia</first><last>Chunxuan</last></author>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Kun</first><last>Zhou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>388-399</pages>
      <abstract>To facilitate the research on large language models (LLMs), this paper presents a comprehensive and unified library, LLMBox, to ease the development, use, and evaluation of LLMs. This library is featured with three main merits: (1) a unified data interface that supports the flexible implementation of various training strategies, (2) a comprehensive evaluation that covers extensive tasks, datasets, and models, and (3) more practical consideration, especially on user-friendliness and efficiency. With our library, users can easily reproduce existing methods, train new models, and conduct comprehensive performance comparisons. To rigorously test LLMBox, we conduct extensive experiments in a diverse coverage of evaluation settings, and experimental results demonstrate the effectiveness and efficiency of our library in supporting various implementations related to LLMs. The detailed introduction and usage guidance can be found at <url>https://github.com/RUCAIBox/LLMBox</url>.</abstract>
      <url hash="1ad071d1">2024.acl-demos.37</url>
    </paper>
    <paper id="38">
      <title><fixed-case>L</fixed-case>lama<fixed-case>F</fixed-case>actory: Unified Efficient Fine-Tuning of 100+ Language Models</title>
      <author><first>Yaowei</first><last>Zheng</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Junhao</first><last>Zhang</last></author>
      <author><first>YeYanhan</first><last>YeYanhan</last></author>
      <author><first>Zheyan</first><last>Luo</last></author>
      <pages>400-410</pages>
      <abstract>Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and 3,000 forks.</abstract>
      <url hash="5a5b0cc0">2024.acl-demos.38</url>
    </paper>
  </volume>
  <event id="acl-2023">
    <colocated>
      <volume-id>2024.alvr-1</volume-id>
      <volume-id>2024.arabicnlp-1</volume-id>
      <volume-id>2024.argmining-1</volume-id>
      <volume-id>2024.bionlp-1</volume-id>
      <volume-id>2024.c3nlp-1</volume-id>
      <volume-id>2024.climatenlp-1</volume-id>
      <volume-id>2024.cmcl-1</volume-id>
      <volume-id>2024.conda-1</volume-id>
      <volume-id>2024.fieldmatters-1</volume-id>
      <volume-id>2024.gebnlp-1</volume-id>
      <volume-id>2024.hucllm-1</volume-id>
      <volume-id>2024.iwslt-1</volume-id>
      <volume-id>2024.kallm-1</volume-id>
      <volume-id>2024.knowledgenlp-1</volume-id>
      <volume-id>2024.knowllm-1</volume-id>
      <volume-id>2024.langmol-1</volume-id>
      <volume-id>2024.lchange-1</volume-id>
      <volume-id>2024.loresmt-1</volume-id>
      <volume-id>2024.ml4al-1</volume-id>
      <volume-id>2024.nlp4convai-1</volume-id>
      <volume-id>2024.nlrse-1</volume-id>
      <volume-id>2024.privatenlp-1</volume-id>
      <volume-id>2024.repl4nlp-1</volume-id>
      <volume-id>2024.sdp-1</volume-id>
      <volume-id>2024.sighan-1</volume-id>
      <volume-id>2024.sigturk-1</volume-id>
      <volume-id>2024.smm4h-1</volume-id>
      <volume-id>2024.splurobonlp-1</volume-id>
      <volume-id>2024.teachingnlp-1</volume-id>
      <volume-id>2024.textgraphs-1</volume-id>
      <volume-id>2024.wassa-1</volume-id>
    </colocated>
  </event>
</collection>
