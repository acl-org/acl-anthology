<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.nlpcss">
  <volume id="1" ingest-date="2023-01-17" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS)</booktitle>
      <editor><first>David</first><last>Bamman</last></editor>
      <editor><first>Dirk</first><last>Hovy</last></editor>
      <editor><first>David</first><last>Jurgens</last></editor>
      <editor><first>Katherine</first><last>Keith</last></editor>
      <editor><first>Brendan</first><last>O'Connor</last></editor>
      <editor><first>Svitlana</first><last>Volkova</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, UAE</address>
      <month>November</month>
      <year>2022</year>
      <url hash="ca7a9cbb">2022.nlpcss-1</url>
      <venue>nlpcss</venue>
    </meta>
    <frontmatter>
      <url hash="8d34fb70">2022.nlpcss-1.0</url>
      <bibkey>nlp-css-2022-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Improving the Generalizability of Text-Based Emotion Detection by Leveraging Transformers with Psycholinguistic Features</title>
      <author><first>Sourabh</first><last>Zanwar</last><affiliation>RWTH Aachen University</affiliation></author>
      <author><first>Daniel</first><last>Wiechmann</last><affiliation>Institute for Logic Language and Computation</affiliation></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>RWTH-Achen</affiliation></author>
      <author><first>Elma</first><last>Kerz</last><affiliation>RWTH Aachen University</affiliation></author>
      <pages>1-13</pages>
      <abstract>recent years, there has been increased interest in building predictive models that harness natural language processing and machine learning techniques to detect emotions from various text sources, including social media posts, micro-blogs or news articles. Yet, deployment of such models in real-world sentiment and emotion applications faces challenges, in particular poor out-of-domain generalizability. This is likely due to domain-specific differences (e.g., topics, communicative goals, and annotation schemes) that make transfer between different models of emotion recognition difficult. In this work we propose approaches for text-based emotion detection that leverage transformer models (BERT and RoBERTa) in combination with Bidirectional Long Short-Term Memory (BiLSTM) networks trained on a comprehensive set of psycholinguistic features. First, we evaluate the performance of our models within-domain on two benchmark datasets GoEmotion (Demszky et al., 2020) and ISEAR (Scherer and Wallbott, 1994). Second, we conduct transfer learning experiments on six datasets from the Unified Emotion Dataset (Bostan and Klinger, 2018) to evaluate their out-of-domain robustness. We find that the proposed hybrid models improve the ability to generalize to out-of-distribution data compared to a standard transformer-based approach. Moreover, we observe that these models perform competitively on in-domain data.’</abstract>
      <url hash="ca540d50">2022.nlpcss-1.1</url>
      <bibkey>zanwar-etal-2022-improving</bibkey>
      <video href="2022.nlpcss-1.1.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.1</doi>
    </paper>
    <paper id="2">
      <title>Fine-Grained Extraction and Classification of Skill Requirements in <fixed-case>G</fixed-case>erman-Speaking Job Ads</title>
      <author><first>Ann-sophie</first><last>Gnehm</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Eva</first><last>Bühlmann</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Helen</first><last>Buchs</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Simon</first><last>Clematide</last><affiliation>University of Zurich</affiliation></author>
      <pages>14-24</pages>
      <abstract>Monitoring the development of labor market skill requirements is an information need that is more and more approached by applying text mining methods to job advertisement data. We present an approach for fine-grained extraction and classification of skill requirements from German-speaking job advertisements. We adapt pre-trained transformer-based language models to the domain and task of computing meaningful representations of sentences or spans. By using context from job advertisements and the large ESCO domain ontology we improve our similarity-based unsupervised multi-label classification results. Our best model achieves a mean average precision of 0.969 on the skill class level.</abstract>
      <url hash="30993f27">2022.nlpcss-1.2</url>
      <bibkey>gnehm-etal-2022-fine</bibkey>
      <video href="2022.nlpcss-1.2.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.2</doi>
    </paper>
    <paper id="3">
      <title>Experiencer-Specific Emotion and Appraisal Prediction</title>
      <author><first>Maximilian</first><last>Wegge</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Enrica</first><last>Troiano</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Laura Ana Maria</first><last>Oberlaender</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>25-32</pages>
      <abstract>Emotion classification in NLP assigns emotions to texts, such as sentences or paragraphs. With texts like “I felt guilty when he cried”, focusing on the sentence level disregards the standpoint of each participant in the situation: the writer (“I”) and the other entity (“he”) could in fact have different affective states. The emotions of different entities have been considered only partially in emotion semantic role labeling, a task that relates semantic roles to emotion cue words. Proposing a related task, we narrow the focus on the experiencers of events, and assign an emotion (if any holds) to each of them. To this end, we represent each emotion both categorically and with appraisal variables, as a psychological access to explaining why a person develops a particular emotion. On an event description corpus, our experiencer-aware models of emotions and appraisals outperform the experiencer-agnostic baselines, showing that disregarding event participants is an oversimplification for the emotion detection task.</abstract>
      <url hash="d4b61ca6">2022.nlpcss-1.3</url>
      <bibkey>wegge-etal-2022-experiencer</bibkey>
      <video href="2022.nlpcss-1.3.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.3</doi>
    </paper>
    <paper id="4">
      <title>Understanding Narratives from Demographic Survey Data: a Comparative Study with Multiple Neural Topic Models</title>
      <author><first>Xiao</first><last>Xu</last><affiliation>NIDI-KNAW / University of Groningen</affiliation></author>
      <author><first>Gert</first><last>Stulp</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Antal</first><last>Van Den Bosch</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Anne</first><last>Gauthier</last><affiliation>Nidi-knaw</affiliation></author>
      <pages>33-38</pages>
      <abstract>Fertility intentions as verbalized in surveys are a poor predictor of actual fertility outcomes, the number of children people have. This can partly be explained by the uncertainty people have in their intentions. Such uncertainties are hard to capture through traditional survey questions, although open-ended questions can be used to get insight into people’s subjective narratives of the future that determine their intentions. Analyzing such answers to open-ended questions can be done through Natural Language Processing techniques. Traditional topic models (e.g., LSA and LDA), however, often fail to do since they rely on co-occurrences, which are often rare in short survey responses. The aim of this study was to apply and evaluate topic models on demographic survey data. In this study, we applied neural topic models (e.g. BERTopic, CombinedTM) based on language models to responses from Dutch women on their fertility plans, and compared the topics and their coherence scores from each model to expert judgments. Our results show that neural models produce topics more in line with human interpretation compared to LDA. However, the coherence score could only partly reflect on this, depending on the corpus used for calculation. This research is important because, first, it helps us develop more informed strategies on model selection and evaluation for topic modeling on survey data; and second, it shows that the field of demography has much to gain from adopting NLP methods.</abstract>
      <url hash="5c9fdd29">2022.nlpcss-1.4</url>
      <bibkey>xu-etal-2022-understanding-narratives</bibkey>
      <doi>10.18653/v1/2022.nlpcss-1.4</doi>
    </paper>
    <paper id="6">
      <title>To Prefer or to Choose? Generating Agency and Power Counterfactuals Jointly for Gender Bias Mitigation</title>
      <author><first>Maja</first><last>Stahl</last><affiliation>Leibniz University Hannover</affiliation></author>
      <author><first>Maximilian</first><last>Spliethöver</last><affiliation>Leibniz University Hannover</affiliation></author>
      <author><first>Henning</first><last>Wachsmuth</last><affiliation>Leibniz University Hannover</affiliation></author>
      <pages>39-51</pages>
      <abstract>Gender bias may emerge from an unequal representation of agency and power, for example, by portraying women frequently as passive and powerless (“She accepted her future”) and men as proactive and powerful (“He chose his future”). When language models learn from respective texts, they may reproduce or even amplify the bias. An effective way to mitigate bias is to generate counterfactual sentences with opposite agency and power to the training. Recent work targeted agency-specific verbs from a lexicon to this end. We argue that this is insufficient, due to the interaction of agency and power and their dependence on context. In this paper, we thus develop a new rewriting model that identifies verbs with the desired agency and power in the context of the given sentence. The verbs’ probability is then boosted to encourage the model to rewrite both connotations jointly. According to automatic metrics, our model effectively controls for power while being competitive in agency to the state of the art. In our main evaluation, human annotators favored its counterfactuals in terms of both connotations, also deeming its meaning preservation better.</abstract>
      <url hash="c9712fbe">2022.nlpcss-1.6</url>
      <attachment type="note" hash="314b9476">2022.nlpcss-1.6.note.txt</attachment>
      <bibkey>stahl-etal-2022-prefer</bibkey>
      <doi>10.18653/v1/2022.nlpcss-1.6</doi>
    </paper>
    <paper id="8">
      <title>Conspiracy Narratives in the Protest Movement Against <fixed-case>COVID</fixed-case>-19 Restrictions in <fixed-case>G</fixed-case>ermany. A Long-term Content Analysis of Telegram Chat Groups.</title>
      <author><first>Manuel</first><last>Weigand</last><affiliation>Goethe University</affiliation></author>
      <author><first>Maximilian</first><last>Weber</last><affiliation>Goethe University</affiliation></author>
      <author><first>Johannes</first><last>Gruber</last><affiliation>European University Viadrina</affiliation></author>
      <pages>52-58</pages>
      <abstract>From the start of the COVID-19 pandemic in Germany, different groups have been protesting measures implemented by different government bodies in Germany to control the pandemic. It was widely claimed that many of the offline and online protests were driven by conspiracy narratives disseminated through groups and channels on the messenger app Telegram. We investigate this claim by measuring the frequency of conspiracy narratives in messages from open Telegram chat groups of the Querdenken movement, set up to organize protests against COVID-19 restrictions in Germany. We furthermore explore the content of these messages using topic modelling. To this end, we collected 822k text messages sent between April 2020 and May 2022 in 34 chat groups. By fine-tuning a Distilbert model, using self-annotated data, we find that 8.24% of the sent messages contain signs of conspiracy narratives. This number is not static, however, as the share of conspiracy messages grew while the overall number of messages shows a downward trend since its peak at the end of 2020. We further find a mix of known conspiracy narratives make up the topics in our topic model. Our findings suggest that the Querdenken movement is getting smaller over time, but its remaining members focus even more on conspiracy narratives.</abstract>
      <url hash="91dc331e">2022.nlpcss-1.8</url>
      <bibkey>weigand-etal-2022-conspiracy</bibkey>
      <doi>10.18653/v1/2022.nlpcss-1.8</doi>
    </paper>
    <paper id="9">
      <title>Conditional Language Models for Community-Level Linguistic Variation</title>
      <author><first>Bill</first><last>Noble</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Jean-philippe</first><last>Bernardy</last><affiliation>University of Gothenburg</affiliation></author>
      <pages>59-78</pages>
      <abstract>Community-level linguistic variation is a core concept in sociolinguistics. In this paper, we use conditioned neural language models to learn vector representations for 510 online communities. We use these representations to measure linguistic variation between commu-nities and investigate the degree to which linguistic variation corresponds with social connections between communities. We find that our sociolinguistic embeddings are highly correlated with a social network-based representation that does not use any linguistic input.</abstract>
      <url hash="9d04bdf8">2022.nlpcss-1.9</url>
      <bibkey>noble-bernardy-2022-conditional</bibkey>
      <video href="2022.nlpcss-1.9.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.9</doi>
    </paper>
    <paper id="10">
      <title>Understanding Interpersonal Conflict Types and their Impact on Perception Classification</title>
      <author><first>Charles</first><last>Welch</last><affiliation>University of Marburg</affiliation></author>
      <author><first>Joan</first><last>Plepi</last><affiliation>CAISA Lab, Faculty of Mathematics and Informatics, Philipps University of Marburg</affiliation></author>
      <author><first>Béla</first><last>Neuendorf</last><affiliation>Philipps-Universität Marburg</affiliation></author>
      <author><first>Lucie</first><last>Flek</last><affiliation>CAISA Lab, Faculty of Mathematics and Informatics, Philipps University of Marburg</affiliation></author>
      <pages>79-88</pages>
      <abstract>Studies on interpersonal conflict have a long history and contain many suggestions for conflict typology. We use this as the basis of a novel annotation scheme and release a new dataset of situations and conflict aspect annotations. We then build a classifier to predict whether someone will perceive the actions of one individual as right or wrong in a given situation. Our analyses include conflict aspects, but also generated clusters, which are human validated, and show differences in conflict content based on the relationship of participants to the author. Our findings have important implications for understanding conflict and social norms.</abstract>
      <url hash="d94f777b">2022.nlpcss-1.10</url>
      <bibkey>welch-etal-2022-understanding</bibkey>
      <video href="2022.nlpcss-1.10.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.10</doi>
    </paper>
    <paper id="11">
      <title>Examining Political Rhetoric with Epistemic Stance Detection</title>
      <author><first>Ankita</first><last>Gupta</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Su Lin</first><last>Blodgett</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Justin</first><last>Gross</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Brendan</first><last>O’connor</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>89-104</pages>
      <abstract>Participants in political discourse employ rhetorical strategies—such as hedging, attributions, or denials—to display varying degrees of belief commitments to claims proposed by themselves or others. Traditionally, political scientists have studied these epistemic phenomena through labor-intensive manual content analysis. We propose to help automate such work through epistemic stance prediction, drawn from research in computational semantics, to distinguish at the clausal level what is asserted, denied, or only ambivalently suggested by the author or other mentioned entities (belief holders). We first develop a simple RoBERTa-based model for multi-source stance predictions that outperforms more complex state-of-the-art modeling. Then we demonstrate its novel application to political science by conducting a large-scale analysis of the Mass Market Manifestos corpus of U.S. political opinion books, where we characterize trends in cited belief holders—respected allies and opposed bogeymen—across U.S. political ideologies.</abstract>
      <url hash="83e8880d">2022.nlpcss-1.11</url>
      <bibkey>gupta-etal-2022-examining</bibkey>
      <doi>10.18653/v1/2022.nlpcss-1.11</doi>
    </paper>
    <paper id="12">
      <title>Linguistic Elements of Engaging Customer Service Discourse on Social Media</title>
      <author><first>Sonam</first><last>Singh</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <author><first>Anthony</first><last>Rios</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <pages>105-117</pages>
      <abstract>Customers are rapidly turning to social media for customer support. While brand agents on these platforms are motivated and well-intentioned to help and engage with customers, their efforts are often ignored if their initial response to the customer does not match a specific tone, style, or topic the customer is aiming to receive. The length of a conversation can reflect the effort and quality of the initial response made by a brand toward collaborating and helping consumers, even when the overall sentiment of the conversation might not be very positive. Thus, through this study, we aim to bridge this critical gap in the existing literature by analyzing language’s content and stylistic aspects such as expressed empathy, psycho-linguistic features, dialogue tags, and metrics for quantifying personalization of the utterances that can influence the engagement of an interaction. This paper demonstrates that we can predict engagement using initial customer and brand posts.</abstract>
      <url hash="63566ecc">2022.nlpcss-1.12</url>
      <bibkey>singh-rios-2022-linguistic</bibkey>
      <video href="2022.nlpcss-1.12.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.12</doi>
    </paper>
    <paper id="13">
      <title>Measuring Harmful Representations in <fixed-case>S</fixed-case>candinavian Language Models</title>
      <author><first>Samia</first><last>Touileb</last><affiliation>University of Bergen</affiliation></author>
      <author><first>Debora</first><last>Nozza</last><affiliation>Bocconi University</affiliation></author>
      <pages>118-125</pages>
      <abstract>Scandinavian countries are perceived as role-models when it comes to gender equality. With the advent of pre-trained language models and their widespread usage, we investigate to what extent gender-based harmful and toxic content exists in selected Scandinavian language models. We examine nine models, covering Danish, Swedish, and Norwegian, by manually creating template-based sentences and probing the models for completion. We evaluate the completions using two methods for measuring harmful and toxic completions and provide a thorough analysis of the results. We show that Scandinavian pre-trained language models contain harmful and gender-based stereotypes with similar values across all languages. This finding goes against the general expectations related to gender equality in Scandinavian countries and shows the possible problematic outcomes of using such models in real-world settings. Warning: Some of the examples provided in this paper can be upsetting and offensive.</abstract>
      <url hash="5d201f77">2022.nlpcss-1.13</url>
      <bibkey>touileb-nozza-2022-measuring</bibkey>
      <video href="2022.nlpcss-1.13.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.13</doi>
    </paper>
    <paper id="14">
      <title>Can Contextualizing User Embeddings Improve Sarcasm and Hate Speech Detection?</title>
      <author><first>Kim</first><last>Breitwieser</last><affiliation>MaibornWolff GmbH</affiliation></author>
      <pages>126-139</pages>
      <abstract>While implicit embeddings so far have been mostly concerned with creating an overall representation of the user, we evaluate a different approach. By only considering content directed at a specific topic, we create sub-user embeddings, and measure their usefulness on the tasks of sarcasm and hate speech detection. In doing so, we show that task-related topics can have a noticeable effect on model performance, especially when dealing with intended expressions like sarcasm, but less so for hate speech, which is usually labelled as such on the receiving end.</abstract>
      <url hash="ff78f696">2022.nlpcss-1.14</url>
      <attachment type="dataset" hash="84cc6588">2022.nlpcss-1.14.dataset.zip</attachment>
      <bibkey>breitwieser-2022-contextualizing</bibkey>
      <doi>10.18653/v1/2022.nlpcss-1.14</doi>
    </paper>
    <paper id="15">
      <title>Professional Presentation and Projected Power: A Case Study of Implicit Gender Information in <fixed-case>E</fixed-case>nglish <fixed-case>CV</fixed-case>s</title>
      <author><first>Jinrui</first><last>Yang</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Sheilla</first><last>Njoto</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Marc</first><last>Cheong</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Leah</first><last>Ruppanner</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Lea</first><last>Frermann</last><affiliation>Melbourne University</affiliation></author>
      <pages>140-150</pages>
      <abstract>Gender discrimination in hiring is a pertinent and persistent bias in society, and a common motivating example for exploring bias in NLP. However, the manifestation of gendered language in application materials has received limited attention. This paper investigates the framing of skills and background in CVs of self-identified men and women. We introduce a data set of 1.8K authentic, English-language, CVs from the US, covering 16 occupations, allowing us to partially control for the confound occupation-specific gender base rates. We find that (1) women use more verbs evoking impressions of low power; and (2) classifiers capture gender signal even after data balancing and removal of pronouns and named entities, and this holds for both transformer-based and linear classifiers.</abstract>
      <url hash="fa01e3dc">2022.nlpcss-1.15</url>
      <bibkey>yang-etal-2022-professional</bibkey>
      <doi>10.18653/v1/2022.nlpcss-1.15</doi>
    </paper>
    <paper id="16">
      <title>Detecting Dissonant Stance in Social Media: The Role of Topic Exposure</title>
      <author><first>Vasudha</first><last>Varadarajan</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Nikita</first><last>Soni</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Weixi</first><last>Wang</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Christian</first><last>Luhmann</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>H. Andrew</first><last>Schwartz</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Naoya</first><last>Inoue</last><affiliation>Japan Advanced Institute of Science and Technology</affiliation></author>
      <pages>151-156</pages>
      <abstract>We address dissonant stance detection, classifying conflicting stance between two input statements. Computational models for traditional stance detection have typically been trained to indicate pro/con for a given target topic (e.g. gun control) and thus do not generalize well to new topics. In this paper, we systematically evaluate the generalizability of dissonant stance detection to situations where examples of the topic have not been seen at all or have only been seen a few times. We show that dissonant stance detection models trained on only 8 topics, none of which are the target topic, can perform as well as those trained only on a target topic. Further, adding non-target topics boosts performance further up to approximately 32 topics where accuracies start to plateau. Taken together, our experiments suggest dissonant stance detection models can generalize to new unanticipated topics, an important attribute for the social scientific study of social media where new topics emerge daily.</abstract>
      <url hash="48cab82f">2022.nlpcss-1.16</url>
      <bibkey>varadarajan-etal-2022-detecting</bibkey>
      <doi>10.18653/v1/2022.nlpcss-1.16</doi>
    </paper>
    <paper id="17">
      <title>An Analysis of Acknowledgments in <fixed-case>NLP</fixed-case> Conference Proceedings</title>
      <author><first>Winston</first><last>Wu</last><affiliation>University of Michigan</affiliation></author>
      <pages>157-163</pages>
      <abstract>While acknowledgments are often overlooked and sometimes entirely missing from publications, this short section of a paper can provide insights on the state of a field. We characterize and perform a textual analysis of acknowledgments in NLP conference proceedings across the last 17 years, revealing broader trends in funding and research directions in NLP as well as interesting phenomena including career incentives and the influence of defaults.</abstract>
      <url hash="edb5c99f">2022.nlpcss-1.17</url>
      <bibkey>wu-2022-analysis</bibkey>
      <video href="2022.nlpcss-1.17.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.17</doi>
    </paper>
    <paper id="18">
      <title>Extracting Associations of Intersectional Identities with Discourse about Institution from <fixed-case>N</fixed-case>igeria</title>
      <author><first>Pavan</first><last>Kantharaju</last><affiliation>Smart Information Flow Technologies</affiliation></author>
      <author><first>Sonja</first><last>Schmer-galunder</last><affiliation>Smart Information Flow Technologies</affiliation></author>
      <pages>164-169</pages>
      <abstract>Word embedding models have been used in prior work to extract associations of intersectional identities within discourse concerning institutions of power, but restricted its focus on narratives of the nineteenth-century U.S. south. This paper leverages this prior work and introduces an initial study on the association of intersected identities with discourse concerning social institutions within social media from Nigeria. Specifically, we use word embedding models trained on tweets from Nigeria and extract associations of intersected social identities with institutions (e.g., domestic, culture, etc.) to provide insight into the alignment of identities with institutions. Our initial experiments indicate that identities at the intersection of gender and economic status groups have significant associations with discourse about the economic, political, and domestic institutions.</abstract>
      <url hash="d03269d7">2022.nlpcss-1.18</url>
      <bibkey>kantharaju-schmer-galunder-2022-extracting</bibkey>
      <video href="2022.nlpcss-1.18.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>OLALA</fixed-case>: Object-Level Active Learning for Efficient Document Layout Annotation</title>
      <author><first>Zejiang</first><last>Shen</last><affiliation>Mit</affiliation></author>
      <author><first>Weining</first><last>Li</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jian</first><last>Zhao</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Yaoliang</first><last>Yu</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Melissa</first><last>Dell</last><affiliation>Harvard University</affiliation></author>
      <pages>170-182</pages>
      <abstract>Layout detection is an essential step for accurately extracting structured contents from historical documents. The intricate and varied layouts present in these document images make it expensive to label the numerous layout regions that can be densely arranged on each page. Current active learning methods typically rank and label samples at the image level, where the annotation budget is not optimally spent due to the overexposure of common objects per image. Inspired by recent progress in semi-supervised learning and self-training, we propose OLALA, an Object-Level Active Learning framework for efficient document layout Annotation. OLALA aims to optimize the annotation process by selectively annotating only the most ambiguous regions within an image, while using automatically generated labels for the rest. Central to OLALA is a perturbation-based scoring function that determines which objects require manual annotation. Extensive experiments show that OLALA can significantly boost model performance and improve annotation efficiency, facilitating the extraction of masses of structured text for downstream NLP applications.</abstract>
      <url hash="c075c104">2022.nlpcss-1.19</url>
      <bibkey>shen-etal-2022-olala</bibkey>
      <doi>10.18653/v1/2022.nlpcss-1.19</doi>
    </paper>
    <paper id="20">
      <title>Towards Few-Shot Identification of Morality Frames using In-Context Learning</title>
      <author><first>Shamik</first><last>Roy</last><affiliation>Purdue University</affiliation></author>
      <author><first>Nishanth Sridhar</first><last>Nakshatri</last><affiliation>Purdue University</affiliation></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University</affiliation></author>
      <pages>183-196</pages>
      <abstract>Data scarcity is a common problem in NLP, especially when the annotation pertains to nuanced socio-linguistic concepts that require specialized knowledge. As a result, few-shot identification of these concepts is desirable. Few-shot in-context learning using pre-trained Large Language Models (LLMs) has been recently applied successfully in many NLP tasks. In this paper, we study few-shot identification of a psycho-linguistic concept, Morality Frames (Roy et al., 2021), using LLMs. Morality frames are a representation framework that provides a holistic view of the moral sentiment expressed in text, identifying the relevant moral foundation (Haidt and Graham, 2007) and at a finer level of granularity, the moral sentiment expressed towards the entities mentioned in the text. Previous studies relied on human annotation to identify morality frames in text which is expensive. In this paper, we propose prompting based approaches using pretrained Large Language Models for identification of morality frames, relying only on few-shot exemplars. We compare our models’ performance with few-shot RoBERTa and found promising results.</abstract>
      <url hash="afb757cc">2022.nlpcss-1.20</url>
      <bibkey>roy-etal-2022-towards</bibkey>
      <video href="2022.nlpcss-1.20.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.20</doi>
    </paper>
    <paper id="22">
      <title>Utilizing Weak Supervision to Create <fixed-case>S</fixed-case>3<fixed-case>D</fixed-case>: A Sarcasm Annotated Dataset</title>
      <author><first>Jordan</first><last>Painter</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Helen</first><last>Treharne</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Diptesh</first><last>Kanojia</last><affiliation>University of Surrey</affiliation></author>
      <pages>197-206</pages>
      <abstract>Sarcasm is prevalent in all corners of social media, posing many challenges within Natural Language Processing (NLP), particularly for sentiment analysis. Sarcasm detection remains a largely unsolved problem in many NLP tasks due to its contradictory and typically derogatory nature as a figurative language construct. With recent strides in NLP, many pre-trained language models exist that have been trained on data from specific social media platforms, i.e., Twitter. In this paper, we evaluate the efficacy of multiple sarcasm detection datasets using machine and deep learning models. We create two new datasets - a manually annotated gold standard Sarcasm Annotated Dataset (SAD) and a Silver-Standard Sarcasm-annotated Dataset (S3D). Using a combination of existing sarcasm datasets with SAD, we train a sarcasm detection model over a social-media domain pre-trained language model, BERTweet, which yields an F1-score of 78.29%. Using an Ensemble model with an underlying majority technique, we further label S3D to produce a weakly supervised dataset containing over 100,000 tweets. We publicly release all the code, our manually annotated and weakly supervised datasets, and fine-tuned models for further research.</abstract>
      <url hash="677f21de">2022.nlpcss-1.22</url>
      <bibkey>painter-etal-2022-utilizing</bibkey>
      <video href="2022.nlpcss-1.22.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.22</doi>
    </paper>
    <paper id="23">
      <title>A Robust Bias Mitigation Procedure Based on the Stereotype Content Model</title>
      <author><first>Eddie</first><last>Ungless</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Amy</first><last>Rafferty</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Hrichika</first><last>Nag</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Björn</first><last>Ross</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>207-217</pages>
      <abstract>The Stereotype Content model (SCM) states that we tend to perceive minority groups as cold, incompetent or both. In this paper we adapt existing work to demonstrate that the Stereotype Content model holds for contextualised word embeddings, then use these results to evaluate a fine-tuning process designed to drive a language model away from stereotyped portrayals of minority groups. We find the SCM terms are better able to capture bias than demographic agnostic terms related to pleasantness. Further, we were able to reduce the presence of stereotypes in the model through a simple fine-tuning procedure that required minimal human and computer resources, without harming downstream performance. We present this work as a prototype of a debiasing procedure that aims to remove the need for a priori knowledge of the specifics of bias in the model.</abstract>
      <url hash="5a29286b">2022.nlpcss-1.23</url>
      <bibkey>ungless-etal-2022-robust</bibkey>
      <video href="2022.nlpcss-1.23.mp4"/>
      <doi>10.18653/v1/2022.nlpcss-1.23</doi>
    </paper>
    <paper id="24">
      <title>Who is <fixed-case>GPT</fixed-case>-3? An exploration of personality, values and demographics</title>
      <author><first>Marilù</first><last>Miotto</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Nicola</first><last>Rossberg</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Bennett</first><last>Kleinberg</last><affiliation>Tilburg University</affiliation></author>
      <pages>218-227</pages>
      <abstract>Language models such as GPT-3 have caused a furore in the research community. Some studies found that GPT-3 has some creative abilities and makes mistakes that are on par with human behaviour. This paper answers a related question: Who is GPT-3? We administered two validated measurement tools to GPT-3 to assess its personality, the values it holds and its self-reported demographics. Our results show that GPT-3 scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds. We provide the first evidence of psychological assessment of the GPT-3 model and thereby add to our understanding of this language model. We close with suggestions for future research that moves social science closer to language models and vice versa.</abstract>
      <url hash="3f69f51f">2022.nlpcss-1.24</url>
      <bibkey>miotto-etal-2022-gpt</bibkey>
      <doi>10.18653/v1/2022.nlpcss-1.24</doi>
    </paper>
  </volume>
</collection>
