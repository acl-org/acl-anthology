<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.inlg">
  <volume id="main" ingest-date="2022-11-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 15th International Conference on Natural Language Generation</booktitle>
      <editor><first>Samira</first><last>Shaikh</last></editor>
      <editor><first>Thiago</first><last>Ferreira</last></editor>
      <editor><first>Amanda</first><last>Stent</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Waterville, Maine, USA and virtual meeting</address>
      <month>July</month>
      <year>2022</year>
      <url hash="784ed015">2022.inlg-main</url>
      <venue>inlg</venue>
    </meta>
    <frontmatter>
      <url hash="c454d918">2022.inlg-main.0</url>
      <bibkey>inlg-2022-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Evaluating Referring Form Selection Models in Partially-Known Environments</title>
      <author><first>Zhao</first><last>Han</last></author>
      <author><first>Polina</first><last>Rygina</last></author>
      <author><first>Thomas</first><last>Williams</last></author>
      <pages>1-14</pages>
      <abstract/>
      <url hash="0c98da54">2022.inlg-main.1</url>
      <attachment type="software" hash="bfbbdb14">2022.inlg-main.1.software.zip</attachment>
      <bibkey>han-etal-2022-evaluating</bibkey>
      <doi>10.18653/v1/2022.inlg-main.1</doi>
    </paper>
    <paper id="2">
      <title>Template-based Approach to Zero-shot Intent Recognition</title>
      <author><first>Dmitry</first><last>Lamanov</last></author>
      <author><first>Pavel</first><last>Burnyshev</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <author><first>Valentin</first><last>Malykh</last></author>
      <author><first>Andrey</first><last>Bout</last></author>
      <author><first>Irina</first><last>Piontkovskaya</last></author>
      <pages>15-28</pages>
      <abstract/>
      <url hash="7da089e9">2022.inlg-main.2</url>
      <attachment type="software" hash="0e11fdd8">2022.inlg-main.2.software.zip</attachment>
      <bibkey>lamanov-etal-2022-template</bibkey>
      <doi>10.18653/v1/2022.inlg-main.2</doi>
    </paper>
    <paper id="3">
      <title>“Slow Service” ↛ “Great Food”: Enhancing Content Preservation in Unsupervised Text Style Transfer</title>
      <author><first>Wanzheng</first><last>Zhu</last></author>
      <author><first>Suma</first><last>Bhat</last></author>
      <pages>29-39</pages>
      <abstract/>
      <url hash="0395840c">2022.inlg-main.3</url>
      <attachment type="software" hash="366788c0">2022.inlg-main.3.software.zip</attachment>
      <bibkey>zhu-bhat-2022-slow</bibkey>
      <doi>10.18653/v1/2022.inlg-main.3</doi>
    </paper>
    <paper id="4">
      <title><fixed-case>A</fixed-case>rabic Image Captioning using Pre-training of Deep Bidirectional Transformers</title>
      <author><first>Jonathan</first><last>Emami</last></author>
      <author><first>Pierre</first><last>Nugues</last></author>
      <author><first>Ashraf</first><last>Elnagar</last></author>
      <author><first>Imad</first><last>Afyouni</last></author>
      <pages>40-51</pages>
      <abstract/>
      <url hash="4c081471">2022.inlg-main.4</url>
      <attachment type="software" hash="108d5f4d">2022.inlg-main.4.software.zip</attachment>
      <bibkey>emami-etal-2022-arabic</bibkey>
      <doi>10.18653/v1/2022.inlg-main.4</doi>
    </paper>
    <paper id="5">
      <title>Plot Writing From Pre-Trained Language Models</title>
      <author><first>Yiping</first><last>Jin</last></author>
      <author><first>Vishakha</first><last>Kadam</last></author>
      <author><first>Dittaya</first><last>Wanvarie</last></author>
      <pages>52-67</pages>
      <abstract/>
      <url hash="09e08029">2022.inlg-main.5</url>
      <attachment type="software" hash="73d25213">2022.inlg-main.5.software.zip</attachment>
      <bibkey>jin-etal-2022-plot</bibkey>
      <doi>10.18653/v1/2022.inlg-main.5</doi>
    </paper>
    <paper id="6">
      <title>Paraphrasing via Ranking Many Candidates</title>
      <author><first>Joosung</first><last>Lee</last></author>
      <pages>68-72</pages>
      <abstract/>
      <url hash="e92b1c1d">2022.inlg-main.6</url>
      <attachment type="software" hash="d93c9651">2022.inlg-main.6.software.zip</attachment>
      <bibkey>lee-2022-paraphrasing</bibkey>
      <doi>10.18653/v1/2022.inlg-main.6</doi>
    </paper>
    <paper id="7">
      <title>Evaluating Legal Accuracy of Neural Generators on the Generation of Criminal Court Dockets Description</title>
      <author><first>Nicolas</first><last>Garneau</last></author>
      <author><first>Eve</first><last>Gaumond</last></author>
      <author><first>Luc</first><last>Lamontagne</last></author>
      <author><first>Pierre-Luc</first><last>Déziel</last></author>
      <pages>73-99</pages>
      <abstract/>
      <url hash="5f43f1cc">2022.inlg-main.7</url>
      <attachment type="software" hash="51b25aff">2022.inlg-main.7.software.zip</attachment>
      <bibkey>garneau-etal-2022-evaluating</bibkey>
      <doi>10.18653/v1/2022.inlg-main.7</doi>
    </paper>
    <paper id="8">
      <title>Automatic Generation of Factual News Headlines in <fixed-case>F</fixed-case>innish</title>
      <author><first>Maximilian</first><last>Koppatz</last></author>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <pages>100-109</pages>
      <abstract/>
      <url hash="c4a2b96f">2022.inlg-main.8</url>
      <attachment type="software" hash="a4fb9656">2022.inlg-main.8.software.zip</attachment>
      <bibkey>koppatz-etal-2022-automatic</bibkey>
      <doi>10.18653/v1/2022.inlg-main.8</doi>
    </paper>
    <paper id="9">
      <title>Generating Coherent and Informative Descriptions for Groups of Visual Objects and Categories: A Simple Decoding Approach</title>
      <author><first>Nazia</first><last>Attari</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <author><first>Martin</first><last>Heckmann</last></author>
      <author><first>Heiko</first><last>Wersing</last></author>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <pages>110-120</pages>
      <abstract/>
      <url hash="3d6ba171">2022.inlg-main.9</url>
      <attachment type="software" hash="a7ede148">2022.inlg-main.9.software.zip</attachment>
      <bibkey>attari-etal-2022-generating</bibkey>
      <doi>10.18653/v1/2022.inlg-main.9</doi>
    </paper>
    <paper id="10">
      <title>Dealing with hallucination and omission in neural Natural Language Generation: A use case on meteorology.</title>
      <author><first>Javier</first><last>González Corbelle</last></author>
      <author><first>Alberto</first><last>Bugarín-Diz</last></author>
      <author><first>Jose</first><last>Alonso-Moral</last></author>
      <author><first>Juan</first><last>Taboada</last></author>
      <pages>121-130</pages>
      <abstract/>
      <url hash="a4d3233d">2022.inlg-main.10</url>
      <attachment type="software" hash="95fe3dff">2022.inlg-main.10.software.zip</attachment>
      <bibkey>gonzalez-corbelle-etal-2022-dealing</bibkey>
      <doi>10.18653/v1/2022.inlg-main.10</doi>
    </paper>
    <paper id="11">
      <title>Amortized Noisy Channel Neural Machine Translation</title>
      <author><first>Richard Yuanzhe</first><last>Pang</last></author>
      <author><first>He</first><last>He</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <pages>131-143</pages>
      <abstract/>
      <url hash="d56e5f7d">2022.inlg-main.11</url>
      <attachment type="software" hash="55618b2a">2022.inlg-main.11.software.zip</attachment>
      <bibkey>pang-etal-2022-amortized</bibkey>
      <doi>10.18653/v1/2022.inlg-main.11</doi>
    </paper>
    <paper id="12">
      <title>Math Word Problem Generation with Multilingual Language Models</title>
      <author><first>Kashyapa</first><last>Niyarepola</last></author>
      <author><first>Dineth</first><last>Athapaththu</last></author>
      <author><first>Savindu</first><last>Ekanayake</last></author>
      <author><first>Surangika</first><last>Ranathunga</last></author>
      <pages>144-155</pages>
      <abstract/>
      <url hash="48a1fb78">2022.inlg-main.12</url>
      <attachment type="software" hash="37566ec6">2022.inlg-main.12.software.zip</attachment>
      <bibkey>niyarepola-etal-2022-math</bibkey>
      <doi>10.18653/v1/2022.inlg-main.12</doi>
    </paper>
    <paper id="13">
      <title>Comparing informativeness of an <fixed-case>NLG</fixed-case> chatbot vs graphical app in diet-information domain</title>
      <author><first>Simone</first><last>Balloccu</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <pages>156-185</pages>
      <abstract/>
      <url hash="f1197d0f">2022.inlg-main.13</url>
      <attachment type="software" hash="3145bf40">2022.inlg-main.13.software.zip</attachment>
      <bibkey>balloccu-reiter-2022-comparing</bibkey>
      <doi>10.18653/v1/2022.inlg-main.13</doi>
    </paper>
    <paper id="14">
      <title>Generation of Student Questions for Inquiry-based Learning</title>
      <author><first>Kevin</first><last>Ros</last></author>
      <author><first>Maxwell</first><last>Jong</last></author>
      <author><first>Chak Ho</first><last>Chan</last></author>
      <author><first>ChengXiang</first><last>Zhai</last></author>
      <pages>186-195</pages>
      <abstract/>
      <url hash="89d436f2">2022.inlg-main.14</url>
      <attachment type="software" hash="469e960a">2022.inlg-main.14.software.zip</attachment>
      <bibkey>ros-etal-2022-generation</bibkey>
      <doi>10.18653/v1/2022.inlg-main.14</doi>
    </paper>
    <paper id="15">
      <title>Keyword Provision Question Generation for Facilitating Educational Reading Comprehension Preparation</title>
      <author><first>Ying-Hong</first><last>Chan</last></author>
      <author><first>Ho-Lam</first><last>Chung</last></author>
      <author><first>Yao-Chung</first><last>Fan</last></author>
      <pages>196-202</pages>
      <abstract/>
      <url hash="1d091c19">2022.inlg-main.15</url>
      <attachment type="software" hash="3a0a7c97">2022.inlg-main.15.software.zip</attachment>
      <bibkey>chan-etal-2022-keyword</bibkey>
      <doi>10.18653/v1/2022.inlg-main.15</doi>
    </paper>
    <paper id="16">
      <title>Generating Landmark-based Manipulation Instructions from Image Pairs</title>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <author><first>Henrik</first><last>Voigt</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <author><first>Philipp</first><last>Sadler</last></author>
      <pages>203-211</pages>
      <abstract/>
      <url hash="7ab68662">2022.inlg-main.16</url>
      <attachment type="software" hash="3d1d44bb">2022.inlg-main.16.software.zip</attachment>
      <bibkey>zarriess-etal-2022-generating</bibkey>
      <doi>10.18653/v1/2022.inlg-main.16</doi>
    </paper>
    <paper id="17">
      <title>Zero-shot Cross-Linguistic Learning of Event Semantics</title>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <author><first>Thomas</first><last>Kober</last></author>
      <author><first>Bashar</first><last>Alhafni</last></author>
      <author><first>Yue</first><last>Chen</last></author>
      <author><first>Mert</first><last>Inan</last></author>
      <author><first>Elizabeth</first><last>Nielsen</last></author>
      <author><first>Shahab</first><last>Raji</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <author><first>Matthew</first><last>Stone</last></author>
      <pages>212-224</pages>
      <abstract/>
      <url hash="fba33e46">2022.inlg-main.17</url>
      <attachment type="software" hash="2f487f07">2022.inlg-main.17.software.zip</attachment>
      <bibkey>alikhani-etal-2022-zero</bibkey>
      <doi>10.18653/v1/2022.inlg-main.17</doi>
    </paper>
    <paper id="18">
      <title>Nominal Metaphor Generation with Multitask Learning</title>
      <author><first>Yucheng</first><last>Li</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Frank</first><last>Guerin</last></author>
      <pages>225-235</pages>
      <abstract/>
      <url hash="be82da68">2022.inlg-main.18</url>
      <attachment type="software" hash="aae2cc96">2022.inlg-main.18.software.zip</attachment>
      <bibkey>li-etal-2022-nominal</bibkey>
      <revision id="1" href="2022.inlg-main.18v1" hash="8444c67c"/>
      <revision id="2" href="2022.inlg-main.18v2" hash="be82da68" date="2022-11-29">Updated paper pdf due to openreview issue.</revision>
      <doi>10.18653/v1/2022.inlg-main.18</doi>
    </paper>
    <paper id="19">
      <title>Look and Answer the Question: On the Role of Vision in Embodied Question Answering</title>
      <author><first>Nikolai</first><last>Ilinykh</last></author>
      <author><first>Yasmeen</first><last>Emampoor</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>236-245</pages>
      <abstract/>
      <url hash="0860f8be">2022.inlg-main.19</url>
      <attachment type="software" hash="a402b5ab">2022.inlg-main.19.software.zip</attachment>
      <bibkey>ilinykh-etal-2022-look</bibkey>
      <doi>10.18653/v1/2022.inlg-main.19</doi>
    </paper>
    <paper id="20">
      <title>Strategies for framing argumentative conclusion generation</title>
      <author><first>Philipp</first><last>Heinisch</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <author><first>Juri</first><last>Opitz</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <pages>246-259</pages>
      <abstract/>
      <url hash="08fbab70">2022.inlg-main.20</url>
      <attachment type="software" hash="047dd236">2022.inlg-main.20.software.zip</attachment>
      <bibkey>heinisch-etal-2022-strategies</bibkey>
      <doi>10.18653/v1/2022.inlg-main.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>LAFT</fixed-case>: Cross-lingual Transfer for Text Generation by Language-Agnostic Finetuning</title>
      <author><first>Xianze</first><last>Wu</last></author>
      <author><first>Zaixiang</first><last>Zheng</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Yong</first><last>Yu</last></author>
      <pages>260-266</pages>
      <abstract/>
      <url hash="f6b8b91c">2022.inlg-main.21</url>
      <attachment type="software" hash="0dcdeb59">2022.inlg-main.21.software.zip</attachment>
      <bibkey>wu-etal-2022-laft</bibkey>
      <doi>10.18653/v1/2022.inlg-main.21</doi>
    </paper>
    <paper id="22">
      <title>Quantum Natural Language Generation on Near-Term Devices</title>
      <author><first>Amin</first><last>Karamlou</last></author>
      <author><first>James</first><last>Wootton</last></author>
      <author><first>Marcel</first><last>Pfaffhauser</last></author>
      <pages>267-277</pages>
      <abstract/>
      <url hash="50daf1ca">2022.inlg-main.22</url>
      <attachment type="software" hash="0eeb4efb">2022.inlg-main.22.software.zip</attachment>
      <bibkey>karamlou-etal-2022-quantum</bibkey>
      <doi>10.18653/v1/2022.inlg-main.22</doi>
    </paper>
    <paper id="23">
      <title>Towards Evaluation of Multi-party Dialogue Systems</title>
      <author><first>Khyati</first><last>Mahajan</last></author>
      <author><first>Sashank</first><last>Santhanam</last></author>
      <author><first>Samira</first><last>Shaikh</last></author>
      <pages>278-287</pages>
      <abstract/>
      <url hash="ace10072">2022.inlg-main.23</url>
      <attachment type="software" hash="7d8ec7f2">2022.inlg-main.23.software.zip</attachment>
      <bibkey>mahajan-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.inlg-main.23</doi>
    </paper>
    <paper id="24">
      <title>Are Current Decoding Strategies Capable of Facing the Challenges of Visual Dialogue?</title>
      <author><first>Amit Kumar</first><last>Chaudhary</last></author>
      <author><first>Alex J.</first><last>Lucassen</last></author>
      <author><first>Ioanna</first><last>Tsani</last></author>
      <author><first>Alberto</first><last>Testoni</last></author>
      <pages>288-297</pages>
      <abstract/>
      <url hash="8920ae30">2022.inlg-main.24</url>
      <attachment type="software" hash="fc61c1a2">2022.inlg-main.24.software.zip</attachment>
      <bibkey>chaudhary-etal-2022-current</bibkey>
      <doi>10.18653/v1/2022.inlg-main.24</doi>
    </paper>
    <paper id="25">
      <title>Analogy Generation by Prompting Large Language Models: A Case Study of <fixed-case>I</fixed-case>nstruct<fixed-case>GPT</fixed-case></title>
      <author><first>Bhavya</first><last>Bhavya</last></author>
      <author><first>Jinjun</first><last>Xiong</last></author>
      <author><first>ChengXiang</first><last>Zhai</last></author>
      <pages>298-312</pages>
      <abstract/>
      <url hash="1ead88db">2022.inlg-main.25</url>
      <bibkey>bhavya-etal-2022-analogy</bibkey>
      <doi>10.18653/v1/2022.inlg-main.25</doi>
    </paper>
  </volume>
  <volume id="demos" ingest-date="2022-11-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 15th International Conference on Natural Language Generation: System Demonstrations</booktitle>
      <editor><first>Samira</first><last>Shaikh</last></editor>
      <editor><first>Thiago</first><last>Ferreira</last></editor>
      <editor><first>Amanda</first><last>Stent</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Waterville, Maine, USA and virtual meeting</address>
      <month>July</month>
      <year>2022</year>
      <url hash="7ad46490">2022.inlg-demos</url>
      <venue>inlg</venue>
    </meta>
    <frontmatter>
      <url hash="a9f9243d">2022.inlg-demos.0</url>
      <bibkey>inlg-2022-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>BLAB</fixed-case> Reporter: Automated journalism covering the Blue <fixed-case>A</fixed-case>mazon</title>
      <author><first>Yan</first><last>Sym</last></author>
      <author><first>João</first><last>Campos</last></author>
      <author><first>Fabio</first><last>Cozman</last></author>
      <pages>1-3</pages>
      <abstract>This demo paper introduces BLAB reporter, a robot-journalist system covering the Brazilian Blue Amazon. The application is based on a pipeline architecture for Natural Language Generation, which offers daily reports, news summaries and curious facts in Brazilian Portuguese. By collecting, storing and analysing structured data from publicly available sources, the robot-journalist uses domain knowledge to generate, validate and publish texts in Twitter. Code and corpus are publicly available.</abstract>
      <url hash="f3979072">2022.inlg-demos.1</url>
      <attachment type="software" hash="e384dfc0">2022.inlg-demos.1.software.zip</attachment>
      <bibkey>sym-etal-2022-blab</bibkey>
    </paper>
    <paper id="2">
      <title>Generating Quizzes to Support Training on Quality Management and Assurance in Space Science and Engineering</title>
      <author><first>Andres</first><last>Garcia-Silva</last></author>
      <author><first>Cristian</first><last>Berrio Aroca</last></author>
      <author><first>Jose Manuel</first><last>Gomez-Perez</last></author>
      <author><first>Jose</first><last>Martinez</last></author>
      <author><first>Patrick</first><last>Fleith</last></author>
      <author><first>Stefano</first><last>Scaglioni</last></author>
      <pages>4-6</pages>
      <abstract>Quality management and assurance is key for space agencies to guarantee the success of space missions, which are high-risk and extremely costly. In this paper, we present a system to generate quizzes, a common resource to evaluate the effectiveness of training sessions, from documents about quality assurance procedures in the Space domain. Our system leverages state of the art auto-regressive models like T5 and BART to generate questions, and a RoBERTa model to extract answers for such questions, thus verifying their suitability.</abstract>
      <url hash="c452ae10">2022.inlg-demos.2</url>
      <attachment type="software" hash="2b0f132d">2022.inlg-demos.2.software.zip</attachment>
      <bibkey>garcia-silva-etal-2022-generating</bibkey>
    </paper>
    <paper id="3">
      <title>Automated Ad Creative Generation</title>
      <author><first>Vishakha</first><last>Kadam</last></author>
      <author><first>Yiping</first><last>Jin</last></author>
      <author><first>Bao-Dai</first><last>Nguyen-Hoang</last></author>
      <pages>7-9</pages>
      <abstract>Ad creatives are ads served to users on a webpage, app, or other digital environments. The demand for compelling ad creatives surges drastically with the ever-increasing popularity of digital marketing. The two most essential elements of (display) ad creatives are the advertising message, such as headlines and description texts, and the visual component, such as images and videos. Traditionally, ad creatives are composed by professional copywriters and creative designers. The process requires significant human effort, limiting the scalability and efficiency of digital ad campaigns. This work introduces AUTOCREATIVE, a novel system to automatically generate ad creatives relying on natural language generation and computer vision techniques. The system generates multiple ad copies (ad headlines/description texts) using a sequence-to-sequence model and selects images most suitable to the generated ad copies based on heuristic-based visual appeal metrics and a text-image retrieval pipeline.</abstract>
      <url hash="30ead3f5">2022.inlg-demos.3</url>
      <attachment type="software" hash="dc175510">2022.inlg-demos.3.software.zip</attachment>
      <bibkey>kadam-etal-2022-automated</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>THE</fixed-case>ai<fixed-case>TR</fixed-case>obot: An Interactive Tool for Generating Theatre Play Scripts</title>
      <author><first>Rudolf</first><last>Rosa</last></author>
      <author><first>Patrícia</first><last>Schmidtová</last></author>
      <author><first>Alisa</first><last>Zakhtarenko</last></author>
      <author><first>Ondrej</first><last>Dusek</last></author>
      <author><first>Tomáš</first><last>Musil</last></author>
      <author><first>David</first><last>Mareček</last></author>
      <author><first>Saad</first><last>Ul Islam</last></author>
      <author><first>Marie</first><last>Novakova</last></author>
      <author><first>Klara</first><last>Vosecka</last></author>
      <author><first>Daniel</first><last>Hrbek</last></author>
      <author><first>David</first><last>Kostak</last></author>
      <pages>10-13</pages>
      <abstract>We present a free online demo of THEaiTRobot, an open-source bilingual tool for interactively generating theatre play scripts, in two versions. THEaiTRobot 1.0 uses the GPT-2 language model with minimal adjustments. THEaiTRobot 2.0 uses two models created by fine-tuning GPT-2 on purposefully collected and processed datasets and several other components, generating play scripts in a hierarchical fashion (title <tex-math>\rightarrow</tex-math> synopsis <tex-math>\rightarrow</tex-math> script). The underlying tool is used in the THEaiTRE project to generate scripts for plays, which are then performed on stage by a professional theatre.</abstract>
      <url hash="d3731bd4">2022.inlg-demos.4</url>
      <attachment type="software" hash="698bc76d">2022.inlg-demos.4.software.zip</attachment>
      <bibkey>rosa-etal-2022-theaitrobot</bibkey>
    </paper>
  </volume>
  <volume id="genchal" ingest-date="2022-12-20" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 15th International Conference on Natural Language Generation: Generation Challenges</booktitle>
      <editor><first>Samira</first><last>Shaikh</last></editor>
      <editor><first>Thiago</first><last>Ferreira</last></editor>
      <editor><first>Amanda</first><last>Stent</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Waterville, Maine, USA and virtual meeting</address>
      <month>July</month>
      <year>2022</year>
      <venue>inlg</venue>
      <url hash="3a9013e9">2022.inlg-genchal</url>
    </meta>
    <frontmatter>
      <url hash="3a9013e9">2022.inlg-genchal.0</url>
      <bibkey>-2022-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The Second Automatic Minuting (<fixed-case>A</fixed-case>uto<fixed-case>M</fixed-case>in) Challenge: Generating and Evaluating Minutes from Multi-Party Meetings</title>
      <author><first>Tirthankar</first><last>Ghosal</last></author>
      <author><first>Marie</first><last>Hledíková</last></author>
      <author><first>Muskaan</first><last>Singh</last></author>
      <author><first>Anna</first><last>Nedoluzhko</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>1-11</pages>
      <abstract>We would host the AutoMin generation chal- lenge at INLG 2023 as a follow-up of the first AutoMin shared task at Interspeech 2021. Our shared task primarily concerns the automated generation of meeting minutes from multi-party meeting transcripts. In our first venture, we ob- served the difficulty of the task and highlighted a number of open problems for the community to discuss, attempt, and solve. Hence, we invite the Natural Language Generation (NLG) com- munity to take part in the second iteration of AutoMin. Like the first, the second AutoMin will feature both English and Czech meetings and the core task of summarizing the manually- revised transcripts into bulleted minutes. A new challenge we are introducing this year is to devise efficient metrics for evaluating the quality of minutes. We will also host an optional track to generate minutes for European parliamentary sessions. We carefully curated the datasets for the above tasks. Our ELITR Minuting Corpus has been recently accepted to LREC 2022 and publicly released. We are already preparing a new test set for evaluating the new shared tasks. We hope to carry forward the learning from the first AutoMin and instigate more community attention and interest in this timely yet chal- lenging problem. INLG, the premier forum for the NLG community, would be an appropriate venue to discuss the challenges and future of Automatic Minuting. The main objective of the AutoMin GenChal at INLG 2023 would be to come up with efficient methods to auto- matically generate meeting minutes and design evaluation metrics to measure the quality of the minutes.</abstract>
      <url hash="460f2228">2022.inlg-genchal.1</url>
      <bibkey>ghosal-etal-2022-second</bibkey>
    </paper>
    <paper id="2">
      <title>The Cross-lingual Conversation Summarization Challenge</title>
      <author><first>Yulong</first><last>Chen</last></author>
      <author><first>Ming</first><last>Zhong</last></author>
      <author><first>Xuefeng</first><last>Bai</last></author>
      <author><first>Naihao</first><last>Deng</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Xianchao</first><last>Zhu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>12-18</pages>
      <abstract>We propose the shared task of cross-lingual conversation summarization, ConvSumX Challenge, opening new avenues for researchers to investigate solutions that integrate conversation summarization and machine translation. This task can be particularly useful due to the emergence of online meetings and conferences. We use a new benchmark, covering 2 real-world scenarios and 3 language directions, including a low-resource language, for evaluation. We hope that ConvSumX can motivate research to go beyond English and break the barrier for non-English speakers to benefit from recent advances of conversation summarization.</abstract>
      <url hash="a2071f68">2022.inlg-genchal.2</url>
      <bibkey>chen-etal-2022-cross</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>H</fixed-case>inglish<fixed-case>E</fixed-case>val Generation Challenge on Quality Estimation of Synthetic Code-Mixed Text: Overview and Results</title>
      <author><first>Vivek</first><last>Srivastava</last></author>
      <author><first>Mayank</first><last>Singh</last></author>
      <pages>19-25</pages>
      <abstract>We hosted a shared task to investigate the factors influencing the quality of the code- mixed text generation systems. The teams experimented with two systems that gener- ate synthetic code-mixed Hinglish sentences. They also experimented with human ratings that evaluate the generation quality of the two systems. The first-of-its-kind, proposed sub- tasks, (i) quality rating prediction and (ii) an- notators’ disagreement prediction of the syn- thetic Hinglish dataset made the shared task quite popular among the multilingual research community. A total of 46 participants com- prising 23 teams from 18 institutions reg- istered for this shared task. The detailed description of the task and the leaderboard is available at <url>https://codalab.lisn.upsaclay.fr/competitions/1688</url>.</abstract>
      <url hash="3023a5bf">2022.inlg-genchal.3</url>
      <bibkey>srivastava-singh-2022-hinglisheval</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>P</fixed-case>re<fixed-case>C</fixed-case>og<fixed-case>IIITH</fixed-case> at <fixed-case>H</fixed-case>inglish<fixed-case>E</fixed-case>val : Leveraging Code-Mixing Metrics &amp; Language Model Embeddings To Estimate Code-Mix Quality</title>
      <author><first>Prashant</first><last>Kodali</last></author>
      <author><first>Tanmay</first><last>Sachan</last></author>
      <author><first>Akshay</first><last>Goindani</last></author>
      <author><first>Anmol</first><last>Goel</last></author>
      <author><first>Naman</first><last>Ahuja</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <pages>26-30</pages>
      <abstract>Code-Mixing is a phenomenon of mixing two or more languages in a speech event and is prevalent in multilingual societies. Given the low-resource nature of Code-Mixing, machine generation of code-mixed text is a prevalent approach for data augmentation. However, evaluating the quality of such machine gen- erated code-mixed text is an open problem. In our submission to HinglishEval, a shared- task collocated with INLG2022, we attempt to build models factors that impact the quality of synthetically generated code-mix text by pre- dicting ratings for code-mix quality. Hingli- shEval Shared Task consists of two sub-tasks - a) Quality rating prediction); b) Disagree- ment prediction. We leverage popular code- mixed metrics and embeddings of multilin- gual large language models (MLLMs) as fea- tures, and train task specific MLP regression models. Our approach could not beat the baseline results. However, for Subtask-A our team ranked a close second on F-1 and Co- hen’s Kappa Score measures and first for Mean Squared Error measure. For Subtask-B our ap- proach ranked third for F1 score, and first for Mean Squared Error measure. Code of our submission can be accessed here.</abstract>
      <url hash="596c6bad">2022.inlg-genchal.4</url>
      <bibkey>kodali-etal-2022-precogiiith</bibkey>
    </paper>
    <paper id="5">
      <title>niksss at <fixed-case>H</fixed-case>inglish<fixed-case>E</fixed-case>val: Language-agnostic <fixed-case>BERT</fixed-case>-based Contextual Embeddings with Catboost for Quality Evaluation of the Low-Resource Synthetically Generated Code-Mixed <fixed-case>H</fixed-case>inglish Text</title>
      <author><first>Nikhil</first><last>Singh</last></author>
      <pages>31-34</pages>
      <abstract>This paper describes the system description for the HinglishEval challenge at INLG 2022. The goal of this task was to investigate the factors influencing the quality of the code- mixed text generation system. The task was divided into two subtasks, quality rating prediction and annotators’ disagreement prediction of the synthetic Hinglish dataset. We attempted to solve these tasks using sentence-level embeddings, which are obtained from mean pooling the contextualized word embeddings for all input tokens in our text. We experimented with various classifiers on top of the embeddings produced for respective tasks. Our best-performing system ranked 1st on subtask B and 3rd on subtask A. We make our code available here: <url>https://github.com/nikhilbyte/Hinglish-qEval</url></abstract>
      <url hash="282566e2">2022.inlg-genchal.5</url>
      <bibkey>singh-2022-niksss-hinglisheval</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>BITS</fixed-case> Pilani at <fixed-case>H</fixed-case>inglish<fixed-case>E</fixed-case>val: Quality Evaluation for Code-Mixed <fixed-case>H</fixed-case>inglish Text Using Transformers</title>
      <author><first>Shaz</first><last>Furniturewala</last></author>
      <author><first>Vijay</first><last>Kumari</last></author>
      <author><first>Amulya Ratna</first><last>Dash</last></author>
      <author><first>Hriday</first><last>Kedia</last></author>
      <author><first>Yashvardhan</first><last>Sharma</last></author>
      <pages>35-38</pages>
      <abstract>Code-Mixed text data consists of sentences having words or phrases from more than one language. Most multi-lingual communities worldwide communicate using multiple languages, with English usually one of them. Hinglish is a Code-Mixed text composed of Hindi and English but written in Roman script. This paper aims to determine the factors influencing the quality of Code-Mixed text data generated by the system. For the HinglishEval task, the proposed model uses multilingual BERT to find the similarity between synthetically generated and human-generated sentences to predict the quality of synthetically generated Hinglish sentences.</abstract>
      <url hash="53daa48b">2022.inlg-genchal.6</url>
      <bibkey>furniturewala-etal-2022-bits</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>JU</fixed-case>_<fixed-case>NLP</fixed-case> at <fixed-case>H</fixed-case>inglish<fixed-case>E</fixed-case>val: Quality Evaluation of the Low-Resource Code-Mixed <fixed-case>H</fixed-case>inglish Text</title>
      <author><first>Prantik</first><last>Guha</last></author>
      <author><first>Rudra</first><last>Dhar</last></author>
      <author><first>Dipankar</first><last>Das</last></author>
      <pages>39-42</pages>
      <abstract>In this paper we describe a system submitted to the INLG 2022 Generation Challenge (GenChal) on Quality Evaluation of the Low-Resource Synthetically Generated Code-Mixed Hinglish Text. We implement a Bi-LSTM-based neural network model to predict the Average rating score and Disagreement score of the synthetic Hinglish dataset. In our models, we used word embeddings for English and Hindi data, and one hot encodings for Hinglish data. We achieved a F1 score of 0.11, and mean squared error of 6.0 in the average rating score prediction task. In the task of Disagreement score prediction, we achieve a F1 score of 0.18, and mean squared error of 5.0.</abstract>
      <url hash="93d56482">2022.inlg-genchal.7</url>
      <bibkey>guha-etal-2022-ju</bibkey>
    </paper>
    <paper id="8">
      <title>The 2022 <fixed-case>R</fixed-case>epro<fixed-case>G</fixed-case>en Shared Task on Reproducibility of Evaluations in <fixed-case>NLG</fixed-case>: Overview and Results</title>
      <author><first>Anya</first><last>Belz</last></author>
      <author><first>Anastasia</first><last>Shimorina</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <pages>43-51</pages>
      <abstract>Against a background of growing interest in reproducibility in NLP and ML, and as part of an ongoing research programme designed to develop theory and practice of reproducibility assessment in NLP, we organised the second shared task on reproducibility of evaluations in NLG, ReproGen 2022. This paper describes the shared task, summarises results from the reproduction studies submitted, and provides further comparative analysis of the results. Out of six initial team registrations, we received submissions from five teams. Meta-analysis of the five reproduction studies revealed varying degrees of reproducibility, and allowed further tentative conclusions about what types of evaluation tend to have better reproducibility.</abstract>
      <url hash="a2c42e90">2022.inlg-genchal.8</url>
      <bibkey>belz-etal-2022-2022</bibkey>
    </paper>
    <paper id="9">
      <title>Two Reproductions of a Human-Assessed Comparative Evaluation of a Semantic Error Detection System</title>
      <author><first>Rudali</first><last>Huidrom</last></author>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <author><first>Zdeněk</first><last>Kasner</last></author>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Anya</first><last>Belz</last></author>
      <pages>52-61</pages>
      <abstract>In this paper, we present the results of two reproduction studies for the human evaluation originally reported by Dušek and Kasner (2020) in which the authors comparatively evaluated outputs produced by a semantic error detection system for data-to-text generation against reference outputs. In the first reproduction, the original evaluators repeat the evaluation, in a test of the repeatability of the original evaluation. In the second study, two new evaluators carry out the evaluation task, in a test of the reproducibility of the original evaluation under otherwise identical conditions. We describe our approach to reproduction, and present and analyse results, finding different degrees of reproducibility depending on result type, data and labelling task. Our resources are available and open-sourced.</abstract>
      <url hash="e58575d6">2022.inlg-genchal.9</url>
      <bibkey>huidrom-etal-2022-two</bibkey>
    </paper>
    <paper id="10">
      <title>Reproducibility of Exploring Neural Text Simplification Models: A Review</title>
      <author><first>Mohammad</first><last>Arvan</last></author>
      <author><first>Luís</first><last>Pina</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>62-70</pages>
      <abstract>The reproducibility of NLP research has drawn increased attention over the last few years. Several tools, guidelines, and metrics have been introduced to address concerns in regard to this problem; however, much work still remains to ensure widespread adoption of effective reproducibility standards. In this work, we review the reproducibility of Exploring Neural Text Simplification Models by Nisioi et al. (2017), evaluating it from three main aspects: data, software artifacts, and automatic evaluations. We discuss the challenges and issues we faced during this process. Furthermore, we explore the adequacy of current reproducibility standards. Our code, trained models, and a docker container of the environment used for training and evaluation are made publicly available.</abstract>
      <url hash="f0101231">2022.inlg-genchal.10</url>
      <bibkey>arvan-etal-2022-reproducibility</bibkey>
    </paper>
    <paper id="11">
      <title>The Accuracy Evaluation Shared Task as a Retrospective Reproduction Study</title>
      <author><first>Craig</first><last>Thomson</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <pages>71-79</pages>
      <abstract>We investigate the data collected for the Accuracy Evaluation Shared Task as a retrospective reproduction study. The shared task was based upon errors found by human annotation of computer generated summaries of basketball games. Annotation was performed in three separate stages, with texts taken from the same three systems and checked for errors by the same three annotators. We show that the mean count of errors was consistent at the highest level for each experiment, with increased variance when looking at per-system and/or per-error- type breakdowns.</abstract>
      <url hash="ff3034b7">2022.inlg-genchal.11</url>
      <bibkey>thomson-reiter-2022-accuracy</bibkey>
    </paper>
    <paper id="12">
      <title>Reproducing a Manual Evaluation of the Simplicity of Text Simplification System Outputs</title>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Sheila</first><last>Castilho</last></author>
      <author><first>Rudali</first><last>Huidrom</last></author>
      <author><first>Anya</first><last>Belz</last></author>
      <pages>80-85</pages>
      <abstract>In this paper we describe our reproduction study of the human evaluation of text simplic- ity reported by Nisioi et al. (2017). The work was carried out as part of the ReproGen Shared Task 2022 on Reproducibility of Evaluations in NLG. Our aim was to repeat the evaluation of simplicity for nine automatic text simplification systems with a different set of evaluators. We describe our experimental design together with the known aspects of the original experimental design and present the results from both studies. Pearson correlation between the original and reproduction scores is moderate to high (0.776). Inter-annotator agreement in the reproduction study is lower (0.40) than in the original study (0.66). We discuss challenges arising from the unavailability of certain aspects of the origi- nal set-up, and make several suggestions as to how reproduction of similar evaluations can be made easier in future.</abstract>
      <url hash="bc07bde7">2022.inlg-genchal.12</url>
      <bibkey>popovic-etal-2022-reproducing</bibkey>
    </paper>
    <paper id="13">
      <title>A reproduction study of methods for evaluating dialogue system output: Replicating Santhanam and Shaikh (2019)</title>
      <author><first>Anouck</first><last>Braggaar</last></author>
      <author><first>Frédéric</first><last>Tomas</last></author>
      <author><first>Peter</first><last>Blomsma</last></author>
      <author><first>Saar</first><last>Hommes</last></author>
      <author><first>Nadine</first><last>Braun</last></author>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <author><first>Chris</first><last>van der Lee</last></author>
      <author><first>Martijn</first><last>Goudbeek</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <pages>86-93</pages>
      <abstract>In this paper, we describe our reproduction ef- fort of the paper: Towards Best Experiment Design for Evaluating Dialogue System Output by Santhanam and Shaikh (2019) for the 2022 ReproGen shared task. We aim to produce the same results, using different human evaluators, and a different implementation of the automatic metrics used in the original paper. Although overall the study posed some challenges to re- produce (e.g. difficulties with reproduction of automatic metrics and statistics), in the end we did find that the results generally replicate the findings of Santhanam and Shaikh (2019) and seem to follow similar trends.</abstract>
      <url hash="84e92f32">2022.inlg-genchal.13</url>
      <bibkey>braggaar-etal-2022-reproduction</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>D</fixed-case>ialog<fixed-case>S</fixed-case>um Challenge: Results of the Dialogue Summarization Shared Task</title>
      <author><first>Yulong</first><last>Chen</last></author>
      <author><first>Naihao</first><last>Deng</last></author>
      <author id="yang-liu-edinburgh"><first>Yang</first><last>Liu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>94-103</pages>
      <abstract>We report the results of DialogSum Challenge, the shared task on summarizing real-life sce- nario dialogues at INLG 2022. Four teams participate in this shared task and three submit their system reports, exploring different meth- ods to improve the performance of dialogue summarization. Although there is a great im- provement over the baseline models regarding automatic evaluation metrics, such as ROUGE scores, we find that there is a salient gap be- tween model generated outputs and human an- notated summaries by human evaluation from multiple aspects. These findings demonstrate the difficulty of dialogue summarization and suggest that more fine-grained evaluatuion met- rics are in need.</abstract>
      <url hash="14221668">2022.inlg-genchal.14</url>
      <bibkey>chen-etal-2022-dialogsum</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>TCS</fixed-case>_<fixed-case>WITM</fixed-case>_2022 @ <fixed-case>D</fixed-case>ialog<fixed-case>S</fixed-case>um : Topic oriented Summarization using Transformer based Encoder Decoder Model</title>
      <author><first>Vipul</first><last>Chauhan</last></author>
      <author><first>Prasenjeet</first><last>Roy</last></author>
      <author><first>Lipika</first><last>Dey</last></author>
      <author><first>Tushar</first><last>Goel</last></author>
      <pages>104-109</pages>
      <abstract>In this paper, we present our approach to the DialogSum challenge, which was proposed as a shared task aimed to summarize dialogues from real-life scenarios. The challenge was to design a system that can generate fluent and salient summaries of a multi-turn dialogue text. Dialogue summarization has many commercial applications as it can be used to summarize conversations between customers and service agents, meeting notes, conference proceedings etc. Appropriate dialogue summarization can enhance the experience of conversing with chat- bots or personal digital assistants. We have pro- posed a topic-based abstractive summarization method, which is generated by fine-tuning PE- GASUS1, which is the state of the art abstrac- tive summary generation model. We have com- pared different types of fine-tuning approaches that can lead to different types of summaries. We found that since conversations usually veer around a topic, using topics along with the di- aloagues, helps to generate more human-like summaries. The topics in this case resemble user perspective, around which summaries are usually sought. The generated summary has been evaluated with ground truth summaries provided by the challenge owners. We use the py-rouge score and BERT-Score metrics to compare the results.</abstract>
      <url hash="c984de0e">2022.inlg-genchal.15</url>
      <bibkey>chauhan-etal-2022-tcs</bibkey>
    </paper>
    <paper id="16">
      <title>A Multi-Task Learning Approach for Summarization of Dialogues</title>
      <author><first>Saprativa</first><last>Bhattacharjee</last></author>
      <author><first>Kartik</first><last>Shinde</last></author>
      <author><first>Tirthankar</first><last>Ghosal</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>110-120</pages>
      <abstract>We describe our multi-task learning based ap- proach for summarization of real-life dialogues as part of the DialogSum Challenge shared task at INLG 2022. Our approach intends to im- prove the main task of abstractive summariza- tion of dialogues through the auxiliary tasks of extractive summarization, novelty detection and language modeling. We conduct extensive experimentation with different combinations of tasks and compare the results. In addition, we also incorporate the topic information provided with the dataset to perform topic-aware sum- marization. We report the results of automatic evaluation of the generated summaries in terms of ROUGE and BERTScore.</abstract>
      <url hash="8598893e">2022.inlg-genchal.16</url>
      <bibkey>bhattacharjee-etal-2022-multi</bibkey>
    </paper>
    <paper id="17">
      <title>Dialogue Summarization using <fixed-case>BART</fixed-case></title>
      <author><first>Conrad</first><last>Lundberg</last></author>
      <author><first>Leyre</first><last>Sánchez Viñuela</last></author>
      <author><first>Siena</first><last>Biales</last></author>
      <pages>121-125</pages>
      <abstract>This paper introduces the model and settings submitted to the INLG 2022 DialogSum Chal- lenge, a shared task to generate summaries of real-life scenario dialogues between two peo- ple. In this paper, we explored using interme- diate task transfer learning, reported speech, and the use of a supplementary dataset in addi- tion to our base fine-tuned BART model. How- ever, we did not use such a method in our final model, as none improved our results. Our final model for this dialogue task achieved scores only slightly below the top submission, with hidden test set scores of 49.62, 24.98, 46.25 and 91.54 for ROUGE-1, ROUGE-2, ROUGE-L and BERTSCORE respectively. The top submitted models will also receive human evaluation.</abstract>
      <url hash="829913f9">2022.inlg-genchal.17</url>
      <bibkey>lundberg-etal-2022-dialogue</bibkey>
    </paper>
  </volume>
  <event id="inlg-2022">
    <colocated>
      <volume-id>2022.nlg4health-1</volume-id>
    </colocated>
  </event>
</collection>
