<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.cawl">
  <volume id="1" ingest-date="2024-05-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Computation and Written Language (CAWL) @ LREC-COLING 2024</booktitle>
      <editor><first>Kyle</first><last>Gorman</last></editor>
      <editor><first>Emily</first><last>Prud'hommeaux</last></editor>
      <editor><first>Brian</first><last>Roark</last></editor>
      <editor><first>Richard</first><last>Sproat</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="11e8e3dc">2024.cawl-1</url>
      <venue>cawl</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="8b70f576">2024.cawl-1.0</url>
      <bibkey>cawl-2024-computation</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>P</fixed-case>ars<fixed-case>T</fixed-case>ext: A Digraphic Corpus for <fixed-case>T</fixed-case>ajik-<fixed-case>F</fixed-case>arsi Transliteration</title>
      <author><first>Rayyan</first><last>Merchant</last></author>
      <author><first>Kevin</first><last>Tang</last></author>
      <pages>1–7</pages>
      <abstract>Despite speaking dialects of the same language, Persian speakers from Tajikistan cannot read Persian texts from Iran and Afghanistan. This is due to the fact that Tajik Persian is written in the Tajik-Cyrillic script, while Iranian and Afghan Persian are written in the Perso-Arabic script. As the formal registers of these dialects all maintain high levels of mutual intelligibility with each other, machine transliteration has been proposed as a more practical and appropriate solution than machine translation. Unfortunately, Persian texts written in both scripts are much more common in print in Tajikistan than online. This paper introduces a novel corpus meant to remedy that gap: ParsText. ParsText contains 2,813 Persian sentences written in both Tajik-Cyrillic and Perso-Arabic manually collected from blog pages and news articles online. This paper presents the need for such a corpus, previous and related work, data collection and alignment procedures, corpus statistics, and discusses directions for future work.</abstract>
      <url hash="83587211">2024.cawl-1.1</url>
      <bibkey>merchant-tang-2024-parstext</bibkey>
    </paper>
    <paper id="2">
      <title>A Joint Approach for Automatic Analysis of Reading and Writing Errors</title>
      <author><first>Wieke</first><last>Harmsen</last></author>
      <author><first>Catia</first><last>Cucchiarini</last></author>
      <author><first>Roeland</first><last>van Hout</last></author>
      <author><first>Helmer</first><last>Strik</last></author>
      <pages>8–17</pages>
      <abstract>Analyzing the errors that children make on their ways to becoming fluent readers and writers can provide invaluable scientific insights into the processes that underlie literacy acquisition. To this end, we present in this paper an extension of an earlier developed spelling error detection and classification algorithm for Dutch, so that reading errors can also be automatically detected from their phonetic transcription. The strength of this algorithm lies in its ability to detect errors at Phoneme-Corresponding Unit (PCU) level, where a PCU is a sequence of letters corresponding to one phoneme. We validated this algorithm and found good agreement between manual and automatic reading error classifications. We also used the algorithm to analyze written words by second graders and phonetic transcriptions of read words by first graders. With respect to the writing data, we found that the PCUs ‘ei’, ‘eu’, ‘g’, ‘ij’ and ‘ch’ were most frequently written incorrectly, for the reading data, these were the PCUs ‘v’, ‘ui’, ‘ng’, ‘a’ and ‘g’. This study presents a first attempt at developing a joint method for detecting reading and writing errors. In future research this algorithm can be used to analyze corpora containing reading and writing data from the same children.</abstract>
      <url hash="d818197a">2024.cawl-1.2</url>
      <attachment type="OptionalSupplementaryMaterial" hash="548d87c8">2024.cawl-1.2.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>harmsen-etal-2024-joint</bibkey>
    </paper>
    <paper id="3">
      <title>Tool for Constructing a Large-Scale Corpus of Code Comments and Other Source Code Annotations</title>
      <author><first>Luna</first><last>Peck</last></author>
      <author><first>Susan</first><last>Brown</last></author>
      <pages>18–22</pages>
      <abstract>The sublanguage of source code annotations—explanatory natural language writing that accompanies programming source code—is little-studied in linguistics. To facilitate research into this domain, we have developed a program prototype that can extract code comments and changelogs (i.e. commit messages) from public, open-source code repositories, with automatic tokenization and part-of-speech tagging on the extracted text. The program can also automatically detect and discard “commented-out” source code in data from Python repositories, to prevent it from polluting the corpus, demonstrating that such sanitization is likely feasible for other programming languages as well. With the current tool, we have produced a 6-million word corpus of English-language comments extracted from three different programming languages: Python, C, and C++.</abstract>
      <url hash="f22cebf9">2024.cawl-1.3</url>
      <bibkey>peck-brown-2024-tool</bibkey>
    </paper>
    <paper id="4">
      <title>Tokenization via Language Modeling: the Role of Preceding Text</title>
      <author><first>Rastislav</first><last>Hronsky</last></author>
      <author><first>Emmanuel</first><last>Keuleers</last></author>
      <pages>23–35</pages>
      <abstract>While language models benefit immensely from their capacity to model large context (i.e., sequence of preceding tokens), the role of context is unclear in text tokenization, which is, in many cases, language model-driven to begin with. In this paper, we attempt to explore the role in three different writing systems and using three different text tokenization strategies (word-based, Morfessor, and BPE). In the first experiment, we examined how the size of context used for predicting the next token affects the ranking of the segmentation strategies i.t.o. language model surprisal. This effect was very writing system specific: minimal in case of English, and rank-reversing due to increased context size and token granularity in case of Turkish and Chinese. In the second experiment, we examined how context alters segmentation hypotheses when using language models to identify word boundaries. In this case, the effect was subtle: using context-aware, rather than context-free segment scores improved boundary recognition accuracy by up to 0.5%, once baseline effects were exploited.</abstract>
      <url hash="eed0710b">2024.cawl-1.4</url>
      <bibkey>hronsky-keuleers-2024-tokenization</bibkey>
    </paper>
    <paper id="5">
      <title>Abbreviation Across the World’s Languages and Scripts</title>
      <author><first>Kyle</first><last>Gorman</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <pages>36–42</pages>
      <abstract>Detailed taxonomies for non-standard words, including abbreviations, have been developed for speech and language processing, though mostly with reference to English. In this paper, we examine abbreviation formation strategies in a diverse sample of more than 50 languages, dialects and scripts. The resulting taxonomy—and data about which strategies are attested in which languages—provides key information needed to create multilingual systems for abbreviation expansion, an essential component for speech processing and text understanding</abstract>
      <url hash="1e5d9c77">2024.cawl-1.5</url>
      <bibkey>gorman-roark-2024-abbreviation</bibkey>
    </paper>
    <paper id="6">
      <title>Now You See Me, Now You Don’t: ‘Poverty of the Stimulus’ Problems and Arbitrary Correspondences in End-to-End Speech Models</title>
      <author><first>Daan</first><last>van Esch</last></author>
      <pages>43–52</pages>
      <abstract>End-to-end models for speech recognition and speech synthesis have many benefits, but we argue they also face a unique set of challenges not encountered in conventional multi-stage hybrid systems, which relied on the explicit injection of linguistic knowledge through resources such as phonemic dictionaries and verbalization grammars. These challenges include handling words with unusual grapheme-to-phoneme correspondences, converting between written forms like ‘12’ and spoken forms such as ‘twelve’, and contextual disambiguation o homophones or homographs. We describe the mitigation strategies that have been used for these problems in end-to-end systems, either implicitly or explicitly, and call out that the most commonly used mitigation techniques are likely incompatible with newly emerging approaches that use minimal amounts of supervised audio training data. We review best-of-both-world approaches that allow the use of end-to-end models combined with traditional linguistic resources, which we show are increasingly straightforward to create at scale, and close with an optimistic outlook for bringing speech technologies to many more languages by combining these strands of research.</abstract>
      <url hash="18452f49">2024.cawl-1.6</url>
      <bibkey>van-esch-2024-now</bibkey>
    </paper>
    <paper id="7">
      <title>Towards Fast Cognate Alignment on Imbalanced Data</title>
      <author><first>Logan</first><last>Born</last></author>
      <author><first>M. Willis</first><last>Monroe</last></author>
      <author><first>Kathryn</first><last>Kelley</last></author>
      <author><first>Anoop</first><last>Sarkar</last></author>
      <pages>53–58</pages>
      <abstract>Cognate alignment models purport to enable decipherment, but their speed and need for clean data can make them unsuitable for realistic decipherment problems. We seek to draw attention to these shortcomings in the hopes that future work may avoid them, and we outline two techniques which begin to overcome the described problems.</abstract>
      <url hash="534284a7">2024.cawl-1.7</url>
      <bibkey>born-etal-2024-towards</bibkey>
    </paper>
    <paper id="8">
      <title>Simplified <fixed-case>C</fixed-case>hinese Character Distance Based on Ideographic Description Sequences</title>
      <author><first>Yixia</first><last>Wang</last></author>
      <author><first>Emmanuel</first><last>Keuleers</last></author>
      <pages>59–66</pages>
      <abstract>Character encoding systems have long overlooked the internal structure of characters. Ideographic Description Sequences, which explicitly represent spatial relations between character components, are a potential solution to this problem. In this paper, we illustrate the utility of Ideographic Description Sequences in computing edit distance and finding orthographic neighbors for Simplified Chinese characters. In addition, we explore the possibility of using Ideographic Description Sequences to encode spatial relations between components in other scripts.</abstract>
      <url hash="fc34088b">2024.cawl-1.8</url>
      <bibkey>wang-keuleers-2024-simplified</bibkey>
    </paper>
  </volume>
</collection>
