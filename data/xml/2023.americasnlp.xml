<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.americasnlp">
  <volume id="1" ingest-date="2023-07-12" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)</booktitle>
      <editor><first>Manuel</first><last>Mager</last></editor>
      <editor><first>Abteen</first><last>Ebrahimi</last></editor>
      <editor><first>Arturo</first><last>Oncevay</last></editor>
      <editor><first>Enora</first><last>Rice</last></editor>
      <editor><first>Shruti</first><last>Rijhwani</last></editor>
      <editor><first>Alexis</first><last>Palmer</last></editor>
      <editor><first>Katharina</first><last>Kann</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="5b003f99">2023.americasnlp-1</url>
      <venue>americasnlp</venue>
    </meta>
    <frontmatter>
      <url hash="09460534">2023.americasnlp-1.0</url>
      <bibkey>americasnlp-2023-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Use of <fixed-case>NLP</fixed-case> in the Context of Belief states of Ethnic Minorities in <fixed-case>L</fixed-case>atin <fixed-case>A</fixed-case>merica</title>
      <author><first>Olga</first><last>Kellert</last><affiliation>University of Gttingen</affiliation></author>
      <author><first>Mahmud</first><last>Zaman</last><affiliation>University of Gttingen</affiliation></author>
      <pages>1-5</pages>
      <abstract>The major goal of our study is to test methods in NLP in the domain of health care education related to Covid-19 of vulnerable groups such as indigenous people from Latin America. In order to achieve this goal, we asked participants in a survey questionnaire to provide answers about health related topics. We used these answers to measure the health education status ofour participants. In this paper, we summarize the results from our NLP-application on the participants’ answers. In the first experiment, we use embeddings-based tools to measure the semantic similarity between participants’ answers and “expert” or “reference” answers. In the second experiment, we use synonym-based methods to classify answers under topics. We compare the results from both experiments with human annotations. Our results show that the tested NLP-methods reach a significantly lower accuracy score than human annotations in both experiments. We explain this difference by the assumption that human annotators are much better in pragmatic inferencing necessary to classify the semantic similarity and topic classification of answers.</abstract>
      <url hash="58b13d1a">2023.americasnlp-1.1</url>
      <bibkey>kellert-zaman-2023-use</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Neural Machine Translation through Active Learning on low-resource languages: The case of <fixed-case>S</fixed-case>panish to <fixed-case>M</fixed-case>apudungun</title>
      <author><first>Begoña</first><last>Pendas</last><affiliation>Pontificia Universidad Catolica de Chile</affiliation></author>
      <author><first>Andres</first><last>Carvallo</last><affiliation>CENIA</affiliation></author>
      <author><first>Carlos</first><last>Aspillaga</last><affiliation>CENIA</affiliation></author>
      <pages>6-11</pages>
      <abstract>Active learning is an algorithmic approach that strategically selects a subset of examples for labeling, with the goal of reducing workload and required resources. Previous research has applied active learning to Neural Machine Translation (NMT) for high-resource or well-represented languages, achieving significant reductions in manual labor. In this study, we explore the application of active learning for NMT in the context of Mapudungun, a low-resource language spoken by the Mapuche community in South America. Mapudungun was chosen due to the limited number of fluent speakers and the pressing need to provide access to content predominantly available in widely represented languages. We assess both model-dependent and model-agnostic active learning strategies for NMT between Spanish and Mapudungun in both directions, demonstrating that we can achieve over 40% reduction in manual translation workload in both cases.</abstract>
      <url hash="36ad9279">2023.americasnlp-1.2</url>
      <bibkey>pendas-etal-2023-neural</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.2</doi>
    </paper>
    <paper id="3">
      <title>Understanding Native Language Identification for <fixed-case>B</fixed-case>razilian Indigenous Languages</title>
      <author><first>Paulo</first><last>Cavalin</last><affiliation>IBM Research - Brazil</affiliation></author>
      <author><first>Pedro</first><last>Domingues</last><affiliation>IBM Research Brazil</affiliation></author>
      <author><first>Julio</first><last>Nogima</last><affiliation>IBM Research - Brazil</affiliation></author>
      <author><first>Claudio</first><last>Pinhanez</last><affiliation>IBM Research</affiliation></author>
      <pages>12-18</pages>
      <abstract>We investigate native language identification (LangID) for Brazilian Indigenous Languages (BILs), using the Bible as training data. Our research extends from previous work, by presenting two analyses on the generalization of Bible-based LangID in non-biblical data. First, with newly collected non-biblical datasets, we show that such a LangID can still provide quite reasonable accuracy in languages for which there are more established writing standards, such as Guarani Mbya and Kaigang, but there can be a quite drastic drop in accuracy depending on the language. Then, we applied the LangID on a large set of texts, about 13M sentences from the Portuguese Wikipedia, towards understanding the difficulty factors may come out of such task in practice. The main outcome is that the lack of handling other American indigenous languages can affect considerably the precision for BILs, suggesting the need of a joint effort with related languages from the Americas.</abstract>
      <url hash="11d70989">2023.americasnlp-1.3</url>
      <bibkey>cavalin-etal-2023-understanding</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Codex to corpus: Exploring annotation and processing for an open and extensible machine-readable edition of the Florentine Codex</title>
      <author><first>Francis</first><last>Tyers</last><affiliation>Indiana University</affiliation></author>
      <author><first>Robert</first><last>Pugh</last><affiliation>Indiana University</affiliation></author>
      <author><first>Valery</first><last>Berthoud F.</last><affiliation>Humboldt-Universitt zu Berlin</affiliation></author>
      <pages>19-29</pages>
      <abstract>This paper describes an ongoing effort to create, from the original hand-written text, a machine-readable, linguistically-annotated, and easily-searchable corpus of the Nahuatl portion of the Florentine Codex, a 16th century Mesoamerican manuscript written in Nahuatl and Spanish. The Codex consists of 12 books and over 300,000 tokens. We describe the process of annotating 3 of these books, the steps of text preprocessing undertaken, our approach to efficient manual processing and annotation, and some of the challenges faced along the way. We also report on a set of experiments evaluating our ability to automate the text processing tasks to aid in the remaining annotation effort, and find the results promising despite the relatively low volume of training data. Finally, we briefly present a real use case from the humanities that would benefit from the searchable, linguistically annotated corpus we describe.</abstract>
      <url hash="61d0c697">2023.americasnlp-1.4</url>
      <bibkey>tyers-etal-2023-codex</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.4</doi>
    </paper>
    <paper id="5">
      <title>Developing finite-state language technology for <fixed-case>M</fixed-case>aya</title>
      <author><first>Robert</first><last>Pugh</last><affiliation>Indiana University</affiliation></author>
      <author><first>Francis</first><last>Tyers</last><affiliation>Indiana University</affiliation></author>
      <author><first>Quetzil</first><last>Castañeda</last><affiliation>Indiana University</affiliation></author>
      <pages>30-39</pages>
      <abstract>We describe a suite of finite-state language technologies for Maya, a Mayan language spoken in Mexico. At the core is a computational model of Maya morphology and phonology using a finite-state transducer. This model results in a morphological analyzer and a morphologically-informed spell-checker. All of these technologies are designed for use as both a pedagogical reading/writing aid for L2 learners and as a general language processing tool capable of supporting much of the natural variation in written Maya. We discuss the relevant features of Maya morphosyntax and orthography, and then outline the implementation details of the analyzer. To conclude, we present a longer-term vision for these tools and their use by both native speakers and learners.</abstract>
      <url hash="e93af2ce">2023.americasnlp-1.5</url>
      <bibkey>pugh-etal-2023-developing</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Modelling the Reduplicating <fixed-case>L</fixed-case>ushootseed Morphology with an <fixed-case>FST</fixed-case> and <fixed-case>LSTM</fixed-case></title>
      <author><first>Jack</first><last>Rueter</last><affiliation>University of Helsinki, Digital Humanities</affiliation></author>
      <author><first>Mika</first><last>Hämäläinen</last><affiliation>Rootroo Ltd</affiliation></author>
      <author><first>Khalid</first><last>Alnajjar</last><affiliation>University of Helsinki</affiliation></author>
      <pages>40-46</pages>
      <abstract>In this paper, we present an FST based approach for conducting morphological analysis, lemmatization and generation of Lushootseed words. Furthermore, we use the FST to generate training data for an LSTM based neural model and train this model to do morphological analysis. The neural model reaches a 71.9% accuracy on the test data. Furthermore, we discuss reduplication types in the Lushootseed language forms. The approach involves the use of both attested instances of reduplication and bare stems for applying a variety of reduplications to, as it is unclear just how much variation can be attributed to the individual speakers and authors of the source materials. That is, there may be areal factors that can be aligned with certain types of reduplication and their frequencies.</abstract>
      <url hash="00bdb295">2023.americasnlp-1.6</url>
      <bibkey>rueter-etal-2023-modelling</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Fine-tuning Sentence-<fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a to Construct Word Embeddings for Low-resource Languages from Bilingual Dictionaries</title>
      <author><first>Diego</first><last>Bear</last><affiliation>University of New Brunswick</affiliation></author>
      <author><first>Paul</first><last>Cook</last><affiliation>University of New Brunswick</affiliation></author>
      <pages>47-57</pages>
      <abstract>Conventional approaches to learning word embeddings (Mikolov et al., 2013; Pennington et al., 2014) are limited to relatively few languages with sufficiently large training corpora. To address this limitation, we propose an alternative approach to deriving word embeddings for Wolastoqey and Mi’kmaq that leverages definitions from a bilingual dictionary. More specifically, following Bear and Cook (2022), we experiment with encoding English definitions of Wolastoqey and Mi’kmaq words into vector representations using English sequence representation models. For this, we consider using and finetuning sentence-RoBERTa models (Reimers and Gurevych, 2019). We evaluate our word embeddings using a similar methodology to that of Bear and Cook using evaluations based on word classification, clustering and reverse dictionary search. We additionally construct word embeddings for higher-resource languages English, German and Spanishusing our methods and evaluate our embeddings on existing word-similarity datasets. Our findings indicate that our word embedding methods can be used to produce meaningful vector representations for low-resource languages such as Wolastoqey and Mi’kmaq and for higher-resource languages.</abstract>
      <url hash="412950de">2023.americasnlp-1.7</url>
      <bibkey>bear-cook-2023-fine</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Identification of Dialect for Eastern and <fixed-case>S</fixed-case>outhwestern <fixed-case>O</fixed-case>jibwe Words Using a Small Corpus</title>
      <author><first>Kalvin</first><last>Hartwig</last><affiliation>Unaffiliated</affiliation></author>
      <author><first>Evan</first><last>Lucas</last><affiliation>Michigan Technological University</affiliation></author>
      <author><first>Timothy</first><last>Havens</last><affiliation>Michigan Technological University</affiliation></author>
      <pages>58-66</pages>
      <abstract>The Ojibwe language has several dialects that vary to some degree in both spoken and written form. We present a method of using support vector machines to classify two different dialects (Eastern and Southwestern Ojibwe) using a very small corpus of text. Classification accuracy at the sentence level is 90% across a five-fold cross validation and 72% when the sentence-trained model is applied to a data set of individual words. Our code and the word level data set are released openly on Github at [link to be inserted for final version, working demonstration notebook uploaded with paper].</abstract>
      <url hash="0b27cefb">2023.americasnlp-1.8</url>
      <bibkey>hartwig-etal-2023-identification</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.8</doi>
    </paper>
    <paper id="9">
      <title>Enriching <fixed-case>W</fixed-case>ayúunaiki-<fixed-case>S</fixed-case>panish Neural Machine Translation with Linguistic Information</title>
      <author><first>Nora</first><last>Graichen</last><affiliation>UdS</affiliation></author>
      <author><first>Josef</first><last>Van Genabith</last><affiliation>DFKI</affiliation></author>
      <author><first>Cristina</first><last>España-bonet</last><affiliation>DFKI GmbH</affiliation></author>
      <pages>67-83</pages>
      <abstract>We present the first neural machine translation system for the low-resource language pair Wayúunaiki–Spanish and explore strategies to inject linguistic knowledge into the model to improve translation quality. We explore a wide range of methods and combine complementary approaches. Results indicate that incorporating linguistic information through linguistically motivated subword segmentation, factored models, and pretrained embeddings helps the system to generate improved translations, with the segmentation contributing most. In order to evaluate translation quality in a general domain and go beyond the available religious domain data, we gather and make publicly available a new test set and supplementary material. Although translation quality as measured with automatic metrics is low, we hope these resources will facilitate and support further research on Wayúunaiki.</abstract>
      <url hash="1889dd25">2023.americasnlp-1.9</url>
      <bibkey>graichen-etal-2023-enriching</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.9</doi>
    </paper>
    <paper id="10">
      <title>Towards the First Named Entity Recognition of <fixed-case>I</fixed-case>nuktitut for an Improved Machine Translation</title>
      <author><first>Ngoc Tan</first><last>Le</last><affiliation>Universite du Quebec a Montreal</affiliation></author>
      <author><first>Soumia</first><last>Kasdi</last><affiliation>Universite du Quebec a Montreal</affiliation></author>
      <author><first>Fatiha</first><last>Sadat</last><affiliation>UQAM</affiliation></author>
      <pages>84-93</pages>
      <abstract>Named Entity Recognition is a crucial step to ensure good quality performance of several Natural Language Processing applications and tools, including machine translation and information retrieval. Moreover, it is considered as a fundamental module of many Natural Language Understanding tasks such as question-answering systems. This paper presents a first study on NER for an under-represented Indigenous Inuit language of Canada, Inuktitut, which lacks linguistic resources and large labeled data. Our proposed NER model for Inuktitut is built by transferring linguistic characteristics from English to Inuktitut, based on either rules or bilingual word embeddings. We provide an empirical study based on a comparison with the state of the art models and as well as intrinsic and extrinsic evaluations. In terms of Recall, Precision and F-score, the obtained results show the effectiveness of the proposed NER methods. Furthermore, it improved the performance of Inuktitut-English Neural Machine Translation.</abstract>
      <url hash="b8695299">2023.americasnlp-1.10</url>
      <bibkey>le-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.10</doi>
    </paper>
    <paper id="11">
      <title>Parallel Corpus for Indigenous Language Translation: <fixed-case>S</fixed-case>panish-Mazatec and <fixed-case>S</fixed-case>panish-<fixed-case>M</fixed-case>ixtec</title>
      <author><first>Atnafu Lambebo</first><last>Tonja</last><affiliation>Instituto Politcnico Nacional (IPN), Centro de Investigacin en Computacin (CIC)</affiliation></author>
      <author><first>Christian</first><last>Maldonado-sifuentes</last><affiliation>TRAI-L .com</affiliation></author>
      <author><first>David Alejandro</first><last>Mendoza Castillo</last><affiliation>TRAI-L.com</affiliation></author>
      <author><first>Olga</first><last>Kolesnikova</last><affiliation>Instituto Politecnico Nacional</affiliation></author>
      <author><first>Noé</first><last>Castro-Sánchez</last><affiliation>TecNM/Cenidet</affiliation></author>
      <author><first>Grigori</first><last>Sidorov</last><affiliation>CIC-IPN</affiliation></author>
      <author><first>Alexander</first><last>Gelbukh</last><affiliation>Instituto Politcnico Nacional</affiliation></author>
      <pages>94-102</pages>
      <abstract>In this paper, we present a parallel Spanish- Mazatec and Spanish-Mixtec corpus for machine translation (MT) tasks, where Mazatec and Mixtec are two indigenous Mexican languages. We evaluated the usability of the collected corpus using three different approaches: transformer, transfer learning, and fine-tuning pre-trained multilingual MT models. Fine-tuning the Facebook m2m100-48 model outperformed the other approaches, with BLEU scores of 12.09 and 22.25 for Mazatec-Spanish and Spanish-Mazatec translations, respectively, and 16.75 and 22.15 for Mixtec-Spanish and Spanish-Mixtec translations, respectively. The results indicate that translation performance is influenced by the dataset size (9,799 sentences in Mazatec and 13,235 sentences in Mixtec) and is more effective when indigenous languages are used as target languages. The findings emphasize the importance of creating parallel corpora for indigenous languages and fine-tuning models for low-resource translation tasks. Future research will investigate zero-shot and few-shot learning approaches to further improve translation performance in low-resource settings.</abstract>
      <url hash="23179653">2023.americasnlp-1.11</url>
      <bibkey>tonja-etal-2023-parallel</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>A finite-state morphological analyser for <fixed-case>H</fixed-case>ighland <fixed-case>P</fixed-case>uebla <fixed-case>N</fixed-case>ahuatl</title>
      <author><first>Robert</first><last>Pugh</last><affiliation>Indiana University</affiliation></author>
      <author><first>Francis</first><last>Tyers</last><affiliation>Indiana University</affiliation></author>
      <pages>103-108</pages>
      <abstract>This paper describes the development of a free/open-source finite-state morphologicaltransducer for Highland Puebla Nahuatl, a Uto-Aztecan language spoken in and around the stateof Puebla in Mexico. The finite-state toolkit used for the work is the Helsinki Finite-StateToolkit (HFST); we use the lexc formalism for modelling the morphotactics and twol formal-ism for modelling morphophonological alternations. An evaluation is presented which showsthat the transducer has a reasonable coveragearound 90%on freely-available corpora of the language, and high precisionover 95%on a manually verified test set</abstract>
      <url hash="7b34f300">2023.americasnlp-1.12</url>
      <bibkey>pugh-tyers-2023-finite</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Neural Machine Translation for the Indigenous Languages of the <fixed-case>A</fixed-case>mericas: An Introduction</title>
      <author><first>Manuel</first><last>Mager</last><affiliation>Amazon AWS</affiliation></author>
      <author><first>Rajat</first><last>Bhatnagar</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Ngoc Thang</first><last>Vu</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Katharina</first><last>Kann</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>109-133</pages>
      <abstract>Neural models have drastically advanced state of the art for machine translation (MT) between high-resource languages. Traditionally, these models rely on large amounts of training data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of parallel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open questions, product of an increased interest of the NLP community in these languages.</abstract>
      <url hash="eb6b5d3d">2023.americasnlp-1.13</url>
      <bibkey>mager-etal-2023-neural</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.13</doi>
    </paper>
    <paper id="14">
      <title>Community consultation and the development of an online Akuzipik-<fixed-case>E</fixed-case>nglish dictionary</title>
      <author><first>Benjamin</first><last>Hunt</last><affiliation>George Mason University</affiliation></author>
      <author><first>Lane</first><last>Schwartz</last><affiliation>University of Alaska Fairbanks</affiliation></author>
      <author><first>Sylvia</first><last>Schreiner</last><affiliation>George Mason University</affiliation></author>
      <author><first>Emily</first><last>Chen</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>134-143</pages>
      <abstract>In this paper, we present a new online dictionary of Akuzipik, an Indigenous language of St. Lawrence Island (Alaska) and Chukotka (Russia).We discuss community desires for strengthening language use in the community and in educational settings, and present specific features of an online dictionary designed to serve these community goals.</abstract>
      <url hash="e9dd2c88">2023.americasnlp-1.14</url>
      <bibkey>hunt-etal-2023-community</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Finding words that aren’t there: Using word embeddings to improve dictionary search for low-resource languages</title>
      <author><first>Antti</first><last>Arppe</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Andrew</first><last>Neitsch</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Daniel</first><last>Dacanay</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Jolene</first><last>Poulin</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Daniel</first><last>Hieber</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Atticus</first><last>Harrigan</last><affiliation>University of Alberta</affiliation></author>
      <pages>144-155</pages>
      <abstract>Modern machine learning techniques have produced many impressive results in language technology, but these techniques generally require an amount of training data that is many orders of magnitude greater than what exists for low-resource languages in general, and endangered ones in particular. However, dictionary definitions in a comparatively much more well-resourced majority language can provide a link between low-resource languages and machine learning models trained on massive amounts of majority-language data. By leveraging a pre-trained English word embedding to compute sentence embeddings for definitions in bilingual dictionaries for four Indigenous languages spoken in North America, Plains Cree (nhiyawwin), Arapaho (Hinno’itit), Northern Haida (Xaad Kl), and Tsuut’ina (Tst’n), we have obtained promising results for dictionary search. Not only are the search results in the majority language of the definitions more relevant, but they can be semantically relevant in ways not achievable with classic information retrieval techniques: users can perform successful searches for words that do not occur at all in the dictionary. These techniques are directly applicable to any bilingual dictionary providing translations between a high- and low-resource language.</abstract>
      <url hash="ed4e596a">2023.americasnlp-1.15</url>
      <bibkey>arppe-etal-2023-finding</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Enhancing <fixed-case>S</fixed-case>panish-<fixed-case>Q</fixed-case>uechua Machine Translation with Pre-Trained Models and Diverse Data Sources: <fixed-case>LCT</fixed-case>-<fixed-case>EHU</fixed-case> at <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> Shared Task</title>
      <author><first>Nouman</first><last>Ahmed</last><affiliation>University of the Basque Country</affiliation></author>
      <author><first>Natalia</first><last>Flechas Manrique</last><affiliation>University of the Basque Country</affiliation></author>
      <author><first>Antonije</first><last>Petrović</last><affiliation>University of the Basque Country</affiliation></author>
      <pages>156-162</pages>
      <abstract>We present the LCT-EHU submission to the AmericasNLP 2023 low-resource machine translation shared task. We focus on the Spanish-Quechua language pair and explore the usage of different approaches: (1) Obtain new parallel corpora from the literature and legal domains, (2) Compare a high-resource Spanish-English pre-trained MT model with a Spanish-Finnish pre-trained model (with Finnish being chosen as a target language due to its morphological similarity to Quechua), and (3) Explore additional techniques such as copied corpus and back-translation. Overall, we show that the Spanish-Finnish pre-trained model outperforms other setups, while low-quality synthetic data reduces the performance.</abstract>
      <url hash="c319b076">2023.americasnlp-1.16</url>
      <bibkey>ahmed-etal-2023-enhancing</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> is not a good indigenous translator</title>
      <author><first>David</first><last>Stap</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Ali</first><last>Araabi</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>163-167</pages>
      <abstract>This report investigates the continuous challenges of Machine Translation (MT) systems on indigenous and extremely low-resource language pairs. Despite the notable achievements of Large Language Models (LLMs) that excel in various tasks, their applicability to low-resource languages remains questionable. In this study, we leveraged the AmericasNLP competition to evaluate the translation performance of different systems for Spanish to 11 indigenous languages from South America. Our team, LTLAmsterdam, submitted a total of four systems including GPT-4, a bilingual model, fine-tuned M2M100, and a combination of fine-tuned M2M100 with $k$NN-MT. We found that even large language models like GPT-4 are not well-suited for extremely low-resource languages. Our results suggest that fine-tuning M2M100 models can offer significantly better performance for extremely low-resource translation.</abstract>
      <url hash="29f87327">2023.americasnlp-1.17</url>
      <bibkey>stap-araabi-2023-chatgpt</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.17</doi>
    </paper>
    <paper id="18">
      <title>Few-shot <fixed-case>S</fixed-case>panish-<fixed-case>A</fixed-case>ymara Machine Translation Using <fixed-case>E</fixed-case>nglish-<fixed-case>A</fixed-case>ymara Lexicon</title>
      <author><first>Nat</first><last>Gillin</last></author>
      <author><first>Brian</first><last>Gummibaerhausen</last></author>
      <pages>168-172</pages>
      <url hash="a6d82c1e">2023.americasnlp-1.18</url>
      <abstract>This paper presents the experiments to train a Spanish-Aymara machine translation model for the AmericasNLP 2023 Machine Translation shared task. We included the English-Aymara GlobalVoices corpus and an English-Aymara lexicon to train the model and limit our training resources to train the model in a \textit{few-shot} manner.</abstract>
      <bibkey>tan-2023-shot</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>P</fixed-case>lay<fixed-case>G</fixed-case>round Low Resource Machine Translation System for the 2023 <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> Shared Task</title>
      <author><first>Tianrui</first><last>Gu</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Kaie</first><last>Chen</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Siqi</first><last>Ouyang</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>University of California Santa Barbara</affiliation></author>
      <pages>173-176</pages>
      <abstract>This paper presents PlayGround’s submission to the AmericasNLP 2023 shared task on machine translation (MT) into indigenous languages. We finetuned NLLB-600M, a multilingual MT model pre-trained on Flores-200, on 10 low-resource language directions and examined the effectiveness of weight averaging and back translation. Our experiments showed that weight averaging, on average, led to a 0.0169 improvement in the ChrF++ score. Additionally, we found that back translation resulted in a 0.008 improvement in the ChrF++ score.</abstract>
      <url hash="70c4244c">2023.americasnlp-1.19</url>
      <bibkey>gu-etal-2023-playground</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.19</doi>
    </paper>
    <paper id="20">
      <title>Four Approaches to Low-Resource Multilingual <fixed-case>NMT</fixed-case>: The <fixed-case>H</fixed-case>elsinki Submission to the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2023 Shared Task</title>
      <author><first>Ona</first><last>De Gibert</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Raúl</first><last>Vázquez</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Mikko</first><last>Aulamo</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Yves</first><last>Scherrer</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Sami</first><last>Virpioja</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Jörg</first><last>Tiedemann</last><affiliation>University of Helsinki</affiliation></author>
      <pages>177-191</pages>
      <abstract>The Helsinki-NLP team participated in the AmericasNLP 2023 Shared Task with 6 submissions for all 11 language pairs arising from 4 different multilingual systems. We provide a detailed look at the work that went into collecting and preprocessing the data that led to our submissions. We explore various setups for multilingual Neural Machine Translation (NMT), namely knowledge distillation and transfer learning, multilingual NMT including a high-resource language (English), language-specific fine-tuning, and multilingual NMT exclusively using low-resource data. Our multilingual Model B ranks first in 4 out of the 11 language pairs.</abstract>
      <url hash="0400493b">2023.americasnlp-1.20</url>
      <bibkey>de-gibert-etal-2023-four</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>S</fixed-case>heffield’s Submission to the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> Shared Task on Machine Translation into Indigenous Languages</title>
      <author><first>Edward</first><last>Gow-Smith</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Danae</first><last>Sánchez Villegas</last><affiliation>University of Sheffield</affiliation></author>
      <pages>192-199</pages>
      <abstract>The University of Sheffield took part in the shared task 2023 AmericasNLP for all eleven language pairs. Our models consist of training different variations of NLLB-200 model on data provided by the organizers and available data from various sources such as constitutions, handbooks and news articles. Our models outperform the baseline model on the development set on chrF with substantial improvements particularly for Aymara, Guarani and Quechua. On the test set, our best submission achieves the highest average chrF of all the submissions, we rank first in four of the eleven languages, and at least one of our models ranks in the top 3 for all languages.</abstract>
      <url hash="6ab71941">2023.americasnlp-1.21</url>
      <bibkey>gow-smith-snchez-villegas-2023-sheffields</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.21</doi>
    </paper>
    <paper id="22">
      <title>Enhancing Translation for Indigenous Languages: Experiments with Multilingual Models</title>
      <author><first>Atnafu Lambebo</first><last>Tonja</last><affiliation>Instituto Politcnico Nacional (IPN), Centro de Investigacin en Computacin (CIC)</affiliation></author>
      <author><first>Hellina Hailu</first><last>Nigatu</last><affiliation>UC Berkeley</affiliation></author>
      <author><first>Olga</first><last>Kolesnikova</last><affiliation>Instituto Politecnico Nacional</affiliation></author>
      <author><first>Grigori</first><last>Sidorov</last><affiliation>CIC-IPN</affiliation></author>
      <author><first>Alexander</first><last>Gelbukh</last><affiliation>Instituto Politcnico Nacional</affiliation></author>
      <author><first>Jugal</first><last>Kalita</last><affiliation>University of Colorado</affiliation></author>
      <pages>200-205</pages>
      <abstract>This paper describes CIC NLP’s submission to the AmericasNLP 2023 Shared Task on machine translation systems for indigenous languages of the Americas. We present the system descriptions for three methods. We used two multilingual models, namely M2M-100 and mBART50, and one bilingual (one-to-one) — Helsinki NLP Spanish-English translation model, and experimented with different transfer learning setups. We experimented with 11 languages from America and report the setups we used as well as the results we achieved. Overall, the mBART setup was able to improve upon the baseline for three out of the eleven languages.</abstract>
      <url hash="43f2bd8d">2023.americasnlp-1.22</url>
      <bibkey>tonja-etal-2023-enhancing</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.22</doi>
    </paper>
    <paper id="23">
      <title>Findings of the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2023 Shared Task on Machine Translation into Indigenous Languages</title>
      <author><first>Abteen</first><last>Ebrahimi</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Manuel</first><last>Mager</last><affiliation>Amazon AWS</affiliation></author>
      <author><first>Shruti</first><last>Rijhwani</last><affiliation>Google</affiliation></author>
      <author><first>Enora</first><last>Rice</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Arturo</first><last>Oncevay</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Claudia</first><last>Baltazar</last><affiliation/></author>
      <author><first>María</first><last>Cortés</last><affiliation/></author>
      <author><first>Cynthia</first><last>Montaño</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>John E.</first><last>Ortega</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Rolando</first><last>Coto-solano</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Hilaria</first><last>Cruz</last><affiliation>University of Louisville</affiliation></author>
      <author><first>Alexis</first><last>Palmer</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Katharina</first><last>Kann</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>206-219</pages>
      <abstract>In this work, we present the results of the AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages of the Americas. This edition of the shared task featured eleven language pairs, one of which – Chatino-Spanish – uses a newly collected evaluation dataset, consisting of professionally translated text from the legal domain. Seven teams participated in the shared task, with a total of 181 submissions. Additionally, we conduct a human evaluation of the best system outputs, and compare them to the best submissions from the prior shared task. We find that this analysis agrees with the quantitative measures used to rank submissions, which shows further improvements of 9.64 ChrF on average across all languages, when compared to the prior winning system.</abstract>
      <url hash="946c2904">2023.americasnlp-1.23</url>
      <bibkey>ebrahimi-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.americasnlp-1.23</doi>
    </paper>
  </volume>
</collection>
