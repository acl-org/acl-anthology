<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.ranlp">
  <volume id="1" ingest-date="2023-10-31" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing</booktitle>
      <editor><first>Ruslan</first><last>Mitkov</last></editor>
      <editor><first>Galia</first><last>Angelova</last></editor>
      <publisher>INCOMA Ltd., Shoumen, Bulgaria</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2023</year>
      <url hash="81592834">2023.ranlp-1</url>
      <venue>ranlp</venue>
    </meta>
    <frontmatter>
      <url hash="7a7921b8">2023.ranlp-1.0</url>
      <bibkey>ranlp-2023-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Bipol: Multi-Axes Evaluation of Bias with Explainability in Benchmark Datasets</title>
      <author><first>Tosin</first><last>Adewumi</last></author>
      <author><first>Isabella</first><last>Södergren</last></author>
      <author><first>Lama</first><last>Alkhaled</last></author>
      <author><first>Sana</first><last>Al-azzawi</last></author>
      <author><first>Foteini</first><last>Simistira Liwicki</last></author>
      <author><first>Marcus</first><last>Liwicki</last></author>
      <pages>1–10</pages>
      <abstract>We investigate five English NLP benchmark datasets (on the superGLUE leaderboard) and two Swedish datasets for bias, along multiple axes. The datasets are the following: Boolean Question (Boolq), CommitmentBank (CB), Winograd Schema Challenge (WSC), Winogender diagnostic (AXg), Recognising Textual Entailment (RTE), Swedish CB, and SWEDN. Bias can be harmful and it is known to be common in data, which ML models learn from. In order to mitigate bias in data, it is crucial to be able to estimate it objectively. We use bipol, a novel multi-axes bias metric with explainability, to estimate and explain how much bias exists in these datasets. Multilingual, multi-axes bias evaluation is not very common. Hence, we also contribute a new, large Swedish bias-labelled dataset (of 2 million samples), translated from the English version and train the SotA mT5 model on it. In addition, we contribute new multi-axes lexica for bias detection in Swedish. We make the codes, model, and new dataset publicly available.</abstract>
      <url hash="cd89695c">2023.ranlp-1.1</url>
      <bibkey>adewumi-etal-2023-bipol</bibkey>
    </paper>
    <paper id="2">
      <title>Automatically Generating <fixed-case>H</fixed-case>indi <fixed-case>W</fixed-case>ikipedia Pages Using <fixed-case>W</fixed-case>ikidata as a Knowledge Graph: A Domain-Specific Template Sentences Approach</title>
      <author><first>Aditya</first><last>Agarwal</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>11–21</pages>
      <abstract>This paper presents a method for automatically generating Wikipedia articles in the Hindi language, using Wikidata as a knowledge base. Our method extracts structured information from Wikidata, such as the names of entities, their properties, and their relationships, and then uses this information to generate natural language text that conforms to a set of templates designed for the domain of interest. We evaluate our method by generating articles about scientists, and we compare the resulting articles to machine-translated articles. Our results show that more than 70% of the generated articles using our method are better in terms of coherence, structure, and readability. Our approach has the potential to significantly reduce the time and effort required to create Wikipedia articles in Hindi and could be extended to other languages and domains as well.</abstract>
      <url hash="aa2d0ead">2023.ranlp-1.2</url>
      <bibkey>agarwal-mamidi-2023-automatically</bibkey>
    </paper>
    <paper id="3">
      <title>Cross-lingual Classification of Crisis-related Tweets Using Machine Translation</title>
      <author><first>Shareefa</first><last>Al Amer</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <author><first>Phillip</first><last>Smith</last></author>
      <pages>22–31</pages>
      <abstract>Utilisation of multilingual language models such as mBERT and XLM-RoBERTa has increasingly gained attention in recent work by exploiting the multilingualism of such models in different downstream tasks across different languages. However, performance degradation is expected in transfer learning across languages compared to monolingual performance although it is an acceptable trade-off considering the sparsity of resources and lack of available training data in low-resource languages. In this work, we study the effect of machine translation on the cross-lingual transfer learning in a crisis event classification task. Our experiments include measuring the effect of machine-translating the target data into the source language and vice versa. We evaluated and compared the performance in terms of accuracy and F1-Score. The results show that translating the source data into the target language improves the prediction accuracy by 14.8% and the Weighted Average F1-Score by 19.2% when compared to zero-shot transfer to an unseen language.</abstract>
      <url hash="000c9d33">2023.ranlp-1.3</url>
      <bibkey>al-amer-etal-2023-cross</bibkey>
    </paper>
    <paper id="4">
      <title>Lexicon-Driven Automatic Sentence Generation for the Skills Section in a Job Posting</title>
      <author><first>Vera</first><last>Aleksic</last></author>
      <author><first>Mona</first><last>Brems</last></author>
      <author><first>Anna</first><last>Mathes</last></author>
      <author><first>Theresa</first><last>Bertele</last></author>
      <pages>32–40</pages>
      <abstract>This paper presents a sentence generation pipeline as implemented on the online job board Stepstone. The goal is to automatically create a set of sentences for the candidate profile and the task description sections in a job ad, related to a given input skill. They must cover two different “tone of voice” variants in German (Du, Sie), three experience levels (junior, mid, senior), and two optionality values (skill is mandatory or optional/nice to have). The generation process considers the difference between soft skills, natural language competencies and hard skills, as well as more specific sub-categories such as IT skills, programming languages and similar. To create grammatically consistent text, morphosyntactic features from the proprietary skill ontology and lexicon are consulted. The approach is a lexicon-driven generation process that compares all lexical features of the new input skills with the ones already added to the sentence database and creates new sentences according to the corresponding templates.</abstract>
      <url hash="8d1d9e4c">2023.ranlp-1.4</url>
      <bibkey>aleksic-etal-2023-lexicon</bibkey>
    </paper>
    <paper id="5">
      <title>Multilingual Racial Hate Speech Detection Using Transfer Learning</title>
      <author><first>Abinew Ali</first><last>Ayele</last></author>
      <author><first>Skadi</first><last>Dinter</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>41–48</pages>
      <abstract>The rise of social media eases the spread of hateful content, especially racist content with severe consequences. In this paper, we analyze the tweets targeting the death of George Floyd in May 2020 as the event accelerated debates on racism globally. We focus on the tweets published in French for a period of one month since the death of Floyd. Using the Yandex Toloka platform, we annotate the tweets into categories as hate, offensive or normal. Tweets that are offensive or hateful are further annotated as racial or non-racial. We build French hate speech detection models based on the multilingual BERT and CamemBERT and apply transfer learning by fine-tuning the HateXplain model. We compare different approaches to resolve annotation ties and find that the detection model based on CamemBERT yields the best results in our experiments.</abstract>
      <url hash="563c2466">2023.ranlp-1.5</url>
      <bibkey>ayele-etal-2023-multilingual</bibkey>
    </paper>
    <paper id="6">
      <title>Exploring <fixed-case>A</fixed-case>mharic Hate Speech Data Collection and Classification Approaches</title>
      <author><first>Abinew Ali</first><last>Ayele</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last></author>
      <author><first>Tadesse Destaw</first><last>Belay</last></author>
      <author><first>Tesfa</first><last>Asfaw</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>49–59</pages>
      <abstract>In this paper, we present a study of efficient data selection and annotation strategies for Amharic hate speech. We also build various classification models and investigate the challenges of hate speech data selection, annotation, and classification for the Amharic language. From a total of over 18 million tweets in our Twitter corpus, 15.1k tweets are annotated by two independent native speakers, and a Cohen’s kappa score of 0.48 is achieved. A third annotator, a curator, is also employed to decide on the final gold labels. We employ both classical machine learning and deep learning approaches, which include fine-tuning AmFLAIR and AmRoBERTa contextual embedding models. Among all the models, AmFLAIR achieves the best performance with an F1-score of 72%. We publicly release the annotation guidelines, keywords/lexicon entries, datasets, models, and associated scripts with a permissive license.</abstract>
      <url hash="af030c59">2023.ranlp-1.6</url>
      <bibkey>ayele-etal-2023-exploring</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>B</fixed-case>hojpuri <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et: Problems in Translating <fixed-case>H</fixed-case>indi Synsets into <fixed-case>B</fixed-case>hojpuri</title>
      <author><first>Imran</first><last>Ali</last></author>
      <author><first>Praveen</first><last>Gatla</last></author>
      <pages>60–68</pages>
      <abstract>Today, artificial intelligence systems are incredibly intelligent, however they lack the human like capacity for understanding. In this context, sense-based lexical resources become a requirement for artificially intelligent machines. Lexical resources like Wordnets have received scholarly attention because they are considered as the crucial sense-based resources in the field of natural language understanding. They can help in knowing the intended meaning of the communicated texts, as they are focused on the concept rather than the words. Wordnets are available only for 18 Indian languages. Keeping this in mind, we have initiated the development of a comprehensive wordnet for Bhojpuri. The present paper describes the creation of the synsets of Bhojpuri and discusses the problems that we faced while translating Hindi synsets into Bhojpuri. They are lexical anomalies, lexical mismatch words, synthesized forms, lack of technical words etc. Nearly 4000 Hindi synsets were mapped for their equivalent synsets in Bhojpuri following the expansion approach. We have also worked on the language-specific synsets, which are unique to Bhojpuri. This resource is useful in machine translation, sentiment analysis, word sense disambiguation, cross-lingual references among Indian languages, and Bhojpuri language teaching and learning.</abstract>
      <url hash="4d7f9e1d">2023.ranlp-1.7</url>
      <bibkey>ali-gatla-2023-bhojpuri</bibkey>
    </paper>
    <paper id="8">
      <title>3<fixed-case>D</fixed-case>-<fixed-case>EX</fixed-case>: A Unified Dataset of Definitions and Dictionary Examples</title>
      <author><first>Fatemah</first><last>Almeman</last></author>
      <author><first>Hadi</first><last>Sheikhi</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <pages>69–79</pages>
      <abstract>Definitions are a fundamental building block in lexicography, linguistics and computational semantics. In NLP, they have been used for retrofitting word embeddings or augmenting contextual representations in language models. However, lexical resources containing definitions exhibit a wide range of properties, which has implications in the behaviour of models trained and evaluated on them. In this paper, we introduce 3D-EX, a dataset that aims to fill this gap by combining well-known English resources into one centralized knowledge repository in the form of &lt;term, definition, example&gt; triples. 3D-EX is a unified evaluation framework with carefully pre-computed train/validation/test splits to prevent memorization. We report experimental results that suggest that this dataset could be effectively leveraged in downstream NLP tasks. Code and data are available at https://github.com/F-Almeman/3D-EX.</abstract>
      <url hash="456bb028">2023.ranlp-1.8</url>
      <bibkey>almeman-etal-2023-3d</bibkey>
    </paper>
    <paper id="9">
      <title>Are You Not moved? Incorporating Sensorimotor Knowledge to Improve Metaphor Detection</title>
      <author><first>Ghadi</first><last>Alnafesah</last></author>
      <author><first>Phillip</first><last>Smith</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>80–89</pages>
      <abstract>Metaphors use words from one domain of knowledge to describe another, which can make the meaning less clear and require human interpretation to understand. This makes it difficult for automated models to detect metaphorical usage. The objective of the experiments in the paper is to enhance the ability of deep learning models to detect metaphors automatically. This is achieved by using two elements of semantic richness, sensory experience, and body-object interaction, as the main lexical features, combined with the contextual information present in the metaphorical sentences. The tests were conducted using classification and sequence labeling models for metaphor detection on the three metaphorical corpora VUAMC, MOH-X, and TroFi. The sensory experience led to significant improvements in the classification and sequence labelling models across all datasets. The highest gains were seen on the VUAMC dataset: recall increased by 20.9%, F1 by 7.5% for the classification model, and Recall increased by 11.66% and F1 by 3.69% for the sequence labelling model. Body-object interaction also showed positive impact on the three datasets.</abstract>
      <url hash="4b40e8cc">2023.ranlp-1.9</url>
      <bibkey>alnafesah-etal-2023-moved</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>HAQA</fixed-case> and <fixed-case>QUQA</fixed-case>: Constructing Two <fixed-case>A</fixed-case>rabic Question-Answering Corpora for the <fixed-case>Q</fixed-case>uran and <fixed-case>H</fixed-case>adith</title>
      <author><first>Sarah</first><last>Alnefaie</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <author><first>Mohammad Ammar</first><last>Alsalka</last></author>
      <pages>90–97</pages>
      <abstract>It is neither possible nor fair to compare the performance of question-answering systems for the Holy Quran and Hadith Sharif in Arabic due to both the absence of a golden test dataset on the Hadith Sharif and the small size and easy questions of the newly created golden test dataset on the Holy Quran. This article presents two question–answer datasets: Hadith Question–Answer pairs (HAQA) and Quran Question–Answer pairs (QUQA). HAQA is the first Arabic Hadith question–answer dataset available to the research community, while the QUQA dataset is regarded as the more challenging and the most extensive collection of Arabic question–answer pairs on the Quran. HAQA was designed and its data collected from several expert sources, while QUQA went through several steps in the construction phase; that is, it was designed and then integrated with existing datasets in different formats, after which the datasets were enlarged with the addition of new data from books by experts. The HAQA corpus consists of 1598 question–answer pairs, and that of QUQA contains 3382. They may be useful as gold–standard datasets for the evaluation process, as training datasets for language models with question-answering tasks and for other uses in artificial intelligence.</abstract>
      <url hash="489dfa0f">2023.ranlp-1.10</url>
      <bibkey>alnefaie-etal-2023-haqa</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>C</fixed-case>onfli<fixed-case>BERT</fixed-case>-<fixed-case>A</fixed-case>rabic: A Pre-trained <fixed-case>A</fixed-case>rabic Language Model for Politics, Conflicts and Violence</title>
      <author><first>Sultan</first><last>Alsarra</last></author>
      <author><first>Luay</first><last>Abdeljaber</last></author>
      <author><first>Wooseong</first><last>Yang</last></author>
      <author><first>Niamat</first><last>Zawad</last></author>
      <author><first>Latifur</first><last>Khan</last></author>
      <author><first>Patrick</first><last>Brandt</last></author>
      <author><first>Javier</first><last>Osorio</last></author>
      <author><first>Vito</first><last>D’Orazio</last></author>
      <pages>98–108</pages>
      <abstract>This study investigates the use of Natural Language Processing (NLP) methods to analyze politics, conflicts and violence in the Middle East using domain-specific pre-trained language models. We introduce Arabic text and present ConfliBERT-Arabic, a pre-trained language models that can efficiently analyze political, conflict and violence-related texts. Our technique hones a pre-trained model using a corpus of Arabic texts about regional politics and conflicts. Performance of our models is compared to baseline BERT models. Our findings show that the performance of NLP models for Middle Eastern politics and conflict analysis are enhanced by the use of domain-specific pre-trained local language models. This study offers political and conflict analysts, including policymakers, scholars, and practitioners new approaches and tools for deciphering the intricate dynamics of local politics and conflicts directly in Arabic.</abstract>
      <url hash="4567df76">2023.ranlp-1.11</url>
      <bibkey>alsarra-etal-2023-conflibert</bibkey>
    </paper>
    <paper id="12">
      <title>A Review in Knowledge Extraction from Knowledge Bases</title>
      <author><first>Fabio</first><last>Yanez</last></author>
      <author><first>Andrés</first><last>Montoyo</last></author>
      <author><first>Yoan</first><last>Gutierrez</last></author>
      <author><first>Rafael</first><last>Muñoz</last></author>
      <author><first>Armando</first><last>Suarez</last></author>
      <pages>109–116</pages>
      <abstract>Generative language models achieve the state of the art in many tasks within natural language processing (NLP). Although these models correctly capture syntactic information, they fail to interpret knowledge (semantics). Moreover, the lack of interpretability of these models promotes the use of other technologies as a replacement or complement to generative language models. This is the case with research focused on incorporating knowledge by resorting to knowledge bases mainly in the form of graphs. The generation of large knowledge graphs is carried out with unsupervised or semi-supervised techniques, which promotes the validation of this knowledge with the same type of techniques due to the size of the generated databases. In this review, we will explain the different techniques used to test and infer knowledge from graph structures with machine learning algorithms. The motivation of validating and inferring knowledge is to use correct knowledge in subsequent tasks with improved embeddings.</abstract>
      <url hash="6ad00a1d">2023.ranlp-1.12</url>
      <bibkey>yanez-etal-2023-review</bibkey>
    </paper>
    <paper id="13">
      <title>Evaluating of Large Language Models in Relationship Extraction from Unstructured Data: Empirical Study from Holocaust Testimonies</title>
      <author><first>Isuri</first><last>Anuradha</last></author>
      <author><first>Le An</first><last>Ha</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <author><first>Vinita</first><last>Nahar</last></author>
      <pages>117–123</pages>
      <abstract>Relationship extraction from unstructured data remains one of the most challenging tasks in the field of Natural Language Processing (NLP). The complexity of relationship extraction arises from the need to comprehend the underlying semantics, syntactic structures, and contextual dependencies within the text. Unstructured data poses challenges with diverse linguistic patterns, implicit relationships, contextual nuances, complicating accurate relationship identification and extraction. The emergence of Large Language Models (LLMs), such as GPT (Generative Pre-trained Transformer), has indeed marked a significant advancement in the field of NLP. In this work, we assess and evaluate the effectiveness of LLMs in relationship extraction in the Holocaust testimonies within the context of the Historical realm. By delving into this domain-specific context, we aim to gain deeper insights into the performance and capabilities of LLMs in accurately capturing and extracting relationships within the Holocaust domain by developing a novel knowledge graph to visualise the relationships of the Holocaust. To the best of our knowledge, there is no existing study which discusses relationship extraction in Holocaust testimonies. The majority of current approaches for Information Extraction (IE) in historic documents are either manual or OCR based. Moreover, in this study, we found that the Subject-Object-Verb extraction using GPT3-based relations produced more meaningful results compared to the Semantic Role labeling-based triple extraction.</abstract>
      <url hash="098e3c4c">2023.ranlp-1.13</url>
      <bibkey>anuradha-etal-2023-evaluating</bibkey>
    </paper>
    <paper id="14">
      <title>Impact of Emojis on Automatic Analysis of Individual Emotion Categories</title>
      <author><first>Ratchakrit</first><last>Arreerard</last></author>
      <author><first>Scott</first><last>Piao</last></author>
      <pages>124–131</pages>
      <abstract>Automatic emotion analysis is a highly challenging task for Natural Language Processing, which has so far mainly relied on textual contents to determine the emotion of text. However, words are not the only media that carry emotional information. In social media, people also use emojis to convey their feelings. Recently, researchers have studied emotional aspects of emojis, and use emoji information to improve the emotion detection and classification, but many issues remain to be addressed. In this study, we examine the impact of emoji embedding on emotion classification and intensity prediction on four individual emotion categories, including anger, fear, joy, and sadness, in order to investigate how emojis affect the automatic analysis of individual emotion categories and intensity. We conducted a comparative study by testing five machine learning models with and without emoji embeddings involved. Our experiment demonstrates that emojis have varying impact on different emotion categories, and there is potential that emojis can be used to enhance emotion information processing.</abstract>
      <url hash="c765fb44">2023.ranlp-1.14</url>
      <bibkey>arreerard-piao-2023-impact</bibkey>
    </paper>
    <paper id="15">
      <title>Was That a Question? Automatic Classification of Discourse Meaning in <fixed-case>S</fixed-case>panish</title>
      <author><first>Santiago</first><last>Arróniz</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <pages>132–142</pages>
      <abstract>This paper examines the effectiveness of different feature representations of audio data in accurately classifying discourse meaning in Spanish. The task involves determining whether an utterance is a declarative sentence, an interrogative, an imperative, etc. We explore how pitch contour can be represented for a discourse-meaning classification task, employing three different audio features: MFCCs, Mel-scale spectrograms, and chromagrams. We also determine if utilizing means is more effective in representing the speech signal, given the large number of coefficients produced during the feature extraction process. Finally, we evaluate whether these feature representation techniques are sensitive to speaker information. Our results show that a recurrent neural network architecture in conjunction with all three feature sets yields the best results for the task.</abstract>
      <url hash="6c5c3918">2023.ranlp-1.15</url>
      <bibkey>arroniz-kubler-2023-question</bibkey>
    </paper>
    <paper id="16">
      <title>Designing the <fixed-case>LECOR</fixed-case> Learner Corpus for <fixed-case>R</fixed-case>omanian</title>
      <author><first>Ana Maria</first><last>Barbu</last></author>
      <author><first>Elena</first><last>Irimia</last></author>
      <author><first>Carmen Mîrzea</first><last>Vasile</last></author>
      <author><first>Vasile</first><last>Păiș</last></author>
      <pages>143–152</pages>
      <abstract>This article presents a work-in-progress project, which aims to build and utilize a corpus of Romanian texts written or spoken by non-native students of different nationalities, who learn Romanian as a foreign language in the one-year, intensive academic program organized by the University of Bucharest. This corpus, called LECOR – Learner Corpus for Romanian – is made up of pairs of texts: a version of the student and a corrected one of the teacher. Each version is automatically annotated with lemma and POS-tag, and the two versions are then compared, and the differences are marked as errors at this stage. The corpus also contains metadata file sets about students and their samples. In this article, the conceptual framework for building and utilization of the corpus is presented, including the acquisition and organization phases of the primary material, the annotation process, and the first attempts to adapt the NoSketch Engine query interface to the project’s objectives. The article concludes by outlining the next steps in the development of the corpus aimed at quantitative accumulation and the development of the error correction process and the complex error annotation.</abstract>
      <url hash="1db6fd4d">2023.ranlp-1.16</url>
      <bibkey>barbu-etal-2023-designing</bibkey>
    </paper>
    <paper id="17">
      <title>Non-Parametric Memory Guidance for Multi-Document Summarization</title>
      <author><first>Florian</first><last>Baud</last></author>
      <author><first>Alex</first><last>Aussem</last></author>
      <pages>153–158</pages>
      <abstract>Multi-document summarization (MDS) is a difficult task in Natural Language Processing, aiming to summarize information from several documents. However, the source documents are often insufficient to obtain a qualitative summary. We propose a retriever-guided model combined with non-parametric memory for summary generation. This model retrieves relevant candidates from a database and then generates the summary considering the candidates with a copy mechanism and the source documents. The retriever is implemented with Approximate Nearest Neighbor Search (ANN) to search large databases. Our method is evaluated on the MultiXScience dataset which includes scientific articles. Finally, we discuss our results and possible directions for future work.</abstract>
      <url hash="c95618c8">2023.ranlp-1.17</url>
      <bibkey>baud-aussem-2023-non</bibkey>
    </paper>
    <paper id="18">
      <title>Beyond Information: Is <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> Empathetic Enough?</title>
      <author><first>Ahmed</first><last>Belkhir</last></author>
      <author><first>Fatiha</first><last>Sadat</last></author>
      <pages>159–169</pages>
      <abstract>This paper aims to explore and enhance ChatGPT’s abilities to generate more human-like conversations by taking into account the emotional state of the user. To achieve this goal, a prompt-driven Emotional Intelligence is used through the empathetic dialogue dataset in order to propose a more empathetic conversational language model. We propose two altered versions of ChatGPT as follows: (1) an emotion-infused version which takes the user’s emotion as input before generating responses using an emotion classifier based on ELECTRA ; and (2) the emotion adapting version that tries to accommodate for how the user feels without any external component. By analyzing responses of the two proposed altered versions and comparing them to the standard version of ChatGPT, we find that using the external emotion classifier leads to more frequent and pronounced use of positive emotions compared to the standard version. On the other hand, using simple prompt engineering to take the user emotion into consideration, does the opposite. Finally, comparisons with state-of-the-art models highlight the potential of prompt engineering to enhance the emotional abilities of chatbots based on large language models.</abstract>
      <url hash="6c2f7c6c">2023.ranlp-1.18</url>
      <bibkey>belkhir-sadat-2023-beyond</bibkey>
    </paper>
    <paper id="19">
      <title>Using <fixed-case>W</fixed-case>ikidata for Enhancing Compositionality in Pretrained Language Models</title>
      <author><first>Meriem</first><last>Beloucif</last></author>
      <author><first>Mihir</first><last>Bansal</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>170–178</pages>
      <abstract>One of the many advantages of pre-trained language models (PLMs) such as BERT and RoBERTa is their flexibility and contextual nature. These features give PLMs strong capabilities for representing lexical semantics. However, PLMs seem incapable of capturing high-level semantics in terms of compositionally. We show that when augmented with the relevant semantic knowledge, PMLs learn to capture a higher degree of lexical compositionality. We annotate a large dataset from Wikidata highlighting a type of semantic inference that is easy for humans to understand but difficult for PLMs, like the correlation between age and date of birth. We use this resource for finetuning DistilBERT, BERT large and RoBERTa. Our results show that the performance of PLMs against the test data continuously improves when augmented with such a rich resource. Our results are corroborated by a consistent improvement over most GLUE benchmark natural language understanding tasks.</abstract>
      <url hash="9d1989be">2023.ranlp-1.19</url>
      <bibkey>beloucif-etal-2023-using</bibkey>
    </paper>
    <paper id="20">
      <title>Multimodal Learning for Accurate Visual Question Answering: An Attention-Based Approach</title>
      <author><first>Jishnu</first><last>Bhardwaj</last></author>
      <author><first>Anurag</first><last>Balakrishnan</last></author>
      <author><first>Satyam</first><last>Pathak</last></author>
      <author><first>Ishan</first><last>Unnarkar</last></author>
      <author><first>Aniruddha</first><last>Gawande</last></author>
      <author><first>Benyamin</first><last>Ahmadnia</last></author>
      <pages>179–186</pages>
      <abstract>This paper proposes an open-ended task for Visual Question Answering (VQA) that leverages the InceptionV3 Object Detection model and an attention-based Long Short-Term Memory (LSTM) network for question answering. Our proposed model provides accurate natural language answers to questions about an image, including those that require understanding contextual information and background details. Our findings demonstrate that the proposed approach can achieve high accuracy, even with complex and varied visual information. The proposed method can contribute to developing more advanced vision systems that can process and interpret visual information like humans.</abstract>
      <url hash="1373485b">2023.ranlp-1.20</url>
      <bibkey>bhardwaj-etal-2023-multimodal</bibkey>
    </paper>
    <paper id="21">
      <title>Generative Models For <fixed-case>I</fixed-case>ndic Languages: Evaluating Content Generation Capabilities</title>
      <author><first>Savita</first><last>Bhat</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <author><first>Niranjan</first><last>Pedanekar</last></author>
      <pages>187–195</pages>
      <abstract>Large language models (LLMs) and generative AI have emerged as the most important areas in the field of natural language processing (NLP). LLMs are considered to be a key component in several NLP tasks, such as summarization, question-answering, sentiment classification, and translation. Newer LLMs, such as ChatGPT, BLOOMZ, and several such variants, are known to train on multilingual training data and hence are expected to process and generate text in multiple languages. Considering the widespread use of LLMs, evaluating their efficacy in multilingual settings is imperative. In this work, we evaluate the newest generative models (ChatGPT, mT0, and BLOOMZ) in the context of Indic languages. Specifically, we consider natural language generation (NLG) applications such as summarization and question-answering in monolingual and cross-lingual settings. We observe that current generative models have limited capability for generating text in Indic languages in a zero-shot setting. In contrast, generative models perform consistently better on manual quality-based evaluation in both Indic languages and English language generation. Considering limited generation performance, we argue that these LLMs are not intended to use in zero-shot fashion in downstream applications.</abstract>
      <url hash="313203c6">2023.ranlp-1.21</url>
      <bibkey>bhat-etal-2023-generative</bibkey>
    </paper>
    <paper id="22">
      <title>Measuring Spurious Correlation in Classification: “Clever Hans” in Translationese</title>
      <author><first>Angana</first><last>Borah</last></author>
      <author><first>Daria</first><last>Pylypenko</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>196–206</pages>
      <abstract>Recent work has shown evidence of “Clever Hans” behavior in high-performance neural translationese classifiers, where BERT-based classifiers capitalize on spurious correlations, in particular topic information, between data and target classification labels, rather than genuine translationese signals. Translationese signals are subtle (especially for professional translation) and compete with many other signals in the data such as genre, style, author, and, in particular, topic. This raises the general question of how much of the performance of a classifier is really due to spurious correlations in the data versus the signals actually targeted for by the classifier, especially for subtle target signals and in challenging (low resource) data settings. We focus on topic-based spurious correlation and approach the question from two directions: (i) where we have no knowledge about spurious topic information and its distribution in the data, (ii) where we have some indication about the nature of spurious topic correlations. For (i) we develop a measure from first principles capturing alignment of unsupervised topics with target classification labels as an indication of spurious topic information in the data. We show that our measure is the same as purity in clustering and propose a “topic floor” (as in a “noise floor”) for classification. For (ii) we investigate masking of known spurious topic carriers in classification. Both (i) and (ii) contribute to quantifying and (ii) to mitigating spurious correlations.</abstract>
      <url hash="ec9bbcb2">2023.ranlp-1.22</url>
      <bibkey>borah-etal-2023-measuring</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>WIKITIDE</fixed-case>: A <fixed-case>W</fixed-case>ikipedia-Based Timestamped Definition Pairs Dataset</title>
      <author><first>Hsuvas</first><last>Borkakoty</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <pages>207–216</pages>
      <abstract>A fundamental challenge in the current NLP context, dominated by language models, comes from the inflexibility of current architectures to “learn” new information. While model-centric solutions like continual learning or parameter-efficient fine-tuning are available, the question still remains of how to reliably identify changes in language or in the world. In this paper, we propose WikiTiDe, a dataset derived from pairs of timestamped definitions extracted from Wikipedia. We argue that such resources can be helpful for accelerating diachronic NLP, specifically, for training models able to scan knowledge resources for core updates concerning a concept, an event, or a named entity. Our proposed end-to-end method is fully automatic and leverages a bootstrapping algorithm for gradually creating a high-quality dataset. Our results suggest that bootstrapping the seed version of WikiTiDe leads to better-fine-tuned models. We also leverage fine-tuned models in a number of downstream tasks, showing promising results with respect to competitive baselines.</abstract>
      <url hash="36b365f5">2023.ranlp-1.23</url>
      <bibkey>borkakoty-espinosa-anke-2023-wikitide</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>BERT</fixed-case>abaporu: Assessing a Genre-Specific Language Model for <fixed-case>P</fixed-case>ortuguese <fixed-case>NLP</fixed-case></title>
      <author><first>Pablo Botton</first><last>Costa</last></author>
      <author><first>Matheus Camasmie</first><last>Pavan</last></author>
      <author><first>Wesley Ramos</first><last>Santos</last></author>
      <author><first>Samuel Caetano</first><last>Silva</last></author>
      <author><first>Ivandré</first><last>Paraboni</last></author>
      <pages>217–223</pages>
      <abstract>Transformer-based language models such as Bidirectional Encoder Representations from Transformers (BERT) are now mainstream in the NLP field, but extensions to languages other than English, to new domains and/or to more specific text genres are still in demand. In this paper we introduced BERTabaporu, a BERT language model that has been pre-trained on Twitter data in the Brazilian Portuguese language. The model is shown to outperform the best-known general-purpose model for this language in three Twitter-related NLP tasks, making a potentially useful resource for Portuguese NLP in general.</abstract>
      <url hash="310894cd">2023.ranlp-1.24</url>
      <bibkey>costa-etal-2023-bertabaporu</bibkey>
    </paper>
    <paper id="25">
      <title>Comparison of Multilingual Entity Linking Approaches</title>
      <author><first>Ivelina</first><last>Bozhinova</last></author>
      <author><first>Andrey</first><last>Tagarev</last></author>
      <pages>224–233</pages>
      <abstract>Despite rapid developments in the field of Natural Language Processing (NLP) in the past few years, the task of Multilingual Entity Linking (MEL) and especially its end-to-end formulation remains challenging. In this paper we aim to evaluate solutions for general end-to-end multilingual entity linking by conducting experiments using both existing complete approaches and novel combinations of pipelines for solving the task. The results identify the best performing current solutions and suggest some directions for further research.</abstract>
      <url hash="ae31c39e">2023.ranlp-1.25</url>
      <bibkey>bozhinova-tagarev-2023-comparison</bibkey>
    </paper>
    <paper id="26">
      <title>Automatic Extraction of the <fixed-case>R</fixed-case>omanian Academic Word List: Data and Methods</title>
      <author><first>Ana-Maria</first><last>Bucur</last></author>
      <author><first>Andreea</first><last>Dincă</last></author>
      <author><first>Madalina</first><last>Chitez</last></author>
      <author><first>Roxana</first><last>Rogobete</last></author>
      <pages>234–241</pages>
      <abstract>This paper presents the methodology and data used for the automatic extraction of the Romanian Academic Word List (Ro-AWL). Academic Word Lists are useful in both L2 and L1 teaching contexts. For the Romanian language, no such resource exists so far. Ro-AWL has been generated by combining methods from corpus and computational linguistics with L2 academic writing approaches. We use two types of data: (a) existing data, such as the Romanian Frequency List based on the ROMBAC corpus, and (b) self-compiled data, such as the expert academic writing corpus EXPRES. For constructing the academic word list, we follow the methodology for building the Academic Vocabulary List for the English language. The distribution of Ro-AWL features (general distribution, POS distribution) into four disciplinary datasets is in line with previous research. Ro-AWL is freely available and can be used for teaching, research and NLP applications.</abstract>
      <url hash="0701d7b9">2023.ranlp-1.26</url>
      <bibkey>bucur-etal-2023-automatic</bibkey>
    </paper>
    <paper id="27">
      <title>Stance Prediction from Multimodal Social Media Data</title>
      <author><first>Lais Carraro Leme</first><last>Cavalheiro</last></author>
      <author><first>Matheus Camasmie</first><last>Pavan</last></author>
      <author><first>Ivandré</first><last>Paraboni</last></author>
      <pages>242–248</pages>
      <abstract>Stance prediction - the computational task of inferring attitudes towards a given target topic of interest - relies heavily on text data provided by social media or similar sources, but it may also benefit from non-text information such as demographics (e.g., users’ gender, age, etc.), network structure (e.g., friends, followers, etc.), interactions (e.g., mentions, replies, etc.) and other non-text properties (e.g., time information, etc.). However, so-called hybrid (or in some cases multimodal) approaches to stance prediction have only been developed for a small set of target languages, and often making use of count-based text models (e.g., bag-of-words) and time-honoured classification methods (e.g., support vector machines). As a means to further research in the field, in this work we introduce a number of text- and non-text models for stance prediction in the Portuguese language, which make use of more recent methods based on BERT and an ensemble architecture, and ask whether a BERT stance classifier may be enhanced with different kinds of network-related information.</abstract>
      <url hash="60916def">2023.ranlp-1.27</url>
      <bibkey>cavalheiro-etal-2023-stance</bibkey>
    </paper>
    <paper id="28">
      <title>From Stigma to Support: A Parallel Monolingual Corpus and <fixed-case>NLP</fixed-case> Approach for Neutralizing Mental Illness Bias</title>
      <author><first>Mason</first><last>Choey</last></author>
      <pages>249–254</pages>
      <abstract>Negative attitudes and perceptions towards mental illness continue to be pervasive in our society. One of the factors contributing to and reinforcing this stigma is the usage of language that is biased against mental illness. Identifying biased language and replacing it with person-first, neutralized language is a first step towards eliminating harmful stereotypes and creating a supportive and inclusive environment for those living with mental illness. This paper presents a novel Natural Language Processing (NLP) system that aims to automatically identify biased text related to mental illness and suggest neutral language replacements without altering the original text’s meaning. Building on previous work in the field, this paper presents the Mental Illness Neutrality Corpus (MINC) comprising over 5500 mental illness-biased text and neutralized sentence pairs (in English), which is used to fine-tune a CONCURRENT model system developed by Pryzant et al. (2020). After evaluation, the model demonstrates high proficiency in neutralizing mental illness bias with an accuracy of 98.7%. This work contributes a valuable resource for reducing mental illness bias in text and has the potential for further research in tackling more complex nuances and multilingual biases.</abstract>
      <url hash="66674dd0">2023.ranlp-1.28</url>
      <bibkey>choey-2023-stigma</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>BB</fixed-case>25<fixed-case>HL</fixed-case>egal<fixed-case>S</fixed-case>um: Leveraging <fixed-case>BM</fixed-case>25 and <fixed-case>BERT</fixed-case>-Based Clustering for the Summarization of Legal Documents</title>
      <author><first>Leonardo</first><last>de Andrade</last></author>
      <author><first>Karin</first><last>Becker</last></author>
      <pages>255–263</pages>
      <abstract>Legal document summarization aims to provide a clear understanding of the main points and arguments in a legal document, contributing to the efficiency of the judicial system. In this paper, we propose BB25HLegalSum, a method that combines BERT clusters with the BM25 algorithm to summarize legal documents and present them to users with highlighted important information. The process involves selecting unique, relevant sentences from the original document, clustering them to find sentences about a similar subject, combining them to generate a summary according to three strategies, and highlighting them to the user in the original document. We outperformed baseline techniques using the BillSum dataset, a widely used benchmark in legal document summarization. Legal workers positively assessed the highlighted presentation.</abstract>
      <url hash="4bc32ed3">2023.ranlp-1.29</url>
      <bibkey>de-andrade-becker-2023-bb25hlegalsum</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>SSSD</fixed-case>: Leveraging Pre-trained Models and Semantic Search for Semi-supervised Stance Detection</title>
      <author><first>André</first><last>de Sousa</last></author>
      <author><first>Karin</first><last>Becker</last></author>
      <pages>264–273</pages>
      <abstract>Pre-trained models (PTMs) based on the Transformers architecture are trained on massive amounts of data and can capture nuances and complexities in linguistic expressions, making them a powerful tool for many natural language processing tasks. In this paper, we present SSSD (Semantic Similarity Stance Detection), a semi-supervised method for stance detection on Twitter that automatically labels a large, domain-related corpus for training a stance classification model. The method assumes as input a domain set of tweets about a given target and a labeled query set of tweets of representative arguments related to the stances. It scales the automatic labeling of a large number of tweets, and improves classification accuracy by leveraging the power of PTMs and semantic search to capture context and meaning. We largely outperformed all baselines in experiments using the Semeval benchmark.</abstract>
      <url hash="e1e719a9">2023.ranlp-1.30</url>
      <bibkey>de-sousa-becker-2023-sssd</bibkey>
    </paper>
    <paper id="31">
      <title>Detecting Text Formality: A Study of Text Classification Approaches</title>
      <author><first>Daryna</first><last>Dementieva</last></author>
      <author><first>Nikolay</first><last>Babakov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>274–284</pages>
      <abstract>Formality is one of the important characteristics of text documents. The automatic detection of the formality level of a text is potentially beneficial for various natural language processing tasks. Before, two large-scale datasets were introduced for multiple languages featuring formality annotation—GYAFC and X-FORMAL. However, they were primarily used for the training of style transfer models. At the same time, the detection of text formality on its own may also be a useful application. This work proposes the first to our knowledge systematic study of formality detection methods based on statistical, neural-based, and Transformer-based machine learning methods and delivers the best-performing models for public usage. We conducted three types of experiments – monolingual, multilingual, and cross-lingual. The study shows the overcome of Char BiLSTM model over Transformer-based ones for the monolingual and multilingual formality classification task, while Transformer-based classifiers are more stable to cross-lingual knowledge transfer.</abstract>
      <url hash="723ee157">2023.ranlp-1.31</url>
      <bibkey>dementieva-etal-2023-detecting</bibkey>
    </paper>
    <paper id="32">
      <title>Developing a Multilingual Corpus of <fixed-case>W</fixed-case>ikipedia Biographies</title>
      <author><first>Hannah</first><last>Devinney</last></author>
      <author><first>Anton</first><last>Eklund</last></author>
      <author><first>Igor</first><last>Ryazanov</last></author>
      <author><first>Jingwen</first><last>Cai</last></author>
      <pages>285–294</pages>
      <abstract>For many languages, Wikipedia is the most accessible source of biographical information. Studying how Wikipedia describes the lives of people can provide insights into societal biases, as well as cultural differences more generally. We present a method for extracting datasets of Wikipedia biographies. The accompanying codebase is adapted to English, Swedish, Russian, Chinese, and Farsi, and is extendable to other languages. We present an exploratory analysis of biographical topics and gendered patterns in four languages using topic modelling and embedding clustering. We find similarities across languages in the types of categories present, with the distribution of biographies concentrated in the language’s core regions. Masculine terms are over-represented and spread out over a wide variety of topics. Feminine terms are less frequent and linked to more constrained topics. Non-binary terms are nearly non-represented.</abstract>
      <url hash="396a9ba3">2023.ranlp-1.32</url>
      <bibkey>devinney-etal-2023-developing</bibkey>
    </paper>
    <paper id="33">
      <title>A Computational Analysis of the Voices of Shakespeare’s Characters</title>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <author><first>Ana Sabina</first><last>Uban</last></author>
      <pages>295–300</pages>
      <abstract>In this paper we propose a study of a relatively novel problem in authorship attribution research: that of classifying the stylome of characters in a literary work. We choose as a case study the plays of William Shakespeare, presumably the most renowned and respected dramatist in the history of literature. Previous research in the field of authorship attribution has shown that the writing style of an author can be characterized and distinguished from that of other authors automatically. The question we propose to answer is a related but different one: can the styles of different characters be distinguished? We aim to verify in this way if an author managed to create believable characters with individual styles, and focus on Shakespeare’s iconic characters. We present our experiments using various features and models, including an SVM and a neural network, show that characters in Shakespeare’s plays can be classified with up to 50% accuracy.</abstract>
      <url hash="2bc64502">2023.ranlp-1.33</url>
      <bibkey>dinu-uban-2023-computational</bibkey>
    </paper>
    <paper id="34">
      <title>Source Code Plagiarism Detection with Pre-Trained Model Embeddings and Automated Machine Learning</title>
      <author><first>Fahad</first><last>Ebrahim</last></author>
      <author><first>Mike</first><last>Joy</last></author>
      <pages>301–309</pages>
      <abstract>Source code plagiarism is a critical ethical issue in computer science education where students use someone else’s work as their own. It can be treated as a binary classification problem where the output can be either: yes (plagiarism found) or no (plagiarism not found). In this research, we have taken the open-source dataset ‘SOCO’, which contains two programming languages (PLs), namely Java and C/C++ (although our method could be applied to any PL). Source codes should be converted to vector representations that capture both the syntax and semantics of the text, known as contextual embeddings. These embeddings would be generated using source code pre-trained models (CodePTMs). The cosine similarity scores of three different CodePTMs were selected as features. The classifier selection and parameter tuning were conducted with the assistance of Automated Machine Learning (AutoML). The selected classifiers were tested, initially on Java, and the proposed approach produced average to high results compared to other published research, and surpassed the baseline (the JPlag plagiarism detection tool). For C/C++, the approach outperformed other research work and produced the highest ranking score.</abstract>
      <url hash="cf818e1b">2023.ranlp-1.34</url>
      <bibkey>ebrahim-joy-2023-source</bibkey>
    </paper>
    <paper id="35">
      <title>Identifying Semantic Argument Types in Predication and Copredication Contexts: A Zero-Shot Cross-Lingual Approach</title>
      <author><first>Deniz Ekin</first><last>Yavas</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Rainer</first><last>Osswald</last></author>
      <author><first>Elisabetta</first><last>Jezek</last></author>
      <author><first>Marta</first><last>Ricchiardi</last></author>
      <author><first>Long</first><last>Chen</last></author>
      <pages>310–320</pages>
      <abstract>Identifying semantic argument types in predication contexts is not a straightforward task for several reasons, such as inherent polysemy, coercion, and copredication phenomena. In this paper, we train monolingual and multilingual classifiers with a zero-shot cross-lingual approach to identify semantic argument types in predications using pre-trained language models as feature extractors. We train classifiers for different semantic argument types and for both verbal and adjectival predications. Furthermore, we propose a method to detect copredication using these classifiers through identifying the argument semantic type targeted in different predications over the same noun in a sentence. We evaluate the performance of the method on copredication test data with Food•Event nouns for 5 languages.</abstract>
      <url hash="ab3ed19a">2023.ranlp-1.35</url>
      <bibkey>yavas-etal-2023-identifying</bibkey>
    </paper>
    <paper id="36">
      <title>A Review of Research-Based Automatic Text Simplification Tools</title>
      <author><first>Isabel</first><last>Espinosa-Zaragoza</last></author>
      <author><first>José</first><last>Abreu-Salas</last></author>
      <author><first>Elena</first><last>Lloret</last></author>
      <author><first>Paloma</first><last>Moreda</last></author>
      <author><first>Manuel</first><last>Palomar</last></author>
      <pages>321–330</pages>
      <abstract>In the age of knowledge, the democratisation of information facilitated through the Internet may not be as pervasive if written language poses challenges to particular sectors of the population. The objective of this paper is to present an overview of research-based automatic text simplification tools. Consequently, we describe aspects such as the language, language phenomena, language levels simplified, approaches, specific target populations these tools are created for (e.g. individuals with cognitive impairment, attention deficit, elderly people, children, language learners), and accessibility and availability considerations. The review of existing studies covering automatic text simplification tools is undergone by searching two databases: Web of Science and Scopus. The eligibility criteria involve text simplification tools with a scientific background in order to ascertain how they operate. This methodology yielded 27 text simplification tools that are further analysed. Some of the main conclusions reached with this review are the lack of resources accessible to the public, the need for customisation to foster the individual’s independence by allowing the user to select what s/he finds challenging to understand while not limiting the user’s capabilities and the need for more simplification tools in languages other than English, to mention a few.</abstract>
      <url hash="f9c5f5c4">2023.ranlp-1.36</url>
      <bibkey>espinosa-zaragoza-etal-2023-review</bibkey>
    </paper>
    <paper id="37">
      <title>Vocab-Expander: A System for Creating Domain-Specific Vocabularies Based on Word Embeddings</title>
      <author><first>Michael</first><last>Faerber</last></author>
      <author><first>Nicholas</first><last>Popovic</last></author>
      <pages>331–335</pages>
      <abstract>In this paper, we propose Vocab-Expander at https://vocab-expander.com, an online tool that enables end-users (e.g., technology scouts) to create and expand a vocabulary of their domain of interest. It utilizes an ensemble of state-of-the-art word embedding techniques based on web text and ConceptNet, a common-sense knowledge base, to suggest related terms for already given terms. The system has an easy-to-use interface that allows users to quickly confirm or reject term suggestions. Vocab-Expander offers a variety of potential use cases, such as improving concept-based information retrieval in technology and innovation management, enhancing communication and collaboration within organizations or interdisciplinary projects, and creating vocabularies for specific courses in education.</abstract>
      <url hash="b7faf149">2023.ranlp-1.37</url>
      <bibkey>faerber-popovic-2023-vocab</bibkey>
    </paper>
    <paper id="38">
      <title>On the Generalization of Projection-Based Gender Debiasing in Word Embedding</title>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <author><first>Antonio</first><last>Candelieri</last></author>
      <author><first>Lorenzo</first><last>Pastore</last></author>
      <pages>336–343</pages>
      <abstract>Gender bias estimation and mitigation techniques in word embeddings lack an understanding of their generalization capabilities. In this work, we complement prior research by comparing in a systematic way four gender bias metrics (Word Embedding Association Tes, Relative Negative Sentiment Bias, Embedding Coherence Test and Bias Analogy Test), two types of projection-based gender mitigation strategies (hard- and soft-debiasing) on three well-known word embedding representations (Word2Vec, FastText and Glove). The experiments have shown that the considered word embeddings are consistent between them but the debiasing techniques are inconsistent across the different metrics, also highlighting the potential risk of unintended bias after the mitigation strategies.</abstract>
      <url hash="ad022cf7">2023.ranlp-1.38</url>
      <bibkey>fersini-etal-2023-generalization</bibkey>
    </paper>
    <paper id="39">
      <title>Mapping Explicit and Implicit Discourse Relations between the <fixed-case>RST</fixed-case>-<fixed-case>DT</fixed-case> and the <fixed-case>PDTB</fixed-case> 3.0</title>
      <author><first>Nelson Filipe</first><last>Costa</last></author>
      <author><first>Nadia</first><last>Sheikh</last></author>
      <author><first>Leila</first><last>Kosseim</last></author>
      <pages>344–352</pages>
      <abstract>In this paper we propose a first empirical mapping between the RST-DT and the PDTB 3.0. We provide an original algorithm which allowed the mapping of 6,510 (80.0%) explicit and implicit discourse relations between the overlapping articles of the RST-DT and PDTB 3.0 discourse annotated corpora. Results of the mapping show that while it is easier to align segments of implicit discourse relations, the mapping obtained between the aligned explicit discourse relations is more unambiguous.</abstract>
      <url hash="796a0f16">2023.ranlp-1.39</url>
      <bibkey>costa-etal-2023-mapping</bibkey>
    </paper>
    <paper id="40">
      <title>Bigfoot in Big Tech: Detecting Out of Domain Conspiracy Theories</title>
      <author><first>Matthew</first><last>Fort</last></author>
      <author><first>Zuoyu</first><last>Tian</last></author>
      <author><first>Elizabeth</first><last>Gabel</last></author>
      <author><first>Nina</first><last>Georgiades</last></author>
      <author><first>Noah</first><last>Sauer</last></author>
      <author><first>Daniel</first><last>Dakota</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <pages>353–363</pages>
      <abstract>We investigate approaches to classifying texts into either conspiracy theory or mainstream using the Language Of Conspiracy (LOCO) corpus. Since conspiracy theories are not monolithic constructs, we need to identify approaches that robustly work in an out-of- domain setting (i.e., across conspiracy topics). We investigate whether optimal in-domain set- tings can be transferred to out-of-domain set- tings, and we investigate different methods for bleaching to steer classifiers away from words typical for an individual conspiracy theory. We find that BART works better than an SVM, that we can successfully classify out-of-domain, but there are no clear trends in how to choose the best source training domains. Addition- ally, bleaching only topic words works better than bleaching all content words or completely delexicalizing texts.</abstract>
      <url hash="2aee0f90">2023.ranlp-1.40</url>
      <bibkey>fort-etal-2023-bigfoot</bibkey>
    </paper>
    <paper id="41">
      <title>Deep Learning Approaches to Detecting Safeguarding Concerns in Schoolchildren’s Online Conversations</title>
      <author><first>Emma</first><last>Franklin</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <pages>364–372</pages>
      <abstract>For school teachers and Designated Safeguarding Leads (DSLs), computers and other school-owned communication devices are both indispensable and deeply worrisome. For their education, children require access to the Internet, as well as a standard institutional ICT infrastructure, including e-mail and other forms of online communication technology. Given the sheer volume of data being generated and shared on a daily basis within schools, most teachers and DSLs can no longer monitor the safety and wellbeing of their students without the use of specialist safeguarding software. In this paper, we experiment with the use of state-of-the-art neural network models on the modelling of a dataset of almost 9,000 anonymised child-generated chat messages on the Microsoft Teams platform. The data was manually classified into eight fine-grained classes of safeguarding concerns (or false alarms) that a monitoring program would be interested in, and these were further split into two binary classes: true positives (real safeguarding concerns) and false positives (false alarms). For the fine grained classification, our models achieved a macro F1 score of 73.56, while for the binary classification, we achieved a macro F1 score of 87.32. This first experiment into the use of Deep Learning for detecting safeguarding concerns represents an important step towards achieving high-accuracy and reliable monitoring information for busy teachers and safeguarding leads.</abstract>
      <url hash="2361b79b">2023.ranlp-1.41</url>
      <bibkey>franklin-ranasinghe-2023-deep</bibkey>
    </paper>
    <paper id="42">
      <title>On the Identification and Forecasting of Hate Speech in Inceldom</title>
      <author><first>Paolo</first><last>Gajo</last></author>
      <author><first>Arianna</first><last>Muti</last></author>
      <author><first>Katerina</first><last>Korre</last></author>
      <author><first>Silvia</first><last>Bernardini</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <pages>373–384</pages>
      <abstract>Spotting hate speech in social media posts is crucial to increase the civility of the Web and has been thoroughly explored in the NLP community. For the first time, we introduce a multilingual corpus for the analysis and identification of hate speech in the domain of inceldom, built from incel Web forums in English and Italian, including expert annotation at the post level for two kinds of hate speech: misogyny and racism. This resource paves the way for the development of mono- and cross-lingual models for (a) the identification of hateful (misogynous and racist) posts and (b) the forecasting of the amount of hateful responses that a post is likely to trigger. Our experiments aim at improving the performance of Transformer-based models using masked language modeling pre-training and dataset merging. The results show that these strategies boost the models’ performance in all settings (binary classification, multi-label classification and forecasting), especially in the cross-lingual scenarios.</abstract>
      <url hash="5946ad56">2023.ranlp-1.42</url>
      <bibkey>gajo-etal-2023-identification</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>T</fixed-case>2<fixed-case>KG</fixed-case>: Transforming Multimodal Document to Knowledge Graph</title>
      <author><first>Santiago</first><last>Galiano</last></author>
      <author><first>Rafael</first><last>Muñoz</last></author>
      <author><first>Yoan</first><last>Gutiérrez</last></author>
      <author><first>Andrés</first><last>Montoyo</last></author>
      <author><first>Jose Ignacio</first><last>Abreu</last></author>
      <author><first>Luis Alfonso</first><last>Ureña</last></author>
      <pages>385–391</pages>
      <abstract>The large amount of information in digital format that exists today makes it unfeasible to use manual means to acquire the knowledge contained in these documents. Therefore, it is necessary to develop tools that allow us to incorporate this knowledge into a structure that is easy to use by both machines and humans. This paper presents a system that can incorporate the relevant information from a document in any format, structured or unstructured, into a semantic network that represents the existing knowledge in the document. The system independently processes from structured documents based on its annotation scheme to unstructured documents, written in natural language, for which it uses a set of sensors that identifies the relevant information and subsequently incorporates it to enrich the semantic network that is created by linking all the information based on the knowledge discovered.</abstract>
      <url hash="87fde17c">2023.ranlp-1.43</url>
      <bibkey>galiano-etal-2023-t2kg</bibkey>
    </paper>
    <paper id="44">
      <title>!Translate: When You Cannot Cook Up a Translation, Explain</title>
      <author><first>Federico</first><last>Garcea</last></author>
      <author><first>Margherita</first><last>Martinelli</last></author>
      <author><first>Maja</first><last>Milicević Petrović</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <pages>392–398</pages>
      <abstract>In the domain of cuisine, both dishes and ingredients tend to be heavily rooted in the local context they belong to. As a result, the associated terms are often realia tied to specific cultures and languages. This causes difficulties for non-speakers of the local language and ma- chine translation (MT) systems alike, as it implies a lack of the concept and/or of a plausible translation. MT typically opts for one of two alternatives: keeping the source language terms untranslated or relying on a hyperonym/near-synonym in the target language, provided one exists. !Translate proposes a better alternative: explaining. Given a cuisine entry such as a restaurant menu item, we identify culture-specific terms and enrich the output of the MT system with automatically retrieved definitions of the non-translatable terms in the target language, making the translation more actionable for the final user.</abstract>
      <url hash="f8e079a8">2023.ranlp-1.44</url>
      <bibkey>garcea-etal-2023-translate</bibkey>
    </paper>
    <paper id="45">
      <title>An Evaluation of Source Factors in Concatenation-Based Context-Aware Neural Machine Translation</title>
      <author><first>Harritxu</first><last>Gete</last></author>
      <author><first>Thierry</first><last>Etchegoyhen</last></author>
      <pages>399–407</pages>
      <abstract>We explore the use of source factors in context-aware neural machine translation, specifically concatenation-based models, to improve the translation quality of inter-sentential phenomena. Context sentences are typically concatenated to the sentence to be translated, with string-based markers to separate the latter from the former. Although previous studies have measured the impact of prefixes to identify and mark context information, the use of learnable factors has only been marginally explored. In this study, we evaluate the impact of single and multiple source context factors in English-German and Basque-Spanish contextual translation. We show that this type of factors can significantly enhance translation accuracy for phenomena such as gender and register coherence in Basque-Spanish, while also improving BLEU results in some scenarios. These results demonstrate the potential of factor-based context identification to improve context-aware machine translation in future research.</abstract>
      <url hash="024b6980">2023.ranlp-1.45</url>
      <bibkey>gete-etchegoyhen-2023-evaluation</bibkey>
    </paper>
    <paper id="46">
      <title>Lessons Learnt from Linear Text Segmentation: a Fair Comparison of Architectural and Sentence Encoding Strategies for Successful Segmentation</title>
      <author><first>Iacopo</first><last>Ghinassi</last></author>
      <author><first>Lin</first><last>Wang</last></author>
      <author><first>Chris</first><last>Newell</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <pages>408–418</pages>
      <abstract>Recent works on linear text segmentation have shown new state-of-the-art results nearly every year. Most times, however, these recent advances include a variety of different elements which makes it difficult to evaluate which individual components of the proposed methods bring about improvements for the task and, more generally, what actually works for linear text segmentation. Moreover, evaluating text segmentation is notoriously difficult and the use of a metric such as Pk, which is widely used in existing literature, presents specific problems that complicates a fair comparison between segmentation models. In this work, then, we draw from a number of existing works to assess which is the state-of-the-art in linear text segmentation, investigating what architectures and features work best for the task. For doing so, we present three models representative of a variety of approaches, we compare them to existing methods and we inspect elements composing them, so as to give a more complete picture of which technique is more successful and why that might be the case. At the same time, we highlight a specific feature of Pk which can bias the results and we report our results using different settings, so as to give future literature a more comprehensive set of baseline results for future developments. We then hope that this work can serve as a solid foundation to foster research in the area, overcoming task-specific difficulties such as evaluation setting and providing new state-of-the-art results.</abstract>
      <url hash="34447304">2023.ranlp-1.46</url>
      <bibkey>ghinassi-etal-2023-lessons</bibkey>
    </paper>
    <paper id="47">
      <title>Student’s t-Distribution: On Measuring the Inter-Rater Reliability When the Observations are Scarce</title>
      <author><first>Serge</first><last>Gladkoff</last></author>
      <author><first>Lifeng</first><last>Han</last></author>
      <author><first>Goran</first><last>Nenadic</last></author>
      <pages>419–428</pages>
      <abstract>In natural language processing (NLP) we always rely on human judgement as the golden quality evaluation method. However, there has been an ongoing debate on how to better evaluate inter-rater reliability (IRR) levels for certain evaluation tasks, such as translation quality evaluation (TQE), especially when the data samples (observations) are very scarce. In this work, we first introduce the study on how to estimate the confidence interval for the measurement value when only one data (evaluation) point is available. Then, this leads to our example with two human-generated observational scores, for which, we introduce “Student’s <i>t</i>-Distribution” method and explain how to use it to measure the IRR score using only these two data points, as well as the confidence intervals (CIs) of the quality evaluation. We give a quantitative analysis of how the evaluation confidence can be greatly improved by introducing more observations, even if only one extra observation. We encourage researchers to report their IRR scores in all possible means, e.g. using Student’s <i>t</i>-Distribution method whenever possible; thus making the NLP evaluation more meaningful, transparent, and trustworthy. This <i>t</i>-Distribution method can be also used outside of NLP fields to measure IRR level for trustworthy evaluation of experimental investigations, whenever the observational data is scarce.</abstract>
      <url hash="1976ffd1">2023.ranlp-1.47</url>
      <bibkey>gladkoff-etal-2023-students</bibkey>
    </paper>
    <paper id="48">
      <title>Data Augmentation for Fake News Detection by Combining Seq2seq and <fixed-case>NLI</fixed-case></title>
      <author><first>Anna</first><last>Glazkova</last></author>
      <pages>429–439</pages>
      <abstract>State-of-the-art data augmentation methods help improve the generalization of deep learning models. However, these methods often generate examples that contradict the preserving class labels. This is crucial for some natural language processing tasks, such as fake news detection. In this work, we combine sequence-to-sequence and natural language inference models for data augmentation in the fake news detection domain using short news texts, such as tweets and news titles. This approach allows us to generate new training examples that do not contradict facts from the original texts. We use the non-entailment probability for the pair of the original and generated texts as a loss function for a transformer-based sequence-to-sequence model. The proposed approach has demonstrated the effectiveness on three classification benchmarks in fake news detection in terms of the F1-score macro and ROC AUC. Moreover, we showed that our approach retains the class label of the original text more accurately than other transformer-based methods.</abstract>
      <url hash="6ac899af">2023.ranlp-1.48</url>
      <bibkey>glazkova-2023-data</bibkey>
    </paper>
    <paper id="49">
      <title>Exploring Unsupervised Semantic Similarity Methods for Claim Verification in Health Care News Articles</title>
      <author><first>Vishwani</first><last>Gupta</last></author>
      <author><first>Astrid</first><last>Viciano</last></author>
      <author><first>Holger</first><last>Wormer</last></author>
      <author><first>Najmehsadat</first><last>Mousavinezhad</last></author>
      <pages>440–447</pages>
      <abstract>In the 21st century, the proliferation of fake information has emerged as a significant threat to society. Particularly, healthcare medical reporters face challenges when verifying claims related to treatment effects, side effects, and risks mentioned in news articles, relying on scientific publications for accuracy. The accurate communication of scientific information in news articles has long been a crucial concern in the scientific community, as the dissemination of misinformation can have dire consequences in the healthcare domain. Healthcare medical reporters would greatly benefit from efficient methods to retrieve evidence from scientific publications supporting specific claims. This paper delves into the application of unsupervised semantic similarity models to facilitate claim verification for medical reporters, thereby expediting the process. We explore unsupervised multilingual evidence retrieval techniques aimed at reducing the time required to obtain evidence from scientific studies. Instead of employing content classification, we propose an approach that retrieves relevant evidence from scientific publications for claim verification within the healthcare domain. Given a claim and a set of scientific publications, our system generates a list of the most similar paragraphs containing supporting evidence. Furthermore, we evaluate the performance of state-of-the-art unsupervised semantic similarity methods in this task. As the claim and evidence are present in a cross-lingual space, we find that the XML-RoBERTa model exhibits high accuracy in achieving our objective. Through this research, we contribute to enhancing the efficiency and reliability of claim verification for healthcare medical reporters, enabling them to accurately source evidence from scientific publications in a timely manner.</abstract>
      <url hash="b823b513">2023.ranlp-1.49</url>
      <bibkey>gupta-etal-2023-exploring</bibkey>
    </paper>
    <paper id="50">
      <title><fixed-case>A</fixed-case>lpha<fixed-case>MWE</fixed-case>-<fixed-case>A</fixed-case>rabic: <fixed-case>A</fixed-case>rabic Edition of Multilingual Parallel Corpora with Multiword Expression Annotations</title>
      <author><first>Najet</first><last>Hadj Mohamed</last></author>
      <author><first>Malak</first><last>Rassem</last></author>
      <author><first>Lifeng</first><last>Han</last></author>
      <author><first>Goran</first><last>Nenadic</last></author>
      <pages>448–457</pages>
      <abstract>Multiword Expressions (MWEs) have been a bottleneck for Natural Language Understanding (NLU) and Natural Language Generation (NLG) tasks due to their idiomaticity, ambiguity, and non-compositionality. Bilingual parallel corpora introducing MWE annotations are very scarce which set another challenge for current Natural Language Processing (NLP) systems, especially in a multilingual setting. This work presents AlphaMWE-Arabic, an Arabic edition of the AlphaMWE parallel corpus with MWE annotations. We introduce how we created this corpus including machine translation (MT), post-editing, and annotations for both standard and dialectal varieties, i.e. Tunisian and Egyptian Arabic. We analyse the MT errors when they meet MWEs-related content, both quantitatively using the human-in-the-loop metric HOPE and qualitatively. We report the current state-of-the-art MT systems are far from reaching human parity performances. We expect our bilingual English-Arabic corpus will be an asset for multilingual research on MWEs such as translation and localisation, as well as for monolingual settings including the study of Arabic-specific lexicography and phrasal verbs on MWEs. Our corpus and experimental data are available at <url>https://github.com/aaronlifenghan/AlphaMWE</url>.</abstract>
      <url hash="9d291827">2023.ranlp-1.50</url>
      <bibkey>hadj-mohamed-etal-2023-alphamwe</bibkey>
    </paper>
    <paper id="51">
      <title>Performance Analysis of <fixed-case>A</fixed-case>rabic Pre-trained Models on Named Entity Recognition Task</title>
      <author><first>Abdelhalim Hafedh</first><last>Dahou</last></author>
      <author><first>Mohamed Amine</first><last>Cheragui</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <pages>458–467</pages>
      <abstract>Named Entity Recognition (NER) is a crucial task within natural language processing (NLP) that entails the identification and classification of entities, such as person, organization and location. This study delves into NER specifically in the Arabic language, focusing on the Algerian dialect. While previous research in NER has primarily concentrated on Modern Standard Arabic (MSA), the advent of social media has prompted a need to address the variations found in different Arabic dialects. Moreover, given the notable achievements of Large-scale pre-trained models (PTMs) based on the BERT architecture, this paper aims to evaluate Arabic pre-trained models using an Algerian dataset that covers different domains and writing styles. Additionally, an error analysis is conducted to identify PTMs’ limitations, and an investigation is carried out to assess the performance of trained MSA models on the Algerian dialect. The experimental results and subsequent analysis shed light on the complexities of NER in Arabic, offering valuable insights for future research endeavors.</abstract>
      <url hash="46719bd0">2023.ranlp-1.51</url>
      <bibkey>dahou-etal-2023-performance</bibkey>
    </paper>
    <paper id="52">
      <title>Discourse Analysis of Argumentative Essays of <fixed-case>E</fixed-case>nglish Learners Based on <fixed-case>CEFR</fixed-case> Level</title>
      <author><first>Blaise</first><last>Hanel</last></author>
      <author><first>Leila</first><last>Kosseim</last></author>
      <pages>468–474</pages>
      <abstract>In this paper, we investigate the relationship between the use of discourse relations and the CEFR-level of argumentative English learner essays. Using both the Rhetorical Structure Theory (RST) and the Penn Discourse TreeBank (PDTB) frameworks, we analyze essays from The International Corpus Network of Asian Learners (ICNALE), and the Corpus and Repository of Writing (CROW). Results show that the use of the RST relations of Explanation and Background, as well as the first-level PDTB sense of Contingency, are influenced by the English proficiency level of the writer.</abstract>
      <url hash="ec4b5fa1">2023.ranlp-1.52</url>
      <bibkey>hanel-kosseim-2023-discourse</bibkey>
    </paper>
    <paper id="53">
      <title>Improving Translation Quality for Low-Resource <fixed-case>I</fixed-case>nuktitut with Various Preprocessing Techniques</title>
      <author><first>Mathias Hans Erik</first><last>Stenlund</last></author>
      <author><first>Mathilde</first><last>Nanni</last></author>
      <author><first>Micaella</first><last>Bruton</last></author>
      <author><first>Meriem</first><last>Beloucif</last></author>
      <pages>475–479</pages>
      <abstract>Neural machine translation has been shown to outperform all other machine translation paradigms when trained in a high-resource setting. However, it still performs poorly when dealing with low-resource languages, for which parallel data for training is scarce. This is especially the case for morphologically complex languages such as Turkish, Tamil, Uyghur, etc. In this paper, we investigate various preprocessing methods for Inuktitut, a low-resource indigenous language from North America, without a morphological analyzer. On both the original and romanized scripts, we test various preprocessing techniques such as Byte-Pair Encoding, random stemming, and data augmentation using Hungarian for the Inuktitut-to-English translation task. We found that there are benefits to retaining the original script as it helps to achieve higher BLEU scores than the romanized models.</abstract>
      <url hash="b25d97e5">2023.ranlp-1.53</url>
      <bibkey>stenlund-etal-2023-improving</bibkey>
    </paper>
    <paper id="54">
      <title>Enriched Pre-trained Transformers for Joint Slot Filling and Intent Detection</title>
      <author><first>Momchil</first><last>Hardalov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>480–493</pages>
      <abstract>Detecting the user’s intent and finding the corresponding slots among the utterance’s words are important tasks in natural language understanding. Their interconnected nature makes their joint modeling a standard part of training such models. Moreover, data scarceness and specialized vocabularies pose additional challenges. Recently, the advances in pre-trained language models, namely contextualized models such as ELMo and BERT have revolutionized the field by tapping the potential of training very large models with just a few steps of fine-tuning on a task-specific dataset. Here, we leverage such models, and we design a novel architecture on top of them. Moreover, we propose an intent pooling attention mechanism, and we reinforce the slot filling task by fusing intent distributions, word features, and token representations. The experimental results on standard datasets show that our model outperforms both the current non-BERT state of the art as well as stronger BERT-based baselines.</abstract>
      <url hash="038219b4">2023.ranlp-1.54</url>
      <bibkey>hardalov-etal-2023-enriched</bibkey>
    </paper>
    <paper id="55">
      <title>Unimodal Intermediate Training for Multimodal Meme Sentiment Classification</title>
      <author><first>Muzhaffar</first><last>Hazman</last></author>
      <author><first>Susan</first><last>McKeever</last></author>
      <author><first>Josephine</first><last>Griffith</last></author>
      <pages>494–506</pages>
      <abstract>Internet Memes remain a challenging form of user-generated content for automated sentiment classification. The availability of labelled memes is a barrier to developing sentiment classifiers of multimodal memes. To address the shortage of labelled memes, we propose to supplement the training of a multimodal meme classifier with unimodal (image-only and text-only) data. In this work, we present a novel variant of supervised intermediate training that uses relatively abundant sentiment-labelled unimodal data. Our results show a statistically significant performance improvement from the incorporation of unimodal text data. Furthermore, we show that the training set of labelled memes can be reduced by 40% without reducing the performance of the downstream model.</abstract>
      <url hash="60e7774b">2023.ranlp-1.55</url>
      <bibkey>hazman-etal-2023-unimodal</bibkey>
    </paper>
    <paper id="56">
      <title>Explainable Event Detection with Event Trigger Identification as Rationale Extraction</title>
      <author><first>Hansi</first><last>Hettiarachchi</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <pages>507–518</pages>
      <abstract>Most event detection methods act at the sentence-level and focus on identifying sentences related to a particular event. However, identifying certain parts of a sentence that act as event triggers is also important and more challenging, especially when dealing with limited training data. Previous event detection attempts have considered these two tasks separately and have developed different methods. We hypothesise that similar to humans, successful sentence-level event detection models rely on event triggers to predict sentence-level labels. By exploring feature attribution methods that assign relevance scores to the inputs to explain model predictions, we study the behaviour of state-of-the-art sentence-level event detection models and show that explanations (i.e. rationales) extracted from these models can indeed be used to detect event triggers. We, therefore, (i) introduce a novel weakly-supervised method for event trigger detection; and (ii) propose to use event triggers as an explainable measure in sentence-level event detection. To the best of our knowledge, this is the first explainable machine learning approach to event trigger identification.</abstract>
      <url hash="7e745822">2023.ranlp-1.56</url>
      <bibkey>hettiarachchi-ranasinghe-2023-explainable</bibkey>
    </paper>
    <paper id="57">
      <title>Clinical Text Classification to <fixed-case>SNOMED</fixed-case> <fixed-case>CT</fixed-case> Codes Using Transformers Trained on Linked Open Medical Ontologies</title>
      <author><first>Anton</first><last>Hristov</last></author>
      <author><first>Petar</first><last>Ivanov</last></author>
      <author><first>Anna</first><last>Aksenova</last></author>
      <author><first>Tsvetan</first><last>Asamov</last></author>
      <author><first>Pavlin</first><last>Gyurov</last></author>
      <author><first>Todor</first><last>Primov</last></author>
      <author><first>Svetla</first><last>Boytcheva</last></author>
      <pages>519–526</pages>
      <abstract>We present an approach for medical text coding with SNOMED CT. Our approach uses publicly available linked open data from terminologies and ontologies as training data for the algorithms. We claim that even small training corpora made of short text snippets can be used to train models for the given task. We propose a method based on transformers enhanced with clustering and filtering of the candidates. Further, we adopt a classical machine learning approach - support vector classification (SVC) using transformer embeddings. The resulting approach proves to be more accurate than the predictions given by Large Language Models. We evaluate on a dataset generated from linked open data for SNOMED codes related to morphology and topography for four use cases. Our transformers-based approach achieves an F1-score of 0.82 for morphology and 0.99 for topography codes. Further, we validate the applicability of our approach in a clinical context using labelled real clinical data that are not used for model training.</abstract>
      <url hash="dbd6ce86">2023.ranlp-1.57</url>
      <bibkey>hristov-etal-2023-clinical</bibkey>
    </paper>
    <paper id="58">
      <title>Towards a Consensus Taxonomy for Annotating Errors in Automatically Generated Text</title>
      <author><first>Rudali</first><last>Huidrom</last></author>
      <author><first>Anya</first><last>Belz</last></author>
      <pages>527–540</pages>
      <abstract>Error analysis aims to provide insights into system errors at different levels of granularity. NLP as a field has a long-standing tradition of analysing and reporting errors which is generally considered good practice. There are existing error taxonomies tailored for different types of NLP task. In this paper, we report our work reviewing existing research on meaning/content error types in generated text, attempt to identify emerging consensus among existing meaning/content error taxonomies, and propose a standardised error taxonomy on this basis. We find that there is virtually complete agreement at the highest taxonomic level where errors of meaning/content divide into (1) Content Omission, (2) Content Addition, and (3) Content Substitution. Consensus in the lower levels is less pronounced, but a compact standardised consensus taxonomy can nevertheless be derived that works across generation tasks and application domains.</abstract>
      <url hash="b909a340">2023.ranlp-1.58</url>
      <bibkey>huidrom-belz-2023-towards</bibkey>
    </paper>
    <paper id="59">
      <title>Uncertainty Quantification of Text Classification in a Multi-Label Setting for Risk-Sensitive Systems</title>
      <author><first>Jinha</first><last>Hwang</last></author>
      <author><first>Carol</first><last>Gudumotu</last></author>
      <author><first>Benyamin</first><last>Ahmadnia</last></author>
      <pages>541–547</pages>
      <abstract>This paper addresses the challenge of uncertainty quantification in text classification for medical purposes and provides a three-fold approach to support robust and trustworthy decision-making by medical practitioners. Also, we address the challenge of imbalanced datasets in the medical domain by utilizing the Mondrian Conformal Predictor with a Naïve Bayes classifier.</abstract>
      <url hash="f950555d">2023.ranlp-1.59</url>
      <bibkey>hwang-etal-2023-uncertainty</bibkey>
    </paper>
    <paper id="60">
      <title>Pretraining Language- and Domain-Specific <fixed-case>BERT</fixed-case> on Automatically Translated Text</title>
      <author><first>Tatsuya</first><last>Ishigaki</last></author>
      <author><first>Yui</first><last>Uehara</last></author>
      <author><first>Goran</first><last>Topić</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <pages>548–555</pages>
      <abstract>Domain-specific pretrained language models such as SciBERT are effective for various tasks involving text in specific domains. However, pretraining BERT requires a large-scale language resource, which is not necessarily available in fine-grained domains, especially in non-English languages. In this study, we focus on a setting with no available domain-specific text for pretraining. To this end, we propose a simple framework that trains a BERT on text in the target language automatically translated from a resource-rich language, e.g., English. In this paper, we particularly focus on the materials science domain in Japanese. Our experiments pertain to the task of entity and relation extraction for this domain and language. The experiments demonstrate that the various models pretrained on translated texts consistently perform better than the general BERT in terms of F1 scores although the domain-specific BERTs do not use any human-authored domain-specific text. These results imply that BERTs for various low-resource domains can be successfully trained on texts automatically translated from resource-rich languages.</abstract>
      <url hash="6643375b">2023.ranlp-1.60</url>
      <bibkey>ishigaki-etal-2023-pretraining</bibkey>
    </paper>
    <paper id="61">
      <title>Categorising Fine-to-Coarse Grained Misinformation: An Empirical Study of the <fixed-case>COVID</fixed-case>-19 Infodemic</title>
      <author><first>Ye</first><last>Jiang</last></author>
      <author><first>Xingyi</first><last>Song</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Iknoor</first><last>Singh</last></author>
      <author><first>Ahmet</first><last>Aker</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <pages>556–567</pages>
      <abstract>The spread of COVID-19 misinformation on social media became a major challenge for citizens, with negative real-life consequences. Prior research focused on detection and/or analysis of COVID-19 misinformation. However, fine-grained classification of misinformation claims has been largely overlooked. The novel contribution of this paper is in introducing a new dataset which makes fine-grained distinctions between statements that assert, comment or question on false COVID-19 claims. This new dataset not only enables social behaviour analysis but also enables us to address both evidence-based and non-evidence-based misinformation classification tasks. Lastly, through leave claim out cross-validation, we demonstrate that classifier performance on unseen COVID-19 misinformation claims is significantly different, as compared to performance on topics present in the training data.</abstract>
      <url hash="0472d5a0">2023.ranlp-1.61</url>
      <bibkey>jiang-etal-2023-categorising</bibkey>
    </paper>
    <paper id="62">
      <title>Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models</title>
      <author><first>Shun</first><last>Kiyono</last></author>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Shengzhe</first><last>Li</last></author>
      <author><first>Toshinori</first><last>Sato</last></author>
      <pages>568–577</pages>
      <abstract>Pretrained language models require the use of consistent segmentation (e.g., subword- or character-level segmentation) in pretraining and finetuning. In NLP, many tasks are modeled by subword-level segmentation better than by character-level segmentation. However, because of their format, several tasks require the use of character-level segmentation. Thus, in order to tackle both types of NLP tasks, language models must be independently pretrained for both subword and character-level segmentation. However, this is an inefficient and costly procedure. Instead, this paper proposes a method for training a language model with unified segmentation. This means that the trained model can be finetuned on both subword- and character-level segmentation. The principle of the method is to apply the subword regularization technique to generate a mixture of subword- and character-level segmentation. Through experiment on BERT models, we demonstrate that our method can halve the computational cost of pretraining.</abstract>
      <url hash="4310450d">2023.ranlp-1.62</url>
      <bibkey>kiyono-etal-2023-bridging</bibkey>
    </paper>
    <paper id="63">
      <title>Evaluating Data Augmentation for Medication Identification in Clinical Notes</title>
      <author><first>Jordan</first><last>Koontz</last></author>
      <author><first>Maite</first><last>Oronoz</last></author>
      <author><first>Alicia</first><last>Pérez</last></author>
      <pages>578–585</pages>
      <abstract>We evaluate the effectiveness of using data augmentation to improve the generalizability of a Named Entity Recognition model for the task of medication identification in clinical notes. We compare disparate data augmentation methods, namely mention-replacement and a generative model, for creating synthetic training examples. Through experiments on the n2c2 2022 Track 1 Contextualized Medication Event Extraction data set, we show that data augmentation with supplemental examples created with GPT-3 can boost the performance of a transformer-based model for small training sets.</abstract>
      <url hash="92433dd4">2023.ranlp-1.63</url>
      <bibkey>koontz-etal-2023-evaluating</bibkey>
    </paper>
    <paper id="64">
      <title>Advancing Topical Text Classification: A Novel Distance-Based Method with Contextual Embeddings</title>
      <author><first>Andriy</first><last>Kosar</last></author>
      <author><first>Guy</first><last>De Pauw</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>586–597</pages>
      <abstract>This study introduces a new method for distance-based unsupervised topical text classification using contextual embeddings. The method applies and tailors sentence embeddings for distance-based topical text classification. This is achieved by leveraging the semantic similarity between topic labels and text content, and reinforcing the relationship between them in a shared semantic space. The proposed method outperforms a wide range of existing sentence embeddings on average by 35%. Presenting an alternative to the commonly used transformer-based zero-shot general-purpose classifiers for multiclass text classification, the method demonstrates significant advantages in terms of computational efficiency and flexibility, while maintaining comparable or improved classification results.</abstract>
      <url hash="b731f642">2023.ranlp-1.64</url>
      <bibkey>kosar-etal-2023-advancing</bibkey>
    </paper>
    <paper id="65">
      <title>Taxonomy-Based Automation of Prior Approval Using Clinical Guidelines</title>
      <author><first>Saranya</first><last>Krishnamoorthy</last></author>
      <author><first>Ayush</first><last>Singh</last></author>
      <pages>598–607</pages>
      <abstract>Performing prior authorization on patients in a medical facility is a time-consuming and challenging task for insurance companies. Automating the clinical decisions that lead to authorization can reduce the time that staff spend executing such procedures. To better facilitate such critical decision making, we present an automated approach to predict one of the challenging tasks in the process called primary clinical indicator prediction, which is the outcome of this procedure. The proposed solution is to create a taxonomy to capture the main categories in primary clinical indicators. Our approach involves an important step of selecting what is known as the “primary indicator” – one of the several heuristics based on clinical guidelines that are published and publicly available. A taxonomy based PI classification system was created to help in the recognition of PIs from free text in electronic health records (EHRs). This taxonomy includes comprehensive explanations of each PI, as well as examples of free text that could be used to detect each PI. The major contribution of this work is to introduce a taxonomy created by three professional nurses with many years of experience. We experiment with several state-of-the-art supervised and unsupervised techniques with a focus on prior approval for spinal imaging. The results indicate that the proposed taxonomy is capable of increasing the performance of unsupervised approaches by up to 10 F1 points. Further, in the supervised setting, we achieve an F1 score of 0.61 using a conventional technique based on term frequency–inverse document frequency that outperforms other deep-learning approaches.</abstract>
      <url hash="78d8aec9">2023.ranlp-1.65</url>
      <bibkey>krishnamoorthy-singh-2023-taxonomy</bibkey>
    </paper>
    <paper id="66">
      <title>Simultaneous Interpreting as a Noisy Channel: How Much Information Gets Through</title>
      <author><first>Maria</first><last>Kunilovskaya</last></author>
      <author><first>Heike</first><last>Przybyl</last></author>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Elke</first><last>Teich</last></author>
      <pages>608–618</pages>
      <abstract>We explore the relationship between information density/surprisal of source and target texts in translation and interpreting in the language pair English-German, looking at the specific properties of translation (“translationese”). Our data comes from two bidirectional English-German subcorpora representing written and spoken mediation modes collected from European Parliament proceedings. Within each language, we (a) compare original speeches to their translated or interpreted counterparts, and (b) explore the association between segment-aligned sources and targets in each translation direction. As additional variables, we consider source delivery mode (read-out, impromptu) and source speech rate in interpreting. We use language modelling to measure the information rendered by words in a segment and to characterise the cross-lingual transfer of information under various conditions. Our approach is based on statistical analyses of surprisal values, extracted from n-gram models of our dataset. The analysis reveals that while there is a considerable positive correlation between the average surprisal of source and target segments in both modes, information output in interpreting is lower than in translation, given the same amount of input. Significantly lower information density in spoken mediated production compared to non-mediated speech in the same language can indicate a possible simplification effect in interpreting.</abstract>
      <url hash="f74d94d7">2023.ranlp-1.66</url>
      <bibkey>kunilovskaya-etal-2023-simultaneous</bibkey>
    </paper>
    <paper id="67">
      <title>Challenges of <fixed-case>GPT</fixed-case>-3-Based Conversational Agents for Healthcare</title>
      <author><first>Fabian</first><last>Lechner</last></author>
      <author><first>Allison</first><last>Lahnala</last></author>
      <author><first>Charles</first><last>Welch</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>619–630</pages>
      <abstract>The potential of medical domain dialogue agents lies in their ability to provide patients with faster information access while enabling medical specialists to concentrate on critical tasks. However, the integration of large-language models (LLMs) into these agents presents certain limitations that may result in serious consequences. This paper investigates the challenges and risks of using GPT-3-based models for medical question-answering (MedQA). We perform several evaluations contextualized in terms of standard medical principles. We provide a procedure for manually designing patient queries to stress-test high-risk limitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to respond adequately to these queries, generating erroneous medical information, unsafe recommendations, and content that may be considered offensive.</abstract>
      <url hash="7cc051b5">2023.ranlp-1.67</url>
      <bibkey>lechner-etal-2023-challenges</bibkey>
    </paper>
    <paper id="68">
      <title>Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks</title>
      <author><first>João</first><last>Leite</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Diego</first><last>Silva</last></author>
      <pages>631–640</pages>
      <abstract>Online social media is rife with offensive and hateful comments, prompting the need for their automatic detection given the sheer amount of posts created every second. Creating high-quality human-labelled datasets for this task is difficult and costly, especially because non-offensive posts are significantly more frequent than offensive ones. However, unlabelled data is abundant, easier, and cheaper to obtain. In this scenario, self-training methods, using weakly-labelled examples to increase the amount of training data, can be employed. Recent “noisy” self-training approaches incorporate data augmentation techniques to ensure prediction consistency and increase robustness against noisy data and adversarial attacks. In this paper, we experiment with default and noisy self-training using three different textual data augmentation techniques across five different pre-trained BERT architectures varying in size. We evaluate our experiments on two offensive/hate-speech datasets and demonstrate that (i) self-training consistently improves performance regardless of model size, resulting in up to +1.5% F1-macro on both datasets, and (ii) noisy self-training with textual data augmentations, despite being successfully applied in similar settings, decreases performance on offensive and hate-speech domains when compared to the default method, even with state-of-the-art augmentations such as backtranslation.</abstract>
      <url hash="21294342">2023.ranlp-1.68</url>
      <bibkey>leite-etal-2023-noisy</bibkey>
    </paper>
    <paper id="69">
      <title>A Practical Survey on Zero-Shot Prompt Design for In-Context Learning</title>
      <author><first>Yinheng</first><last>Li</last></author>
      <pages>641–647</pages>
      <abstract>The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, few-shot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single “best” prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, optimization techniques, and rigorous evaluation for more effective and efficient use of LLMs in various NLP tasks.</abstract>
      <url hash="06a8f0e9">2023.ranlp-1.69</url>
      <bibkey>li-2023-practical</bibkey>
    </paper>
    <paper id="70">
      <title>Classifying <fixed-case>COVID</fixed-case>-19 Vaccine Narratives</title>
      <author><first>Yue</first><last>Li</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Xingyi</first><last>Song</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <pages>648–657</pages>
      <abstract>Vaccine hesitancy is widespread, despite the government’s information campaigns and the efforts of the World Health Organisation (WHO). Categorising the topics within vaccine-related narratives is crucial to understand the concerns expressed in discussions and identify the specific issues that contribute to vaccine hesitancy. This paper addresses the need for monitoring and analysing vaccine narratives online by introducing a novel vaccine narrative classification task, which categorises COVID-19 vaccine claims into one of seven categories. Following a data augmentation approach, we first construct a novel dataset for this new classification task, focusing on the minority classes. We also make use of fact-checker annotated data. The paper also presents a neural vaccine narrative classifier that achieves an accuracy of 84% under cross-validation. The classifier is publicly available for researchers and journalists.</abstract>
      <url hash="0b019be5">2023.ranlp-1.70</url>
      <bibkey>li-etal-2023-classifying</bibkey>
    </paper>
    <paper id="71">
      <title>Sign Language Recognition and Translation: A Multi-Modal Approach Using Computer Vision and Natural Language Processing</title>
      <author><first>Jacky</first><last>Li</last></author>
      <author><first>Jaren</first><last>Gerdes</last></author>
      <author><first>James</first><last>Gojit</last></author>
      <author><first>Austin</first><last>Tao</last></author>
      <author><first>Samyak</first><last>Katke</last></author>
      <author><first>Kate</first><last>Nguyen</last></author>
      <author><first>Benyamin</first><last>Ahmadnia</last></author>
      <pages>658–665</pages>
      <abstract>Sign-to-Text (S2T) is a hand gesture recognition program in the American Sign Language (ASL) domain. The primary objective of S2T is to classify standard ASL alphabets and custom signs and convert the classifications into a stream of text using neural networks. This paper addresses the shortcomings of pure Computer Vision techniques and applies Natural Language Processing (NLP) as an additional layer of complexity to increase S2T’s robustness.</abstract>
      <url hash="b053ef40">2023.ranlp-1.71</url>
      <bibkey>li-etal-2023-sign</bibkey>
    </paper>
    <paper id="72">
      <title>Classification-Aware Neural Topic Model Combined with Interpretable Analysis - for Conflict Classification</title>
      <author><first>Tianyu</first><last>Liang</last></author>
      <author><first>Yida</first><last>Mu</last></author>
      <author><first>Soonho</first><last>Kim</last></author>
      <author><first>Darline</first><last>Kuate</last></author>
      <author><first>Julie</first><last>Lang</last></author>
      <author><first>Rob</first><last>Vos</last></author>
      <author><first>Xingyi</first><last>Song</last></author>
      <pages>666–672</pages>
      <abstract>A large number of conflict events are affecting the world all the time. In order to analyse such conflict events effectively, this paper presents a Classification-Aware Neural Topic Model (CANTM-IA) for Conflict Information Classification and Topic Discovery. The model provides a reliable interpretation of classification results and discovered topics by introducing interpretability analysis. At the same time, interpretation is introduced into the model architecture to improve the classification performance of the model and to allow interpretation to focus further on the details of the data. Finally, the model architecture is optimised to reduce the complexity of the model.</abstract>
      <url hash="9cc345fc">2023.ranlp-1.72</url>
      <bibkey>liang-etal-2023-classification</bibkey>
    </paper>
    <paper id="73">
      <title>Data Augmentation for Fake Reviews Detection</title>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>673–680</pages>
      <abstract>In this research, we studied the relationship between data augmentation and model accuracy for the task of fake review detection. We used data generation methods to augment two different fake review datasets and compared the performance of models trained with the original data and with the augmented data. Our results show that the accuracy of our fake review detection model can be improved by 0.31 percentage points on DeRev Test and by 7.65 percentage points on Amazon Test by using the augmented datasets.</abstract>
      <url hash="5ae02e66">2023.ranlp-1.73</url>
      <bibkey>liu-poesio-2023-data</bibkey>
    </paper>
    <paper id="74">
      <title>Coherent Story Generation with Structured Knowledge</title>
      <author><first>Congda</first><last>Ma</last></author>
      <author><first>Kotaro</first><last>Funakoshi</last></author>
      <author><first>Kiyoaki</first><last>Shirai</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>681–690</pages>
      <abstract>The emergence of pre-trained language models has taken story generation, which is the task of automatically generating a comprehensible story from limited information, to a new stage. Although generated stories from the language models are fluent and grammatically correct, the lack of coherence affects their quality. We propose a knowledge-based multi-stage model that incorporates the schema, a kind of structured knowledge, to guide coherent story generation. Our framework includes a schema acquisition module, a plot generation module, and a surface realization module. In the schema acquisition module, high-relevant structured knowledge pieces are selected as a schema. In the plot generation module, a coherent plot plan is navigated by the schema. In the surface realization module, conditioned by the generated plot, a story is generated. Evaluations show that our methods can generate more comprehensible stories than strong baselines, especially with higher global coherence and less repetition.</abstract>
      <url hash="0302b155">2023.ranlp-1.74</url>
      <bibkey>ma-etal-2023-coherent</bibkey>
    </paper>
    <paper id="75">
      <title>Studying Common Ground Instantiation Using Audio, Video and Brain Behaviours: The <fixed-case>B</fixed-case>rain<fixed-case>KT</fixed-case> Corpus</title>
      <author><first>Eliot</first><last>Maës</last></author>
      <author><first>Thierry</first><last>Legou</last></author>
      <author><first>Leonor</first><last>Becerra</last></author>
      <author><first>Philippe</first><last>Blache</last></author>
      <pages>691–702</pages>
      <abstract>An increasing amount of multimodal recordings has been paving the way for the development of a more automatic way to study language and conversational interactions. However this data largely comprises of audio and video recordings, leaving aside other modalities that might complement this external view of the conversation but might be more difficult to collect in naturalistic setups, such as participants brain activity. In this context, we present BrainKT, a natural conversational corpus with audio, video and neuro-physiological signals, collected with the aim of studying information exchanges and common ground instantiation in conversation in a new, more in-depth way. We recorded conversations from 28 dyads (56 participants) during 30 minutes experiments where subjects were first tasked to collaborate on a joint information game, then freely drifted to the topic of their choice. During each session, audio and video were captured, along with the participants’ neural signal (EEG with Biosemi 64) and their electro-physiological activity (with Empatica-E4). The paper situates this new type of resources in the literature, presents the experimental setup and describes the different kinds of annotations considered for the corpus.</abstract>
      <url hash="5cbb72c3">2023.ranlp-1.75</url>
      <bibkey>maes-etal-2023-studying</bibkey>
    </paper>
    <paper id="76">
      <title>Reading between the Lines: Information Extraction from Industry Requirements</title>
      <author><first>Ole Magnus</first><last>Holter</last></author>
      <author><first>Basil</first><last>Ell</last></author>
      <pages>703–711</pages>
      <abstract>Industry requirements describe the qualities that a project or a service must provide. Most requirements are, however, only available in natural language format and are embedded in textual documents. To be machine-understandable, a requirement needs to be represented in a logical format. We consider that a requirement consists of a scope, which is the requirement’s subject matter, a condition, which is any condition that must be fulfilled for the requirement to be relevant, and a demand, which is what is required. We introduce a novel task, the identification of the semantic components scope, condition, and demand in a requirement sentence, and establish baselines using sequence labelling and few-shot learning. One major challenge with this task is the implicit nature of the scope, often not stated in the sentence. By including document context information, we improved the average performance for scope detection. Our study provides insights into the difficulty of machine understanding of industry requirements and suggests strategies for addressing this challenge.</abstract>
      <url hash="021ae308">2023.ranlp-1.76</url>
      <bibkey>holter-ell-2023-reading</bibkey>
    </paper>
    <paper id="77">
      <title>Transformer-Based Language Models for <fixed-case>B</fixed-case>ulgarian</title>
      <author><first>Iva</first><last>Marinova</last></author>
      <author><first>Kiril</first><last>Simov</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <pages>712–720</pages>
      <abstract>This paper presents an approach for training lightweight and robust language models for Bulgarian that mitigate gender, political, racial, and other biases in the data. Our method involves scraping content from major Bulgarian online media providers using a specialized procedure for source filtering, topic selection, and lexicon-based removal of inappropriate language during the pre-training phase. We continuously improve the models by incorporating new data from various domains, including social media, books, scientific literature, and linguistically modified corpora. Our motivation is to provide a solution that is sufficient for all natural language processing tasks in Bulgarian, and to address the lack of existing procedures for guaranteeing the robustness of such models.</abstract>
      <url hash="1ba59200">2023.ranlp-1.77</url>
      <bibkey>marinova-etal-2023-transformer</bibkey>
    </paper>
    <paper id="78">
      <title>Multi-task Ensemble Learning for Fake Reviews Detection and Helpfulness Prediction: A Novel Approach</title>
      <author><first>Alimuddin</first><last>Melleng</last></author>
      <author><first>Anna</first><last>Jurek-Loughrey</last></author>
      <author><first>Deepak</first><last>P</last></author>
      <pages>721–729</pages>
      <abstract>Research on fake reviews detection and review helpfulness prediction is prevalent, yet most studies tend to focus solely on either fake reviews detection or review helpfulness prediction, considering them separate research tasks. In contrast to this prevailing pattern, we address both challenges concurrently by employing a multi-task learning approach. We posit that undertaking these tasks simultaneously can enhance the performance of each task through shared information among features. We utilize pre-trained RoBERTa embeddings with a document-level data representation. This is coupled with an array of deep learning and neural network models, including Bi-LSTM, LSTM, GRU, and CNN. Additionally, we em- ploy ensemble learning techniques to integrate these models, with the objective of enhancing overall prediction accuracy and mitigating the risk of overfitting. The findings of this study offer valuable insights to the fields of natural language processing and machine learning and present a novel perspective on leveraging multi-task learning for the twin challenges of fake reviews detection and review helpfulness prediction</abstract>
      <url hash="3189bb5f">2023.ranlp-1.78</url>
      <bibkey>melleng-etal-2023-multi</bibkey>
    </paper>
    <paper id="79">
      <title>Data Fusion for Better Fake Reviews Detection</title>
      <author><first>Alimuddin</first><last>Melleng</last></author>
      <author><first>Anna</first><last>Jurek-Loughrey</last></author>
      <author><first>Deepak</first><last>P</last></author>
      <pages>730–738</pages>
      <abstract>Online reviews have become critical in informing purchasing decisions, making the detection of fake reviews a crucial challenge to tackle. Many different Machine Learning based solutions have been proposed, using various data representations such as n-grams or document embeddings. In this paper, we first explore the effectiveness of different data representations, including emotion, document embedding, n-grams, and noun phrases in embedding for mat, for fake reviews detection. We evaluate these representations with various state-of-the-art deep learning models, such as BILSTM, LSTM, GRU, CNN, and MLP. Following this, we propose to incorporate different data repre- sentations and classification models using early and late data fusion techniques in order to im- prove the prediction performance. The experiments are conducted on four datasets: Hotel, Restaurant, Amazon, and Yelp. The results demonstrate that combination of different data representations significantly outperform any of the single data representations</abstract>
      <url hash="5198f5fe">2023.ranlp-1.79</url>
      <bibkey>melleng-etal-2023-data</bibkey>
    </paper>
    <paper id="80">
      <title>Dimensions of Quality: Contrasting Stylistic vs. Semantic Features for Modelling Literary Quality in 9,000 Novels</title>
      <author><first>Pascale</first><last>Moreira</last></author>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <pages>739–747</pages>
      <abstract>In computational literary studies, the challenging task of predicting quality or reader-appreciation of narrative texts is confounded by volatile definitions of quality and the vast feature space that may be considered in modeling. In this paper, we explore two different types of feature sets: stylistic features on one hand, and semantic features on the other. We conduct experiments on a corpus of 9,089 English language literary novels published in the 19th and 20th century, using GoodReads’ ratings as a proxy for reader-appreciation. Examining the potential of both approaches, we find that some types of books are more predictable in one model than in the other, which may indicate that texts have different prominent characteristics (stylistic complexity, a certain narrative progression at the sentiment-level).</abstract>
      <url hash="18363835">2023.ranlp-1.80</url>
      <bibkey>moreira-bizzoni-2023-dimensions</bibkey>
    </paper>
    <paper id="81">
      <title><fixed-case>B</fixed-case>angla<fixed-case>B</fixed-case>ait: Semi-Supervised Adversarial Approach for Clickbait Detection on <fixed-case>B</fixed-case>angla Clickbait Dataset</title>
      <author><first>Md. Motahar</first><last>Mahtab</last></author>
      <author><first>Monirul</first><last>Haque</last></author>
      <author><first>Mehedi</first><last>Hasan</last></author>
      <author><first>Farig</first><last>Sadeque</last></author>
      <pages>748–758</pages>
      <abstract>Intentionally luring readers to click on a particular content by exploiting their curiosity defines a title as clickbait. Although several studies focused on detecting clickbait titles in English articles, low-resource language like Bangla has not been given adequate attention. To tackle clickbait titles in Bangla, we have constructed the first Bangla clickbait detection dataset containing 15,056 labeled news articles and 65,406 unlabelled news articles extracted from clickbait-dense news sites. Each article has been labeled by three expert linguists and includes an article’s title, body, and other metadata. By incorporating labeled and unlabelled data, we finetune a pre-trained Bangla transformer model in an adversarial fashion using Semi-Supervised Generative Adversarial Networks (SS-GANs). The proposed model acts as a good baseline for this dataset, outperforming traditional neural network models (LSTM, GRU, CNN) and linguistic feature-based models. We expect that this dataset and the detailed analysis and comparison of these clickbait detection models will provide a fundamental basis for future research into detecting clickbait titles in Bengali articles.</abstract>
      <url hash="eeae1388">2023.ranlp-1.81</url>
      <bibkey>mahtab-etal-2023-banglabait</bibkey>
    </paper>
    <paper id="82">
      <title><fixed-case>T</fixed-case>ree<fixed-case>S</fixed-case>wap: Data Augmentation for Machine Translation via Dependency Subtree Swapping</title>
      <author><first>Attila</first><last>Nagy</last></author>
      <author><first>Dorina</first><last>Lakatos</last></author>
      <author><first>Botond</first><last>Barta</last></author>
      <author><first>Judit</first><last>Ács</last></author>
      <pages>759–768</pages>
      <abstract>Data augmentation methods for neural machine translation are particularly useful when limited amount of training data is available, which is often the case when dealing with low-resource languages. We introduce a novel augmentation method, which generates new sentences by swapping objects and subjects across bisentences. This is performed simultaneously based on the dependency parse trees of the source and target sentences. We name this method TreeSwap. Our results show that TreeSwap achieves consistent improvements over baseline models in 4 language pairs in both directions on resource-constrained datasets. We also explore domain-specific corpora, but find that our method does not make significant improvements on law, medical and IT data. We report the scores of similar augmentation methods and find that TreeSwap performs comparably. We also analyze the generated sentences qualitatively and find that the augmentation produces a correct translation in most cases. Our code is available on Github.</abstract>
      <url hash="9e4636a6">2023.ranlp-1.82</url>
      <bibkey>nagy-etal-2023-treeswap</bibkey>
    </paper>
    <paper id="83">
      <title>Automatic Assessment Of Spoken <fixed-case>E</fixed-case>nglish Proficiency Based on Multimodal and Multitask Transformers</title>
      <author><first>Kamel</first><last>Nebhi</last></author>
      <author><first>György</first><last>Szaszák</last></author>
      <pages>769–776</pages>
      <abstract>This paper describes technology developed to automatically grade students on their English spontaneous spoken language proficiency with common european framework of reference for languages (CEFR) level. Our automated assessment system contains two tasks: elicited imitation and spontaneous speech assessment. Spontaneous speech assessment is a challenging task that requires evaluating various aspects of speech quality, content, and coherence. In this paper, we propose a multimodal and multitask transformer model that leverages both audio and text features to perform three tasks: scoring, coherence modeling, and prompt relevancy scoring. Our model uses a fusion of multiple features and multiple modality attention to capture the interactions between audio and text modalities and learn from different sources of information.</abstract>
      <url hash="cc854ba7">2023.ranlp-1.83</url>
      <bibkey>nebhi-szaszak-2023-automatic</bibkey>
    </paper>
    <paper id="84">
      <title>Medical Concept Mention Identification in Social Media Posts Using a Small Number of Sample References</title>
      <author><first>Vasudevan</first><last>Nedumpozhimana</last></author>
      <author><first>Sneha</first><last>Rautmare</last></author>
      <author><first>Meegan</first><last>Gower</last></author>
      <author><first>Nishtha</first><last>Jain</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Patricia</first><last>Buffini</last></author>
      <author><first>John</first><last>Kelleher</last></author>
      <pages>777–784</pages>
      <abstract>Identification of mentions of medical concepts in social media text can provide useful information for caseload prediction of diseases like Covid-19 and Measles. We propose a simple model for the automatic identification of the medical concept mentions in the social media text. We validate the effectiveness of the proposed model on Twitter, Reddit, and News/Media datasets.</abstract>
      <url hash="afbc9dcf">2023.ranlp-1.84</url>
      <bibkey>nedumpozhimana-etal-2023-medical</bibkey>
    </paper>
    <paper id="85">
      <title>Context-Aware Module Selection in Modular Dialog Systems</title>
      <author><first>Jan</first><last>Nehring</last></author>
      <author><first>René Marcel</first><last>Berk</last></author>
      <author><first>Stefan</first><last>Hillmann</last></author>
      <pages>785–791</pages>
      <abstract>In modular dialog systems, a dialog system consists of multiple conversational agents. The task “module selection” selects the appropriate sub-dialog system for an incoming user utterance. Current models for module selection use features derived from the current user turn only, such as the utterances text or confidence values of the natural language understanding systems of the individual conversational agents, or they perform text classification on the user utterance. However, dialogs often span multiple turns, and turns are embedded into a context. Therefore, looking at the current user turn only is a source of error in certain situations. This work proposes four models for module selection that include the dialog history and the current user turn into module selection. We show that these models surpass the current state of the art in module selection.</abstract>
      <url hash="156d6351">2023.ranlp-1.85</url>
      <bibkey>nehring-etal-2023-context</bibkey>
    </paper>
    <paper id="86">
      <title>Human Value Detection from Bilingual Sensory Product Reviews</title>
      <author><first>Boyu</first><last>Niu</last></author>
      <author><first>Céline</first><last>Manetta</last></author>
      <author><first>Frédérique</first><last>Segond</last></author>
      <pages>792–802</pages>
      <abstract>We applied text classification methods on a corpus of product reviews we created with the help of a questionnaire. We found that for certain values, “traditional” deep neural networks like CNN can give promising results compared to the baseline. We propose some ideas to improve the results in the future. The bilingual corpus we created which contains more than 16 000 consumer reviews associated to the human value profile of the authors can be used for different marketing purposes.</abstract>
      <url hash="9f9f60f8">2023.ranlp-1.86</url>
      <bibkey>niu-etal-2023-human</bibkey>
    </paper>
    <paper id="87">
      <title>Word Sense Disambiguation for Automatic Translation of Medical Dialogues into Pictographs</title>
      <author><first>Magali</first><last>Norré</last></author>
      <author><first>Rémi</first><last>Cardon</last></author>
      <author><first>Vincent</first><last>Vandeghinste</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <pages>803–812</pages>
      <abstract>Word sense disambiguation is an NLP task embedded in different applications. We propose to evaluate its contribution to the automatic translation of French texts into pictographs, in the context of communication between doctors and patients with an intellectual disability. Different general and/or medical language models (Word2Vec, fastText, CamemBERT, FlauBERT, DrBERT, and CamemBERT-bio) are tested in order to choose semantically correct pictographs leveraging the synsets in the French WordNets (WOLF and WoNeF). The results of our automatic evaluations show that our method based on Word2Vec and fastText significantly improves the precision of medical translations into pictographs. We also present an evaluation corpus adapted to this task.</abstract>
      <url hash="dbf9b125">2023.ranlp-1.87</url>
      <bibkey>norre-etal-2023-word</bibkey>
    </paper>
    <paper id="88">
      <title>A Research-Based Guide for the Creation and Deployment of a Low-Resource Machine Translation System</title>
      <author><first>John E.</first><last>Ortega</last></author>
      <author><first>Kenneth</first><last>Church</last></author>
      <pages>813–823</pages>
      <abstract>The machine translation (MT) field seems to focus heavily on English and other high-resource languages. Though, low-resource MT (LRMT) is receiving more attention than in the past. Successful LRMT systems (LRMTS) should make a compelling business case in terms of demand, cost and quality in order to be viable for end users. When used by communities where low-resource languages are spoken, LRMT quality should not only be determined by the use of traditional metrics like BLEU, but it should also take into account other factors in order to be inclusive and not risk overall rejection by the community. MT systems based on neural methods tend to perform better with high volumes of training data, but they may be unrealistic and even harmful for LRMT. It is obvious that for research purposes, the development and creation of LRMTS is necessary. However, in this article, we argue that two main workarounds could be considered by companies that are considering deployment of LRMTS in the wild: human-in-the-loop and sub-domains.</abstract>
      <url hash="e42e9b6e">2023.ranlp-1.88</url>
      <bibkey>ortega-church-2023-research</bibkey>
    </paper>
    <paper id="89">
      <title><fixed-case>MQDD</fixed-case>: Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain</title>
      <author><first>Jan</first><last>Pasek</last></author>
      <author><first>Jakub</first><last>Sido</last></author>
      <author><first>Miloslav</first><last>Konopik</last></author>
      <author><first>Ondrej</first><last>Prazak</last></author>
      <pages>824–835</pages>
      <abstract>This work proposes a new pipeline for leveraging data collected on the Stack Overflow website for pre-training a multimodal model for searching duplicates on question answering websites. Our multimodal model is trained on question descriptions and source codes in multiple programming languages. We design two new learning objectives to improve duplicate detection capabilities. The result of this work is a mature, fine-tuned Multimodal Question Duplicity Detection (MQDD) model, ready to be integrated into a Stack Overflow search system, where it can help users find answers for already answered questions. Alongside the MQDD model, we release two datasets related to the software engineering domain. The first Stack Overflow Dataset (SOD) represents a massive corpus of paired questions and answers. The second Stack Overflow Duplicity Dataset (SODD) contains data for training duplicate detection models.</abstract>
      <url hash="17a9f6eb">2023.ranlp-1.89</url>
      <bibkey>pasek-etal-2023-mqdd</bibkey>
    </paper>
    <paper id="90">
      <title>Forming Trees with Treeformers</title>
      <author><first>Nilay</first><last>Patel</last></author>
      <author><first>Jeffrey</first><last>Flanigan</last></author>
      <pages>836–845</pages>
      <abstract>Human language is known to exhibit a nested, hierarchical structure, allowing us to form complex sentences out of smaller pieces. However, many state-of-the-art neural networks models such as Transformers have no explicit hierarchical structure in their architecture—that is, they don’t have an inductive bias toward hierarchical structure. Additionally, Transformers are known to perform poorly on compositional generalization tasks which require such structures. In this paper, we introduce Treeformer, a general-purpose encoder module inspired by the CKY algorithm which learns a composition operator and pooling function to construct hierarchical encodings for phrases and sentences. Our extensive experiments demonstrate the benefits of incorporating hierarchical structure into the Transformer and show significant improvements in compositional generalization as well as in downstream tasks such as machine translation, abstractive summarization, and various natural language understanding tasks.</abstract>
      <url hash="b3dd67f1">2023.ranlp-1.90</url>
      <bibkey>patel-flanigan-2023-forming</bibkey>
    </paper>
    <paper id="91">
      <title>Evaluating Unsupervised Hierarchical Topic Models Using a Labeled Dataset</title>
      <author><first>Judicael</first><last>Poumay</last></author>
      <author><first>Ashwin</first><last>Ittoo</last></author>
      <pages>846–853</pages>
      <abstract>Topic modeling is a commonly used method for identifying and extracting topics from a corpus of documents. While several evaluation techniques, such as perplexity and topic coherence, have been developed to assess the quality of extracted topics, they fail to determine whether all topics have been identified and to what extent they have been represented. Additionally, hierarchical topic models have been proposed, but the quality of the hierarchy produced has not been adequately evaluated. This study proposes a novel approach to evaluating topic models that supplements existing methods. Using a labeled dataset, we trained hierarchical topic models in an unsupervised manner and used the known labels to evaluate the accuracy of the results. Our findings indicate that labels encompassing a substantial number of documents achieve high accuracy of over 70%. Although there are 90 labels in the dataset, labels that cover only 1% of the data still achieve an average accuracy of 37.9%, demonstrating the effectiveness of hierarchical topic models even on smaller subsets. Furthermore, we demonstrate that these labels can be used to assess the quality of the topic tree and confirm that hierarchical topic models produce coherent taxonomies for the labels.</abstract>
      <url hash="b280dc13">2023.ranlp-1.91</url>
      <bibkey>poumay-ittoo-2023-evaluating</bibkey>
    </paper>
    <paper id="92">
      <title><fixed-case>HTMOT</fixed-case>: Hierarchical Topic Modelling over Time</title>
      <author><first>Judicael</first><last>Poumay</last></author>
      <author><first>Ashwin</first><last>Ittoo</last></author>
      <pages>854–863</pages>
      <abstract>Topic models provide an efficient way of extracting insights from text and supporting decision-making. Recently, novel methods have been proposed to model topic hierarchy or temporality. Modeling temporality provides more precise topics by separating topics that are characterized by similar words but located over distinct time periods. Conversely, modeling hierarchy provides a more detailed view of the content of a corpus by providing topics and sub-topics. However, no models have been proposed to incorporate both hierarchy and temporality which could be beneficial for applications such as environment scanning. Therefore, we propose a novel method to perform Hierarchical Topic Modelling Over Time (HTMOT). We evaluate the performance of our approach on a corpus of news articles using the Word Intrusion task. Results demonstrate that our model produces topics that elegantly combine a hierarchical structure and a temporal aspect. Furthermore, our proposed Gibbs sampling implementation shows competitive performance compared to previous state-of-the-art methods.</abstract>
      <url hash="6ee5f492">2023.ranlp-1.92</url>
      <bibkey>poumay-ittoo-2023-htmot</bibkey>
    </paper>
    <paper id="93">
      <title>Multilingual Continual Learning Approaches for Text Classification</title>
      <author><first>Karan</first><last>Praharaj</last></author>
      <author><first>Irina</first><last>Matveeva</last></author>
      <pages>864–870</pages>
      <abstract>Multilingual continual learning is important for models that are designed to be deployed over long periods of time and are required to be updated when new data becomes available. Such models are continually applied to new unseen data that can be in any of the supported languages. One challenge in this scenario is to ensure consistent performance of the model throughout the deployment lifecycle, beginning from the moment of first deployment. We empirically assess the strengths and shortcomings of some continual learning methods in a multilingual setting across two tasks.</abstract>
      <url hash="16226663">2023.ranlp-1.93</url>
      <bibkey>praharaj-matveeva-2023-multilingual</bibkey>
    </paper>
    <paper id="94">
      <title>Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study</title>
      <author><first>Damith</first><last>Premasiri</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>871–878</pages>
      <abstract>Text classification is an area of research which has been studied over the years in Natural Language Processing (NLP). Adapting NLP to multiple domains has introduced many new challenges for text classification and one of them is long document classification. While state-of-the-art transformer models provide excellent results in text classification, most of them have limitations in the maximum sequence length of the input sequence. The majority of the transformer models are limited to 512 tokens, and therefore, they struggle with long document classification problems. In this research, we explore on employing Model Fusing for long document classification while comparing the results with well-known BERT and Longformer architectures.</abstract>
      <url hash="7230872c">2023.ranlp-1.94</url>
      <bibkey>premasiri-etal-2023-model</bibkey>
    </paper>
    <paper id="95">
      <title>Deep Learning Methods for Identification of Multiword Flower and Plant Names</title>
      <author><first>Damith</first><last>Premasiri</last></author>
      <author><first>Amal</first><last>Haddad Haddad</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>879–887</pages>
      <abstract>Multiword Terms (MWTs) are domain-specific Multiword Expressions (MWE) where two or more lexemes converge to form a new unit of meaning. The task of processing MWTs is crucial in many Natural Language Processing (NLP) applications, including Machine Translation (MT) and terminology extraction. However, the automatic detection of those terms is a difficult task and more research is still required to give more insightful and useful results in this field. In this study, we seek to fill this gap using state-of-the-art transformer models. We evaluate both BERT like discriminative transformer models and generative pre-trained transformer (GPT) models on this task, and we show that discriminative models perform better than current GPT models in multi-word terms identification task in flower and plant names in English and Spanish languages. Best discriminate models perform 94.3127%, 82.1733% F1 scores in English and Spanish data, respectively while ChatGPT could only perform 63.3183% and 47.7925% respectively.</abstract>
      <url hash="c873a837">2023.ranlp-1.95</url>
      <bibkey>premasiri-etal-2023-deep</bibkey>
    </paper>
    <paper id="96">
      <title>Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model</title>
      <author><first>Pavel</first><last>Přibáň</last></author>
      <author><first>Ondrej</first><last>Prazak</last></author>
      <pages>888–897</pages>
      <abstract>This paper presents a series of approaches aimed at enhancing the performance of Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic information from a Semantic Role Labeling (SRL) model. We propose a novel end-to-end Semantic Role Labeling model that effectively captures most of the structured semantic information within the Transformer hidden state. We believe that this end-to-end model is well-suited for our newly proposed models that incorporate semantic information. We evaluate the proposed models in two languages, English and Czech, employing ELECTRA-small models. Our combined models improve ABSA performance in both languages. Moreover, we achieved new state-of-the-art results on the Czech ABSA.</abstract>
      <url hash="561ff23f">2023.ranlp-1.96</url>
      <bibkey>priban-prazak-2023-improving</bibkey>
    </paper>
    <paper id="97">
      <title>hu<fixed-case>PWKP</fixed-case>: A <fixed-case>H</fixed-case>ungarian Text Simplification Corpus</title>
      <author><first>Noémi</first><last>Prótár</last></author>
      <author><first>Dávid Márk</first><last>Nemeskey</last></author>
      <pages>898–907</pages>
      <abstract>In this article we introduce huPWKP, the first parallel corpus consisting of Hungarian standard language-simplified sentence pairs. As Hungarian is a quite low-resource language in regards to text simplification, we opted for translating an already existing corpus, PWKP (Zhu et al., 2010), on which we performed some cleaning in order to improve its quality. We evaluated the corpus both with the help of human evaluators and by training a seq2seq model on both the Hungarian corpus and the original (cleaned) English corpus. The Hungarian model performed slightly worse in terms of automatic metrics; however, the English model attains a SARI score close to the state of the art on the official PWKP set. According to the human evaluation, the corpus performs at around 3 on a scale ranging from 1 to 5 in terms of information retention and increase in simplification and around 3.7 in terms of grammaticality.</abstract>
      <url hash="ba23e7b9">2023.ranlp-1.97</url>
      <bibkey>protar-nemeskey-2023-hupwkp</bibkey>
    </paper>
    <paper id="98">
      <title>Topic Modeling Using Community Detection on a Word Association Graph</title>
      <author><first>Mahfuzur Rahman</first><last>Chowdhury</last></author>
      <author><first>Intesur</first><last>Ahmed</last></author>
      <author><first>Farig</first><last>Sadeque</last></author>
      <author><first>Muhammad</first><last>Yanhaona</last></author>
      <pages>908–917</pages>
      <abstract>Topic modeling of a text corpus is one of the most well-studied areas of information retrieval and knowledge discovery. Despite several decades of research in the area that begets an array of modeling tools, some common problems still obstruct automated topic modeling from matching users’ expectations. In particular, existing topic modeling solutions suffer when the distribution of words among the underlying topics is uneven or the topics are overlapped. Furthermore, many solutions ask the user to provide a topic count estimate as input, which limits their usefulness in modeling a corpus where such information is unavailable. We propose a new topic modeling approach that overcomes these shortcomings by formulating the topic modeling problem as a community detection problem in a word association graph/network that we generate from the text corpus. Experimental evaluation using multiple data sets of three different types of text corpora shows that our approach is superior to prominent topic modeling alternatives in most cases. This paper describes our approach and discusses the experimental findings.</abstract>
      <url hash="61e51c90">2023.ranlp-1.98</url>
      <bibkey>chowdhury-etal-2023-topic</bibkey>
    </paper>
    <paper id="99">
      <title>Exploring Techniques to Detect and Mitigate Non-Inclusive Language Bias in Marketing Communications Using a Dictionary-Based Approach</title>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Prasanna Kumar</first><last>Kumaresan</last></author>
      <author><first>Rahul</first><last>Ponnusamy</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <author><first>Michaela</first><last>Comerford</last></author>
      <author><first>Jay</first><last>Megaro</last></author>
      <author><first>Deniz</first><last>Keles</last></author>
      <author><first>Last</first><last>Feremenga</last></author>
      <pages>918–925</pages>
      <abstract>We propose a new dataset for detecting non-inclusive language in sentences in English. These sentences were gathered from public sites, explaining what is inclusive and what is non-inclusive. We also extracted potentially non-inclusive keywords/phrases from the guidelines from business websites. A phrase dictionary was created by using an automatic extension with a word embedding trained on a massive corpus of general English text. In the end, a phrase dictionary was constructed by hand-editing the previous one to exclude inappropriate expansions and add the keywords from the guidelines. In a business context, the words individuals use can significantly impact the culture of inclusion and the quality of interactions with clients and prospects. Knowing the right words to avoid helps customers of different backgrounds and historically excluded groups feel included. They can make it easier to have productive, engaging, and positive communications. You can find the dictionaries, the code, and the method for making requests for the corpus at (we will release the link for data and code once the paper is accepted).</abstract>
      <url hash="7e12f04b">2023.ranlp-1.99</url>
      <bibkey>chakravarthi-etal-2023-exploring</bibkey>
    </paper>
    <paper id="100">
      <title>Does the “Most Sinfully Decadent Cake Ever” Taste Good? Answering Yes/No Questions from Figurative Contexts</title>
      <author><first>Geetanjali</first><last>Rakshit</last></author>
      <author><first>Jeffrey</first><last>Flanigan</last></author>
      <pages>926–936</pages>
      <abstract>Figurative language is commonplace in natural language, and while making communication memorable and creative, can be difficult to understand. In this work, we investigate the robustness of Question Answering (QA) models on figurative text. Yes/no questions, in particular, are a useful probe of figurative language understanding capabilities of large language models. We propose FigurativeQA, a set of 1000 yes/no questions with figurative and non-figurative contexts, extracted from the domains of restaurant and product reviews. We show that state-of-the-art BERT-based QA models exhibit an average performance drop of up to 15% points when answering questions from figurative contexts, as compared to non-figurative ones. While models like GPT-3 and ChatGPT are better at handling figurative texts, we show that further performance gains can be achieved by automatically simplifying the figurative contexts into their non-figurative (literal) counterparts. We find that the best overall model is ChatGPT with chain-of-thought prompting to generate non-figurative contexts. Our work provides a promising direction for building more robust QA models with figurative language understanding capabilities.</abstract>
      <url hash="7218e57a">2023.ranlp-1.100</url>
      <bibkey>rakshit-flanigan-2023-sinfully</bibkey>
    </paper>
    <paper id="101">
      <title>Modeling Easiness for Training Transformers with Curriculum Learning</title>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Giulia</first><last>Pucci</last></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last></author>
      <pages>937–948</pages>
      <abstract>Directly learning from complex examples is generally problematic for humans and machines. Indeed, a better strategy is exposing learners to examples in a reasonable, pedagogically-motivated order. Curriculum Learning (CL) has been proposed to import this strategy when training machine learning models. In this paper, building on Curriculum Learning, we propose a novel, linguistically motivated measure to determine example complexity for organizing examples during learning. Our complexity measure - LRC- is based on length, rarity, and comprehensibility. Our resulting learning model is CL-LRC, that is, CL with LRC. Experiments on downstream tasks show that CL-LRC outperforms existing CL and non-CL methods for training BERT and RoBERTa from scratch. Furthermore, we analyzed different measures, including perplexity, loss, and learning curve of different models pre-trained from scratch, showing that CL-LRC performs better than the state-of-the-art.</abstract>
      <url hash="4db9d34d">2023.ranlp-1.101</url>
      <bibkey>ranaldi-etal-2023-modeling</bibkey>
    </paper>
    <paper id="102">
      <title>The Dark Side of the Language: Pre-trained Transformers in the <fixed-case>D</fixed-case>ark<fixed-case>N</fixed-case>et</title>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Aria</first><last>Nourbakhsh</last></author>
      <author><first>Elena Sofia</first><last>Ruzzetti</last></author>
      <author><first>Arianna</first><last>Patrizi</last></author>
      <author><first>Dario</first><last>Onorati</last></author>
      <author><first>Michele</first><last>Mastromattei</last></author>
      <author><first>Francesca</first><last>Fallucchi</last></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last></author>
      <pages>949–960</pages>
      <abstract>Pre-trained Transformers are challenging human performances in many Natural Language Processing tasks. The massive datasets used for pre-training seem to be the key to their success on existing tasks. In this paper, we explore how a range of pre-trained natural language understanding models performs on definitely unseen sentences provided by classification tasks over a DarkNet corpus. Surprisingly, results show that syntactic and lexical neural networks perform on par with pre-trained Transformers even after fine-tuning. Only after what we call extreme domain adaptation, that is, retraining with the masked language model task on all the novel corpus, pre-trained Transformers reach their standard high results. This suggests that huge pre-training corpora may give Transformers unexpected help since they are exposed to many of the possible sentences.</abstract>
      <url hash="ddbd036a">2023.ranlp-1.102</url>
      <bibkey>ranaldi-etal-2023-dark</bibkey>
    </paper>
    <paper id="103">
      <title><fixed-case>P</fixed-case>re<fixed-case>C</fixed-case>og: Exploring the Relation between Memorization and Performance in Pre-trained Language Models</title>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Elena Sofia</first><last>Ruzzetti</last></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last></author>
      <pages>961–967</pages>
      <abstract>Large Language Models (LLMs) are impressive machines with the ability to memorize, possibly generalized learning examples. We present here a small, focused contribution to the analysis of the interplay between memorization and performance of BERT in downstream tasks. We propose PreCog, a measure for evaluating memorization from pre-training, and we analyze its correlation with the BERT’s performance. Our experiments show that highly memorized examples are better classified, suggesting memorization is an essential key to success for BERT.</abstract>
      <url hash="7ee1b9f6">2023.ranlp-1.103</url>
      <bibkey>ranaldi-etal-2023-precog</bibkey>
    </paper>
    <paper id="104">
      <title>Publish or Hold? Automatic Comment Moderation in <fixed-case>L</fixed-case>uxembourgish News Articles</title>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Alistair</first><last>Plum</last></author>
      <author><first>Christoph</first><last>Purschke</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>968–978</pages>
      <abstract>Recently, the internet has emerged as the primary platform for accessing news. In the majority of these news platforms, the users now have the ability to post comments on news articles and engage in discussions on various social media. While these features promote healthy conversations among users, they also serve as a breeding ground for spreading fake news, toxic discussions and hate speech. Moderating or removing such content is paramount to avoid unwanted consequences for the readers. How- ever, apart from a few notable exceptions, most research on automatic moderation of news article comments has dealt with English and other high resource languages. This leaves under-represented or low-resource languages at a loss. Addressing this gap, we perform the first large-scale qualitative analysis of more than one million Luxembourgish comments posted over the course of 14 years. We evaluate the performance of state-of-the-art transformer models in Luxembourgish news article comment moderation. Furthermore, we analyse how the language of Luxembourgish news article comments has changed over time. We observe that machine learning models trained on old comments do not perform well on recent data. The findings in this work will be beneficial in building news comment moderation systems for many low-resource languages</abstract>
      <url hash="9c38e87a">2023.ranlp-1.104</url>
      <bibkey>ranasinghe-etal-2023-publish</bibkey>
    </paper>
    <paper id="105">
      <title>Cross-Lingual Speaker Identification for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Amaan</first><last>Rizvi</last></author>
      <author><first>Anupam</first><last>Jamatia</last></author>
      <author><first>Dwijen</first><last>Rudrapal</last></author>
      <author><first>Kunal</first><last>Chakma</last></author>
      <author><first>Björn</first><last>Gambäck</last></author>
      <pages>979–987</pages>
      <abstract>The paper introduces a cross-lingual speaker identification system for Indian languages, utilising a Long Short-Term Memory dense neural network (LSTM-DNN). The system was trained on audio recordings in English and evaluated on data from Hindi, Kannada, Malayalam, Tamil, and Telugu, with a view to how factors such as phonetic similarity and native accent affect performance. The model was fed with MFCC (mel-frequency cepstral coefficient) features extracted from the audio file. For comparison, the corresponding mel-spectrogram images were also used as input to a ResNet-50 model, while the raw audio was used to train a Siamese network. The LSTM-DNN model outperformed the other two models as well as two more traditional baseline speaker identification models, showing that deep learning models are superior to probabilistic models for capturing low-level speech features and learning speaker characteristics.</abstract>
      <url hash="42c89e0f">2023.ranlp-1.105</url>
      <bibkey>rizvi-etal-2023-cross</bibkey>
    </paper>
    <paper id="106">
      <title>‘<fixed-case>C</fixed-case>hem<fixed-case>X</fixed-case>tract’ A System for Extraction of Chemical Events from Patent Documents</title>
      <author><first>Pattabhi</first><last>RK Rao</last></author>
      <author><first>Sobha</first><last>Lalitha Devi</last></author>
      <pages>988–995</pages>
      <abstract>ChemXtraxt main goal is to extract the chemical events from patent documents. Event extraction requires that we first identify the names of chemical compounds involved in the events. Thus, in this work two extractions are done and they are (a) names of chemical compounds and (b) event that identify the specific involvement of the chemical compounds in a chemical reaction. Extraction of essential elements of a chemical reaction, generally known as Named Entity Recognition (NER), extracts the compounds, condition and yields, their specific role in reaction and assigns a label according to the role it plays within a chemical reaction. Whereas event extraction identifies the chemical event relations between the chemical compounds identified. Here in this work we have used Neural Conditional Random Fields (NCRF), which combines the power of artificial neural network (ANN) and CRFs. Different levels of features that include linguistic, orthographical and lexical clues are used. The results obtained are encouraging.</abstract>
      <url hash="950e4bd2">2023.ranlp-1.106</url>
      <bibkey>rk-rao-lalitha-devi-2023-chemxtract</bibkey>
    </paper>
    <paper id="107">
      <title>Mind the User! Measures to More Accurately Evaluate the Practical Value of Active Learning Strategies</title>
      <author><first>Julia</first><last>Romberg</last></author>
      <pages>996–1006</pages>
      <abstract>One solution to limited annotation budgets is active learning (AL), a collaborative process of human and machine to strategically select a small but informative set of examples. While current measures optimize AL from a pure machine learning perspective, we argue that for a successful transfer into practice, additional criteria must target the second pillar of AL, the human annotator. In text classification, e.g., where practitioners regularly encounter datasets with an increased number of imbalanced classes, measures like F1 fall short when finding all classes or identifying rare cases is required. We therefore introduce four measures that reflect class-related demands that users place on data acquisition. In a comprehensive comparison of uncertainty-based, diversity-based, and hybrid query strategies on six different datasets, we find that strong F1 performance is not necessarily associated with full class coverage. Uncertainty sampling outperforms diversity sampling in selecting minority classes and covering classes more efficiently, while diversity sampling excels in selecting less monotonous batches. Our empirical findings emphasize that a holistic view is essential when evaluating AL approaches to ensure their usefulness in practice - the actual, but often overlooked, goal of development. To this end, standard measures for assessing the performance of text classification need to be complemented by such that more appropriately reflect user needs.</abstract>
      <url hash="9ca23899">2023.ranlp-1.107</url>
      <bibkey>romberg-2023-mind</bibkey>
    </paper>
    <paper id="108">
      <title>Event Annotation and Detection in <fixed-case>K</fixed-case>annada-<fixed-case>E</fixed-case>nglish Code-Mixed Social Media Data</title>
      <author><first>Sumukh</first><last>S</last></author>
      <author><first>Abhinav</first><last>Appidi</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>1007–1014</pages>
      <abstract>Code-mixing (CM) is a frequently observed phenomenon on social media platforms in multilingual societies such as India. While the increase in code-mixed content on these platforms provides good amount of data for studying various aspects of code-mixing, the lack of automated text analysis tools makes such studies difficult. To overcome the same, tools such as language identifiers, Parts-of-Speech (POS) taggers and Named Entity Recognition (NER) for analysing code-mixed data have been developed. One such important tool is Event Detection, an important information retrieval task which can be used to identify critical facts occurring in the vast streams of unstructured text data available. While event detection from text is a hard problem on its own, social media data adds to it with its informal nature, and code-mixed (Kannada-English) data further complicates the problem due to its word-level mixing, lack of structure and incomplete information. In this work, we have tried to address this problem. We have proposed guidelines for the annotation of events in Kannada-English CM data and provided some baselines for the same with careful feature selection.</abstract>
      <url hash="2726512b">2023.ranlp-1.108</url>
      <bibkey>s-etal-2023-event</bibkey>
    </paper>
    <paper id="109">
      <title>Three Approaches to Client Email Topic Classification</title>
      <author><first>Branislava</first><last>Šandrih Todorović</last></author>
      <author><first>Katarina</first><last>Josipović</last></author>
      <author><first>Jurij</first><last>Kodre</last></author>
      <pages>1015–1022</pages>
      <abstract>This paper describes a use case that was implemented and is currently running in production at the Nova Ljubljanska Banka, that involves classifying incoming client emails in the Slovenian language according to their topics and priorities. Since the proposed approach relies only on the Named Entity Recogniser (NER) of personal names as a language-dependent resource (for the purpose of anonymisation), that is the only prerequisite for applying the approach to any other language.</abstract>
      <url hash="7589463f">2023.ranlp-1.109</url>
      <bibkey>sandrih-todorovic-etal-2023-three</bibkey>
    </paper>
    <paper id="110">
      <title>Exploring Abstractive Text Summarisation for Podcasts: A Comparative Study of <fixed-case>BART</fixed-case> and T5 Models</title>
      <author><first>Parth</first><last>Saxena</last></author>
      <author><first>Mo</first><last>El-Haj</last></author>
      <pages>1023–1033</pages>
      <abstract>Podcasts have become increasingly popular in recent years, resulting in a massive amount of audio content being produced every day. Efficient summarisation of podcast episodes can enable better content management and discovery for users. In this paper, we explore the use of abstractive text summarisation methods to generate high-quality summaries of podcast episodes. We use pre-trained models, BART and T5, to fine-tune on a dataset of Spotify’s 100K podcast. We evaluate our models using automated metrics and human evaluation, and find that the BART model fine-tuned on the podcast dataset achieved a higher ROUGE-1 and ROUGE-L score compared to other models, while the T5 model performed better in terms of semantic meaning. The human evaluation indicates that both models produced high-quality summaries that were well received by participants. Our study demonstrates the effectiveness of abstractive summarisation methods for podcast episodes and offers insights for improving the summarisation of audio content.</abstract>
      <url hash="925dd46d">2023.ranlp-1.110</url>
      <bibkey>saxena-el-haj-2023-exploring</bibkey>
    </paper>
    <paper id="111">
      <title>Exploring the Landscape of Natural Language Processing Research</title>
      <author><first>Tim</first><last>Schopf</last></author>
      <author><first>Karim</first><last>Arabi</last></author>
      <author><first>Florian</first><last>Matthes</last></author>
      <pages>1034–1045</pages>
      <abstract>As an efficient approach to understand, generate, and process natural language texts, research in natural language processing (NLP) has exhibited a rapid spread and wide adoption in recent years. Given the increasing research work in this area, several NLP-related approaches have been surveyed in the research community. However, a comprehensive study that categorizes established topics, identifies trends, and outlines areas for future research remains absent. Contributing to closing this gap, we have systematically classified and analyzed research papers in the ACL Anthology. As a result, we present a structured overview of the research landscape, provide a taxonomy of fields of study in NLP, analyze recent developments in NLP, summarize our findings, and highlight directions for future work.</abstract>
      <url hash="4ba21ca8">2023.ranlp-1.111</url>
      <bibkey>schopf-etal-2023-exploring</bibkey>
    </paper>
    <paper id="112">
      <title>Efficient Domain Adaptation of Sentence Embeddings Using Adapters</title>
      <author><first>Tim</first><last>Schopf</last></author>
      <author><first>Dennis N.</first><last>Schneider</last></author>
      <author><first>Florian</first><last>Matthes</last></author>
      <pages>1046–1053</pages>
      <abstract>Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model’s weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always using the same base model and only exchanging the domain-specific adapters to adapt sentence embeddings to a specific domain. We show that using adapters for parameter-efficient domain adaptation of sentence embeddings yields competitive performance within 1% of a domain-adapted, entirely fine-tuned sentence embedding model while only training approximately 3.6% of the parameters.</abstract>
      <url hash="ec7e585b">2023.ranlp-1.112</url>
      <bibkey>schopf-etal-2023-efficient</bibkey>
    </paper>
    <paper id="113">
      <title><fixed-case>A</fixed-case>spect<fixed-case>CSE</fixed-case>: Sentence Embeddings for Aspect-Based Semantic Textual Similarity Using Contrastive Learning and Structured Knowledge</title>
      <author><first>Tim</first><last>Schopf</last></author>
      <author><first>Emanuel</first><last>Gerber</last></author>
      <author><first>Malte</first><last>Ostendorff</last></author>
      <author><first>Florian</first><last>Matthes</last></author>
      <pages>1054–1065</pages>
      <abstract>Generic sentence embeddings provide coarse-grained approximation of semantic textual similarity, but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose the use of Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform even single-aspect embeddings on aspect-specific information retrieval tasks. Finally, we examine the aspect-based sentence embedding space and demonstrate that embeddings of semantically similar aspect labels are often close, even without explicit similarity training between different aspect labels.</abstract>
      <url hash="75c4157b">2023.ranlp-1.113</url>
      <bibkey>schopf-etal-2023-aspectcse</bibkey>
    </paper>
    <paper id="114">
      <title>Tackling the Myriads of Collusion Scams on <fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ube Comments of Cryptocurrency Videos</title>
      <author><first>Sadat</first><last>Shahriar</last></author>
      <author><first>Arjun</first><last>Mukherjee</last></author>
      <pages>1066–1075</pages>
      <abstract>Despite repeated measures, YouTube’s comment section has been a fertile ground for scammers. With the growth of the cryptocurrency market and obscurity around it, a new form of scam, namely “Collusion Scam” has emerged as a dominant force within YouTube’s comment space. Unlike typical scams and spams, collusion scams employ a cunning persuasion strategy, using the facade of genuine social interactions within comment threads to create an aura of trust and success to entrap innocent users. In this research, we collect 1,174 such collusion scam threads and perform a detailed analysis, which is tailored towards the successful detection of these scams. We find that utilization of the collusion dynamics can provide an accuracy of 96.67% and an F1-score of 93.04%. Furthermore, we demonstrate the robust predictive power of metadata associated with these threads and user channels, which act as compelling indicators of collusion scams. Finally, we show that modern LLM, like chatGPT, can effectively detect collusion scams without the need for any training.</abstract>
      <url hash="ccdd0d67">2023.ranlp-1.114</url>
      <bibkey>shahriar-mukherjee-2023-tackling</bibkey>
    </paper>
    <paper id="115">
      <title>Exploring Deceptive Domain Transfer Strategies: Mitigating the Differences among Deceptive Domains</title>
      <author><first>Sadat</first><last>Shahriar</last></author>
      <author><first>Arjun</first><last>Mukherjee</last></author>
      <author><first>Omprakash</first><last>Gnawali</last></author>
      <pages>1076–1084</pages>
      <abstract>Deceptive text poses a significant threat to users, resulting in widespread misinformation and disorder. While researchers have created numerous cutting-edge techniques for detecting deception in domain-specific settings, whether there is a generic deception pattern so that deception-related knowledge in one domain can be transferred to the other remains mostly unexplored. Moreover, the disparities in textual expression across these many mediums pose an additional obstacle for generalization. To this end, we present a Multi-Task Learning (MTL)-based deception generalization strategy to reduce the domain-specific noise and facilitate a better understanding of deception via a generalized training. As deceptive domains, we use News (fake news), Tweets (rumors), and Reviews (fake reviews) and employ LSTM and BERT model to incorporate domain transfer techniques. Our proposed architecture for the combined approach of domain-independent and domain-specific training improves the deception detection performance by up to 5.28% in F1-score.</abstract>
      <url hash="c427d18a">2023.ranlp-1.115</url>
      <bibkey>shahriar-etal-2023-exploring</bibkey>
    </paper>
    <paper id="116">
      <title>Party Extraction from Legal Contract Using Contextualized Span Representations of Parties</title>
      <author><first>Sanjeepan</first><last>Sivapiran</last></author>
      <author><first>Charangan</first><last>Vasantharajan</last></author>
      <author><first>Uthayasanker</first><last>Thayasivam</last></author>
      <pages>1085–1094</pages>
      <abstract>Extracting legal entities from legal documents, particularly legal parties in contract documents, poses a significant challenge for legal assistive software. Many existing party extraction systems tend to generate numerous false positives due to the complex structure of the legal text. In this study, we present a novel and accurate method for extracting parties from legal contract documents by leveraging contextual span representations. To facilitate our approach, we have curated a large-scale dataset comprising 1000 contract documents with party annotations. Our method incorporates several enhancements to the SQuAD 2.0 question-answering system, specifically tailored to handle the intricate nature of the legal text. These enhancements include modifications to the activation function, an increased number of encoder layers, and the addition of normalization and dropout layers stacked on top of the output encoder layer. Baseline experiments reveal that our model, fine-tuned on our dataset, outperforms the current state-of-the-art model. Furthermore, we explore various combinations of the aforementioned techniques to further enhance the accuracy of our method. By employing a hybrid approach that combines 24 encoder layers with normalization and dropout layers, we achieve the best results, exhibiting an exact match score of 0.942 (+6.2% improvement).</abstract>
      <url hash="f03bebd5">2023.ranlp-1.116</url>
      <bibkey>sivapiran-etal-2023-party</bibkey>
    </paper>
    <paper id="117">
      <title>From Fake to Hyperpartisan News Detection Using Domain Adaptation</title>
      <author><first>Răzvan-Alexandru</first><last>Smădu</last></author>
      <author><first>Sebastian-Vasile</first><last>Echim</last></author>
      <author><first>Dumitru-Clementin</first><last>Cercel</last></author>
      <author><first>Iuliana</first><last>Marin</last></author>
      <author><first>Florin</first><last>Pop</last></author>
      <pages>1095–1109</pages>
      <abstract>Unsupervised Domain Adaptation (UDA) is a popular technique that aims to reduce the domain shift between two data distributions. It was successfully applied in computer vision and natural language processing. In the current work, we explore the effects of various unsupervised domain adaptation techniques between two text classification tasks: fake and hyperpartisan news detection. We investigate the knowledge transfer from fake to hyperpartisan news detection without involving target labels during training. Thus, we evaluate UDA, cluster alignment with a teacher, and cross-domain contrastive learning. Extensive experiments show that these techniques improve performance, while including data augmentation further enhances the results. In addition, we combine clustering and topic modeling algorithms with UDA, resulting in improved performances compared to the initial UDA setup.</abstract>
      <url hash="593f21d8">2023.ranlp-1.117</url>
      <bibkey>smadu-etal-2023-fake</bibkey>
    </paper>
    <paper id="118">
      <title>Prompt-Based Approach for <fixed-case>C</fixed-case>zech Sentiment Analysis</title>
      <author><first>Jakub</first><last>Šmíd</last></author>
      <author><first>Pavel</first><last>Přibáň</last></author>
      <pages>1110–1120</pages>
      <abstract>This paper introduces the first prompt-based methods for aspect-based sentiment analysis and sentiment classification in Czech. We employ the sequence-to-sequence models to solve the aspect-based tasks simultaneously and demonstrate the superiority of our prompt-based approach over traditional fine-tuning. In addition, we conduct zero-shot and few-shot learning experiments for sentiment classification and show that prompting yields significantly better results with limited training examples compared to traditional fine-tuning. We also demonstrate that pre-training on data from the target domain can lead to significant improvements in a zero-shot scenario.</abstract>
      <url hash="14876e1c">2023.ranlp-1.118</url>
      <bibkey>smid-priban-2023-prompt</bibkey>
    </paper>
    <paper id="119">
      <title>Measuring Gender Bias in Natural Language Processing: Incorporating Gender-Neutral Linguistic Forms for Non-Binary Gender Identities in Abusive Speech Detection</title>
      <author><first>Nasim</first><last>Sobhani</last></author>
      <author><first>Kinshuk</first><last>Sengupta</last></author>
      <author><first>Sarah Jane</first><last>Delany</last></author>
      <pages>1121–1131</pages>
      <abstract>Predictions from machine learning models can reflect bias in the data on which they are trained. Gender bias has been shown to be prevalent in natural language processing models. The research into identifying and mitigating gender bias in these models predominantly considers gender as binary, male and female, neglecting the fluidity and continuity of gender as a variable. In this paper, we present an approach to evaluate gender bias in a prediction task, which recognises the non-binary nature of gender. We gender-neutralise a random subset of existing real-world hate speech data. We extend the existing template approach for measuring gender bias to include test examples that are gender-neutral. Measuring the bias across a selection of hate speech datasets we show that the bias for the gender-neutral data is closer to that seen for test instances that identify as male than those that identify as female.</abstract>
      <url hash="de6726d5">2023.ranlp-1.119</url>
      <bibkey>sobhani-etal-2023-measuring</bibkey>
    </paper>
    <paper id="120">
      <title><fixed-case>L</fixed-case>e<fixed-case>SS</fixed-case>: A Computationally-Light Lexical Simplifier for <fixed-case>S</fixed-case>panish</title>
      <author><first>Sanja</first><last>Stajner</last></author>
      <author><first>Daniel</first><last>Ibanez</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>1132–1142</pages>
      <abstract>Due to having knowledge of only basic vocabulary, many people cannot understand up-to-date written information and thus make informed decisions and fully participate in the society. We propose LeSS, a modular lexical simplification architecture that outperforms state-of-the-art lexical simplification systems for Spanish. In addition to its state-of-the-art performance, LeSS is computationally light, using much less disk space, CPU and GPU, and having faster loading and execution time than the transformer-based lexical simplification models which are predominant in the field.</abstract>
      <url hash="1559b358">2023.ranlp-1.120</url>
      <bibkey>stajner-etal-2023-less</bibkey>
    </paper>
    <paper id="121">
      <title><fixed-case>H</fixed-case>indi to <fixed-case>D</fixed-case>ravidian Language Neural Machine Translation Systems</title>
      <author><first>Vijay</first><last>Sundar Ram</last></author>
      <author><first>Sobha</first><last>Lalitha Devi</last></author>
      <pages>1143–1150</pages>
      <abstract>Neural machine translation (NMT) has achieved state-of-art performance in high-resource language pairs, but the performance of NMT drops in low-resource conditions. Morphologically rich languages are yet another challenge in NMT. The common strategy to handle this issue is to apply sub-word segmentation. In this work, we compare the morphologically inspired segmentation methods against the Byte Pair Encoding (BPE) in processing the input for building NMT systems for Hindi to Malayalam and Hindi to Tamil, where Hindi is an Indo-Aryan language and Malayalam and Tamil are south Dravidian languages. These two languages are low resource, morphologically rich and agglutinative. Malayalam is more agglutinative than Tamil. We show that for both the language pairs, the morphological segmentation algorithm out-performs BPE. We also present an elaborate analysis on translation outputs from both the NMT systems.</abstract>
      <url hash="3b945dde">2023.ranlp-1.121</url>
      <bibkey>sundar-ram-lalitha-devi-2023-hindi</bibkey>
    </paper>
    <paper id="122">
      <title>Looking for Traces of Textual Deepfakes in <fixed-case>B</fixed-case>ulgarian on Social Media</title>
      <author><first>Irina</first><last>Temnikova</last></author>
      <author><first>Iva</first><last>Marinova</last></author>
      <author><first>Silvia</first><last>Gargova</last></author>
      <author><first>Ruslana</first><last>Margova</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <pages>1151–1161</pages>
      <abstract>Textual deepfakes can cause harm, especially on social media. At the moment, there are models trained to detect deepfake messages mainly for the English language, but no research or datasets currently exist for detecting them in most low-resource languages, such as Bulgarian. To address this gap, we explore three approaches. First, we machine translate an English-language social media dataset with bot messages into Bulgarian. However, the translation quality is unsatisfactory, leading us to create a new Bulgarian-language dataset with real social media messages and those generated by two language models (a new Bulgarian GPT-2 model – GPT-WEB-BG, and ChatGPT). We machine translate it into English and test existing English GPT-2 and ChatGPT detectors on it, achieving only 0.44-0.51 accuracy. Next, we train our own classifiers on the Bulgarian dataset, obtaining an accuracy of 0.97. Additionally, we apply the classifier with the highest results to a recently released Bulgarian social media dataset with manually fact-checked messages, which successfully identifies some of the messages as generated by Language Models (LM). Our results show that the use of machine translation is not suitable for textual deepfakes detection. We conclude that combining LM text detection with fact-checking is the most appropriate method for this task, and that identifying Bulgarian textual deepfakes is indeed possible.</abstract>
      <url hash="aae80bc7">2023.ranlp-1.122</url>
      <bibkey>temnikova-etal-2023-looking</bibkey>
    </paper>
    <paper id="123">
      <title>Propaganda Detection in <fixed-case>R</fixed-case>ussian Telegram Posts in the Scope of the <fixed-case>R</fixed-case>ussian Invasion of <fixed-case>U</fixed-case>kraine</title>
      <author><first>Natalia</first><last>Vanetik</last></author>
      <author><first>Marina</first><last>Litvak</last></author>
      <author><first>Egor</first><last>Reviakin</last></author>
      <author><first>Margarita</first><last>Tiamanova</last></author>
      <pages>1162–1170</pages>
      <abstract>The emergence of social media has made it more difficult to recognize and analyze misinformation efforts. Popular messaging software Telegram has developed into a medium for disseminating political messages and misinformation, particularly in light of the conflict in Ukraine. In this paper, we introduce a sizable corpus of Telegram posts containing pro-Russian propaganda and benign political texts. We evaluate the corpus by applying natural language processing (NLP) techniques to the task of text classification in this corpus. Our findings indicate that, with an overall accuracy of over 96% for confirmed sources as propagandists and oppositions and 92% for unconfirmed sources, our method can successfully identify and categorize pro- Russian propaganda posts. We highlight the consequences of our research for comprehending political communications and propaganda on social media.</abstract>
      <url hash="1ea3b2ae">2023.ranlp-1.123</url>
      <bibkey>vanetik-etal-2023-propaganda</bibkey>
    </paper>
    <paper id="124">
      <title>Auto-Encoding Questions with Retrieval Augmented Decoding for Unsupervised Passage Retrieval and Zero-Shot Question Generation</title>
      <author><first>Stalin</first><last>Varanasi</last></author>
      <author><first>Muhammad Umer Tariq</first><last>Butt</last></author>
      <author><first>Guenter</first><last>Neumann</last></author>
      <pages>1171–1179</pages>
      <abstract>Dense passage retrieval models have become state-of-the-art for information retrieval on many Open-domain Question Answering (ODQA) datasets. However, most of these models rely on supervision obtained from the ODQA datasets, which hinders their performance in a low-resource setting. Recently, retrieval-augmented language models have been proposed to improve both zero-shot and supervised information retrieval. However, these models have pre-training tasks that are agnostic to the target task of passage retrieval. In this work, we propose Retrieval Augmented Auto-encoding of Questions for zero-shot dense information retrieval. Unlike other pre-training methods, our pre-training method is built for target information retrieval, thereby making the pre-training more efficient. Our method consists of a dense IR model for encoding questions and retrieving documents during training and a conditional language model that maximizes the question’s likelihood by marginalizing over retrieved documents. As a by-product, we can use this conditional language model for zero-shot question generation from documents. We show that the IR model obtained through our method improves the current state-of-the-art of zero-shot dense information retrieval, and we improve the results even further by training on a synthetic corpus created by zero-shot question generation.</abstract>
      <url hash="bad5954d">2023.ranlp-1.124</url>
      <bibkey>varanasi-etal-2023-auto</bibkey>
    </paper>
    <paper id="125">
      <title><fixed-case>N</fixed-case>o<fixed-case>H</fixed-case>ate<fixed-case>B</fixed-case>razil: A <fixed-case>B</fixed-case>razilian <fixed-case>P</fixed-case>ortuguese Text Offensiveness Analysis System</title>
      <author><first>Francielle</first><last>Vargas</last></author>
      <author><first>Isabelle</first><last>Carvalho</last></author>
      <author><first>Wolfgang</first><last>Schmeisser-Nieto</last></author>
      <author><first>Fabrício</first><last>Benevenuto</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>1180–1186</pages>
      <abstract>Hate speech is a surely relevant problem in Brazil. Nevertheless, its regulation is not effective due to the difficulty to identify, quantify and classify offensive comments. Here, we introduce a novel system for offensive comment analysis in Brazilian Portuguese. The system titled “NoHateBrazil” recognizes explicit and implicit offensiveness in context at a fine-grained level. Specifically, we propose a framework for data collection, human annotation and machine learning models that were used to build the system. In addition, we assess the potential of our system to reflect stereotypical beliefs against marginalized groups by contrasting them with counter-stereotypes. As a result, a friendly web application was implemented, which besides presenting relevant performance, showed promising results towards mitigation of the risk of reinforcing social stereotypes. Lastly, new measures were proposed to improve the explainability of offensiveness classification and reliability of the model’s predictions.</abstract>
      <url hash="65a30726">2023.ranlp-1.125</url>
      <bibkey>vargas-etal-2023-nohatebrazil</bibkey>
    </paper>
    <paper id="126">
      <title>Socially Responsible Hate Speech Detection: Can Classifiers Reflect Social Stereotypes?</title>
      <author><first>Francielle</first><last>Vargas</last></author>
      <author><first>Isabelle</first><last>Carvalho</last></author>
      <author><first>Ali</first><last>Hürriyetoğlu</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <author><first>Fabrício</first><last>Benevenuto</last></author>
      <pages>1187–1196</pages>
      <abstract>Recent studies have shown that hate speech technologies may propagate social stereotypes against marginalized groups. Nevertheless, there has been a lack of realistic approaches to assess and mitigate biased technologies. In this paper, we introduce a new approach to analyze the potential of hate-speech classifiers to reflect social stereotypes through the investigation of stereotypical beliefs by contrasting them with counter-stereotypes. We empirically measure the distribution of stereotypical beliefs by analyzing the distinctive classification of tuples containing stereotypes versus counter-stereotypes in machine learning models and datasets. Experiment results show that hate speech classifiers attribute unreal or negligent offensiveness to social identity groups by reflecting and reinforcing stereotypical beliefs regarding minorities. Furthermore, we also found that models that embed expert and context information from offensiveness markers present promising results to mitigate social stereotype bias towards socially responsible hate speech detection.</abstract>
      <url hash="7e2fcd32">2023.ranlp-1.126</url>
      <bibkey>vargas-etal-2023-socially</bibkey>
    </paper>
    <paper id="127">
      <title>Predicting Sentence-Level Factuality of News and Bias of Media Outlets</title>
      <author><first>Francielle</first><last>Vargas</last></author>
      <author><first>Kokil</first><last>Jaidka</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <author><first>Fabrício</first><last>Benevenuto</last></author>
      <pages>1197–1206</pages>
      <abstract>Automated news credibility and fact-checking at scale require accurate prediction of news factuality and media bias. This paper introduces a large sentence-level dataset, titled “FactNews”, composed of 6,191 sentences expertly annotated according to factuality and media bias definitions proposed by AllSides. We use FactNews to assess the overall reliability of news sources by formulating two text classification problems for predicting sentence-level factuality of news reporting and bias of media outlets. Our experiments demonstrate that biased sentences present a higher number of words compared to factual sentences, besides having a predominance of emotions. Hence, the fine-grained analysis of subjectivity and impartiality of news articles showed promising results for predicting the reliability of entire media outlets. Finally, due to the severity of fake news and political polarization in Brazil, and the lack of research for Portuguese, both dataset and baseline were proposed for Brazilian Portuguese.</abstract>
      <url hash="0b2d8f82">2023.ranlp-1.127</url>
      <bibkey>vargas-etal-2023-predicting</bibkey>
    </paper>
    <paper id="128">
      <title>Classification of <fixed-case>US</fixed-case> <fixed-case>S</fixed-case>upreme <fixed-case>C</fixed-case>ourt Cases Using <fixed-case>BERT</fixed-case>-Based Techniques</title>
      <author><first>Shubham</first><last>Vatsal</last></author>
      <author><first>Adam</first><last>Meyers</last></author>
      <author><first>John E.</first><last>Ortega</last></author>
      <pages>1207–1215</pages>
      <abstract>Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80% on the 15 broad categories and 60% on the fine-grained 279 categories which marks an improvement of 8% and 28% respectively from previously reported SOTA results.</abstract>
      <url hash="c58834a3">2023.ranlp-1.128</url>
      <bibkey>vatsal-etal-2023-classification</bibkey>
    </paper>
    <paper id="129">
      <title>Kāraka-Based Answer Retrieval for Question Answering in <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Devika</first><last>Verma</last></author>
      <author><first>Ramprasad S.</first><last>Joshi</last></author>
      <author><first>Aiman A.</first><last>Shivani</last></author>
      <author><first>Rohan D.</first><last>Gupta</last></author>
      <pages>1216–1224</pages>
      <abstract>Kārakas from ancient Paninian grammar form a concise set of semantic roles that capture crucial aspect of sentence meaning pivoted on the action verb. In this paper, we propose employing a kāraka-based approach for retrieving answers in Indic question-answering systems. To study and evaluate this novel approach, empirical experiments are conducted over large benchmark corpora in Hindi and Marathi. The results obtained demonstrate the effectiveness of the proposed method. Additionally, we explore the varying impact of two approaches for extracting kārakas. The literature surveyed and experiments conducted encourage hope that kāraka annotation can improve communication with machines using natural languages, particularly in low-resource languages.</abstract>
      <url hash="4b0f3763">2023.ranlp-1.129</url>
      <bibkey>verma-etal-2023-karaka</bibkey>
    </paper>
    <paper id="130">
      <title>Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain</title>
      <author><first>Gayashan</first><last>Weerasundara</last></author>
      <author><first>Nisansa</first><last>de Silva</last></author>
      <pages>1225–1233</pages>
      <abstract>Some Natural Language Processing (NLP) tasks that are in the sufficiently solved state for general domain English still struggle to attain the same level of performance in specific domains. Named Entity Recognition (NER), which aims to find and categorize entities in text is such a task met with difficulties in adapting to domain specificity. This paper compares the performance of 10 NER models on 7 adventure books from the Dungeons and Dragons (D&amp;D) domain which is a subdomain of fantasy literature. Fantasy literature, being rich and diverse in vocabulary, poses considerable challenges for conventional NER. In this study, we use open-source Large Language Models (LLM) to annotate the named entities and character names in each number of official D&amp;D books and evaluate the precision and distribution of each model. The paper aims to identify the challenges and opportunities for improving NER in fantasy literature. Our results show that even in the off-the-shelf configuration, Flair, Trankit, and Spacy achieve better results for identifying named entities in the D&amp;D domain compared to their peers.</abstract>
      <url hash="f32fdc22">2023.ranlp-1.130</url>
      <bibkey>weerasundara-de-silva-2023-comparative</bibkey>
    </paper>
    <paper id="131">
      <title>Comparative Analysis of Anomaly Detection Algorithms in Text Data</title>
      <author><first>Yizhou</first><last>Xu</last></author>
      <author><first>Kata</first><last>Gábor</last></author>
      <author><first>Jérôme</first><last>Milleret</last></author>
      <author><first>Frédérique</first><last>Segond</last></author>
      <pages>1234–1245</pages>
      <abstract>Text anomaly detection (TAD) is a crucial task that aims to identify texts that deviate significantly from the norm within a corpus. Despite its importance in various domains, TAD remains relatively underexplored in natural language processing. This article presents a systematic evaluation of 22 TAD algorithms on 17 corpora using multiple text representations, including monolingual and multilingual SBERT. The performance of the algorithms is compared based on three criteria: degree of supervision, theoretical basis, and architecture used. The results demonstrate that semi-supervised methods utilizing weak labels outperform both unsupervised methods and semi-supervised methods using only negative samples for training. Additionally, we explore the application of TAD techniques in hate speech detection. The results provide valuable insights for future TAD research and guide the selection of suitable algorithms for detecting text anomalies in different contexts.</abstract>
      <url hash="9b4cf9cb">2023.ranlp-1.131</url>
      <bibkey>xu-etal-2023-comparative</bibkey>
    </paper>
    <paper id="132">
      <title>Poetry Generation Combining Poetry Theme Labels Representations</title>
      <author><first>Yingyu</first><last>Yan</last></author>
      <author><first>Dongzhen</first><last>Wen</last></author>
      <author><first>Liang</first><last>Yang</last></author>
      <author><first>Dongyu</first><last>Zhang</last></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <pages>1246–1255</pages>
      <abstract>Ancient Chinese poetry is the earliest literary genre that took shape in Chinese literature and has a dissemination effect, showing China’s profound cultural heritage. At the same time, the generation of ancient poetry is an important task in the field of digital humanities, which is of great significance to the inheritance of national culture and the education of ancient poetry. The current work in the field of poetry generation is mainly aimed at improving the fluency and structural accuracy of words and sentences, ignoring the theme unity of poetry generation results. In order to solve this problem, this paper proposes a graph neural network poetry theme representation model based on label embedding. On the basis of the network representation of poetry, the topic feature representation of poetry is constructed and learned from the granularity of words. Then, the features of the poetry theme representation model are combined with the autoregressive language model to construct a theme-oriented ancient Chinese poetry generation model TLPG (Poetry Generation with Theme Label). Through machine evaluation and evaluation by experts in related fields, the model proposed in this paper has significantly improved the topic consistency of poetry generation compared with existing work on the premise of ensuring the fluency and format accuracy of poetry.</abstract>
      <url hash="927fb050">2023.ranlp-1.132</url>
      <bibkey>yan-etal-2023-poetry</bibkey>
    </paper>
    <paper id="133">
      <title>Evaluating Generative Models for Graph-to-Text Generation</title>
      <author><first>Shuzhou</first><last>Yuan</last></author>
      <author><first>Michael</first><last>Faerber</last></author>
      <pages>1256–1264</pages>
      <abstract>Large language models (LLMs) have been widely employed for graph-to-text generation tasks. However, the process of finetuning LLMs requires significant training resources and annotation work. In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART. Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information. As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores. We have made the text generated by generative models publicly available.</abstract>
      <url hash="831b7f30">2023.ranlp-1.133</url>
      <bibkey>yuan-faerber-2023-evaluating</bibkey>
    </paper>
    <paper id="134">
      <title>Microsyntactic Unit Detection Using Word Embedding Models: Experiments on <fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Iuliia</first><last>Zaitova</last></author>
      <author><first>Irina</first><last>Stenger</last></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <pages>1265–1273</pages>
      <abstract>Microsyntactic units have been defined as language-specific transitional entities between lexicon and grammar, whose idiomatic properties are closely tied to syntax. These units are typically described based on individual constructions, making it difficult to understand them comprehensively as a class. This study proposes a novel approach to detect microsyntactic units using Word Embedding Models (WEMs) trained on six Slavic languages, namely Belarusian, Bulgarian, Czech, Polish, Russian, and Ukrainian, and evaluates how well these models capture the nuances of syntactic non-compositionality. To evaluate the models, we develop a cross-lingual inventory of microsyntactic units using the lists of microsyntantic units available at the Russian National Corpus. Our results demonstrate the effectiveness of WEMs in capturing microsyntactic units across all six Slavic languages under analysis. Additionally, we find that WEMs tailored for syntax-based tasks consistently outperform other WEMs at the task. Our findings contribute to the theory of microsyntax by providing insights into the detection of microsyntactic units and their cross-linguistic properties.</abstract>
      <url hash="73e00140">2023.ranlp-1.134</url>
      <bibkey>zaitova-etal-2023-microsyntactic</bibkey>
    </paper>
    <paper id="135">
      <title>Systematic <fixed-case>T</fixed-case>ext<fixed-case>R</fixed-case>ank Optimization in Extractive Summarization</title>
      <author><first>Morris</first><last>Zieve</last></author>
      <author><first>Anthony</first><last>Gregor</last></author>
      <author><first>Frederik Juul</first><last>Stokbaek</last></author>
      <author><first>Hunter</first><last>Lewis</last></author>
      <author><first>Ellis Marie</first><last>Mendoza</last></author>
      <author><first>Benyamin</first><last>Ahmadnia</last></author>
      <pages>1274–1281</pages>
      <abstract>With the ever-growing amount of textual data, extractive summarization has become increasingly crucial for efficiently processing information. The TextRank algorithm, a popular unsupervised method, offers excellent potential for this task. In this paper, we aim to optimize the performance of TextRank by systematically exploring and verifying the best preprocessing and fine-tuning techniques. We extensively evaluate text preprocessing methods, such as tokenization, stemming, and stopword removal, to identify the most effective combination with TextRank. Additionally, we examine fine-tuning strategies, including parameter optimization and incorporation of domain-specific knowledge, to achieve superior summarization quality.</abstract>
      <url hash="94cf6aad">2023.ranlp-1.135</url>
      <bibkey>zieve-etal-2023-systematic</bibkey>
    </paper>
  </volume>
  <volume id="stud" ingest-date="2023-11-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 8th Student Research Workshop associated with the International Conference Recent Advances in Natural Language Processing</booktitle>
      <editor><first>Momchil</first><last>Hardalov</last></editor>
      <editor><first>Zara</first><last>Kancheva</last></editor>
      <editor><first>Boris</first><last>Velichkov</last></editor>
      <editor><first>Ivelina</first><last>Nikolova-Koleva</last></editor>
      <editor><first>Milena</first><last>Slavcheva</last></editor>
      <publisher>INCOMA Ltd., Shoumen, Bulgaria</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2023</year>
      <url hash="ef06c29f">2023.ranlp-stud</url>
      <venue>ranlp</venue>
    </meta>
    <frontmatter>
      <url hash="08309289">2023.ranlp-stud.0</url>
      <bibkey>ranlp-2023-student</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Detecting <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>: A Survey of the State of Detecting <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>-Generated Text</title>
      <author><first>Mahdi</first><last>Dhaini</last></author>
      <author><first>Wessel</first><last>Poelman</last></author>
      <author><first>Ege</first><last>Erdogan</last></author>
      <pages>1–12</pages>
      <abstract>While recent advancements in the capabilities and widespread accessibility of generative language models, such as ChatGPT (OpenAI, 2022), have brought about various benefits by generating fluent human-like text, the task of distinguishing between human- and large language model (LLM) generated text has emerged as a crucial problem. These models can potentially deceive by generating artificial text that appears to be human-generated. This issue is particularly significant in domains such as law, education, and science, where ensuring the integrity of text is of the utmost importance. This survey provides an overview of the current approaches employed to differentiate between texts generated by humans and ChatGPT. We present an account of the different datasets constructed for detecting ChatGPT-generated text, the various methods utilized, what qualitative analyses into the characteristics of human versus ChatGPT-generated text have been performed, and finally, summarize our findings into general insights.</abstract>
      <url hash="98e67dab">2023.ranlp-stud.1</url>
      <bibkey>dhaini-etal-2023-detecting</bibkey>
    </paper>
    <paper id="2">
      <title>Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models</title>
      <author><first>Lautaro</first><last>Estienne</last></author>
      <pages>13–22</pages>
      <abstract>A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only a few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach where calibration is performed without using any adaptation data.</abstract>
      <url hash="a948098c">2023.ranlp-stud.2</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b2944fcc">2023.ranlp-stud.2.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>estienne-2023-unsupervised</bibkey>
    </paper>
    <paper id="3">
      <title>Controllable Active-Passive Voice Generation using Prefix Tuning</title>
      <author><first>Valentin</first><last>Knappich</last></author>
      <author><first>Timo Pierre</first><last>Schrader</last></author>
      <pages>23–32</pages>
      <abstract>The prompting paradigm is an uprising trend in the field of Natural Language Processing (NLP) that aims to learn tasks by finding appropriate prompts rather than fine-tuning the model weights. Such prompts can express an intention, e.g., they can instruct a language model to generate a summary of a given event. In this paper, we study how to influence (”control”) the language generation process such that the outcome fulfills a requested linguistic property. More specifically, we look at controllable active-passive (AP) voice generation, i.e., we require the model to generate a sentence in the requested voice. We build upon the prefix tuning approach and introduce control tokens that are trained on controllable AP generation. We create an AP subset of the WebNLG dataset to fine-tune these control tokens. Among four different models, the one trained with a contrastive learning approach yields the best results in terms of AP accuracy ( 95%) but at the cost of decreased performance on the original WebNLG task.</abstract>
      <url hash="499c9345">2023.ranlp-stud.3</url>
      <bibkey>knappich-schrader-2023-controllable</bibkey>
    </paper>
    <paper id="4">
      <title>Age-Specific Linguistic Features of Depression via Social Media</title>
      <author><first>Charlotte</first><last>Rosario</last></author>
      <pages>33–43</pages>
      <abstract>Social media data has become a crucial resource for understanding and detecting mental health challenges. However, there is a significant gap in our understanding of age-specific linguistic markers associated with classifying depression. This study bridges the gap by analyzing 25,241 text samples from 15,156 Reddit users with self-reported depression across two age groups: adolescents (13-20 year olds) and adults (21+). Through a quantitative exploratory analysis using LIWC, topic modeling, and data visualization, distinct patterns and topical differences emerged in the language of depression for adolescents and adults, including social concerns, temporal focuses, emotions, and cognition. These findings enhance our understanding of how depression is expressed on social media, bearing implications for accurate classification and tailored interventions across different age groups.</abstract>
      <url hash="2d21091b">2023.ranlp-stud.4</url>
      <bibkey>rosario-2023-age</bibkey>
    </paper>
    <paper id="5">
      <title>Trigger Warnings: A Computational Approach to Understanding User-Tagged Trigger Warnings</title>
      <author><first>Sarthak</first><last>Tyagi</last></author>
      <author><first>Adwita</first><last>Arora</last></author>
      <author><first>Krish</first><last>Chopra</last></author>
      <author><first>Manan</first><last>Suri</last></author>
      <pages>44–54</pages>
      <abstract>Content and trigger warnings give information about the content of material prior to receiving it and are used by social media users to tag their content when discussing sensitive topics. Trigger warnings are known to yield benefits in terms of an increased individual agency to make an informed decision about engaging with content. At the same time, some studies contest the benefits of trigger warnings suggesting that they can induce anxiety and reinforce the traumatic experience of specific identities. Our study involves the analysis of the nature and implications of the usage of trigger warnings by social media users using empirical methods and machine learning. Further, we aim to study the community interactions associated with trigger warnings in online communities, precisely the diversity and content of responses and inter-user interactions. The domains of trigger warnings covered will include self-harm, drug abuse, suicide, and depression. The analysis of the above domains will assist in a better understanding of online behaviour associated with them and help in developing domain-specific datasets for further research</abstract>
      <url hash="f3a1c63c">2023.ranlp-stud.5</url>
      <bibkey>tyagi-etal-2023-trigger</bibkey>
    </paper>
    <paper id="6">
      <title>Evaluating Hallucinations in Large Language Models for <fixed-case>B</fixed-case>ulgarian Language</title>
      <author><first>Melania</first><last>Berbatova</last></author>
      <author><first>Yoan</first><last>Salambashev</last></author>
      <pages>55–63</pages>
      <abstract>In this short paper, we introduce the task of evaluating the hallucination of large language models for the Bulgarian language. We first give definitions of what is a hallucination in large language models and what evaluation methods for measuring hallucinations exist. Next, we give an overview of the multilingual evaluation of the latest large language models, focusing on the evaluation of the performance in Bulgarian on tasks, related to hallucination. We then present a method to evaluate the level of hallucination in a given language with no reference data, and provide some initial experiments with this method in Bulgarian. Finally, we provide directions for future research on the topic.</abstract>
      <url hash="e37dc754">2023.ranlp-stud.6</url>
      <bibkey>berbatova-salambashev-2023-evaluating</bibkey>
    </paper>
    <paper id="7">
      <title>Leveraging Probabilistic Graph Models in Nested Named Entity Recognition for <fixed-case>P</fixed-case>olish</title>
      <author><first>Jędrzej</first><last>Jamnicki</last></author>
      <pages>64–67</pages>
      <abstract>This paper presents ongoing work on leveraging probabilistic graph models, specifically conditional random fields and hidden Markov models, in nested named entity recognition for the Polish language. NER is a crucial task in natural language processing that involves identifying and classifying named entities in text documents. Nested NER deals with recognizing hierarchical structures of entities that overlap with one another, presenting additional challenges. The paper discusses the methodologies and approaches used in nested NER, focusing on CRF and HMM. Related works and their contributions are reviewed, and experiments using the KPWr dataset are conducted, particularly with the BiLSTM-CRF model and Word2Vec and HerBERT embeddings. The results show promise in addressing nested NER for Polish, but further research is needed to develop robust and accurate models for this complex task.</abstract>
      <url hash="545ceafb">2023.ranlp-stud.7</url>
      <bibkey>jamnicki-2023-leveraging</bibkey>
    </paper>
    <paper id="8">
      <title>Crowdsourcing Veridicality Annotations in <fixed-case>S</fixed-case>panish: Can Speakers Actually Agree?</title>
      <author><first>Teresa</first><last>Martín Soeder</last></author>
      <pages>68–77</pages>
      <abstract>In veridicality studies, an area of research of Natural Language Inference (NLI), the factuality of different contexts is evaluated. This task, known to be a difficult one since often it is not clear what the interpretation should be Uma et al. (2021), is key for building any Natural Language Understanding (NLU) system that aims at making the right inferences. Here the results of a study that analyzes the veridicality of mood alternation and specificity in Spanish, and whose labels are based on those of Saurí and Pustejovsky (2009) are presented. It has an inter-annotator agreement of AC2 = 0.114, considerably lower than that of de Marneffe et al. (2012) (κ = 0.53), a main reference to this work; and a couple of mood-related significant effects. Due to this strong lack of agreement, an analysis of what factors cause disagreement is presented together with a discussion based on the work of de Marneffe et al. (2012) and Pavlick and Kwiatkowski (2019) about the quality of the annotations gathered and whether other types of analysis like entropy distribution could better represent this corpus. The annotations collected are available at https://github.com/narhim/veridicality_spanish.</abstract>
      <url hash="af99977c">2023.ranlp-stud.8</url>
      <bibkey>martin-soeder-2023-crowdsourcing</bibkey>
    </paper>
    <paper id="9">
      <title>Weakly supervised learning for aspect based sentiment analysis of <fixed-case>U</fixed-case>rdu Tweets</title>
      <author><first>Zoya</first><last>Maqsood</last></author>
      <pages>78–86</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) is vital for text comprehension which benefits applications across various domains. This field involves the two main sub-tasks including aspect extraction and sentiment classification. Existing methods to tackle this problem normally address only one sub-task or utilize topic models that may result in overlapping concepts. Moreover, such algorithms often rely on extensive labeled data and external language resources, making their application costly and time-consuming in new domains and especially for resource-poor languages like Urdu. The lack of aspect mining studies in Urdu literature further exacerbates the inapplicability of existing methods for Urdu language. The primary challenge lies in the preprocessing of data to ensure its suitability for language comprehension by the model, as well as the availability of appropriate pre-trained models, domain embeddings, and tools. This paper implements an ABSA model (CITATION) for unlabeled Urdu tweets with minimal user guidance, utilizing a small set of seed words for each aspect and sentiment class. The model first learns sentiment and aspect joint topic embeddings in the word embedding space with regularization to encourage topic distinctiveness. Afterwards, it employs deep neural models for pre-training with embedding-based predictions and self-training on unlabeled data. Furthermore, we optimize the model for improved performance by substituting the CNN with the BiLSTM classifier for sentence-level sentiment and aspect classification. Our optimized model achieves significant improvements over baselines in aspect and sentiment classification for Urdu tweets with accuracy of 64.8% and 72.8% respectively, demonstrating its effectiveness in generating joint topics and addressing existing limitations in Urdu ABSA.</abstract>
      <url hash="601f56e5">2023.ranlp-stud.9</url>
      <bibkey>maqsood-2023-weakly</bibkey>
    </paper>
    <paper id="10">
      <title>Exploring Low-resource Neural Machine Translation for <fixed-case>S</fixed-case>inhala-<fixed-case>T</fixed-case>amil Language Pair</title>
      <author><first>Ashmari</first><last>Pramodya</last></author>
      <pages>87–97</pages>
      <abstract>At present, Neural Machine Translation is a promising approach for machine translation. Transformer-based deep learning architectures in particular show a substantial performance increase in translating between various language pairs. However, many low-resource language pairs still struggle to lend themselves to Neural Machine Translation due to their data-hungry nature. In this article, we investigate methods of expanding the parallel corpus to enhance translation quality within a model training pipeline, starting from the initial collection of parallel data to the training process of baseline models. Grounded on state-of-the-art Neural Machine Translation approaches such as hyper-parameter tuning, and data augmentation with forward and backward translation, we define a set of best practices for improving Tamil-to-Sinhala machine translation and empirically validate our methods using standard evaluation metrics. Our results demonstrate that the Neural Machine Translation models trained on larger amounts of back-translated data outperform other synthetic data generation approaches in Transformer base training settings. We further demonstrate that, even for language pairs with limited resources, Transformer models are able to tune to outperform existing state-of-the-art Statistical Machine Translation models by as much as 3.28 BLEU points in the Tamil to Sinhala translation scenarios.</abstract>
      <url hash="0bc96722">2023.ranlp-stud.10</url>
      <bibkey>pramodya-2023-exploring</bibkey>
    </paper>
    <paper id="11">
      <title>Prompting <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> to Draw Morphological Connections for New Word Comprehension</title>
      <author><first>Bianca-Madalina</first><last>Zgreaban</last></author>
      <author><first>Rishabh</first><last>Suresh</last></author>
      <pages>98–107</pages>
      <abstract>Though more powerful, Large Language Models need to be periodically retrained for updated information, consuming resources and energy. In this respect, prompt engineering can prove a possible solution to re-training. To explore this line of research, this paper uses a case study, namely, finding the best prompting strategy for asking ChatGPT to define new words based on morphological connections. To determine the best prompting strategy, each definition provided by the prompt was ranked in terms of plausibility and humanlikeness criteria. The findings of this paper show that adding contextual information, operationalised as the keywords ‘new’ and ‘morpheme’, significantly improve the performance of the model for any prompt. While no single prompt significantly outperformed all others, there were differences between performances on the two criteria for most prompts. ChatGPT also provided the most correct definitions with a persona-type prompt.</abstract>
      <url hash="b952d44d">2023.ranlp-stud.11</url>
      <attachment type="OptionalSupplementaryMaterial" hash="58b2657c">2023.ranlp-stud.11.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>zgreaban-suresh-2023-prompting</bibkey>
    </paper>
  </volume>
  <event id="ranlp-2023">
    <colocated>
      <volume-id>2023.alp-1</volume-id>
      <volume-id>2023.contents-1</volume-id>
      <volume-id>2023.humeval-1</volume-id>
      <volume-id>2023.nlp4tia-1</volume-id>
      <volume-id>2023.tsar-1</volume-id>
      <volume-id>2023.case-1</volume-id>
      <volume-id>2023.dravidianlangtech-1</volume-id>
      <volume-id>2023.ltedi-1</volume-id>
    </colocated>
  </event>
</collection>
