<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.quantumnlp">
  <volume id="1" ingest-date="2026-01-13" type="proceedings">
    <meta>
      <booktitle>Proceedings of the QuantumNLP{:} Integrating Quantum Computing with Natural Language Processing</booktitle>
      <editor><first>Santanu</first><last>Pal</last></editor>
      <editor><first>Partha</first><last>Pakray</last></editor>
      <editor><first>Priyanka</first><last>Jain</last></editor>
      <editor><first>Asif</first><last>Ekbal</last></editor>
      <editor id="sivaji-bandyopadhyay"><first>Sivaji</first><last>Bandyopadhyay</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mumbai, India (Hybrid)</address>
      <month>November</month>
      <year>2025</year>
      <url hash="1ebb63fa">2025.quantumnlp-1</url>
      <venue>quantumnlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-306-7</isbn>
    </meta>
    <frontmatter>
      <url hash="93c0c4f7">2025.quantumnlp-1.0</url>
      <bibkey>quantumnlp-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Quantum-Infused Whisper: A Framework for Replacing Classical Components</title>
      <author><first>Tapabrata</first><last>Mondal</last></author>
      <author id="debjit-dhar" orcid="0009-0004-8495-895X"><first>Debjit</first><last>Dhar</last></author>
      <author><first>Soham</first><last>Lahiri</last></author>
      <author id="sivaji-bandyopadhyay"><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>1-5</pages>
      <abstract>We propose a compact hybrid quantum–classical extension of OpenAI’s Whisper in which classical components are replaced by Quantum Convolutional Neural Networks (QCNN), Quantum LSTMs (QLSTM), and optional Quantum Adaptive Self-Attention (QASA). Log-mel spectrograms are angle encoded and processed by QCNN kernels, whose outputs feed a Transformer encoder, while QLSTM-based decoding introduces quantum-enhanced temporal modeling. The design incorporates pretrained acoustic embeddings and is constrained to NISQ-feasible circuit depths and qubit counts. Although this work is primarily architectural, we provide a fully specified, reproducible evaluation plan using Speech Commands, LibriSpeech, and Common Voice, along with strong classical baselines and measurable hypotheses for assessing noise robustness, efficiency, and parameter sparsity. To our knowledge, this is the first hardware-aware, module-wise quantum replacement framework for Whisper.</abstract>
      <url hash="1b65c798">2025.quantumnlp-1.1</url>
      <bibkey>mondal-etal-2025-quantum</bibkey>
    </paper>
    <paper id="2">
      <title>These Aren’t the Vectors You’re Looking For: A Proof of Quantum Advantage in Compositional Generalization</title>
      <author><first>Karthik</first><last>Srikumar</last><affiliation>, University of Connecticut</affiliation></author>
      <pages>6-9</pages>
      <abstract>Compositional generalization, the ability to systematically combine known concepts to understand and produce novel expressions, remains a fundamental, unsolved challenge for classical neural language models, whose reliance on statistical correlations in high-dimensional vector spaces inherently limits them. This paper establishes the first rigorous theoretical guarantee of an exponential quantum advantage for compositional generalization. We prove that classical language models, which represent concepts as vectors in <tex-math>\mathbb{R}^d</tex-math>, require a latent dimension scaling linearly with the number of concepts and compositional rules to avoid catastrophic interference. In contrast, we introduce the Quantum Compositional Embedding (QCE) framework, which leverages the intrinsic properties of quantum mechanics. In doing so, we demonstrate that QCE, utilizing only a logarithmic number of qubits, can perfectly represent and generalize compositional structures, a task provably impossible for classical models of equivalent dimensionality. The separation is proven to be exponential, providing a compelling theoretical foundation for quantum natural language processing.</abstract>
      <url hash="0c3f6409">2025.quantumnlp-1.2</url>
      <bibkey>srikumar-2025-arent</bibkey>
    </paper>
    <paper id="3">
      <title>Hybrid Classical-Quantum Framework for Sentiment Classification and Claim Check-Worthiness Identification in <fixed-case>B</fixed-case>engali</title>
      <author><first>Pritam</first><last>Pal</last><affiliation>Jadavpur University</affiliation></author>
      <author id="dipankar-das" orcid="0000-0002-8110-9344"><first>Dipankar</first><last>Das</last><affiliation>Jadavpur University</affiliation></author>
      <author id="anup-kumar-kolya"><first>Anup Kumar</first><last>Kolya</last><affiliation>Government Technology Agency of Singapore</affiliation></author>
      <author><first>Siddhartha</first><last>Bhattacharyya</last></author>
      <pages>10-19</pages>
      <abstract>Traditional machine learning and deep learning models have demonstrated remarkable performance across various NLP tasks in multiple languages. However, these conventional models often struggle with languages with complex linguistic structures and nuanced contexts, such as Bengali. Recent advancements in quantum computing offer promising solutions for tackling complex, computationally challenging problems, providing faster, more efficient processing than classical systems. This research aims to address the challenges posed by the intricate linguistic structure of the less-resourced Bengali language by developing a quantum-enhanced framework for sentiment classification and claim-checkworthiness identification. We created a classical LSTM framework and proposed novel 2-qubit and 4-qubit classical-quantum frameworks, evaluating their effectiveness for sentiment classification and claim-checkworthiness identification tasks in Bengali. An entirely new dataset comprising <tex-math>\approx</tex-math>3K samples was developed by curating Bengali news headlines from prominent sources. We tagged these headlines with sentiment and claim checkworthy labels using state-of-the-art LLMs. Our findings indicate that the quantum-enhanced frameworks outperform the traditional models in both tasks. Notably, the 4-qubit-based framework achieved the highest F1-score in sentiment classification, while the 2-qubit-based framework demonstrated the best F1-score in claim checkworthiness identification.</abstract>
      <url hash="10cdf86b">2025.quantumnlp-1.3</url>
      <bibkey>pal-etal-2025-hybrid</bibkey>
    </paper>
    <paper id="4">
      <title>A Hybrid Quantum-Classical Fusion for Deep Semantic Paraphrase Detection</title>
      <author><first>Devanarayanan</first><last>K</last></author>
      <author><first>Fayas S</first><last>Mohamad</last></author>
      <author><first>Dheeraj V</first><last>Mohan</last></author>
      <author><first>Reshma</first><last>Sheik</last><affiliation>Thangal Kunju Musaliar College of Engineering, Kollam</affiliation></author>
      <pages>20-25</pages>
      <abstract>Paraphrase Detection is a core task in natural language processing (NLP) that aims to determine whether two sentences convey equivalent meanings. This work proposes a hybrid quantum–classical framework that integrates Sentence-BERT embeddings, simulated quantum feature encoding, and classical machine learning models to enhance semantic similarity detection. Initially, sentence pairs are embedded using Sentence-BERT and standardized through feature scaling. These representations are then transformed via rotation-based quantum circuits to capture higher-order feature interactions and non-linear dependencies. The resulting hybrid feature space, combining classical and quantum-inspired components, is evaluated using LightGBM and deep neural network classifiers. Experimental results show that the hybrid model incorporating quantum-inspired features achieved superior classification performance, yielding a 10% improvement in overall accuracy outperforming standalone deep learning baselines. These findings demonstrate that quantum–classical fusion enhances semantic feature extraction and significantly improves paraphrase detection performance.</abstract>
      <url hash="2d6c2a8c">2025.quantumnlp-1.4</url>
      <bibkey>k-etal-2025-hybrid</bibkey>
    </paper>
    <paper id="5">
      <title>Quantum-Enhanced Gated Recurrent Units for Part-of-Speech Tagging</title>
      <author><first>Ashutosh</first><last>Rai</last></author>
      <author><first>Shyambabu</first><last>Pandey</last></author>
      <author id="partha-pakray" orcid="0000-0003-3834-5154"><first>Partha</first><last>Pakray</last><affiliation>National Institute of Technology Silchar</affiliation></author>
      <pages>26-32</pages>
      <abstract>Deep learning models for Natural Language Processing (NLP) tasks, such as Part-of-Speech (POS) tagging, usually have significant parameter counts that make them costly to train and deploy. Quantum Machine Learning (QML) offers a potential approach for building more parameter-efficient models. This paper proposes a hybrid quantum-classical gated recurrent unit model for POS tagging in code-mixed social media text. By integrating a quantum layer into the recurrent framework, our model achieved an accuracy comparable to the baseline classical model, while needing fewer parameters. Although the cut-off point in the parameters is modest in our setup, the approach is promising when scaled to deeper architectures. These results suggest that hybrid models can offer a resource-efficient alternative for NLP tasks.</abstract>
      <url hash="f5d60d22">2025.quantumnlp-1.5</url>
      <bibkey>rai-etal-2025-quantum</bibkey>
    </paper>
    <paper id="6">
      <title>A Review of Quantum Computing Approaches to Semantic Search and Text Classification in Natural Language Processing</title>
      <author><first>Sauvik</first><last>Bal</last><affiliation>Techno India University</affiliation></author>
      <pages>33-43</pages>
      <abstract>While having enhanced NLP, deep learning and pre-trained language models requires a lot of processing power. The work showcases the potential of quantum computing by mapping linguistic data into vast, high-dimensional Hilbert spaces through entanglement and superposition. It focuses on mathematical concepts that set quantum approaches apart from classical ones, among them being the fidelity-based similarity and quantum probability. Various quantum machine learning models are considered in this article, including Quantum Neural Networks and Quantum Support Vector Machines, each discussing the computational advantages in pattern recognition. In addition, it considers retrieval techniques like Grover’s algorithm, showing how quantum similarity functions give better semantic search. Indeed, the comparison does show that quantum techniques might yield advantages regarding expressiveness and scalability, despite obstacles such as hardware noise and data encoding. Notwithstanding that quantum technology is still in its infancy, future improvements might advance language understanding.</abstract>
      <url hash="b2eb47ab">2025.quantumnlp-1.6</url>
      <bibkey>bal-2025-review</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>QCNN</fixed-case>-<fixed-case>MFND</fixed-case>: A Novel Quantum <fixed-case>CNN</fixed-case> Framework for Multimodal Fake News Detection in Social Media</title>
      <author><first>Arya</first><last>Suneesh</last><affiliation>TIFIN</affiliation></author>
      <author><first>Balasubramanian</first><last>Palani</last><affiliation>Indian Institute of Information Technology Kottayam, Indian Institute of Information Technology Kottayam</affiliation></author>
      <pages>44-52</pages>
      <abstract>Fake news on social media platforms poses significant threats to public trust and information integrity. This research explores the application of quantum machine learning (QML) techniques for detecting fake news by leveraging quantum computing’s unique capabilities. Our work introduces a hybrid quantum-classical framework that utilizes quantum convolutional neural networks (QCNNs) with angle and amplitude encoding schemes for processing multimodal features from text and images. Experiments conducted on benchmark datasets - GossipCop and Politifact - demonstrate that our quantum-enhanced model achieves superior performance compared to classical approaches, with accuracy rates of 88.52% and 85.58%, and F1 scores of 93.19% and 90.20% respectively. Our findings establish QML as a viable approach for addressing the challenges of fake news detection in the digital era.</abstract>
      <url hash="da9a96f1">2025.quantumnlp-1.7</url>
      <bibkey>suneesh-palani-2025-qcnn</bibkey>
    </paper>
    <paper id="8">
      <title>A Systematic Survey of Quantum Natural Language Processing: Models, Encoding Paradigms, and Evaluation Methods</title>
      <author id="arpita-vats" orcid="0009-0009-4831-4109"><first>Arpita</first><last>Vats</last></author>
      <author><first>Rahul</first><last>Raja</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Ashish</first><last>Kattamuri</last></author>
      <author><first>Abhinav</first><last>Bohra</last><affiliation>Amazon</affiliation></author>
      <pages>53-64</pages>
      <abstract>Quantum Natural Language Processing (QNLP) is an emerging interdisciplinary field at the intersection of quantum computing, natural language understanding, and formal linguistic theory. As advances in quantum hardware and algorithms accelerate, QNLP promises new paradigms for representation learning, semantic modeling, and efficient computation. However, existing literature remains fragmented, with no unified synthesis across modeling, encoding, and evaluation dimensions. In this work, we present the first systematic and taxonomy driven survey of QNLP that holistically organizes research spanning three core dimensions: computational models, encoding paradigms, and evaluation frameworks. First, we analyze foundational approaches that map linguistic structures into quantum formalism, including categorical compositional models, variational quantum circuits, and hybrid quantum classical architectures. Second, we introduce a unified taxonomy of encoding strategies, ranging from quantum tokenization and state preparation to embedding based encodings, highlighting tradeoffs in scalability, noise resilience, and expressiveness. Third, we provide the first comparative synthesis of evaluation methodologies, benchmark datasets, and performance metrics, while identifying reproducibility and standardization gaps.We further contrast quantum inspired NLP methods with fully quantum implemented systems, offering insights into resource efficiency, hardware feasibility, and real world applicability. Finally, we outline open challenges such as integration with LLMs and unified benchmark design, and propose a research agenda for advancing QNLP as a scalable and reliable discipline.</abstract>
      <url hash="8c857e89">2025.quantumnlp-1.8</url>
      <bibkey>vats-etal-2025-systematic</bibkey>
    </paper>
    <paper id="9">
      <title>A Survey of Quantum Natural Language Processing: From Compositional Models to <fixed-case>NISQ</fixed-case>-Era Empiricism</title>
      <author id="arpan-phukan" orcid="0000-0002-9253-1022"><first>Arpan</first><last>Phukan</last><affiliation>Indian Institute of Technology, Patna</affiliation></author>
      <author id="asif-ekbal" orcid="0000-0003-3612-8834"><first>Asif</first><last>Ekbal</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <pages>65-75</pages>
      <abstract>Quantum Natural Language Processing (QNLP) has emerged as a novel paradigm that leverages the principles of quantum mechanics to address fundamental challenges in language modeling, particularly in capturing compositional meaning. This survey charts the evolution of QNLP, from its theoretical foundations in the Distributional Compositional Categorical (DisCoCat) framework to its modern implementation on Noisy Intermediate-Scale Quantum (NISQ) hardware. We review the primary architectural approaches, including variational quantum circuits and tensor networks, and summarize the growing body of empirical work in tasks such as text classification, sentence similarity, and question answering. A recurring finding is the potential for QNLP models to achieve competitive performance with significantly fewer parameters than their classical counterparts. However, the field is critically constrained by the limitations of NISQ-era hardware. We conclude by discussing these challenges and outlining the future trajectory towards achieving a demonstrable quantum advantage and building more interpretable, efficient language models.</abstract>
      <url hash="f4c823cb">2025.quantumnlp-1.9</url>
      <bibkey>phukan-ekbal-2025-survey</bibkey>
    </paper>
  </volume>
</collection>
