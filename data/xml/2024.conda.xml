<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.conda">
  <volume id="1" ingest-date="2024-07-23" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Data Contamination (CONDA)</booktitle>
      <editor><first>Oscar</first><last>Sainz</last></editor>
      <editor><first>Iker</first><last>García Ferrero</last></editor>
      <editor><first>Eneko</first><last>Agirre</last></editor>
      <editor><first>Jon</first><last>Ander Campos</last></editor>
      <editor><first>Alon</first><last>Jacovi</last></editor>
      <editor><first>Yanai</first><last>Elazar</last></editor>
      <editor><first>Yoav</first><last>Goldberg</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="7812d39d">2024.conda-1</url>
      <venue>conda</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="1374a644">2024.conda-1.0</url>
      <bibkey>conda-2024-data</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Evaluating <fixed-case>C</fixed-case>hinese Large Language Models on Discipline Knowledge Acquisition via Memorization and Robustness Assessment</title>
      <author><first>Chuang</first><last>Liu</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Renren</first><last>Jin</last></author>
      <author><first>Mark</first><last>Steedman</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>1-12</pages>
      <abstract>Chinese LLMs demonstrate impressive performance on NLP tasks, particularly on discipline knowledge benchmarks, with some results approaching those of GPT-4. Previous research has viewed these advancements as potential outcomes of data contamination or leakage, prompting efforts to create new detection methods and address evaluation issues in LLM benchmarks. However, there has been a lack of comprehensive assessment of the evolution of Chinese LLMs. To address this gap, this paper offers a thorough investigation of Chinese LLMs on discipline knowledge evaluation, delving into the advancements of various LLMs, including a group of related models and others. Specifically, we have conducted six assessments ranging from knowledge memorization to comprehension for robustness, encompassing tasks like predicting incomplete questions and options, identifying behaviors by the contaminational fine-tuning, and answering rephrased questions. Experimental findings indicate a positive correlation between the release time of LLMs and their memorization capabilities, but they struggle with variations in original question-options pairs. Additionally, our findings suggest that question descriptions have a more significant impact on LLMs’ performance.</abstract>
      <url hash="c02a0e92">2024.conda-1.1</url>
      <bibkey>liu-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.conda-1.1</doi>
    </paper>
    <paper id="2">
      <title>Confounders in Instance Variation for the Analysis of Data Contamination</title>
      <author><first>Behzad</first><last>Mehrbakhsh</last><affiliation>Universidad Politécnica de Valencia</affiliation></author>
      <author><first>Dario</first><last>Garigliotti</last><affiliation>University of Bergen</affiliation></author>
      <author><first>Fernando</first><last>Martínez-Plumed</last><affiliation>Universitat Politècnica de València</affiliation></author>
      <author><first>Jose</first><last>Hernandez-Orallo</last><affiliation>Universitat Politecnica de Valencia</affiliation></author>
      <pages>13-21</pages>
      <abstract>Test contamination is a serious problem for the evaluation of large language models (LLMs) because it leads to the overestimation of their performance and a quick saturation of benchmarks, even before the actual capability is achieved. One strategy to address this issue is the (adversarial) generation of variations, by including different exemplars and different rephrasings of the questions. However, these two interventions can lead to instances that can be more difficult (accumulating on the expected loss of performance by partly removing the contamination) but also to instances that can be less difficult (cancelling the expected loss of performance), which would make contamination undetectable. Understanding these two phenomena in terms of instance difficulty is critical to determine and measure contamination. In this paper we conduct a comprehensive analysis of these two interventions on an addition task with fine-tuned LLAMA-2 models.</abstract>
      <url hash="32ffcf83">2024.conda-1.2</url>
      <bibkey>mehrbakhsh-etal-2024-confounders</bibkey>
      <doi>10.18653/v1/2024.conda-1.2</doi>
    </paper>
    <paper id="3">
      <title>A Taxonomy for Data Contamination in Large Language Models</title>
      <author><first>Medha</first><last>Palavalli</last></author>
      <author><first>Amanda</first><last>Bertsch</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Matthew</first><last>Gormley</last><affiliation>Solventum and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>22-40</pages>
      <abstract>Large language models pretrained on extensive web corpora demonstrate remarkable performance across a wide range of downstream tasks. However, a growing concern is data contamination, where evaluation datasets may unintentionally be contained in the pretraining corpus, inflating model performance. Decontamination, the process of detecting and removing such data, is a potential solution; yet these contaminants may originate from altered versions of the test set, evading detection during decontamination. How different types of contamination impact the performance of language models on downstream tasks is not fully understood. We present a taxonomy that categorizes the various types of contamination encountered by LLMs during the pretraining phase and identify which types pose the highest risk. We analyze the impact of contamination on two key NLP tasks—summarization and question answering—revealing how different types of contamination influence task performance during evaluation.</abstract>
      <url hash="fa3c42d3">2024.conda-1.3</url>
      <bibkey>palavalli-etal-2024-taxonomy</bibkey>
      <doi>10.18653/v1/2024.conda-1.3</doi>
    </paper>
    <paper id="4">
      <title>Data Contamination Report from the 2024 <fixed-case>CONDA</fixed-case> Shared Task</title>
      <author><first>Oscar</first><last>Sainz</last></author>
      <author><first>Iker</first><last>García-Ferrero</last></author>
      <author><first>Alon</first><last>Jacovi</last></author>
      <author><first>Jon</first><last>Ander Campos</last></author>
      <author><first>Yanai</first><last>Elazar</last></author>
      <author><first>Eneko</first><last>Agirre</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Wei-Lin</first><last>Chen</last></author>
      <author><first>Jenny</first><last>Chim</last></author>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Luca</first><last>D’Amico-Wong</last></author>
      <author><first>Melissa</first><last>Dell</last></author>
      <author><first>Run-Ze</first><last>Fan</last></author>
      <author><first>Shahriar</first><last>Golchin</last></author>
      <author><first>Yucheng</first><last>Li</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Bhavish</first><last>Pahwa</last></author>
      <author><first>Ameya</first><last>Prabhu</last></author>
      <author><first>Suryansh</first><last>Sharma</last></author>
      <author><first>Emily</first><last>Silcock</last></author>
      <author><first>Kateryna</first><last>Solonko</last></author>
      <author><first>David</first><last>Stap</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <author><first>Yu-Min</first><last>Tseng</last></author>
      <author><first>Vishaal</first><last>Udandarao</last></author>
      <author><first>Zengzhi</first><last>Wang</last></author>
      <author><first>Ruijie</first><last>Xu</last></author>
      <author><first>Jinglin</first><last>Yang</last></author>
      <pages>41-56</pages>
      <abstract>The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant aspects of data contamination in natural language processing, where data contamination is understood as situations where evaluation data is included in pre-training corpora used to train large scale models, compromising evaluation results. The workshop fostered a shared task to collect evidence on data contamination in current available datasets and models. The goal of the shared task and associated database is to assist the community in understanding the extent of the problem and to assist researchers in avoiding reporting evaluation results on known contaminated resources. The shared task provides a structured, centralized public database for the collection of contamination evidence, open to contributions from the community via GitHub pool requests. This first compilation paper is based on 566 reported entries over 91 contaminated sources from a total of 23 contributors. The details of the individual contamination events are available in the platform. The platform continues to be online, open to contributions from the community.</abstract>
      <url hash="311b4de7">2024.conda-1.4</url>
      <bibkey>sainz-etal-2024-data</bibkey>
      <doi>10.18653/v1/2024.conda-1.4</doi>
    </paper>
  </volume>
</collection>
