<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.cl4health">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Patient-Oriented Language Processing (CL4Health)</booktitle>
      <editor><first>Sophia</first><last>Ananiadou</last></editor>
      <editor><first>Dina</first><last>Demner-Fushman</last></editor>
      <editor><first>Deepak</first><last>Gupta</last></editor>
      <editor><first>Paul</first><last>Thompson</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>May</month>
      <year>2025</year>
      <url hash="f33b6f56">2025.cl4health-1</url>
      <venue>cl4health</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-238-1</isbn>
    </meta>
    <frontmatter>
      <url hash="52a529b8">2025.cl4health-1.0</url>
      <bibkey>cl4health-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>P</fixed-case>atient<fixed-case>D</fixed-case>x: Merging Large Language Models for Protecting Data-Privacy in Healthcare</title>
      <author><first>Jose G.</first><last>Moreno</last><affiliation>Paul Sabatier University - IRIT</affiliation></author>
      <author><first>Jesus</first><last>Lovon-Melgarejo</last><affiliation>IRIT</affiliation></author>
      <author><first>M’rick</first><last>Robin-Charlet</last><affiliation>Université Paul Sabatier</affiliation></author>
      <author><first>Christine</first><last>Damase-Michel</last><affiliation>Université Paul Sabatier</affiliation></author>
      <author><first>Lynda</first><last>Tamine</last><affiliation>IRIT</affiliation></author>
      <pages>1-11</pages>
      <url hash="e02310ce">2025.cl4health-1.1</url>
      <bibkey>moreno-etal-2025-patientdx</bibkey>
    </paper>
    <paper id="2">
      <title>Synthetic Documents for Medical Tasks: Bridging Privacy with Knowledge Injection and Reward Mechanism</title>
      <author><first>Simon</first><last>Meoni</last><affiliation>Arkhn/INRIA</affiliation></author>
      <author><first>Éric</first><last>De La Clergerie</last><affiliation>INRIA</affiliation></author>
      <author><first>Théo</first><last>Ryffel</last><affiliation>Arkhn</affiliation></author>
      <pages>12-25</pages>
      <abstract>Electronic Health Records (EHR) store valuable patient-staff interaction data. Recent advancements in proprietary online large language models (LLMs) have shown promising capabilities in analyzing EHR notes. However, transmitting patient information through external APIs to LLMs like ChatGPT introduces privacy risks, necessitating alternative approaches that conform to hospital practices.To address privacy concerns, we propose generating synthetic documents based on a reward-mechanism-trained model from real documents without leaking sensitive information. These synthetic documents may be annotated by large proprietary models or existing public ones, and used to train small specialized models that can run on constrained medical infrastructure. We validate our approach through a proof-of-concept scenario using {mimic, assessing the effectiveness of the generated documents through several downstream tasks: a series of ICD-9 multi-label classifications of varying complexity and a synthetic Named Entity Recognition (NER) task. The results demonstrate that synthetic documents preserve privacy and improve performance when real annotated data are sparse</abstract>
      <url hash="0a1f86f4">2025.cl4health-1.2</url>
      <bibkey>meoni-etal-2025-synthetic</bibkey>
    </paper>
    <paper id="3">
      <title>Prefix-Enhanced Large Language Models with Reused Training Data in Multi-Turn Medical Dialogue</title>
      <author><first>Suxue</first><last>Ma</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Zhicheng</first><last>Yang</last><affiliation>PAII Inc.</affiliation></author>
      <author><first>Ruei-Sung</first><last>Lin</last><affiliation>PAII Inc.</affiliation></author>
      <author><first>Youbao</first><last>Tang</last><affiliation>PAII Inc.</affiliation></author>
      <author><first>Ning</first><last>Zhang</last><affiliation>PAII</affiliation></author>
      <author><first>Zhenjie</first><last>Cao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yuan</first><last>Ni</last><affiliation>pingan health tech</affiliation></author>
      <author><first>Jing</first><last>Xiao</last><affiliation>Ping An Technology</affiliation></author>
      <author><first>Jieke</first><last>Hou</last><affiliation>Pingan health and technology</affiliation></author>
      <author><first>Peng</first><last>Chang</last><affiliation>PAII Inc.</affiliation></author>
      <pages>26-33</pages>
      <abstract>Large Language Models have made impressive progress in the medical field. In medical dialogue scenarios, unlike traditional single-turn question-answering tasks, multi-turn doctor-patient dialogue tasks require AI doctors to interact with patients in multiple rounds, where the quality of each response impacts the overall model performance. In this paper, we propose PERT to re-explore values of multi-turn dialogue training data after the supervised fine-tuning phase by integrating a prefix learning strategy, further enhancing the response quality. Our preliminary results show that PERT achieves notable improvements on gynecological data, with an increase of up to 0.22 on a 5-point rating scale.</abstract>
      <url hash="55ead9ff">2025.cl4health-1.3</url>
      <bibkey>ma-etal-2025-prefix</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>S</fixed-case>pecialty<fixed-case>S</fixed-case>cribe: Enhancing <fixed-case>SOAP</fixed-case> note Scribing for Medical Specialties using <fixed-case>LLM</fixed-case>’s</title>
      <author><first>Sagar</first><last>Goyal</last><affiliation>DeepScribe Inc.</affiliation></author>
      <author><first>Eti</first><last>Rastogi</last><affiliation>DeepScribe</affiliation></author>
      <author><first>Fen</first><last>Zhao</last><affiliation>DeepScribe Inc.</affiliation></author>
      <author><first>Dong</first><last>Yuan</last><affiliation>Google</affiliation></author>
      <author><first>Andrew</first><last>Beinstein</last><affiliation>DeepScribe Inc.</affiliation></author>
      <pages>34-45</pages>
      <abstract>The healthcare industry has accumulated vast amounts of clinical data, much of which has traditionally been unstructured, including medical records, clinical data, patient communications, and visit notes. Clinician-patient conversations form a crucial part of medical records, with the resulting medical note serving as the ground truth for future interactions and treatment plans. Generating concise and accurate SOAP notes is critical for quality patient care and is especially challenging in specialty care, where relevance, clarity, and adherence to clinician preferences are paramount. These requirements make general-purpose LLMs unsuitable for producing high-quality specialty notes. While recent LLMs like GPT-4 and Sonnet 3.5 have shown promise, their high cost, size, latency, and privacy issues remain barriers for many healthcare providers.We introduce SpecialtyScribe, a modular pipeline for generating specialty-specific medical notes. It features three components: an Information Extractor to capture relevant data, a Context Retriever to verify and augment content from transcripts, and a Note Writer to produce high quality notes. Our framework and in-house models outperform similarly sized open-source models by over 12% on ROUGE metrics.Additionally, these models match top closed-source LLMs’ performance while being under 1% of their size. We specifically evaluate our framework for oncology, with the potential for adaptation to other specialties.</abstract>
      <url hash="20421889">2025.cl4health-1.4</url>
      <bibkey>goyal-etal-2025-specialtyscribe</bibkey>
    </paper>
    <paper id="5">
      <title>Explainability for <fixed-case>NLP</fixed-case> in Pharmacovigilance: A Study on Adverse Event Report Triage in <fixed-case>S</fixed-case>wedish</title>
      <author><first>Luise</first><last>Dürlich</last><affiliation>Research Institutes of Sweden (RISE), Uppsala University, Swedish Medical Products Agency</affiliation></author>
      <author><first>Erik</first><last>Bergman</last><affiliation>Swedish Medical Products Agency</affiliation></author>
      <author><first>Maria</first><last>Larsson</last><affiliation>Swedish Medical Products Agency</affiliation></author>
      <author><first>Hercules</first><last>Dalianis</last><affiliation>DSV/Stockholm University</affiliation></author>
      <author><first>Seamus</first><last>Doyle</last><affiliation>Swedish Medical Products Agency</affiliation></author>
      <author><first>Gabriel</first><last>Westman</last><affiliation>Swedish Medical Products Agency</affiliation></author>
      <author><first>Joakim</first><last>Nivre</last><affiliation>Uppsala University</affiliation></author>
      <pages>46-68</pages>
      <abstract>In fields like healthcare and pharmacovigilance, explainability has been raised as one way of approaching regulatory compliance with machine learning and automation.This paper explores two feature attribution methods to explain predictions of four different classifiers trained to assess the seriousness of adverse event reports. On a global level, differences between models and how well important features for serious predictions align with regulatory criteria for what constitutes serious adverse reactions are analysed. In addition, explanations of reports with incorrect predictions are manually explored to find systematic features explaining the misclassification.We find that while all models seemingly learn the importance of relevant concepts for adverse event report triage, the priority of these concepts varies from model to model and between explanation methods, and the analysis of misclassified reports indicates that reporting style may affect prediction outcomes.</abstract>
      <url hash="cd4033bb">2025.cl4health-1.5</url>
      <bibkey>durlich-etal-2025-explainability</bibkey>
    </paper>
    <paper id="6">
      <title>When Multilingual Models Compete with Monolingual Domain-Specific Models in Clinical Question Answering</title>
      <author><first>Vojtech</first><last>Lanz</last><affiliation>Charles University</affiliation></author>
      <author><first>Pavel</first><last>Pecina</last><affiliation>Charles University</affiliation></author>
      <pages>69-82</pages>
      <abstract>This paper explores the performance of multilingual models in the general domain on the clinical Question Answering (QA) task to observe their potential medical support for languages that do not benefit from the existence of clinically trained models. In order to improve the model’s performance, we exploit multilingual data augmentation by translating an English clinical QA dataset into six other languages. We propose a translation pipeline including projection of the evidences (answers) into the target languages and thoroughly evaluate several multilingual models fine-tuned on the augmented data, both in mono- and multilingual settings. We find that the translation itself and the subsequent QA experiments present a differently challenging problem for each of the languages. Finally, we compare the performance of multilingual models with pretrained medical domain-specific English models on the original clinical English test set. Contrary to expectations, we find that monolingual domain-specific pretraining is not always superior to general-domain multilingual pretraining. The source code is available at https://github.com/lanzv/Multilingual-emrQA</abstract>
      <url hash="1d6554f8">2025.cl4health-1.6</url>
      <bibkey>lanz-pecina-2025-multilingual</bibkey>
    </paper>
    <paper id="7">
      <title>Mining Social Media for Barriers to Opioid Recovery with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Vinu</first><last>Ekanayake</last><affiliation>University of Kentucky</affiliation></author>
      <author><first>Md Sultan Al</first><last>Nahian</last><affiliation>University of Kentucky</affiliation></author>
      <author><first>Ramakanth</first><last>Kavuluru</last><affiliation>University of Kentucky</affiliation></author>
      <pages>83-99</pages>
      <abstract>Opioid abuse and addiction remain a major public health challenge in the US. At a broad level, barriers to recovery often take the form of individual, social, and structural issues. However, it is crucial to know the specific barriers patients face to help design better treatment interventions and healthcare policies. Researchers typically discover barriers through focus groups and surveys. While scientists can exercise better control over these strategies, such methods are both expensive and time consuming, needing repeated studies across time as new barriers emerge. We believe, this traditional approach can be complemented by automatically mining social media to determine high-level trends in both well-known and emerging barriers. In this paper, we report on such an effort by mining messages from the r/OpiatesRecovery subreddit to extract, classify, and examine barriers to opioid recovery, with special attention to the COVID-19 pandemic’s impact. Our methods involve multi-stage prompting to arrive at barriers from each post and map them to existing barriers or identify new ones. The new barriers are refined into coherent categories using embedding-based similarity measures and hierarchical clustering. Temporal analysis shows that some stigma-related barriers declined (relative to pre-pandemic), whereas systemic obstacles—such as treatment discontinuity and exclusionary practices—rose significantly during the pandemic. Our method is general enough to be applied to barrier extraction for other substance abuse scenarios (e.g., alcohol or stimulants)</abstract>
      <url hash="94a59087">2025.cl4health-1.7</url>
      <bibkey>ekanayake-etal-2025-mining</bibkey>
    </paper>
    <paper id="8">
      <title>Multimodal Transformers for Clinical Time Series Forecasting and Early Sepsis Prediction</title>
      <author><first>Jinghua</first><last>Xu</last><affiliation>Heidelberg University, Germany</affiliation></author>
      <author><first>Michael</first><last>Staniek</last><affiliation>Heidelberg University</affiliation></author>
      <pages>100-108</pages>
      <abstract>Sepsis is a leading cause of death in Intensive Care Units (ICU). Early detection of sepsis is crucial to patient survival. Existing works in the clinical domain focus mainly on directly predicting a ground truth label that is the outcome of a medical syndrome or condition such as sepsis. In this work, we primarily focus on clinical time series forecasting as a means to solve downstream predictive tasks intermediately. We base our work on a strong monomodal baseline and propose multimodal transformers using set functions via fusing both physiological features and texts in electronic health record (EHR) data. Furthermore, we propose hierarchical transformers to effectively represent clinical document time series via attention mechanism and continuous time encoding. Our multimodal models significantly outperform baseline on MIMIC-III data by notable gaps. Our ablation analysis show that our atomic approaches to multimodal fusion and hierarchical transformers for document series embedding are effective in forecasting. We further fine-tune the forecasting models with labelled data and found some of the multimodal models consistently outperforming baseline on downstream sepsis prediction task.</abstract>
      <url hash="bb1a5ec0">2025.cl4health-1.8</url>
      <bibkey>xu-staniek-2025-multimodal</bibkey>
    </paper>
    <paper id="9">
      <title>Comparing representations of long clinical texts for the task of patient-note identification</title>
      <author><first>Safa</first><last>Alsaidi</last><affiliation>Inria, Inserm, Université Paris Cité, HeKA U1346, Paris, France</affiliation></author>
      <author><first>Marc</first><last>Vincent</last><affiliation>Data Science Platform, Imagine Institute, INSERM U1163, Université Paris Cité, Paris, France</affiliation></author>
      <author><first>Olivia</first><last>Boyer</last><affiliation>Néphrologie Pédiatrique, Centre de Référence MARHEA, Hôpital Universitaire Necker-Enfants Malades, Assistance Publique - Hôpitaux de Paris (APHP), Institut Imagine, INSERM U1163, Université Paris Cité, Paris, France</affiliation></author>
      <author><first>Nicolas</first><last>Garcelon</last><affiliation>Data Science Platform, Imagine Institute, INSERM U1163, Université Paris Cité, Paris, France</affiliation></author>
      <author><first>Miguel</first><last>Couceiro</last><affiliation>LORIA, CNRS, Université de Lorraine, Nancy, France; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal</affiliation></author>
      <author><first>Adrien</first><last>Coulet</last><affiliation>Inria, Inserm, Université Paris Cité, HeKA U1346, Paris, France</affiliation></author>
      <pages>109-123</pages>
      <abstract>In this paper, we address the challenge of patient-note identification, which involves accurately matching an anonymized clinical note to its corresponding patient, represented by a set of related notes. This task has broad applications, including duplicate records detection and patient similarity analysis, which require robust patient-level representations. We explore various embedding methods, including Hierarchical Attention Networks (HAN), three-level Hierarchical Transformer Networks (HTN), LongFormer, and advanced BERT-based models, focusing on their ability to process medium-to-long clinical texts effectively. Additionally, we evaluate different pooling strategies (mean, max, and mean_max) for aggregating word-level embeddings into patient-level representations and we examine the impact of sliding windows on model performance. Our results indicate that BERT-based embeddings outperform traditional and hierarchical models, particularly in processing lengthy clinical notes and capturing nuanced patient representations. Among the pooling strategies, mean_max pooling consistently yields the best results, highlighting its ability to capture critical features from clinical notes. Furthermore, the reproduction of our results on both MIMIC dataset and Necker hospital data warehouse illustrates the generalizability of these approaches to real-world applications, emphasizing the importance of both embedding methods and aggregation strategies in optimizing patient-note identification and enhancing patient-level modeling.</abstract>
      <url hash="b7d022b7">2025.cl4health-1.9</url>
      <bibkey>alsaidi-etal-2025-comparing</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>M</fixed-case>e<fixed-case>D</fixed-case>i<fixed-case>S</fixed-case>um<fixed-case>QA</fixed-case>: Patient-Oriented Question-Answer Generation from Discharge Letters</title>
      <author><first>Amin</first><last>Dada</last><affiliation>IKIM</affiliation></author>
      <author><first>Osman</first><last>Koras</last><affiliation>Institute for AI in Medicine (IKIM), University Hospital Essen</affiliation></author>
      <author><first>Marie</first><last>Bauer</last><affiliation>Institute for AI in Medicine (IKIM), University Hospital Essen</affiliation></author>
      <author><first>Amanda</first><last>Butler</last><affiliation>NVIDIA, Santa Clara</affiliation></author>
      <author><first>Kaleb</first><last>Smith</last><affiliation>NVIDIA, Santa Clara</affiliation></author>
      <author><first>Jens</first><last>Kleesiek</last><affiliation>Institute for AI in Medicine (IKIM), University Hospital Essen</affiliation></author>
      <author><first>Julian</first><last>Friedrich</last><affiliation>Institute for AI in Medicine (IKIM), University Hospital Essen</affiliation></author>
      <pages>124-136</pages>
      <abstract>While increasing patients’ access to medical documents improves medical care, this benefit is limited by varying health literacy levels and complex medical terminology. Large language models (LLMs) offer solutions by simplifying medical information. However, evaluating LLMs for safe and patient-friendly text generation is difficult due to the lack of standardized evaluation resources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset created from MIMIC-IV discharge summaries through an automated pipeline combining LLM-based question-answer generation with manual quality checks. We use this dataset to evaluate various LLMs on patient-oriented question-answering. Our findings reveal that general-purpose LLMs frequently surpass biomedical-adapted models, while automated metrics correlate with human judgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the development of LLMs to enhance patient understanding and ultimately improve care outcomes.</abstract>
      <url hash="811d02fa">2025.cl4health-1.10</url>
      <bibkey>dada-etal-2025-medisumqa</bibkey>
    </paper>
    <paper id="11">
      <title>Using <fixed-case>LLM</fixed-case>s to improve <fixed-case>RL</fixed-case> policies in personalized health adaptive interventions</title>
      <author><first>Karine</first><last>Karine</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Benjamin</first><last>Marlin</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>137-147</pages>
      <abstract>Reinforcement learning (RL) is increasingly used in the healthcare domain, particularly for the development of personalized adaptive health interventions. However, RL methods are often applied to this domain using small state spaces to mitigate data scarcity. In this paper, we aim to use Large Language Models (LLMs) to incorporate text-based user preferences and constraints, to update the RL policy. The LLM acts as a filter in the action selection. To evaluate our method, we develop a novel simulation environment that generates text-based user preferences and incorporates corresponding constraints that impact behavioral dynamics. We show that our method can take into account the text-based user preferences, while improving the RL policy, thus improving personalization in adaptive intervention.</abstract>
      <url hash="261e20a1">2025.cl4health-1.11</url>
      <bibkey>karine-marlin-2025-using</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>LLM</fixed-case> Based Efficient <fixed-case>CSR</fixed-case> Summarization using Structured Fact Extraction and Feedback</title>
      <author><first>Kunwar</first><last>Zaid</last><affiliation>TCS Research</affiliation></author>
      <author><first>Amit</first><last>Sangroya</last><affiliation>TCS Research</affiliation></author>
      <author><first>Lovekesh</first><last>Vig</last><affiliation>TCS Research</affiliation></author>
      <pages>148-157</pages>
      <abstract>Summarizing clinical trial data poses a significant challenge due to the structured, voluminous, and domain-specific nature of clinical tables. While large language models (LLMs) such as ChatGPT, Llama, and DeepSeek demonstrate potential in table-to-text generation, they struggle with raw clinical tables that exceed context length, leading to incomplete, inconsistent, or imprecise summaries. These challenges stem from the structured nature of clinical tables, complex study designs, and the necessity for precise medical terminology. To address these limitations, we propose an end-to-end pipeline that enhances the summarization process by integrating fact selection, ensuring that only the most relevant data points are extracted for summary generation. Our approach also incorporates a feedback-driven refinement mechanism, allowing for iterative improvements based on domain-specific requirements and external expert input. By systematically filtering critical information and refining outputs, our method enhances the accuracy, completeness, and clinical reliability of generated summaries while reducing irrelevant or misleading content. This pipeline significantly improves the usability of LLM-generated summaries for medical professionals, regulators, and researchers, facilitating more efficient interpretation of clinical trial results. Our findings suggest that targeted preprocessing and iterative refinement strategies within the proposed piepline can mitigate LLM limitations, offering a scalable solution for summarizing complex clinical trial tables.</abstract>
      <url hash="07ee082f">2025.cl4health-1.12</url>
      <bibkey>zaid-etal-2025-llm</bibkey>
    </paper>
    <paper id="13">
      <title>On Large Foundation Models and <fixed-case>A</fixed-case>lzheimer’s Disease Detection</title>
      <author><first>Chuyuan</first><last>Li</last><affiliation>The University of British Columbia</affiliation></author>
      <author><first>Giuseppe</first><last>Carenini</last><affiliation>university of british columbia</affiliation></author>
      <author><first>Thalia</first><last>Field</last><affiliation>University of British Columbia</affiliation></author>
      <pages>158-168</pages>
      <abstract>Large Foundation Models have displayed incredible capabilities in a wide range of domains and tasks. However, it is unclear whether these models match specialist capabilities without special training or fine-tuning. In this paper, we investigate the innate ability of foundation models as neurodegenerative disease specialists. Precisely, we use a language model, Llama-3.1, and a visual language model, Llama3-LLaVA-NeXT, to detect language specificity between Alzheimer’s Disease patients and healthy controls through a well-known Picture Description task. Results show that Llama is comparable to supervised classifiers, while LLaVA, despite its additional “vision”, lags behind.</abstract>
      <url hash="67c39e26">2025.cl4health-1.13</url>
      <bibkey>li-etal-2025-large-foundation</bibkey>
    </paper>
    <paper id="14">
      <title>Benchmarking <fixed-case>I</fixed-case>si<fixed-case>X</fixed-case>hosa Automatic Speech Recognition and Machine Translation for Digital Health Provision</title>
      <author><first>Abby</first><last>Blocker</last><affiliation>University of Cape Town</affiliation></author>
      <author><first>Francois</first><last>Meyer</last><affiliation>University of Cape Town</affiliation></author>
      <author><first>Ahmed</first><last>Biyabani</last><affiliation>Carnegie Mellon University Africa</affiliation></author>
      <author><first>Joyce</first><last>Mwangama</last><affiliation>University of Cape Town</affiliation></author>
      <author><first>Mohammed</first><last>Datay</last><affiliation>University of Cape Town</affiliation></author>
      <author><first>Bessie</first><last>Malila</last><affiliation>University of Cape Town</affiliation></author>
      <pages>169-179</pages>
      <abstract>As digital health becomes more ubiquitous, people from different geographic regions are connected and there is thus a need for accurate language translation services. South Africa presents opportunity and need for digital health innovation, but implementing indigenous translation systems for digital health is difficult due to a lack of language resources. Understanding the accuracy of current models for use in medical translation of indigenous languages is crucial for designers looking to build quality digital health solutions. This paper presents a new dataset with audio and text of primary health consultations for automatic speech recognition and machine translation in South African English and the indigenous South African language of isiXhosa. We then evaluate the performance of well-established pretrained models on this dataset. We found that isiXhosa had limited support in speech recognition models and showed high, variable character error rates for transcription (26-70%). For translation tasks, Google Cloud Translate and ChatGPT outperformed the other evaluated models, indicating large language models can have similar performance to dedicated machine translation models for low-resource language translation.</abstract>
      <url hash="7793cd57">2025.cl4health-1.14</url>
      <bibkey>blocker-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="15">
      <title>Preliminary Evaluation of an Open-Source <fixed-case>LLM</fixed-case> for Lay Translation of <fixed-case>G</fixed-case>erman Clinical Documents</title>
      <author><first>Tabea</first><last>Pakull</last><affiliation>Institute for Transfusion Medicine, University Hospital Essen</affiliation></author>
      <author><first>Amin</first><last>Dada</last><affiliation>Institute for AI in Medicine (IKIM), University Hospital Essen</affiliation></author>
      <author><first>Hendrik</first><last>Damm</last><affiliation>Fachhochschule Dortmund</affiliation></author>
      <author><first>Anke</first><last>Fleischhauer</last><affiliation>West German Cancer Center, University Hospital Essen</affiliation></author>
      <author><first>Sven</first><last>Benson</last><affiliation>Institute for Medical Education, Center for Translational Neuro- and Behavioral Sciences (C-TNBS), University Hospital Essen</affiliation></author>
      <author><first>Noëlle</first><last>Bender</last><affiliation>Social Psychology Department of Human-Centred Computing &amp; Cognitive Science, University of Duisburg-Essen</affiliation></author>
      <author><first>Nicola</first><last>Prasuhn</last><affiliation>Patient Advisory Board, West German Cancer Center, University Hospital Essen</affiliation></author>
      <author><first>Katharina</first><last>Kaminski</last><affiliation>West German Cancer Center, University Hospital Essen</affiliation></author>
      <author><first>Christoph</first><last>Friedrich</last><affiliation>Department of Computer Science, University of Applied Arts and Science Dortmund</affiliation></author>
      <author><first>Peter</first><last>Horn</last><affiliation>Institute for Transfusion Medicine, University Hospital Essen</affiliation></author>
      <author><first>Jens</first><last>Kleesiek</last><affiliation>Institute for AI in Medicine (IKIM), University Hospital Essen</affiliation></author>
      <author><first>Dirk</first><last>Schadendorf</last><affiliation>West German Cancer Center, University Hospital Essen</affiliation></author>
      <author><first>Ina</first><last>Pretzell</last><affiliation>West German Cancer Center, University Hospital Essen</affiliation></author>
      <pages>180-192</pages>
      <abstract>Clinical documents are essential to patient care, but their complexity often makes them inaccessible to patients. Large Language Models (LLMs) are a promising solution to support the creation of lay translations of these documents, addressing the infeasibility of manually creating these translations in busy clinical settings. However, the integration of LLMs into medical practice in Germany is challenging due to data scarcity and privacy regulations. This work evaluates an open-source LLM for lay translation in this data-scarce environment using datasets of German synthetic clinical documents and real tumor board protocols. The evaluation framework used combines readability, semantic, and lexical measures with the G-Eval framework. Preliminary results show that zero-shot prompts significantly improve readability (e.g., FREde: 21.4 → 39.3) and few-shot prompts improve semantic and lexical fidelity. However, the results also reveal G-Eval’s limitations in distinguishing between intentional omissions and factual inaccuracies. These findings underscore the need for manual review in clinical applications to ensure both accessibility and accuracy in lay translations. Furthermore, the effectiveness of prompting highlights the need for future work to develop applications that use predefined prompts in the background to reduce clinician workload.</abstract>
      <url hash="5d2b4fdb">2025.cl4health-1.15</url>
      <bibkey>pakull-etal-2025-preliminary</bibkey>
    </paper>
    <paper id="16">
      <title>Leveraging External Knowledge Bases: Analyzing Presentation Methods and Their Impact on Model Performance</title>
      <author><first>Hui-Syuan</first><last>Yeh</last><affiliation>LISN/CNRS &amp; Université Paris Saclay</affiliation></author>
      <author><first>Thomas</first><last>Lavergne</last><affiliation>LISN/CNRS &amp; Université Paris Saclay</affiliation></author>
      <author><first>Pierre</first><last>Zweigenbaum</last><affiliation>LISN, CNRS, Université Paris-Saclay</affiliation></author>
      <pages>193-204</pages>
      <abstract>Integrating external knowledge into large language models has demonstrated potential for performance improvement across a wide range of tasks. This approach is particularly appealing in domain-specific applications, such as in the biomedical field. However, the strategies for effectively presenting external knowledge to these models remain underexplored. This study investigates the impact of different knowledge presentation methods and their influence on model performance. Our results show that inserting knowledge between demonstrations helps the models perform better, and improve smaller LLMs (7B) to perform on par with larger LLMs (175B). Our further investigation indicates that the performance improvement, however, comes more from the effect of additional tokens and positioning than from the relevance of the knowledge.</abstract>
      <url hash="93c9bd12">2025.cl4health-1.16</url>
      <bibkey>yeh-etal-2025-leveraging</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>LT</fixed-case>3: Generating Medication Prescriptions with Conditional Transformer</title>
      <author><first>Samuel</first><last>Belkadi</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Nicolo</first><last>Micheletti</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Lifeng</first><last>Han</last><affiliation>The University of Manchester</affiliation></author>
      <author><first>Warren</first><last>Del-Pinto</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <pages>205-218</pages>
      <abstract>Access to real-world medication prescriptions is essential for medical research and healthcare quality improvement. However, access to real medication prescriptions is often limited due to the {textit{sensitive} nature of the information expressed. Additionally, manually labelling these instructions for training and fine-tuning Natural Language Processing (NLP) models can be tedious and expensive. We introduce a novel task-specific model architecture, {textbf{Label-To-Text-Transformer }({textbf{LT3}), tailored to generate synthetic medication prescriptions based on provided labels, such as a vocabulary list of medications and their attributes, to facilitate {textit{safe} healthcare research.LT3 is trained on a set of around 2K lines of medication prescriptions extracted from the MIMIC-III database, allowing the model to produce valuable synthetic medication prescriptions. We evaluate LT3’s performance by contrasting it with state-of-the-art Pre-trained Language Models (PLMs), T5-small/base/large, analysing the quality and diversity of generated texts. We deploy the generated synthetic data to train the SpacyNER model for the Named Entity Recognition (NER) task over the n2c2-2018 dataset.The experiments show that the model trained on synthetic data can achieve a 96-98{% F1 score at Label Recognition on Drug, Frequency, Route, Strength, and Form.LT3 codes and data will be shared for research purposes at {url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}</abstract>
      <url hash="c2ef74b8">2025.cl4health-1.17</url>
      <bibkey>belkadi-etal-2025-lt3</bibkey>
    </paper>
    <paper id="18">
      <title>Explainable <fixed-case>ICD</fixed-case> Coding via Entity Linking</title>
      <author><first>Leonor</first><last>Barreiros</last><affiliation>Priberam</affiliation></author>
      <author><first>Isabel</first><last>Coutinho</last><affiliation>IST and INESC-ID</affiliation></author>
      <author><first>Gonçalo</first><last>Correia</last><affiliation>Priberam</affiliation></author>
      <author><first>Bruno</first><last>Martins</last><affiliation>IST and INESC-ID</affiliation></author>
      <pages>219-227</pages>
      <abstract>Clinical coding is a critical task in healthcare, although traditional methods for automating clinical coding may not provide sufficient explicit evidence for coders in production environments. This evidence is crucial, as medical coders have to make sure there exists at least one explicit passage in the input health record that justifies the attribution of a code. We therefore propose to reframe the task as an entity linking problem, in which each document is annotated with its set of codes and respective textual evidence, enabling better human-machine collaboration. By leveraging parameter-efficient fine-tuning of Large Language Models (LLMs), together with constrained decoding, we introduce three approaches to solve this problem that prove effective at disambiguating clinical mentions and that perform well in few-shot scenarios.</abstract>
      <url hash="18dbf40a">2025.cl4health-1.18</url>
      <bibkey>barreiros-etal-2025-explainable</bibkey>
    </paper>
    <paper id="19">
      <title>Will Gen <fixed-case>Z</fixed-case> users look for evidence to verify <fixed-case>QA</fixed-case> System-generated answers?</title>
      <author><first>Souma</first><last>Gayen</last><affiliation>NLM NIH</affiliation></author>
      <author><first>Dina</first><last>Demner-Fushman</last><affiliation>National Library of Medicine</affiliation></author>
      <author><first>Deepak</first><last>Gupta</last><affiliation>National Library of Medicine, NIH</affiliation></author>
      <pages>228-235</pages>
      <abstract>The remarkable results shown by medicalquestion-answering systems lead to theiradoption in real-life applications. The systems,however, may misinform the users, even whendrawing on scientific evidence to ground theresults. The quality of the answers maybe verified by the users if they analyze theevidence provided by the systems. Userinterfaces play an important role in engagingthe users. While studies of the user interfacesfor biomedical literature search and clinicaldecision support are abundant, little is knownabout users’ interactions with medical questionanswering systems and the impact of thesesystems on health-related decisions. In a studyof several different user interface layouts, wefound that only a small number of participantsfollowed the links to verify automaticallygenerated answers, independently of theinterface design. The users who followed thelinks made better health-related decisions.</abstract>
      <url hash="5271fe82">2025.cl4health-1.19</url>
      <bibkey>gayen-etal-2025-will</bibkey>
    </paper>
    <paper id="20">
      <title>Predicting Chronic Kidney Disease Progression from Stage <fixed-case>III</fixed-case> to Stage <fixed-case>V</fixed-case> using Language Models</title>
      <author><first>Zainab</first><last>Awan</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Rafael</first><last>Henkin</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Nick</first><last>Reynolds</last><affiliation>Newcastle University</affiliation></author>
      <author><first>Michael</first><last>Barnes</last><affiliation>Queen Mary University of London</affiliation></author>
      <pages>236-242</pages>
      <abstract>Chronic Kidney Disease (CKD) is a global health challenge, affecting 5–10{% of the population, with a significant burden on healthcare systems. Early prediction of CKD progression from stage III to stage V is crucial to enable timely interventions. Traditional predictive methods rely on biochemical markers and demographic factors, but are often limited by issues such as missing data and reliance on structured inputs. This study explores the potential of several encoder-based language models, to predict CKD progression using a cohort from the Clinical Practice Research Datalink (CPRD) GOLD database. We applied both Full Fine-Tuning (FFT) and Parameter-Efficient Fine-Tuning (PEFT) with LoRA to pre-trained models, comparing them against traditional machine learning algorithms such as Random Forest and XGBoost. Our results show that fine-tuned models, particularly dmis-lab/biobert-v1.1-FFT, outperform traditional models in predicting CKD progression, with an AUC of 0.7787, precision of 0.7261, and accuracy of 0.7045. Although LoRA-based models are more computationally efficient, they consistenly exhibit lower performance. These findings suggest that fine-tuned encoder models hold significant potential for improving CKD progression prediction. However, there is still room for further enhancement in their accuracy and applicability in clinical settings.</abstract>
      <url hash="dd110806">2025.cl4health-1.20</url>
      <bibkey>awan-etal-2025-predicting</bibkey>
    </paper>
    <paper id="21">
      <title>Am <fixed-case>I</fixed-case> eligible? Natural Language Inference for Clinical Trial Patient Recruitment: the Patient’s Point of View</title>
      <author><first>Mathilde</first><last>Aguiar</last><affiliation>Université Paris-Saclay, CNRS, Laboratoire Interdisciplinaire des Sciences du Numérique, 91400, Orsay, France</affiliation></author>
      <author><first>Pierre</first><last>Zweigenbaum</last><affiliation>LISN, CNRS, Université Paris-Saclay</affiliation></author>
      <author><first>Nona</first><last>Naderi</last><affiliation>UniversitÃ© Paris-Saclay</affiliation></author>
      <pages>243-259</pages>
      <abstract>Recruiting patients to participate in clinical trials can be challenging and time-consuming. Usually, participation in a clinical trial is initiated by a healthcare professional and proposed to the patient. Promoting clinical trials directly to patients via online recruitment might help to reach them more efficiently. In this study, we address the case where a patient is initiating their own recruitment process and wants to determine whether they are eligible for a given clinical trial, using their own language to describe their medical profile. To study whether this creates difficulties in the patient-trial matching process, we design a new dataset and task, Natural Language Inference for Patient Recruitment (NLI4PR), in which patient-language profiles must be matched to clinical trials. We create it by adapting the TREC 2022 Clinical Trial Track dataset, which provides patients’ medical profiles, and rephrasing them manually using patient language. We also use the associated clinical trial reports where the patients are either eligible or excluded. We prompt several open-source Large Language Models on our task and achieve from 56.5 to 71.8 of F1 score using patient language, against 64.7 to 73.1 for the same task using medical language. When using patient language, we observe only a small loss in performance for the best model, suggesting that having the patient as a starting point could be adopted to help recruit patients for clinical trials. The corpus and code bases are all freely available on our GitHub and HuggingFace repositories.</abstract>
      <url hash="df4ce91f">2025.cl4health-1.21</url>
      <bibkey>aguiar-etal-2025-eligible</bibkey>
    </paper>
    <paper id="22">
      <title>Towards Understanding <fixed-case>LLM</fixed-case>-Generated Biomedical Lay Summaries</title>
      <author><first>Rohan Charudatt</first><last>Salvi</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Swapnil</first><last>Panigrahi</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Dhruv</first><last>Jain</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Shweta</first><last>Yadav</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Md. Shad</first><last>Akhtar</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <pages>260-268</pages>
      <abstract>In this paper, we investigate using large language models to generate accessible lay summaries of medical abstracts, targeting non-expert audiences. We assess the ability of models like GPT-4 and LLaMA 3-8B-Instruct to simplify complex medical information, focusing on layness, comprehensiveness, and factual accuracy. Utilizing both automated and human evaluations, we discover that automatic metrics do not always align with human judgments. Our analysis highlights the potential benefits of developing clear guidelines for consistent evaluations conducted by non-expert reviewers. It also points to areas for improvement in the evaluation process and the creation of lay summaries for future research.</abstract>
      <url hash="fe63c25e">2025.cl4health-1.22</url>
      <bibkey>salvi-etal-2025-towards</bibkey>
    </paper>
    <paper id="23">
      <title>Bridging the Gap in Health Literacy: Harnessing the Power of Large Language Models to Generate Plain Language Summaries from Biomedical Texts</title>
      <author><first>Andrés</first><last>Arias-Russi</last><affiliation>Universidad de los Andes</affiliation></author>
      <author><first>Carolina</first><last>Salazar-Lara</last><affiliation>Universidad de los Andes</affiliation></author>
      <author><first>Rubén</first><last>Manrique</last><affiliation>Universidad de los Andes</affiliation></author>
      <pages>269-284</pages>
      <abstract>Health literacy enables individuals to navigate healthcare systems and make informed decisions. Plain language summaries (PLS) can bridge comprehension gaps by simplifying complex biomedical texts, yet their manual creation is both time-consuming and challenging. This study advances the field by (1) constructing a novel corpus of paired technical and plain language texts from medical trial libraries, (2) developing machine learning classifiers to rapidly identify plain language features, and (3) establishing a multi-dimensional evaluation framework that integrates computational metrics with human expertise. We iteratively optimized prompts for diverse large language models (LLMs)—including GPT models, Gemini 1.5, DeepSeek-R1, and Llama-3.2—to generate PLS variants aligned with domain-specific guidelines. Our classifier achieved 97.5{% accuracy in distinguishing plain from technical language, and the generated summaries demonstrated high semantic equivalence to expert-written versions.</abstract>
      <url hash="adb97724">2025.cl4health-1.23</url>
      <bibkey>arias-russi-etal-2025-bridging</bibkey>
    </paper>
    <paper id="24">
      <title>Towards Knowledge-Guided Biomedical Lay Summarization using Large Language Models</title>
      <author><first>Shufan</first><last>Ming</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Yue</first><last>Guo</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Halil</first><last>Kilicoglu</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <pages>285-297</pages>
      <abstract>The massive size, continual growth, and technical jargon in biomedical publications make it difficult for laypeople to stay informed about the latest scientific advances, motivating research on lay summarization of biomedical literature. Large language models (LLMs) are increasingly used for this task. Unlike typical automatic summarization, lay summarization requires incorporating background knowledge not found in a paper and explanations of technical jargon. This study explores the use of MeSH terms (Medical Subject Headings), which represent an article’s main topics, to enhance background information generation in biomedical lay summarization. Furthermore, we introduced a multi-turn dialogue approach that more effectively leverages MeSH terms in the instruction-tuning of LLMs to enhance the quality of lay summaries. The best model improved the state-of-the-art on the eLife test set in terms of the ROUGE-1 score by nearly 2%, with competitive scores in other metrics. These results indicate that MeSH terms can guide LLMs to generate more relevant background information for laypeople. Additionally, evaluation on a held-out dataset, one that was not used during model pre-training, shows that this capability generalizes well to unseen data, further demonstrating the effectiveness of our approach.</abstract>
      <url hash="0aaa53b7">2025.cl4health-1.24</url>
      <bibkey>ming-etal-2025-towards</bibkey>
    </paper>
    <paper id="25">
      <title>A Preliminary Study on <fixed-case>NLP</fixed-case>-Based Personalized Support for Type 1 Diabetes Management</title>
      <author><first>Sandra</first><last>Mitrović</last><affiliation>Dalle Molle Institute for Artificial Intelligence (SUPSI)</affiliation></author>
      <author><first>Federico</first><last>Fontana</last><affiliation>Enhance-d</affiliation></author>
      <author><first>Andrea</first><last>Zignoli</last><affiliation>Enhance-d</affiliation></author>
      <author><first>Felipe</first><last>Mattioni Maturana</last><affiliation>Enhance-d</affiliation></author>
      <author><first>Christian</first><last>Berchtold</last><affiliation>Dalle Molle Institute for Artificial Intelligence (SUPSI)</affiliation></author>
      <author><first>Daniele</first><last>Malpetti</last><affiliation>Dalle Molle Institute for Artificial Intelligence (SUPSI)</affiliation></author>
      <author><first>Sam</first><last>Scott</last><affiliation>Enhance-d</affiliation></author>
      <author><first>Laura</first><last>Azzimonti</last><affiliation>Dalle Molle Institute for Artificial Intelligence (SUPSI)</affiliation></author>
      <pages>298-302</pages>
      <abstract>The proliferation of wearable devices and sports monitoring apps has made tracking physical activity more accessible than ever. For individuals with Type 1 diabetes, regular exercise is essential for managing the condition, making personalized feedback particularly valuable. By leveraging data from physical activity sessions, NLP-generated messages can offer tailored guidance to help users optimize their workouts and make informed decisions. In this study, we assess several open-source pre-trained NLP models for this purpose. Contrary to expectations, our findings reveal that models fine-tuned on medical data or excelling in medical benchmarks do not necessarily produce high-quality messages.</abstract>
      <url hash="9e70b5fa">2025.cl4health-1.25</url>
      <bibkey>mitrovic-etal-2025-preliminary</bibkey>
    </paper>
    <paper id="26">
      <title>Medication Extraction and Entity Linking using Stacked and Voted Ensembles on <fixed-case>LLM</fixed-case>s</title>
      <author><first>Pablo</first><last>Romero</last><affiliation>Manchester Metropolitan University</affiliation></author>
      <author><first>Lifeng</first><last>Han</last><affiliation>The University of Manchester</affiliation></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <pages>303-315</pages>
      <abstract>Medication Extraction and Mining play an important role in healthcare NLP research due to its practical applications in hospital settings, such as their mapping into standard clinical knowledge bases (SNOMED-CT, BNF, etc.).In this work, we investigate state-of-the-art LLMs in text mining tasks on medications and their related attributes such as dosage, route, strength, and adverse effects. In addition, we explore different ensemble learning methods ({textsc{Stack-Ensemble} and {textsc{Voting-Ensemble}) to augment the model performances from individual LLMs. Our ensemble learning result demonstrated better performances than individually fine-tuned base models BERT, RoBERTa, RoBERTa-L, BioBERT, BioClinicalBERT, BioMedRoBERTa, ClinicalBERT, and PubMedBERT across general and specific domains. Finally, we build up an entity linking function to map extracted medical terminologies into the SNOMED-CT codes and the British National Formulary (BNF) codes, which are further mapped to the Dictionary of Medicines and Devices (dm+d), and ICD.We host the fine-tuned models at {url{https://github.com/pabloRom2004/Insight-Buddy-AI-App}</abstract>
      <url hash="b280b49c">2025.cl4health-1.26</url>
      <bibkey>romero-etal-2025-medication</bibkey>
    </paper>
    <paper id="27">
      <title>Bias in <fixed-case>D</fixed-case>anish Medical Notes: Infection Classification of Long Texts Using Transformer and <fixed-case>LSTM</fixed-case> Architectures Coupled with <fixed-case>BERT</fixed-case></title>
      <author><first>Mehdi</first><last>Parviz</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Rudi</first><last>Agius</last><affiliation>Rigshospitalet</affiliation></author>
      <author><first>Carsten</first><last>Niemann</last><affiliation>Rigshospitalet</affiliation></author>
      <author><first>Rob</first><last>Van Der Goot</last><affiliation>IT University of Copenhagen</affiliation></author>
      <pages>316-320</pages>
      <abstract>Medical notes contain a wealth of information related to diagnosis, prognosis, and overall patient care that can be used to help physicians make informed decisions. However, like any other data sets consisting of data from diverse demographics, they may be biased toward certain subgroups or subpopulations. Consequently, any bias in the data will be reflected in the output of the machine learning models trained on them. In this paper, we investigate the existence of such biases in Danish medical notes related to three types of blood cancer, with the goal of classifying whether the medical notes indicate severe infection. By employing a hierarchical architecture that combines a sequence model (Transformer and LSTM) with a BERT model to classify long notes, we uncover biases related to demographics and cancer types. Furthermore, we observe performance differences between hospitals. These findings underscore the importance of investigating bias in critical settings such as healthcare and the urgency of monitoring and mitigating it when developing AI-based systems.</abstract>
      <url hash="fb13aba7">2025.cl4health-1.27</url>
      <bibkey>parviz-etal-2025-bias</bibkey>
    </paper>
    <paper id="28">
      <title>Capturing Patients’ Lived Experiences with Chronic Pain through Motivational Interviewing and Information Extraction</title>
      <author><first>Hadeel R A</first><last>Elyazori</last><affiliation>George Mason University</affiliation></author>
      <author><first>Rusul</first><last>Abdulrazzaq</last><affiliation>George Mason University</affiliation></author>
      <author><first>Hana</first><last>Al Shawi</last><affiliation>George Mason University</affiliation></author>
      <author><first>Isaac</first><last>Amouzou</last><affiliation>George Mason University</affiliation></author>
      <author><first>Patrick</first><last>King</last><affiliation>George Mason University</affiliation></author>
      <author><first>Syleah</first><last>Manns</last><affiliation>George Mason University</affiliation></author>
      <author><first>Mahdia</first><last>Popal</last><affiliation>George Mason University</affiliation></author>
      <author><first>Zarna</first><last>Patel</last><affiliation>George Mason University</affiliation></author>
      <author><first>Secili</first><last>Destefano</last><affiliation>George Mason University</affiliation></author>
      <author><first>Jay</first><last>Shah</last><affiliation>National Institute of Health</affiliation></author>
      <author><first>Naomi</first><last>Gerber</last><affiliation>George Mason University</affiliation></author>
      <author><first>Siddhartha</first><last>Sikdar</last><affiliation>George Mason University</affiliation></author>
      <author><first>Seiyon</first><last>Lee</last><affiliation>George Mason University</affiliation></author>
      <author><first>Samuel</first><last>Acuna</last><affiliation>George Mason University</affiliation></author>
      <author><first>Kevin</first><last>Lybarger</last><affiliation>George Mason University</affiliation></author>
      <pages>321-330</pages>
      <abstract>Chronic pain affects millions, yet traditional assessments often fail to capture patients’ lived experiences comprehensively. In this study, we used a Motivational Interviewing framework to conduct semi-structured interviews with eleven adults experiencing chronic pain and then applied Natural Language Processing (NLP) to their narratives. We developed an annotation schema that integrates the International Classification of Functioning, Disability, and Health (ICF) with Aspect-Based Sentiment Analysis (ABSA) to convert unstructured narratives into structured representations of key patient experience dimensions. Furthermore, we evaluated whether Large Language Models (LLMs) can automatically extract information using this schema. Our findings advance scalable, patient-centered approaches to chronic pain assessment, paving the way for more effective, data-driven management strategies.</abstract>
      <url hash="31d33419">2025.cl4health-1.28</url>
      <bibkey>elyazori-etal-2025-capturing</bibkey>
    </paper>
    <paper id="29">
      <title>Medifact at <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm 2025: Leveraging Lightweight Models for Perspective-Specific Summarization of Clinical <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case> Forums</title>
      <author><first>Nadia</first><last>Saeed</last><affiliation>National University of Computer &amp; Emerging Sciences</affiliation></author>
      <pages>331-339</pages>
      <abstract>The PerAnsSumm 2025 challenge focuses on perspective-aware healthcare answer summarization (Agarwal et al., 2025). This work proposes a few-shot learning framework using a Snorkel-BART-SVM pipeline for classifying and summarizing open-ended healthcare community question-answering (CQA).An SVM model is trained with weak supervision via Snorkel, enhancing zero-shot learning. Extractive classification identifies perspective-relevant sentences, which are then summarized using a pretrained BART-CNN model. The approach achieved 12th place among 100 teams in the shared task, demonstrating computational efficiency and contextual accuracy. By leveraging pretrained summarization models, this work advances medical CQA research and contributes to clinical decision support systems.</abstract>
      <url hash="f72a1c22">2025.cl4health-1.29</url>
      <bibkey>saeed-2025-medifact</bibkey>
    </paper>
    <paper id="30">
      <title>The <fixed-case>M</fixed-case>anchester Bees at <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm 2025: Iterative Self-Prompting with Claude and o1 for Perspective-aware Healthcare Answer Summarisation</title>
      <author><first>Pablo</first><last>Romero</last><affiliation>Manchester Metropolitan University</affiliation></author>
      <author><first>Libo</first><last>Ren</last><affiliation>University of Manchester, UK</affiliation></author>
      <author><first>Lifeng</first><last>Han</last><affiliation>The University of Manchester</affiliation></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <pages>340-348</pages>
      <abstract>In this system report, we present an innovative approach to the PerAnsSumm2025 shared task at the Workshop CL4Health, addressing the critical challenges of perspective-aware healthcare answer summarization. Our method, Iterative Self-Prompting (ISP) with Claude and o1, introduces a novel framework that leverages large language models’ ability to iteratively refine their own instructions, achieving competitive results without traditional model training.Despite utilizing only API calls rather than computational-intensive training, our system “The Manchester Bees” secured 15th place among 23 systems overall, while demonstrating exceptional performance in key metrics - ranking 6th in Strict-matching-F1 for span identification (Task A) and achieving the highest Factuality score for summary generation (Task B). Notably, our approach achieved state-of-the-art results in specific metrics, including the highest Strict-matching precision (0.2267) for Task A and AlignScore (0.5888) for Task B.This performance, accomplished with minimal computational resources and development time measured in hours rather than weeks, demonstrates the potential of ISP to democratize access to advanced NLP capabilities in healthcare applications. Our complete implementation is available as an open-source project on {url{https://github.com/pabloRom2004/-PerAnsSumm-2025}.</abstract>
      <url hash="10a5003d">2025.cl4health-1.30</url>
      <bibkey>romero-etal-2025-manchester</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>MNLP</fixed-case> at <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm: A Classifier-Refiner Architecture for Improving the Classification of Consumer Health User Responses</title>
      <author><first>Jooyeon</first><last>Lee</last><affiliation>George Mason University</affiliation></author>
      <author><first>Luan</first><last>Pham</last><affiliation>George Mason University</affiliation></author>
      <author><first>Özlem</first><last>Uzuner</last><affiliation>George Mason University</affiliation></author>
      <pages>349-358</pages>
      <abstract>Community question-answering (CQA) platforms provide a crucial space for users to share experiences, seek medical advice, and exchange health-related information. However, these platforms, by nature of their user-generated content as well as the complexity and subjectivity of natural language, remain a significant challenge for tasks related to the automatic classification of diverse perspectives. The PerAnsSumm shared task involves extracting perspective spans from community users’ answers, classifying them into specific perspective categories (Task A), and then using these perspectives and spans to generate structured summaries (Task B). Our focus is on Task A. To address this challenge, we propose a Classifier-Refiner Architecture (CRA), a two-stage framework designed to enhance classification accuracy. The first stage employs a Classifier to segment user responses into self-contained snippets and assign initial perspective labels along with a binary confidence value. If the classifier is not confident, a secondary Refiner stage is triggered, incorporating retrieval-augmented generation to enhance classification through contextual examples. Our methodology integrates instruction-driven classification, tone definitions, and Chain-of-Thought (CoT) prompting, leading to improved F1 scores compared to single-pass approaches. Experimental evaluations on the Perspective Summarization Dataset (PUMA) demonstrate that our framework improves classification performance by leveraging multi-stage decision-making. Our submission ranked among the top-performing teams, achieving an overall score of 0.6090, with high precision and recall in perspective classification.</abstract>
      <url hash="75eb8d2b">2025.cl4health-1.31</url>
      <bibkey>lee-etal-2025-mnlp</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>W</fixed-case>is<fixed-case>P</fixed-case>er<fixed-case>M</fixed-case>ed @ <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm 2025: Strong Reasoning Through Structured Prompting and Careful Answer Selection Enhances Perspective Extraction and Summarization of Healthcare Forum Threads</title>
      <author><first>Tabea</first><last>Pakull</last><affiliation>Institute for Transfusion Medicine, University Hospital Essen</affiliation></author>
      <author><first>Hendrik</first><last>Damm</last><affiliation>Fachhochschule Dortmund</affiliation></author>
      <author><first>Henning</first><last>Schäfer</last><affiliation>Institute for Transfusion Medicine, University Hospital Essen</affiliation></author>
      <author><first>Peter</first><last>Horn</last><affiliation>Institute for Transfusion Medicine, University Hospital Essen</affiliation></author>
      <author><first>Christoph</first><last>Friedrich</last><affiliation>Fachhochschule Dortmund</affiliation></author>
      <pages>359-373</pages>
      <abstract>Healthcare community question-answering (CQA) forums provide multi-perspective insights into patient experiences and medical advice. Summarizations of these threads must account for these perspectives, rather than relying on a single “best” answer. This paper presents the participation of the WisPerMed team in the PerAnsSumm shared task 2025, which consists of two sub-tasks: (A) span identification and classification, and (B) perspectivebased summarization. For Task A, encoder models, decoder-based LLMs, and reasoningfocused models are evaluated under finetuning, instruction-tuning, and prompt-based paradigms. The experimental evaluations employing automatic metrics demonstrate that DeepSeek-R1 attains a high proportional recall (0.738) and F1-Score (0.676) in zero-shot settings, though strict boundary alignment remains challenging (F1-Score: 0.196). For Task B, filtering answers by labeling them with perspectives prior to summarization with Mistral-7B-v0.3 enhances summarization. This approach ensures that the model is trained exclusively on relevant data, while discarding non-essential information, leading to enhanced relevance (ROUGE-1: 0.452) and balanced factuality (SummaC: 0.296). The analysis uncovers two key limitations: data imbalance and hallucinations of decoder-based LLMs, with underrepresented perspectives exhibiting suboptimal performance. The WisPerMed team’s approach secured the highest overall ranking in the shared task.</abstract>
      <url hash="886fc72b">2025.cl4health-1.32</url>
      <bibkey>pakull-etal-2025-wispermed</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>D</fixed-case>ata<fixed-case>H</fixed-case>acks at <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm 2025: <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>-Driven Prompt Engineering for Perspective Aware Span Identification and Summarization</title>
      <author><first>Vansh</first><last>Nawander</last><affiliation>IIIT Hyderabad</affiliation></author>
      <author><first>Chaithra Reddy</first><last>Nerella</last><affiliation>IIIT Hyderabad</affiliation></author>
      <pages>374-379</pages>
      <abstract>This paper presents the approach of the DataHacks team in the PerAnsSumm Shared Task at CL4Health 2025, which focuses on perspective-aware summarization of healthcare community question-answering (CQA) forums. Unlike traditional CQA summarization, which relies on the best-voted answer, this task captures diverse perspectives, including ‘cause,’ ‘suggestion,’ ‘experience,’ ‘question,’ and ‘information.’ The task is divided into two subtasks: (1) identifying and classifying perspective-specific spans, and (2) generating perspective-specific summaries. We addressed these tasks using Large Language Models (LLM), fine-tuning it with different low-rank adaptation (LoRA) configurations to balance performance and computational efficiency under resource constraints. In addition, we experimented with various prompt strategies and analyzed their impact on performance. Our approach achieved a combined average score of 0.42, demonstrating the effectiveness of fine-tuned LLMs with adaptive LoRA configurations for perspective-aware summarization.</abstract>
      <url hash="583d506f">2025.cl4health-1.33</url>
      <bibkey>nawander-nerella-2025-datahacks</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>LMU</fixed-case> at <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm 2025: <fixed-case>L</fixed-case>la<fixed-case>MA</fixed-case>-in-the-loop at Perspective-Aware Healthcare Answer Summarization Task 2.2 Factuality</title>
      <author><first>Tanalp</first><last>Ağustoslu</last><affiliation>Ludwig Maximilian University of Munich</affiliation></author>
      <pages>380-388</pages>
      <abstract>In this paper, we describe our submission for the shared task on Perspective-aware Healthcare Answer Summarization. Our system consists of two quantized models of the LlaMA family, applied across fine-tuning and few-shot settings. Additionally, we adopt the SumCoT prompting technique to improve the factual correctness of the generated summaries. We show that SumCoT yields more factually accurate summaries, even though this improvement comes at the expense of lower performance on lexical overlap and semantic similarity metrics such as ROUGE and BERTScore. Our work highlights an important trade-off when evaluating summarization models.</abstract>
      <url hash="dad88382">2025.cl4health-1.34</url>
      <bibkey>agustoslu-2025-lmu</bibkey>
    </paper>
    <paper id="35">
      <title>Lightweight <fixed-case>LLM</fixed-case> Adaptation for Medical Summarisation: Roux-lette at <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm Shared Task</title>
      <author><first>Anson</first><last>Antony</last><affiliation>Institute for Experiential AI</affiliation></author>
      <author><first>Peter</first><last>Vickers</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Suzanne</first><last>Wendelken</last><affiliation>The Roux Institute at Northeastern University</affiliation></author>
      <pages>389-397</pages>
      <abstract>The PerAnsSumm Shared Task at CL4Health@NAACL 2025 focused on Perspective-Aware Summarization of Healthcare Q/A forums, requiring participants to extract and summarize spans based on predefined perspective categories. Our approach leveraged LLM-based zero-shot prompting enhanced by semantically-similar In-Context Learning (ICL) examples. Using Qwen-Turbo with 20 exemplar samples retrieved through NV-Embed-v2 embeddings, we achieved a mean score of 0.58 on Task A (span identification) and Task B (summarization) mean scores of 0.36 in Relevance and 0.28 in Factuality, finishing 12th on the final leaderboard. Notably, our system achieved higher precision in strict matching (0.20) than the top-performing system, demonstrating the effectiveness of our post-processing techniques. In this paper, we detail our ICL approach for adapting Large Language Models to Perspective-Aware Medical Summarization, analyze the improvements across development iterations, and finally discuss both the limitations of the current evaluation framework and future challenges in modeling this task. We release our code for reproducibility.</abstract>
      <url hash="e9e5e81b">2025.cl4health-1.35</url>
      <bibkey>antony-etal-2025-lightweight</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>AICOE</fixed-case> at <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm 2025: An Ensemble of Large Language Models for Perspective-Aware Healthcare Answer Summarization</title>
      <author><first>Rakshith</first><last>R</last><affiliation>Tredence</affiliation></author>
      <author><first>Mohammed Sameer</first><last>Khan</last><affiliation>Tredence</affiliation></author>
      <author><first>Ankush</first><last>Chopra</last><affiliation>Tredence Analytics</affiliation></author>
      <pages>398-408</pages>
      <abstract>The PerAnsSumm 2024 shared task at the CL4Health workshop focuses on generating structured, perspective-specific summaries to enhance the accessibility of health-related information. Given a Healthcare community QA dataset containing a question, context, and multiple user-answers, the task involves identifying relevant perspective categories, extracting spans from these perspectives, and generating concise summaries for the extracted spans. We fine-tuned open-source models such as Llama-3.2 3B, Llama-3.1 8B, and Gemma-2 9B, while also experimenting with proprietary models including GPT-4o, o1, Gemini-1.5 Pro, and Gemini-2 Flash Experimental using few-shot prompting. Our best-performing approach leveraged an ensemble strategy, combining span outputs from o1 (CoT) and Gemini-2 Flash Experimental. For overlapping perspectives, we prioritized Gemini. The final spans were summarized using Gemini, preserving the higher classification accuracy of o1 while leveraging Gemini’s superior span extraction and summarization capabilities. This hybrid method secured fourth place on the final leaderboard among 100 participants and 206 submissions.</abstract>
      <url hash="7fb45e28">2025.cl4health-1.36</url>
      <bibkey>r-etal-2025-aicoe</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>LTRC</fixed-case>-<fixed-case>IIITH</fixed-case> at <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm 2025: <fixed-case>S</fixed-case>pan<fixed-case>S</fixed-case>ense - Perspective-specific span identification and Summarization</title>
      <author><first>Sushvin</first><last>Marimuthu</last><affiliation>Student</affiliation></author>
      <author><first>Parameswari</first><last>Krishnamurthy</last><affiliation>Assistant Professor, IIIT Hyderabad</affiliation></author>
      <pages>409-414</pages>
      <abstract>Healthcare community question-answering (CQA) forums have become popular for users seeking medical advice, offering answers that range from personal experiences to factual information. Traditionally, CQA summarization relies on the best-voted answer as a reference summary. However, this approach overlooks the diverse perspectives across multiple responses. Structuring summaries by perspective could better meet users’ informational needs. The PerAnsSumm shared task addresses this by identifying and classifying perspective-specific spans (Task_A) and generating perspective-specific summaries from question-answer threads (Task_B). In this paper, we present our work on the PerAnsSumm shared task 2025 at the CL4Health Workshop, NAACL 2025. Our system leverages the RoBERTa-large model for identifying perspective-specific spans and the BART-large model for summarization. We achieved a Macro-F1 score of 0.9 (90%) and a Weighted-F1 score of 0.92 (92%) for classification. For span matching, our strict matching F1 score was 0.21 (21%), while proportional matching reached 0.68 (68%), resulting in an average Task A score of 0.6 (60%). For Task B, we achieved a ROUGE-1 score of 0.4 (40%), ROUGE-2 of 0.18 (18%), and ROUGE-L of 0.36 (36%). Additionally, we obtained a BERTScore of 0.84 (84%), METEOR of 0.37 (37%), and BLEU of 0.13 (13%), resulting in an average Task B score of 0.38 (38%). Combining both tasks, our system achieved an overall average score of 49% and ranked 6th on the official leaderboard for the shared task.</abstract>
      <url hash="f0adc075">2025.cl4health-1.37</url>
      <bibkey>marimuthu-krishnamurthy-2025-ltrc</bibkey>
    </paper>
    <paper id="38">
      <title><fixed-case>Y</fixed-case>ale<fixed-case>NLP</fixed-case> @ <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm 2025: Multi-Perspective Integration via Mixture-of-Agents for Enhanced Healthcare <fixed-case>QA</fixed-case> Summarization</title>
      <author><first>Dongsuk</first><last>Jang</last><affiliation>Yale University</affiliation></author>
      <author><first>Haoxin</first><last>Li</last><affiliation>University of Washington</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University</affiliation></author>
      <pages>415-427</pages>
      <abstract>Automated summarization of healthcare community question-answering forums is challenging due to diverse perspectives presented across multiple user responses to each question. The PerAnsSumm Shared Task was therefore proposed to tackle this challenge by identifying perspectives from different answers and then generating a comprehensive answer to the question. In this study, we address the PerAnsSumm Shared Task using two complementary paradigms: (i) a training-based approach through QLoRA fine-tuning of LLaMA-3.3-70B-Instruct, and (ii) agentic approaches including zero- and few-shot prompting with frontier LLMs (LLaMA-3.3-70B-Instruct and GPT-4o) and a Mixture-of-Agents (MoA) framework that leverages a diverse set of LLMs by combining outputs from multi-layer feedback aggregation. For perspective span identification/classification, GPT-4o zero-shot achieves an overall score of 0.57, substantially outperforming the 0.40 score of the LLaMA baseline. With a 2-layer MoA configuration, we were able to improve LLaMA performance up by 28{% to 0.51. For perspective-based summarization, GPT-4o zero-shot attains an overall score of 0.42 compared to 0.28 for the best LLaMA zero-shot, and our 2-layer MoA approach boosts LLaMA performance by 32{% to 0.37. Furthermore, in few-shot setting, our results show that the sentence-transformer embedding-based exemplar selection provides more gain than manually selected exemplars on LLaMA models, although the few-shot prompting is not always helpful for GPT-4o.</abstract>
      <url hash="a64b9157">2025.cl4health-1.38</url>
      <bibkey>jang-etal-2025-yalenlp</bibkey>
    </paper>
    <paper id="39">
      <title>Abdelmalak at <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm 2025: Leveraging a Domain-Specific <fixed-case>BERT</fixed-case> and <fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case> for Perspective-Aware Healthcare Answer Summarization</title>
      <author><first>Abanoub</first><last>Abdelmalak</last><affiliation>ZB MED - Information Centre for Life Sciences</affiliation></author>
      <pages>428-436</pages>
      <abstract>The PerAnsSumm Shared Task - CL4Health@NAACL 2025 aims to enhance healthcare community question-answering (CQA) by summarizing diverse user perspectives. It consists of two tasks: identifying and classifying perspective-specific spans (Task A) and generating structured, perspective-specific summaries from question-answer threads (Task B). The dataset used for this task is the PUMA dataset. For Task A, a COVID-Twitter-BERT model pre-trained on COVID-related text from Twitter was employed, improving the model’s understanding of relevant vocabulary and context. For Task B, LLaMA was utilized in a prompt-based fashion. The proposed approach achieved 9th place in Task A and 16th place overall, with the best proportional classification F1-score of 0.74.</abstract>
      <url hash="21ccedf9">2025.cl4health-1.39</url>
      <bibkey>abdelmalak-2025-abdelmalak</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>UMB</fixed-case>@<fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm 2025: Enhancing Perspective-Aware Summarization with Prompt Optimization and Supervised Fine-Tuning</title>
      <author><first>Kristin</first><last>Qi</last><affiliation>University of Massachusetts Boston</affiliation></author>
      <author><first>Youxiang</first><last>Zhu</last><affiliation>University of Massachusetts Boston</affiliation></author>
      <author><first>Xiaohui</first><last>Liang</last><affiliation>University of Massachusetts Boston</affiliation></author>
      <pages>437-444</pages>
      <abstract>We present our approach to the PerAnsSumm Shared Task, which involves perspective span identification and perspective-aware summarization in community question-answering (CQA) threads. For span identification, we adopt ensemble learning that integrates three transformer models through averaging to exploit individual model strengths, achieving an 82.91% F1-score on test data. For summarization, we design a suite of Chain-of-Thought (CoT) prompting strategies that incorporate keyphrases and guide information to structure summary generation into manageable steps. To further enhance summary quality, we apply prompt optimization using the DSPy framework and supervised fine-tuning (SFT) on Llama-3 to adapt the model to domain-specific data. Experimental results on validation and test sets show that structured prompts with keyphrases and guidance improve summaries aligned with references, while the combination of prompt optimization and fine-tuning together yields significant improvement in both relevance and factuality evaluation metrics.</abstract>
      <url hash="544317a6">2025.cl4health-1.40</url>
      <bibkey>qi-etal-2025-umb</bibkey>
    </paper>
    <paper id="41">
      <title>Overview of the <fixed-case>P</fixed-case>er<fixed-case>A</fixed-case>ns<fixed-case>S</fixed-case>umm 2025 Shared Task on Perspective-aware Healthcare Answer Summarization</title>
      <author><first>Siddhant</first><last>Agarwal</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Md. Shad</first><last>Akhtar</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Shweta</first><last>Yadav</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>445-455</pages>
      <abstract>This paper presents an overview of the Perspective-aware Answer Summarization (PerAnsSumm) Shared Task on summarizing healthcare answers in Community Question Answering forums hosted at the CL4Health Workshop at NAACL 2025. In this shared task, we approach healthcare answer summarization with two subtasks: (a) perspective span identification and classification and (b) perspective-based answer summarization (summaries focused on one of the perspective classes). Wedefine a benchmarking setup for comprehensive evaluation of predicted spans and generated summaries. We encouraged participants to explore novel solutions to the proposed problem and received high interest in the task with 23 participating teams and 155 submissions. This paper describes the task objectives, the dataset, the evaluation metrics and our findings. We share the results of the novel approaches adopted by task participants, especially emphasizing the applicability of Large Language Models in this perspective-based answer summarization task.</abstract>
      <url hash="daf8543c">2025.cl4health-1.41</url>
      <bibkey>agarwal-etal-2025-overview</bibkey>
    </paper>
    <paper id="42">
      <title>Bridging the Gap: Inclusive Artificial Intelligence for Patient-Oriented Language Processing in Conversational Agents in Healthcare</title>
      <author><first>Kerstin</first><last>Denecke</last><affiliation>BFH</affiliation></author>
      <pages>456-462</pages>
      <abstract>Conversational agents (CAs), such as medical interview assistants, are increasingly used in healthcare settings due to their potential for intuitive user interaction. Ensuring the inclusivity of these systems is critical to provide equitable and effective digital health support. However, the underlying technology, models and data can foster inequalities and exclude certain individuals. This paper explores key principles of inclusivity in patient-oriented language processing (POLP) for healthcare CAs to improve accessibility, cultural sensitivity, and fairness in patient interactions. We will outline, how considering the six facets of inclusive Artificial Intelligence (AI) will shape POLP within healthcare CA. Key considerations include leveraging diverse datasets, incorporating gender-neutral and inclusive language, supporting varying levels of health literacy, and ensuring culturally relevant communication. To address these issues, future research in POLP should focus on optimizing conversation structure, enhancing the adaptability of CAs’ language and content, integrating cultural awareness, improving explainability, managing cognitive load, and addressing bias and fairness concerns.</abstract>
      <url hash="87ae9e73">2025.cl4health-1.42</url>
      <bibkey>denecke-2025-bridging</bibkey>
    </paper>
  </volume>
</collection>
