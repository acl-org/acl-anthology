<?xml version='1.0' encoding='UTF-8'?>
<collection id="2001.jeptalnrecital">
  <volume id="invite" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 8ème conférence sur le Traitement Automatique des Langues Naturelles. Conférences invitées</booktitle>
      <editor><first>Denis</first><last>Maurel</last></editor>
      <publisher>ATALA</publisher>
      <address>Tours, France</address>
      <month>July</month>
      <year>2001</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="a721700f">2001.jeptalnrecital-invite.0</url>
      <bibkey>jep-taln-recital-2001-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Language Processing with Weighted Transducers</title>
      <author><first>Mehryar</first><last>Mohri</last></author>
      <pages>5–14</pages>
      <abstract>Weighted automata and transducers are used in a variety of applications ranging from automatic speech recognition and synthesis to computational biology. They give a unifying framework for the representation of the components of complex systems. This provides opportunities for the application of general optimization algorithms such as determinization, epsilon-removal and minimization of weighted transducers. We give a brief survey of recent advances in language processing with weighted automata and transducers, including an overview of speech recognition with weighted transducers and recent algorithmic results in that field. We also present new results related to the approximation of weighted context-free grammars and language recognition with weighted automata.</abstract>
      <url hash="e3ed39ab">2001.jeptalnrecital-invite.1</url>
      <bibkey>mohri-2001-language</bibkey>
    </paper>
    <paper id="2">
      <title>Analyse syntaxique automatique de langues du combinatoire au calculatoire</title>
      <author><first>Jacques</first><last>Vergne</last></author>
      <pages>15–29</pages>
      <abstract>Nous proposons de montrer comment l’analyse syntaxique automatique est aujourd’hui à un tournant de son évolution, en mettant l’accent sur l’évolution des modèles d’analyse syntaxique : de l’analyse de langages de programmation (compilation) à l’analyse de langues, et, dans le cadre de l’analyse de langues, de l’analyse combinatoire à l’analyse calculatoire, en passant par le tagging et le chunking (synthèse en section 4). On marquera d’abord le poids historique des grammaires formelles, comme outil de modélisation des langues et des langages formels (section 1), et comment la compilation a été transposée en traduction automatique par Bernard Vauquois. On analysera ensuite pourquoi il n’a pas été possible d’obtenir en analyse de langue un fonctionnement analogue à la compilation, et pourquoi la complexité linéaire de la compilation n’a pas pu être transposée en analyse syntaxique (section 2). Les codes analysés étant fondamentalement différents, et le tagging ayant montré la voie, nous en avons pris acte en abandonnant la compilation transposée : plus de dictionnaire exhaustif en entrée, plus de grammaire formelle pour modéliser les structures linguistiques (section 3). Nous montrerons comment, dans nos analyseurs, nous avons implémenté une solution calculatoire, de complexité linéaire (section 5). Nous conclurons (section 6) en pointant quelques évolutions des tâches de l’analyse syntaxique.</abstract>
      <url hash="e95d128d">2001.jeptalnrecital-invite.2</url>
      <language>fra</language>
      <bibkey>vergne-2001-analyse</bibkey>
    </paper>
  </volume>
  <volume id="long" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 8ème conférence sur le Traitement Automatique des Langues Naturelles. Articles longs</booktitle>
      <editor><first>Denis</first><last>Maurel</last></editor>
      <publisher>ATALA</publisher>
      <address>Tours, France</address>
      <month>July</month>
      <year>2001</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="55d60984">2001.jeptalnrecital-long.0</url>
      <bibkey>jep-taln-recital-2001-actes-de</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Un corpus français arboré : quelques interrogations</title>
      <author><first>Anne</first><last>Abeillé</last></author>
      <author><first>Lionel</first><last>Clément</last></author>
      <author><first>Alexandra</first><last>Kinyon</last></author>
      <author><first>François</first><last>Toussenel</last></author>
      <pages>33–42</pages>
      <abstract>Dans cet article nous présentons les premiers résultats de l’exploitation d’un Corpus français arboré (Abeillé et al., 2001). Le corpus comprend 1 million de mots entièrement annotés et validé pour les parties du discours, la morphologie, les mots composés et les lemmes, et partiellement annotés pour les constituants syntaxiques. Il comprend des extraits de journaux parus entre 1989 et 1993 et écrits par divers auteurs, et couvre différents thèmes (économie, littérature, politique, etc.). Après avoir expliqué comment ce corpus a été construit, et comment l’exploiter à l’aide d’un outil de recherche spécifique, nous exposerons quelques résultats linguistiques concernant les fréquences et les préférences lexicales et syntaxiques. Nous expliquerons pourquoi nous pensons que certains de ces résultats sont pertinents en linguistique théorique et en psycholinguistique.</abstract>
      <url hash="f00b5825">2001.jeptalnrecital-long.1</url>
      <language>fra</language>
      <bibkey>abeille-etal-2001-un</bibkey>
    </paper>
    <paper id="2">
      <title>Représenter le temps en langue dans le formalisme des graphes conceptuels une approche basée sur les schèmes sémantico-cognitifs</title>
      <author><first>Tassadit</first><last>Amghar</last></author>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <pages>43–52</pages>
      <abstract>L’objectif de notre travail est de construire une représentation sémantique d’un corpus de textes français au sein des graphes conceptuels simples. Notre conceptualisation est fondée sur les Schèmes Sémantico-Cognitifs et la théorie aspecto-temporelle introduits par J. P. Desclés. Un texte est représenté par deux structures. La première modélise la représention semanticocognitive des propositions du texte, et la seconde le diagramme temporel exprimant les contraintes temporelles entre les différentes situations décrites dans le texte. La prise en compte de ces deux structures et des liens qu’elles entretiennent nous a amenés à modifier le modèle des graphes conceptuels simples et à envisager les modes d’interaction entre temps, aspect (grammatical) et significations des lexèmes verbaux.</abstract>
      <url hash="712ef815">2001.jeptalnrecital-long.2</url>
      <language>fra</language>
      <bibkey>amghar-etal-2001-representer</bibkey>
    </paper>
    <paper id="3">
      <title>Aides à l’analyse pour la construction de banque d’arbres : étude de l’effort</title>
      <author><first>Nicolas</first><last>Auclerc</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <pages>53–62</pages>
      <abstract>La construction de banque d’arbres est une entreprise lourde qui prend du temps. Pour faciliter cette construction, nous voyons la construction de banques d’arbres comme une série d’opérations d’édition et de recherche. Le but de cet article est d’estimer l’effort, en nombre d’opérations d’éditions, nécessaire pour ajouter une nouvelle phrase dans la banque d’arbres. Nous avons proposé un outil, Boardedit, qui inclut un éditeur d’arbres et des aides a l’analyse. Comme l’effort nécessaire dépend bien sûr de la qualité des réponses fournies par les aides a l’analyse, il peut être vue comme une mesure de la qualité de ces aides. L’éditeur d’arbres restant indispensable a notre outil pendant l’eXpérience, les aides a l’analyse seront donc toujours associées a l’éditeur d’arbres. Dans l’eXpérience proposée, nous augmentons une banque d’arbres de 5 000 phrases par l 553 nouvelles phrases. La réduction obtenue est supérieure auX 4/5 de l’effort.</abstract>
      <url hash="260b3e67">2001.jeptalnrecital-long.3</url>
      <language>fra</language>
      <bibkey>auclerc-lepage-2001-aides</bibkey>
    </paper>
    <paper id="4">
      <title>Atelier <fixed-case>ATOLL</fixed-case> pour les grammaires d’arbres adjoints</title>
      <author><first>François</first><last>Barthélemy</last></author>
      <author><first>Pierre</first><last>Boullier</last></author>
      <author><first>Philippe</first><last>Deschamp</last></author>
      <author><first>Linda</first><last>Kaouane</last></author>
      <author><first>Éric</first><last>Villemonte De La Clergerie</last></author>
      <pages>63–72</pages>
      <abstract>Cet article présente l’environnement de travail que nous développons au sein de l’équipe ATOLL pour les grammaires d’arbres adjoints. Cet environnement comprend plusieurs outils et ressources fondés sur l’emploi du langage de balisage XML. Ce langage facilite la mise en forme et l’échange de ressources linguistiques.</abstract>
      <url hash="781e4edb">2001.jeptalnrecital-long.4</url>
      <language>fra</language>
      <bibkey>barthelemy-etal-2001-atelier</bibkey>
    </paper>
    <paper id="5">
      <title>Modèle d’exploration contextuelle pour l’analyse sémantique de textes</title>
      <author><first>Slim</first><last>Ben Hazez</last></author>
      <author><first>Jean-Pierre</first><last>Desclés</last></author>
      <author><first>Jean-Luc</first><last>Minel</last></author>
      <pages>73–82</pages>
      <abstract>Nous présentons dans cet article un modèle d’exploration contextuelle et une plate-forme logicielle qui permet d’accéder au contenu sémantique des textes et d’en extraire des séquences particulièrement pertinentes. L’objectif est de développer et d’exploiter des ressources linguistiques pour identifier dans les textes, indépendamment des domaines traités, certaines des relations organisatrices des connaissances ainsi que les organisations discursives mises en places par l’auteur. L’analyse sémantique du texte est guidée par le repérage d’indices linguistiques déclencheurs dont l’emploi est représentatif des notions étudiées.</abstract>
      <url hash="b5524a45">2001.jeptalnrecital-long.5</url>
      <language>fra</language>
      <bibkey>ben-hazez-etal-2001-modele</bibkey>
    </paper>
    <paper id="6">
      <title>Intégration probabiliste de sens dans la représentation de textes</title>
      <author><first>Romaric</first><last>Besançon</last></author>
      <author><first>Antoine</first><last>Rozenknop</last></author>
      <author><first>Jean-Cédric</first><last>Chappelier</last></author>
      <author><first>Martin</first><last>Rajman</last></author>
      <pages>83–91</pages>
      <abstract>Le sujet du présent article est l’intégration des sens portés par les mots en contexte dans une représentation vectorielle de textes, au moyen d’un modèle probabiliste. La représentation vectorielle considérée est le modèle DSIR, qui étend le modèle vectoriel (VS) standard en tenant compte à la fois des occurrences et des co-occurrences de mots dans les documents. L’intégration des sens dans cette représentation se fait à l’aide d’un modèle de Champ de Markov avec variables cachées, en utilisant une information sémantique dérivée de relations de synonymie extraites d’un dictionnaire de synonymes.</abstract>
      <url hash="2196669a">2001.jeptalnrecital-long.6</url>
      <language>fra</language>
      <bibkey>besancon-etal-2001-integration</bibkey>
    </paper>
    <paper id="7">
      <title>Les n-grams de caractères pour l’aide à l’extraction de connaissances dans des bases de données textuelles multilingues</title>
      <author><first>Ismaïl</first><last>Biskri</last></author>
      <author><first>Sylvain</first><last>Delisle</last></author>
      <pages>92–101</pages>
      <abstract>Une véritable classification numérique multilingue est impossible si on considère seulement le mot comme unité d’information privilégiée. En traitant les mots comme jetons, la tokenisation s’avère relativement simple pour le français et l’anglais, mais très difficile pour des langues comme l’allemand ou l’arabe. D’autre part, la lemmatisation utilisée comme moyen de normalisation et de réduction du lexique constitue un écueil non moins négligeable. La notion de n-grams, qui depuis une décennie donne de bons résultats dans Pidentification de la langue ou dans l’analyse de l’oral, est, par les recherches récentes, devenue un axe privilégié dans l’acquisition et l’extraction des connaissances dans les textes. Dans cet article, nous présenterons un outil de classification numérique basé sur le concept de n-grams de caractères. Nous évaluons aussi les résultats de cet outil que nous comparons à des résultats obtenus au moyen d’une classification fondée sur des mots.</abstract>
      <url hash="aae498c6">2001.jeptalnrecital-long.7</url>
      <language>fra</language>
      <bibkey>biskri-delisle-2001-les</bibkey>
    </paper>
    <paper id="8">
      <title>Dépendances à distance dans les grammaires de propriétés : l’exemple des disloquées</title>
      <author><first>Philippe</first><last>Blache</last></author>
      <pages>102–111</pages>
      <abstract>Cet article propose une description des dépendances à distances s’appuyant sur une approche totalement déclarative, les grammaires de propriétés, décrivant l’information linguistique sous la forme de contraintes. L’approche décrite ici consiste à introduire de façon dynamique en cours d’analyse de nouvelles contraintes, appelées propriétés distantes. Cette notion est illustrée par la description du phénomène des disloquées en français.</abstract>
      <url hash="720555e3">2001.jeptalnrecital-long.8</url>
      <language>fra</language>
      <bibkey>blache-2001-dependances</bibkey>
    </paper>
    <paper id="9">
      <title>L’interrogation de bases de données comme application des classes d’objets</title>
      <author><first>Béatrice</first><last>Bouchou</last></author>
      <author><first>Julien</first><last>Lerat</last></author>
      <author><first>Denis</first><last>Maurel</last></author>
      <pages>112–121</pages>
      <abstract>En travaillant sur l’interrogation de bases de données en langue naturelle, nous sommes amenés à exploiter les propositions du Laboratoire de Linguistique Informatique (LLI) en matière de représentation de la langue : les classes d’objets. Un outil d’interrogation définit une application du langage vers le modèle de l’information stockée. Ici les classes d’objets et leurs prédicats appropriés modélisent le langage source, tandis que le modèle relationnel sert pour les données interrogées. Nous présentons d’abord ce contexte d’application, puis comment nous utilisons les classes d’objets et prédicats appropriés dans ce cadre.</abstract>
      <url hash="ad4f5541">2001.jeptalnrecital-long.9</url>
      <language>fra</language>
      <bibkey>bouchou-etal-2001-linterrogation</bibkey>
    </paper>
    <paper id="10">
      <title>Etiquetage prosodique semi-automatique des corpus oraux</title>
      <author><first>Estelle</first><last>Campione</last></author>
      <author><first>Jean</first><last>Véronis</last></author>
      <pages>122–131</pages>
      <abstract>La transcription manuelle de la prosodie est une tâche extrêmement coûteuse en temps, qui requiert des annotateurs très spécialisés, et qui est sujette à de multiples erreurs et une grande part de subjectivité. Une automatisation complète n’est pas envisageable dans l’état actuel de la technologie, mais nous présentons dans cette communication des outils et une méthodologie qui permettent une réduction substantielle du temps d’intervention manuelle, et améliorent l’objectivité et la cohérence du résultat. De plus, les étapes manuelles nécessaires ne demandent pas une expertise phonétique poussée et peuvent être menées à bien par des étudiants et des “linguistes de corpus”.</abstract>
      <url hash="c1072247">2001.jeptalnrecital-long.10</url>
      <language>fra</language>
      <bibkey>campione-veronis-2001-etiquetage</bibkey>
    </paper>
    <paper id="11">
      <title>Grammaire à substitution d’arbre de complexité polynomiale : un cadre efficace pour <fixed-case>DOP</fixed-case></title>
      <author><first>Jean-Cédric</first><last>Chappelier</last></author>
      <author><first>Martin</first><last>Rajman</last></author>
      <pages>132–141</pages>
      <abstract>Trouver l’arbre d’analyse le plus probable dans le cadre du modèle DOP (Data-Oriented Parsing) — une version probabiliste de grammaire à substitution d’arbres développée par R. Bod (1992) — est connu pour être un problème NP-difficile dans le cas le plus général (Sima’an, 1996a). Cependant, si l’on introduit des restrictions a priori sur le choix des arbres élémentaires, on peut obtenir des instances particulières de DOP pour lesquelles la recherche de l’arbre d’analyse le plus probable peut être effectuée en un temps polynomial (par rapport à la taille de la phrase à analyser). La présente contribution se propose d’étudier une telle instance polynomiale de DOP, fondée sur le principe de sélection miminale-maximale et d’en évaluer les performances sur deux corpus différents.</abstract>
      <url hash="13266fb5">2001.jeptalnrecital-long.11</url>
      <language>fra</language>
      <bibkey>chappelier-rajman-2001-grammaire</bibkey>
    </paper>
    <paper id="12">
      <title>Aggregation by Conflation of Quasi-Synonymous Units in Author Abstracting</title>
      <author><first>Choy-Kim</first><last>Chuah</last></author>
      <pages>142–151</pages>
      <abstract>In text generation, studies on aggregation often focus on the use of connectives to combine short made-up sentences. But connectives restrict the number of units that may be combined at any one time. So, how does information get condensed into fewer units without excessive use of connectives? From a comparison of document and abstract, this reconnaissance study reports on some preferred patterns in aggregation when authors write abstracts for journal articles on biology. The paper also discusses some prerequisites and difficulties anticipated for abstracting systems. More sentences were aggregated without than with the use of an explicit sign, such as a connective or a (semi-)colon.</abstract>
      <url hash="46d823d3">2001.jeptalnrecital-long.12</url>
      <bibkey>chuah-2001-aggregation</bibkey>
    </paper>
    <paper id="13">
      <title>Utilisation des entités nommées et des variantes terminologiques dans un système de question-réponse</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Martine</first><last>Hurault-Plantet</last></author>
      <author><first>Gabriel</first><last>Illouz</last></author>
      <author><first>Christian</first><last>Jacquemin</last></author>
      <pages>152–161</pages>
      <abstract>Nous présentons dans cet article le système QALC qui a participé à la tâche Question Answering de la conférence d’évaluation TREC. Ce système repose sur un ensemble de modules de Traitement Automatique des Langues (TAL) intervenant essentiellement en aval d’un moteur de recherche opérant sur un vaste ensemble de documents : typage des questions, reconnaissance des entités nommées, extraction et reconnaissance de termes, simples et complexes, et de leurs variantes. Ces traitements permettent soit de mieux sélectionner ces documents, soit de décider quelles sont les phrases susceptibles de contenir la réponse à une question.</abstract>
      <url hash="c6027f29">2001.jeptalnrecital-long.13</url>
      <language>fra</language>
      <bibkey>ferret-etal-2001-utilisation</bibkey>
    </paper>
    <paper id="14">
      <title>Repérage de structures thématiques dans des textes</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Jean-Luc</first><last>Minel</last></author>
      <author><first>Sylvie</first><last>Porhiel</last></author>
      <pages>162–171</pages>
      <abstract>Afin d’améliorer les performances des systèmes de résumé automatique ou de filtrage sémantique concernant la prise en charge de la cohérence thématique, nous proposons un modèle faisant collaborer une méthode d’analyse statistique qui identifie les ruptures thématiques avec un système d’analyse linguistique qui identifie les cadres de discours.</abstract>
      <url hash="e1fa83c4">2001.jeptalnrecital-long.14</url>
      <language>fra</language>
      <bibkey>ferret-etal-2001-reperage</bibkey>
    </paper>
    <paper id="15">
      <title>Influence de facteurs stylistiques, syntaxiques et lexicaux sur la réalisation de la liaison en français</title>
      <author><first>Cécile</first><last>Fougeron</last></author>
      <author><first>Jean-Philippe</first><last>Goldman</last></author>
      <author><first>Alicia</first><last>Dart</last></author>
      <author><first>Laurence</first><last>Guélat</last></author>
      <author><first>Clémentine</first><last>Jeager</last></author>
      <pages>172–181</pages>
      <abstract>Les nombreuses recherches portant sur le phénomène de la liaison en français ont pu mettre en évidence l’influence de divers paramètres linguistiques et para-linguistiques sur la réalisation des liaisons. Notre contribution vise à déterminer la contribution relative de certains de ces facteurs en tirant parti d’une méthodologie robuste ainsi que d’outils de traitement automatique du langage. A partir d’un corpus de 5h de parole produit par 10 locuteurs, nous étudions les effets du style de parole (lecture oralisée/parole spontanée), du débit de parole (lecture normale/rapide), ainsi que la contribution de facteurs syntaxiques et lexicaux (longueur et fréquence lexicale) sur la réalisation de la liaison. Les résultats montrent que si plusieurs facteurs étudiés prédisent certaines liaisons, ces facteurs sont souvent interdépendants et ne permettent pas de modéliser avec exactitude la réalisation des liaisons.</abstract>
      <url hash="957c7945">2001.jeptalnrecital-long.15</url>
      <language>fra</language>
      <bibkey>fougeron-etal-2001-influence</bibkey>
    </paper>
    <paper id="16">
      <title>Elaboration d’une cascade de transducteurs pour l’extraction des noms de personnes dans les textes</title>
      <author><first>Nathalie</first><last>Friburger</last></author>
      <author><first>Denis</first><last>Maurel</last></author>
      <pages>182–191</pages>
      <abstract>Cet article décrit une cascade de transducteurs pour l’extraction de noms propres dans des textes. Après une phase de pré-traitement (découpage du texte en phrases, étiquetage à l’aide de dictionnaires), une série de transducteurs sont appliqués les uns après les autres sur le texte et permettent de repérer, dans les contextes gauches et droits des éléments “déclencheurs” qui signalent la présence d’un nom de personne. Une évaluation sur un corpus journalistique (journal Le Monde) fait apparaître un taux de précision de 98,7% pour un taux de rappel de 91,9%.</abstract>
      <url hash="b87af477">2001.jeptalnrecital-long.16</url>
      <language>fra</language>
      <bibkey>friburger-maurel-2001-elaboration</bibkey>
    </paper>
    <paper id="17">
      <title>Extraction automatique de motifs syntaxiques</title>
      <author><first>Jean-Gabriel</first><last>Ganascia</last></author>
      <pages>192–201</pages>
      <abstract>Cet article présente un nouvel algorithme de détection de motifs syntaxiques récurrents dans les textes écrits en langage naturel. Il décrit d’abord l’algorithme d’extraction fondé sur un modèle d’édition généralisé à des arbres stratifiés ordonnés (ASO). Il décrit ensuite les expérimentations qui valident l’approche préconisée sur des textes de la littérature française classique des XVIIIe et XIXe siècle. Une sous-partie est consacrée à l’évaluation empirique de la complexité algorithmique. La dernière sous-partie donnera quelques exemples de motifs récurrents typiques d’un auteur du XVIIIe siècle, Madame de Lafayette.</abstract>
      <url hash="6122a7b1">2001.jeptalnrecital-long.17</url>
      <language>fra</language>
      <bibkey>ganascia-2001-extraction</bibkey>
    </paper>
    <paper id="18">
      <title>Compréhension Automatique de la Parole combinant syntaxe locale et sémantique globale pour une <fixed-case>CHM</fixed-case> portant sur des tâches relativement complexes</title>
      <author><first>Jérôme</first><last>Goulian</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <pages>202–211</pages>
      <abstract>Nous présentons dans cet article un système de Compréhension Automatique de la Parole (CAP) tentant de concilier les contraintes antinomiques de robustesse et d’analyse détaillée de la parole spontanée. Dans une première partie, nous montrons l’importance de la mise en oeuvre d’une CAP fine dans l’optique d’une Communication Homme-Machine (CHM) sur des tâches moyennement complexes. Nous présentons ensuite l’architecture de notre système qui repose sur une analyse en deux étapes : une première étape d’analyse syntaxique de surface (Shallow Parsing) générique suivie d’une seconde étape d’analyse sémantico-pragmatique – dépendante du domaine d’application – de la structure profonde de l’ ́enoncé complet.</abstract>
      <url hash="1a474a1a">2001.jeptalnrecital-long.18</url>
      <language>fra</language>
      <bibkey>goulian-antoine-2001-comprehension</bibkey>
    </paper>
    <paper id="19">
      <title>Exploitation de l’expertise humaine dans un processus de constitution de terminologie</title>
      <author><first>Thierry</first><last>Hamon</last></author>
      <author><first>Adeline</first><last>Nazarenko</last></author>
      <pages>212–221</pages>
      <abstract>Le processus de construction de terminologie ne peut être entièrement automatisé. Les méthodes et des outils de la terminologie computationnelle permettent de prendre en charge une partie de la tâche, mais l’expertise humaine garde une place prépondérant. Le défi pour les outils terminologiques est de dégrossir les tâches qui sont soit trop longues soit trop complexes pour l’utilisateur tout en permettant à ce dernier d’intégrer ses propres connaissances spécialisées et en lui laissant le contrôle sur la terminologie à construire. Nous montrons ici comment le rôle de cette expertise est pris en compte dans SynoTerm, l’outil d’acquisition de relation de synonymie entre termes que nous avons d ́eveloppé.</abstract>
      <url hash="cf18a247">2001.jeptalnrecital-long.19</url>
      <language>fra</language>
      <bibkey>hamon-nazarenko-2001-exploitation</bibkey>
    </paper>
    <paper id="20">
      <title>Analogies morpho-synonymiques. Une méthode d’acquisition automatique de liens morphologiques à partir d’un dictionnaire de synonymes</title>
      <author><first>Nabil</first><last>Hathout</last></author>
      <pages>222–231</pages>
      <abstract>Cet article présente une méthode de construction automatique de liens morphologiques à partir d’un dictionnaire de synonymes. Une analyse de ces liens met en lumière certains aspects de la structure morphologique du lexique dont on peut tirer partie pour identifier les variations allomorphiques des suffixations extraites.</abstract>
      <url hash="1fa86d9e">2001.jeptalnrecital-long.20</url>
      <language>fra</language>
      <bibkey>hathout-2001-analogies</bibkey>
    </paper>
    <paper id="21">
      <title>Synonymies et vecteurs conceptuels</title>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Violaine</first><last>Prince</last></author>
      <pages>232–241</pages>
      <abstract>La synonymie est une relation importante en TAL mais qui reste problématique. La distinction entre synonymie relative et synonymie subjective permet de contourner certaines difficultés. Dans le cadre des vecteurs conceptuels, il est alors possible de définir formellement des fonctions de test de synonymie et d’en expérimenter l’usage.</abstract>
      <url hash="d13b438e">2001.jeptalnrecital-long.21</url>
      <language>fra</language>
      <bibkey>lafourcade-prince-2001-synonymies</bibkey>
    </paper>
    <paper id="22">
      <title>Récupération de segments sous-phrastiques dans une mémoire de traduction</title>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>Michel</first><last>Simard</last></author>
      <pages>242–251</pages>
      <abstract>L’utilité des outils d’aide à la traduction reposant sur les mémoires de traduction est souvent limitée par la nature des segments que celles-ci mettent en correspondance, le plus souvent des phrases entières. Cet article examine le potentiel d’un type de système qui serait en mesure de récupérer la traduction de séquences de mots de longueur arbitraire.</abstract>
      <url hash="c6a6f7d7">2001.jeptalnrecital-long.22</url>
      <language>fra</language>
      <bibkey>langlais-simard-2001-recuperation</bibkey>
    </paper>
    <paper id="23">
      <title>Vers une plate-forme multi-agents pour l’exploration et le traitement linguistiques</title>
      <author><first>Thomas</first><last>Lebarbé</last></author>
      <pages>252–261</pages>
      <abstract>Dans cet article, nous proposons une plate-forme multi-agents pour l’expérimentation et le traitement linguistique. Après une description du modèle d’agent APA, nous présentons l’état actuel de nos travaux: une implémentation en système multi-agents de l’analyse syntaxique selon le paradigme des grammaires de dépendances en chunk. Nous montrons ensuite d’autres possibilités d’implémentation selon d’autres paradigmes syntaxiques mais aussi au delà de la simple syntaxe.</abstract>
      <url hash="475541c2">2001.jeptalnrecital-long.23</url>
      <language>fra</language>
      <bibkey>lebarbe-2001-vers</bibkey>
    </paper>
    <paper id="24">
      <title>Une typologie des énumérations basée sur les structures rhétoriques et architecturales du texte</title>
      <author><first>Christophe</first><last>Luc</last></author>
      <pages>262–271</pages>
      <abstract>Cet article concerne la caractérisation et la représentation de la structure interne des énumérations. Pour ce faire, nous utilisons deux modèles de texte : d’une part la Théorie des Structures Rhétoriques (RST) qui fournit un cadre d’interprétation pour la structure discursive des textes et d’autre part le modèle de représentation de l’architecture textuelle qui est principalement dédié à l’étude et à la représentation des structures visuelles des textes. Après une brève présentation des modèles, nous nous concentrons sur l’étude de l’objet “énumérations”. Nous exhibons et commentons trois exemples d’énumérations spécifiques que nous appelons des énumérations non-parallèles. Nous analysons la structure de ces énumérations et proposons un principe de composition des modèles de référence pour représenter ces énumérations. Enfin, nous présentons une classification des énumérations s’appuyant sur les caractéristiques de ces modèles.</abstract>
      <url hash="a506cf18">2001.jeptalnrecital-long.24</url>
      <language>fra</language>
      <bibkey>luc-2001-une</bibkey>
    </paper>
    <paper id="25">
      <title>Désambiguïsation syntaxique des groupes nominaux en anglais médical : étude des structures adjectivales à partir d’un corpus bilingue</title>
      <author><first>François</first><last>Maniez</last></author>
      <pages>272–281</pages>
      <abstract>L’ambiguïté syntaxique constitue un problème particulièrement délicat à résoudre pour les analyseurs morphosyntaxiques des logiciels d’aide à la traduction, en particulier dans le cas des longs groupes nominaux typiques des langues de spécialité. En utilisant un corpus bilingue d’articles médicaux anglais traduits vers le français, nous examinons divers moyens de résoudre l’ambiguïté du rattachement de l’adjectif à l’un des deux noms qui le suivent dans les tournures anglaises de forme adjectif-nom-nom.</abstract>
      <url hash="994ca899">2001.jeptalnrecital-long.25</url>
      <language>fra</language>
      <bibkey>maniez-2001-desambiguisation</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>DEFI</fixed-case>, un outil d’aide à la compréhension</title>
      <author><first>Archibald</first><last>Michiels</last></author>
      <pages>282–292</pages>
      <abstract/>
      <url hash="1e4605e0">2001.jeptalnrecital-long.26</url>
      <language>fra</language>
      <bibkey>michiels-2001-defi</bibkey>
    </paper>
    <paper id="27">
      <title>Extraction d’information dans les bases de données textuelles en génomique au moyen de transducteurs à nombre fini d’états</title>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <pages>293–302</pages>
      <abstract>Cet article décrit un système d’extraction d’information sur les interactions entre gènes à partir de grandes bases de données textuelles. Le système est fondé sur une analyse au moyen de transducteurs à nombre fini d’états. L’article montre comment une partie des ressources (verbes d’interaction) peut être acquise de manière semi-automatique. Une évaluation détaillée du système est fournie.</abstract>
      <url hash="7547dd7a">2001.jeptalnrecital-long.27</url>
      <language>fra</language>
      <bibkey>poibeau-2001-extraction</bibkey>
    </paper>
    <paper id="28">
      <title>Ontologies for Information Retrieval</title>
      <author><first>Amalia</first><last>Todiraşcu</last></author>
      <author><first>François</first><last>Rousselot</last></author>
      <pages>303–312</pages>
      <abstract>The paper presents a system for querying (in natural language) a set of text documents from a limited domain. The domain knowledge, represented in description logics (DL), is used for filtering the documents returned as answer and it is extended dynamically (when new concepts are identified in the texts), as result of DL inference mechanisms. The conceptual hierarchy is built semi-automatically from the texts. Concept instances are identified using shallow natural language parsing techniques.</abstract>
      <url hash="a0b03b51">2001.jeptalnrecital-long.28</url>
      <bibkey>todirascu-rousselot-2001-ontologies</bibkey>
    </paper>
    <paper id="29">
      <title>A System for Extraction of Temporal Expressions from <fixed-case>F</fixed-case>rench Texts</title>
      <author><first>Nikolai</first><last>Vazov</last></author>
      <pages>313–322</pages>
      <abstract>We present a system for extraction of temporal expressions from French texts. The identification of the temporal expressions is based on a context-scanning strategy (CSS) which is carried out by two complementary techniques: search for regular expressios and left-to-right and right-to-left local chartparsing. A number of semantic and distant-dependency constraints have been integrated to the chartparsing procedure in order to improve the precision of the system.</abstract>
      <url hash="0fbf3149">2001.jeptalnrecital-long.29</url>
      <bibkey>vazov-2001-system-extraction</bibkey>
    </paper>
  </volume>
  <volume id="poster" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 8ème conférence sur le Traitement Automatique des Langues Naturelles. Posters</booktitle>
      <editor><first>Denis</first><last>Maurel</last></editor>
      <publisher>ATALA</publisher>
      <address>Tours, France</address>
      <month>July</month>
      <year>2001</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="0574ea8e">2001.jeptalnrecital-poster.0</url>
      <bibkey>jep-taln-recital-2001-actes-de-la</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Modèles de langage hiérarchiques pour les applications de dialogue en parole spontanée</title>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <author><first>Renato</first><last>De Mori</last></author>
      <pages>325–330</pages>
      <abstract>Le cadre de cette étude concerne les systèmes de dialogue via le téléphone entre un serveur de données et un utilisateur. Nous nous intéresserons au cas de dialogues non contraints où l’utilisateur à toute liberté pour formuler ses requêtes. Généralement, le module de Reconnaissance Automatique de la Parole (RAP) de tels serveurs utilise un seul Modèle de Langage (ML) de type bigramme ou trigramme pour modéliser l’ensemble des interventions possibles de l’utilisateur. Ces ML sont appris sur des corpus de phrases retranscrites à partir de sessions entre le serveur et plusieurs utilisateurs. Nous proposons dans cette étude une méthode de segmentation de corpus d’apprentissage de dialogue utilisant une stratégie mixte basée à la fois sur des connaissances explicites mais aussi sur l’optimisation d’un critère statistique. Nous montrons qu’un gain en terme de perplexité et de taux d’erreurs/mot peut être constaté en utilisant un ensemble de sous modèles de langage issus de la segmentation plutôt qu’un modèle unique appris sur l’ensemble du corpus.</abstract>
      <url hash="39836326">2001.jeptalnrecital-poster.1</url>
      <language>fra</language>
      <bibkey>bechet-etal-2001-modeles</bibkey>
    </paper>
    <paper id="2">
      <title>Ressources linguistiques informatisées de l’<fixed-case>ATILF</fixed-case></title>
      <author><first>Pascale</first><last>Bernard</last></author>
      <author><first>Charles</first><last>Bernet</last></author>
      <author><first>Jacques</first><last>Dendien</last></author>
      <author><first>Jean-Marie</first><last>Pierrel</last></author>
      <author><first>Gilles</first><last>Souvay</last></author>
      <author><first>Zina</first><last>Tucsnak</last></author>
      <pages>331–336</pages>
      <abstract>Cette contribution présente les ressources linguistiques informatisées du laboratoire ATILF (Analyses et Traitements Informatiques du Lexique Français) disponibles sur la toile et sert de support aux démonstrations prévues dans le cadre de TALN 2001. L’ATILF est la nouvelle U1[R créée en association entre le CNRS et l’Université Nancy 2 qui, depuis le 2 janvier 2001, a succédé à la composante nancéienne de l’INaLF. Ces importantes ressources sur la langue française regroupent un ensemble de plus de 3500 textes réunis dans Frantext et divers dictionnaires, lexiques et autres bases de données. Ces ressources exploitent les fonctionnalités du logiciel Stella, qui correspond à un véritable moteur de recherche dédié aux bases textuelles s’appuyant sur une nouvelle théorie des objets textuels. La politique du laboratoire consiste à ouvrir très largement ses ressources en particulier au monde de la recherche et de l’enseignement.</abstract>
      <url hash="4c973fe7">2001.jeptalnrecital-poster.2</url>
      <language>fra</language>
      <bibkey>bernard-etal-2001-ressources</bibkey>
    </paper>
    <paper id="3">
      <title>Just What May be Deleted or Compressed in Abstracting?</title>
      <author><first>Choy-Kim</first><last>Chuah</last></author>
      <pages>337–342</pages>
      <abstract>Abstracts constituted from extracted sentences contain unneeded information that may be deleted, or compressed into simpler units. By comparing full text sentences used in abstracting with correspond-ing sentences in abstract, the study found such units to include metadiscourse phrases, parenthetical texts, redundant units inserted for emphasis, or are repetitions. Apposed texts and units such as modifiers and relative clauses which provide details and precision in the full text, but are out of place in an abstract, are also deleted.</abstract>
      <url hash="83cf8ae9">2001.jeptalnrecital-poster.3</url>
      <bibkey>chuah-2001-just</bibkey>
    </paper>
    <paper id="4">
      <title>Cartographie de Textes: Une aide à l’utilisateur dans le cadre de la découverte de nouveaux domaines</title>
      <author><first>Isabelle</first><last>Debourges</last></author>
      <author><first>Sylvie</first><last>Guilloré-Billot</last></author>
      <author><first>Christel</first><last>Vrain</last></author>
      <pages>343–348</pages>
      <abstract>Nous présentons les avancées d’un projet dans un thème que nous qualifions de Cartographie de Textes qui permet à l’utilisateur novice d’explorer un nouveau domaine par navigation au sein d’un corpus homogène grâce à des cartes conceptuelles interactives. Une carte est composée de concepts pertinents relativement à la requête initiale et à son évolution, au sein du corpus; des relations extraites du corpus les lient aux mots de la requête. Des techniques d’apprentissage automatique sont combinées avec des heuristiques statistiques de Traitement Automatique des Langues pour la mise en évidence de collocations afin de construire les cartes.</abstract>
      <url hash="f4b2afb7">2001.jeptalnrecital-poster.4</url>
      <language>fra</language>
      <bibkey>debourges-etal-2001-cartographie</bibkey>
    </paper>
    <paper id="5">
      <title>Un Modèle Cognitif pour la Résolution de la Référence dans le Dialogue Homme-Machine</title>
      <author><first>Sébastien</first><last>Gérard</last></author>
      <author><first>Jean</first><last>Paul Sansonnet</last></author>
      <pages>349–354</pages>
      <abstract>Dans cette étude, nous proposons un modèle pour la résolution de la référence dans le cadre du dialogue homme machine. Partant de considérations psychologiques sur la nécessité d’un partage du système inférenciel pour permettre la communication, nous définissons un alisme basé sur des règles de production associées à des coûts cognitifs. Au travers d’exemples, nous montrons comment ce formalisme peut être utilisé comme cadre pour intégrer le traitement de différents phénomènes liés à la référence, et comment cette tégration peut conduire à des interfaces en langue naturelle plus efficaces.</abstract>
      <url hash="1b48a0ce">2001.jeptalnrecital-poster.5</url>
      <language>fra</language>
      <bibkey>gerard-paul-sansonnet-2001-un</bibkey>
    </paper>
    <paper id="6">
      <title>Critères de sélection d’une approche pour le suivi automatique du courriel</title>
      <author><first>Leila</first><last>Kosseim</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <pages>355–361</pages>
      <abstract>Cet article discute de différentes approches pour faire le suivi automatique du courrier-électronique. Nous présentons tout d’abord les méthodes de traitement automatique de la langue (TAL) les plus utilisées pour cette tâche, puis un ensemble de critères influençant le choix d’une approche. Ces critères ont été développés grâce à une étude de cas sur un corpus fourni par Bell Canada Entreprises. Avec notre corpus, il est apparu que si aucune méthode n’est complètement satisfaisante par elle-même, une approche combinée semble beaucoup plus prometteuse.</abstract>
      <url hash="d383f10d">2001.jeptalnrecital-poster.6</url>
      <language>fra</language>
      <bibkey>kosseim-lapalme-2001-criteres</bibkey>
    </paper>
    <paper id="7">
      <title>Extraction de noms propres à partir de textes variés: problématique et enjeux</title>
      <author><first>Leila</first><last>Kosseim</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <pages>362–368</pages>
      <abstract>Cet article porte sur l’identification de noms propres à partir de textes écrits. Les stratégies à base de règles développées pour des textes de type journalistique se révèlent généralement insuffisantes pour des corpus composés de textes ne répondant pas à des critères rédactionnels stricts. Après une brève revue des travaux effectués sur des corpus de textes de nature journalistique, nous présentons la problématique de l’analyse de textes variés en nous basant sur deux corpus composés de courriers électroniques et de transcriptions manuelles de conversations téléphoniques. Une fois les sources d’erreurs présentées, nous décrivons l’approche utilisée pour adapter un système d’extraction de noms propres développé pour des textes journalistiques à l’analyse de messages électroniques.</abstract>
      <url hash="03df08df">2001.jeptalnrecital-poster.7</url>
      <language>fra</language>
      <bibkey>kosseim-poibeau-2001-extraction</bibkey>
    </paper>
    <paper id="8">
      <title>Défense et illustration de l’analogie</title>
      <author><first>Yves</first><last>Lepage</last></author>
      <pages>369–374</pages>
      <abstract>L’argumentation générativiste contre l’analogie tenait en trois points: l’hypothèse de l’inné, celle du hors-contexte et la surproduction. Des résultats théoriques et expérimen- taux reposant sur une formulation calculatoire nouvelle de l’analogie contribuent de façon constructive a la réfutation de ces points.</abstract>
      <url hash="c35cfb1c">2001.jeptalnrecital-poster.8</url>
      <language>fra</language>
      <bibkey>lepage-2001-defense</bibkey>
    </paper>
    <paper id="9">
      <title>Identification, interprétation et représentation de relations sémantiques entre concepts</title>
      <author><first>Florence</first><last>Le Priol</last></author>
      <pages>375–380</pages>
      <abstract>SEEK-JAVA est un système permettant Pidentification, l’interprétation et la représentation de connaissances à partir de textes. Il attribue une étiquette aux relations et identifie automatiquement les concepts arguments des relations. Les résultats, capitalisés dans une base de données, sont proposés, par le biais d’une interface, soit sous forme de graphes soit sous forme de tables. Ce système, intégré dans la plate-forme FilText, s’appuie sur la méthode d’ exploration contextuelle.</abstract>
      <url hash="5c78c260">2001.jeptalnrecital-poster.9</url>
      <language>fra</language>
      <bibkey>le-priol-2001-identification</bibkey>
    </paper>
    <paper id="10">
      <title>Gestionnaire de dialogue pour un système d’informations à reconnaissance vocale</title>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Lori</first><last>Lamel</last></author>
      <pages>381–386</pages>
      <abstract>Dans cet article, nous présentons un gestionnaire de dialogue pour un système de demande d’informations à reconnaissance vocale. Le gestionnaire de dialogue dispose de différentes sources de connaissance, des connaissances statiques et des connaissances dynamiques. Ces connaissances sont gérées et utilisées par le gestionnaire de dialogue via des stratégies. Elles sont mises en oeuvre et organisées en fonction des objectifs concernant le système de dialogue et en fonction des choix ergonomiques que nous avons retenus. Le gestionnaire de dialogue utilise un modèle de dialogue fondé sur la détermination de phases et un modèle de la tâche dynamique. Il augmente les possibilités d’adaptation de la stratégie en fonction des historiques et de l’état du dialogue. Ce gestionnaire de dialogue, implémenté et évalué lors de la dernière campagne d’évaluation du projet LE-3 ARISE, a permi une amélioration du taux de succès de dialogue (de 53% à 85%).</abstract>
      <url hash="168e9022">2001.jeptalnrecital-poster.10</url>
      <language>fra</language>
      <bibkey>rosset-lamel-2001-gestionnaire</bibkey>
    </paper>
    <paper id="11">
      <title>Algorithme de décodage de treillis selon le critère du coût moyen pour la reconnaissance de la parole</title>
      <author><first>Antoine</first><last>Rozenknop</last></author>
      <author><first>Marius</first><last>Silaghi</last></author>
      <pages>387–392</pages>
      <abstract>Les modèles de langage stochastiques utilisés pour la reconnaissance de la parole continue, ainsi que dans certains systèmes de traitement automatique de la langue, favorisent pour la plupart l’interprétation d’un signal par les phrases les plus courtes possibles, celles-ci étant par construction bien souvent affectées des coûts les plus bas. Cet article expose un algorithme permettant de répondre à ce problème en remplaçant le coût habituel affecté par le modèle de langage par sa moyenne sur la longueur de la phrase considérée. Cet algorithme est très général et peut être adapté aisément à de nombreux modèles de langage, y compris sur des tâches d’analyse syntaxique.</abstract>
      <url hash="dea0fba9">2001.jeptalnrecital-poster.11</url>
      <language>fra</language>
      <bibkey>rozenknop-silaghi-2001-algorithme</bibkey>
    </paper>
    <paper id="12">
      <title>Word Sense Disambiguation in a <fixed-case>S</fixed-case>panish Explanatory Dictionary</title>
      <author><first>Grigori</first><last>Sidorov</last></author>
      <author><first>Alexander</first><last>Gelbukh</last></author>
      <pages>393–398</pages>
      <abstract>We apply word sense disambiguation to the definitions in a Spanish explanatory dictionary. To calculate the scores of word senses basing on the context (which in our case is the dictionary definition), we use a modification of Lesk’s algorithm. The algorithm relies on a comparison between two words. In the original Lesk’s algorithm, the comparison is trivial: two words are either the same lexeme or not; our modification consists in fuzzy (weighted) comparison using a large synonym dictionary and a simple derivational morphology system. Application of disambiguation to dictionary definitions (in contrast to usual texts) allows for some simplifications of the algorithm, e.g., we do not have to care of context window size.</abstract>
      <url hash="b3ced2ab">2001.jeptalnrecital-poster.12</url>
      <bibkey>sidorov-gelbukh-2001-word</bibkey>
    </paper>
    <paper id="13">
      <title>L’apport de connaissances morphologiques pour la projection de requêtes sur une terminologie normalisée</title>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Stefan</first><last>Darmoni</last></author>
      <pages>399–404</pages>
      <abstract>L’apport de connaissances linguistiques à la recherche d’information reste un sujet de débat. Nous examinons ici l’influence de connaissances morphologiques (flexion, dérivation) sur les résultats d’une tâche spécifique de recherche d’information dans un domaine spécialisé. Cette influence est étudiée à l’aide d’une liste de requêtes réelles recueillies sur un serveur opérationnel ne disposant pas de connaissances linguistiques. Nous observons que pour cette tâche, flexion et dérivation apportent un gain modéré mais réel.</abstract>
      <url hash="8e2630af">2001.jeptalnrecital-poster.13</url>
      <language>fra</language>
      <bibkey>zweigenbaum-etal-2001-lapport</bibkey>
    </paper>
  </volume>
  <volume id="tutoriel" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 8ème conférence sur le Traitement Automatique des Langues Naturelles. Tutoriels</booktitle>
      <editor><first>Denis</first><last>Maurel</last></editor>
      <publisher>ATALA</publisher>
      <address>Tours, France</address>
      <month>July</month>
      <year>2001</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="aedb2db0">2001.jeptalnrecital-tutoriel.0</url>
      <bibkey>jep-taln-recital-2001-actes-de-la-8eme</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Extraction de collocations à partir de textes</title>
      <author><first>Béatrice</first><last>Daille</last></author>
      <pages>3–8</pages>
      <abstract>Les collocations sont intéressantes dans de nombreuses applications du TALN comme la l’analyse ou la génération de textes ou encore la lexicographie monolingue ou bilingue. Les premières tentatives d’extraction automatique de collocations à partir de textes ou de dictionnaires ont vu le jour dans les années 1970. Il s’agissait principalement de méthodes à base de statistiques lexicales. Aujourd’hui, les méthodes d’identification automatique font toujours appel à des statistiques mais qu’elles combinent avec des analyses linguistiques. Nous examinons quelques méthodes d’identification des collocations en corpus en soulignant pour chaque méthode les propriétés linguistiques des collocations qui ont été prises en compte.</abstract>
      <url hash="98c8901d">2001.jeptalnrecital-tutoriel.1</url>
      <language>fra</language>
      <bibkey>daille-2001-extraction</bibkey>
    </paper>
    <paper id="2">
      <title>Sur les caractéristiques de la collocation</title>
      <author><first>Geoffrey</first><last>Williams</last></author>
      <pages>9–16</pages>
      <abstract>Le terme “collocation “a été introduit dans les années ’30 par J. R. F irth, membre-fondateur de l’école contextualiste britannique, pour caractériser certains phénomènes linguistiques de cooccurrence. Ce phénomène est maintenant accepté comme central dans la compétence linguistique des locuteurs natifs et de grande importance pour l enseignement, la traduction, la lexicographie, et dorénavant, le TALN. Malheureusement, le concept est difiicile a formaliser et ne peut être étudié que par rapport a des exemples prototypiques. Quatre caractéristiques sont analysées, leur nature habituelle, lexicalement transparente, arbitraire et syntactiquement bien formée. Les avantages et inconvénients de chaque critère sont discutés.</abstract>
      <url hash="9fc271d4">2001.jeptalnrecital-tutoriel.2</url>
      <language>fra</language>
      <bibkey>williams-2001-sur</bibkey>
    </paper>
    <paper id="3">
      <title>Grammaires de dpendance formelles et thorie Sens-Texte</title>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <pages>17–76</pages>
      <abstract>On appelle grammaire de dpendance toute grammaire formelle qui manipule comme reprsentations syntaxiques des structures de dpendance. Le but de ce cours est de prsenter  la fois les grammaires de dpendance (formalismes et algorithmes de synthse et dÕanalyse) et la thorie Sens-Texte, une thorie linguistique riche et pourtant mconnue, dans laquelle la dpendance joue un rle crucial et qui sert de base thorique  plusieurs grammaires de dpendance.</abstract>
      <url hash="6fea8f1f">2001.jeptalnrecital-tutoriel.3</url>
      <language>fra</language>
      <bibkey>kahane-2001-grammaires</bibkey>
    </paper>
    <paper id="4">
      <title>Formal Languages for Linguists: Classical and Nonclassical Models</title>
      <author><first>Carlos</first><last>Martín-Vide</last></author>
      <pages>77–127</pages>
      <abstract/>
      <url hash="1c7a8b1d">2001.jeptalnrecital-tutoriel.4</url>
      <language>fra</language>
      <bibkey>martin-vide-2001-formal</bibkey>
    </paper>
    <paper id="5">
      <title>L’apport de connaissances linguistiques en recherche documentaire</title>
      <author><first>Claude</first><last>De Loupy</last></author>
      <pages>128–142</pages>
      <abstract>L’utilisation de connaissances et de traitements linguistiques évolués en recherche documentaire ne fait pas l’unanimité dans le milieu scientifique. En effet, de nombreuses expériences semblent montrer que les résultats obtenus ne sont pas améliorés, voire sont parfois dégradés, lorsque de telles connaissances sont utilisées dans un système de RD. Dans ce tutoriel, nous montrons que les environnements d’évaluation ne sont pas adaptés aux besoins réels d’un utilisateur car celui-ci recherche presque toujours une information. Il veut donc retrouver des documents pertinents le plus rapidement possible car ce n’est pas là le but de sa recherche. Le temps global de la recherche est donc fondamentalement important. Néanmoins, le cadre d’évaluation TREC nous permet de montrer que l’utilisation de connaissances linguistiques permet d’augmenter la précision des premiers documents renvoyés, ce qui est très important pour diminuer le temps de recherche.</abstract>
      <url hash="a9e0e1a7">2001.jeptalnrecital-tutoriel.5</url>
      <language>fra</language>
      <bibkey>de-loupy-2001-lapport</bibkey>
    </paper>
    <paper id="6">
      <title>Intex et ses applications informatiques</title>
      <author><first>Max</first><last>Silberztein</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <author><first>Antonio</first><last>Balvet</last></author>
      <pages>143–172</pages>
      <abstract>Intex est un environnement de développement utilisé pour construire, tester et accumuler rapidement des motifs morpho-syntaxiques qui apparaissent dans des textes écrits en langue naturelle. Un survol du système est présenté dans [Silberztein, 1999] , le manuel d’instruction est disponible [Silberztein 2000]. Chaque description élémentaire est représentée par une grammaire locale, qui est habituellement entrée en machine grâce à l’éditeur de graphe d’Intex. Une caractéristique importante d’Intex est que chaque grammaire locale peut être facilement réemployée dans d’autres grammaires locales. Typiquement, les développeurs construisent des graphes élémentaires qui sont équivalents à des transducteurs à états finis, et réemploient ces graphes dans d’autres graphes de plus en plus complexes. Une seconde caractéristique d’Intex est que les objets traités (grammaires, dictionnaires et textes) sont représentés de façon interne par des transducteurs à états finis. En conséquence, toutes les fonctionnalités du système se ramènent à un nombre limité d’opérations sur des transducteurs. Par exemple, appliquer une grammaire à un texte revient à construire l’union des transducteurs élémentaires, la déterminiser, puis à calculer l’intersection du résultat avec le transducteur du texte. Cette architecture permet d’utiliser des algorithmes efficaces (par ex. lorsqu’on applique un transducteur déterministe à un texte préalablement indexé), et donne à Intex la puissance d’une machine de Turing (grâce à la possibilité d’appliquer des transducteurs en cascade). Dans ce tutoriel, nous montrerons comment utiliser un outil linguistique tel qu’Intex dans des environnements informatiques. Nous nous appuierons sur des applications de filtrage et d’extraction d’information, réalisées notamment au centre de recherche de Thales. Les applications suivantes seront détaillées, tant sur le plan linguistique qu’informatique filtrage d’information a partir d’un flux AFP [Meunier et al. l999] extraction de tables d’interaction entre gènes à partir de bases de données textuelles en génomique. [Poibeau 2001] Le tutoriel montrera comment Intex peut être employé comme moteur de filtrage d’un flux de dépêches de type AFP dans un cadre industriel. Il détaillera également les fonctionnalités de transformations des textes (transduction) permettant de passer rapidement de structures linguistiques variées à des formes normalisées permettant de remplir une base de données. Sur le plan informatique, on détaillera l’appel aux routines Intex, les paramétrages possibles (découpage en phrases, choix des dictionnaires...), et on survolera les nouvelles possibilités d’intégration (Intex API).</abstract>
      <url hash="3229545a">2001.jeptalnrecital-tutoriel.6</url>
      <language>fra</language>
      <bibkey>silberztein-etal-2001-intex</bibkey>
    </paper>
  </volume>
  <volume id="recital" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 8ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues</booktitle>
      <editor><first>Béatrice</first><last>Bouchou</last></editor>
      <publisher>ATALA</publisher>
      <address>Tours, France</address>
      <month>July</month>
      <year>2001</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="cdf07b54">2001.jeptalnrecital-recital.0</url>
      <bibkey>jep-taln-recital-2001-actes-de-la-8eme-sur</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>L</fixed-case>o<fixed-case>X</fixed-case> : outil polyvalent pour l’exploration de corpus annotés</title>
      <author><first>Laurent</first><last>Audibert</last></author>
      <pages>405–413</pages>
      <abstract>Cet article présente une application permettant d’écrire des requêtes complexes sur des corpus étiquetés et de formater librement les résultats de ces requêtes. Le formalisme des requêtes est basé sur le principe des expressions régulières bien connu de la plupart des linguistes travaillant sur des corpus écrits. Contrairement à certains logiciels, qui ne permettent que l’extraction de concordances au format relativement figé, le formatage libre du résultat des requêtes permet leur réutilisation par des programmes ultérieurs et autorise une grande diversité d’applications, s’écartant largement du cadre des simples concordanciers.</abstract>
      <url hash="942e8a01">2001.jeptalnrecital-recital.1</url>
      <language>fra</language>
      <bibkey>audibert-2001-lox</bibkey>
    </paper>
    <paper id="2">
      <title>Filtrage d’information par analyse partielle Grammaires locales, dictionnaires électroniques et lexique- grammaire pour la recherche d’information</title>
      <author><first>Antonio</first><last>Balvet</last></author>
      <pages>414–423</pages>
      <abstract>Nous présentons une approche de filtrage d’information par analyse partielle, reprenant les résultats de recherches issues aussi bien de la recherche documentaire que du traitement automatique des langues. Nous précisons les contraintes liées au domaine du filtrage d’information qui militent, à nos yeux, pour une approche linguistique permettant d’obtenir des performances importantes, ainsi qu’une transparence de fonctionnement. Nous présentons quelques résultats concrets pour illustrer le potentiel de l’approche décrite.</abstract>
      <url hash="49b0b74d">2001.jeptalnrecital-recital.2</url>
      <language>fra</language>
      <bibkey>balvet-2001-filtrage</bibkey>
    </paper>
    <paper id="3">
      <title>Bibliothèques d’automates finis et grammaires context-free : de nouveaux traitements informatiques</title>
      <author><first>Matthieu</first><last>Constant</last></author>
      <pages>424–433</pages>
      <abstract>La quantité de documents disponibles via Internet explose. Cette situation nous incite à rechercher de nouveaux outils de localisation d’information dans des documents et, en particulier, à nous pencher sur l’algorithmique des grammaires context-free appliquée à des familles de graphes d’automates finis (strictement finis ou à cycles). Nous envisageons une nouvelle représentation et de nouveaux traitements informatiques sur ces grammaires, afin d’assurer un accès rapide aux données et un stockage peu coûteux en mémoire.</abstract>
      <url hash="3081d62e">2001.jeptalnrecital-recital.3</url>
      <language>fra</language>
      <bibkey>constant-2001-bibliotheques</bibkey>
    </paper>
    <paper id="4">
      <title>Identification et catégorisation automatiques des anthroponymes du Français</title>
      <author><first>Nordine</first><last>Fourour</last></author>
      <pages>434–443</pages>
      <abstract>Cet article préente un système de reconnaissance des noms propres pour le Français. Les spécifications de ce système ont été réalisées à la suite d’une étude en corpus et s’appuient sur des critères graphiques et référentiels. Les critères graphiques permettent de concevoir les traitements à mettre en place pour la délimitation des noms propres et la catégorisation repose sur les critères référentiels. Le système se base sur des règles de grammaire, exploite des lexiques spécialisés et comporte un module d’apprentissage. Les performances atteintes par le système, sur les anthroponymes, sont de 89,4% pour le rappel et 94,6% pour la précision.</abstract>
      <url hash="0b66ea8e">2001.jeptalnrecital-recital.4</url>
      <language>fra</language>
      <bibkey>fourour-2001-identification</bibkey>
    </paper>
    <paper id="5">
      <title>Pour un autre traitement de la temporalité narrative</title>
      <author><first>Stéphanie</first><last>Girault</last></author>
      <pages>444–453</pages>
      <abstract>Tous les médias continus (parole, texte, musique, cinéma) ont, par définition, une structure linéaire, à partir de laquelle un processus cognitif est capable de reconstituer une organisation temporelle différente. Mais jusqu’à quel point faut-il comprendre un texte pour le segmenter en situations et les articuler entre elles ? Autrement dit : jusqu’à quel point faut-il connaître la musique pour différencier couplet et refrain ? Dans un grand nombre de cas, il est possible d’effectuer une telle segmentation automatiquement, et cela uniquement à partir d’indices morpho-syntaxiques. Notre prototype de programme identifie des situations référentielles et analyse la façon dont elles sont articulées pour reconstruire la structure temporelle d’un récit. L’objectif de cette communication n’est pas la description de ce programme, mais plutôt le point de vue du linguiste : comment détecter les discontinuités, c’est-à-dire comment décider s’il y a complétion ou rupture.</abstract>
      <url hash="0c320d9b">2001.jeptalnrecital-recital.5</url>
      <language>fra</language>
      <bibkey>girault-2001-pour</bibkey>
    </paper>
    <paper id="6">
      <title>Analyse sémantique dans un système de question-réponse</title>
      <author><first>Laura</first><last>Monceaux</last></author>
      <pages>454–461</pages>
      <abstract>Dans cet article, nous présentons le système QALC (Question Answering Langage Cognition) qui a participé à la tâche Question Réponse de la conférence d’évaluation TREC. Ce système a pour but d’extraire la réponse à une question d’une grande masse de documents. Afin d’améliorer les résultats de notre système, nous avons réfléchi à la nécessité de développer, dans le module d’analyse, le typage des questions mais aussi d’introduire des connaissances syntaxico-sémantiques pour une meilleure recherche de la réponse.</abstract>
      <url hash="512c77fb">2001.jeptalnrecital-recital.6</url>
      <language>fra</language>
      <bibkey>monceaux-2001-analyse</bibkey>
    </paper>
    <paper id="7">
      <title>La recherche documentaire : une activité langagière</title>
      <author><first>Vincent</first><last>Perlerin</last></author>
      <pages>462–471</pages>
      <abstract>Un nombre important de requêtes soumises aux moteurs de recherche du W3 ne satisfont pas pleinement les attentes des utilisateurs. La liste de documents proposée en retour est souvent trop longue : son exploration représente un travail exagérément laborieux pour l’auteur de la requête. Nous proposons d’apporter une valeur ajoutée aux systèmes de recherche documentaire (RD) existants en y ajoutant un filtrage n’utilisant que des données fournies par l’utilisateur. L’objectif de notre étude est de confronter un modèle dynamique de la mémoire sémantique des individus (ou des agents) développé par notre équipe à une tâche nécessitant une compétence interprétative de la part des machines. Nous souhaitons dépasser la sémantique lexicale couramment utilisée dans ce champ d’application pour aboutir à l’utilisation d’une sémantique des textes et accroître par ce biais, à la fois la qualité des résultats et la qualité de leur présentation aux usagers.</abstract>
      <url hash="f24ae909">2001.jeptalnrecital-recital.7</url>
      <language>fra</language>
      <bibkey>perlerin-2001-la</bibkey>
    </paper>
    <paper id="8">
      <title>Dictionnaires distributionnels et étiquetage lexical de corpus</title>
      <author><first>Delphine</first><last>Reymond</last></author>
      <pages>472–481</pages>
      <abstract>Ce papier présente la première partie d’un travail de thèse qui vise à construire un « dictionnaire distributionnel » à partir d’un corpus de référence. Le dictionnaire proposé est basé sur un ensemble de critères différentiels stricts qui constituent des indices exploitables par des machines pour discriminer le sens des mots en contexte. Pour l’instant, le travail a porté sur 50 000 occurrences qui ont été étiquetées de façon manuelle. Ce sous-corpus pourra servir de corpus d’amorçage pour la constitution d’un corpus étiqueté plus grand, qui pourrait servir à différents tests et travaux sur la désambiguïsation automatique.</abstract>
      <url hash="f911d25e">2001.jeptalnrecital-recital.8</url>
      <language>fra</language>
      <bibkey>reymond-2001-dictionnaires</bibkey>
    </paper>
  </volume>
  <volume id="recitalposter" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 8ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues (Posters)</booktitle>
      <editor><first>Béatrice</first><last>Bouchou</last></editor>
      <publisher>ATALA</publisher>
      <address>Tours, France</address>
      <month>July</month>
      <year>2001</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="503bb95e">2001.jeptalnrecital-recitalposter.0</url>
      <bibkey>jep-taln-recital-2001-actes-de-la-8eme-sur-le</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Du texte vers le sens en analyse par contraintes</title>
      <author><first>Francis</first><last>Brunet-Manquat</last></author>
      <pages>485–490</pages>
      <abstract>Les progrès réalisés ces dernières années dans le domaine du traitement automatique des langues naturelles (TALN) ouvrent la voie à des traitements encore plus sophistiqués dans lesquels la sémantique devrait tenir une place centrale. Notre objectif, à long terme, est de réaliser un analyseur texte vers sens s’appuyant sur la théorie Sens-Texte d’Igor Mel’cuk. Cette analyse viserait une compréhension plus approfondie du texte, permettant donc d’atteindre une représentation de niveau sémantique, et une grande robustesse face à des entrées plus ou moins bien formées telles que celles issues de dialogues oraux. Mais renverser la théorie Sens-Texte passe par la définition et la mise en oeuvre de structures de données et d’algorithmes spécifiques pour la représentation et la manipulation automatique des informations linguistiques, notamment des entrées lexicales. Pour cela, nous proposons l’utilisation du paradigme de programmation par contraintes qui offre un moyen efficace d’atteindre nos objectifs.</abstract>
      <url hash="67100811">2001.jeptalnrecital-recitalposter.1</url>
      <language>fra</language>
      <bibkey>brunet-manquat-2001-du</bibkey>
    </paper>
    <paper id="2">
      <title>Outils d’assistance à la construction de Webs personnels : Utilisation des traitements des langues naturelles dans l’aide à la reformulation de requêtes</title>
      <author><first>Mohamed</first><last>Yassine El Amrani</last></author>
      <pages>491–496</pages>
      <abstract>Nous présentons dans cet article le projet au sein duquel nous développons un logiciel permettant d’assister l’utilisateur lors de la formulation de sa requête de recherche sur le Web et de personnaliser des sous-ensembles du Web selon ses besoins informationnels. L’architecture du logiciel est basée sur l’intégration de plusieurs outils numériques et linguistiques de traitements des langues naturelles (TALN). Le logiciel utilise une stratégie semi-automatique où la contribution de l’utilisateur assure la concordance entre ses attentes et les résultats obtenus. Ces résultats sont stockés dans diverses bases de données permettant de conserver différents types d’informations (classes de sites/pages Web similaires, profils de l’usager, lexiques, etc.) constituant une projection locale et personnalisée du Web.</abstract>
      <url hash="eaadcba6">2001.jeptalnrecital-recitalposter.2</url>
      <language>fra</language>
      <bibkey>yassine-el-amrani-2001-outils</bibkey>
    </paper>
    <paper id="3">
      <title>Extraction d’information de documents textuels associés à des contenus audiovisuels</title>
      <author><first>Estelle</first><last>Le Roux</last></author>
      <pages>497–502</pages>
      <abstract>L’indexation audiovisuelle, indispensable pour l’archivage et l’exploitation des documents, se révèle être un processus délicat, notamment à cause de la multiplicité de significations qui peuvent être attachées aux images. Nous proposons dans cette communication une méthode d’instanciation de ”patrons d’indexation” à partir d’un corpus d’articles de journaux écrits. Cette méthode repose sur un processus ”d’amorçage hiérachisé”, qui permet de trouver de nouveaux termes à partir de termes connus dans leur voisinage et de leurs relations taxinomiques sous forme d’ontologie.</abstract>
      <url hash="7665503d">2001.jeptalnrecital-recitalposter.3</url>
      <language>fra</language>
      <bibkey>le-roux-2001-extraction</bibkey>
    </paper>
  </volume>
</collection>
