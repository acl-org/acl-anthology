<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.nlpcss">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science</booktitle>
      <editor><first>David</first><last>Bamman</last></editor>
      <editor><first>Dirk</first><last>Hovy</last></editor>
      <editor><first>David</first><last>Jurgens</last></editor>
      <editor><first>Brendan</first><last>O'Connor</last></editor>
      <editor><first>Svitlana</first><last>Volkova</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="f83c4f5a">2020.nlpcss-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Measuring Linguistic Diversity During <fixed-case>COVID</fixed-case>-19</title>
      <author><first>Jonathan</first><last>Dunn</last></author>
      <author><first>Tom</first><last>Coupe</last></author>
      <author><first>Benjamin</first><last>Adams</last></author>
      <pages>1–10</pages>
      <abstract>Computational measures of linguistic diversity help us understand the linguistic landscape using digital language data. The contribution of this paper is to calibrate measures of linguistic diversity using restrictions on international travel resulting from the COVID-19 pandemic. Previous work has mapped the distribution of languages using geo-referenced social media and web data. The goal, however, has been to describe these corpora themselves rather than to make inferences about underlying populations. This paper shows that a difference-in-differences method based on the Herfindahl-Hirschman Index can identify the bias in digital corpora that is introduced by non-local populations. These methods tell us where significant changes have taken place and whether this leads to increased or decreased diversity. This is an important step in aligning digital corpora like social media with the real-world populations that have produced them.</abstract>
      <url hash="5c09c398">2020.nlpcss-1.1</url>
      <attachment type="OptionalSupplementaryMaterial" hash="da9c4005">2020.nlpcss-1.1.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.nlpcss-1.1</doi>
    </paper>
    <paper id="2">
      <title>Using <fixed-case>BERT</fixed-case> for Qualitative Content Analysis in Psychosocial Online Counseling</title>
      <author><first>Philipp</first><last>Grandeit</last></author>
      <author><first>Carolyn</first><last>Haberkern</last></author>
      <author><first>Maximiliane</first><last>Lang</last></author>
      <author><first>Jens</first><last>Albrecht</last></author>
      <author><first>Robert</first><last>Lehmann</last></author>
      <pages>11–23</pages>
      <abstract>Qualitative content analysis is a systematic method commonly used in the social sciences to analyze textual data from interviews or online discussions. However, this method usually requires high expertise and manual effort because human coders need to read, interpret, and manually annotate text passages. This is especially true if the system of categories used for annotation is complex and semantically rich. Therefore, qualitative content analysis could benefit greatly from automated coding. In this work, we investigate the usage of machine learning-based text classification models for automatic coding in the area of psycho-social online counseling. We developed a system of over 50 categories to analyze counseling conversations, labeled over 10.000 text passages manually, and evaluated the performance of different machine learning-based classifiers against human coders.</abstract>
      <url hash="29222607">2020.nlpcss-1.2</url>
      <doi>10.18653/v1/2020.nlpcss-1.2</doi>
    </paper>
    <paper id="3">
      <title>Swimming with the Tide? Positional Claim Detection across Political Text Types</title>
      <author><first>Nico</first><last>Blokker</last></author>
      <author><first>Erenay</first><last>Dayanik</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>24–34</pages>
      <abstract>Manifestos are official documents of political parties, providing a comprehensive topical overview of the electoral programs. Voters, however, seldom read them and often prefer other channels, such as newspaper articles, to understand the party positions on various policy issues. The natural question to ask is how compatible these two formats (manifesto and newspaper reports) are in their representation of party positioning. We address this question with an approach that combines political science (manual annotation and analysis) and natural language processing (supervised claim identification) in a cross-text type setting: we train a classifier on annotated newspaper data and test its performance on manifestos. Our findings show a) strong performance for supervised classification even across text types and b) a substantive overlap between the two formats in terms of party positioning, with differences regarding the salience of specific issues.</abstract>
      <url hash="c3816eb8">2020.nlpcss-1.3</url>
      <doi>10.18653/v1/2020.nlpcss-1.3</doi>
    </paper>
    <paper id="4">
      <title>Does Social Support (Expressed in Post Titles) Elicit Comments in Online Substance Use Recovery Forums?</title>
      <author><first>Anietie</first><last>Andy</last></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last></author>
      <pages>35–40</pages>
      <abstract>Individuals recovering from substance use often seek social support (emotional and informational) on online recovery forums, where they can both write and comment on posts, expressing their struggles and successes. A common challenge in these forums is that certain posts (some of which may be support seeking) receive no comments. In this work, we use data from two Reddit substance recovery forums: /r/Leaves and /r/OpiatesRecovery, to determine the relationship between the social supports expressed in the titles of posts and the number of comments they receive. We show that the types of social support expressed in post titles that elicit comments vary from one substance use recovery forum to the other.</abstract>
      <url hash="286d4acc">2020.nlpcss-1.4</url>
      <doi>10.18653/v1/2020.nlpcss-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>I</fixed-case> miss you babe: Analyzing Emotion Dynamics During <fixed-case>COVID</fixed-case>-19 Pandemic</title>
      <author><first>Hui Xian Lynnette</first><last>Ng</last></author>
      <author><first>Roy Ka-Wei</first><last>Lee</last></author>
      <author><first>Md Rabiul</first><last>Awal</last></author>
      <pages>41–49</pages>
      <abstract>With the world on a lockdown due to the COVID-19 pandemic, this paper studies emotions expressed on Twitter. Using a combined strategy of time series analysis of emotions augmented by tweet topics, this study provides an insight into emotion transitions during the pandemic. After tweets are annotated with dominant emotions and topics, a time-series emotion analysis is used to identify disgust and anger as the most commonly identified emotions. Through longitudinal analysis of each user, we construct emotion transition graphs, observing key transitions between disgust and anger, and self-transitions within anger and disgust emotional states. Observing user patterns through clustering of user longitudinal analyses reveals emotional transitions fall into four main clusters: (1) erratic motion over short period of time, (2) disgust -&gt; anger, (3) optimism -&gt; joy. (4) erratic motion over a prolonged period. Finally, we propose a method for predicting users subsequent topic, and by consequence their emotions, through constructing an Emotion Topic Hidden Markov Model, augmenting emotion transition states with topic information. Results suggests that the predictions fare better than baselines, spurring directions of predicting emotional states based on Twitter posts.</abstract>
      <url hash="2c6b77a9">2020.nlpcss-1.5</url>
      <doi>10.18653/v1/2020.nlpcss-1.5</doi>
    </paper>
    <paper id="6">
      <title>Assessing population-level symptoms of anxiety, depression, and suicide risk in real time using <fixed-case>NLP</fixed-case> applied to social media data</title>
      <author><first>Alex</first><last>Fine</last></author>
      <author><first>Patrick</first><last>Crutchley</last></author>
      <author><first>Jenny</first><last>Blase</last></author>
      <author><first>Joshua</first><last>Carroll</last></author>
      <author><first>Glen</first><last>Coppersmith</last></author>
      <pages>50–54</pages>
      <abstract>Prevailing methods for assessing population-level mental health require costly collection of large samples of data through instruments such as surveys, and are thus slow to reflect current, rapidly changing social conditions. This constrains how easily population-level mental health data can be integrated into health and policy decision-making. Here, we demonstrate that natural language processing applied to publicly-available social media data can provide real-time estimates of psychological distress in the population (specifically, English-speaking Twitter users in the US). We examine population-level changes in linguistic correlates of mental health symptoms in response to the COVID-19 pandemic and to the killing of George Floyd. As a case study, we focus on social media data from healthcare providers, compared to a control sample. Our results provide a concrete demonstration of how the tools of computational social science can be applied to provide real-time or near-real-time insight into the impact of public events on mental health.</abstract>
      <url hash="7dd8ac74">2020.nlpcss-1.6</url>
      <doi>10.18653/v1/2020.nlpcss-1.6</doi>
    </paper>
    <paper id="7">
      <title>Viable Threat on News Reading: Generating Biased News Using Natural Language Models</title>
      <author><first>Saurabh</first><last>Gupta</last></author>
      <author><first>Hong Huy</first><last>Nguyen</last></author>
      <author><first>Junichi</first><last>Yamagishi</last></author>
      <author><first>Isao</first><last>Echizen</last></author>
      <pages>55–65</pages>
      <abstract>Recent advancements in natural language generation has raised serious concerns. High-performance language models are widely used for language generation tasks because they are able to produce fluent and meaningful sentences. These models are already being used to create fake news. They can also be exploited to generate biased news, which can then be used to attack news aggregators to change their reader’s behavior and influence their bias. In this paper, we use a threat model to demonstrate that the publicly available language models can reliably generate biased news content based on an input original news. We also show that a large number of high-quality biased news articles can be generated using controllable text generation. A subjective evaluation with 80 participants demonstrated that the generated biased news is generally fluent, and a bias evaluation with 24 participants demonstrated that the bias (left or right) is usually evident in the generated articles and can be easily identified.</abstract>
      <url hash="5dc1b7f6">2020.nlpcss-1.7</url>
      <doi>10.18653/v1/2020.nlpcss-1.7</doi>
    </paper>
    <paper id="8">
      <title>Unsupervised Anomaly Detection in Parole Hearings using Language Models</title>
      <author><first>Graham</first><last>Todd</last></author>
      <author><first>Catalin</first><last>Voss</last></author>
      <author><first>Jenny</first><last>Hong</last></author>
      <pages>66–71</pages>
      <abstract>Each year, thousands of roughly 150-page parole hearing transcripts in California go unread because legal experts lack the time to review them. Yet, reviewing transcripts is the only means of public oversight in the parole process. To assist reviewers, we present a simple unsupervised technique for using language models (LMs) to identify procedural anomalies in long-form legal text. Our technique highlights unusual passages that suggest further review could be necessary. We utilize a contrastive perplexity score to identify passages, defined as the scaled difference between its perplexities from two LMs, one fine-tuned on the target (parole) domain, and another pre-trained on out-of-domain text to normalize for grammatical or syntactic anomalies. We present quantitative analysis of the results and note that our method has identified some important cases for review. We are also excited about potential applications in unsupervised anomaly detection, and present a brief analysis of results for detecting fake TripAdvisor reviews.</abstract>
      <url hash="8784d02d">2020.nlpcss-1.8</url>
      <attachment type="OptionalSupplementaryMaterial" hash="9c891991">2020.nlpcss-1.8.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.nlpcss-1.8</doi>
    </paper>
    <paper id="9">
      <title>Identifying Worry in <fixed-case>T</fixed-case>witter: Beyond Emotion Analysis</title>
      <author><first>Reyha</first><last>Verma</last></author>
      <author><first>Christian</first><last>von der Weth</last></author>
      <author><first>Jithin</first><last>Vachery</last></author>
      <author><first>Mohan</first><last>Kankanhalli</last></author>
      <pages>72–82</pages>
      <abstract>Identifying the worries of individuals and societies plays a crucial role in providing social support and enhancing policy decision-making. Due to the popularity of social media platforms such as Twitter, users share worries about personal issues (e.g., health, finances, relationships) and broader issues (e.g., changes in society, environmental concerns, terrorism) freely. In this paper, we explore and evaluate a wide range of machine learning models to predict worry on Twitter. While this task has been closely associated with emotion prediction, we argue and show that identifying worry needs to be addressed as a separate task given the unique challenges associated with it. We conduct a user study to provide evidence that social media posts express two basic kinds of worry – normative and pathological – as stated in psychology literature. In addition, we show that existing emotion detection techniques underperform, especially while capturing normative worry. Finally, we discuss the current limitations of our approach and propose future applications of the worry identification system.</abstract>
      <url hash="148e3b48">2020.nlpcss-1.9</url>
      <doi>10.18653/v1/2020.nlpcss-1.9</doi>
    </paper>
    <paper id="10">
      <title>Text Zoning and Classification for Job Advertisements in <fixed-case>G</fixed-case>erman, <fixed-case>F</fixed-case>rench and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Ann-Sophie</first><last>Gnehm</last></author>
      <author><first>Simon</first><last>Clematide</last></author>
      <pages>83–93</pages>
      <abstract>We present experiments to structure job ads into text zones and classify them into pro- fessions, industries and management functions, thereby facilitating social science analyses on labor marked demand. Our main contribution are empirical findings on the benefits of contextualized embeddings and the potential of multi-task models for this purpose. With contextualized in-domain embeddings in BiLSTM-CRF models, we reach an accuracy of 91% for token-level text zoning and outperform previous approaches. A multi-tasking BERT model performs well for our classification tasks. We further compare transfer approaches for our multilingual data.</abstract>
      <url hash="d4e64389">2020.nlpcss-1.10</url>
      <attachment type="OptionalSupplementaryMaterial" hash="9bf96bbd">2020.nlpcss-1.10.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.nlpcss-1.10</doi>
    </paper>
    <paper id="11">
      <title>Is <fixed-case>W</fixed-case>ikipedia succeeding in reducing gender bias? Assessing changes in gender bias in <fixed-case>W</fixed-case>ikipedia using word embeddings</title>
      <author><first>Katja Geertruida</first><last>Schmahl</last></author>
      <author><first>Tom Julian</first><last>Viering</last></author>
      <author><first>Stavros</first><last>Makrodimitris</last></author>
      <author><first>Arman</first><last>Naseri Jahfari</last></author>
      <author><first>David</first><last>Tax</last></author>
      <author><first>Marco</first><last>Loog</last></author>
      <pages>94–103</pages>
      <abstract>Large text corpora used for creating word embeddings (vectors which represent word meanings) often contain stereotypical gender biases. As a result, such unwanted biases will typically also be present in word embeddings derived from such corpora and downstream applications in the field of natural language processing (NLP). To minimize the effect of gender bias in these settings, more insight is needed when it comes to where and how biases manifest themselves in the text corpora employed. This paper contributes by showing how gender bias in word embeddings from Wikipedia has developed over time. Quantifying the gender bias over time shows that art related words have become more female biased. Family and science words have stereotypical biases towards respectively female and male words. These biases seem to have decreased since 2006, but these changes are not more extreme than those seen in random sets of words. Career related words are more strongly associated with male than with female, this difference has only become smaller in recently written articles. These developments provide additional understanding of what can be done to make Wikipedia more gender neutral and how important time of writing can be when considering biases in word embeddings trained from Wikipedia or from other text corpora.</abstract>
      <url hash="5f5dff15">2020.nlpcss-1.11</url>
      <doi>10.18653/v1/2020.nlpcss-1.11</doi>
    </paper>
    <paper id="12">
      <title>Effects of Anonymity on Comment Persuasiveness in <fixed-case>W</fixed-case>ikipedia Articles for Deletion Discussions</title>
      <author><first>Yimin</first><last>Xiao</last></author>
      <author><first>Lu</first><last>Xiao</last></author>
      <pages>104–115</pages>
      <abstract>It has been shown that anonymity affects various aspects of online communications such as message credibility, the trust among communicators, and the participants’ accountability and reputation. Anonymity influences social interactions in online communities in these many ways, which can lead to influences on opinion change and the persuasiveness of a message. Prior studies also suggest that the effect of anonymity can vary in different online communication contexts and online communities. In this study, we focus on Wikipedia Articles for Deletion (AfD) discussions as an example of online collaborative communities to study the relationship between anonymity and persuasiveness in this context. We find that in Wikipedia AfD discussions, more identifiable users tend to be more persuasive. The higher persuasiveness can be related to multiple aspects, including linguistic features of the comments, the user’s motivation to participate, persuasive skills the user learns over time, and the user’s identity and credibility established in the community through participation.</abstract>
      <url hash="c3ce6610">2020.nlpcss-1.12</url>
      <doi>10.18653/v1/2020.nlpcss-1.12</doi>
    </paper>
    <paper id="13">
      <title>Uncertainty over Uncertainty: Investigating the Assumptions, Annotations, and Text Measurements of Economic Policy Uncertainty</title>
      <author><first>Katherine</first><last>Keith</last></author>
      <author><first>Christoph</first><last>Teichmann</last></author>
      <author><first>Brendan</first><last>O’Connor</last></author>
      <author><first>Edgar</first><last>Meij</last></author>
      <pages>116–131</pages>
      <abstract>Methods and applications are inextricably linked in science, and in particular in the domain of text-as-data. In this paper, we examine one such text-as-data application, an established economic index that measures economic policy uncertainty from keyword occurrences in news. This index, which is shown to correlate with firm investment, employment, and excess market returns, has had substantive impact in both the private sector and academia. Yet, as we revisit and extend the original authors’ annotations and text measurements we find interesting text-as-data methodological research questions: (1) Are annotator disagreements a reflection of ambiguity in language? (2) Do alternative text measurements correlate with one another and with measures of external predictive validity? We find for this application (1) some annotator disagreements of economic policy uncertainty can be attributed to ambiguity in language, and (2) switching measurements from keyword-matching to supervised machine learning classifiers results in low correlation, a concerning implication for the validity of the index.</abstract>
      <url hash="b0acedb9">2020.nlpcss-1.13</url>
      <doi>10.18653/v1/2020.nlpcss-1.13</doi>
    </paper>
    <paper id="14">
      <title>Recalibrating classifiers for interpretable abusive content detection</title>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <author><first>Scott</first><last>Hale</last></author>
      <author><first>Sam</first><last>Staton</last></author>
      <author><first>Tom</first><last>Melham</last></author>
      <author><first>Helen</first><last>Margetts</last></author>
      <author><first>Ohad</first><last>Kammar</last></author>
      <author><first>Marcin</first><last>Szymczak</last></author>
      <pages>132–138</pages>
      <abstract>We investigate the use of machine learning classifiers for detecting online abuse in empirical research. We show that uncalibrated classifiers (i.e. where the ‘raw’ scores are used) align poorly with human evaluations. This limits their use for understanding the dynamics, patterns and prevalence of online abuse. We examine two widely used classifiers (created by Perspective and Davidson et al.) on a dataset of tweets directed against candidates in the UK’s 2017 general election. A Bayesian approach is presented to recalibrate the raw scores from the classifiers, using probabilistic programming and newly annotated data. We argue that interpretability evaluation and recalibration is integral to the application of abusive content classifiers.</abstract>
      <url hash="d914fd9c">2020.nlpcss-1.14</url>
      <attachment type="OptionalSupplementaryMaterial" hash="4a119199">2020.nlpcss-1.14.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.nlpcss-1.14</doi>
    </paper>
    <paper id="15">
      <title>Predicting independent living outcomes from written reports of social workers</title>
      <author><first>Angelika</first><last>Maier</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <pages>139–148</pages>
      <abstract>In social care environments, the main goal of social workers is to foster independent living by their clients. An important task is thus to monitor progress towards reaching independence in different areas of their patients’ life. To support this task, we present an approach that extracts indications of independence on different life aspects from the day-to-day documentation that social workers create. We describe the process of collecting and annotating a corresponding corpus created from data records of two social work institutions with a focus on disability care. We show that the agreement on the task of annotating the observations of social workers with respect to discrete independent levels yields a high agreement of .74 as measured by Fleiss’ Kappa. We present a classification approach towards automatically classifying an observation into the discrete independence levels and present results for different types of classifiers. Against our original expectation, we show that we reach F-Measures (macro) of 95% averaged across topics, showing that this task can be automatically solved.</abstract>
      <url hash="a67cc98a">2020.nlpcss-1.15</url>
      <doi>10.18653/v1/2020.nlpcss-1.15</doi>
    </paper>
    <paper id="16">
      <title>Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity</title>
      <author><first>Wei-Fan</first><last>Chen</last></author>
      <author><first>Khalid</first><last>Al Khatib</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>149–154</pages>
      <abstract>Media is an indispensable source of information and opinion, shaping the beliefs and attitudes of our society. Obviously, media portals can also provide overly biased content, e.g., by reporting on political events in a selective or incomplete manner. A relevant question hence is whether and how such a form of unfair news coverage can be exposed. This paper addresses the automatic detection of bias, but it goes one step further in that it explores how political bias and unfairness are manifested linguistically. We utilize a new corpus of 6964 news articles with labels derived from adfontesmedia.com to develop a neural model for bias assessment. Analyzing the model on article excerpts, we find insightful bias patterns at different levels of text granularity, from single words to the whole article discourse.</abstract>
      <url hash="31e3a648">2020.nlpcss-1.16</url>
      <doi>10.18653/v1/2020.nlpcss-1.16</doi>
    </paper>
    <paper id="17">
      <title>Mapping Local News Coverage: Precise location extraction in textual news content using fine-tuned <fixed-case>BERT</fixed-case> based language model</title>
      <author><first>Sarang</first><last>Gupta</last></author>
      <author><first>Kumari</first><last>Nishu</last></author>
      <pages>155–162</pages>
      <abstract>Mapping local news coverage from textual content is a challenging problem that requires extracting precise location mentions from news articles. While traditional named entity taggers are able to extract geo-political entities and certain non geo-political entities, they cannot recognize precise location mentions such as addresses, streets and intersections that are required to accurately map the news article. We fine-tune a BERT-based language model for achieving high level of granularity in location extraction. We incorporate the model into an end-to-end tool that further geocodes the extracted locations for the broader objective of mapping news coverage.</abstract>
      <url hash="f49f3623">2020.nlpcss-1.17</url>
      <doi>10.18653/v1/2020.nlpcss-1.17</doi>
    </paper>
    <paper id="18">
      <title>Foreigner-directed speech is simpler than native-directed: Evidence from social media</title>
      <author><first>Aleksandrs</first><last>Berdicevskis</last></author>
      <pages>163–172</pages>
      <abstract>I test two hypotheses that play an important role in modern sociolinguistics and language evolution studies: first, that non-native production is simpler than native; second, that production addressed to non-native speakers is simpler than that addressed to natives. The second hypothesis is particularly important for theories about contact-induced simplification, since the accommodation to non-natives may explain how the simplification can spread from adult learners to the whole community. To test the hypotheses, I create a very large corpus of native and non-native written speech in four languages (English, French, Italian, Spanish), extracting data from an internet forum where native languages of the participants are known and the structure of the interactions can be inferred. The corpus data yield inconsistent evidence with respect to the first hypothesis, but largely support the second one, suggesting that foreigner-directed speech is indeed simpler than native-directed. Importantly, when testing the first hypothesis, I contrast production of different speakers, which can introduce confounds and is a likely reason for the inconsistencies. When testing the second hypothesis, the comparison is always within the production of the same speaker (but with different addressees), which makes it more reliable.</abstract>
      <url hash="d08974a6">2020.nlpcss-1.18</url>
      <doi>10.18653/v1/2020.nlpcss-1.18</doi>
    </paper>
    <paper id="19">
      <title>Diachronic Embeddings for People in the News</title>
      <author><first>Felix</first><last>Hennig</last></author>
      <author><first>Steven</first><last>Wilson</last></author>
      <pages>173–183</pages>
      <abstract>Previous English-language diachronic change models based on word embeddings have typically used single tokens to represent entities, including names of people. This leads to issues with both ambiguity (resulting in one embedding representing several distinct and unrelated people) and unlinked references (leading to several distinct embeddings which represent the same person). In this paper, we show that using named entity recognition and heuristic name linking steps before training a diachronic embedding model leads to more accurate representations of references to people, as compared to the token-only baseline. In large news corpus of articles from The Guardian, we provide examples of several types of analysis that can be performed using these new embeddings. Further, we show that real world events and context changes can be detected using our proposed model.</abstract>
      <url hash="39f90c4d">2020.nlpcss-1.19</url>
      <doi>10.18653/v1/2020.nlpcss-1.19</doi>
    </paper>
    <paper id="20">
      <title>Social media data as a lens onto care-seeking behavior among women veterans of the <fixed-case>US</fixed-case> armed forces</title>
      <author><first>Kacie</first><last>Kelly</last></author>
      <author><first>Alex</first><last>Fine</last></author>
      <author><first>Glen</first><last>Coppersmith</last></author>
      <pages>184–192</pages>
      <abstract>In this article, we examine social media data as a lens onto support-seeking among women veterans of the US armed forces. Social media data hold a great deal of promise as a source of information on needs and support-seeking among individuals who are excluded from or systematically prevented from accessing clinical or other institutions ostensibly designed to support them. We apply natural language processing (NLP) techniques to more than 3 million Tweets collected from 20,000 Twitter users. We find evidence that women veterans are more likely to use social media to seek social and community engagement and to discuss mental health and veterans’ issues significantly more frequently than their male counterparts. By contrast, male veterans tend to use social media to amplify political ideologies or to engage in partisan debate. Our results have implications for how organizations can provide outreach and services to this uniquely vulnerable population, and illustrate the utility of non-traditional observational data sources such as social media to understand the needs of marginalized groups.</abstract>
      <url hash="c1044f35">2020.nlpcss-1.20</url>
      <doi>10.18653/v1/2020.nlpcss-1.20</doi>
    </paper>
    <paper id="21">
      <title>Understanding Weekly <fixed-case>COVID</fixed-case>-19 Concerns through Dynamic Content-Specific <fixed-case>LDA</fixed-case> Topic Modeling</title>
      <author><first>Mohammadzaman</first><last>Zamani</last></author>
      <author><first>H. Andrew</first><last>Schwartz</last></author>
      <author><first>Johannes</first><last>Eichstaedt</last></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last></author>
      <author><first>Adithya</first><last>Virinchipuram Ganesan</last></author>
      <author><first>Sean</first><last>Clouston</last></author>
      <author><first>Salvatore</first><last>Giorgi</last></author>
      <pages>193–198</pages>
      <abstract>The novelty and global scale of the COVID-19 pandemic has lead to rapid societal changes in a short span of time. As government policy and health measures shift, public perceptions and concerns also change, an evolution documented within discourse on social media.We propose a dynamic content-specific LDA topic modeling technique that can help to identify different domains of COVID-specific discourse that can be used to track societal shifts in concerns or views. Our experiments show that these model-derived topics are more coherent than standard LDA topics, and also provide new features that are more helpful in prediction of COVID-19 related outcomes including social mobility and unemployment rate.</abstract>
      <url hash="0273ecef">2020.nlpcss-1.21</url>
      <doi>10.18653/v1/2020.nlpcss-1.21</doi>
    </paper>
    <paper id="22">
      <title>Emoji and Self-Identity in <fixed-case>T</fixed-case>witter Bios</title>
      <author><first>Jinhang</first><last>Li</last></author>
      <author><first>Giorgos</first><last>Longinos</last></author>
      <author><first>Steven</first><last>Wilson</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>199–211</pages>
      <abstract>Emoji are widely used to express emotions and concepts on social media, and prior work has shown that users’ choice of emoji reflects the way that they wish to present themselves to the world. Emoji usage is typically studied in the context of posts made by users, and this view has provided important insights into phenomena such as emotional expression and self-representation. In addition to making posts, however, social media platforms like Twitter allow for users to provide a short bio, which is an opportunity to briefly describe their account as a whole. In this work, we focus on the use of emoji in these bio statements. We explore the ways in which users include emoji in these self-descriptions, finding different patterns than those observed around emoji usage in tweets. We examine the relationships between emoji used in bios and the content of users’ tweets, showing that the topics and even the average sentiment of tweets varies for users with different emoji in their bios. Lastly, we confirm that homophily effects exist with respect to the types of emoji that are included in bios of users and their followers.</abstract>
      <url hash="5f5de8a8">2020.nlpcss-1.22</url>
      <doi>10.18653/v1/2020.nlpcss-1.22</doi>
    </paper>
    <paper id="23">
      <title>Analyzing Gender Bias within Narrative Tropes</title>
      <author><first>Dhruvil</first><last>Gala</last></author>
      <author><first>Mohammad Omar</first><last>Khursheed</last></author>
      <author><first>Hannah</first><last>Lerner</last></author>
      <author><first>Brendan</first><last>O’Connor</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>212–217</pages>
      <abstract>Popular media reflects and reinforces societal biases through the use of tropes, which are narrative elements, such as archetypal characters and plot arcs, that occur frequently across media. In this paper, we specifically investigate gender bias within a large collection of tropes. To enable our study, we crawl tvtropes.org, an online user-created repository that contains 30K tropes associated with 1.9M examples of their occurrences across film, television, and literature. We automatically score the “genderedness” of each trope in our TVTROPES dataset, which enables an analysis of (1) highly-gendered topics within tropes, (2) the relationship between gender bias and popular reception, and (3) how the gender of a work’s creator correlates with the types of tropes that they use.</abstract>
      <url hash="adb90b2e">2020.nlpcss-1.23</url>
      <doi>10.18653/v1/2020.nlpcss-1.23</doi>
    </paper>
  </volume>
</collection>
