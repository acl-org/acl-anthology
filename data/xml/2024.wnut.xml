<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.wnut">
  <volume id="1" ingest-date="2024-03-04" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Ninth Workshop on Noisy and User-generated Text (W-NUT 2024)</booktitle>
      <editor><first>Rob</first><last>van der Goot</last></editor>
      <editor><first>JinYeong</first><last>Bak</last></editor>
      <editor><first>Max</first><last>Müller-Eberstein</last></editor>
      <editor><first>Wei</first><last>Xu</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <editor><first>Tim</first><last>Baldwin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>San Ġiljan, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="0477fc84">2024.wnut-1</url>
      <venue>wnut</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="b970b71d">2024.wnut-1.0</url>
      <bibkey>wnut-2024-noisy</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Correcting Challenging <fixed-case>F</fixed-case>innish Learner Texts With Claude, <fixed-case>GPT</fixed-case>-3.5 and <fixed-case>GPT</fixed-case>-4 Large Language Models</title>
      <author><first>Mathias</first><last>Creutz</last><affiliation>University of Helsinki</affiliation></author>
      <pages>1-10</pages>
      <abstract>This paper studies the correction of challenging authentic Finnish learner texts at beginner level (CEFR A1). Three state-of-the-art large language models are compared, and it is shown that GPT-4 outperforms GPT-3.5, which in turn outperforms Claude v1 on this task. Additionally, ensemble models based on classifiers combining outputs of multiple single models are evaluated. The highest accuracy for an ensemble model is 84.3%, whereas the best single model, which is a GPT-4 model, produces sentences that are fully correct 83.3% of the time. In general, the different models perform on a continuum, where grammatical correctness, fluency and coherence go hand in hand.</abstract>
      <url hash="6ccd815b">2024.wnut-1.1</url>
      <bibkey>creutz-2024-correcting</bibkey>
    </paper>
    <paper id="2">
      <title>Context-aware Adversarial Attack on Named Entity Recognition</title>
      <author><first>Shuguang</first><last>Chen</last></author>
      <author><first>Leonardo</first><last>Neves</last></author>
      <author><first>Thamar</first><last>Solorio</last><affiliation>University of Houston and University of Houston</affiliation></author>
      <pages>11-16</pages>
      <abstract>In recent years, large pre-trained language models (PLMs) have achieved remarkable performance on many natural language processing benchmarks. Despite their success, prior studies have shown that PLMs are vulnerable to attacks from adversarial examples. In this work, we focus on the named entity recognition task and study context-aware adversarial attack methods to examine the model’s robustness. Specifically, we propose perturbing the most informative words for recognizing entities to create adversarial examples and investigate different candidate replacement methods to generate natural and plausible adversarial examples. Experiments and analyses show that our methods are more effective in deceiving the model into making wrong predictions than strong baselines.</abstract>
      <url hash="3ab7912f">2024.wnut-1.2</url>
      <bibkey>chen-etal-2024-context</bibkey>
    </paper>
    <paper id="3">
      <title>Effects of different types of noise in user-generated reviews on human and machine translations including <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case></title>
      <author><first>Maja</first><last>Popovic</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last><affiliation>Universität Hildesheim</affiliation></author>
      <author><first>Maarit</first><last>Koponen</last><affiliation>University of Eastern Finland</affiliation></author>
      <pages>17-30</pages>
      <abstract>This paper investigates effects of noisy source texts (containing spelling and grammar errors, informal words or expressions, etc.) on human and machine translations, namely whether the noisy phenomena are kept in the translations, corrected, or caused errors. The analysed data consists of English user reviews of Amazon products translated into Croatian, Russian and Finnish by professional translators, translation students, machine translation (MT) systems, and ChatGPT language model. The results show that overall, ChatGPT and professional translators mostly correct/standardise those parts, while students are often keeping them. Furthermore, MT systems are most prone to errors while ChatGPT is more robust, but notably less robust than human translators. Finally, some of the phenomena are particularly challenging both for MT systems and for ChatGPT, especially spelling errors and informal constructions.</abstract>
      <url hash="6fde7a26">2024.wnut-1.3</url>
      <bibkey>popovic-etal-2024-effects</bibkey>
    </paper>
    <paper id="4">
      <title>Stanceosaurus 2.0 - Classifying Stance Towards <fixed-case>R</fixed-case>ussian and <fixed-case>S</fixed-case>panish Misinformation</title>
      <author><first>Anton</first><last>Lavrouk</last></author>
      <author><first>Ian</first><last>Ligon</last></author>
      <author><first>Jonathan</first><last>Zheng</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Tarek</first><last>Naous</last></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Alan</first><last>Ritter</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>31-43</pages>
      <abstract>The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide high-quality, annotated, 5-way stance data extracted from Twitter, suitable for analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus 2.0 iteration, we extend this framework to encompass Russian and Spanish. The former is of current significance due to prevalent misinformation amid escalating tensions with the West and the violent incursion into Ukraine. The latter, meanwhile, represents an enormous community that has been largely overlooked on major social media platforms. By incorporating an additional 3,874 Spanish and Russian tweets over 41 misinformation claims, our objective is to support research focused on these issues. To demonstrate the value of this data, we employed zero-shot cross-lingual transfer on multilingual BERT, yielding results on par with the initial Stanceosaurus study with a macro F1 score of 43 for both languages. This underlines the viability of stance classification as an effective tool for identifying multicultural misinformation.</abstract>
      <url hash="db36f96f">2024.wnut-1.4</url>
      <bibkey>lavrouk-etal-2024-stanceosaurus</bibkey>
    </paper>
    <paper id="5">
      <title>A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis on Noisy <fixed-case>B</fixed-case>angla Texts</title>
      <author><first>Kazi</first><last>Elahi</last></author>
      <author><first>Tasnuva</first><last>Rahman</last></author>
      <author><first>Shakil</first><last>Shahriar</last></author>
      <author><first>Samir</first><last>Sarker</last></author>
      <author><first>Md.</first><last>Shawon</last><affiliation>Ahsanullah University of Science &amp; Technology</affiliation></author>
      <author><first>G. M.</first><last>Shibli</last><affiliation>Bangladesh University of Engineering and Technology and Ahsanullah University of Science &amp; Technology</affiliation></author>
      <pages>44-57</pages>
      <abstract>While Bangla is considered a language with limited resources, sentiment analysis has been a subject of extensive research in the literature. Nevertheless, there is a scarcity of exploration into sentiment analysis specifically in the realm of noisy Bangla texts. In this paper, we introduce a dataset (NC-SentNoB) that we annotated manually to identify ten different types of noise found in a pre-existing sentiment analysis dataset comprising of around 15K noisy Bangla texts. At first, given an input noisy text, we identify the noise type, addressing this as a multi-label classification task. Then, we introduce baseline noise reduction methods to alleviate noise prior to conducting sentiment analysis. Finally, we assess the performance of fine-tuned sentiment analysis models with both noisy and noise-reduced texts to make comparisons. The experimental findings indicate that the noise reduction methods utilized are not satisfactory, highlighting the need for more suitable noise reduction methods in future research endeavors. We have made the implementation and dataset presented in this paper publicly available at https://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts</abstract>
      <url hash="44a3dd08">2024.wnut-1.5</url>
      <bibkey>elahi-etal-2024-comparative</bibkey>
    </paper>
    <paper id="6">
      <title>Label Supervised Contrastive Learning for Imbalanced Text Classification in <fixed-case>E</fixed-case>uclidean and Hyperbolic Embedding Spaces</title>
      <author><first>Baber</first><last>Khalid</last><affiliation>Amazon</affiliation></author>
      <author><first>Shuyang</first><last>Dai</last><affiliation>Amazon</affiliation></author>
      <author><first>Tara</first><last>Taghavi</last></author>
      <author><first>Sungjin</first><last>Lee</last><affiliation>Amazon</affiliation></author>
      <pages>58-67</pages>
      <abstract>Text classification is an important problem with a wide range of applications in NLP. However, naturally occurring data is imbalanced which can induce biases when training classification models. In this work, we introduce a novel contrastive learning (CL) approach to help with imbalanced text classification task. CL has an inherent structure which pushes similar data closer in embedding space and vice versa using data samples anchors. However, in traditional CL methods text embeddings are used as anchors, which are scattered over the embedding space. We propose a CL approach which learns key anchors in the form of label embeddings and uses them as anchors. This allows our approach to bring the embeddings closer to their labels in the embedding space and divide the embedding space between labels in a fairer manner. We also introduce a novel method to improve the interpretability of our approach in a multi-class classification scenario. This approach learns the inter-class relationships during training which provide insight into the model decisions. Since our approach is focused on dividing the embedding space between different labels we also experiment with hyperbolic embeddings since they have been proven successful in embedding hierarchical information. Our proposed method outperforms several state-of-the-art baselines by an average 11% F1. Our interpretable approach highlights key data relationships and our experiments with hyperbolic embeddings give us important insights for future investigations. We will release the implementation of our approach with the publication.</abstract>
      <url hash="c91618b5">2024.wnut-1.6</url>
      <bibkey>khalid-etal-2024-label</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>M</fixed-case>aint<fixed-case>N</fixed-case>orm: A corpus and benchmark model for lexical normalisation and masking of industrial maintenance short text</title>
      <author><first>Tyler</first><last>Bikaun</last><affiliation>University of Western Australia</affiliation></author>
      <author><first>Melinda</first><last>Hodkiewicz</last><affiliation>University of Western Australia</affiliation></author>
      <author><first>Wei</first><last>Liu</last><affiliation>University of Western Australia</affiliation></author>
      <pages>68-78</pages>
      <abstract>Maintenance short texts are invaluable unstructured data sources, serving as a diagnostic and prognostic window into the operational health and status of physical assets. These user-generated texts, created during routine or ad-hoc maintenance activities, offer insights into equipment performance, potential failure points, and maintenance needs. However, the use of information captured in these texts is hindered by inherent challenges: the prevalence of engineering jargon, domain-specific vernacular, random spelling errors without identifiable patterns, and the absence of standard grammatical structures. To transform these texts into accessible and analysable data, we introduce the MaintNorm dataset, the first resource specifically tailored for the lexical normalisation task of maintenance short texts. Comprising 12,000 examples, this dataset enables the efficient processing and interpretation of these texts. We demonstrate the utility of MaintNorm by training a lexical normalisation model as a sequence-to-sequence learning task with two learning objectives, namely, enhancing the quality of the texts and masking segments to obscure sensitive information to anonymise data. Our benchmark model demonstrates a universal error reduction rate of 95.8%. The dataset and benchmark outcomes are available to the public.</abstract>
      <url hash="89a732ab">2024.wnut-1.7</url>
      <bibkey>bikaun-etal-2024-maintnorm</bibkey>
    </paper>
    <paper id="8">
      <title>The Effects of Data Quality on Named Entity Recognition</title>
      <author><first>Divya</first><last>Bhadauria</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>Alejandro</first><last>Sierra Múnera</last><affiliation>Hasso Plattner Institute</affiliation></author>
      <author><first>Ralf</first><last>Krestel</last><affiliation>Leibniz Information Center for Economics</affiliation></author>
      <pages>79-88</pages>
      <abstract>The extraction of valuable information from the vast amount of digital data available today has become increasingly important, making Named Entity Recognition models an essential component of information extraction tasks. This emphasizes the importance of understanding the factors that can compromise the performance of these models. Many studies have examined the impact of data annotation errors on NER models, leaving the broader implication of overall data quality on these models unexplored. In this work, we evaluate the robustness of three prominent NER models on datasets with varying amounts of textual noise types. The results show that as the noise in the dataset increases, model performance declines, with a minor impact for some noise types and a significant drop in performance for others. The findings of this research can be used as a foundation for building robust NER systems by enhancing dataset quality beforehand.</abstract>
      <url hash="ebce6a12">2024.wnut-1.8</url>
      <bibkey>bhadauria-etal-2024-effects</bibkey>
    </paper>
    <paper id="9">
      <title>Topic Bias in Emotion Classification</title>
      <author><first>Maximilian</first><last>Wegge</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <pages>89-103</pages>
      <abstract>Emotion corpora are typically sampled based on keyword/hashtag search or by asking study participants to generate textual instances. In any case, these corpora are not uniform samples representing the entirety of a domain. We hypothesize that this practice of data acquision leads to unrealistic correlations between overrepresented topics in these corpora that harm the generalizability of models. Such topic bias could lead to wrong predictions for instances like “I organized the service for my aunt’s funeral.” when funeral events are overpresented for instances labeled with sadness, despite the emotion of pride being more appropriate here. In this paper, we study this topic bias both from the data and the modeling perspective. We first label a set of emotion corpora automatically via topic modeling and show that emotions in fact correlate with specific topics. Further, we see that emotion classifiers are confounded by such topics. Finally, we show that the established debiasing method of adversarial correction via gradient reversal mitigates the issue. Our work points out issues with existing emotion corpora and that more representative resources are required for fair evaluation of models predicting affective concepts from text.</abstract>
      <url hash="ef2b3a85">2024.wnut-1.9</url>
      <bibkey>wegge-klinger-2024-topic</bibkey>
    </paper>
    <paper id="10">
      <title>Stars Are All You Need: A Distantly Supervised Pyramid Network for Unified Sentiment Analysis</title>
      <author><first>Wenchang</first><last>Li</last></author>
      <author><first>Yixing</first><last>Chen</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Shuang</first><last>Zheng</last></author>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>John</first><last>Lalor</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>104-118</pages>
      <abstract>Data for the Rating Prediction (RP) sentiment analysis task such as star reviews are readily available. However, data for aspect-category sentiment analysis (ACSA) is often desired because of the fine-grained nature but are expensive to collect. In this work we present a method for learning ACSA using only RP labels. We propose Unified Sentiment Analysis (Uni-SA) to efficiently understand aspect and review sentiment in a unified manner. We propose a Distantly Supervised Pyramid Network (DSPN) to efficiently perform Aspect-Category Detection (ACD), ACSA, and OSA using only RP labels for training. We evaluate DSPN on multi-aspect review datasets in English and Chinese and find that with only star rating labels for supervision, DSPN performs comparably well to a variety of benchmark models. We also demonstrate the interpretability of DSPN’s outputs on reviews to show the pyramid structure inherent in document level end-to-end sentiment analysis.</abstract>
      <url hash="e4f40d8b">2024.wnut-1.10</url>
      <bibkey>li-etal-2024-stars</bibkey>
    </paper>
  </volume>
</collection>
