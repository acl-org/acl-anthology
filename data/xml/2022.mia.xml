<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.mia">
  <volume id="1" ingest-date="2022-06-29" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop on Multilingual Information Access (MIA)</booktitle>
      <editor><first>Akari</first><last>Asai</last></editor>
      <editor><first>Eunsol</first><last>Choi</last></editor>
      <editor><first>Jonathan H.</first><last>Clark</last></editor>
      <editor><first>Junjie</first><last>Hu</last></editor>
      <editor><first>Chia-Hsuan</first><last>Lee</last></editor>
      <editor><first>Jungo</first><last>Kasai</last></editor>
      <editor><first>Shayne</first><last>Longpre</last></editor>
      <editor><first>Ikuya</first><last>Yamada</last></editor>
      <editor><first>Rui</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, USA</address>
      <month>July</month>
      <year>2022</year>
      <url hash="f450f52b">2022.mia-1</url>
      <venue>mia</venue>
    </meta>
    <frontmatter>
      <url hash="3b3c3cb6">2022.mia-1.0</url>
      <bibkey>mia-2022-multilingual</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Geographical Distance Is The New Hyperparameter: A Case Study Of Finding The Optimal Pre-trained Language For <fixed-case>E</fixed-case>nglish-isi<fixed-case>Z</fixed-case>ulu Machine Translation.</title>
      <author><first>Muhammad Umair</first><last>Nasir</last></author>
      <author><first>Innocent</first><last>Mchechesi</last></author>
      <pages>1-8</pages>
      <abstract>Stemming from the limited availability of datasets and textual resources for low-resource languages such as isiZulu, there is a significant need to be able to harness knowledge from pre-trained models to improve low resource machine translation. Moreover, a lack of techniques to handle the complexities of morphologically rich languages has compounded the unequal development of translation models, with many widely spoken African languages being left behind. This study explores the potential benefits of transfer learning in an English-isiZulu translation framework. The results indicate the value of transfer learning from closely related languages to enhance the performance of low-resource translation models, thus providing a key strategy for low-resource translation going forward. We gathered results from 8 different language corpora, including one multi-lingual corpus, and saw that isiXhosa-isiZulu outperformed all languages, with a BLEU score of 8.56 on the test set which was better from the multi-lingual corpora pre-trained model by 2.73. We also derived a new coefficient, Nasir’s Geographical Distance Coefficient (NGDC) which provides an easy selection of languages for the pre-trained models. NGDC also indicated that isiXhosa should be selected as the language for the pre-trained model.</abstract>
      <url hash="bc4e66cb">2022.mia-1.1</url>
      <bibkey>nasir-mchechesi-2022-geographical</bibkey>
      <doi>10.18653/v1/2022.mia-1.1</doi>
    </paper>
    <paper id="2">
      <title>An Annotated Dataset and Automatic Approaches for Discourse Mode Identification in Low-resource <fixed-case>B</fixed-case>engali Language</title>
      <author><first>Salim</first><last>Sazzed</last></author>
      <pages>9-15</pages>
      <abstract>The modes of discourse aid in comprehending the convention and purpose of various forms of languages used during communication. In this study, we introduce a discourse mode annotated corpus for the low-resource Bangla (also referred to as Bengali) language. The corpus consists of sentence-level annotation of three different discourse modes, narrative, descriptive, and informative of the text excerpted from a number of Bangla novels. We analyze the annotated corpus to expose various linguistic aspects of discourse modes, such as class distributions and average sentence lengths. To automatically determine the mode of discourse, we apply CML (classical machine learning) classifiers with n-gram based statistical features and a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) based language model. We observe that fine-tuned BERT-based approach yields more promising results than n-gram based CML classifiers. Our created discourse mode annotated dataset, the first of its kind in Bangla, and the evaluation, provide baselines for the automatic discourse mode identification in Bangla and can assist various downstream natural language processing tasks.</abstract>
      <url hash="08b9be29">2022.mia-1.2</url>
      <bibkey>sazzed-2022-annotated</bibkey>
      <doi>10.18653/v1/2022.mia-1.2</doi>
      <video href="2022.mia-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Pivot Through <fixed-case>E</fixed-case>nglish: Reliably Answering Multilingual Questions without Document Retrieval</title>
      <author><first>Ivan</first><last>Montero</last></author>
      <author><first>Shayne</first><last>Longpre</last></author>
      <author><first>Ni</first><last>Lao</last></author>
      <author><first>Andrew</first><last>Frank</last></author>
      <author><first>Christopher</first><last>DuBois</last></author>
      <pages>16-28</pages>
      <abstract>Existing methods for open-retrieval question answering in lower resource languages (LRLs) lag significantly behind English. They not only suffer from the shortcomings of non-English document retrieval, but are reliant on language-specific supervision for either the task or translation. We formulate a task setup more realistic to available resources, that circumvents document retrieval to reliably transfer knowledge from English to lower resource languages. Assuming a strong English question answering model or database, we compare and analyze methods that pivot through English: to map foreign queries to English and then English answers back to target language answers. Within this task setup we propose Reranked Multilingual Maximal Inner Product Search (RM-MIPS), akin to semantic similarity retrieval over the English training set with reranking, which outperforms the strongest baselines by 2.7% on XQuAD and 6.2% on MKQA. Analysis demonstrates the particular efficacy of this strategy over state-of-the-art alternatives in challenging settings: low-resource languages, with extensive distractor data and query distribution misalignment. Circumventing retrieval, our analysis shows this approach offers rapid answer generation to many other languages off-the-shelf, without necessitating additional training data in the target language.</abstract>
      <url hash="ac60c183">2022.mia-1.3</url>
      <bibkey>montero-etal-2022-pivot</bibkey>
      <doi>10.18653/v1/2022.mia-1.3</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="4">
      <title>Cross-Lingual <fixed-case>QA</fixed-case> as a Stepping Stone for Monolingual Open <fixed-case>QA</fixed-case> in <fixed-case>I</fixed-case>celandic</title>
      <author><first>Vésteinn</first><last>Snæbjarnarson</last></author>
      <author><first>Hafsteinn</first><last>Einarsson</last></author>
      <pages>29-36</pages>
      <abstract>It can be challenging to build effective open question answering (open QA) systems for languages other than English, mainly due to a lack of labeled data for training. We present a data efficient method to bootstrap such a system for languages other than English. Our approach requires only limited QA resources in the given language, along with machine-translated data, and at least a bilingual language model. To evaluate our approach, we build such a system for the Icelandic language and evaluate performance over trivia style datasets. The corpora used for training are English in origin but machine translated into Icelandic. We train a bilingual Icelandic/English language model to embed English context and Icelandic questions following methodology introduced with DensePhrases (Lee et al., 2021). The resulting system is an open domain cross-lingual QA system between Icelandic and English. Finally, the system is adapted for Icelandic only open QA, demonstrating how it is possible to efficiently create an open QA system with limited access to curated datasets in the language of interest.</abstract>
      <url hash="133427ab">2022.mia-1.4</url>
      <bibkey>snaebjarnarson-einarsson-2022-cross</bibkey>
      <doi>10.18653/v1/2022.mia-1.4</doi>
      <video href="2022.mia-1.4.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/nqii">NQiI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
    </paper>
    <paper id="5">
      <title>Multilingual Event Linking to <fixed-case>W</fixed-case>ikidata</title>
      <author><first>Adithya</first><last>Pratapa</last></author>
      <author><first>Rishubh</first><last>Gupta</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <pages>37-58</pages>
      <abstract>We present a task of multilingual linking of events to a knowledge base. We automatically compile a large-scale dataset for this task, comprising of 1.8M mentions across 44 languages referring to over 10.9K events from Wikidata. We propose two variants of the event linking task: 1) multilingual, where event descriptions are from the same language as the mention, and 2) crosslingual, where all event descriptions are in English. On the two proposed tasks, we compare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and multilingual adaptations of the biencoder and crossencoder architectures from BLINK (Wu et al., 2020). In our experiments on the two task variants, we find both biencoder and crossencoder models significantly outperform the BM25+ baseline. Our results also indicate that the crosslingual task is in general more challenging than the multilingual task. To test the out-of-domain generalization of the proposed linking systems, we additionally create a Wikinews-based evaluation set. We present qualitative analysis highlighting various aspects captured by the proposed dataset, including the need for temporal reasoning over context and tackling diverse event descriptions across languages.</abstract>
      <url hash="a481c2fa">2022.mia-1.5</url>
      <bibkey>pratapa-etal-2022-multilingual</bibkey>
      <doi>10.18653/v1/2022.mia-1.5</doi>
      <pwccode url="https://github.com/adithya7/xlel-wd" additional="false">adithya7/xlel-wd</pwccode>
    </paper>
    <paper id="6">
      <title>Complex Word Identification in <fixed-case>V</fixed-case>ietnamese: Towards <fixed-case>V</fixed-case>ietnamese Text Simplification</title>
      <author><first>Phuong</first><last>Nguyen</last></author>
      <author><first>David</first><last>Kauchak</last></author>
      <pages>59-68</pages>
      <abstract>Text Simplification has been an extensively researched problem in English, but has not been investigated in Vietnamese. We focus on the Vietnamese-specific Complex Word Identification task, often the first step in Lexical Simplification (Shardlow, 2013). We examine three different Vietnamese datasets constructed for other Natural Language Processing tasks and show that, like in other languages, frequency is a strong signal in determining whether a word is complex, with a mean accuracy of 86.87%. Across the datasets, we find that the 10% most frequent words in many corpus can be labelled as simple, and the rest as complex, though this is more variable for smaller corpora. We also examine how human annotators perform at this task. Given the subjective nature, there is a fair amount of variability in which words are seen as difficult, though majority results are more consistent.</abstract>
      <url hash="e6a52624">2022.mia-1.6</url>
      <bibkey>nguyen-kauchak-2022-complex</bibkey>
      <doi>10.18653/v1/2022.mia-1.6</doi>
    </paper>
    <paper id="7">
      <title>Benchmarking Language-agnostic Intent Classification for Virtual Assistant Platforms</title>
      <author><first>Gengyu</first><last>Wang</last></author>
      <author><first>Cheng</first><last>Qian</last></author>
      <author><first>Lin</first><last>Pan</last></author>
      <author><first>Haode</first><last>Qi</last></author>
      <author><first>Ladislav</first><last>Kunc</last></author>
      <author><first>Saloni</first><last>Potdar</last></author>
      <pages>69-76</pages>
      <abstract>Current virtual assistant (VA) platforms are beholden to the limited number of languages they support. Every component, such as the tokenizer and intent classifier, is engineered for specific languages in these intricate platforms. Thus, supporting a new language in such platforms is a resource-intensive operation requiring expensive re-training and re-designing. In this paper, we propose a benchmark for evaluating language-agnostic intent classification, the most critical component of VA platforms. To ensure the benchmarking is challenging and comprehensive, we include 29 public and internal datasets across 10 low-resource languages and evaluate various training and testing settings with consideration of both accuracy and training time. The benchmarking result shows that Watson Assistant, among 7 commercial VA platforms and pre-trained multilingual language models (LMs), demonstrates close-to-best accuracy with the best accuracy-training time trade-off.</abstract>
      <url hash="29d74020">2022.mia-1.7</url>
      <bibkey>wang-etal-2022-benchmarking</bibkey>
      <doi>10.18653/v1/2022.mia-1.7</doi>
      <pwccode url="https://github.com/posuer/benchmark-multilingual-intent-classification" additional="false">posuer/benchmark-multilingual-intent-classification</pwccode>
    </paper>
    <paper id="8">
      <title><fixed-case>Z</fixed-case>usammen<fixed-case>QA</fixed-case>: Data Augmentation with Specialized Models for Cross-lingual Open-retrieval Question Answering System</title>
      <author><first>Chia-Chien</first><last>Hung</last></author>
      <author><first>Tommaso</first><last>Green</last></author>
      <author><first>Robert</first><last>Litschko</last></author>
      <author><first>Tornike</first><last>Tsereteli</last></author>
      <author><first>Sotaro</first><last>Takeshita</last></author>
      <author><first>Marco</first><last>Bombieri</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <pages>77-90</pages>
      <abstract>This paper introduces our proposed system for the MIA Shared Task on Cross-lingual Openretrieval Question Answering (COQA). In this challenging scenario, given an input question the system has to gather evidence documents from a multilingual pool and generate from them an answer in the language of the question. We devised several approaches combining different model variants for three main components: Data Augmentation, Passage Retrieval, and Answer Generation. For passage retrieval, we evaluated the monolingual BM25 ranker against the ensemble of re-rankers based on multilingual pretrained language models (PLMs) and also variants of the shared task baseline, re-training it from scratch using a recently introduced contrastive loss that maintains a strong gradient signal throughout training by means of mixed negative samples. For answer generation, we focused on languageand domain-specialization by means of continued language model (LM) pretraining of existing multilingual encoders. Additionally, for both passage retrieval and answer generation, we augmented the training data provided by the task organizers with automatically generated question-answer pairs created from Wikipedia passages to mitigate the issue of data scarcity, particularly for the low-resource languages for which no training data were provided. Our results show that language- and domain-specialization as well as data augmentation help, especially for low-resource languages.</abstract>
      <url hash="2a21c67b">2022.mia-1.8</url>
      <bibkey>hung-etal-2022-zusammenqa</bibkey>
      <doi>10.18653/v1/2022.mia-1.8</doi>
      <video href="2022.mia-1.8.mp4"/>
      <pwccode url="https://github.com/umanlp/zusammenqa" additional="true">umanlp/zusammenqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xor-tydi-qa">XOR-TYDI QA</pwcdataset>
    </paper>
    <paper id="9">
      <title>Zero-shot cross-lingual open domain question answering</title>
      <author><first>Sumit</first><last>Agarwal</last></author>
      <author><first>Suraj</first><last>Tripathi</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <author><first>Carolyn Penstein</first><last>Rose</last></author>
      <pages>91-99</pages>
      <abstract>People speaking different kinds of languages search for information in a cross-lingual manner. They tend to ask questions in their language and expect the answer to be in the same language, despite the evidence lying in another language. In this paper, we present our approach for this task of cross-lingual open-domain question-answering. Our proposed method employs a passage reranker, the fusion-in-decoder technique for generation, and a wiki data entity-based post-processing system to tackle the inability to generate entities across all languages. Our end-2-end pipeline shows an improvement of 3 and 4.6 points on F1 and EM metrics respectively, when compared with the baseline CORA model on the XOR-TyDi dataset. We also evaluate the effectiveness of our proposed techniques in the zero-shot setting using the MKQA dataset and show an improvement of 5 points in F1 for high-resource and 3 points improvement for low-resource zero-shot languages. Our team, CMUmQA’s submission in the MIA-Shared task ranked 1st in the constrained setup for the dev and 2nd in the test setting.</abstract>
      <url hash="572ac35a">2022.mia-1.9</url>
      <bibkey>agarwal-etal-2022-zero</bibkey>
      <doi>10.18653/v1/2022.mia-1.9</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="10">
      <title><fixed-case>MIA</fixed-case> 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering</title>
      <author><first>Zhucheng</first><last>Tu</last></author>
      <author><first>Sarguna Janani</first><last>Padmanabhan</last></author>
      <pages>100-107</pages>
      <abstract>We describe our two-stage system for the Multilingual Information Access (MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The first stage consists of multilingual passage retrieval with a hybrid dense and sparse retrieval strategy. The second stage consists of a reader which outputs the answer from the top passages returned by the first stage. We show the efficacy of using entity representations, sparse retrieval signals to help dense retrieval, and Fusion-in-Decoder. On the development set, we obtain 43.46 F1 on XOR-TyDi QA and 21.99 F1 on MKQA, for an average F1 score of 32.73. On the test set, we obtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an average F1 score of 31.61. We improve over the official baseline by over 4 F1 points on both the development and test sets.</abstract>
      <url hash="420c4ce4">2022.mia-1.10</url>
      <bibkey>tu-padmanabhan-2022-mia</bibkey>
      <doi>10.18653/v1/2022.mia-1.10</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mr-tydi">Mr. TYDI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDiQA</pwcdataset>
    </paper>
    <paper id="11">
      <title><fixed-case>MIA</fixed-case> 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages</title>
      <author><first>Akari</first><last>Asai</last></author>
      <author><first>Shayne</first><last>Longpre</last></author>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Chia-Hsuan</first><last>Lee</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Junjie</first><last>Hu</last></author>
      <author><first>Ikuya</first><last>Yamada</last></author>
      <author><first>Jonathan H.</first><last>Clark</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <pages>108-120</pages>
      <abstract>We present the results of the Workshop on Multilingual Information Access (MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question answering (QA) systems in 16 typologically diverse languages. In this task, we adapted two large-scale cross-lingual open-retrieval QA datasets in 14 typologically diverse languages, and newly annotated open-retrieval QA data in 2 underrepresented languages: Tagalog and Tamil. Four teams submitted their systems. The best constrained system uses entity-aware contextualized representations for document retrieval, thereby achieving an average F1 score of 31.6, which is 4.1 F1 absolute higher than the challenging baseline. The best system obtains particularly significant improvements in Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores. The best unconstrained system achieves 32.2 F1, outperforming our baseline by 4.5 points.</abstract>
      <url hash="adedb5ae">2022.mia-1.11</url>
      <bibkey>asai-etal-2022-mia</bibkey>
      <doi>10.18653/v1/2022.mia-1.11</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDiQA</pwcdataset>
    </paper>
  </volume>
</collection>
