<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.htres">
  <volume id="1" ingest-date="2024-05-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First Workshop on Holocaust Testimonies as Language Resources (HTRes) @ LREC-COLING 2024</booktitle>
      <editor><first>Isuri</first><last>Anuradha</last></editor>
      <editor><first>Martin</first><last>Wynne</last></editor>
      <editor><first>Francesca</first><last>Frontini</last></editor>
      <editor><first>Alistair</first><last>Plum</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="153171d4">2024.htres-1</url>
      <venue>htres</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="17b02769">2024.htres-1.0</url>
      <bibkey>htres-2024-holocaust</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The Impact of Digital Editing on the Study of Holocaust Survivors’ Testimonies in the context of Voci dall’Inferno Project</title>
      <author><first>Angelo Mario</first><last>Del Grosso</last></author>
      <author><first>Marina</first><last>Riccucci</last></author>
      <author><first>Elvira</first><last>Mercatanti</last></author>
      <pages>1–9</pages>
      <abstract>In Nazi concentration camps, approximately 20 million people perished. This included young and old, men and women, Jews, dissidents, and homosexuals. Only 10% of those deported survived. This paper introduces “Voci dall’Inferno” project, which aims to achieve two key objectives: a) Create a comprehensive digital archive: by encoding a corpus of non-literary testimonies including both written and oral sources. b) Analyze the use of Dante’s language: by identifying the presence of Dante’s lexicon and allusions. Currently, the project holds 47 testimonies, with 29 transcribed in full text and 18 encoded using the XML-TEI format. This project is propelled by a multidisciplinary and educational context with experts in humanities and computer science. The project’s findings will be disseminated through a user-friendly web application built on an XML foundation. Though currently in its prototyping phase, the application boasts several features, including a search engine for testimonies, terms, or phrases within the corpus. Additionally, a browsing interface allows users to read and listen the original testimonies, while a visualization tool enables deeper exploration of the corpus’s content. Adhering to the Text Encoding Initiative (TEI) guidelines, the project ensures a structured digital archive, aligned with the FAIR principles for data accessibility and reusability.</abstract>
      <url hash="15875fff">2024.htres-1.1</url>
      <bibkey>del-grosso-etal-2024-impact</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>TEI</fixed-case> Specifications for a Sustainable Management of Digitized Holocaust Testimonies</title>
      <author><first>Sarah</first><last>Bénière</last></author>
      <author><first>Floriane</first><last>Chiffoleau</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <pages>10–17</pages>
      <abstract>Data modeling and standardization are central issues in the field of Digital Humanities, and all the more so when dealing with Holocaust testimonies, where stable preservation and long-term accessibility are key. The EHRI Online Editions are composed of documents of diverse nature (testimonies, letters, diplomatic reports, etc.), held by EHRI’s partnering institutions, and selected, gathered thematically and encoded according to the TEI Guidelines by the editors within the EHRI Consortium. Standardization is essential in order to make sure that the editions are consistent with one another. The issue of consistency also encourages a broader reflection on the usage of standards when processing data, and on the standardization of digital scholarly editions of textual documents in general. In this paper, we present the normalization work we carried out on the EHRI Online Editions. It includes a customization of the TEI adapted to Holocaust-related documents, and a focus on the implementation of controlled vocabulary. We recommend the use of these encoding specifications as a tool for researchers and/or non-TEI experts to ensure their encoding is valid and consistent across editions, but also as a mechanism for integrating the edition work smoothly within a wider workflow leading from image digitization to publication.</abstract>
      <url hash="7cdf7364">2024.htres-1.2</url>
      <bibkey>beniere-etal-2024-tei</bibkey>
    </paper>
    <paper id="3">
      <title>Repurposing Holocaust-Related Digital Scholarly Editions to Develop Multilingual Domain-Specific Named Entity Recognition Tools</title>
      <author><first>Maria</first><last>Dermentzi</last></author>
      <author><first>Hugo</first><last>Scheithauer</last></author>
      <pages>18–28</pages>
      <abstract>The European Holocaust Research Infrastructure (EHRI) aims to support Holocaust research by making information about dispersed Holocaust material accessible and interconnected through its services. Creating a tool capable of detecting named entities in texts such as Holocaust testimonies or archival descriptions would make it easier to link more material with relevant identifiers in domain-specific controlled vocabularies, semantically enriching it, and making it more discoverable. With this paper, we release EHRI-NER, a multilingual dataset (Czech, German, English, French, Hungarian, Dutch, Polish, Slovak, Yiddish) for Named Entity Recognition (NER) in Holocaust-related texts. EHRI-NER is built by aggregating all the annotated documents in the EHRI Online Editions and converting them to a format suitable for training NER models. We leverage this dataset to fine-tune the multilingual Transformer-based language model XLM-RoBERTa (XLM-R) to determine whether a single model can be trained to recognize entities across different document types and languages. The results of our experiments show that despite our relatively small dataset, in a multilingual experiment setup, the overall F1 score achieved by XLM-R fine-tuned on multilingual annotations is 81.5%. We argue that this score is sufficiently high to consider the next steps towards deploying this model.</abstract>
      <url hash="b8f66755">2024.htres-1.3</url>
      <bibkey>dermentzi-scheithauer-2024-repurposing</bibkey>
    </paper>
    <paper id="4">
      <title>Dates and places as points of attachment for memorial contents in the <fixed-case>ISW</fixed-case> corpus: 1938 as a turning point</title>
      <author><first>Carolina</first><last>Flinz</last></author>
      <author><first>Simona</first><last>Leonardi</last></author>
      <pages>29–36</pages>
      <abstract>Aim of the paper is the identification and subsequent analysis of crisis years in the narrative biographical interviews with German speaking Jews from the corpus ISW (Emigrantendeutsch in Israel: Wiener in Jerusalem/ Migrant German in Israel: Viennese in Jerusalem); also the possible “chronological landmarks” within a year will be tackled, investigating how a certain year – 1938 – represents in the life story of the narrators a turning point, as it clusters most traumatic events linked to the Shoah. The transcripts were analysed using the tool Sketch Engine. An alternation of corpus-driven and corpus-based steps characterizes this study, which uses a quantitative-qualitative approach (see Lemnitzer and Zinsmeister, 2015) and integrates also approaches from narrative analysis. The research questions that guide our investigation are as follows: Are there any special dates that recur as chronological landmarks of crisis situations (Leonardi 2023a)? Which are they? Do they recur in connection with special places? which ones?</abstract>
      <url hash="3ba8534a">2024.htres-1.4</url>
      <bibkey>flinz-leonardi-2024-dates</bibkey>
    </paper>
    <paper id="5">
      <title>Creating a Typology of Places to Annotate Holocaust Testimonies Through Machine Learning</title>
      <author><first>Christine</first><last>Liu</last></author>
      <author><first>William J.B.</first><last>Mattingly</last></author>
      <pages>37</pages>
      <abstract>The Holocaust was not only experienced in iconic places like Auschwitz or the Warsaw ghetto. Ordinary places, such as city streets, forests, hills, and homes, were transformed by occupation and systematic violence. While most of these places are unnamed and locationally ambiguous, their omnipresence throughout post-war testimonies from witnesses and survivors of the Holocaust emphasize their undeniable importance. This paper shares a methodology for developing a typology of places in order to annotate both named and unnamed places within interview transcripts from the United States Holocaust Memorial Museum (USHMM) through a machine learning model. The approach underscores the benefits of hybrid analysis through both automated extraction and manual review to create distinct categories of places. This paper also reviews how testimony transcripts were converted into structured data for annotation and previews ongoing work to design a search engine for users to dynamically query this place-based approach to studying the Holocaust.</abstract>
      <url hash="50efb52e">2024.htres-1.5</url>
      <bibkey>liu-mattingly-2024-creating</bibkey>
    </paper>
    <paper id="6">
      <title>Speech Technology Services for Oral History Research</title>
      <author><first>Christoph</first><last>Draxler</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <author><first>Arjan</first><last>van Hessen</last></author>
      <author><first>Pavel</first><last>Ircing</last></author>
      <author><first>Jan</first><last>Lehečka</last></author>
      <pages>38–43</pages>
      <abstract>Oral history is about oral sources of witnesses and commentors on historical events. Speech technology is an important instrument to process such recordings in order to obtain transcription and further enhancements to structure the oral account In this contribution we address the transcription portal and the webservices associated with speech processing at BAS, speech solutions developed at LINDAT, how to do it yourself with Whisper, remaining challenges, and future developments.</abstract>
      <url hash="7aa301e1">2024.htres-1.6</url>
      <bibkey>draxler-etal-2024-speech</bibkey>
    </paper>
    <paper id="7">
      <title>Identifying Narrative Patterns and Outliers in Holocaust Testimonies Using Topic Modeling</title>
      <author><first>Maxim</first><last>Ifergan</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <author><first>Renana</first><last>Keydar</last></author>
      <author><first>Amit</first><last>Pinchevski</last></author>
      <pages>44–52</pages>
      <abstract>The vast collection of Holocaust survivor testimonies presents invaluable historical insights but poses challenges for manual analysis. This paper leverages advanced Natural Language Processing (NLP) techniques to explore the USC Shoah Foundation Holocaust testimony corpus. By treating testimonies as structured question-and-answer sections, we apply topic modeling to identify key themes. We experiment with BERTopic, which leverages recent advances in language modeling technology. We align testimony sections into fixed parts, revealing the evolution of topics across the corpus of testimonies. This highlights both a common narrative schema and divergences between subgroups based on age and gender. We introduce a novel method to identify testimonies within groups that exhibit atypical topic distributions resembling those of other groups. This study offers unique insights into the complex narratives of Holocaust survivors, demonstrating the power of NLP to illuminate historical discourse and identify potential deviations in survivor experiences.</abstract>
      <url hash="f1dc9284">2024.htres-1.7</url>
      <bibkey>ifergan-etal-2024-identifying</bibkey>
    </paper>
    <paper id="8">
      <title>Tracing the deportation to define Holocaust geometries. The exploratory case of Milan</title>
      <author><first>Giovanni Pietro</first><last>Vitali</last></author>
      <author><first>Laura</first><last>Brazzo</last></author>
      <pages>53–62</pages>
      <abstract>This paper presents a pilot project conducted in collaboration with the Fondazione CDEC to shed light on the historical dynamics of the arrests and deportations of Jews from Italy to foreign concentration camps between 1943 and 1945. Led by a multidisciplinary team, including a Digital Humanities expert, an archivist, a GIS developer, and an education manager, the project aimed to rework archival information into data visualisation models utilising a subset of data from the CDEC LOD dataset of the victims of the Holocaust in Italy to construct detailed visual representations of deportation routes. Drawing inspiration from previous projects like the Atlas of Nazi-Fascist Massacres and research on Holocaust testimonies, this project sought to create interactive maps, network and graphs illustrating the paths of forced transfers endured by arrested Jews, particularly focusing on those born or arrested in Milan. Despite challenges such as incomplete or imprecise data, the team managed to reconstruct deportation routes and classify transport convoys, enhancing the understanding of this dark period in history. The visualisations, along with detailed repositories and links provided on GitHub, serve as valuable research tools for both scholarly and educational purposes, offering users varying levels of granularity to explore historical events and timelines. Through meticulous data analysis and visualisation techniques, this project contributes to ongoing efforts to preserve and understand the tragic events of the Holocaust, emphasizing the importance of archival work and interdisciplinary collaboration in historical research.</abstract>
      <url hash="df20f39f">2024.htres-1.8</url>
      <bibkey>vitali-brazzo-2024-tracing</bibkey>
    </paper>
    <paper id="9">
      <title>Zero-shot Trajectory Mapping in Holocaust Testimonies</title>
      <author><first>Eitan</first><last>Wagner</last></author>
      <author><first>Renana</first><last>Keydar</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>63–70</pages>
      <abstract>This work presents the task of Zero-shot Trajectory Mapping, which focuses on the spatial dimension of narratives. The task consists of two parts: (1) creating a “map” with all the locations mentioned in a set of texts, and (2) extracting a trajectory from a single testimony and positioning it within the map. Following recent advances in context length capabilities of large language models, we propose a pipeline for this task in a completely unsupervised manner, without the requirement of any type of labels. We demonstrate the pipeline on a set of ≈ 75 testimonies and present the resulting map and samples of the trajectory. We conclude that current long-range models succeed in generating meaningful maps and trajectories. Other than the visualization and indexing, we propose future directions for adaptation of the task as a step for dividing testimony sets into clusters and for alignment between parallel parts of different testimonies.</abstract>
      <url hash="7ab7d8d3">2024.htres-1.9</url>
      <bibkey>wagner-etal-2024-zero</bibkey>
    </paper>
  </volume>
</collection>
