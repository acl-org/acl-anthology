<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.aisd">
  <volume id="main" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on AI and Scientific Discovery: Directions and Opportunities</booktitle>
      <editor><first>Peter</first><last>Jansen</last></editor>
      <editor><first>Bhavana</first><last>Dalvi Mishra</last></editor>
      <editor><first>Harsh</first><last>Trivedi</last></editor>
      <editor><first>Bodhisattwa</first><last>Prasad Majumder</last></editor>
      <editor><first>Tom</first><last>Hope</last></editor>
      <editor><first>Tushar</first><last>Khot</last></editor>
      <editor><first>Doug</first><last>Downey</last></editor>
      <editor><first>Eric</first><last>Horvitz</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico, USA</address>
      <month>May</month>
      <year>2025</year>
      <url hash="2c19046d">2025.aisd-main</url>
      <venue>aisd</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-224-4</isbn>
    </meta>
    <frontmatter>
      <url hash="fb506c6c">2025.aisd-main.0</url>
      <bibkey>aisd-ws-2025-main</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Variable Extraction for Model Recovery in Scientific Literature</title>
      <author><first>Chunwei</first><last>Liu</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Enrique</first><last>Noriega-Atala</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Adarsh</first><last>Pyarelal</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Clayton T</first><last>Morrison</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Mike</first><last>Cafarella</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>1-12</pages>
      <abstract>Due to the increasing productivity in the scientific community, it is difficult to keep up with the literature without the assistance of AI methods. This paper evaluates various methods for extracting mathematical model variables from epidemiological studies, such as ‘infection rate (<tex-math>\alpha</tex-math>),” ‘recovery rate (<tex-math>\gamma</tex-math>),” and ‘mortality rate (<tex-math>\mu</tex-math>).” Variable extraction appears to be a basic task, but plays a pivotal role in recovering models from scientific literature. Once extracted, we can use these variables for automatic mathematical modeling, simulation, and replication of published results. We also introduce a benchmark dataset comprising manually-annotated variable descriptions and variable values extracted from scientific papers. Our analysis shows that LLM-based solutions perform the best. Despite the incremental benefits of combining rule-based extraction outputs with LLMs, the leap in performance attributed to the transfer-learning and instruction-tuning capabilities of LLMs themselves is far more significant. This investigation demonstrates the potential of LLMs to enhance automatic comprehension of scientific artifacts and for automatic model recovery and simulation.</abstract>
      <url hash="68e3667b">2025.aisd-main.1</url>
      <bibkey>liu-etal-2025-variable</bibkey>
    </paper>
    <paper id="2">
      <title>How Well Do Large Language Models Extract Keywords? A Systematic Evaluation on Scientific Corpora</title>
      <author><first>Nacef Ben</first><last>Mansour</last></author>
      <author><first>Hamed</first><last>Rahimi</last></author>
      <author><first>Motasem</first><last>Alrahabi</last></author>
      <pages>13-21</pages>
      <abstract>Automatic keyword extraction from scientific articles is pivotal for organizing scholarly archives, powering semantic search engines, and mapping interdisciplinary research trends. However, existing methods—including statistical and graph-based approaches—struggle to handle domain-specific challenges such as technical terminology, cross-disciplinary ambiguity, and dynamic scientific jargon. This paper presents an empirical comparison of traditional keyword extraction methods (e.g. TextRank and YAKE) with approaches based on Large Language Model. We introduce a novel evaluation framework that combines fuzzy semantic matching based on Levenshtein Distance with exact-match metrics (F1, precision, recall) to address inconsistencies in keyword normalization across scientific corpora. Through an extensive ablation study across nine different LLMs, we analyze their performance and associated costs. Our findings reveal that LLM-based methods consistently achieve superior precision and relevance compared to traditional approaches. This performance advantage suggests significant potential for improving scientific search systems and information retrieval in academic contexts.</abstract>
      <url hash="75c24ca3">2025.aisd-main.2</url>
      <bibkey>mansour-etal-2025-well</bibkey>
    </paper>
    <paper id="3">
      <title>A Human-<fixed-case>LLM</fixed-case> Note-Taking System with Case-Based Reasoning as Framework for Scientific Discovery</title>
      <author><first>Douglas B</first><last>Craig</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>22-30</pages>
      <abstract>Scientific discovery is an iterative process that requires transparent reasoning, empirical validation, and structured problem-solving. This work presents a novel human-in-the-loop AI system that leverages case-based reasoning to facilitate structured scientific inquiry. The system is designed to be note-centric, using the Obsidian note-taking application as the primary interface where all components, including user inputs, system cases, and tool specifications, are represented as plain-text notes. This approach ensures that every step of the research process is visible, editable, and revisable by both the user and the AI. The system dynamically retrieves relevant cases from past experience, refines hypotheses, and structures research workflows in a transparent and iterative manner. The methodology is demonstrated through a case study investigating the role of TLR4 in sepsis, illustrating how the system supports problem framing, literature review, hypothesis formulation, and empirical validation. The results highlight the potential of AI-assisted scientific workflows to enhance research efficiency while preserving human oversight and interpretability.</abstract>
      <url hash="36e49f58">2025.aisd-main.3</url>
      <bibkey>craig-2025-human</bibkey>
    </paper>
    <paper id="4">
      <title>Towards <fixed-case>AI</fixed-case>-assisted Academic Writing</title>
      <author><first>Daniel J.</first><last>Liebling</last><affiliation>Google</affiliation></author>
      <author><first>Malcolm</first><last>Kane</last><affiliation>Google</affiliation></author>
      <author><first>Madeleine</first><last>Grunde-McLaughlin</last><affiliation>University of Washington</affiliation></author>
      <author><first>Ian</first><last>Lang</last><affiliation>Google</affiliation></author>
      <author><first>Subhashini</first><last>Venugopalan</last><affiliation>Google</affiliation></author>
      <author><first>Michael</first><last>Brenner</last><affiliation>Harvard University</affiliation></author>
      <pages>31-45</pages>
      <abstract>We present components of an AI-assisted academic writing system including citation recommendation and introduction writing. The system recommends citations by considering the user’s current document context to provide relevant suggestions. It generates introductions in a structured fashion, situating the contributions of the research relative to prior work. We demonstrate the effectiveness of the components through quantitative evaluations. Finally, the paper presents qualitative research exploring how researchers incorporate citations into their writing workflows. Our findings indicate that there is demand for precise AI-assisted writing systems and simple, effective methods for meeting those needs.</abstract>
      <url hash="64f558d3">2025.aisd-main.4</url>
      <bibkey>liebling-etal-2025-towards</bibkey>
    </paper>
    <paper id="5">
      <title>Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications</title>
      <author><first>Ethan</first><last>Lin</last></author>
      <author><first>Zhiyuan</first><last>Peng</last></author>
      <author><first>Yi</first><last>Fang</last><affiliation>Santa Clara University</affiliation></author>
      <pages>46-57</pages>
      <abstract>Recent studies have evaluated creativity, where novelty is an important aspect, of large language models (LLMs) primarily from a semantic perspective, using benchmarks from cognitive science. However, assessing the novelty in scholarly publications, a critical facet of evaluating LLMs as scientific discovery assistants, remains underexplored, despite its potential to accelerate research cycles and prioritize high-impact contributions in scientific workflows. We introduce SchNovel, a benchmark to evaluate LLMs’ ability to assess novelty in scholarly papers, a task central to streamlining discovery pipeline. SchNovel consists of 15000 pairs of papers across six fields sampled from the arXiv dataset with publication dates spanning 2 to 10 years apart. In each pair, the more recently published paper is assumed to be more novel. Additionally, we propose RAG-Novelty, a retrieval-augmented method that mirrors human peer review by grounding novelty assessment in retrieved context. Extensive experiments provide insights into the capabilities of different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms recent baseline models highlight LLMs’ promise as tools for automating novelty detection in scientific workflows.</abstract>
      <url hash="bfa7e91f">2025.aisd-main.5</url>
      <bibkey>lin-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>LLM</fixed-case>-Assisted Translation of Legacy <fixed-case>FORTRAN</fixed-case> Codes to <fixed-case>C</fixed-case>++: A Cross-Platform Study</title>
      <author><first>Nishath Rajiv</first><last>Ranasinghe</last></author>
      <author><first>Shawn M.</first><last>Jones</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Michal</first><last>Kucer</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Ayan</first><last>Biswas</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Daniel</first><last>O’Malley</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Alexander</first><last>Most</last></author>
      <author><first>Selma Liliane</first><last>Wanna</last></author>
      <author><first>Ajay</first><last>Sreekumar</last></author>
      <pages>58-69</pages>
      <abstract>Large Language Models (LLMs) are increasinglybeing leveraged for generating andtranslating scientific computer codes by bothdomain-experts and non-domain experts. Fortranhas served as one of the go to programminglanguages in legacy high-performance computing(HPC) for scientific discoveries. Despitegrowing adoption, LLM-based code translationof legacy code-bases has not been thoroughlyassessed or quantified for its usability.Here, we studied the applicability of LLMbasedtranslation of Fortran to C++ as a step towardsbuilding an agentic-workflow using openweightLLMs on two different computationalplatforms. We statistically quantified the compilationaccuracy of the translated C++ codes,measured the similarity of the LLM translatedcode to the human translated C++ code, andstatistically quantified the output similarity ofthe Fortran to C++ translation.</abstract>
      <url hash="d3512fed">2025.aisd-main.6</url>
      <bibkey>ranasinghe-etal-2025-llm</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>F</fixed-case>lavor<fixed-case>D</fixed-case>iffusion: Modeling Food-Chemical Interactions with Diffusion</title>
      <author><first>Junpyo</first><last>Seo</last></author>
      <pages>70-77</pages>
      <abstract>The study of food pairing has evolved beyond subjective expertise with the advent of machine learning. This paper presents FlavorDiffusion, a novel framework leveraging diffusion models to predict food-chemical interactions and ingredient pairings without relying on chromatography. By integrating graph-based embeddings, diffusion processes, and chemical property encoding, FlavorDiffusion addresses data imbalances and enhances clustering quality. Using a heterogeneous graph derived from datasets like Recipe1M and FlavorDB, our model demonstrates superior performance in reconstructing ingredient-ingredient relationships. The addition of a Chemical Structure Prediction (CSP) layer further refines the embedding space, achieving state-of-the-art NMI scores and enabling meaningful discovery of novel ingredient combinations. The proposed framework represents a significant step forward in computational gastronomy, offering scalable, interpretable, and chemically informed solutions for food science.</abstract>
      <url hash="106786ba">2025.aisd-main.7</url>
      <bibkey>seo-2025-flavordiffusion</bibkey>
    </paper>
  </volume>
</collection>
