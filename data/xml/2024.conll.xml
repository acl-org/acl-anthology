<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.conll">
  <volume id="1" ingest-date="2024-11-05" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 28th Conference on Computational Natural Language Learning</booktitle>
      <editor><first>Libby</first><last>Barak</last></editor>
      <editor><first>Malihe</first><last>Alikhani</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Miami, FL, USA</address>
      <month>November</month>
      <year>2024</year>
      <url hash="dae0c156">2024.conll-1</url>
      <venue>conll</venue>
    </meta>
    <frontmatter>
      <url hash="8f22d13b">2024.conll-1.0</url>
      <bibkey>conll-2024-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Words That Stick: Using Keyword Cohesion to Improve Text Segmentation</title>
      <author><first>Amit</first><last>Maraj</last></author>
      <author><first>Miguel</first><last>Vargas Martin</last><affiliation>Ontario Tech University</affiliation></author>
      <author><first>Masoud</first><last>Makrehchi</last><affiliation>Ontario Tech University</affiliation></author>
      <pages>1-9</pages>
      <abstract>Text Segmentation (TS) is the idea of segmenting bodies of text into coherent blocks, mostly defined by the topics each segment contains. Historically, techniques in this area have been unsupervised, with more success recently coming from supervised methods instead. Although these approaches see better performance, they require training data and upfront training time. We propose a new method called Coherence, where we use strong sentence embeddings to pull representational keywords as the main constructor of sentences when comparing them to one another. Additionally, we include a storage of previously found keywords for the purposes of creating a more accurate segment representation instead of just the immediate sentence in question. With our system, we show improved results over current state-of-the-art unsupervised techniques when analyzed using Pk and WindowDiff scores. Because its unsupervised, Coherence requires no fine-tuning.</abstract>
      <url hash="d0c2b3fd">2024.conll-1.1</url>
      <bibkey>maraj-etal-2024-words</bibkey>
    </paper>
    <paper id="2">
      <title>Investigating large language models for their competence in extracting grammatically sound sentences from transcribed noisy utterances</title>
      <author><first>Alina</first><last>Wróblewska</last></author>
      <pages>10-23</pages>
      <abstract>Selectively processing noisy utterances while effectively disregarding speech-specific elements poses no considerable challenge for humans, as they exhibit remarkable cognitive abilities to separate semantically significant content from speech-specific noise (i.e. filled pauses, disfluencies, and restarts). These abilities may be driven by mechanisms based on acquired grammatical rules that compose abstract syntactic-semantic structures within utterances. Segments without syntactic and semantic significance are consistently disregarded in these structures. The structures, in tandem with lexis, likely underpin language comprehension and thus facilitate effective communication.In our study, grounded in linguistically motivated experiments, we investigate whether large language models (LLMs) can effectively perform analogical speech comprehension tasks. In particular, we examine the ability of LLMs to extract well-structured utterances from transcriptions of noisy dialogues. We conduct two evaluation experiments in the Polish language scenario, using a dataset presumably unfamiliar to LLMs to mitigate the risk of data contamination. Our results show that not all extracted utterances are correctly structured, indicating that either LLMs do not fully acquire syntactic-semantic rules or they acquire them but cannot apply them effectively. We conclude that the ability of LLMs to comprehend noisy utterances is still relatively superficial compared to human proficiency in processing them.</abstract>
      <url hash="8701f0ad">2024.conll-1.2</url>
      <bibkey>wroblewska-2024-investigating</bibkey>
    </paper>
    <paper id="3">
      <title>Multi-Cultural Norm Base: Frame-based Norm Discovery in Multi-Cultural Settings</title>
      <author><first>Viet Thanh</first><last>Pham</last><affiliation>Monash University</affiliation></author>
      <author><first>Shilin</first><last>Qu</last></author>
      <author><first>Farhad</first><last>Moghimifar</last><affiliation>Monash University</affiliation></author>
      <author><first>Suraj</first><last>Sharma</last></author>
      <author><first>Yuan-Fang</first><last>Li</last><affiliation>Monash University and Oracle</affiliation></author>
      <author><first>Weiqing</first><last>Wang</last></author>
      <author><first>Reza</first><last>Haf</last><affiliation>Monash University</affiliation></author>
      <pages>24-35</pages>
      <abstract>Sociocultural norms serve as guiding principles for personal conduct in social interactions within a particular society or culture. The study of norm discovery has seen significant development over the last few years, with various interesting approaches. However, it is difficult to adopt these approaches to discover norms in a new culture, as they rely either on human annotations or real-world dialogue contents. This paper presents a robust automatic norm discovery pipeline, which utilizes the cultural knowledge of GPT-3.5 Turbo (ChatGPT) along with several social factors. By using these social factors and ChatGPT, our pipeline avoids the use of human dialogues that tend to be limited to specific scenarios, as well as the use of human annotations that make it difficult and costly to enlarge the dataset. The resulting database - Multi-cultural Norm Base (MNB) - covers 6 distinct cultures, with over 150k sociocultural norm statements in total. A state-of-the-art Large Language Model (LLM), Llama 3, fine-tuned with our proposed dataset, shows remarkable results on various downstream tasks, outperforming models fine-tuned on other datasets significantly.</abstract>
      <url hash="3fe8a2b1">2024.conll-1.3</url>
      <bibkey>pham-etal-2024-multi</bibkey>
    </paper>
    <paper id="4">
      <title>Lossy Context Surprisal Predicts Task-Dependent Patterns in Relative Clause Processing</title>
      <author><first>Kate</first><last>McCurdy</last><affiliation>Universität des Saarlandes and University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Michael</first><last>Hahn</last></author>
      <pages>36-45</pages>
      <abstract>English relative clauses are a critical test case for theories of syntactic processing. Expectation- and memory-based accounts make opposing predictions, and behavioral experiments have found mixed results. We present a technical extension of Lossy Context Surprisal (LCS) and use it to model relative clause processing in three behavioral experiments. LCS predicts key results at distinct retention rates, showing that task-dependent memory demands can account for discrepant behavioral patterns in the literature.</abstract>
      <url hash="0ed72b74">2024.conll-1.4</url>
      <bibkey>mccurdy-hahn-2024-lossy</bibkey>
    </paper>
    <paper id="5">
      <title>Global-Pruner: A Stable and Efficient Pruner for Retraining-Free Pruning of Encoder-Based Language Models</title>
      <author><first>Guangzhen</first><last>Yao</last></author>
      <author><first>Yuehan</first><last>Wang</last></author>
      <author><first>Hui</first><last>Xu</last></author>
      <author><first>Long</first><last>Zhang</last></author>
      <author><first>MiaoQI</first><last>MiaoQI</last></author>
      <pages>46-55</pages>
      <abstract>Large language models (LLMs) have achieved significant success in complex tasks across various domains, but they come with high computational costs and inference latency issues. Pruning, as an effective method, can significantly reduce inference costs. However, current pruning algorithms for encoder-based language models often focus on locally optimal solutions, neglecting a comprehensive exploration of the global solution space. This oversight can lead to instability in the solution process, thereby affecting the overall performance of the model. To address these challenges, we propose a structured pruning algorithm named G-Pruner (Global Pruner), comprising two integral components: PPOM (Proximal Policy Optimization Mask) and CG²MT (Conjugate Gradient Squared Mask Tuning), utilizing a global optimization strategy. This strategy not only eliminates the need for retraining but also ensures the algorithm’s stability and adaptability to environmental changes, effectively addressing the issue of focusing solely on immediate optima while neglecting long-term effects. This method is evaluated on the GLUE and SQuAD benchmarks using BERTBASE and DistilBERT models. The experimental results indicate that without any retraining, G-Pruner achieves significant accuracy improvements on the SQuAD<tex-math>_{2.0}</tex-math> task with a FLOPs constraint of 60%, demonstrating a 6.02% increase in F1 score compared with baseline algorithms.</abstract>
      <url hash="5fc9744d">2024.conll-1.5</url>
      <bibkey>yao-etal-2024-global</bibkey>
    </paper>
    <paper id="6">
      <title>Transformer verbatim in-context retrieval across time and scale</title>
      <author><first>Kristijan</first><last>Armeni</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Marko</first><last>Pranjić</last><affiliation>Jozef Stefan Institute and Jozef Stefan International Postgraduate School</affiliation></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>56-68</pages>
      <abstract>To predict upcoming text, language models must in some cases retrieve in-context information verbatim. In this report, we investigated how the ability of language models to retrieve arbitrary in-context nouns developed during training (across time) and as language models trained on the same dataset increase in size (across scale). We then asked whether learning of in-context retrieval correlates with learning of more challenging zero-shot benchmarks. Furthermore, inspired by semantic effects in human short-term memory, we evaluated the retrieval with respect to a major semantic component of target nouns, namely whether they denote a concrete or abstract entity, as rated by humans. We show that verbatim in-context retrieval developed in a sudden transition early in the training process, after about 1% of the training tokens. This was observed across model sizes (from 14M and up to 12B parameters), and the transition occurred slightly later for the two smallest models. We further found that the development of verbatim in-context retrieval is positively correlated with the learning of zero-shot benchmarks. Around the transition point, all models showed the advantage of retrieving concrete nouns as opposed to abstract nouns. In all but two smallest models, the advantage dissipated away toward the end of training.</abstract>
      <url hash="c5018a5a">2024.conll-1.6</url>
      <bibkey>armeni-etal-2024-transformer</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>E</fixed-case>dit<fixed-case>E</fixed-case>val: An Instruction-Based Benchmark for Text Improvements</title>
      <author><first>Jane</first><last>Dwivedi-Yu</last><affiliation>Meta AI</affiliation></author>
      <author><first>Timo</first><last>Schick</last><affiliation>Facebook</affiliation></author>
      <author><first>Zhengbao</first><last>Jiang</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Maria</first><last>Lomeli</last><affiliation>Meta</affiliation></author>
      <author><first>Patrick</first><last>Lewis</last></author>
      <author><first>Gautier</first><last>Izacard</last></author>
      <author><first>Edouard</first><last>Grave</last><affiliation>Facebook</affiliation></author>
      <author><first>Sebastian</first><last>Riedel</last><affiliation>Google and University College London</affiliation></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <pages>69-83</pages>
      <abstract>Evaluation of text generation to date has primarily focused on content created sequentially, rather than improvements on a piece of text. Writing, however, is naturally an iterative and incremental process that requires expertise in different modular skills such as fixing outdated information or making the writing style more consistent. Even so, comprehensive evaluation of a model’s capacity to perform these skills and the ability to edit remains sparse. This work introduces EditEval: An instruction-based, benchmark and evaluation suite that leverages high-quality existing and new datasets in English for the automatic evaluation of editing capabilities, such as making text more cohesive and paraphrasing. We evaluate several pre-trained models, which shows that InstructGPT and PEER on average perform the best, but that most baselines fall below the supervised state-of-the-art, particularly when neutralizing and updating information. Our analysis also shows that commonly used metrics for editing tasks do not always correlate well, and that prompts leading to the strongest performance do not necessarily elicit strong performance across different models. Through the release of this benchmark (code and data available at https://github.com/facebookresearch/EditEval) and a publicly available leaderboard challenge, we hope to unlock future work on developing models more capable of controllable and iterative editing.</abstract>
      <url hash="ab60aa3a">2024.conll-1.7</url>
      <bibkey>dwivedi-yu-etal-2024-editeval</bibkey>
    </paper>
    <paper id="8">
      <title>An Empirical Comparison of Vocabulary Expansion and Initialization Approaches For Language Models</title>
      <author><first>Nandini</first><last>Mundra</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras and AI4Bharat</affiliation></author>
      <author><first>Aditya Nanda Kishore</first><last>Khandavally</last></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Ratish</first><last>Puduppully</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Anoop</first><last>Kunchukuttan</last><affiliation>Microsoft and Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Mitesh M</first><last>Khapra</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>84-104</pages>
      <abstract>Language Models (LMs) excel in natural language processing tasks for English but show reduced performance in most other languages. This problem is commonly tackled by continually pre-training and fine-tuning these models for said languages. A significant issue in this process is the limited vocabulary coverage in the original model’s tokenizer, leading to inadequate representation of new languages and necessitating an expansion of the tokenizer. The initialization of the embeddings corresponding to new vocabulary items presents a further challenge. Current strategies require cross-lingual embeddings and lack a solid theoretical foundation as well as comparisons with strong baselines. In this paper, we first establish theoretically that initializing within the convex hull of existing embeddings is a good initialization, followed by a novel but simple approach, <i>Constrained Word2Vec (CW2V)</i>, which does not require cross-lingual embeddings. Our study evaluates different initialization methods for expanding RoBERTa and LLaMA 2 across four languages and five tasks. The results show that CW2V performs equally well or even better than more advanced techniques. Additionally, simpler approaches like multivariate initialization perform on par with these advanced methods indicating that efficient large-scale multilingual continued pretraining can be achieved even with simpler initialization methods.</abstract>
      <url hash="1f522855">2024.conll-1.8</url>
      <bibkey>mundra-etal-2024-empirical</bibkey>
    </paper>
    <paper id="9">
      <title>Critical Questions Generation: Motivation and Challenges</title>
      <author><first>Blanca</first><last>Calvo Figueras</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Rodrigo</first><last>Agerri</last><affiliation>University of the Basque Country</affiliation></author>
      <pages>105-116</pages>
      <abstract>The development of Large Language Models (LLMs) has brought impressive performances on mitigation strategies against misinformation, such as counterargument generation. However, LLMs are still seriously hindered by outdated knowledge and by their tendency to generate hallucinated content. In order to circumvent these issues, we propose a new task, namely, Critical Questions Generation, consisting of processing an argumentative text to generate the critical questions (CQs) raised by it.In argumentation theory CQs are tools designed to lay bare the blind spots of an argument by pointing at the information it could be missing.Thus, instead of trying to deploy LLMs to produce knowledgeable and relevant counterarguments, we use them to question arguments, without requiring any external knowledge.Research on CQs Generation using LLMs requires a reference dataset for large scale experimentation. Thus, in this work we investigate two complementary methods to create such a resource: (i) instantiating CQs templates as defined by Walton’s argumentation theory and (ii), using LLMs as CQs generators. By doing so, we contribute with a procedure to establish what is a valid CQ and conclude that, while LLMs are reasonable CQ generators, they still have a wide margin for improvement in this task.</abstract>
      <url hash="760a4b80">2024.conll-1.9</url>
      <bibkey>calvo-figueras-agerri-2024-critical</bibkey>
    </paper>
    <paper id="10">
      <title>Information Association for Language Model Updating by Mitigating <fixed-case>LM</fixed-case>-Logical Discrepancy</title>
      <author><first>Pengfei</first><last>Yu</last><affiliation>Boson AI and University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>117-129</pages>
      <abstract>Large Language Models (LLMs) struggle with providing current information due to the outdated pre-training data. Existing methods for updating LLMs, such as knowledge editing and continual fine-tuning, have significant drawbacks in generalizability of new information and the requirements on structured updating corpus. We identify the core challenge behind these drawbacks: the LM-logical discrepancy featuring the difference between language modeling probabilities and logical probabilities. To evaluate and address the core challenge, we propose a new task formulation of the information updating task that only requires the provision of an unstructured updating corpus and evaluates the performance of information updating on the generalizability to question-answer pairs pertaining to the updating information.We further propose a novel and effective pipeline approach for the task, highlighting a self-prompting-based question-answer generation process and a associative distillation methods to bridge the LM-logical discrepancy.We develop two datasets for evaluation, one sourced from news articles published in March and April 2023, and the other from the Natural Questions benchmark.Experimental results demonstrate the superiority of our approach, significantly increasing the factual consistency score (on a scale from 0 to 1) by up to 0.16. Furthermore, our method effectively mitigates forgetting utilizing a compact replay buffer with only 2.3% of the training tokens.</abstract>
      <url hash="2a536c37">2024.conll-1.10</url>
      <bibkey>yu-ji-2024-information</bibkey>
    </paper>
    <paper id="11">
      <title>Causal <fixed-case>ATE</fixed-case> Mitigates Unintended Bias in Controlled Text Generation</title>
      <author><first>Rahul</first><last>Madhavan</last><affiliation>Indian Institute of Management, Ahmedabad, Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology and Indian Institute of Science, Bangalore</affiliation></author>
      <author><first>Kahini</first><last>Wadhawan</last></author>
      <pages>130-142</pages>
      <abstract>We study attribute control in language models through the method of Causal Average Treatment Effect (Causal ATE). Existing methodsfor the attribute control task in Language Models(LMs) check for the co-occurrence of words in a sentence with the attribute of interest, and control for them. However, spurious correlation of the words with the attribute in the training dataset, can cause models to hallucinate the presence of the attribute when presented with the spurious correlate during inference. We show that the simple perturbation-based method of Causal ATE removes this unintended effect. Specifically, we ground it in the problem of toxicity mitigation, where a significant challenge lies in the inadvertent bias that often emerges towards protected groups post detoxification. We show that this unintended bias can be solved by the use of the Causal ATE metric. We provide experimental validations for our claims and release our code (anonymously) here: [github.com/causalate-mitigates-bias](https://github.com/causalate-mitigates-bias/causal-ate-mitigates-bias).</abstract>
      <url hash="467da957">2024.conll-1.11</url>
      <bibkey>madhavan-wadhawan-2024-causal</bibkey>
    </paper>
    <paper id="12">
      <title>On Functional Competence of <fixed-case>LLM</fixed-case>s for Linguistic Disambiguation</title>
      <author><first>Raihan</first><last>Kibria</last></author>
      <author><first>Sheikh Intiser Uddin</first><last>Dipta</last></author>
      <author><first>Muhammad Abdullah</first><last>Adnan</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <pages>143-160</pages>
      <abstract>We study some Large Language Models to explore their deficiencies in resolving sense ambiguities. In this connection, we evaluate their performance on well-known word sense disambiguation datasets. Word Sense Disambiguation (WSD) has been a long-standing NLP problem, which has given rise to many evaluation datasets and models over the decades. Recently the emergence of Large Language Models (LLM) raises much hope in improving accuracy. In this work, we evaluate word sense disambiguation capabilities of four LLMs: OpenAI’s ChatGPT-3.5, Mistral’s 7b parameter model, Meta’s Llama 70b, and Google’s Gemini Pro. We evaluate many well-established datasets containing a variety of texts and senses on these. After observing the performances of some datasets, we selectively study some failure cases and identify the reasons for failures. We explore human judgments that would correct these failures. Our findings suggest that many failure cases are related to a lack of world knowledge and the reasoning to amalgamate this knowledge rather than the lack of linguistic knowledge. We categorize the judgments so that the next generation of LLMs can improve by incorporating deeper world knowledge and reasoning. We conclude that word sense disambiguation could serve as a guide for probing the reasoning power of LLMs to measure their functional competency. We also list the accuracy of these datasets. We find that on many occasions, accuracy drops to below 70%, which is much less than that of well-performing existing models.</abstract>
      <url hash="b5485806">2024.conll-1.12</url>
      <bibkey>kibria-etal-2024-functional</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>AIS</fixed-case>tory<fixed-case>S</fixed-case>imilarity: Quantifying Story Similarity Using Narrative for Search, <fixed-case>IP</fixed-case> Infringement, and Guided Creativity</title>
      <author><first>Jon</first><last>Chun</last><affiliation>Kenyon College</affiliation></author>
      <pages>161-177</pages>
      <abstract>Stories are central for interpreting experiences, communicating, and influencing each other via films, medical, media, and other narratives. Quantifying the similarity between stories has numerous applications including detecting IP infringement, detecting hallucinations, search/recommendation engines, and guiding human-AI collaborations. Despite this, traditional NLP text similarity metrics are limited to short text distance metrics like n-gram overlaps and embeddings. Larger texts require preprocessing with significant information loss through paraphrasing or multi-step decomposition. This paper introduces AIStorySimilarity, a novel benchmark to measure the semantic distance between long-text stories based on core structural elements drawn from narrative theory and script writing. Based on four narrative elements (characters, plot, setting, and themes) as well 31 sub-features within these, we use a SOTA LLM (gpt-3.5-turbo) to extract and evaluate the semantic similarity of a diverse set of major Hollywood movies. In addition, we compare human evaluation with story similarity scores computed three ways: extracting elements from film scripts before evaluation (Elements), directly evaluating entire scripts (Scripts), and extracting narrative elements from the parametric memory of SOTA LLMs without any provided scripts (GenAI). To the best of our knowledge, AIStorySimilarity is the first benchmark to measure long-text story similarity using a comprehensive approach to narrative theory. Code and data are available at https://github.com/jon-chun/AIStorySimiliarity.</abstract>
      <url hash="9edef01b">2024.conll-1.13</url>
      <bibkey>chun-2024-aistorysimilarity</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>SPAWN</fixed-case>ing Structural Priming Predictions from a Cognitively Motivated Parser</title>
      <author><first>Grusha</first><last>Prasad</last><affiliation>Colgate University</affiliation></author>
      <author><first>Tal</first><last>Linzen</last><affiliation>New York University and Google</affiliation></author>
      <pages>178-197</pages>
      <abstract>Structural priming is a widely used psycholinguistic paradigm to study human sentence representations. In this work we introduce SPAWN, a cognitively motivated parser that can generate quantitative priming predictions from contemporary theories in syntax which assume a lexicalized grammar. By generating and testing priming predictions from competing theoretical accounts, we can infer which assumptions from syntactic theory are useful for characterizing the representations humans build when processing sentences. As a case study, we use SPAWN to generate priming predictions from two theories (Whiz-Deletion and Participial-Phase) which make different assumptions about the structure of English relative clauses. By modulating the reanalysis mechanism that the parser uses and strength of the parser’s prior knowledge, we generated nine sets of predictions from each of the two theories. Then, we tested these predictions using a novel web-based comprehension-to-production priming paradigm. We found that while the some of the predictions from the Participial-Phase theory aligned with human behavior, none of the predictions from the the Whiz-Deletion theory did, thus suggesting that the Participial-Phase theory might better characterize human relative clause representations.</abstract>
      <url hash="adbac3ca">2024.conll-1.14</url>
      <bibkey>prasad-linzen-2024-spawning</bibkey>
    </paper>
    <paper id="15">
      <title>Global Learning with Triplet Relations in Abstractive Summarization</title>
      <author><first>Fengyu</first><last>Lu</last></author>
      <author><first>Jiaxin</first><last>Duan</last></author>
      <author><first>Junfei</first><last>Liu</last></author>
      <pages>198-208</pages>
      <abstract>Abstractive summarization models learned with token-level maximum likelihood estimation suffer from exposure bias, that the condition for predicting the next token is discrepant during training and inference. Existing solutions bridge this gap by learning to estimate semantic or lexical qualities of a candidate summary from the global view, namely global learning (GL), yet ignore maintaining rational triplet-relations among document, reference summary, and candidate summaries, e.g., the candidate and reference summaries should have a similar faithfulness degree judging by a source document. In this paper, we propose an iterative autoregressive summarization paradigm - IARSum, which fuses the learning of triplet relations into a GL framework and further enhances summarization performance. Specifically, IARSum develops a dual-encoder network to enable the simultaneous input of a document and its candidate (or reference) summary. On this basis, it learns to 1) model the relative semantics defined over tuples (candidate, document) and (reference, document) respectively and balance them; 2) reduce lexical differences between candidate and reference summaries. Furthermore, IARSum iteratively reprocesses a generated candidate at inference time to ground higher quality. We conduct extensive experiments on two widely used datasets to test our method, and IARSum shows the new or matched state-of-the-art on diverse metrics.</abstract>
      <url hash="f8773231">2024.conll-1.15</url>
      <bibkey>lu-etal-2024-global</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>T</fixed-case>p<fixed-case>T</fixed-case>-<fixed-case>ADE</fixed-case>: Transformer Based Two-Phase <fixed-case>ADE</fixed-case> Extraction</title>
      <author><first>Suryamukhi</first><last>Kuchibhotla</last><affiliation>Indian Institute of Technology, Hyderabad, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Manish</first><last>Singh</last></author>
      <pages>209-218</pages>
      <abstract>Extracting adverse reactions to medications or treatments is a crucial activity in the biomedical domain. The task involves identifying mentions of drugs and their adverse effects/events in raw text, which is challenging due to the unstructured nature of clinical narratives. In this paper, we propose TpT-ADE, a novel joint two-phase transformer model combined with natural language processing (NLP) techniques, to identify adverse events (AEs) caused by drugs. In the first phase of TpT-ADE, entities are extracted and are grounded with their standard terms using the Unified Medical Language System (UMLS) knowledge base. In the second phase, entity and relation classification is performed to determine the presence of a relationship between the drug and AE pairs. TpT-ADE also identifies the intensity of AE entities by constructing a parts-of-speech (POS) embedding model. Unlike previous approaches that use complex classifiers, TpT-ADE employs a shallow neural network and yet outperforms the state-of-the-art methods on the standard ADE corpus.</abstract>
      <url hash="d677d414">2024.conll-1.16</url>
      <bibkey>kuchibhotla-singh-2024-tpt</bibkey>
    </paper>
    <paper id="17">
      <title>The Effect of Surprisal on Reading Times in Information Seeking and Repeated Reading</title>
      <author><first>Keren Gruteke</first><last>Klein</last><affiliation>Technion - Israel Institute of Technology, Technion</affiliation></author>
      <author><first>Yoav</first><last>Meiri</last></author>
      <author><first>Omer</first><last>Shubi</last></author>
      <author><first>Yevgeni</first><last>Berzak</last><affiliation>Technion - Israel Institute of Technology, Technion</affiliation></author>
      <pages>219-230</pages>
      <abstract>The effect of surprisal on processing difficulty has been a central topic of investigation in psycholinguistics. Here, we use eyetracking data to examine three language processing regimes that are common in daily life but have not been addressed with respect to this question: information seeking, repeated processing, and the combination of the two. Using standard regime-agnostic surprisal estimates we find that the prediction of surprisal theory regarding the presence of a linear effect of surprisal on processing times, extends to these regimes. However, when using surprisal estimates from regime-specific contexts that match the contexts and tasks given to humans, we find that in information seeking, such estimates do not improve the predictive power of processing times compared to standard surprisals. Further, regime-specific contexts yield near zero surprisal estimates with no predictive power for processing times in repeated reading. These findings point to misalignments of task and memory representations between humans and current language models, and question the extent to which such models can be used for estimating cognitively relevant quantities. We further discuss theoretical challenges posed by these results.</abstract>
      <url hash="edbdb721">2024.conll-1.17</url>
      <bibkey>klein-etal-2024-effect</bibkey>
    </paper>
    <paper id="18">
      <title>Revisiting Hierarchical Text Classification: Inference and Metrics</title>
      <author><first>Roman</first><last>Plaud</last></author>
      <author><first>Matthieu</first><last>Labeau</last><affiliation>Télécom ParisTech</affiliation></author>
      <author><first>Antoine</first><last>Saillenfest</last><affiliation>onepoint</affiliation></author>
      <author><first>Thomas</first><last>Bonald</last><affiliation>Télécom ParisTech</affiliation></author>
      <pages>231-242</pages>
      <abstract>Hierarchical text classification (HTC) is the task of assigning labels to a text within a structured space organized as a hierarchy. Recent works treat HTC as a conventional multilabel classification problem, therefore evaluating it as such. We instead propose to evaluate models based on specifically designed hierarchical metrics and we demonstrate the intricacy of metric choice and prediction inference method. We introduce a new challenging dataset and we evaluate fairly, recent sophisticated models, comparing them with a range of simple but strong baselines, including a new theoretically motivated loss. Finally, we show that those baselines are very often competitive with the latest models. This highlights the importance of carefully considering the evaluation methodology when proposing new methods for HTC</abstract>
      <url hash="4daae66e">2024.conll-1.18</url>
      <bibkey>plaud-etal-2024-revisiting</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>N</fixed-case>e<fixed-case>LLC</fixed-case>om-<fixed-case>X</fixed-case>: A Comprehensive Neural-Agent Framework to Simulate Language Learning and Group Communication</title>
      <author><first>Yuchen</first><last>Lian</last><affiliation>Leiden University and Xi’an Jiaotong University</affiliation></author>
      <author><first>Tessa</first><last>Verhoef</last><affiliation>Leiden University, Leiden University</affiliation></author>
      <author><first>Arianna</first><last>Bisazza</last><affiliation>University of Groningen</affiliation></author>
      <pages>243-258</pages>
      <abstract>Recent advances in computational linguistics include simulating the emergence of human-like languages with interacting neural network agents, starting from sets of random symbols. The recently introduced NeLLCom framework (Lian et al., 2023) allows agents to first learn an artificial language and then use it to communicate, with the aim of studying the emergence of specific linguistics properties. We extend this framework (NeLLCom-X) by introducing more realistic role-alternating agents and group communication in order to investigate the interplay between language learnability, communication pressures, and group size effects. We validate NeLLCom-X by replicating key findings from prior research simulating the emergence of a word-order/case-marking trade-off. Next, we investigate how interaction affects linguistic convergence and emergence of the trade-off. The novel framework facilitates future simulations of diverse linguistic aspects, emphasizing the importance of interaction and group dynamics in language evolution.</abstract>
      <url hash="10c1cdcf">2024.conll-1.19</url>
      <bibkey>lian-etal-2024-nellcom</bibkey>
    </paper>
    <paper id="20">
      <title>A Novel Instruction Tuning Method for <fixed-case>V</fixed-case>ietnamese Mathematical Reasoning using Trainable Open-Source Large Language Models</title>
      <author><first>Nguyen Quang</first><last>Vinh</last></author>
      <author><first>Thanh-Do</first><last>Nguyen</last></author>
      <author><first>Vinh Van</first><last>Nguyen</last><affiliation>Vietnam National University Hanoi</affiliation></author>
      <author><first>Nam Khac-Hoai</first><last>Bui</last><affiliation>Viettel Group</affiliation></author>
      <pages>259-268</pages>
      <abstract>This study introduces Simple Reasoning with Code (SiRC), a novel instruction fine-tuning method for solving mathematical reasoning problems, particularly effective for Vietnamese, which is considered a low-resource language. Specifically, solving mathematical problems requires strategic and logical reasoning, which remains challenging in this research area. This paper presents a simple yet effective instruction fine-tuning method for mathematical reasoning. Unlike previous approaches, our proposed method effectively combines chain-of-thought reasoning with code transfer methods without requiring a sophisticated inference procedure. Furthermore, we focus on exploiting small open-source large language models (LLMs) for the Vietnamese language. In this regard, we first introduce a trainable Vietnamese math reasoning dataset, which is named ViMath-InstructCode. The proposed dataset is then used for fine-tuning open-source LLMs (e.g., less than 10 billion parameters). Experiments conducted on our custom ViMath-Bench dataset, the largest benchmarking dataset focusing on Vietnamese mathematical problems, indicate the promising results of our proposed method. Our source code and dataset are available for further exploitation.</abstract>
      <url hash="52e9c757">2024.conll-1.20</url>
      <bibkey>vinh-etal-2024-novel</bibkey>
    </paper>
    <paper id="21">
      <title>Generalizations across filler-gap dependencies in neural language models</title>
      <author><first>Katherine</first><last>Howitt</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Sathvik</first><last>Nair</last></author>
      <author><first>Allison</first><last>Dods</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Robert Melvin</first><last>Hopkins</last></author>
      <pages>269-279</pages>
      <abstract>Humans develop their grammars by making structural generalizations from finite input. We ask how filler-gap dependencies (FGDs), which share a structural generalization despite diverse surface forms, might arise from the input. We explicitly control the input to a neural language model (NLM) to uncover whether the model posits a shared representation for FGDs. We show that while NLMs do have success differentiating grammatical from ungrammatical FGDs, they rely on superficial properties of the input, rather than on a shared generalization. Our work highlights the need for specific linguistic inductive biases to model language acquisition.</abstract>
      <url hash="23f7e587">2024.conll-1.21</url>
      <bibkey>howitt-etal-2024-generalizations</bibkey>
    </paper>
    <paper id="22">
      <title>Of Models and Men: Probing Neural Networks for Agreement Attraction with Psycholinguistic Data</title>
      <author><first>Maxim</first><last>Bazhukov</last><affiliation>Higher School of Economics</affiliation></author>
      <author><first>Ekaterina</first><last>Voloshina</last><affiliation>Göteborg University and Chalmers University of Technology</affiliation></author>
      <author><first>Sergey</first><last>Pletenev</last></author>
      <author><first>Arseny</first><last>Anisimov</last></author>
      <author><first>Oleg</first><last>Serikov</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Svetlana</first><last>Toldova</last><affiliation>Higher School of Economics</affiliation></author>
      <pages>280-290</pages>
      <abstract>Interpretability studies have played an important role in the field of NLP. They focus on the problems of how models encode information or, for instance, whether linguistic capabilities allow them to prefer grammatical sentences to ungrammatical. Recently, several studies examined whether the models demonstrate patterns similar to humans and whether they are sensitive to the phenomena of interference like humans’ grammaticality judgements, including the phenomenon of agreement attraction.In this paper, we probe BERT and GPT models on the syntactic phenomenon of agreement attraction in Russian using the psycholinguistic data with syncretism. Working on the language with syncretism between some plural and singular forms allows us to differentiate between the effects of the surface form and of the underlying grammatical feature. Thus we can further investigate models’ sensitivity to this phenomenon and examine if the patterns of their behaviour are similar to human patterns. Moreover, we suggest a new way of comparing models’ and humans’ responses via statistical testing. We show that there are some similarities between models’ and humans’ results, while GPT is somewhat more aligned with human responses than BERT. Finally, preliminary results suggest that surface form syncretism influences attraction, perhaps more so than grammatical form syncretism.</abstract>
      <url hash="a376a481">2024.conll-1.22</url>
      <bibkey>bazhukov-etal-2024-models</bibkey>
    </paper>
    <paper id="23">
      <title>Is Structure Dependence Shaped for Efficient Communication?: A Case Study on Coordination</title>
      <author><first>Kohei</first><last>Kajikawa</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yusuke</first><last>Kubota</last></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <pages>291-302</pages>
      <abstract>Natural language exhibits various universal properties.But why do these universals exist?One explanation is that they arise from functional pressures to achieve <i>efficient communication</i>, a view which attributes cross-linguistic properties to domain-general cognitive abilities.This hypothesis has successfully addressed some syntactic universal properties such as compositionality and Greenbergian word order universals.However, more abstract syntactic universals have not been explored from the perspective of efficient communication.Among such universals, the most notable one is <i>structure dependence</i>, that is, grammar-internal operations crucially depend on hierarchical representations.This property has traditionally been taken to be central to natural language and to involve domain-specific knowledge irreducible to communicative efficiency. In this paper, we challenge the conventional view by investigating whether structure dependence realizes efficient communication, focusing on coordinate structures.We design three types of artificial languages: (i) one with a structure-dependent reduction operation, which is similar to natural language, (ii) one without any reduction operations, and (iii) one with a linear (rather than structure-dependent) reduction operation.We quantify the communicative efficiency of these languages.The results demonstrate that the language with the structure-dependent reduction operation is significantly more communicatively efficient than the counterfactual languages.This suggests that the existence of structure-dependent properties can be explained from the perspective of efficient communication.</abstract>
      <url hash="6533765d">2024.conll-1.23</url>
      <bibkey>kajikawa-etal-2024-structure</bibkey>
    </paper>
    <paper id="24">
      <title>Large Language Model Recall Uncertainty is Modulated by the Fan Effect</title>
      <author><first>Jesse</first><last>Roberts</last><affiliation>Tennessee Technological University</affiliation></author>
      <author><first>Kyle</first><last>Moore</last><affiliation>Vanderbilt University</affiliation></author>
      <author><first>Douglas</first><last>Fisher</last><affiliation>Vanderbilt University and Vanderbilt University</affiliation></author>
      <author><first>Oseremhen</first><last>Ewaleifoh</last></author>
      <author><first>Thao</first><last>Pham</last></author>
      <pages>303-313</pages>
      <abstract>This paper evaluates whether large language models (LLMs) exhibit cognitive fan effects, similar to those discovered by Anderson in humans, after being pre-trained on human textual data. We conduct two sets of in-context recall experiments designed to elicit fan effects. Consistent with human results, we find that LLM recall uncertainty, measured via token probability, is influenced by the fan effect. Our results show that removing uncertainty disrupts the observed effect. The experiments suggest the fan effect is consistent whether the fan value is induced in-context or in the pre-training data. Finally, these findings provide in-silico evidence that fan effects and typicality are expressions of the same phenomena.</abstract>
      <url hash="8847db78">2024.conll-1.24</url>
      <bibkey>roberts-etal-2024-large</bibkey>
    </paper>
    <paper id="25">
      <title>Continuous Attentive Multimodal Prompt Tuning for Few-Shot Multimodal Sarcasm Detection</title>
      <author><first>Soumyadeep</first><last>Jana</last></author>
      <author><first>Animesh</first><last>Dey</last></author>
      <author><first>Ranbir Singh</first><last>Sanasam</last><affiliation>Indian Institute of Technology, Guwahati, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>314-326</pages>
      <abstract>With the steep rise in multimodal content on social media, multimodal sarcasm detection has gained widespread attention from research communities. Existing studies depend on large-scale data, which is challenging to obtain and expensive to annotate. Thus, investigating this problem in a few-shot scenario is required. Overtly complex multimodal models are prone to overfitting on in-domain data, which hampers their performance on out-of-distribution (OOD) data. To address these issues, we propose Continuous Attentive Multimodal Prompt Tuning model (CAMP), that leverages the prompt tuning paradigm to handle few-shot multimodal sarcasm detection. To overcome the siloed learning process of continuous prompt tokens, we design a novel, continuous multimodal attentive prompt where the continuous tokens intricately engage with both image and text tokens, enabling the assimilation of knowledge from different input modalities. Experimental results indicate that our method outperforms other multimodal baseline methods in the few-shot setting and OOD scenarios.</abstract>
      <url hash="d7c43722">2024.conll-1.25</url>
      <bibkey>jana-etal-2024-continuous</bibkey>
    </paper>
    <paper id="26">
      <title>Aligning Alignments: Do Colexification and Distributional Similarity Align as Measures of cross-lingual Lexical Alignment?</title>
      <author><first>Taelin</first><last>Karidi</last></author>
      <author><first>Eitan</first><last>Grossman</last></author>
      <author><first>Omri</first><last>Abend</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <pages>327-341</pages>
      <abstract>The data-driven investigation of the extent to which lexicons of different languages align has mostly fallen into one of two categories:colexification-based and distributional. The two approaches are grounded in distinct methodologies, operate on different assumptions, and are used in diverse ways.This raises two important questions: (a) are there settings in which the predictions of the two approaches can be directly compared? and if so, (b) what is the extent of the similarity and what are its determinants? We offer novel operationalizations for the two approaches in a manner that allows for their direct comparison, and conduct a comprehensive analysis on a diverse set of 16 languages.Our analysis is carried out at different levels of granularity. At the word-level, the two methods present different results across the board. However, intriguingly, at the level of semantic domains (e.g., kinship, quantity), the two methods show considerable convergence in their predictions.A detailed comparison of the metrics against a carefully validated dataset of kinship terms shows that the distributional methods likely capture a more fine-grained alignment than their counterpart colexification-based methods, and may thus be more suited for settings where fewer languages are evaluated.</abstract>
      <url hash="63fdae41">2024.conll-1.26</url>
      <bibkey>karidi-etal-2024-aligning</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>T</fixed-case>ext2<fixed-case>A</fixed-case>fford: Probing Object Affordance Prediction abilities of Language Models solely from Text</title>
      <author><first>Sayantan</first><last>Adak</last></author>
      <author><first>Daivik</first><last>Agrawal</last></author>
      <author><first>Animesh</first><last>Mukherjee</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Somak</first><last>Aditya</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <pages>342-364</pages>
      <abstract>We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs).A growing body of literature shows that PTLMs fail inconsistently and non-intuitively, demonstrating a lack of reasoning and grounding. To take a first step toward quantifying the effect of grounding (or lack thereof), we curate a novel and comprehensive dataset of object affordances – Text2Afford, characterized by 15 affordance classes. Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances. Experimental results reveal that PTLMs exhibit limited reasoning abilities when it comes to uncommon object affordances. We also observe that pre-trained VLMs do not necessarily capture object affordances effectively. Through few-shot fine-tuning, we demonstrate improvement in affordance knowledge in PTLMs and VLMs. Our research contributes a novel dataset for language grounding tasks, and presents insights into LM capabilities, advancing the understanding of object affordances.</abstract>
      <url hash="9bbf748d">2024.conll-1.27</url>
      <bibkey>adak-etal-2024-text2afford</bibkey>
    </paper>
    <paper id="28">
      <title>How Are Metaphors Processed by Language Models? The Case of Analogies</title>
      <author><first>Joanne</first><last>Boisson</last></author>
      <author><first>Asahi</first><last>Ushio</last></author>
      <author><first>Hsuvas</first><last>Borkakoty</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Kiamehr</first><last>Rezaee</last></author>
      <author><first>Dimosthenis</first><last>Antypas</last></author>
      <author><first>Zara</first><last>Siddique</last></author>
      <author><first>Nina</first><last>White</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last><affiliation>Cardiff University</affiliation></author>
      <pages>365-387</pages>
      <abstract>The ability to compare by analogy, metaphorically or not, lies at the core of how humans understand the world and communicate. In this paper, we study the likelihood of metaphoric outputs, and the capability of a wide range of pretrained transformer-based language models to identify metaphors from other types of analogies, including anomalous ones. In particular, we are interested in discovering whether language models recognise metaphorical analogies equally well as other types of analogies, and whether the model size has an impact on this ability. The results show that there are relevant differences using perplexity as a proxy, with the larger models reducing the gap when it comes to analogical processing, and for distinguishing metaphors from incorrect analogies. This behaviour does not result in increased difficulties for larger generative models in identifying metaphors in comparison to other types of analogies from anomalous sentences in a zero-shot generation setting, when perplexity values of metaphoric and non-metaphoric analogies are similar.</abstract>
      <url hash="035f5e6b">2024.conll-1.28</url>
      <bibkey>boisson-etal-2024-metaphors</bibkey>
    </paper>
    <paper id="29">
      <title>Further Compressing Distilled Language Models via Frequency-aware Partial Sparse Coding of Embeddings</title>
      <author><first>Kohki</first><last>Tamura</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Naoki</first><last>Yoshinaga</last><affiliation>Institute of Industrial Science, the University of Tokyo</affiliation></author>
      <author><first>Masato</first><last>Neishi</last></author>
      <pages>388-399</pages>
      <abstract>Although pre-trained language models (PLMs) are effective for natural language understanding (NLU) tasks, they demand a huge computational resource, thus preventing us from deploying them on edge devices. Researchers have therefore applied compression techniques for neural networks, such as pruning, quantization, and knowledge distillation, to the PLMs. Although these generic techniques can reduce the number of internal parameters of hidden layers in the PLMs, the embedding layers tied to the tokenizer arehard to compress, occupying a non-negligible portion of the compressed model. In this study, aiming to further compress PLMs reduced by the generic techniques, we exploit frequency-aware sparse coding to compress the embedding layers of the PLMs fine-tuned to downstream tasks. To minimize the impact of the compression on the accuracy, we retain the embeddings of common tokens as they are and use them to reconstruct embeddings of rare tokens by locally linear mapping. Experimental results on the GLUE and JGLUE benchmarks for language understanding in English and Japanese confirmed that our method can further compress the fine-tuned DistilBERT models models while maintaining accuracy.</abstract>
      <url hash="9d1e10e8">2024.conll-1.29</url>
      <bibkey>tamura-etal-2024-compressing</bibkey>
    </paper>
    <paper id="30">
      <title>Translating Across Cultures: <fixed-case>LLM</fixed-case>s for Intralingual Cultural Adaptation</title>
      <author><first>Pushpdeep</first><last>Singh</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Mayur</first><last>Patidar</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <pages>400-418</pages>
      <abstract>LLMs are increasingly being deployed for multilingual applications and have demonstrated impressive translation capabilities between several low and high-resource languages. An aspect of translation that often gets overlooked is that of cultural adaptation, or modifying source culture references to suit the target culture. While specialized translation models still outperform LLMs on the machine translation task when viewed from the lens of correctness, they are not sensitive to cultural differences often requiring manual correction. LLMs on the other hand have a rich reservoir of cultural knowledge embedded within its parameters that can be potentially exploited for such applications. In this paper, we define the task of cultural adaptation and create an evaluation framework to evaluate the performance of modern LLMs for cultural adaptation and analyze their cross-cultural knowledge while connecting related concepts across different cultures. We also analyze possible issues with automatic adaptation. We hope that this task will offer more insight into the cultural understanding of LLMs and their creativity in cross-cultural scenarios.</abstract>
      <url hash="1a572826">2024.conll-1.30</url>
      <bibkey>singh-etal-2024-translating</bibkey>
    </paper>
    <paper id="31">
      <title>Explaining the Hardest Errors of Contextual Embedding Based Classifiers</title>
      <author><first>Claudio Moisés Valiense De</first><last>Andrade</last><affiliation>Universidade Federal de Minas Gerais, Universidade Federal de Minas Gerais</affiliation></author>
      <author><first>Washington</first><last>Cunha</last><affiliation>Universidade Federal de Minas Gerais and Universidade Federal de Minas Gerais</affiliation></author>
      <author><first>Guilherme</first><last>Fonseca</last></author>
      <author><first>Ana Clara Souza</first><last>Pagano</last></author>
      <author><first>Luana De Castro</first><last>Santos</last></author>
      <author><first>Adriana Silvina</first><last>Pagano</last><affiliation>Universidade Federal de Minas Gerais, Universidade Federal de Minas Gerais</affiliation></author>
      <author><first>Leonardo Chaves Dutra Da</first><last>Rocha</last><affiliation>Universidade Federal de São João del-Rei</affiliation></author>
      <author><first>Marcos André</first><last>Gonçalves</last><affiliation>Universidade Federal de Minas Gerais, Universidade Federal de Minas Gerais</affiliation></author>
      <pages>419-434</pages>
      <abstract>We seek to explain the causes of the misclassification of the most challenging documents, namely those that no classifier using state-of-the-art, very semantically-separable contextual embedding representations managed to predict accurately. To do so, we propose a taxonomy of incorrect predictions, which we used to perform qualitative human evaluation. We posed two (research) questions, considering three sentiment datasets in two different domains – movie and product reviews. Evaluators with two different backgrounds evaluated documents by comparing the predominant sentiment assigned by the model to the label in the gold dataset in order to decide on a likely misclassification reason. Based on a high inter-evaluator agreement (81.7%), we observed significant differences between the product and movie review domains, such as the prevalence of ambivalence in product reviews and sarcasm in movie reviews. Our analysis also revealed an unexpectedly high rate of incorrect labeling in the gold dataset (up to 33%) and a significant amount of incorrect prediction by the model due to a series of linguistic phenomena (including amplified words, contrastive markers, comparative sentences, and references to world knowledge). Overall, our taxonomy and methodology allow us to explain between 80%-85% of the errors with high confidence (agreement) – enabling us to point out where future efforts to improve models should be concentrated.</abstract>
      <url hash="ebecbc77">2024.conll-1.31</url>
      <bibkey>andrade-etal-2024-explaining</bibkey>
    </paper>
    <paper id="32">
      <title>A Multimodal Large Language Model “Foresees” Objects Based on Verb Information but Not Gender</title>
      <author><first>Shuqi</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xufeng</first><last>Duan</last></author>
      <author><first>Zhenguang</first><last>Cai</last></author>
      <pages>435-441</pages>
      <abstract>This study employs the classical psycholinguistics paradigm, the visual world eye-tracking paradigm (VWP), to explore the predictive capabilities of LLAVA, a multimodal large language model (MLLM), and compare them with human anticipatory gaze behaviors. Specifically, we examine the attention weight distributions of LLAVA when presented with visual displays and English sentences containing verb and gender cues. Our findings reveal that LLAVA, like humans, can predictively attend to objects relevant to verbs, but fails to demonstrate gender-based anticipatory attention. Layer-wise analysis indicates that the middle layers of the model are more related to predictive attention than the early or late layers. This study is pioneering in applying psycholinguistic paradigms to compare the multimodal predictive attention of humans and MLLMs, revealing both similarities and differences between them.</abstract>
      <url hash="67a8081f">2024.conll-1.32</url>
      <bibkey>wang-etal-2024-multimodal</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>PRACT</fixed-case>: Optimizing Principled Reasoning and Acting of <fixed-case>LLM</fixed-case> Agent</title>
      <author><first>Zhiwei</first><last>Liu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Weiran</first><last>Yao</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Jianguo</first><last>Zhang</last><affiliation>SalesForce AI Research</affiliation></author>
      <author><first>Zuxin</first><last>Liu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Liangwei</first><last>Yang</last></author>
      <author><first>Rithesh</first><last>R N</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Tian</first><last>Lan</last><affiliation>SalesForce</affiliation></author>
      <author><first>Ming</first><last>Zhu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Juntao</first><last>Tan</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Shirley</first><last>Kokane</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Thai Quoc</first><last>Hoang</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Juan Carlos</first><last>Niebles</last><affiliation>Salesforce Research and Stanford University</affiliation></author>
      <author><first>Shelby</first><last>Heinecke</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Huan</first><last>Wang</last></author>
      <author><first>Silvio</first><last>Savarese</last><affiliation>Salesforce and Stanford University</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <pages>442-446</pages>
      <abstract>We introduce the Principled Reasoning and Acting (PRAct) framework, a novel method for learning and enforcing action principles from trajectory data. Central to our approach is the use of text gradients from a reflection and optimization engine to derive these action principles. To adapt action principles to specific task requirements, we propose a new optimization framework, Reflective Principle Optimization (RPO). After execution, RPO employs a reflector to critique current action principles and an optimizer to update them accordingly.We investigate the RPO framework under two scenarios: Reward-RPO, which uses environmental rewards for reflection, and Self-RPO, which conducts self-reflection without external rewards. Additionally, we developed two RPO methods, RPO-Traj and RPO-Batch, to adapt to different settings.Experimental results across four environments demonstrate that the PRAct agent, leveraging the RPO framework, can effectively learn and apply action principles to enhance performance.</abstract>
      <url hash="e0d7f7e2">2024.conll-1.33</url>
      <bibkey>liu-etal-2024-pract</bibkey>
    </paper>
    <paper id="34">
      <title>Image-conditioned human language comprehension and psychometric benchmarking of visual language models</title>
      <author><first>Subha Nawer</first><last>Pushpita</last></author>
      <author><first>Roger P.</first><last>Levy</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>447-457</pages>
      <abstract>Large language model (LLM)s’ next-word predictions have shown impressive performance in capturing human expectations during real-time language comprehension. This finding has enabled a line of research on psychometric benchmarking of LLMs against human language-comprehension data in order to reverse-engineer humans’ linguistic subjective probability distributions and representations. However, to date, this work has exclusively involved unimodal (language-only) comprehension data, whereas much human language use takes place in rich multimodal contexts. Here we extend psychometric benchmarking to visual language models (VLMs). We develop a novel experimental paradigm, <tex-math>\textit{Image-Conditioned Maze Reading}</tex-math>, in which participants first view an image and then read a text describing an image within the Maze paradigm, yielding word-by-word reaction-time measures with high signal-to-noise ratio and good localization of expectation-driven language processing effects. We find a large facilitatory effect of correct image context on language comprehension, not only for words such as concrete nouns that are directly grounded in the image but even for ungrounded words in the image descriptions. Furthermore, we find that VLM surprisal captures most to all of this effect. We use these findings to benchmark a range of VLMs, showing that models with lower perplexity generally have better psychometric performance, but that among the best VLMs tested perplexity and psychometric performance dissociate. Overall, our work offers new possibilities for connecting psycholinguistics with multimodal LLMs for both scientific and engineering goals.</abstract>
      <url hash="d684404c">2024.conll-1.34</url>
      <bibkey>pushpita-levy-2024-image</bibkey>
    </paper>
    <paper id="35">
      <title>Self-supervised speech representations display some human-like cross-linguistic perceptual abilities</title>
      <author><first>Joselyn</first><last>Rodriguez</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Kamala</first><last>Sreepada</last></author>
      <author><first>Ruolan Leslie</first><last>Famularo</last></author>
      <author><first>Sharon</first><last>Goldwater</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Naomi</first><last>Feldman</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>458-463</pages>
      <abstract>State of the art models in automatic speech recognition have shown remarkable improvements due to modern self-supervised (SSL) transformer-based architectures such as wav2vec 2.0 (Baevski et al., 2020). However, how these models encode phonetic information is still not well understood. We explore whether SSL speech models display a linguistic property that characterizes human speech perception: language specificity. We show that while wav2vec 2.0 displays an overall language specificity effect when tested on Hindi vs. English, it does not resemble human speech perception when tested on finer-grained differences in Hindi speech contrasts.</abstract>
      <url hash="d8f0f12c">2024.conll-1.35</url>
      <bibkey>rodriguez-etal-2024-self</bibkey>
    </paper>
    <paper id="36">
      <title>One-Vs-Rest Neural Network <fixed-case>E</fixed-case>nglish Grapheme Segmentation: A Linguistic Perspective</title>
      <author><first>Samuel</first><last>Rose</last></author>
      <author><first>Nina</first><last>Dethlefs</last><affiliation>University of Hull</affiliation></author>
      <author><first>C.</first><last>Kambhampati</last></author>
      <pages>464-469</pages>
      <abstract>Grapheme-to-Phoneme (G2P) correspondences form foundational frameworks of tasks such as text-to-speech (TTS) synthesis or automatic speech recognition. The G2P process involves taking words in their written form and generating their pronunciation. In this paper, we critique the status quo definition of a grapheme, currently a forced alignment process relating a single character to either a phoneme or a blank unit, that underlies the majority of modern approaches. We develop a linguistically-motivated redefinition from simple concepts such as vowel and consonant count and word length and offer a proof-of-concept implementation based on a multi-binary neural classification task. Our model achieves state-of-the-art results with a 31.86% Word Error Rate on a standard benchmark, while generating linguistically meaningful grapheme segmentations.</abstract>
      <url hash="8affcd70">2024.conll-1.36</url>
      <bibkey>rose-etal-2024-one</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>C</fixed-case>rowd<fixed-case>C</fixed-case>ounter: A benchmark type-specific multi-target counterspeech dataset</title>
      <author><first>Punyajoy</first><last>Saha</last></author>
      <author><first>Abhilash</first><last>Datta</last></author>
      <author><first>Abhik</first><last>Jana</last><affiliation>IIT Bhubaneswar</affiliation></author>
      <author><first>Animesh</first><last>Mukherjee</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <pages>470-488</pages>
      <abstract>Counterspeech presents a viable alternative to banning or suspending users for hate speech while upholding freedom of expression. However, writing effective counterspeech is challenging for moderators/users. Hence, developing suggestion tools for writing counterspeech is the need of the hour. One critical challenge in developing such a tool is the lack of quality and diversity of the responses in the existing datasets. Hence, we introduce a new dataset - CrowdCounter containing 3,425 hate speech-counterspeech pairs spanning six different counterspeech types (empathy, humor, questioning, warning, shaming, contradiction), which is the first of its kind. The design of our annotation platform itself encourages annotators to write type-specific, non-redundant and high-quality counterspeech. We evaluate two frameworks for generating counterspeech responses - vanilla and type-controlled prompts - across four large language models. In terms of metrics, we evaluate the responses using relevance, diversity and quality. We observe that Flan-T5 is the best model in the vanilla framework across different models. Type-specific prompts enhance the relevance of the responses, although they might reduce the language quality. DialoGPT proves to be the best at following the instructions and generating the type-specific counterspeech accurately.</abstract>
      <url hash="0b08e8b7">2024.conll-1.37</url>
      <bibkey>saha-etal-2024-crowdcounter</bibkey>
    </paper>
    <paper id="38">
      <title>Solving the Challenge Set without Solving the Task: On <fixed-case>W</fixed-case>inograd Schemas as a Test of Pronominal Coreference Resolution</title>
      <author><first>Ian</first><last>Porada</last><affiliation>McGill University</affiliation></author>
      <author><first>Jackie CK</first><last>Cheung</last><affiliation>McGill University, Mila Research Institute and Microsoft</affiliation></author>
      <pages>489-506</pages>
      <abstract>Challenge sets such as the Winograd Schema Challenge (WSC) are used to benchmark systems’ ability to resolve ambiguities in natural language. If one assumes as in existing work that solving a given challenge set is at least as difficult as solving some more general task, then high performance on the challenge set should indicate high performance on the general task overall. However, we show empirically that this assumption of difficulty does not always hold. In particular, we demonstrate that despite the strong performance of prompted language models (LMs) on the WSC and its variants, these same modeling techniques perform relatively poorly at resolving certain pronominal ambiguities attested in OntoNotes and related datasets that are perceived to be easier. Motivated by these findings, we propose a method for ensembling a prompted LM with a supervised, task-specific system that is overall more accurate at resolving pronominal coreference across datasets. Finally, we emphasize that datasets involving the same linguistic phenomenon draw on distinct, but overlapping, capabilities, and evaluating on any one dataset alone does not provide a complete picture of a system’s overall capability.</abstract>
      <url hash="34b3151e">2024.conll-1.38</url>
      <bibkey>porada-cheung-2024-solving</bibkey>
    </paper>
    <paper id="39">
      <title>Advancing <fixed-case>A</fixed-case>rabic Sentiment Analysis: <fixed-case>A</fixed-case>r<fixed-case>S</fixed-case>en Benchmark and the Improved Fuzzy Deep Hybrid Network</title>
      <author><first>Yang</first><last>Fang</last><affiliation>Huaibei Normal University</affiliation></author>
      <author><first>Cheng</first><last>Xu</last></author>
      <author><first>Shuhao</first><last>Guan</last></author>
      <author><first>Nan</first><last>Yan</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yuke</first><last>Mei</last><affiliation>Wuhu Institute of Technology</affiliation></author>
      <pages>507-516</pages>
      <abstract>Sentiment analysis is pivotal in Natural Language Processing for understanding opinions and emotions in text. While advancements in Sentiment analysis for English are notable, Arabic Sentiment Analysis (ASA) lags, despite the growing Arabic online user base. Existing ASA benchmarks are often outdated and lack comprehensive evaluation capabilities for state-of-the-art models. To bridge this gap, we introduce ArSen, a meticulously annotated COVID-19-themed Arabic dataset, and the IFDHN, a novel model incorporating fuzzy logic for enhanced sentiment classification. ArSen provides a contemporary, robust benchmark, and IFDHN achieves state-of-the-art performance on ASA tasks. Comprehensive evaluations demonstrate the efficacy of IFDHN using the ArSen dataset, highlighting future research directions in ASA.</abstract>
      <url hash="89277151">2024.conll-1.39</url>
      <bibkey>fang-etal-2024-advancing</bibkey>
    </paper>
    <paper id="40">
      <title>Leveraging a Cognitive Model to Measure Subjective Similarity of Human and <fixed-case>GPT</fixed-case>-4 Written Content</title>
      <author><first>Tyler</first><last>Malloy</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Maria José</first><last>Ferreira</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Fei</first><last>Fang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Cleotilde</first><last>Gonzalez</last></author>
      <pages>517-527</pages>
      <abstract>Cosine similarity between two documents can be computed using token embeddings formed by Large Language Models (LLMs) such as GPT-4, and used to categorize those documents across a range of uses. However, these similarities are ultimately dependent on the corpora used to train these LLMs, and may not reflect subjective similarity of individuals or how their biases and constraints impact similarity metrics. This lack of cognitively-aware personalization of similarity metrics can be particularly problematic in educational and recommendation settings where there is a limited number of individual judgements of category or preference, and biases can be particularly relevant. To address this, we rely on an integration of an Instance-Based Learning (IBL) cognitive model with LLM embeddings to develop the Instance-Based Individualized Similarity (IBIS) metric. This similarity metric is beneficial in that it takes into account individual biases and constraints in a manner that is grounded in the cognitive mechanisms of decision making. To evaluate the IBIS metric, we also introduce a dataset of human categorizations of emails as being either dangerous (phishing) or safe (ham). This dataset is used to demonstrate the benefits of leveraging a cognitive model to measure the subjective similarity of human participants in an educational setting.</abstract>
      <url hash="e868acd5">2024.conll-1.40</url>
      <bibkey>malloy-etal-2024-leveraging</bibkey>
    </paper>
  </volume>
</collection>
