<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.sdp">
  <volume id="1" ingest-date="2024-07-27" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024)</booktitle>
      <editor><first>Tirthankar</first><last>Ghosal</last></editor>
      <editor><first>Amanpreet</first><last>Singh</last></editor>
      <editor><first>Anita</first><last>Waard</last></editor>
      <editor><first>Philipp</first><last>Mayr</last></editor>
      <editor><first>Aakanksha</first><last>Naik</last></editor>
      <editor><first>Orion</first><last>Weller</last></editor>
      <editor><first>Yoonjoo</first><last>Lee</last></editor>
      <editor><first>Shannon</first><last>Shen</last></editor>
      <editor><first>Yanxia</first><last>Qin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <venue>sdp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="7939d09d">2024.sdp-1.0</url>
      <bibkey>sdp-2024-scholarly</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Overview of the Fourth Workshop on Scholarly Document Processing</title>
      <author><first>Tirthankar</first><last>Ghosal</last><affiliation>Oak Ridge National Laboratory</affiliation></author>
      <author><first>Amanpreet</first><last>Singh</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Anita</first><last>De Waard</last></author>
      <author><first>Philipp</first><last>Mayr</last></author>
      <author><first>Aakanksha</first><last>Naik</last><affiliation>Allen Institute for Artificial Intelligence and National Institutes of Health</affiliation></author>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Yoonjoo</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Zejiang</first><last>Shen</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yanxia</first><last>Qin</last><affiliation>National University of Singapore</affiliation></author>
      <pages>1-6</pages>
      <abstract>The workshop on Scholarly Document Processing (SDP) started in 2020 to accelerate research, inform policy and educate the public on natural language processing for scientific text. The fourth iteration of the workshop, SDP24 was held at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL24) as a hybrid event. The SDP workshop saw a great increase in interest, with 57 submissions, of which 28 were accepted. The program consisted of a research track, four invited talks and two shared tasks: 1) DAGPap24: Detecting automatically generated scientific papers and 2) Context24: Multimodal Evidence and Grounding Context Identification for Scientific Claims. The program was geared towards NLP, information extraction, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges.</abstract>
      <url hash="fc8566b0">2024.sdp-1.1</url>
      <bibkey>ghosal-etal-2024-overview</bibkey>
    </paper>
    <paper id="2">
      <title>Overview of the <fixed-case>D</fixed-case>ag<fixed-case>P</fixed-case>ap24 Shared Task on Detecting Automatically Generated Scientific Paper</title>
      <author><first>Savvas</first><last>Chamezopoulos</last><affiliation>Elsevier, USA</affiliation></author>
      <author><first>Drahomira</first><last>Herrmannova</last><affiliation>Elsevier, USA</affiliation></author>
      <author><first>Anita</first><last>De Waard</last></author>
      <author><first>Drahomira</first><last>Herrmannova</last><affiliation>Elsevier, USA</affiliation></author>
      <author><first>Domenic</first><last>Rosati</last><affiliation>Dalhousie University, Canada</affiliation></author>
      <author><first>Yury</first><last>Kashnitsky</last><affiliation>Elsevier, USA</affiliation></author>
      <pages>7-11</pages>
      <abstract>This paper provides an overview of the 2024 ACL Scholarly Document Processing workshop shared task on the detection of automatically generated scientific papers. Unlike our previous task, which focused on the binary classification of whether scientific passages were machine-generated or not, one likely use case for text generation technology in scientific writing is to intersperse human-written text with passages of machine-generated text. We frame the detection problem as a multiclass span classification task: given an expert of text, label token spans in the text as human-written or machine-generated We shared a dataset containing excerpts from human-written papers as well as artificially generated content collected by Elsevier publishing and editorial teams. As a test set, the participants were provided with a corpus of openly accessible human-written as well as generated papers from the same scientific domains of documents. The shared task saw 457 submissions across 28 participating teams and resulted in three published technical reports. We discuss our findings from the shared task in this overview paper.</abstract>
      <url hash="cbf97619">2024.sdp-1.2</url>
      <bibkey>chamezopoulos-etal-2024-overview</bibkey>
    </paper>
    <paper id="3">
      <title>Overview of the Context24 Shared Task on Contextualizing Scientific Claims</title>
      <author><first>Chu Sern Joel</first><last>Chan</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Aakanksha</first><last>Naik</last><affiliation>Allen Institute for Artificial Intelligence and National Institutes of Health</affiliation></author>
      <author><first>Matthew</first><last>Akamatsu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Hanna</first><last>Bekele</last><affiliation>NA</affiliation></author>
      <author><first>Erin</first><last>Bransom</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Ian</first><last>Campbell</last><affiliation>NA</affiliation></author>
      <author><first>Jenna</first><last>Sparks</last></author>
      <pages>12-21</pages>
      <abstract>To appropriately interpret and use scientific claims for sensemaking and decision-making, it is critical to contextualize them, not just with textual evidence that the claim was in fact asserted, but also with key supporting empirical evidence, such as a figure that describes a key result, and methodological details, such as the methods of data collection. Retrieving this contextual information when encountering claims in isolation, away from their source papers, is difficult and time-consuming for humans. Scholarly document processing models could help to contextualize scientific claims, but there is a lack of datasets designed for this task. Thus, we contribute a dataset of 585 scientific claims with gold annotations for supporting figures and tables, and gold text snippets of methodological details, that ground the key results behind each claim and run the Context24 shared task to encourage model development for this task. This report describes details of our dataset construction process, summarizes results from the shared task conducted at the 4th Workshop on Scholarly Document Processing (SDP), and discusses future research directions in this space. To support further research, we also publicly release the dataset on HuggingFace.</abstract>
      <url hash="c2215e7a">2024.sdp-1.3</url>
      <bibkey>chan-etal-2024-overview</bibkey>
    </paper>
    <paper id="4">
      <title>Controllable Citation Sentence Generation with Language Models</title>
      <author><first>Nianlong</first><last>Gu</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Richard</first><last>Hahnloser</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <pages>22-37</pages>
      <abstract>Citation generation aims to generate a citation sentence that refers to a chosen paper in the context of a manuscript. However, a rigid citation generation process is at odds with an author’s desire to control specific attributes, such as 1) the citation intent, e.g., either introducing background information or comparing results, and 2) keywords that should appear in the citation text. To provide these degrees of controllability during citation generation, we propose to integrate the manuscript context, the context of the referenced paper, and the desired control attributes into a structured template and use it to fine-tune a language model (LM) via next-token prediction. We then utilize Proximal Policy Optimization to directly optimize the LM in favor of a high score of our proposed controllability metric. The proposed workflow harmoniously combines citation attribute suggestion and conditional citation generation into one LM, allowing for better user control.</abstract>
      <url hash="d32a3e4f">2024.sdp-1.4</url>
      <bibkey>gu-hahnloser-2024-controllable</bibkey>
    </paper>
    <paper id="5">
      <title>Toward Structured Related Work Generation with Novelty Statements</title>
      <author><first>Kazuya</first><last>Nishimura</last></author>
      <author><first>Kuniaki</first><last>Saito</last><affiliation>Boston University</affiliation></author>
      <author><first>Tosho</first><last>Hirasawa</last><affiliation>Omron Sinic X</affiliation></author>
      <author><first>Yoshitaka</first><last>Ushiku</last><affiliation>Ridge-i, OMRON SINIC X and National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>38-57</pages>
      <abstract>To help readers understand the novelty and the research context, an excellent related work section is structured (<i>i.e.,</i> the section consists of paragraphs determined by categorizing papers into several topics) and includes descriptions of novelty. However, previous studies viewed related work generation as multi-document summarization, and the structure and novelty statement are ignored in such studies. In this paper, we redefine the related work generation task as summarization with structure (<i>i.e.,</i> multiple paragraphs with citation) and novelty statement. For this task, we propose a quality-oriented dataset and evaluation metrics. Experiments evaluated the state-of-the-art language models on our tasks, and we confirmed the issues with the current models and the validity of the evaluation indicators.</abstract>
      <url hash="f9413134">2024.sdp-1.5</url>
      <bibkey>nishimura-etal-2024-toward</bibkey>
      <revision id="1" href="2024.sdp-1.5v1" hash="314eaa78"/>
      <revision id="2" href="2024.sdp-1.5v2" hash="f9413134" date="2024-08-30">Corrects url of code and dataset.</revision>
    </paper>
    <paper id="6">
      <title>Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning</title>
      <author><first>Jun</first><last>Zhuang</last><affiliation>Boise State University and Indiana University Purdue University Indianapolis</affiliation></author>
      <author><first>Casey</first><last>Kennington</last><affiliation>Boise State University</affiliation></author>
      <pages>58-69</pages>
      <abstract>As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models’ fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-strong generalization in the taxonomy classification task.</abstract>
      <url hash="2b212177">2024.sdp-1.6</url>
      <bibkey>zhuang-kennington-2024-understanding</bibkey>
    </paper>
    <paper id="7">
      <title>Beyond Retrieval: Topic-based Alignment of Scientific Papers to Research Proposal</title>
      <author><first>Rudra</first><last>Palit</last></author>
      <author><first>Manasi</first><last>Patwardhan</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <author><first>Gautam</first><last>Shroff</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <pages>70-83</pages>
      <abstract>The inception of a research agenda typically commences with the creation of a comprehensive research proposal. The efficacy of the proposal often hinges on its ability to connect with the existing scientific literature that supports its ideas. To effectively assess the relevance of existing articles to a research proposal, it is imperative to categorize these articles into high-level thematic groups, referred to as topics, that align with the proposal. This paper introduces a novel task of aligning scientific articles, relevant to a proposal, with researcher-provided proposal topics. Additionally, we construct a dataset to serve as a benchmark for this task. We establish human and Large Language Model (LLM) baselines and propose a novel three-stage approach to address this challenge. We synthesize and use pseudo-labels that map proposal topics to text spans from cited articles to train Language Models (LMs) for two purposes: (i) as a retriever, to extract relevant text spans from cited articles for each topic, and (ii) as a classifier, to categorize the articles into the proposal topics. Our retriever-classifier pipeline, which employs very small open-source LMs fine-tuned with our constructed dataset, achieves results comparable to a vanilla paid LLM-based classifier, demonstrating its efficacy. However, a notable gap of 23.57 F1 score between our approach and the human baseline highlights the complexity of this task and emphasizes the need for further research.</abstract>
      <url hash="cf1500a8">2024.sdp-1.7</url>
      <bibkey>palit-etal-2024-beyond</bibkey>
    </paper>
    <paper id="8">
      <title>Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning</title>
      <author><first>Sai</first><last>Munikoti</last><affiliation>Pacific Northwest National Laboratory</affiliation></author>
      <author><first>Anurag</first><last>Acharya</last><affiliation>Pacific Northwest National Laboratory</affiliation></author>
      <author><first>Sridevi</first><last>Wagle</last><affiliation>Pacific Northwest National Laboratory</affiliation></author>
      <author><first>Sameera</first><last>Horawalavithana</last><affiliation>Pacific Northwest National Laboratory</affiliation></author>
      <pages>84-89</pages>
      <abstract>Despite the dramatic progress in Large Language Model (LLM) development, LLMs often provide seemingly plausible but not factual information, often referred to as hallucinations. Retrieval-augmented LLMs provide a non-parametric approach to solve these issues by retrieving relevant information from external data sources and augment the training process. These models help to trace evidence from an externally provided knowledge base allowing the model predictions to be better interpreted and verified. In this work, we critically evaluate these models in their ability to perform in scientific document reasoning tasks. To this end, we tuned multiple such model variants with science-focused instructions and evaluated them on a scientific document reasoning benchmark for the usefulness of the retrieved document passages. Our findings suggest that models justify predictions in science tasks with fabricated evidence and leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication.</abstract>
      <url hash="4d31586e">2024.sdp-1.8</url>
      <bibkey>munikoti-etal-2024-evaluating</bibkey>
    </paper>
    <paper id="9">
      <title>Cited Text Spans for Scientific Citation Text Generation</title>
      <author><first>Xiangci</first><last>Li</last></author>
      <author><first>Yi-Hui</first><last>Lee</last></author>
      <author><first>Jessica</first><last>Ouyang</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>90-104</pages>
      <abstract>An automatic citation generation system aims to concisely and accurately describe the relationship between two scientific articles. To do so, such a system must ground its outputs to the content of the cited paper to avoid non-factual hallucinations. Due to the length of scientific documents, existing abstractive approaches have conditioned only on cited paper <i>abstracts</i>. We demonstrate empirically that the abstract is not always the most appropriate input for citation generation and that models trained in this way learn to hallucinate. We propose to condition instead on the <i>cited text span</i> (CTS) as an alternative to the abstract. Because manual CTS annotation is extremely time- and labor-intensive, we experiment with distant labeling of candidate CTS sentences, achieving sufficiently strong performance to substitute for expensive human annotations in model training, and we propose a human-in-the-loop, keyword-based CTS retrieval approach that makes generating citation texts grounded in the full text of cited papers both promising and practical.</abstract>
      <url hash="bb9962ae">2024.sdp-1.9</url>
      <bibkey>li-etal-2024-cited</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>C</fixed-case>ite<fixed-case>A</fixed-case>ssist: A System for Automated Preprint Citation and <fixed-case>B</fixed-case>ib<fixed-case>T</fixed-case>e<fixed-case>X</fixed-case> Generation</title>
      <author><first>Lars</first><last>Kaesberg</last></author>
      <author><first>Terry</first><last>Ruas</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Jan Philip</first><last>Wahle</last><affiliation>University of Göttingen, Germany</affiliation></author>
      <author><first>Bela</first><last>Gipp</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <pages>105-119</pages>
      <abstract>We present CiteAssist, a system to automate the generation of BibTeX entries for preprints, streamlining the process of bibliographic annotation. Our system extracts metadata, such as author names, titles, publication dates, and keywords, to create standardized annotations within the document. CiteAssist automatically attaches the BibTeX citation to the end of a PDF and links it on the first page of the document so other researchers gain immediate access to the correct citation of the article. This method promotes platform flexibility by ensuring that annotations remain accessible regardless of the repository used to publish or access the preprint. The annotations remain available even if the preprint is viewed externally to CiteAssist. Additionally, the system adds relevant related papers based on extracted keywords to the preprint, providing researchers with additional publications besides those in related work for further reading. Researchers can enhance their preprints organization and reference management workflows through a free and publicly available web interface.</abstract>
      <url hash="5929d1ec">2024.sdp-1.10</url>
      <bibkey>kaesberg-etal-2024-citeassist</bibkey>
    </paper>
    <paper id="11">
      <title>An end-to-end entity recognition and disambiguation framework for identifying Author Affiliation from literature publications</title>
      <author><first>Lianghong</first><last>Lin</last></author>
      <author><first>Wenxixie-c@my.cityu.edu.hk</first><last>Wenxixie-c@my.cityu.edu.hk</last><affiliation>NA</affiliation></author>
      <author><first>Spczili@speed-polyu.edu.hk</first><last>Spczili@speed-polyu.edu.hk</last><affiliation>NA</affiliation></author>
      <author><first>Tianyong</first><last>Hao</last></author>
      <pages>120-129</pages>
      <abstract>Author affiliation information plays a key role in bibliometric analyses and is essential for evaluating studies. However, as author affiliation information has not been standardized, which leads to difficulties such as synonym ambiguity and incomplete data during automated processing. To address the challenge, this paper proposes an end-to-end entity recognition and disambiguation framework for identifying author affiliation from literature publications. For entity disambiguation, an algorithm combining word embedding and spatial embedding is presented considering that author affiliation texts often contain rich geographic information. The disambiguation algorithm utilizes the semantic information and geographic information, which effectively enhances entity recognition and disambiguation effect. In addition, the proposed framework facilitates the effective utilization of the extensive literature in the PubMed database for comprehensive bibliometric analysis. The experimental results verify the robustness and effectiveness of the algorithm.</abstract>
      <url hash="603b4c0f">2024.sdp-1.11</url>
      <bibkey>lin-etal-2024-end</bibkey>
    </paper>
    <paper id="12">
      <title>Utilizing an Ensemble Model with Anomalous Label Smoothing to Detect Generated Scientific Papers</title>
      <author><first>Yuan</first><last>Zhao</last><affiliation>China Telecom</affiliation></author>
      <author><first>Junruo</first><last>Gao</last><affiliation>China Telecom</affiliation></author>
      <author><first>Junlin</first><last>Wang</last><affiliation>China Telecom</affiliation></author>
      <author><first>Gang</first><last>Luo</last><affiliation>China Telecom</affiliation></author>
      <author><first>Liang</first><last>Tang</last></author>
      <pages>130-134</pages>
      <abstract>Generative AI, as it becomes increasingly integrated into our lives, has brought convenience, though some concerns have arisen regarding its potential impact on the rigor and authenticity of scientific research. To encourage the development of robust and reliable automatically-generated scientific text detection systems, the “DAGPap24: Detecting Automatically Generated Scientific Papers” competition was held and shared the same task with the 4th Workshop on Scholarly Document Processing (SDP 2024) to be held at ACL 2024. In the DAGPap24 competition, participants were tasked with constructing a generative text detection model that could accurately distinguish between the human written fragment, the synonym replacement fragment, the ChatGPT rewrite fragment, and the generated summary fragment of a paper. In this competition, we first conducted a comprehensive analysis of the training set to build a generative paper detection model. Then we tried various language models, including SciBERT, ALBERT, DeBERTa, RoBERTa, etc. After that, we introduced an Anomalous Label Smoothing (ALS) method and a majority voting method to improve the final results. Finally, we achieved 0.9948 and 0.9944 F1 scores during the development and testing phases respectively, and we achieved second place in the competition.</abstract>
      <url hash="74dd1a19">2024.sdp-1.12</url>
      <bibkey>zhao-etal-2024-utilizing</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>A</fixed-case>ffil<fixed-case>G</fixed-case>ood: Building reliable institution name disambiguation tools to improve scientific literature analysis</title>
      <author><first>Nicolau</first><last>Duran-Silva</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author><first>Pablo</first><last>Accuosto</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author><first>Piotr</first><last>Przybyła</last></author>
      <author><first>Horacio</first><last>Saggion</last><affiliation>Universitat Pompeu Fabra and Universitat Pompeu Fabra</affiliation></author>
      <pages>135-144</pages>
      <abstract>The accurate attribution of scientific works to research organizations is hindered by the lack of openly available manually annotated data–in particular when multilingual and complex affiliation strings are considered. The AffilGood framework introduced in this paper addresses this gap. We identify three sub-tasks relevant for institution name disambiguation and make available annotated datasets and tools aimed at each of them, including i) a dataset annotated with affiliation spans in noisy automatically-extracted strings; ii) a dataset annotated with named entities for the identification of organizations and their locations; iii) seven datasets annotated with the Research Organization Registry (ROR) identifiers for the evaluation of entity-linking systems. In addition, we describe, evaluate and make available newly developed tools that use these datasets to provide solutions for each of the identified sub-tasks. Our results confirm the value of the developed resources and methods in addressing key challenges in institution name disambiguation.</abstract>
      <url hash="edf83fa4">2024.sdp-1.13</url>
      <bibkey>duran-silva-etal-2024-affilgood</bibkey>
    </paper>
    <paper id="14">
      <title>Metadata Enhancement Using Large Language Models</title>
      <author><first>Hyunju</first><last>Song</last></author>
      <author><first>Steven</first><last>Bethard</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Andrea</first><last>Thomer</last><affiliation>University of Arizona</affiliation></author>
      <pages>145-154</pages>
      <abstract>In the natural sciences, a common form of scholarly document is a physical sample record, which provides categorical and textual metadata for specimens collected and analyzed for scientific research. Physical sample archives like museums and repositories publish these records in data repositories to support reproducible science and enable the discovery of physical samples. However, the success of resource discovery in such interfaces depends on the completeness of the sample records. We investigate approaches for automatically completing the scientific metadata fields of sample records. We apply large language models in zero and few-shot settings and incorporate the hierarchical structure of the taxonomy. We show that a combination of record summarization, bottom-up taxonomy traversal, and few-shot prompting yield F1 as high as 0.928 on metadata completion in the Earth science domain.</abstract>
      <url hash="d39ec89f">2024.sdp-1.14</url>
      <bibkey>song-etal-2024-metadata</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>MISTI</fixed-case>: Metadata-Informed Scientific Text and Image Representation through Contrastive Learning</title>
      <author><first>Pawin</first><last>Taechoyotin</last></author>
      <author><first>Daniel</first><last>Acuna</last><affiliation>Computer Science Department, University of Colorado at Boulder</affiliation></author>
      <pages>155-164</pages>
      <abstract>In scientific publications, automatic representations of figures and their captions can be used in NLP, computer vision, and information retrieval tasks. Contrastive learning has proven effective for creating such joint representations for natural scenes, but its application to scientific imagery and descriptions remains under-explored. Recent open-access publication datasets provide an opportunity to understand the effectiveness of this technique as well as evaluate the usefulness of additional metadata, which are available only in the scientific context. Here, we introduce MISTI, a novel model that uses contrastive learning to simultaneously learn the representation of figures, captions, and metadata, such as a paper’s title, sections, and curated concepts from the PubMed Open Access Subset. We evaluate our model on multiple information retrieval tasks, showing substantial improvements over baseline models. Notably, incorporating metadata doubled retrieval performance, achieving a Recall@1 of 30% on a 70K-item caption retrieval task. We qualitatively explore how metadata can be used to strategically retrieve distinctive representations of the same concept but for different sections, such as introduction and results. Additionally, we show that our model seamlessly handles out-of-domain tasks related to image segmentation. We share our dataset and methods (https://github.com/Khempawin/scientific-image-caption-pair/tree/section-attr) and outline future research directions.</abstract>
      <url hash="cee52d8e">2024.sdp-1.15</url>
      <bibkey>taechoyotin-acuna-2024-misti</bibkey>
    </paper>
    <paper id="16">
      <title>First Steps in Building a Knowledge Base of Mathematical Results</title>
      <author><first>Shrey</first><last>Mishra</last></author>
      <author><first>Yacine</first><last>Brihmouche</last><affiliation>Criteo</affiliation></author>
      <author><first>Théo</first><last>Delemazure</last></author>
      <author><first>Antoine</first><last>Gauquier</last></author>
      <author><first>Pierre</first><last>Senellart</last><affiliation>Ecole Normale Supérieure</affiliation></author>
      <pages>165-174</pages>
      <abstract>This paper explores the initial steps towards extracting information about theorems and proofs from scholarly documents to build a knowledge base of interlinked results. Specifically, we consider two main tasks: extracting results and their proofs from the PDFs of scientific articles and establishing which results are used in the proofs of others across the scientific literature. We discuss the problem statement, methodologies, and preliminary findings employed in both phases of our approach, highlighting the challenges faced.</abstract>
      <url hash="a60c50f1">2024.sdp-1.16</url>
      <bibkey>mishra-etal-2024-first</bibkey>
    </paper>
    <paper id="17">
      <title>AutoRef: Generating Refinements of Reviews Given Guidelines</title>
      <author><first>Soham</first><last>Chitnis</last></author>
      <author><first>Manasi</first><last>Patwardhan</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Ashwin</first><last>Srinivasan</last></author>
      <author><first>Tanmay Tulsidas</first><last>Verlekar</last></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <author><first>Gautam</first><last>Shroff</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <pages>175-190</pages>
      <abstract>When examining reviews of research papers, we can distinguish between two hypothetical referees: the maximally lenient referee who accepts any paper with a vacuous review and the maximally strict one who rejects any paper with an overly pedantic review. Clearly, both are of no practical value. Our interest is in a referee who makes a balanced judgement and provides a review abiding by the guidelines. In this paper, we present a case study of automatic correction of an existing machine-generated or human review. The <tex-math>{\tt{AutoRef}\ }</tex-math> system implements an iterative approach that progressively “refines” a review by attempting to make it more compliant with pre-defined requirements of a “good” review. It implements the following steps: (1) Translate the review requirements into a specification in natural language, of “yes/no” questions; (2) Given a <tex-math>(paper,review)</tex-math> pair, extract answers to the questions; (3) Use the results in (2) to generate a new review; and (4) Return to Step (2) with the paper and the new review. Here, (2) and (3) are implemented by large language model (LLM) based agents. We present a case study using papers and reviews made available for the International Conference on Learning Representations (ICLR). Our initial empirical results suggest that <tex-math>{\tt{AutoRef}\ }</tex-math> progressively improves the compliance of the generated reviews to the specification. Currently designed specification makes <tex-math>{\tt{AutoRef}\ }</tex-math> progressively generate reviews which are stricter, making the decisions more inclined towards “rejections”. This demonstrates the applicability of $AutoRef $ for: (1) The progressive correction of overly lenient reviews, being useful for referees and meta-reviewers; and (2) The generation of progressively stricter reviews for a paper, starting from a vacuous review (“Great paper. Accept.”), facilitating authors when trying to assess weaknesses in their papers.</abstract>
      <url hash="571e03d4">2024.sdp-1.17</url>
      <bibkey>chitnis-etal-2024-tt</bibkey>
    </paper>
    <paper id="18">
      <title>Artificial Intuition: Efficient Classification of Scientific Abstracts</title>
      <author><first>Harsh</first><last>Sakhrani</last></author>
      <author><first>Naseela</first><last>Pervez</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Anirudh</first><last>Ravikumar</last></author>
      <author><first>Fred</first><last>Morstatter</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <author><first>Alexandra</first><last>Graddy-Reed</last></author>
      <author><first>Andrea</first><last>Belz</last><affiliation>University of Southern California</affiliation></author>
      <pages>191-201</pages>
      <abstract>It is desirable to coarsely classify short scientific texts, such as grant or publication abstracts, for strategic insight or research portfolio management. These texts efficiently transmit dense information to experts possessing a rich body of knowledge to aid interpretation. Yet this task is remarkably difficult to automate because of brevity and the absence of context. To address this gap, we have developed a novel approach to generate and appropriately assign coarse domain-specific labels. We show that a Large Language Model (LLM) can provide metadata essential to the task, in a process akin to the augmentation of supplemental knowledge representing human intuition, and propose a workflow. As a pilot study, we use a corpus of award abstracts from the National Aeronautics and Space Administration (NASA). We develop new assessment tools in concert with established performance metrics.</abstract>
      <url hash="26b0a27d">2024.sdp-1.18</url>
      <bibkey>sakhrani-etal-2024-artificial</bibkey>
    </paper>
    <paper id="19">
      <title>Synthetic Context with <fixed-case>LLM</fixed-case> for Entity Linking from Scientific Tables</title>
      <author><first>Yuji</first><last>Oshima</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hiroyuki</first><last>Shindo</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hiroki</first><last>Teranishi</last><affiliation>Nara Institute of Science and Technology, Japan and RIKEN</affiliation></author>
      <author><first>Hiroki</first><last>Ouchi</last><affiliation>NAIST</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>202-214</pages>
      <abstract>Tables in scientific papers contain crucial information, such as experimental results.Entity Linking (EL) is a promising technology that analyses tables and associates them with a knowledge base.EL for table cells requires identifying the referent concept of each cell while understanding the context relevant to each cell in the paper. However, extracting the relevant context from the paper is challenging because the relevant parts are scattered in the main text and captions.This study defines a rule-based method for extracting broad context from the main text, including table captions and sentences that mention the table.Furthermore, we propose synthetic context as a more refined context generated by large language models (LLMs).In a synthetic context, contexts from the entire paper are refined by summarizing, injecting supplemental knowledge, and clarifying the referent concept.We observe this approach improves accuracy for EL by more than 10 points on the S2abEL dataset, and our qualitative analysis suggests potential future works.</abstract>
      <url hash="917dfe9d">2024.sdp-1.19</url>
      <bibkey>oshima-etal-2024-synthetic</bibkey>
    </paper>
    <paper id="20">
      <title>Papilusion at <fixed-case>DAGP</fixed-case>ap24: Paper or Illusion? Detecting <fixed-case>AI</fixed-case>-generated Scientific Papers</title>
      <author><first>Nikita</first><last>Andreev</last><affiliation>Yandex</affiliation></author>
      <author><first>Alexander</first><last>Shirnin</last><affiliation>Higher School of Economics</affiliation></author>
      <author><first>Vladislav</first><last>Mikhailov</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Ekaterina</first><last>Artemova</last><affiliation>Toloka AI</affiliation></author>
      <pages>215-219</pages>
      <abstract>This paper presents Papilusion, an AI-generated scientific text detector developed within the DAGPap24 shared task on detecting automatically generated scientific papers. We propose an ensemble-based approach and conduct ablation studies to analyze the effect of the detector configurations on the performance. Papilusion is ranked 6th on the leaderboard, and we improve our performance after the competition ended, achieving 99.46 (+9.63) of the F1-score on the official test set.</abstract>
      <url hash="424f830e">2024.sdp-1.20</url>
      <bibkey>andreev-etal-2024-papilusion</bibkey>
    </paper>
    <paper id="21">
      <title>Multi-head Span-based Detector for <fixed-case>AI</fixed-case>-generated Fragments in Scientific Papers</title>
      <author><first>German</first><last>Gritsai</last></author>
      <author><first>Ildar</first><last>Khabutdinov</last></author>
      <author><first>Andrey</first><last>Grabovoy</last><affiliation>Moscow Institute of Physics and Technology</affiliation></author>
      <pages>220-225</pages>
      <abstract>This paper describes a system designed to distinguish between AI-generated and human-written scientific excerpts in the DAGPap24 competition hosted within the Fourth Workshop on Scientific Document Processing. In this competition the task is to find artificially generated token-level text fragments in documents of a scientific domain. Our work focuses on the use of a multi-task learning architecture with two heads. The application of this approach is justified by the specificity of the task, where class spans are continuous over several hundred characters. We considered different encoder variations to obtain a state vector for each token in the sequence, as well as a variation in splitting fragments into tokens to further feed into the input of a transform-based encoder. This approach allows us to achieve a 9% quality improvement relative to the baseline solution score on the development set (from 0.86 to 0.95) using the average macro <tex-math>F_{1}</tex-math>-score, as well as a score of 0.96 on a closed test part of the dataset from the competition.</abstract>
      <url hash="cc41c1fc">2024.sdp-1.21</url>
      <bibkey>gritsai-etal-2024-multi</bibkey>
    </paper>
    <paper id="22">
      <title>Guiding Large Language Models via External Attention Prompting for Scientific Extreme Summarization</title>
      <author><first>Yuan</first><last>Chang</last></author>
      <author><first>Ziyue</first><last>Li</last></author>
      <author><first>Xiaoqiu</first><last>Le</last><affiliation>NATIONAL SCIENCE LIBRARY, CHINESE ACADEMY OF SCIENCES</affiliation></author>
      <pages>226-242</pages>
      <abstract>Scientific extreme summarization, the task of generating concise one-sentence summaries (TLDRs) for scientific papers, presents significant challenges due to the need for deep domain-specific understanding and the ability to distill salient information. This study identifies the critical role of titles and keywords in enhancing TLDR generation through quantitative analysis. We propose a novel method, External Attention Prompting (EAP), which leverages LLMs by guiding them to focus on the most critical parts of the source text through varying degrees of attention signals. Our method employs Markdown emphasis syntax to annotate attention levels, enabling LLMs to prioritize salient information effectively. Extensive experiments demonstrate that EAP significantly outperforms baseline methods across various LLMs and metrics in both zero-shot and few-shot settings. Further evaluations by GPT-4 demonstrate that EAP can enable LLMs to generate TLDRs of higher human-aligned quality.</abstract>
      <url hash="46c5c3d3">2024.sdp-1.22</url>
      <bibkey>chang-etal-2024-guiding</bibkey>
    </paper>
    <paper id="23">
      <title>Simulating Expert Discussions with Multi-agent for Enhanced Scientific Problem Solving</title>
      <author><first>Ziyue</first><last>Li</last></author>
      <author><first>Yuan</first><last>Chang</last></author>
      <author><first>Xiaoqiu</first><last>Le</last><affiliation>NATIONAL SCIENCE LIBRARY, CHINESE ACADEMY OF SCIENCES</affiliation></author>
      <pages>243-256</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable potential across various domains, yet their application in addressing complex scientific problems remains a formidable challenge. This paper presents a novel methodology to augment the problem-solving capabilities of LLMs by assigning them roles as domain-specific experts. By simulating a panel of experts, each LLM is tasked with delivering professional and cautious responses to scientific inquiries. Our approach involves querying multiple LLMs and assessing the consistency of their responses. High agreement among the LLMs suggests greater confidence in the proposed solution, whereas discrepancies prompt a collaborative discussion among the LLMs to reach a consensus. This method emulates real-world scientific problem-solving processes, fostering a more reliable and robust mechanism for LLMs to tackle scientific questions. Our experimental results show that assigning roles to multiple LLMs as domain-specific experts significantly improves their accuracy and reliability in solving scientific problems. This framework has the potential to advance the application of AI in scientific research, enhancing its effectiveness and trustworthiness.</abstract>
      <url hash="d272cb59">2024.sdp-1.23</url>
      <bibkey>li-etal-2024-simulating</bibkey>
    </paper>
    <paper id="24">
      <title>An Analysis of Tasks and Datasets in Peer Reviewing</title>
      <author><first>Moritz</first><last>Staudinger</last><affiliation>Technische Universität Wien</affiliation></author>
      <author><first>Wojciech</first><last>Kusa</last><affiliation>Allegro</affiliation></author>
      <author><first>Florina</first><last>Piroi</last><affiliation>Technische Universität Wien</affiliation></author>
      <author><first>Allan</first><last>Hanbury</last><affiliation>Complexity Science Hub and Technische Universität Wien</affiliation></author>
      <pages>257-268</pages>
      <abstract>Taking note of the current challenges of the peer review system, this paper inventories the research tasks for analysing and possibly automating parts of the reviewing process, like matching submissions with a reviewer’s domain of expertise. For each of these tasks we list their associated datasets, analysing their quality in terms of available documentation of creation and use. Building up on this, we give a set of recommendations to take into account when collecting and releasing data.</abstract>
      <url hash="8edd6977">2024.sdp-1.24</url>
      <bibkey>staudinger-etal-2024-analysis</bibkey>
    </paper>
    <paper id="25">
      <title>Zero-shot Scientific Claim Verification Using <fixed-case>LLM</fixed-case>s and Citation Text</title>
      <author><first>Carlos</first><last>Alvarez</last></author>
      <author><first>Maxwell</first><last>Bennett</last></author>
      <author><first>Lucy</first><last>Wang</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>269-276</pages>
      <abstract>Due to rapidly changing and advancing science, it is important to check the veracity of scientific claims and whether they are supported by research evidence. Previous versions of this task depended on supervised training, where labeled datasets were constructed through manual claim writing and evidence identification, sometimes coupled with mining citation relationships in papers. In this work, we investigate whether zero-shot scientific claim verification could be enabled using large language models (LLMs) and distant supervision examples taken directly from citation texts. We derive an in-context learning (ICL) dataset, SCitance, consisting of citation sentences (“citances”), LLM-generated negations, evidence documents, and veracity labels, and find that prompting GPT-4 with ICL examples from this dataset yields comparable performance (within 1 point F1) to previous finetuned models trained on manually curated claim-evidence pairs. Our results suggest that prompting LLMs with citance-evidence pairs directly poses a viable alternative to finetuning scientific claim verification models with manually-curated data.</abstract>
      <url hash="58f02255">2024.sdp-1.25</url>
      <bibkey>alvarez-etal-2024-zero</bibkey>
    </paper>
    <paper id="26">
      <title>Researcher Representations Based on Aggregating Embeddings of Publication Titles: A Case Study in a <fixed-case>J</fixed-case>apanese Academic Database</title>
      <author><first>Hiroyoshi</first><last>Nagao</last></author>
      <author><first>Marie</first><last>Katsurai</last><affiliation>Doshisha University</affiliation></author>
      <pages>277-282</pages>
      <abstract>Constructing researcher representations is crucial for search and recommendation in academic databases. While recent studies presented methods based on knowledge graph embeddings, obtaining a complete graph of academic entities might be sometimes challenging due to the lack of linked data.By contrast, the textual list of publications of each researcher, which represents their research interests and expertise, is usually easy to obtain.Therefore, this study focuses on creating researcher representations based on textual embeddings of their publication titles and assesses their practicality. We aggregate embeddings of each researcher’s multiple publications into a single vector and apply it to research field classification and similar researcher search tasks. We experimented with multiple language models and embedding aggregation methods to compare their performance.From the model perspective, we confirmed the effectiveness of using sentence embedding models and a simple averaging approach.</abstract>
      <url hash="c22d35bd">2024.sdp-1.26</url>
      <bibkey>nagao-katsurai-2024-researcher</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>C</fixed-case>o<fixed-case>SAE</fixed-case>mb: Contrastive Section-aware Aspect Embeddings for Scientific Articles</title>
      <author><first>Shruti</first><last>Singh</last><affiliation>IIT Gandhinagar</affiliation></author>
      <author><first>Mayank</first><last>Singh</last><affiliation>Indian Institute of Technology Gandhinagar</affiliation></author>
      <pages>283-292</pages>
      <abstract>Research papers are long documents that contain information about various aspects such as background, prior work, methodology, and results. Existing works on scientific document representation learning only leverage the title and abstract of the paper. We present CoSAEmb, a model that learns representations from the full text of 97402 scientific papers from the S2ORC dataset. We present a novel supervised contrastive training framework for long documents using triplet loss and margin gradation. Our framework can be used to learn representations of long documents with any existing encoder-only transformer model without retraining it from scratch. CoSAEmb shows improved performance on information retrieval from the paper’s full text in comparison to models trained only on paper titles and abstracts. We also evaluate CoSAEmb on SciRepEval and CSFCube benchmarks, showing comparable performance with existing state-of-the-art models.</abstract>
      <url hash="44929adb">2024.sdp-1.27</url>
      <bibkey>singh-singh-2024-cosaemb</bibkey>
    </paper>
    <paper id="28">
      <title>Integrating Table Representations into Large Language Models for Improved Scholarly Document Comprehension</title>
      <author><first>Buse Sibel</first><last>Korkmaz</last></author>
      <author><first>Antonio</first><last>Del Rio Chanona</last><affiliation>Imperial college london</affiliation></author>
      <pages>293-306</pages>
      <abstract>We address the challenge of interpreting and reasoning over scientific tables with Large Language Models (LLMs), a crucial aspect of scholarly documents. Despite significant progress in natural language processing, the integration of tabular data into scientific LLMs remains limited. We propose an innovative approach leveraging intermediate task pre-training on table question-answering datasets, followed by model adaptation to comprehend tables in computer science literature. Our findings reveal that incorporating table understanding substantially improves the performance of LLMs on scientific literature understanding tasks, which we showcase in peer-review score prediction. This improvement underscores the importance of utilizing tabular data in the training of scientific language models. The code and models are publicly available at [this link](https://github.com/buseskorkmaz/Integrating-Table-Representations-into-LLMs).</abstract>
      <url hash="b41971e1">2024.sdp-1.28</url>
      <bibkey>korkmaz-del-rio-chanona-2024-integrating</bibkey>
    </paper>
    <paper id="29">
      <title>Harnessing <fixed-case>CLIP</fixed-case> for Evidence Identification in Scientific Literature: A Multimodal Approach to Context24 Shared Task</title>
      <author><first>Anukriti</first><last>Kumar</last></author>
      <author><first>Lucy</first><last>Wang</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>307-313</pages>
      <abstract>Knowing whether scientific claims are supported by evidence is fundamental to scholarly communication and evidence-based decision-making. We present our approach to Task 1 of the Context24 Shared Task—Contextualizing Scientific Figures and Tables (SDP@ACL2024), which focuses on identifying multimodal evidence from scientific publications that support claims. We finetune CLIP, a state-of-the-art model for image-text similarity tasks, to identify and rank figures and tables in papers that substantiate specific claims. Our methods focus on text and image preprocessing techniques and augmenting the organizer-provided training data with labeled examples from the SciMMIR and MedICaT datasets. Our best-performing model achieved NDCG@5 and NDCG@10 values of 0.26 and 0.30, respectively, on the Context24 test split. Our findings underscore the effectiveness of data augmentation and preprocessing in improving the model’s ability in evidence matching.</abstract>
      <url hash="0ddf46c7">2024.sdp-1.29</url>
      <bibkey>kumar-wang-2024-harnessing</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>CSIRO</fixed-case> at Context24: Contextualising Scientific Figures and Tables in Scientific Literature</title>
      <author><first>Necva</first><last>Bölücü</last><affiliation>CSIRO</affiliation></author>
      <author><first>Vincent</first><last>Nguyen</last></author>
      <author><first>Roelien</first><last>Timmer</last></author>
      <author><first>Huichen</first><last>Yang</last><affiliation>, CSIRO</affiliation></author>
      <author><first>Maciej</first><last>Rybinski</last></author>
      <author><first>Stephen</first><last>Wan</last><affiliation>CSIRO</affiliation></author>
      <author><first>Sarvnaz</first><last>Karimi</last><affiliation>CSIRO</affiliation></author>
      <pages>314-323</pages>
      <abstract>Finding evidence for claims from content presented in experimental results of scientific articles is difficult. The evidence is often presented in the form of tables and figures, and correctly matching it to scientific claims presents automation challenges. The Context24 shared task is launched to support the development of systems able to verify claims by extracting supporting evidence from articles. We explore different facets of this shared task modelled as a search problem and as an information extraction task. We experiment with a range of methods in each of these categories for the two sub-tasks of evidence identification and grounding context identification in the Context24 shared task.</abstract>
      <url hash="9d2f9f5e">2024.sdp-1.30</url>
      <bibkey>bolucu-etal-2024-csiro</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>OSX</fixed-case> at Context24: How Well Can <fixed-case>GPT</fixed-case> Tackle Contexualizing Scientific Figures and Tables</title>
      <author><first>Tosho</first><last>Hirasawa</last><affiliation>Omron Sinic X</affiliation></author>
      <pages>324-331</pages>
      <abstract>Identifying the alignment between different parts of a scientific paper is fundamental to scholarly document processing.In the Context24 shared task, participants are given a scientific claim and asked to identify (1) key figures or tables that support the claim and (2) methodological details.While employing a supervised approach to train models on task-specific data is a prevailing strategy for both subtasks, such an approach is not feasible for low-resource domains.Therefore, this paper introduces data-free systems supported by Large Language Models.We propose systems based on GPT-4o and GPT-4-turbo for each task.The experimental results reveal the zero-shot capabilities of GPT-4* in both tasks.</abstract>
      <url hash="6bdab086">2024.sdp-1.31</url>
      <bibkey>hirasawa-2024-osx</bibkey>
    </paper>
  </volume>
</collection>
