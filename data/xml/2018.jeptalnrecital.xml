<?xml version='1.0' encoding='UTF-8'?>
<collection id="2018.jeptalnrecital">
  <volume id="long" ingest-date="2020-07-21">
    <meta>
      <booktitle>Actes de la Conférence TALN. Volume 1 - Articles longs, articles courts de TALN</booktitle>
      <editor><first>Pascale</first><last>Sébillot</last></editor>
      <editor><first>Vincent</first><last>Claveau</last></editor>
      <publisher>ATALA</publisher>
      <address>Rennes, France</address>
      <month>5</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="68804f0c">2018.jeptalnrecital-long.0</url>
    </frontmatter>
    <paper id="1">
      <title>Étude de la lisibilité des documents de santé avec des méthodes d’oculométrie (Study of readability of health documents with eye-tracking methods)</title>
      <language>fra</language>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Emmanuel</first><last>Farce</last></author>
      <author><first>Laurent</first><last>Sparrow</last></author>
      <pages>3–18</pages>
      <abstract>Le domaine médical fait partie de la vie quotidienne pour des raisons de santé, mais la disponibilité des informations médicales ne garantit pas leur compréhension correcte par les patients. Plusieurs études ont démontré qu’il existe une difficulté réelle dans la compréhension de contenus médicaux par les patients. Nous proposons d’exploiter les méthodes d’oculométrie pour étudier ces questions et pour détecter quelles unités linguistiques posent des difficultés de compréhension. Pour cela, des textes médicaux en version originale et simplifiée sont exploités. L’oculométrie permet de suivre le regard des participants de l’étude et de révéler les indicateurs de lecture, comme la durée des fixations, les régressions et les saccades. Les résultats indiquent qu’il existe une différence statistiquement significative lors de la lecture des versions originales et simplifiées des documents de santé testés.</abstract>
      <url hash="5308f7bc">2018.jeptalnrecital-long.1</url>
    </paper>
    <paper id="2">
      <title>Alignement de termes de longueur variable en corpus comparables spécialisés (Alignment of variable length terms in specialized comparable corpora)</title>
      <language>fra</language>
      <author><first>Jingshu</first><last>Liu</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Sebastián</first><last>Peña Saldarriaga</last></author>
      <pages>19–32</pages>
      <abstract>Nous proposons dans cet article une adaptation de l’approche compositionnelle étendue capable d’aligner des termes de longueurs variables à partir de corpus comparables, en modifiant la représentation des termes complexes. Nous proposons également de nouveaux modes de pondération pour l’approche standard qui améliorent les résultats des approches état de l’art pour les termes simples et complexes en domaine de spécialité.</abstract>
      <url hash="af1559eb">2018.jeptalnrecital-long.2</url>
    </paper>
    <paper id="3">
      <title>Etude de la reproductibilité des word embeddings : repérage des zones stables et instables dans le lexique (Reproducibility of word embeddings : identifying stable and unstable zones in the semantic space)</title>
      <language>fra</language>
      <author><first>Bénédicte</first><last>Pierrejean</last></author>
      <author><first>Ludovic</first><last>Tanguy</last></author>
      <pages>33–46</pages>
      <abstract>Les modèles vectoriels de sémantique distributionnelle (ou word embeddings), notamment ceux produits par les méthodes neuronales, posent des questions de reproductibilité et donnent des représentations différentes à chaque utilisation, même sans modifier leurs paramètres. Nous présentons ici un ensemble d’expérimentations permettant de mesurer cette instabilité, à la fois globalement et localement. Globalement, nous avons mesuré le taux de variation du voisinage des mots sur trois corpus différents, qui est estimé autour de 17% pour les 25 plus proches voisins d’un mot. Localement, nous avons identifié et caractérisé certaines zones de l’espace sémantique qui montrent une relative stabilité, ainsi que des cas de grande instabilité.</abstract>
      <url hash="7db4eea2">2018.jeptalnrecital-long.3</url>
    </paper>
    <paper id="4">
      <title>Modeling infant segmentation of two morphologically diverse languages</title>
      <author><first>Georgia-Rengina</first><last>Loukatou</last></author>
      <author><first>Sabine</first><last>Stoll</last></author>
      <author><first>Damian</first><last>Blasi</last></author>
      <author><first>Alejandrina</first><last>Cristia</last></author>
      <pages>47–60</pages>
      <abstract>A rich literature explores unsupervised segmentation algorithms infants could use to parse their input, mainly focusing on English, an analytic language where word, morpheme, and syllable boundaries often coincide. Synthetic languages, where words are multi-morphemic, may present unique difficulties for segmentation. Our study tests corpora of two languages selected to differ in the extent of complexity of their morphological structure, Chintang and Japanese. We use three conceptually diverse word segmentation algorithms and we evaluate them on both word- and morpheme-level representations. As predicted, results for the simpler Japanese are better than those for the more complex Chintang. However, the difference is small compared to the effect of the algorithm (with the lexical algorithm outperforming sub-lexical ones) and the level (scores were lower when evaluating on words versus morphemes). There are also important interactions between language, model, and evaluation level, which ought to be considered in future work.</abstract>
      <url hash="e2b3b8ad">2018.jeptalnrecital-long.4</url>
    </paper>
    <paper id="5">
      <title>Évaluation morphologique pour la traduction automatique : adaptation au français (Morphological Evaluation for Machine Translation : Adaptation to <fixed-case>F</fixed-case>rench)</title>
      <language>fra</language>
      <author><first>Franck</first><last>Burlot</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>61–74</pages>
      <abstract>Le nouvel état de l’art en traduction automatique (TA) s’appuie sur des méthodes neuronales, qui différent profondément des méthodes utilisées antérieurement. Les métriques automatiques classiques sont mal adaptées pour rendre compte de la nature du saut qualitatif observé. Cet article propose un protocole d’évaluation pour la traduction de l’anglais vers le français spécifiquement focalisé sur la compétence morphologique des systèmes de TA, en étudiant leurs performances sur différents phénomènes grammaticaux.</abstract>
      <url hash="f3df84c6">2018.jeptalnrecital-long.5</url>
    </paper>
    <paper id="6">
      <title>Étiquetage en parties du discours de langues peu dotées par spécialisation des plongements lexicaux (<fixed-case>POS</fixed-case> tagging for low-resource languages by adapting word embeddings )</title>
      <language>fra</language>
      <author><first>Pierre</first><last>Magistry</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <pages>75–86</pages>
      <abstract>Cet article présente une nouvelle méthode d’étiquetage en parties du discours adaptée aux langues peu dotées : la définition du contexte utilisé pour construire les plongements lexicaux est adaptée à la tâche, et de nouveaux vecteurs sont créés pour les mots inconnus. Les expériences menées sur le picard, le malgache et l’alsacien montrent que cette méthode améliore l’état de l’art pour ces trois langues peu dotées.</abstract>
      <url hash="e63973ed">2018.jeptalnrecital-long.6</url>
    </paper>
    <paper id="7">
      <title>Modélisation des processus d’acquisition syntaxique par jeux de langage entre agents artificiels (Modeling Syntactic Acquisition by Language Games between Artificial Agents )</title>
      <language>fra</language>
      <author><first>Marie</first><last>Marcia</last></author>
      <author><first>Isabelle</first><last>Tellier</last></author>
      <pages>87–100</pages>
      <abstract>Dans cet article, nous présentons une modélisation de la situation d’acquisition de la syntaxe de sa langue maternelle par un enfant inspirée des “jeux de langages” de Luc Steels. Le modèle suppose que l’enfant a accès à une représentation sémantique des énoncés qui lui sont adressés, et qu’il doit réagir en désignant la tête syntaxique de ces énoncés. Nous décrivons des expériences exploitant des données du corpus CHILDES et mettant en jeu un processus d’acquisition simple mais efficace.</abstract>
      <url hash="d98eeedb">2018.jeptalnrecital-long.7</url>
    </paper>
    <paper id="8">
      <title><fixed-case>MOTS</fixed-case> : un outil modulaire pour le résumé automatique (<fixed-case>MOTS</fixed-case> : A Modular Framework for Automatic Summarization )</title>
      <language>fra</language>
      <author><first>Valentin</first><last>Nyzam</last></author>
      <author><first>Christophe</first><last>Rodrigues</last></author>
      <author><first>Aurélien</first><last>Bossard</last></author>
      <pages>101–114</pages>
      <abstract>Cet article présente un système open source et modulaire pour le résumé automatique : MOTS, développé en Java. Son architecture permet d’implémenter et tester de nouvelles méthodes de résumé automatique et de les comparer avec des méthodes existantes dans un cadre unifié. Ce système, le premier complètement modulaire pour le résumé automatique permet à l’heure actuelle de définir plus de cent combinaisons de modules afin de résumer automatiquement des textes en langage naturel.</abstract>
      <url hash="4bba929c">2018.jeptalnrecital-long.8</url>
    </paper>
    <paper id="9">
      <title>Ordonnancement de réponses dans les systèmes de dialogue basé sur une similarité contexte/réponse (Response ranking in dialogue systems based on context-response similarity)</title>
      <language>fra</language>
      <author><first>Basma</first><last>El Amel Boussaha</last></author>
      <author><first>Nicolas</first><last>Hernandez</last></author>
      <author><first>Christine</first><last>Jacquin</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <pages>115–128</pages>
      <abstract>Construire des systèmes de dialogue qui conversent avec les humains afin de les aider dans leurs tâches quotidiennes est devenu une priorité. Certains de ces systèmes produisent des dialogues en cherchant le meilleur énoncé (réponse) parmi un ensemble d’énoncés candidats. Le choix de la réponse est conditionné par l’historique de la conversation appelé contexte. Ces systèmes ordonnent les énoncés candidats par leur adéquation au contexte, le meilleur est ensuite choisi. Les approches existantes à base de réseaux de neurones profonds sont performantes pour cette tâche. Dans cet article, nous améliorons une approche état de l’art à base d’un dual encodeur LSTM. En se basant sur la similarité sémantique entre le contexte et la réponse, notre approche apprend à mieux distinguer les bonnes réponses des mauvaises. Les résultats expérimentaux sur un large corpus de chats d’Ubuntu montrent une amélioration significative de 7, 6 et 2 points sur le Rappel@(1, 2 et 5) respectivement par rapport au meilleur système état de l’art.</abstract>
      <url hash="77da20e0">2018.jeptalnrecital-long.9</url>
    </paper>
    <paper id="10">
      <title>Intégration de contexte global par amorçage pour la détection d’événements (Integrating global context via bootstrapping for event detection)</title>
      <language>fra</language>
      <author><first>Dorian</first><last>Kodelja</last></author>
      <author><first>Romaric</first><last>Besançon</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>129–142</pages>
      <abstract>Les approches neuronales obtiennent depuis plusieurs années des résultats intéressants en extraction d’événements. Cependant, les approches développées dans ce cadre se limitent généralement à un contexte phrastique. Or, si certains types d’événements sont aisément identifiables à ce niveau, l’exploitation d’indices présents dans d’autres phrases est parfois nécessaire pour permettre de désambiguïser des événements. Dans cet article, nous proposons ainsi l’intégration d’une représentation d’un contexte plus large pour améliorer l’apprentissage d’un réseau convolutif. Cette représentation est obtenue par amorçage en exploitant les résultats d’un premier modèle convolutif opérant au niveau phrastique. Dans le cadre d’une évaluation réalisée sur les données de la campagne TAC 2017, nous montrons que ce modèle global obtient un gain significatif par rapport au modèle local, ces deux modèles étant eux-mêmes compétitifs par rapport aux résultats de TAC 2017. Nous étudions également en détail le gain de performance de notre nouveau modèle au travers de plusieurs expériences complémentaires.</abstract>
      <url hash="903c33fa">2018.jeptalnrecital-long.10</url>
    </paper>
    <paper id="11">
      <title>Construction conjointe d’un corpus et d’un classifieur pour les registres de langue en français (Joint building of a corpus and a classifier for language registers in <fixed-case>F</fixed-case>rench)</title>
      <language>fra</language>
      <author><first>Gwénolé</first><last>Lecorvé</last></author>
      <author><first>Hugo</first><last>Ayats</last></author>
      <author><first>Fournier</first><last>Benoît</last></author>
      <author><first>Jade</first><last>Mekki</last></author>
      <author><first>Jonathan</first><last>Chevelu</last></author>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Nicolas</first><last>Béchet</last></author>
      <pages>143–156</pages>
      <abstract>Les registres de langue sont un trait stylistique marquant dans l’appréciation d’un texte ou d’un discours. Cependant, il sont encore peu étudiés en traitement automatique des langues. Dans cet article, nous présentons une approche semi-supervisée permettant la construction conjointe d’un corpus de textes étiquetés en registres et d’un classifieur associé. Cette approche s’appuie sur un ensemble initial et restreint de données expertes. Via une collecte automatique et massive de pages web, l’approche procède par itérations en alternant l’apprentissage d’un classifieur intermédiaire et l’annotation de nouveaux textes pour augmenter le corpus étiqueté. Nous appliquons cette approche aux registres familier, courant et soutenu. À l’issue du processus de construction, le corpus étiqueté regroupe 800 000 textes et le classifieur, un réseau de neurones, présente un taux de bonne classification de 87 %.</abstract>
      <url hash="2df6f729">2018.jeptalnrecital-long.11</url>
    </paper>
    <paper id="12">
      <title>Approche supervisée à base de cellules <fixed-case>LSTM</fixed-case> bidirectionnelles pour la désambiguïsation lexicale (<fixed-case>LSTM</fixed-case> Based Supervised Approach for Word Sense Disambiguation)</title>
      <language>fra</language>
      <author><first>Loïc</first><last>Vial</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <pages>157–170</pages>
      <abstract>En désambiguïsation lexicale, l’utilisation des réseaux de neurones est encore peu présente et très récente. Cette direction est pourtant très prometteuse, tant les résultats obtenus par ces premiers systèmes arrivent systématiquement en tête des campagnes d’évaluation, malgré une marge d’amélioration qui semble encore importante. Nous présentons dans cet article une nouvelle architecture à base de réseaux de neurones pour la désambiguïsation lexicale. Notre système est à la fois moins complexe à entraîner que les systèmes neuronaux existants et il obtient des résultats état de l’art sur la plupart des tâches d’évaluation de la désambiguïsation lexicale en anglais. L’accent est porté sur la reproductibilité de notre système et de nos résultats, par l’utilisation d’un modèle de vecteurs de mots, de corpus d’apprentissage et d’évaluation librement accessibles.</abstract>
      <url hash="c1d0ba56">2018.jeptalnrecital-long.12</url>
    </paper>
    <paper id="13">
      <title>Correction automatique d’attachements prépositionnels par utilisation de traits visuels (<fixed-case>PP</fixed-case>-attachement resolution using visual features)</title>
      <language>fra</language>
      <author><first>Sébastien</first><last>Delecraz</last></author>
      <author><first>Leonor</first><last>Becerra-Bonache</last></author>
      <author><first>Benoît</first><last>Favre</last></author>
      <author><first>Alexis</first><last>Nasr</last></author>
      <author><first>Frédéric</first><last>Bechet</last></author>
      <pages>171–182</pages>
      <abstract>La désambiguïsation des rattachements prépositionnels est une tâche syntaxique qui demande des connaissances sémantiques, pouvant être extraites d’une image associée au texte traité. Nous présentons et analysons les difficultés de cette tâche pour laquelle nous construisons un système complet entraîné sur une version étendue des annotations du corpus Flickr30k Entities. Lorsque la sémantique lexicale n’est pas disponible, l’information visuelle apporte 3 % d’amélioration.</abstract>
      <url hash="979908a2">2018.jeptalnrecital-long.13</url>
    </paper>
    <paper id="14">
      <title>Décodeur neuronal pour la transcription de documents manuscrits anciens (Neural decoder for the transcription of historical handwritten documents)</title>
      <language>fra</language>
      <author><first>Adeline</first><last>Granet</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Harold</first><last>Mouchère</last></author>
      <author><first>Solen</first><last>Quiniou</last></author>
      <author><first>Christian</first><last>Viard-Gaudin</last></author>
      <pages>183–195</pages>
      <abstract>L’absence de données annotées peut être une difficulté majeure lorsque l’on s’intéresse à l’analyse de documents manuscrits anciens. Pour contourner cette difficulté, nous proposons de diviser le problème en deux, afin de pouvoir s’appuyer sur des données plus facilement accessibles. Dans cet article nous présentons la partie décodeur d’un encodeur-décodeur multimodal utilisant l’apprentissage par transfert de connaissances pour la transcription des titres de pièces de la Comédie Italienne. Le décodeur transforme un vecteur de n-grammes au niveau caractères en une séquence de caractères correspondant à un mot. L’apprentissage par transfert de connaissances est réalisé principalement à partir d’une nouvelle ressource inexploitée contemporaine à la Comédie-Italienne et thématiquement proche ; ainsi que d’autres ressources couvrant d’autres domaines, des langages différents et même des périodes différentes. Nous obtenons 97,27% de caractères bien reconnus sur les données de la Comédie-Italienne, ainsi que 86,57% de mots correctement générés malgré une couverture de 67,58% uniquement entre la Comédie-Italienne et l’ensemble d’apprentissage. Les expériences montrent qu’un tel système peut être une approche efficace dans le cadre d’apprentissage par transfert.</abstract>
      <url hash="ea51f528">2018.jeptalnrecital-long.14</url>
    </paper>
  </volume>
  <volume id="court" ingest-date="2020-07-21">
    <meta>
      <booktitle>Actes de la Conférence TALN. Volume 1 - Articles longs, articles courts de TALN</booktitle>
      <editor><first>Pascale</first><last>Sébillot</last></editor>
      <editor><first>Vincent</first><last>Claveau</last></editor>
      <publisher>ATALA</publisher>
      <address>Rennes, France</address>
      <month>5</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="68804f0c">2018.jeptalnrecital-court.0</url>
    </frontmatter>
    <paper id="1">
      <title>A prototype dependency treebank for <fixed-case>B</fixed-case>reton</title>
      <author><first>Francis M</first><last>Tyers</last></author>
      <author><first>Vinit</first><last>Ravishankar</last></author>
      <pages>197–204</pages>
      <abstract>This paper describes the development of the first syntactically-annotated corpus of Breton. The corpus is part of the Universal Dependencies project. In the paper we describe how the corpus was prepared, some Breton-specific constructions that required special treatment, and in addition we give results for parsing Breton using a number of off-the-shelf data-driven parsers.</abstract>
      <url hash="1cdc3289">2018.jeptalnrecital-court.1</url>
    </paper>
    <paper id="2">
      <title>Détection automatique de phrases en domaine de spécialité en français (Sentence boundary detection for specialized domains in <fixed-case>F</fixed-case>rench )</title>
      <language>fra</language>
      <author><first>Arthur</first><last>Boyer</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <pages>205–214</pages>
      <abstract>La détection de frontières de phrase est généralement considéré comme un problème résolu. Cependant, les outils performant sur des textes en domaine général, ne le sont pas forcement sur des domaines spécialisés, ce qui peut engendrer des dégradations de performance des outils intervenant en aval dans une chaîne de traitement automatique s’appuyant sur des textes découpés en phrases. Dans cet article, nous évaluons 5 outils de segmentation en phrase sur 3 corpus issus de différent domaines. Nous ré-entrainerons l’un de ces outils sur un corpus de spécialité pour étudier l’adaptation en domaine. Notamment, nous utilisons un nouveau corpus biomédical annoté spécifiquement pour cette tâche. La detection de frontières de phrase à l’aide d’un modèle OpenNLP entraîné sur un corpus clinique offre une F-mesure de .73, contre .66 pour la version standard de l’outil.</abstract>
      <url hash="19006d35">2018.jeptalnrecital-court.2</url>
    </paper>
    <paper id="3">
      <title>Des représentations continues de mots pour l’analyse d’opinions en arabe: une étude qualitative (Word embeddings for <fixed-case>A</fixed-case>rabic sentiment analysis : a qualitative study)</title>
      <language>fra</language>
      <author><first>Amira</first><last>Barhoumi</last></author>
      <author><first>Nathalie</first><last>Camelin</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <pages>215–224</pages>
      <abstract>Nous nous intéressons, dans cet article, à la détection d’opinions dans la langue arabe. Ces dernières années, l’utilisation de l’apprentissage profond a amélioré des performances de nombreux systèmes automatiques dans une grande variété de domaines (analyse d’images, reconnaissance de la parole, traduction automatique, . . .) et également celui de l’analyse d’opinions en anglais. Ainsi, nous avons étudié l’apport de deux architectures (CNN et LSTM) dans notre cadre spécifique. Nous avons également testé et comparé plusieurs types de représentations continues de mots (embeddings) disponibles en langue arabe, qui ont permis d’obtenir de bons résultats. Nous avons analysé les erreurs de notre système et la pertinence de ces embeddings. Cette analyse mène à plusieurs perspectives intéressantes de travail, au sujet notamment de la constitution automatique de ressources expert et d’une construction pertinente des embeddings spécifiques à la tâche d’analyse d’opinions.</abstract>
      <url hash="6bc32518">2018.jeptalnrecital-court.3</url>
    </paper>
    <paper id="4">
      <title>Evaluation automatique de la satisfaction client à partir de conversations de type “chat” par réseaux de neurones récurrents avec mécanisme d’attention (Customer satisfaction prediction with attention-based <fixed-case>RNN</fixed-case>s from a chat contact center corpus)</title>
      <language>fra</language>
      <author><first>Jeremy</first><last>Auguste</last></author>
      <author><first>Delphine</first><last>Charlet</last></author>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <author><first>Benoit</first><last>Favre</last></author>
      <author><first>Frederic</first><last>Bechet</last></author>
      <pages>225–232</pages>
      <abstract>Cet article présente des méthodes permettant l’évaluation de la satisfaction client à partir de très vastes corpus de conversation de type “chat” entre des clients et des opérateurs. Extraire des connaissances dans ce contexte demeure un défi pour les méthodes de traitement automatique des langues de par la dimension interactive et les propriétés de ce nouveau type de langage à l’intersection du langage écrit et parlé. Nous présentons une étude utilisant des réponses à des sondages utilisateurs comme supervision faible permettant de prédire la satisfaction des usagers d’un service en ligne d’assistance technique et commerciale.</abstract>
      <url hash="db916812">2018.jeptalnrecital-court.4</url>
    </paper>
    <paper id="5">
      <title>Détection d’erreurs dans des transcriptions <fixed-case>OCR</fixed-case> de documents historiques par réseaux de neurones récurrents multi-niveau (Combining character level and word level <fixed-case>RNN</fixed-case>s for post-<fixed-case>OCR</fixed-case> error detection)</title>
      <language>fra</language>
      <author><first>Thibault</first><last>Magallon</last></author>
      <author><first>Frederic</first><last>Bechet</last></author>
      <author><first>Benoit</first><last>Favre</last></author>
      <pages>233–240</pages>
      <abstract>Le traitement à posteriori de transcriptions OCR cherche à détecter les erreurs dans les sorties d’OCR pour tenter de les corriger, deux tâches évaluées par la compétition ICDAR-2017 Post-OCR Text Correction. Nous présenterons dans ce papier un système de détection d’erreurs basé sur un modèle à réseaux récurrents combinant une analyse du texte au niveau des mots et des caractères en deux temps. Ce système a été classé second dans trois catégories évaluées parmi 11 candidats lors de la compétition.</abstract>
      <url hash="fd0aca58">2018.jeptalnrecital-court.5</url>
    </paper>
    <paper id="6">
      <title>Le benchmarking de la reconnaissance d’entités nommées pour le français (Benchmarking for <fixed-case>F</fixed-case>rench <fixed-case>NER</fixed-case>)</title>
      <language>fra</language>
      <author><first>Jungyeul</first><last>Park</last></author>
      <pages>241–250</pages>
      <abstract>Cet article présente une tâche du benchmarking de la reconnaissance de l’entité nommée (REN) pour le français. Nous entrainons et évaluons plusieurs algorithmes d’étiquetage de séquence, et nous améliorons les résultats de REN avec une approche fondée sur l’utilisation de l’apprentissage semi-supervisé et du reclassement. Nous obtenons jusqu’à 77.95%, améliorant ainsi le résultat de plus de 34 points par rapport du résultat de base du modèle.</abstract>
      <url hash="5638bb7c">2018.jeptalnrecital-court.6</url>
    </paper>
    <paper id="7">
      <title>Une note sur l’analyse du constituant pour le français (A Note on constituent parsing for <fixed-case>F</fixed-case>rench)</title>
      <language>fra</language>
      <author><first>Jungyeul</first><last>Park</last></author>
      <pages>251–260</pages>
      <abstract>Cet article traite des analyses d’erreurs quantitatives et qualitatives sur les résultats de l’analyse syntaxique des constituants pour le français. Pour cela, nous étendons l’approche de Kummerfeld et al. (2012) pour français, et nous présentons les détails de l’analyse. Nous entraînons les systèmes d’analyse syntaxique statistiques et neuraux avec le corpus arboré pour français, et nous évaluons les résultats d’analyse. Le corpus arboré pour le français fournit des étiquettes syntagmatiques à grain fin, et les caractéristiques grammaticales du corpus affectent des erreurs d’analyse syntaxique.</abstract>
      <url hash="a948d7dc">2018.jeptalnrecital-court.7</url>
    </paper>
    <paper id="8">
      <title>Interface syntaxe-sémantique au moyen d’une grammaire d’arbres adjoints pour l’étiquetage sémantique de l’arabe (Syntax-semantic interface using Tree-adjoining grammar for <fixed-case>A</fixed-case>rabic semantic labeling)</title>
      <language>fra</language>
      <author><first>Cherifa</first><last>Ben Khelil</last></author>
      <author><first>Chiraz</first><last>Ben Othmane Zribi</last></author>
      <author><first>Denys</first><last>Duchier</last></author>
      <author><first>Yannick</first><last>Parmentier</last></author>
      <pages>261–270</pages>
      <abstract>Dans une grammaire formelle, le lien entre l’information sémantique et sa structure syntaxique correspondante peut être établi en utilisant une interface syntaxe/sémantique qui permettra la construction du sens de la phrase. L’étiquetage de rôles sémantiques aide à réaliser cette tâche en associant automatiquement des rôles sémantiques à chaque argument du prédicat d’une phrase. Dans ce papier, nous présentons une nouvelle approche qui permet la construction d’une telle interface pour une grammaire d’arbres adjoints de l’arabe. Cette grammaire a été générée semi automatiquement à partir d’une méta-grammaire. Nous détaillons le processus d’interfaçage entre le niveau syntaxique et le niveau sémantique moyennant la sémantique des cadres et comment avons-nous procédé à l’étiquetage de rôles sémantiques en utilisant la ressource lexicale ArabicVerbNet.</abstract>
      <url hash="356f57e5">2018.jeptalnrecital-court.8</url>
    </paper>
    <paper id="9">
      <title><fixed-case>F</fixed-case>in<fixed-case>S</fixed-case>enti<fixed-case>A</fixed-case>: Sentiment Analysis in <fixed-case>E</fixed-case>nglish Financial Microblogs</title>
      <author><first>Thomas</first><last>Gaillat</last></author>
      <author><first>Annanda</first><last>Sousa</last></author>
      <author><first>Manel</first><last>Zarrouk</last></author>
      <author><first>Brian</first><last>Davis</last></author>
      <pages>271–280</pages>
      <abstract>FinSentiA: Sentiment Analysis in English Financial Microblogs The objective of this paper is to report on the building of a Sentiment Analysis (SA) system dedicated to financial microblogs in English. The purpose of our work is to build a financial classifier that predicts the sentiment of stock investors in microblog platforms such as StockTwits and Twitter. Our contribution shows that it is possible to conduct such tasks in order to provide fine grained SA of financial microblogs. We extracted financial entities with relevant contexts and assigned scores on a continuous scale by adopting a deep learning method for the classification.</abstract>
      <url hash="eb81e6b4">2018.jeptalnrecital-court.9</url>
    </paper>
    <paper id="10">
      <title>L’optimisation du plongement de mots pour le français : une application de la classification des phrases (Optimization of Word Embeddings for <fixed-case>F</fixed-case>rench : an Application of Sentence Classification)</title>
      <language>fra</language>
      <author><first>Jungyeul</first><last>Park</last></author>
      <pages>281–292</pages>
      <abstract>Nous proposons trois nouvelles méthodes pour construire et optimiser des plongements de mots pour le français. Nous utilisons les résultats de l’étiquetage morpho-syntaxique, de la détection des expressions multi-mots et de la lemmatisation pour un espace vectoriel continu. Pour l’évaluation, nous utilisons ces vecteurs sur une tâche de classification de phrases et les comparons avec le vecteur du système de base. Nous explorons également l’approche d’adaptation de domaine pour construire des vecteurs. Malgré un petit nombre de vocabulaires et la petite taille du corpus d’apprentissage, les vecteurs spécialisés par domaine obtiennent de meilleures performances que les vecteurs hors domaine.</abstract>
      <url hash="9a8e8a57">2018.jeptalnrecital-court.10</url>
    </paper>
    <paper id="11">
      <title><fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec vs <fixed-case>LSA</fixed-case> pour la détection des erreurs orthographiques produisant un dérèglement sémantique en arabe (<fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec vs <fixed-case>LSA</fixed-case> for detecting semantic errors in <fixed-case>A</fixed-case>rabic language)</title>
      <language>fra</language>
      <author><first>Chiraz</first><last>Ben Othmane Zribi</last></author>
      <pages>293–302</pages>
      <abstract>Les mots en arabe sont très proches lexicalement les uns des autres. La probabilité de tomber sur un mot correct en commettant une erreur typographique est plus importante que pour le français ou pour l’anglais. Nous nous intéressons dans cet article à détecter les erreurs orthographiques plus précisément, celles générant des mots lexicalement corrects mais causant un dérèglement sémantique au niveau de la phrase. Nous décrivons et comparons deux méthodes se basant sur la représentation vectorielle du sens des mots. La première méthode utilise l’analyse sémantique latente (LSA). La seconde s’appuie sur le modèle Word2Vec et plus particulièrement l’architecture Skip-Gram. Les expérimentations ont montré que Skip-Gram surpasse LSA.</abstract>
      <url hash="ed990b92">2018.jeptalnrecital-court.11</url>
    </paper>
    <paper id="12">
      <title>Analyse de sentiments à base d’aspects par combinaison de réseaux profonds : application à des avis en français (A combination of deep learning methods for aspect-based sentiment analysis : application to <fixed-case>F</fixed-case>rench reviews)</title>
      <language>fra</language>
      <author><first>Nihel</first><last>Kooli</last></author>
      <author><first>Erwan</first><last>Pigneul</last></author>
      <pages>303–310</pages>
      <abstract>Cet article propose une approche d’analyse de sentiments à base d’aspects dans un texte d’opinion. Cette approche se base sur deux étapes principales : l’extraction d’aspects et la classification du sentiment relatif à chaque aspect. Pour l’extraction d’aspects, nous proposons une nouvelle approche qui combine un CNN pour l’apprentissage de représentation de caractères, un b-LSTM pour joindre l’apprentissage de représentation de caractères et de mots et un CRF pour l’étiquetage des séquences de mots en entités. Pour la classification de sentiments, nous utilisons un réseau à mémoire d’attention pour associer un sentiment (positif, négatif ou neutre) à une expression d’aspect donnée. Les expérimentations sur des corpus d’avis (publics et industriels) en langue française ont montré des performances qui dépassent les méthodes existantes.</abstract>
      <url hash="39a55278">2018.jeptalnrecital-court.12</url>
    </paper>
    <paper id="13">
      <title>Predicting the Semantic Textual Similarity with <fixed-case>S</fixed-case>iamese <fixed-case>CNN</fixed-case> and <fixed-case>LSTM</fixed-case></title>
      <author><first>Elvys</first><last>Linhares Pontes</last></author>
      <author><first>Stéphane</first><last>Huet</last></author>
      <author><first>Andréa</first><last>Carneiro Linhares</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <pages>311–320</pages>
      <abstract>Semantic Textual Similarity (STS) is the basis of many applications in Natural Language Processing (NLP). Our system combines convolution and recurrent neural networks to measure the semantic similarity of sentences. It uses a convolution network to take account of the local context of words and an LSTM to consider the global context of sentences. This combination of networks helps to preserve the relevant information of sentences and improves the calculation of the similarity between sentences. Our model has achieved good results and is competitive with the best state-of-the-art systems.</abstract>
      <url hash="4b1738f9">2018.jeptalnrecital-court.13</url>
    </paper>
    <paper id="14">
      <title>L’évaluation des représentations vectorielles de mots en utilisant <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et (Evaluating word representations using <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et)</title>
      <language>fra</language>
      <author><first>Nourredine</first><last>Aliane</last></author>
      <author><first>Jean-Jacques</first><last>Mariage</last></author>
      <author><first>Gilles</first><last>Bernard</last></author>
      <pages>321–328</pages>
      <abstract>Les méthodes d’évaluation actuelles des représentations vectorielles de mots utilisent généralement un jeu de données restreint et biaisé. Pour pallier à ce problème nous présentons une nouvelle approche, basée sur la similarité entre les synsets associés aux mots dans la volumineuse base de données lexicale WordNet. Notre méthode d’évaluation consiste dans un premier temps à classer automatiquement les représentions vectorielles de mots à l’aide d’un algorithme de clustering, puis à évaluer la cohérence sémantique et syntaxique des clusters produits. Cette évaluation est effectuée en calculant la similarité entre les mots de chaque cluster, pris deux à deux, en utilisant des mesures de similarité entre les mots dans WordNet proposées par NLTK (wup _similarity). Nous obtenons, pour chaque cluster, une valeur entre 0 et 1. Un cluster dont la valeur est 1 est un cluster dont tous les mots appartiennent au même synset. Nous calculons ensuite la moyenne des mesures de tous les clusters. Nous avons utilisé notre nouvelle approche pour étudier et comparer trois méthodes de représentations vectorielles : une méthode traditionnelle, WebSOM et deux méthodes récentes, word2vec (Skip-Gram et CBOW) et GloVe, sur trois corpus : en anglais, en français et en arabe.</abstract>
      <url hash="619ca0ee">2018.jeptalnrecital-court.14</url>
    </paper>
    <paper id="15">
      <title>Traduction automatique de corpus en anglais annotés en sens pour la désambiguïsation lexicale d’une langue moins bien dotée, l’exemple de l’arabe (Automatic Translation of <fixed-case>E</fixed-case>nglish Sense Annotated Corpora for Word Sense Disambiguation of a Less Well-endowed Language, the Example of <fixed-case>A</fixed-case>rabic)</title>
      <language>fra</language>
      <author><first>Marwa</first><last>Hadj Salah</last></author>
      <author><first>Loïc</first><last>Vial</last></author>
      <author><first>Hervé</first><last>Blanchon</last></author>
      <author><first>Mounir</first><last>Zrigui</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <pages>329–336</pages>
      <abstract>Les corpus annotés en sens sont des ressources cruciales pour la tâche de désambiguïsation lexicale (Word Sense Disambiguation). La plupart des langues n’en possèdent pas ou trop peu pour pouvoir construire des systèmes robustes. Nous nous intéressons ici à la langue arabe et présentons 12 corpus annotés en sens, fabriqués automatiquement à partir de 12 corpus en langue anglaise. Nous évaluons la qualité de nos systèmes de désambiguïsation grâce à un corpus d’évaluation en arabe nouvellement disponible.</abstract>
      <url hash="23b8eb51">2018.jeptalnrecital-court.15</url>
    </paper>
    <paper id="16">
      <title>Détection de mésusages de médicaments dans les réseaux sociaux (Detection of drug misuse in social media)</title>
      <language>fra</language>
      <author><first>Elise</first><last>Bigeard</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Frantz</first><last>Thiessard</last></author>
      <pages>337–346</pages>
      <abstract>Un mésusage apparaît lorsqu’un patient ne respecte pas sa prescription et fait des actions pouvant mener à des effets nocifs. Bien que ces situations soient dangereuses, les patients ne signalent généralement pas les mésusages à leurs médecins. Il est donc nécessaire d’étudier d’autres sources d’information pour découvrir ce qui se passe en réalité. Nous proposons d’étudier les forums de santé en ligne. L’objectif de notre travail consiste à explorer les forums de santé avec des méthodes de classification supervisée afin d’identifier les messages contenant un mésusage de médicament. Notre méthode permet de détecter les mésusages avec une F-mesure allant jusqu’à 0,810. Cette méthode peut aider dans la détection de mésusages et la construction d’un corpus exploitable par les experts pour étudier les types de mésusages commis par les patients.</abstract>
      <url hash="8c3e72f6">2018.jeptalnrecital-court.16</url>
    </paper>
    <paper id="17">
      <title>Utilisation de Représentations Distribuées de Relations pour la Désambiguïsation d’Entités Nommées (Exploiting Relation Embeddings to Improve Entity Linking )</title>
      <language>fra</language>
      <author><first>Nicolas</first><last>Wagner</last></author>
      <author><first>Romaric</first><last>Besançon</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>347–356</pages>
      <abstract>L’identification des entités nommées dans un texte est une étape fondamentale pour de nombreuses tâches d’extraction d’information. Pour avoir une identification complète, une étape de désambiguïsation des entités similaires doit être réalisée. Celle-ci s’appuie souvent sur la seule description textuelle des entités. Or, les bases de connaissances contiennent des informations plus riches, sous la forme de relations entre les entités : cette information peut également être exploitée pour améliorer la désambiguïsation des entités. Nous proposons dans cet article une approche d’apprentissage de représentations distribuées de ces relations et leur utilisation pour la tâche de désambiguïsation d’entités nommées. Nous montrons le gain de cette méthode sur un corpus d’évaluation standard, en anglais, issu de la tâche de désambiguïsation d’entités de la campagne TAC-KBP.</abstract>
      <url hash="d012a7d8">2018.jeptalnrecital-court.17</url>
    </paper>
    <paper id="18">
      <title>Traduction automatique du japonais vers le français Bilan et perspectives (Machine Translation from <fixed-case>J</fixed-case>apanese to <fixed-case>F</fixed-case>rench - Review and Prospects)</title>
      <language>fra</language>
      <author><first>Raoul</first><last>Blin</last></author>
      <pages>357–364</pages>
      <abstract>Nous étudions la possibilité de construire un dispositif de traduction automatique neuronale du japonais vers le français, capable d’obtenir des résultats à la hauteur de l’état de l’art, sachant que l’on ne peut disposer de grands corpus alignés bilingues. Nous proposons un état de l’art et relevons de nombreux signes d’amélioration de la qualité des traductions, en comparaison aux traductions statistiques jusque-là prédominantes. Nous testons ensuite un des baselines librement disponibles, OpenNMT, qui produit des résultats encourageants. Sur la base de cette expérience, nous proposons plusieurs pistes pour améliorer à terme la traduction et pour compenser le manque de corpus.</abstract>
      <url hash="1d373233">2018.jeptalnrecital-court.18</url>
    </paper>
    <paper id="19">
      <title>Des pseudo-sens pour améliorer l’extraction de synonymes à partir de plongements lexicaux (Pseudo-senses for improving the extraction of synonyms from word embeddings)</title>
      <language>fra</language>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>365–374</pages>
      <abstract>Au-delà des modèles destinés à construire des plongements lexicaux à partir de corpus, des méthodes de spécialisation de ces représentations selon différentes orientations ont été proposées. Une part importante d’entre elles repose sur l’utilisation de connaissances externes. Dans cet article, nous proposons Pseudofit, une nouvelle méthode de spécialisation de plongements lexicaux focalisée sur la similarité sémantique et opérant sans connaissances externes. Pseudofit s’appuie sur la notion de pseudo-sens afin d’obtenir plusieurs représentations pour un même mot et utilise cette pluralité pour rendre plus génériques les plongements initiaux. Nous illustrons l’intérêt de Pseudofit pour l’extraction de synonymes et nous explorons dans ce cadre différentes variantes visant à en améliorer les résultats.</abstract>
      <url hash="03ca5cd9">2018.jeptalnrecital-court.19</url>
    </paper>
    <paper id="20">
      <title>Annotation automatique des types de discours dans des livres audio en vue d’une oralisation par un système de synthèse (Automatic annotation of discourse types in audio-books)</title>
      <language>fra</language>
      <author><first>Aghilas</first><last>Sini</last></author>
      <author><first>Elisabeth</first><last>Delais-Roussarie</last></author>
      <author><first>Damien</first><last>Lolive</last></author>
      <pages>375–382</pages>
      <abstract>Pour synthétiser automatiquement et de manière expressive des livres audio, il est nécessaire de connaître le type des discours à oraliser. Ceci étant, dans un roman ou une nouvelle, les perspectives narratives et les types de discours évoluent souvent entre de la narration, du récitatif, du discours direct, du discours rapporté, voire des dialogues. Dans ce travail, nous allons présenter un outil qui a été développé à partir de l’analyse d’un corpus de livres audio (extraits de Madame Bovary et des Mystères de Paris) et qui prend comme unité de base pour l’analyse le paragraphe. Cet outil permet donc non seulement de déterminer automatiquement les types de discours (narration, discours direct, dialogue), et donc de savoir qui parle, mais également d’annoter l’extension des modifications discursives. Ce dernier point est important, notamment dans le cas d’incises de citation où le narrateur reprend la parole dans une séquence au discours direct. Dans sa forme actuelle, l’outil atteint un taux de 89 % de bonne détection.</abstract>
      <url hash="850a5326">2018.jeptalnrecital-court.20</url>
    </paper>
    <paper id="21">
      <title>Impact du Prétraitement Linguistique sur l’Analyse de Sentiment du Dialecte Tunisien ()</title>
      <language>fra</language>
      <author><first>Chedi</first><last>Bechikh Ali</last></author>
      <author><first>Hala</first><last>Mulki</last></author>
      <author><first>Hatem</first><last>Haddad</last></author>
      <pages>383–392</pages>
      <abstract/>
      <url hash="3390d347">2018.jeptalnrecital-court.21</url>
    </paper>
    <paper id="22">
      <title>Detecting context-dependent sentences in parallel corpora</title>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <pages>393–400</pages>
      <abstract>In this article, we provide several approaches to the automatic identification of parallel sentences that require sentence-external linguistic context to be correctly translated. Our long-term goal is to automatically construct a test set of context-dependent sentences in order to evaluate machine translation models designed to improve the translation of contextual, discursive phenomena. We provide a discussion and critique that show that current approaches do not allow us to achieve our goal, and suggest that for now evaluating individual phenomena is likely the best solution.</abstract>
      <url hash="79c511fc">2018.jeptalnrecital-court.22</url>
    </paper>
    <paper id="23">
      <title>Predicting failure of a mediated conversation in the context of asymetric role dialogues</title>
      <author><first>Romain</first><last>Carbou</last></author>
      <author><first>Delphine</first><last>Charlet</last></author>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <author><first>Frédéric</first><last>Landragin</last></author>
      <author><first>Jean</first><last>Léon Bouraoui</last></author>
      <pages>401–408</pages>
      <abstract>In a human-to-human conversation between a user and his interlocutor in an assistance center, we suppose a context where the conclusion of the dialog can characterize a notion of success or failure, explicitly annotated or deduced. The study involves different approaches expected to have an influence on predictive classification model of failures. On the one hand, we will aim at taking into account the asymmetry of the speakers’ roles in the modelling of the lexical distribution. On the other hand, we will determine whether the part of the lexicon most closely relating to the domain of customer assistance studied here, modifies the quality of the prediction. We will eventually assess the perspectives of generalization to morphologically comparable corpora.</abstract>
      <url hash="b8fcc82d">2018.jeptalnrecital-court.23</url>
    </paper>
    <paper id="24">
      <title>Portée de la négation : détection par apprentissage supervisé en français et portugais brésilien (Negation scope : sequence labeling by supervised learning in <fixed-case>F</fixed-case>rench and <fixed-case>B</fixed-case>razilian-<fixed-case>P</fixed-case>ortuguese)</title>
      <language>fra</language>
      <author><first>Clément</first><last>Dalloux</last></author>
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Claudia</first><last>Moro</last></author>
      <pages>409–418</pages>
      <abstract>La détection automatique de la négation fait souvent partie des pré-requis dans les systèmes d’extraction d’information, notamment dans le domaine biomédical. Cet article présente nos contributions concernant la détection de la portée de la négation en français et portugais brésilien. Nous présentons d’une part deux corpus principalement constitués d’extraits de protocoles d’essais cliniques en français et portugais brésilien, dédiés aux critères d’inclusion de patients. Les marqueurs de négation et leurs portées y ont été annotés manuellement. Nous présentons d’autre part une approche par réseau de neurones récurrents pour extraire les portées.</abstract>
      <url hash="b5749924">2018.jeptalnrecital-court.24</url>
    </paper>
    <paper id="25">
      <title>Le corpus <fixed-case>PASTEL</fixed-case> pour le traitement automatique de cours magistraux (<fixed-case>PASTEL</fixed-case> corpus for automatic processing of lectures)</title>
      <language>fra</language>
      <author><first>Salima</first><last>Mdhaffar</last></author>
      <author><first>Antoine</first><last>Laurent</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <pages>419–426</pages>
      <abstract>Le projet PASTEL étudie l’acceptabilité et l’utilisabilité des transcriptions automatiques dans le cadre d’enseignements magistraux. Il s’agit d’outiller les apprenants pour enrichir de manière synchrone et automatique les informations auxquelles ils peuvent avoir accès durant la séance. Cet enrichissement s’appuie sur des traitements automatiques du langage naturel effectués sur les transcriptions automatiques. Nous présentons dans cet article un travail portant sur l’annotation d’enregistrements de cours magistraux enregistrés dans le cadre du projet CominOpenCourseware. Ces annotations visent à effectuer des expériences de transcription automatique, segmentation thématique, appariement automatique en temps réel avec des ressources externes... Ce corpus comprend plus de neuf heures de parole annotées. Nous présentons également des expériences préliminaires réalisées pour évaluer l’adaptation automatique de notre système de reconnaissance de la parole.</abstract>
      <url hash="29c24864">2018.jeptalnrecital-court.25</url>
    </paper>
    <paper id="26">
      <title>Apprendre de la littérature scientifique : Les réseaux de signalisation en biologie systémique (Literature-based discovery: Signaling Systems in Systemic Biology)</title>
      <language>fra</language>
      <author><first>Flavie</first><last>Landomiel</last></author>
      <author><first>Cathy</first><last>Guérineau</last></author>
      <author><first>Anubhav</first><last>Gupta</last></author>
      <author><first>Denis</first><last>Maurel</last></author>
      <author><first>Anne</first><last>Poupon</last></author>
      <pages>427–436</pages>
      <abstract>Cet article a pour but de montrer la faisabilité d’un système de fouille de texte pour alimenter un moteur d’inférences capable de construire, à partir de prédicats extraits des articles scientifiques, un réseau de signalisation en biologie systémique. Cette fouille se réalise en deux étapes : la recherche de phrases d’intérêt dans un grand corpus scientifique, puis la construction automatique de prédicats. Ces deux étapes utilisent un système de cascades de transducteurs.</abstract>
      <url hash="df96d044">2018.jeptalnrecital-court.26</url>
    </paper>
    <paper id="27">
      <title>Détection des couples de termes translittérés à partir d’un corpus parallèle anglais-arabe ()</title>
      <language>fra</language>
      <author><first>Wafa</first><last>Neifar</last></author>
      <author><first>Thierry</first><last>Hamon</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <author><first>Mariem</first><last>Ellouze</last></author>
      <author><first>Lamia-Hadrich</first><last>Belguith</last></author>
      <pages>437–446</pages>
      <abstract/>
      <url hash="977e3b7c">2018.jeptalnrecital-court.27</url>
    </paper>
    <paper id="28">
      <title>Utilisation d’une base de connaissances de spécialité et de sens commun pour la simplification de comptes-rendus radiologiques (Radiological text simplification using a general knowledge base)</title>
      <language>fra</language>
      <author><first>Lionel</first><last>Ramadier</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <pages>447–454</pages>
      <abstract>Dans le domaine médical, la simplification des textes est à la fois une tâche souhaitable pour les patients et scientifiquement stimulante pour le domaine du traitement automatique du langage naturel. En effet, les comptes rendus médicaux peuvent être difficile à comprendre pour les non spécialistes, essentiellement à cause de termes médicaux spécifiques (prurit, par exemple). La substitution de ces termes par des mots du langage courant peut aider le patient à une meilleure compréhension. Dans cet article, nous présentons une méthode de simplification dans le domaine médical (en français) basée sur un réseau lexico-sémantique. Nous traitons cette difficulté sémantique par le remplacement du terme médical difficile par un synonyme ou terme qui lui est lié sémantiquement à l’aide d’un réseau lexico-sémantique français. Nous présentons dans ce papier, une telle méthode ainsi que son évaluation.</abstract>
      <url hash="ca50a0cb">2018.jeptalnrecital-court.28</url>
    </paper>
    <paper id="29">
      <title>Algorithmes à base d’échantillonage pour l’entraînement de modèles de langue neuronaux (Here the title in <fixed-case>E</fixed-case>nglish)</title>
      <language>fra</language>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <pages>455–464</pages>
      <abstract>L’estimation contrastive bruitée (NCE) et l’échantillonage par importance (IS) sont des procédures d’entraînement basées sur l’échantillonage, que l’on utilise habituellement à la place de l’estimation du maximum de vraisemblance (MLE) pour éviter le calcul du softmax lorsque l’on entraîne des modèles de langue neuronaux. Dans cet article, nous cherchons à résumer le fonctionnement de ces algorithmes, et leur utilisation dans la littérature du TAL. Nous les comparons expérimentalement, et présentons des manières de faciliter l’entraînement du NCE.</abstract>
      <url hash="b890bf76">2018.jeptalnrecital-court.29</url>
    </paper>
    <paper id="30">
      <title>Étude Expérimentale d’Extraction d’Information dans des Retranscriptions de Réunions (An Experimental Approach For Information Extraction in Multi-Party Dialogue Discourse)</title>
      <language>fra</language>
      <author><first>Pegah</first><last>Alizadeh</last></author>
      <author><first>Peggy</first><last>Cellier</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <author><first>Bruno</first><last>Cremilleux</last></author>
      <author><first>Albrecht</first><last>Zimmermann</last></author>
      <pages>465–472</pages>
      <abstract>Nous nous intéressons dans cet article à l’extraction de thèmes à partir de retranscriptions textuelles de réunions. Ce type de corpus est bruité, il manque de formatage, il est peu structuré avec plusieurs locuteurs qui interviennent et l’information y est souvent éparpillée. Nous présentons une étude expérimentale utilisant des méthodes fondées sur la mesure tf-idf et l’extraction de topics sur un corpus réel de référence (le corpus AMI) pour l’étude de réunions. Nous comparons nos résultats avec les résumés fournis par le corpus.</abstract>
      <url hash="1e2c4383">2018.jeptalnrecital-court.30</url>
    </paper>
    <paper id="31">
      <title>Analyse morpho-syntaxique en présence d’alternance codique (<fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> tagging of Code Switching)</title>
      <language>fra</language>
      <author><first>José Carlos</first><last>Rosales Núñez</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <pages>473–480</pages>
      <abstract>L’alternance codique est le phénomène qui consiste à alterner les langues au cours d’une même conversation ou d’une même phrase. Avec l’augmentation du volume généré par les utilisateurs, ce phénomène essentiellement oral, se retrouve de plus en plus dans les textes écrits, nécessitant d’adapter les tâches et modèles de traitement automatique de la langue à ce nouveau type d’énoncés. Ce travail présente la collecte et l’annotation en partie du discours d’un corpus d’énoncés comportant des alternances codiques et évalue leur impact sur la tâche d’analyse morpho-syntaxique.</abstract>
      <url hash="c77badc6">2018.jeptalnrecital-court.31</url>
    </paper>
    <paper id="32">
      <title>Simplification de schémas d’annotation : un aller sans retour ? (Annotation scheme simplification : a one way trip with no return ?)</title>
      <language>fra</language>
      <author><first>Cyril</first><last>Grouin</last></author>
      <pages>481–488</pages>
      <abstract>Dans cet article, nous comparons l’impact de la simplification d’un schéma d’annotation sur un système de repérage d’entités nommées (REN). Une simplification consiste à rassembler les types d’entités nommées (EN) sous deux types génériques (personne et lieu), l’autre revient à mieux définir chaque type d’EN. Nous observons une amélioration des résultats sur les deux versions simplifiées. Nous étudions également la possibilité de retrouver le niveau de détail des types d’EN du schéma d’origine à partir des versions simplifiées. L’utilisation de règles de conversion permet de recouvrer les types d’EN d’origine, mais il reste une forme d’ambiguïté contextuelle qu’il est impossible de lever au moyen de règles.</abstract>
      <url hash="ace501e5">2018.jeptalnrecital-court.32</url>
    </paper>
    <paper id="33">
      <title>Apprentissage déséquilibré pour la détection des signaux de l’implication durable dans les conversations en parfumerie (Automatic detection of positive enduring involvement signals in fragrance products reviews)</title>
      <language>fra</language>
      <author><first>Yizhe</first><last>Wang</last></author>
      <author><first>Damien</first><last>Nouvel</last></author>
      <author><first>Gaël</first><last>Patin</last></author>
      <author><first>Marguerite</first><last>Leenhardt</last></author>
      <pages>489–498</pages>
      <abstract>Une simple détection d’opinions positives ou négatives ne satisfait plus les chercheurs et les entreprises. Le monde des affaires est à la recherche d’un «aperçu des affaires». Beaucoup de méthodes peuvent être utilisées pour traiter le problème. Cependant, leurs performances, lorsque les classes ne sont pas équilibrées, peuvent être dégradées. Notre travail se concentre sur l’étude des techniques visant à traiter les données déséquilibrées en parfumerie. Cinq méthodes ont été comparées : Smote, Adasyn, Tomek links, Smote-TL et la modification du poids des classe. L’algorithme d’apprentissage choisi est le SVM et l’évaluation est réalisée par le calcul des scores de précision, de rappel et de f-mesure. Selon les résultats expérimentaux, la méthode en ajustant le poids sur des coût d’erreurs avec SVM, nous permet d’obtenir notre meilleure F-mesure.</abstract>
      <url hash="f7fb2b01">2018.jeptalnrecital-court.33</url>
    </paper>
    <paper id="34">
      <title>A comparative study of word embeddings and other features for lexical complexity detection in <fixed-case>F</fixed-case>rench</title>
      <author><first>Aina</first><last>Garí Soler</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <pages>499–508</pages>
      <abstract>Lexical complexity detection is an important step for automatic text simplification which serves to make informed lexical substitutions. In this study, we experiment with word embeddings for measuring the complexity of French words and combine them with other features that have been shown to be well-suited for complexity prediction. Our results on a synonym ranking task show that embeddings perform better than other features in isolation, but do not outperform frequency-based systems in this language.</abstract>
      <url hash="168ba5b3">2018.jeptalnrecital-court.34</url>
    </paper>
    <paper id="35">
      <title>Approche Hybride pour la translitération de l’<fixed-case>A</fixed-case>rabizi Algérien : une étude préliminaire (A hybrid approach for the transliteration of <fixed-case>A</fixed-case>lgerian <fixed-case>A</fixed-case>rabizi: A primary study)</title>
      <language>fra</language>
      <author><first>Imane</first><last>Guellil</last></author>
      <author><first>Azouaou</first><last>Faical</last></author>
      <author><first>Fodil</first><last>Benali</last></author>
      <author><first>Ala</first><last>Eddine Hachani</last></author>
      <author><first>Houda</first><last>Saadane</last></author>
      <pages>509–518</pages>
      <abstract>Dans cet article, nous présentons une approche hybride pour la translitération de l’arabizi algérien. Nous avons élaboré un ensemble de règles permettant le passage de l’arabizi vers l’arabe. Á partir de ces règles nous générons un ensemble de candidats pour la translitération de chaque mot en arabizi vers l’arabe, et un parmi ces candidats sera ensuite identifié et extrait comme le meilleur candidat. Cette approche a été expérimentée en utilisant trois corpus de tests. Les résultats obtenus montrent une amélioration du score de précision qui était pour le meilleur des cas de l’ordre de 75,11%. Ces résultats ont aussi permis de vérifier que notre approche est très compétitive par rapport aux travaux traitant de la translitération de l’arabizi en général.</abstract>
      <url hash="ab34aace">2018.jeptalnrecital-court.35</url>
    </paper>
    <paper id="36">
      <title>Lieu et nom de lieu, du texte vers sa représentation cartographique (Place and place name, from text to its cartographic portrayal)</title>
      <language>fra</language>
      <author><first>Catherine</first><last>Dominguès</last></author>
      <pages>519–528</pages>
      <abstract>Les lieux constituent une information structurante de nombreux textes (récits, romans, articles journalistiques, guides touristiques, itinéraires de randonnées, etc.) et leur recensement et leur analyse doit tenir compte des aspects thématiques abordés dans les textes. Le travail proposé ici s’inscrit dans les domaines de la linguistique de corpus et de la cartographie. La définition de lieu est augmentée de celle d’objet localisé et la désignation de ces lieux peut alors être construite sur un nom propre ou un nom commun. Des expérimentations sont menées afin d’identifier les lieux noms propres avec des gazetiers et les lieux noms communs grâce à un modèle d’apprentissage automatique. Les résultats sont discutés sous la forme d’une comparaison entre les caractéristiques linguistiques des noms de lieux et les propriétés visuelles que devront satisfaire leur représentation cartographique.</abstract>
      <url hash="c9ac4b26">2018.jeptalnrecital-court.36</url>
    </paper>
    <paper id="37">
      <title><fixed-case>J</fixed-case>eux<fixed-case>D</fixed-case>e<fixed-case>L</fixed-case>iens: Word Embeddings and Path-Based Similarity for Entity Linking using the <fixed-case>F</fixed-case>rench <fixed-case>J</fixed-case>eux<fixed-case>D</fixed-case>e<fixed-case>M</fixed-case>ots Lexical Semantic Network</title>
      <author><first>Julien</first><last>Plu</last></author>
      <author><first>Kevin</first><last>Cousot</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Raphaël</first><last>Troncy</last></author>
      <author><first>Giuseppe</first><last>Rizzo</last></author>
      <pages>529–538</pages>
      <abstract>Entity linking systems typically rely on encyclopedic knowledge bases such as DBpedia or Freebase. In this paper, we use, instead, a French lexical-semantic network named JeuxDeMots to jointly type and link entities. Our approach combines word embeddings and a path-based similarity resulting in encouraging results over a set of documents from the French Le Monde newspaper.</abstract>
      <url hash="16521e81">2018.jeptalnrecital-court.37</url>
    </paper>
    <paper id="38">
      <title>De l’usage réel des emojis à une prédiction de leurs catégories (From Emoji Usage to Emoji-Category Prediction)</title>
      <language>fra</language>
      <author><first>Gaël</first><last>Guibon</last></author>
      <author><first>Magalie</first><last>Ochs</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <pages>539–546</pages>
      <abstract>L’utilisation des emojis dans les messageries sociales n’a eu de cesse d’augmenter ces dernières années. Plusieurs travaux récents ont porté sur la prédiction d’emojis afin d’épargner à l’utillisateur le parcours de librairies d’emojis de plus en plus conséquentes. Nous proposons une méthode permettant de récupérer automatiquement les catégories d’emojis à partir de leur contexte d’utilisation afin d’améliorer la prédiction finale. Pour ce faire nous utilisons des plongements lexicaux en considérant les emojis comme des mots présents dans des tweets. Nous appliquons ensuite un regroupement automatique restreint aux emojis visages afin de vérifier l’adéquation des résultats avec la théorie d’Ekman. L’approche est reproductible et applicable sur tous types d’emojis, ou lorsqu’il est nécessaire de prédire de nombreuses classes.</abstract>
      <url hash="b4c92255">2018.jeptalnrecital-court.38</url>
    </paper>
    <paper id="39">
      <title>Transfert de ressources sémantiques pour l’analyse de sentiments au niveau des aspects (In this paper, we address the problem of automatic polarity detection in the context of Aspect Based)</title>
      <language>fra</language>
      <author><first>Caroline</first><last>Brun</last></author>
      <pages>547–556</pages>
      <abstract>Dans cet article, nous abordons le problème de la détection de la polarité pour l’analyse de sentiments au niveau des aspects dans un contexte bilingue : nous proposons d’adapter le composant de détection de polarité d’un système préexistant d’analyse de sentiments au niveau des aspects, très performant pour la tâche, et reposant sur l’utilisation de ressources sémantiques riches pour une langue donnée, à une langue sémantiquement moins richement dotée. L’idée sous-jacente est de réduire le besoin de supervision nécessaire à la construction des ressources sémantiques essentielles à notre système. À cette fin, la langue source, peu dotée, est traduite vers la langue cible, et les traductions parallèles sont ensuite alignées mot à mot. Les informations sémantiques riches sont alors extraites de la langue cible par le système de détection de polarité, et ces informations sont ensuite alignées vers la langue source. Nous présentons les différentes étapes de cette expérience, ainsi que l’évaluation finale. Nous concluons par quelques perspectives.</abstract>
      <url hash="09ec818d">2018.jeptalnrecital-court.39</url>
    </paper>
    <paper id="40">
      <title>Apport des dépendances syntaxiques et des patrons séquentiels à l’extraction de relations ()</title>
      <language>fra</language>
      <author><first>Kata</first><last>Gábor</last></author>
      <author><first>Nadège</first><last>Lechevrel</last></author>
      <author><first>Isabelle</first><last>Tellier</last></author>
      <author><first>Davide</first><last>Buscaldi</last></author>
      <author><first>Haifa</first><last>Zargayouna</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <pages>557–566</pages>
      <abstract/>
      <url hash="e979e934">2018.jeptalnrecital-court.40</url>
    </paper>
    <paper id="41">
      <title>Divergences entre annotations dans le projet <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies et leur impact sur l’évaluation des performance d’étiquetage morpho-syntaxique (Evaluating Annotation Divergences in the <fixed-case>UD</fixed-case> Project)</title>
      <language>fra</language>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>567–576</pages>
      <abstract>Ce travail montre que la dégradation des performances souvent observée lors de l’application d’un analyseur morpho-syntaxique à des données hors domaine résulte souvent d’incohérences entre les annotations des ensembles de test et d’apprentissage. Nous montrons comment le principe de variation des annotations, introduit par Dickinson &amp; Meurers (2003) pour identifier automatiquement les erreurs d’annotation, peut être utilisé pour identifier ces incohérences et évaluer leur impact sur les performances des analyseurs morpho-syntaxiques.</abstract>
      <url hash="011a1e2c">2018.jeptalnrecital-court.41</url>
    </paper>
    <paper id="42">
      <title>Annotation en Actes de Dialogue pour les Conversations d’Assistance en Ligne (Dialog Acts Annotations for Online Chats)</title>
      <language>fra</language>
      <author><first>Robin</first><last>Perrotin</last></author>
      <author><first>Alexis</first><last>Nasr</last></author>
      <author><first>Jeremy</first><last>Auguste</last></author>
      <pages>577–584</pages>
      <abstract>Les conversations techniques en ligne sont un type de productions linguistiques qui par de nombreux aspects se démarquent des objets plus usuellement étudiés en traitement automatique des langues : il s’agit de dialogues écrits entre deux locuteurs qui servent de support à la résolution coopérative des problèmes des usagers. Nous proposons de décrire ici ces conversations par un étiquetage en actes de dialogue spécifiquement conçu pour les conversations en ligne. Différents systèmes de prédictions ont été évalués ainsi qu’une méthode permettant de s’abstraire des spécificités lexicales du corpus d’apprentissage.</abstract>
      <url hash="d0e65d85">2018.jeptalnrecital-court.42</url>
    </paper>
  </volume>
  <volume id="recital" ingest-date="2020-07-21">
    <meta>
      <booktitle>Actes de la Conférence TALN. Volume 2 - Démonstrations, articles des Rencontres Jeunes Chercheurs, ateliers DeFT</booktitle>
      <editor><first>Pascale</first><last>Sébillot</last></editor>
      <editor><first>Vincent</first><last>Claveau</last></editor>
      <publisher>ATALA</publisher>
      <address>Rennes, France</address>
      <month>5</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="fca9b1c7">2018.jeptalnrecital-recital.0</url>
    </frontmatter>
    <paper id="1">
      <title>Construction de patrons lexico-syntaxiques d’extraction pour l’acquisition de connaissances à partir du web (Relation pattern extraction and information extraction from the web)</title>
      <language>fra</language>
      <author><first>Chloé</first><last>Monnin</last></author>
      <author><first>Olivier</first><last>Hamon</last></author>
      <pages>3–16</pages>
      <abstract>Cet article présente une méthode permettant de collecter sur le web des informations complémentaires à une information prédéfinie, afin de remplir une base de connaissances. Notre méthode utilise des patrons lexico-syntaxiques, servant à la fois de requêtes de recherche et de patrons d’extraction permettant l’analyse de documents non structurés. Pour ce faire, il nous a fallu définir au préalable les critères pertinents issus des analyses dans l’objectif de faciliter la découverte de nouvelles valeurs.</abstract>
      <url hash="5f7e3193">2018.jeptalnrecital-recital.1</url>
    </paper>
    <paper id="2">
      <title>Analysis of Inferences in <fixed-case>C</fixed-case>hinese for Opinion Mining</title>
      <author><first>Liyun</first><last>Yan</last></author>
      <pages>17–26</pages>
      <abstract>Analysis of Inferences in Chinese for Opinion Mining Opinion mining is an essential activity for economic watch, made easier by social networks and ad hoc forums. The analysis generally relies on lexicon of sentiments. Nevertheless, some opinions are expressed through inferences. In this paper, we propose a classification of inferences used in Chinese in tourist comments, for an opinion mining task, based on three levels of analysis (semantic realization, modality of realization and production mode). We proved the interest to analyze the distinct types of inferences to identify the polarity of opinions expressed in corpora. We also present some results based on word embeddings.</abstract>
      <url hash="ab65c0c6">2018.jeptalnrecital-recital.2</url>
    </paper>
    <paper id="3">
      <title>Analyse des noms agentifs dans les espaces vectoriels distributionnels (Analysis of agent nouns in vector space models)</title>
      <language>fra</language>
      <author><first>Marine</first><last>Wauquier</last></author>
      <pages>27–40</pages>
      <abstract>Notre étude s’inscrit dans le cadre d’une thèse ayant pour but d’exploiter les modèles distributionnels pour décrire sémantiquement des classes de mots définies selon des critères morphologiques. Nous utilisons des indices morphologiques et formels fournis par une base lexicale pour cibler les noms agentifs déverbaux construits par suffixation en -eur. Nous montrons qu’il est possible de constituer un représentant prototypique de la classe sémantique des noms agentifs en -eur dans les modèles distributionnels. L’étude de ce représentant met en évidence que l’information sémantique véhiculée par le suffixe varie en fonction du corpus d’étude et du degré de lexicalisation des dérivés.</abstract>
      <url hash="4af62ec0">2018.jeptalnrecital-recital.3</url>
    </paper>
    <paper id="4">
      <title>Analyse formelle d’exigences en langue naturelle pour la conception de systèmes cyber-physiques (Formal analysis of natural language requirements for the design of cyber-physical systems )</title>
      <language>fra</language>
      <author><first>Aurélien</first><last>Lamercerie</last></author>
      <pages>41–54</pages>
      <abstract>Cet article explore la construction de représentations formelles d’énoncés en langue naturelle. Le passage d’un langage naturel à une représentation logique est réalisé avec un formalisme grammatical, reliant l’analyse syntaxique de l’énoncé à une représentation sémantique. Nous ciblons l’aspect comportemental des cahiers des charges pour les systèmes cyber-physiques, c’est-à-dire tout type de systèmes dans lesquels des composants logiciels interagissent étroitement avec un environnement physique. Dans ce cadre, l’enjeu serait d’apporter une aide au concepteur. Il s’agit de permettre de simuler et vérifier, par des méthodes automatiques ou assistées, des cahiers des charges “systèmes” exprimés en langue naturelle. Cet article présente des solutions existantes qui pourraient être combinées en vue de la résolution de la problématique exposée.</abstract>
      <url hash="330c1eb5">2018.jeptalnrecital-recital.4</url>
    </paper>
    <paper id="5">
      <title>Résumé automatique guidé de textes: État de l’art et perspectives (Guided Summarization : State-of-the-art and perspectives )</title>
      <language>fra</language>
      <author><first>Salima</first><last>Lamsiyah</last></author>
      <author><first>Said</first><last>Ouatik El Alaoui</last></author>
      <author><first>Bernard</first><last>Espinasse</last></author>
      <pages>55–72</pages>
      <abstract>Les systèmes de résumé automatique de textes (SRAT) consistent à produire une représentation condensée et pertinente à partir d’un ou de plusieurs documents textuels. La majorité des SRAT sont basés sur des approches extractives. La tendance actuelle consiste à s’orienter vers les approches abstractives. Dans ce contexte, le résumé guidé défini par la campagne d’évaluation internationale TAC (Text Analysis Conference) en 2010, vise à encourager la recherche sur ce type d’approche, en se basant sur des techniques d’analyse en profondeur de textes. Dans ce papier, nous nous penchons sur le résumé automatique guidé de textes. Dans un premier temps, nous définissons les différentes caractéristiques et contraintes liées à cette tâche. Ensuite, nous dressons un état de l’art des principaux systèmes existants en mettant l’accent sur les travaux les plus récents, et en les classifiant selon les approches adoptées, les techniques utilisées, et leurs évaluations sur des corpus de références. Enfin, nous proposons les grandes étapes d’une méthode spécifique devant permettre le développement d’un nouveau type de systèmes de résumé guidé.</abstract>
      <url hash="a339dfad">2018.jeptalnrecital-recital.5</url>
    </paper>
    <paper id="6">
      <title>Identification de descripteurs pour la caractérisation de registres (Feature identification for register characterization)</title>
      <language>fra</language>
      <author><first>Jade</first><last>Mekki</last></author>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Gwénolé</first><last>Lecorvé</last></author>
      <author><first>Nicolas</first><last>Béchet</last></author>
      <pages>73–84</pages>
      <abstract>L’article présente une étude des descripteurs linguistiques pour la caractérisation d’un texte selon son registre de langue (familier, courant, soutenu). Cette étude a pour but de poser un premier jalon pour des tâches futures sur le sujet (classification, extraction de motifs discriminants). À partir d’un état de l’art mené sur la notion de registre dans la littérature linguistique et sociolinguistique, nous avons identifié une liste de 72 descripteurs pertinents. Dans cet article, nous présentons les 30 premiers que nous avons pu valider sur un corpus de textes français de registres distincts.</abstract>
      <url hash="7dfc9a57">2018.jeptalnrecital-recital.6</url>
    </paper>
    <paper id="7">
      <title>Construction d’un corpus multilingue annoté en relations de traduction (Construction of a multilingual corpus annotated with translation relations )</title>
      <language>fra</language>
      <author><first>Yuming</first><last>Zhai</last></author>
      <pages>85–100</pages>
      <abstract>Les relations de traduction, qui distinguent la traduction littérale d’autres procédés, constituent un sujet d’étude important pour les traducteurs humains (Chuquet &amp; Paillard, 1989). Or les traitements automatiques fondés sur des relations entre langues, tels que la traduction automatique ou la méthode de génération de paraphrases par équivalence de traduction, ne les ont pas exploitées explicitement jusqu’à présent. Dans ce travail, nous présentons une catégorisation des relations de traduction et nous les annotons dans un corpus parallèle multilingue (anglais, français, chinois) de présentations orales, les TED Talks. Notre objectif à plus long terme sera d’en faire la détection de manière automatique afin de pouvoir les intégrer comme caractéristiques importantes pour la recherche de segments monolingues en relation d’équivalence (paraphrases) ou d’implication. Le corpus annoté résultant de notre travail sera mis à disposition de la communauté.</abstract>
      <url hash="9cff172b">2018.jeptalnrecital-recital.7</url>
    </paper>
    <paper id="8">
      <title>Automatic image annotation : the case of deforestation</title>
      <author><first>Duy</first><last>Huynh</last></author>
      <author><first>Nathalie</first><last>Neptune</last></author>
      <pages>101–116</pages>
      <abstract>Automatic image annotation : the case of deforestation. This paper aims to present the state of the art of the methods that are used for automatic annotation of earth observation image for deforestation detection. We are interested in the various challenges that the field covers and we present the state of the art methods and the future research that we are considering.</abstract>
      <url hash="ef5fcbf7">2018.jeptalnrecital-recital.8</url>
    </paper>
    <paper id="9">
      <title>Détection d’influenceurs dans des médias sociaux (Influencer detection in social medias)</title>
      <language>fra</language>
      <author><first>Kévin</first><last>Deturck</last></author>
      <pages>117–130</pages>
      <abstract>Les influenceurs ont la capacité d’avoir un impact sur d’autres individus lorsqu’ils interagissent avec eux. Détecter les influenceurs permet d’identifier les quelques individus à cibler pour toucher largement un réseau. Il est possible d’analyser les interactions dans un média social du point de vue de leur structure ou de leur contenu. Dans nos travaux de thèse, nous abordons ces deux aspects. Nous présentons d’abord une évaluation de différentes mesures de centralité sur la structure d’interactions extraites de Twitter puis nous analysons l’impact de la taille du graphe de suivi sur la performance de mesures de centralité. Nous abordons l’aspect linguistique pour identifier le changement d’avis comme un effet de l’influence depuis les messages d’un forum.</abstract>
      <url hash="08ee51a3">2018.jeptalnrecital-recital.9</url>
    </paper>
    <paper id="10">
      <title>Extraction d’interactions entre aliment et médicament : Etat de l’art et premiers résultats (Extraction of food-drug interactions : State of the art and first results)</title>
      <language>fra</language>
      <author><first>Tsanta</first><last>Randriatsitohaina</last></author>
      <pages>131–144</pages>
      <abstract>Dans cet article, nous nous intéressons à l’extraction des interactions entre médicaments et aliments, une tâche qui s’apparente à l’extraction de relations entre termes dans les textes de spécialité. De nombreuses approches ont été proposées pour extraire des relations à partir de textes : des patrons lexico-syntaxiques, de la classification supervisée, et plus récemment de l’apprentissage profond. A partir de cet état de l’art, nous présentons une méthode basée sur un apprentissage supervisé et les résultats d’une première série d’expériences. Malgré le déséquilibre des classes, les résultats sont encourageants. Nous avons ainsi pu identifier les classifieurs les plus performants suivant les étapes. Nous avons également observé l’impact important des catégories sémantiques des termes comme descripteurs.</abstract>
      <url hash="5acfa005">2018.jeptalnrecital-recital.10</url>
    </paper>
    <paper id="11">
      <title>Classification par paires de mention pour la résolution des coréférences en français parlé interactif (Mention-pair classification for corefence resolution on spontaneous spoken <fixed-case>F</fixed-case>rench)</title>
      <language>fra</language>
      <author><first>Maëlle</first><last>Brassier</last></author>
      <author><first>Alexis</first><last>Puret</last></author>
      <author><first>Augustin</first><last>Voisin-Marras</last></author>
      <author><first>Loïc</first><last>Grobol</last></author>
      <pages>145–158</pages>
      <abstract>Cet article présente et analyse les premiers résultats obtenus par notre laboratoire pour la construction d’un modèle de résolution des coréférences en français à l’aide de techniques de classifications parmi lesquelles les arbres de décision et les séparateurs à vaste marge. Ce système a été entraîné sur le corpus ANCOR et s’inspire de travaux antérieurs réalisés au laboratoire LATTICE (système CROC). Nous présentons les expérimentations que nous avons menées pour améliorer le système en passant par des classifieurs spécifiques à chaque type de situation interactive, puis chaque type de relation de coréférence.</abstract>
      <url hash="f48b8a06">2018.jeptalnrecital-recital.11</url>
    </paper>
    <paper id="12">
      <title>Approche lexicale de la simplification automatique de textes médicaux (Lexical approach for the automatic simplification of medical texts)</title>
      <language>fra</language>
      <author><first>Remi</first><last>Cardon</last></author>
      <pages>159–174</pages>
      <abstract>Notre travail traite de la simplification automatique de textes. Ce type d’application vise à rendre des contenus difficiles à comprendre plus lisibles. À partir de trois corpus comparables du domaine médical, d’un lexique existant et d’une terminologie du domaine, nous procédons à des analyses et à des modifications en vue de la simplification lexicale de textes médicaux. L’alignement manuel des phrases provenant de ces corpus comparables fournit des données de référence et permet d’analyser les procédés de simplification mis en place. La substitution lexicale avec la ressource existante permet d’effectuer de premiers tests de simplification lexicale et indique que des ressources plus spécifiques sont nécessaires pour traiter les textes médicaux. L’évaluation des substitutions est effectuée avec trois critères : grammaticalité, simplification et sémantique. Elle indique que la grammaticalité est plutôt bien sauvegardée, alors que la sémantique et la simplicité sont plus difficiles à gérer lors des substitutions avec ce type de méthodes.</abstract>
      <url hash="f3f69820">2018.jeptalnrecital-recital.12</url>
    </paper>
    <paper id="13">
      <title>Classification multi-label à grande dimension pour la détection de concepts médicaux (Large multi-label classification for medical concepts detection)</title>
      <language>fra</language>
      <author><first>Josiane</first><last>Mothe</last></author>
      <author><first>Nomena</first><last>Ny Hoavy</last></author>
      <author><first>Mamitiana-Ignace</first><last>Randrianarivony</last></author>
      <pages>175–184</pages>
      <abstract>Dans ce papier, nous présentons une méthode pour associer de façon automatique des concepts à des images. Nous nous focalisons plus particulièrement sur des images médicales à annoter avec des concepts UMLS. Nous avons développé deux modèles de transfert d’apprentissage à partir des réseaux CNN VGG19 et ResNet50 . Nous avons utilisé des modèles avec des techniques simples et que nous avons optimisés pour l’apprentissage. Les résultats que nous avons obtenus en utilisant les données de la tâche ImageCLEF 2017 sont encourageants et comparables à ceux des autres participants.</abstract>
      <url hash="32046a9d">2018.jeptalnrecital-recital.13</url>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2020-07-21">
    <meta>
      <booktitle>Actes de la Conférence TALN. Volume 2 - Démonstrations, articles des Rencontres Jeunes Chercheurs, ateliers DeFT</booktitle>
      <editor><first>Pascale</first><last>Sébillot</last></editor>
      <editor><first>Vincent</first><last>Claveau</last></editor>
      <publisher>ATALA</publisher>
      <address>Rennes, France</address>
      <month>5</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="fca9b1c7">2018.jeptalnrecital-demo.0</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>C</fixed-case>uriosi<fixed-case>T</fixed-case>ext : application web d’aide au peuplement d’ontologies métiers comme ressources lexicales basée sur <fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec (<fixed-case>C</fixed-case>uriosi<fixed-case>T</fixed-case>ext: a web application based on <fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec helping with the population of ontologies (serving as lexical resources))</title>
      <language>fra</language>
      <author><first>Meryl</first><last>Bothua</last></author>
      <author><first>Delphine</first><last>Lagarde</last></author>
      <author><first>Laurent</first><last>Pierre</last></author>
      <pages>193–196</pages>
      <abstract>Suite à la mise en place d’une chaîne traitement destinée à extraire automatiquement des actions de maintenance réalisées sur des composants dans des comptes rendus, nous avons cherché à constituer des ressources lexicales à partir de textes souvent mal normalisés sur le plan linguistique. Nous avons ainsi développé une application web, CuriosiText, qui permet de lancer un traitement Word2Vec et de peupler semi automatiquement une ontologie métier avec les termes similaires correctement détectés. Des relations métiers spécifiques peuvent également être ajoutées.</abstract>
      <url hash="89569d67">2018.jeptalnrecital-demo.1</url>
    </paper>
    <paper id="2">
      <title><fixed-case>ACCOLÉ</fixed-case> : Annotation Collaborative d’erreurs de traduction pour <fixed-case>CO</fixed-case>rpus a<fixed-case>L</fixed-case>ign<fixed-case>É</fixed-case>s (<fixed-case>ACCOLÉ</fixed-case>: A Collaborative Platform of Error Annotation for Aligned Corpus)</title>
      <language>fra</language>
      <author><first>Francis</first><last>Brunet-Manquat</last></author>
      <author><first>Emmanuelle</first><last>Esperança-Rodier</last></author>
      <pages>197–200</pages>
      <abstract>La plateforme ACCOLÉ (Annotation Collaborative d’erreurs de traduction pour COrpus aLignÉs) propose une palette de services innovants permettant de répondre aux besoins modernes d’analyse d’erreurs de traduction : gestion simplifiée des corpus et des typologies d’erreurs, annotation d’erreurs efficace, collaboration et/ou supervision lors de l’annotation, recherche de modèle d’erreurs dans les annotations.</abstract>
      <url hash="d1844eb2">2018.jeptalnrecital-demo.2</url>
    </paper>
    <paper id="3">
      <title>Néonaute, Enrichissement sémantique pour la recherche d’information ()</title>
      <language>fra</language>
      <author><first>Emmanuel</first><last>Cartier</last></author>
      <author><first>Loïc</first><last>Galand</last></author>
      <author><first>Peter</first><last>Stirling</last></author>
      <author><first>Sara</first><last>Aubry</last></author>
      <pages>201–204</pages>
      <abstract>Avec l’explosion du nombre de documents numériques accessibles, les besoins en outils pour l’enrichissement sémantique des données textuelles, ainsi que des fonctionnalités avancées de recherche et d’exploration des collections, se font sentir. Cette combinaison entre les domaines de la recherche d’information et du traitement automatique des langues est l’une des caractéristiques du projet Néonaute. Ce projet, financé par la DGLFLF 1 en 2017 (appel Langues et numérique), regroupe la Bibliothèque nationale de France (BnF), le LIPN - RCLN (CNRS UMR 7030) et l’Université de Strasbourg (LILPA, EA 1339). Son objectif principal est de doter les observateurs de la langue française d’un moteur de recherche s’appuyant sur une collection de sites de presse d’actualité, collectés automatiquement par la BnF au titre de sa mission de dépôt légal de l’internet. Sur cette collection, le projet vise à proposer un moteur de recherche de nouvelle génération, disposant d’une indexation enrichie par l’analyse automatique des textes (analyse morphosyntaxique, entités nommées, thématiques), d’une part, et d’outils de recherche, d’exploration et de visualisation multidimensionnelle interactive des résultats, d’autre part.</abstract>
      <url hash="92116ada">2018.jeptalnrecital-demo.3</url>
    </paper>
    <paper id="4">
      <title>Nouveautés de l’analyseur linguistique <fixed-case>LIMA</fixed-case> (What’s New in the <fixed-case>LIMA</fixed-case> Language Analyzer)</title>
      <language>fra</language>
      <author><first>Gaël</first><last>de Chalendar</last></author>
      <pages>205–208</pages>
      <abstract>LIMA est un analyseur linguistique libre d’envergure industrielle. Nous présentons ici ses évolutions depuis la dernière publication en 2014.</abstract>
      <url hash="ac992943">2018.jeptalnrecital-demo.4</url>
    </paper>
    <paper id="5">
      <title>Un outil d’étiquetage rapide et un corpus libre en entités nommées du Français (A fast tagging tool and a free <fixed-case>F</fixed-case>rench named entity corpus)</title>
      <language>fra</language>
      <author><first>Yoann</first><last>Dupont</last></author>
      <pages>209–210</pages>
      <abstract>Dans cet article, nous présentons un outil pour effectuer l’étiquetage rapide de textes bruts. Il peut charger des documents annotés depuis divers formats, notamment BRAT et GATE. Il se base sur des raccourcis claviers intuitifs et la diffusion d’annotation à l’échelle du document. Il permet d’entraîner des systèmes par apprentissage que l’on peut alors utiliser pour préannoter les textes.</abstract>
      <url hash="8d8e5b42">2018.jeptalnrecital-demo.5</url>
    </paper>
    <paper id="6">
      <title><fixed-case>P</fixed-case>y<fixed-case>RATA</fixed-case>, Python Rule-based fe<fixed-case>A</fixed-case>ture s<fixed-case>T</fixed-case>ructure Analysis</title>
      <author><first>Nicolas</first><last>Hernandez</last></author>
      <pages>211–212</pages>
      <abstract>Nous présentons PyRATA (Python Rules-based feAture sTructure Analysis) un module Python (version 3) diffusé sous licence Apache V2 et disponible sur github 4 et dans les dépots pypi 5 . PyRATA a pour objectif de permettre de l’analyse à base de règles sur des données structurées. Le langage de PyRATA offre une expressivité qui couvre les fonctionnalités proposées par les modules alternatifs et davantage. Conçu pour être intuitif, la syntaxe des motifs et l’interface de programmation (API) suivent les définitions de standards existants, respectivement la syntaxe des expressions régulières de Perl et l’API du module Python re. PyRATA travaille sur des structures de données simples et natives de Python : une liste de dictionnaires (c-à-d une liste de tables d’associations). Cela lui permet de traiter des données de différentes natures (textuelles ou non) telles qu’une liste de mots, une liste de phrases, une liste de messages d’un fil de discussion, une liste d’événements d’un agenda... Cette spécificité le rend indépendant de la nature des annotations (a fortiori linguistiques) associées à la donnée manipulée. Ce travail a été financé par le projet ANR 2016 PASTEL.</abstract>
      <url hash="0a1b8eb7">2018.jeptalnrecital-demo.6</url>
    </paper>
    <paper id="7">
      <title>Un corpus en arabe annoté manuellement avec des sens <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et (<fixed-case>A</fixed-case>rabic Manually Sense Annotated Corpus with <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Senses)</title>
      <language>fra</language>
      <author><first>Marwa</first><last>Hadj Salah</last></author>
      <author><first>Hervé</first><last>Blanchon</last></author>
      <author><first>Mounir</first><last>Zrigui</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <pages>213–216</pages>
      <abstract>OntoNotes comprend le seul corpus manuellement annoté en sens librement disponible pour l’arabe. Elle reste peu connue et utilisée certainement parce que le projet s’est achevé sans lier cet inventaire au Princeton WordNet qui lui aurait ouvert l’accès à son riche écosystème. Dans cet article, nous présentons une version étendue de OntoNotes Release 5.0 que nous avons créée en suivant une méthodologie de construction semi-automatique. Il s’agit d’une mise à jour de la partie arabe annotée en sens du corpus en ajoutant l’alignement vers le Princeton WordNet 3.0. Cette ressource qui comprend plus de 12 500 mots annotés est librement disponible pour la communauté. Nous espérons qu’elle deviendra un standard pour l’évaluation de la désambiguïsation lexicale de l’arabe.</abstract>
      <url hash="2a33b74b">2018.jeptalnrecital-demo.7</url>
    </paper>
  </volume>
  <volume id="deft" ingest-date="2020-07-21">
    <meta>
      <booktitle>Actes de la Conférence TALN. Volume 2 - Démonstrations, articles des Rencontres Jeunes Chercheurs, ateliers DeFT</booktitle>
      <editor><first>Pascale</first><last>Sébillot</last></editor>
      <editor><first>Vincent</first><last>Claveau</last></editor>
      <publisher>ATALA</publisher>
      <address>Rennes, France</address>
      <month>5</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="fca9b1c7">2018.jeptalnrecital-deft.0</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>DEFT</fixed-case>2018 : recherche d’information et analyse de sentiments dans des tweets concernant les transports en <fixed-case>Î</fixed-case>le de <fixed-case>F</fixed-case>rance (<fixed-case>DEFT</fixed-case>2018 : Information Retrieval and Sentiment Analysis in Tweets about Public Transportation in <fixed-case>Î</fixed-case>le de <fixed-case>F</fixed-case>rance Region )</title>
      <language>fra</language>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Iris</first><last>Eshkol-Taravella</last></author>
      <author><first>Amel</first><last>Fraisse</last></author>
      <author><first>Agata</first><last>Jackiewicz</last></author>
      <author><first>Jihen</first><last>Karoui</last></author>
      <author><first>Laura</first><last>Monceaux</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <pages>219–230</pages>
      <abstract>Cet article présente l’édition 2018 de la campagne d’évaluation DEFT (Défi Fouille de Textes). A partir d’un corpus de tweets, quatre tâches ont été proposées : identifier les tweets sur la thématique des transports, puis parmi ces derniers, identifier la polarité (négatif, neutre, positif, mixte), identifier les marqueurs de sentiment et la cible, et enfin, annoter complètement chaque tweet en source et cible des sentiments exprimés. Douze équipes ont participé, majoritairement sur les deux premières tâches. Sur l’identification de la thématique des transports, la micro F-mesure varie de 0,827 à 0,908. Sur l’identification de la polarité globale, la micro F-mesure varie de 0,381 à 0,823.</abstract>
      <url hash="c646f47b">2018.jeptalnrecital-deft.1</url>
    </paper>
    <paper id="2">
      <title>Participation d’<fixed-case>EDF</fixed-case> <fixed-case>R</fixed-case>&amp;<fixed-case>D</fixed-case> à <fixed-case>DEFT</fixed-case> 2018 (Here the title in <fixed-case>E</fixed-case>nglish)</title>
      <language>fra</language>
      <author><first>Philippe</first><last>Suignard</last></author>
      <author><first>Lou</first><last>Charaudeau</last></author>
      <author><first>Manel</first><last>Boumghar</last></author>
      <author><first>Meryl</first><last>Bothua</last></author>
      <author><first>Delphine</first><last>Lagarde</last></author>
      <pages>231–238</pages>
      <abstract>Ce papier décrit la participation d’EDF R&amp;D à la campagne d’évaluation DEFT 2018. Notre équipe a participé aux deux premières tâches : classification des tweets en transport/non-transport (Tâche T1) et détection de la polarité globale des tweets (Tâche T2). Nous avons utilisé 3 méthodes différentes s’appuyant sur Word2Vec, CNN et LSTM. Aucune donnée supplémentaire, autre que les données d’apprentissage, n’a été utilisée. Notre équipe obtient des résultats très corrects et se classe 1ère équipe non académique. Les méthodes proposées sont facilement transposables à d’autres tâches de classification de textes courts et peuvent intéresser plusieurs entités du groupe EDF.</abstract>
      <url hash="9694975f">2018.jeptalnrecital-deft.2</url>
    </paper>
    <paper id="3">
      <title><fixed-case>CL</fixed-case>a<fixed-case>C</fixed-case> @ <fixed-case>DEFT</fixed-case> 2018: Sentiment analysis of tweets on transport from <fixed-case>Î</fixed-case>le-de-<fixed-case>F</fixed-case>rance</title>
      <author><first>Simon</first><last>Jacques</last></author>
      <author><first>Farhood</first><last>Farahnak</last></author>
      <author><first>Leila</first><last>Kosseim</last></author>
      <pages>239–248</pages>
      <abstract>CLaC @ DEFT 2018: Analysis of tweets on transport on the Île-de-France This paper describes the system deployed by the CLaC lab at Concordia University in Montreal for the DEFT 2018 shared task. The competition consisted in four different tasks; however, due to lack of time, we only participated in the first two. We participated with a system based on conventional supervised learning methods: a support vector machine classifier and an artificial neural network. For task 1, our best approach achieved an F-measure of 87.61%; while at task 2, we achieve 51.03%, situating our system below the average of the other participants.</abstract>
      <url hash="d1d8f323">2018.jeptalnrecital-deft.3</url>
    </paper>
    <paper id="4">
      <title>Modèles en Caractères pour la Détection de Polarité dans les Tweets (Character-level Models for Polarity Detection in Tweets )</title>
      <language>fra</language>
      <author><first>Davide</first><last>Buscaldi</last></author>
      <author><first>Joseph</first><last>Le Roux</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <pages>249–258</pages>
      <abstract>Dans cet article, nous présentons notre contribution au Défi Fouille de Textes 2018 au travers de trois méthodes originales pour la classification thématique et la détection de polarité dans des tweets en français. Nous y avons ajouté un système de vote. Notre première méthode est fondée sur des lexiques (mots et emojis), les n-grammes de caractères et un classificateur à vaste marge (ou SVM). tandis que les deux autres sont des méthodes endogènes fondées sur l’extraction de caractéristiques au grain caractères : un modèle à mémoire à court-terme persistante (ou BiLSTM pour Bidirectionnal Long Short-Term Memory) et perceptron multi-couche d’une part et un modèle de séquences de caractères fermées fréquentes et classificateur SVM d’autre part. Le BiLSTM a produit de loin les meilleurs résultats puisqu’il a obtenu la première place sur la tâche 1, classification binaire de tweets selon qu’ils traitent ou non des transports, et la troisième place sur la tâche 2, classification de la polarité en 4 classes. Ce résultat est d’autant plus intéressant que la méthode proposée est faiblement paramétrique, totalement endogène et qu’elle n’implique aucun pré-traitement.</abstract>
      <url hash="bddfdf82">2018.jeptalnrecital-deft.4</url>
    </paper>
    <paper id="5">
      <title>Concaténation de réseaux de neurones pour la classification de tweets, <fixed-case>DEFT</fixed-case>2018 (Concatenation of neural networks for tweets classification, <fixed-case>DEFT</fixed-case>2018 )</title>
      <language>fra</language>
      <author><first>Damien</first><last>Sileo</last></author>
      <author><first>Tim</first><last>Van de Cruys</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Camille</first><last>Pradel</last></author>
      <pages>259–264</pages>
      <abstract>Nous présentons le système utilisé par l’équipe Melodi/Synapse Développement dans la compétition DEFT2018 portant sur la classification de thématique ou de sentiments de tweets en français. On propose un système unique pour les deux approches qui combine concaténativement deux méthodes d’embedding et trois modèles de représentation séquence. Le système se classe 1/13 en analyse de sentiments et 4/13 en classification thématique.</abstract>
      <url hash="02544ce7">2018.jeptalnrecital-deft.5</url>
    </paper>
    <paper id="6">
      <title>Participation de l’<fixed-case>IRISA</fixed-case> à <fixed-case>D</fixed-case>e<fixed-case>FT</fixed-case> 2018 : classification et annotation d’opinion dans des tweets (<fixed-case>IRISA</fixed-case> at <fixed-case>D</fixed-case>e<fixed-case>FT</fixed-case> 2018: classifying and tagging opinion in tweets )</title>
      <language>fra</language>
      <author><first>Anne-Lyse</first><last>Minard</last></author>
      <author><first>Christian</first><last>Raymond</last></author>
      <author><first>Vincent</first><last>Claveau</last></author>
      <pages>265–278</pages>
      <abstract>Cet article décrit les systèmes développés par l’équipe LinkMedia de l’IRISA pour la campagne d’évaluation DeFT 2018 portant sur l’analyse d’opinion dans des tweets en français. L’équipe a participé à 3 des 4 tâches de la campagne : (i) classification des tweets selon s’ils concernent les transports ou non, (ii) classification des tweets selon leur polarité et (iii) annotation des marqueurs d’opinion et de l’objet à propos duquel est exprimée l’opinion. Nous avons utilisé un algorithme de boosting d’arbres de décision et des réseaux de neurones récurrents (RNN) pour traiter les tâches 1 et 2. Pour la tâche 3 nous avons expérimenté l’utilisation de réseaux de neurones récurrents associés à des CRF. Ces approches donnent des résultats proches, avec un léger avantage aux RNN, et ont permis d’être parmi les premiers classés pour chacune des tâches.</abstract>
      <url hash="aad531cb">2018.jeptalnrecital-deft.6</url>
    </paper>
    <paper id="7">
      <title><fixed-case>DEFT</fixed-case> 2018: Attention sélective pour classification de microblogs (<fixed-case>DEFT</fixed-case> 2018 : Selective Attention for Microblogging Classification )</title>
      <language>fra</language>
      <author><first>Charles-Emmanuel</first><last>Dias</last></author>
      <author><first>Clara</first><last>de Forsan de Gainon Gabriac</last></author>
      <author><first>Patrick</first><last>Gallinari</last></author>
      <author><first>Vincent</first><last>Guigue</last></author>
      <pages>279–286</pages>
      <abstract>Dans le cadre de l’atelier DEFT 2018 nous nous sommes intéressés à la classification de microblogs (ici, des tweets) rédigés en français. Ici, nous proposons une méthode se basant sur un réseau hiérarchique de neurones récurrent avec attention. La spécificité de notre architecture est de prendre en compte –via un mechanisme d’attention et de portes– les hashtags et les mentions directes (e.g., @user), spécifiques aux microblogs. Notre modèle a obtenu de très bon résultats sur la première tâche et des résultats compétitifs sur la seconde.</abstract>
      <url hash="8f3594e5">2018.jeptalnrecital-deft.7</url>
    </paper>
    <paper id="8">
      <title>Notre tweet première fois au <fixed-case>DEFT</fixed-case>-2018 : systèmes de détection de polarité et de transports (Systems for detecting polarity and public transport discussions in <fixed-case>F</fixed-case>rench tweets)</title>
      <language>fra</language>
      <author><first>David</first><last>Graceffa</last></author>
      <author><first>Armelle</first><last>Ramond</last></author>
      <author><first>Emmanuelle</first><last>Dusserre</last></author>
      <author><first>Ruslan</first><last>Kalitvianski</last></author>
      <author><first>Mathieu</first><last>Ruhlmann</last></author>
      <author><first>Muntsa</first><last>Padró</last></author>
      <pages>287–298</pages>
      <abstract>Cet article décrit les systèmes de l’équipe Eloquant pour la catégorisation de tweets en français dans les tâches 1 (détection de la thématique transports en commun) et 2 (détection de la polarité globale) du DEFT 2018. Nos systèmes reposent sur un enrichissement sémantique, l’apprentissage automatique et, pour la tâche 1 une approche symbolique. Nous avons effectué deux runs pour chacune des tâches. Nos meilleures F-mesures (0.897 pour la tâche 1 et 0.800 pour la tâche 2) sont au-dessus de la moyenne globale pour chaque tâche, et nous placent dans les 30% supérieurs de tous les runs pour la tâche 2.</abstract>
      <url hash="48bfcb7e">2018.jeptalnrecital-deft.8</url>
    </paper>
    <paper id="9">
      <title><fixed-case>LSE</fixed-case> au <fixed-case>DEFT</fixed-case> 2018 : Classification de tweets basée sur les réseaux de neurones profonds (<fixed-case>LSE</fixed-case> at <fixed-case>DEFT</fixed-case> 2018 : Sentiment analysis model based on deep learning)</title>
      <language>fra</language>
      <author><first>Antoine</first><last>Sainson</last></author>
      <author><first>Hugo</first><last>Linsenmaier</last></author>
      <author><first>Alexandre</first><last>Majed</last></author>
      <author><first>Xavier</first><last>Cadet</last></author>
      <author><first>Abdessalam</first><last>Bouchekif</last></author>
      <pages>299–310</pages>
      <abstract>Dans ce papier, nous décrivons les systèmes développés au LSE pour le DEFT 2018 sur les tâches 1 et 2 qui consistent à classifier des tweets. La première tâche consiste à déterminer si un message concerne les transports ou non. La deuxième, consiste à classifier les tweets selon leur polarité globale. Pour les deux tâches nous avons développé des systèmes basés sur des réseaux de neurones convolutifs (CNN) et récurrents (LSTM, BLSTM et GRU). Chaque mot d’un tweet donné est représenté par un vecteur dense appris à partir des données relativement proches de celles de la compétition. Le score final officiel est de 0.891 pour la tâche 1 et de 0.781 pour la tâche 2.</abstract>
      <url hash="cd7f086d">2018.jeptalnrecital-deft.9</url>
    </paper>
    <paper id="10">
      <title>Syllabs@<fixed-case>DEFT</fixed-case>2018 : combinaison de méthodes de classification supervisées (Syllabs@<fixed-case>DEFT</fixed-case>2018: Combination of Supervised Classification Methods)</title>
      <language>fra</language>
      <author><first>Chloé</first><last>Monnin</last></author>
      <author><first>Olivier</first><last>Querné</last></author>
      <author><first>Olivier</first><last>Hamon</last></author>
      <pages>311–318</pages>
      <abstract>Nous présentons la participation de Syllabs à la tâche de classification de tweets dans le domaine du transport lors de DEFT 2018. Pour cette première participation à une campagne DEFT, nous avons choisi de tester plusieurs algorithmes de classification état de l’art. Après une étape de prétraitement commune à l’ensemble des algorithmes, nous effectuons un apprentissage sur le seul contenu des tweets. Les résultats étant somme toute assez proches, nous effectuons un vote majoritaire sur les trois algorithmes ayant obtenus les meilleurs résultats.</abstract>
      <url hash="f3b61ba1">2018.jeptalnrecital-deft.10</url>
    </paper>
    <paper id="11">
      <title><fixed-case>LIRMM</fixed-case>@<fixed-case>DEFT</fixed-case>-2018 – Modèle de classification de la vectorisation des documents (<fixed-case>LIRMM</fixed-case> <fixed-case>DEFT</fixed-case>-2018 – Document Vectorization Classification model )</title>
      <language>fra</language>
      <author><first>Waleed</first><last>Mohamed Azmy</last></author>
      <author><first>Bilel</first><last>Moulahi</last></author>
      <author><first>Sandra</first><last>Bringay</last></author>
      <author><first>Maximilien</first><last>Servajean</last></author>
      <pages>319–322</pages>
      <abstract>Dans ce papier, nous décrivons notre participation au défi d’analyse de texte DEFT 2018. Nous avons participé à deux tâches : (i) classification transport/non-transport et (ii) analyse de polarité globale des tweets : positifs, negatifs, neutres et mixtes. Nous avons exploité un réseau de neurone basé sur un perceptron multicouche mais utilisant une seule couche cachée.</abstract>
      <url hash="6131eb9c">2018.jeptalnrecital-deft.11</url>
    </paper>
    <paper id="12">
      <title>Adapted Sentiment Similarity Seed Words For <fixed-case>F</fixed-case>rench Tweets’ Polarity Classification</title>
      <author><first>Amal</first><last>Htait</last></author>
      <pages>323–328</pages>
      <abstract>We present, in this paper, our contribution in DEFT 2018 task 2 : “Global polarity”, determining the overall polarity (Positive, Negative, Neutral or MixPosNeg) of tweets regarding public transport, in French language. Our system is based on a list of sentiment seed-words adapted for French public transport tweets. These seed-words are extracted from DEFT’s training annotated dataset, and the sentiment relations between seed-words and other terms are captured by cosine measure of their word embeddings representations, using a French language word embeddings model of 683k words. Our semi-supervised system achieved an F1-measure equals to 0.64.</abstract>
      <url hash="54c7425d">2018.jeptalnrecital-deft.12</url>
    </paper>
  </volume>
</collection>
