<?xml version='1.0' encoding='UTF-8'?>
<collection id="S17">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*<fixed-case>SEM</fixed-case> 2017)</booktitle>
      <url hash="ab2df2f7">S17-1</url>
      <editor><first>Nancy</first><last>Ide</last></editor>
      <editor><first>Aurélie</first><last>Herbelot</last></editor>
      <editor><first>Lluís</first><last>Màrquez</last></editor>
      <doi>10.18653/v1/S17-1</doi>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vancouver, Canada</address>
      <month>August</month>
      <year>2017</year>
    </meta>
    <frontmatter>
      <url hash="072af725">S17-1000</url>
    </frontmatter>
    <paper id="1">
      <title>What Analogies Reveal about Word Vectors and their Compositionality</title>
      <author><first>Gregory</first> <last>Finley</last></author>
      <author><first>Stephanie</first> <last>Farmer</last></author>
      <author><first>Serguei</first> <last>Pakhomov</last></author>
      <pages>1–11</pages>
      <url hash="8007b00b">S17-1001</url>
      <doi>10.18653/v1/S17-1001</doi>
      <abstract>Analogy completion via vector arithmetic has become a common means of demonstrating the compositionality of word embeddings. Previous work have shown that this strategy works more reliably for certain types of analogical word relationships than for others, but these studies have not offered a convincing account for why this is the case. We arrive at such an account through an experiment that targets a wide variety of analogy questions and defines a baseline condition to more accurately measure the efficacy of our system. We find that the most reliably solvable analogy categories involve either 1) the application of a morpheme with clear syntactic effects, 2) male–female alternations, or 3) named entities. These broader types do not pattern cleanly along a syntactic–semantic divide. We suggest instead that their commonality is distributional, in that the difference between the distributions of two words in any given pair encompasses a relatively small number of word types. Our study offers a needed explanation for why analogy tests succeed and fail where they do and provides nuanced insight into the relationship between word distributions and the theoretical linguistic domains of syntax and semantics.</abstract>
    </paper>
    <paper id="2">
      <title>Learning Antonyms with Paraphrases and a Morphology-Aware Neural Network</title>
      <author><first>Sneha</first> <last>Rajana</last></author>
      <author><first>Chris</first> <last>Callison-Burch</last></author>
      <author><first>Marianna</first> <last>Apidianaki</last></author>
      <author><first>Vered</first> <last>Shwartz</last></author>
      <pages>12–21</pages>
      <url hash="936ecab7">S17-1002</url>
      <doi>10.18653/v1/S17-1002</doi>
      <abstract>Recognizing and distinguishing antonyms from other types of semantic relations is an essential part of language understanding systems. In this paper, we present a novel method for deriving antonym pairs using paraphrase pairs containing negation markers. We further propose a neural network model, AntNET, that integrates morphological features indicative of antonymy into a path-based relation detection algorithm. We demonstrate that our model outperforms state-of-the-art models in distinguishing antonyms from other semantic relations and is capable of efficiently handling multi-word expressions.</abstract>
    </paper>
    <paper id="3">
      <title>Decoding Sentiment from Distributed Representations of Sentences</title>
      <author><first>Edoardo Maria</first> <last>Ponti</last></author>
      <author><first>Ivan</first> <last>Vulić</last></author>
      <author><first>Anna</first> <last>Korhonen</last></author>
      <pages>22–32</pages>
      <url hash="ad76958d">S17-1003</url>
      <doi>10.18653/v1/S17-1003</doi>
      <abstract>Distributed representations of sentences have been developed recently to represent their meaning as real-valued vectors. However, it is not clear how much information such representations retain about the polarity of sentences. To study this question, we decode sentiment from unsupervised sentence representations learned with different architectures (sensitive to the order of words, the order of sentences, or none) in 9 typologically diverse languages. Sentiment results from the (recursive) composition of lexical items and grammatical strategies such as negation and concession. The results are manifold: we show that there is no ‘one-size-fits-all’ representation architecture outperforming the others across the board. Rather, the top-ranking architectures depend on the language at hand. Moreover, we find that in several cases the additive composition model based on skip-gram word vectors may surpass supervised state-of-art architectures such as bi-directional LSTMs. Finally, we provide a possible explanation of the observed variation based on the type of negative constructions in each language.</abstract>
    </paper>
    <paper id="4">
      <title>Detecting Asymmetric Semantic Relations in Context: A Case-Study on Hypernymy Detection</title>
      <author><first>Yogarshi</first> <last>Vyas</last></author>
      <author><first>Marine</first> <last>Carpuat</last></author>
      <pages>33–43</pages>
      <url hash="3287d135">S17-1004</url>
      <doi>10.18653/v1/S17-1004</doi>
      <abstract>We introduce WHiC, a challenging testbed for detecting hypernymy, an asymmetric relation between words. While previous work has focused on detecting hypernymy between word types, we ground the meaning of words in specific contexts drawn from WordNet examples, and require predictions to be sensitive to changes in contexts. WHiC lets us analyze complementary properties of two approaches of inducing vector representations of word meaning in context. We show that such contextualized word representations also improve detection of a wider range of semantic relations in context.</abstract>
    </paper>
    <paper id="5">
      <title>Domain-Specific New Words Detection in <fixed-case>C</fixed-case>hinese</title>
      <author><first>Ao</first> <last>Chen</last></author>
      <author><first>Maosong</first> <last>Sun</last></author>
      <pages>44–53</pages>
      <url hash="ce22b078">S17-1005</url>
      <doi>10.18653/v1/S17-1005</doi>
      <abstract>With the explosive growth of Internet, more and more domain-specific environments appear, such as forums, blogs, MOOCs and etc. Domain-specific words appear in these areas and always play a critical role in the domain-specific NLP tasks. This paper aims at extracting Chinese domain-specific new words automatically. The extraction of domain-specific new words has two parts including both new words in this domain and the especially important words. In this work, we propose a joint statistical model to perform these two works simultaneously. Compared to traditional new words detection models, our model doesn’t need handcraft features which are labor intensive. Experimental results demonstrate that our joint model achieves a better performance compared with the state-of-the-art methods.</abstract>
    </paper>
    <paper id="6">
      <title>Deep Learning Models For Multiword Expression Identification</title>
      <author><first>Waseem</first> <last>Gharbieh</last></author>
      <author><first>Virendrakumar</first> <last>Bhavsar</last></author>
      <author><first>Paul</first> <last>Cook</last></author>
      <pages>54–64</pages>
      <url hash="676391ca">S17-1006</url>
      <doi>10.18653/v1/S17-1006</doi>
      <abstract>Multiword expressions (MWEs) are lexical items that can be decomposed into multiple component words, but have properties that are unpredictable with respect to their component words. In this paper we propose the first deep learning models for token-level identification of MWEs. Specifically, we consider a layered feedforward network, a recurrent neural network, and convolutional neural networks. In experimental results we show that convolutional neural networks are able to outperform the previous state-of-the-art for MWE identification, with a convolutional neural network with three hidden layers giving the best performance.</abstract>
    </paper>
    <paper id="7">
      <title>Emotion Intensities in Tweets</title>
      <author><first>Saif</first> <last>Mohammad</last></author>
      <author><first>Felipe</first> <last>Bravo-Marquez</last></author>
      <pages>65–77</pages>
      <url hash="2b4c4649">S17-1007</url>
      <doi>10.18653/v1/S17-1007</doi>
      <abstract>This paper examines the task of detecting intensity of emotion from text. We create the first datasets of tweets annotated for anger, fear, joy, and sadness intensities. We use a technique called best–worst scaling (BWS) that improves annotation consistency and obtains reliable fine-grained scores. We show that emotion-word hashtags often impact emotion intensity, usually conveying a more intense emotion. Finally, we create a benchmark regression system and conduct experiments to determine: which features are useful for detecting emotion intensity; and, the extent to which two emotions are similar in terms of how they manifest in language.</abstract>
    </paper>
    <paper id="8">
      <title>Deep Active Learning for Dialogue Generation</title>
      <author><first>Nabiha</first> <last>Asghar</last></author>
      <author><first>Pascal</first> <last>Poupart</last></author>
      <author><first>Xin</first> <last>Jiang</last></author>
      <author><first>Hang</first> <last>Li</last></author>
      <pages>78–83</pages>
      <url hash="6239638e">S17-1008</url>
      <doi>10.18653/v1/S17-1008</doi>
      <abstract>We propose an online, end-to-end, neural generative conversational model for open-domain dialogue. It is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning. While most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character user-feedback at each step. Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train agents with customized personas, moods and conversational styles.</abstract>
    </paper>
    <paper id="9">
      <title>Mapping the Paraphrase Database to <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Anne</first> <last>Cocos</last></author>
      <author><first>Marianna</first> <last>Apidianaki</last></author>
      <author><first>Chris</first> <last>Callison-Burch</last></author>
      <pages>84–90</pages>
      <url hash="b06e0349">S17-1009</url>
      <doi>10.18653/v1/S17-1009</doi>
      <abstract>WordNet has facilitated important research in natural language processing but its usefulness is somewhat limited by its relatively small lexical coverage. The Paraphrase Database (PPDB) covers 650 times more words, but lacks the semantic structure of WordNet that would make it more directly useful for downstream tasks. We present a method for mapping words from PPDB to WordNet synsets with 89% accuracy. The mapping also lays important groundwork for incorporating WordNet’s relations into PPDB so as to increase its utility for semantic reasoning in applications.</abstract>
    </paper>
    <paper id="10">
      <title>Semantic Frame Labeling with Target-based Neural Model</title>
      <author><first>Yukun</first> <last>Feng</last></author>
      <author><first>Dong</first> <last>Yu</last></author>
      <author><first>Jian</first> <last>Xu</last></author>
      <author><first>Chunhua</first> <last>Liu</last></author>
      <pages>91–96</pages>
      <url hash="c6e175d3">S17-1010</url>
      <doi>10.18653/v1/S17-1010</doi>
      <abstract>This paper explores the automatic learning of distributed representations of the target’s context for semantic frame labeling with target-based neural model. We constrain the whole sentence as the model’s input without feature extraction from the sentence. This is different from many previous works in which local feature extraction of the targets is widely used. This constraint makes the task harder, especially with long sentences, but also makes our model easily applicable to a range of resources and other similar tasks. We evaluate our model on several resources and get the state-of-the-art result on subtask 2 of SemEval 2015 task 15. Finally, we extend the task to word-sense disambiguation task and we also achieve a strong result in comparison to state-of-the-art work.</abstract>
    </paper>
    <paper id="11">
      <title>Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles</title>
      <author><first>Francis</first> <last>Ferraro</last></author>
      <author><first>Adam</first> <last>Poliak</last></author>
      <author><first>Ryan</first> <last>Cotterell</last></author>
      <author><first>Benjamin</first> <last>Van Durme</last></author>
      <pages>97–103</pages>
      <url hash="38d313ae">S17-1011</url>
      <doi>10.18653/v1/S17-1011</doi>
      <abstract>We study how different frame annotations complement one another when learning continuous lexical semantics. We learn the representations from a tensorized skip-gram model that consistently encodes syntactic-semantic content better, with multiple 10% gains over baselines.</abstract>
    </paper>
    <paper id="12">
      <title>Distributed Prediction of Relations for Entities: The Easy, The Difficult, and The Impossible</title>
      <author><first>Abhijeet</first> <last>Gupta</last></author>
      <author><first>Gemma</first> <last>Boleda</last></author>
      <author><first>Sebastian</first> <last>Padó</last></author>
      <pages>104–109</pages>
      <url hash="50efb725">S17-1012</url>
      <doi>10.18653/v1/S17-1012</doi>
      <abstract>Word embeddings are supposed to provide easy access to semantic relations such as “male of” (man–woman). While this claim has been investigated for concepts, little is known about the distributional behavior of relations of (Named) Entities. We describe two word embedding-based models that predict values for relational attributes of entities, and analyse them. The task is challenging, with major performance differences between relations. Contrary to many NLP tasks, high difficulty for a relation does not result from low frequency, but from (a) one-to-many mappings; and (b) lack of context patterns expressing the relation that are easy to pick up by word embeddings.</abstract>
    </paper>
    <paper id="13">
      <title>Comparing Approaches for Automatic Question Identification</title>
      <author><first>Angel</first> <last>Maredia</last></author>
      <author><first>Kara</first> <last>Schechtman</last></author>
      <author><first>Sarah Ita</first> <last>Levitan</last></author>
      <author><first>Julia</first> <last>Hirschberg</last></author>
      <pages>110–114</pages>
      <url hash="69cccfb3">S17-1013</url>
      <doi>10.18653/v1/S17-1013</doi>
      <abstract>Collecting spontaneous speech corpora that are open-ended, yet topically constrained, is increasingly popular for research in spoken dialogue systems and speaker state, inter alia. Typically, these corpora are labeled by human annotators, either in the lab or through crowd-sourcing; however, this is cumbersome and time-consuming for large corpora. We present four different approaches to automatically tagging a corpus when general topics of the conversations are known. We develop these approaches on the Columbia X-Cultural Deception corpus and find accuracy that significantly exceeds the baseline. Finally, we conduct a cross-corpus evaluation by testing the best performing approach on the Columbia/SRI/Colorado corpus.</abstract>
    </paper>
    <paper id="14">
      <title>Does Free Word Order Hurt? Assessing the Practical Lexical Function Model for <fixed-case>C</fixed-case>roatian</title>
      <author><first>Zoran</first> <last>Medić</last></author>
      <author><first>Jan</first> <last>Šnajder</last></author>
      <author><first>Sebastian</first> <last>Padó</last></author>
      <pages>115–120</pages>
      <url hash="3e24b3e2">S17-1014</url>
      <attachment type="poster" hash="b783686a">S17-1014.Poster.pdf</attachment>
      <doi>10.18653/v1/S17-1014</doi>
      <abstract>The Practical Lexical Function (PLF) model is a model of computational distributional semantics that attempts to strike a balance between expressivity and learnability in predicting phrase meaning and shows competitive results. We investigate how well the PLF carries over to free word order languages, given that it builds on observations of predicate-argument combinations that are harder to recover in free word order languages. We evaluate variants of the PLF for Croatian, using a new lexical substitution dataset. We find that the PLF works about as well for Croatian as for English, but demonstrate that its strength lies in modeling verbs, and that the free word order affects the less robust PLF variant.</abstract>
    </paper>
    <paper id="15">
      <title>A Mixture Model for Learning Multi-Sense Word Embeddings</title>
      <author><first>Dai Quoc</first> <last>Nguyen</last></author>
      <author><first>Dat Quoc</first> <last>Nguyen</last></author>
      <author><first>Ashutosh</first> <last>Modi</last></author>
      <author><first>Stefan</first> <last>Thater</last></author>
      <author><first>Manfred</first> <last>Pinkal</last></author>
      <pages>121–127</pages>
      <url hash="d1d01cc5">S17-1015</url>
      <doi>10.18653/v1/S17-1015</doi>
      <abstract>Word embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks.</abstract>
    </paper>
    <paper id="16">
      <title>Aligning Script Events with Narrative Texts</title>
      <author><first>Simon</first> <last>Ostermann</last></author>
      <author><first>Michael</first> <last>Roth</last></author>
      <author><first>Stefan</first> <last>Thater</last></author>
      <author><first>Manfred</first> <last>Pinkal</last></author>
      <pages>128–134</pages>
      <url hash="f842d882">S17-1016</url>
      <doi>10.18653/v1/S17-1016</doi>
      <revision id="1" href="S17-1016v1" hash="7a8b9dae"/>
      <revision id="2" href="S17-1016v2" hash="f842d882">No description of the changes were recorded.</revision>
      <abstract>Script knowledge plays a central role in text understanding and is relevant for a variety of downstream tasks. In this paper, we consider two recent datasets which provide a rich and general representation of script events in terms of paraphrase sets. We introduce the task of mapping event mentions in narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible.</abstract>
    </paper>
    <paper id="17">
      <title>The (too Many) Problems of Analogical Reasoning with Word Vectors</title>
      <author><first>Anna</first> <last>Rogers</last></author>
      <author><first>Aleksandr</first> <last>Drozd</last></author>
      <author><first>Bofang</first> <last>Li</last></author>
      <pages>135–148</pages>
      <url hash="5f22b1f9">S17-1017</url>
      <doi>10.18653/v1/S17-1017</doi>
      <abstract>This paper explores the possibilities of analogical reasoning with vector space models. Given two pairs of words with the same relation (e.g. man:woman :: king:queen), it was proposed that the offset between one pair of the corresponding word vectors can be used to identify the unknown member of the other pair (king - man + woman = queen). We argue against such “linguistic regularities” as a model for linguistic relations in vector space models and as a benchmark, and we show that the vector offset (as well as two other, better-performing methods) suffers from dependence on vector similarity.</abstract>
    </paper>
    <paper id="18">
      <title>Semantic Frames and Visual Scenes: Learning Semantic Role Inventories from Image and Video Descriptions</title>
      <author><first>Ekaterina</first> <last>Shutova</last></author>
      <author><first>Andreas</first> <last>Wundsam</last></author>
      <author><first>Helen</first> <last>Yannakoudakis</last></author>
      <pages>149–154</pages>
      <url hash="bba9a09e">S17-1018</url>
      <doi>10.18653/v1/S17-1018</doi>
      <abstract>Frame-semantic parsing and semantic role labelling, that aim to automatically assign semantic roles to arguments of verbs in a sentence, have become an active strand of research in NLP. However, to date these methods have relied on a predefined inventory of semantic roles. In this paper, we present a method to automatically learn argument role inventories for verbs from large corpora of text, images and videos. We evaluate the method against manually constructed role inventories in FrameNet and show that the visual model outperforms the language-only model and operates with a high precision.</abstract>
    </paper>
    <paper id="19">
      <title>Acquiring Predicate Paraphrases from News Tweets</title>
      <author><first>Vered</first> <last>Shwartz</last></author>
      <author><first>Gabriel</first> <last>Stanovsky</last></author>
      <author><first>Ido</first> <last>Dagan</last></author>
      <pages>155–160</pages>
      <url hash="9bbfa26b">S17-1019</url>
      <doi>10.18653/v1/S17-1019</doi>
      <abstract>We present a simple method for ever-growing extraction of predicate paraphrases from news headlines in Twitter. Analysis of the output of ten weeks of collection shows that the accuracy of paraphrases with different support levels is estimated between 60-86%. We also demonstrate that our resource is to a large extent complementary to existing resources, providing many novel paraphrases. Our resource is publicly available, continuously expanding based on daily news.</abstract>
    </paper>
    <paper id="20">
      <title>Evaluating Semantic Parsing against a Simple Web-based Question Answering Model</title>
      <author><first>Alon</first> <last>Talmor</last></author>
      <author><first>Mor</first> <last>Geva</last></author>
      <author><first>Jonathan</first> <last>Berant</last></author>
      <pages>161–167</pages>
      <url hash="68f0bc9f">S17-1020</url>
      <doi>10.18653/v1/S17-1020</doi>
      <abstract>Semantic parsing shines at analyzing complex natural language that involves composition and computation over multiple pieces of evidence. However, datasets for semantic parsing contain many factoid questions that can be answered from a single web document. In this paper, we propose to evaluate semantic parsing-based question answering models by comparing them to a question answering baseline that queries the web and extracts the answer only from web snippets, without access to the target knowledge-base. We investigate this approach on COMPLEXQUESTIONS, a dataset designed to focus on compositional language, and find that our model obtains reasonable performance (∼35 F1 compared to 41 F1 of state-of-the-art). We find in our analysis that our model performs well on complex questions involving conjunctions, but struggles on questions that involve relation composition and superlatives.</abstract>
    </paper>
    <paper id="21">
      <title>Logical Metonymy in a Distributional Model of Sentence Comprehension</title>
      <author><first>Emmanuele</first> <last>Chersoni</last></author>
      <author><first>Alessandro</first> <last>Lenci</last></author>
      <author><first>Philippe</first> <last>Blache</last></author>
      <pages>168–177</pages>
      <url hash="51648445">S17-1021</url>
      <doi>10.18653/v1/S17-1021</doi>
      <abstract>In theoretical linguistics, logical metonymy is defined as the combination of an event-subcategorizing verb with an entity-denoting direct object (e.g., The author began the book), so that the interpretation of the VP requires the retrieval of a covert event (e.g., writing). Psycholinguistic studies have revealed extra processing costs for logical metonymy, a phenomenon generally explained with the introduction of new semantic structure. In this paper, we present a general distributional model for sentence comprehension inspired by the Memory, Unification and Control model by Hagoort (2013,2016). We show that our distributional framework can account for the extra processing costs of logical metonymy and can identify the covert event in a classification task.</abstract>
    </paper>
    <paper id="22">
      <title>Double Trouble: The Problem of Construal in Semantic Annotation of Adpositions</title>
      <author><first>Jena D.</first> <last>Hwang</last></author>
      <author><first>Archna</first> <last>Bhatia</last></author>
      <author><first>Na-Rae</first> <last>Han</last></author>
      <author><first>Tim</first> <last>O’Gorman</last></author>
      <author><first>Vivek</first> <last>Srikumar</last></author>
      <author><first>Nathan</first> <last>Schneider</last></author>
      <pages>178–188</pages>
      <url hash="0a323287">S17-1022</url>
      <attachment type="presentation" hash="1c9c87d0">S17-1022.Presentation.pdf</attachment>
      <doi>10.18653/v1/S17-1022</doi>
      <abstract>We consider the semantics of prepositions, revisiting a broad-coverage annotation scheme used for annotating all 4,250 preposition tokens in a 55,000 word corpus of English. Attempts to apply the scheme to adpositions and case markers in other languages, as well as some problematic cases in English, have led us to reconsider the assumption that an adposition’s lexical contribution is equivalent to the role/relation that it mediates. Our proposal is to embrace the potential for construal in adposition use, expressing such phenomena directly at the token level to manage complexity and avoid sense proliferation. We suggest a framework to represent both the scene role and the adposition’s lexical function so they can be annotated at scale—supporting automatic, statistical processing of domain-general language—and discuss how this representation would allow for a simpler inventory of labels.</abstract>
    </paper>
    <paper id="23">
      <title>Issues of Mass and Count: Dealing with ‘Dual-Life’ Nouns</title>
      <author><first>Tibor</first> <last>Kiss</last></author>
      <author><first>Francis Jeffry</first> <last>Pelletier</last></author>
      <author><first>Halima</first> <last>Husić</last></author>
      <author><first>Johanna</first> <last>Poppek</last></author>
      <pages>189–198</pages>
      <url hash="ef9f354b">S17-1023</url>
      <doi>10.18653/v1/S17-1023</doi>
      <abstract>The topics of mass and count have been studied for many decades in philosophy (e.g., Quine, 1960; Pelletier, 1975), linguistics (e.g., McCawley, 1975; Allen, 1980; Krifka, 1991) and psychology (e.g., Middleton et al, 2004; Barner et al, 2009). More recently, interest from within computational linguistics has studied the issues involved (e.g., Pustejovsky, 1991; Bond, 2005; Schmidtke &amp; Kuperman, 2016), to name just a few. As is pointed out in these works, there are many difficult conceptual issues involved in the study of this contrast. In this article we study one of these issues – the “Dual-Life” of being simultaneously +mass and +count – by means of an unusual combination of human annotation, online lexical resources, and online corpora.</abstract>
    </paper>
    <paper id="24">
      <title>Parsing Graphs with Regular Graph Grammars</title>
      <author><first>Sorcha</first> <last>Gilroy</last></author>
      <author><first>Adam</first> <last>Lopez</last></author>
      <author><first>Sebastian</first> <last>Maneth</last></author>
      <pages>199–208</pages>
      <url hash="ed1afd48">S17-1024</url>
      <doi>10.18653/v1/S17-1024</doi>
      <abstract>Recently, several datasets have become available which represent natural language phenomena as graphs. Hyperedge Replacement Languages (HRL) have been the focus of much attention as a formalism to represent the graphs in these datasets. Chiang et al. (2013) prove that HRL graphs can be parsed in polynomial time with respect to the size of the input graph. We believe that HRL are more expressive than is necessary to represent semantic graphs and we propose the use of Regular Graph Languages (RGL; Courcelle 1991), which is a subfamily of HRL, as a possible alternative. We provide a top-down parsing algorithm for RGL that runs in time linear in the size of the input graph.</abstract>
    </paper>
    <paper id="25">
      <title>Embedded Semantic Lexicon Induction with Joint Global and Local Optimization</title>
      <author><first>Sujay Kumar</first> <last>Jauhar</last></author>
      <author><first>Eduard</first> <last>Hovy</last></author>
      <pages>209–219</pages>
      <url hash="b4cf09bc">S17-1025</url>
      <doi>10.18653/v1/S17-1025</doi>
      <abstract>Creating annotated frame lexicons such as PropBank and FrameNet is expensive and labor intensive. We present a method to induce an embedded frame lexicon in an minimally supervised fashion using nothing more than unlabeled predicate-argument word pairs. We hypothesize that aggregating such pair selectional preferences across training leads us to a global understanding that captures predicate-argument frame structure. Our approach revolves around a novel integration between a predictive embedding model and an Indian Buffet Process posterior regularizer. We show, through our experimental evaluation, that we outperform baselines on two tasks and can learn an embedded frame lexicon that is able to capture some interesting generalities in relation to hand-crafted semantic frames.</abstract>
    </paper>
    <paper id="26">
      <title>Generating Pattern-Based Entailment Graphs for Relation Extraction</title>
      <author><first>Kathrin</first> <last>Eichler</last></author>
      <author><first>Feiyu</first> <last>Xu</last></author>
      <author><first>Hans</first> <last>Uszkoreit</last></author>
      <author><first>Sebastian</first> <last>Krause</last></author>
      <pages>220–229</pages>
      <url hash="8ac3b938">S17-1026</url>
      <doi>10.18653/v1/S17-1026</doi>
      <abstract>Relation extraction is the task of recognizing and extracting relations between entities or concepts in texts. A common approach is to exploit existing knowledge to learn linguistic patterns expressing the target relation and use these patterns for extracting new relation mentions. Deriving relation patterns automatically usually results in large numbers of candidates, which need to be filtered to derive a subset of patterns that reliably extract correct relation mentions. We address the pattern selection task by exploiting the knowledge represented by entailment graphs, which capture semantic relationships holding among the learned pattern candidates. This is motivated by the fact that a pattern may not express the target relation explicitly, but still be useful for extracting instances for which the relation holds, because its meaning entails the meaning of the target relation. We evaluate the usage of both automatically generated and gold-standard entailment graphs in a relation extraction scenario and present favorable experimental results, exhibiting the benefits of structuring and selecting patterns based on entailment graphs.</abstract>
    </paper>
    <paper id="27">
      <title>Classifying Semantic Clause Types: Modeling Context and Genre Characteristics with Recurrent Neural Networks and Attention</title>
      <author><first>Maria</first> <last>Becker</last></author>
      <author><first>Michael</first> <last>Staniek</last></author>
      <author><first>Vivi</first> <last>Nastase</last></author>
      <author><first>Alexis</first> <last>Palmer</last></author>
      <author><first>Anette</first> <last>Frank</last></author>
      <pages>230–240</pages>
      <url hash="80b2ee00">S17-1027</url>
      <doi>10.18653/v1/S17-1027</doi>
      <abstract>Detecting aspectual properties of clauses in the form of situation entity types has been shown to depend on a combination of syntactic-semantic and contextual features. We explore this task in a deep-learning framework, where tuned word representations capture lexical, syntactic and semantic features. We introduce an attention mechanism that pinpoints relevant context not only for the current instance, but also for the larger context. Apart from implicitly capturing task relevant features, the advantage of our neural model is that it avoids the need to reproduce linguistic features for other languages and is thus more easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another.</abstract>
    </paper>
    <paper id="28">
      <title>Predictive Linguistic Features of Schizophrenia</title>
      <author><first>Efsun</first> <last>Sarioglu Kayi</last></author>
      <author><first>Mona</first> <last>Diab</last></author>
      <author><first>Luca</first> <last>Pauselli</last></author>
      <author><first>Michael</first> <last>Compton</last></author>
      <author><first>Glen</first> <last>Coppersmith</last></author>
      <pages>241–250</pages>
      <url hash="e6c7b2a5">S17-1028</url>
      <doi>10.18653/v1/S17-1028</doi>
      <abstract>Schizophrenia is one of the most disabling and difficult to treat of all human medical/health conditions, ranking in the top ten causes of disability worldwide. It has been a puzzle in part due to difficulty in identifying its basic, fundamental components. Several studies have shown that some manifestations of schizophrenia (e.g., the negative symptoms that include blunting of speech prosody, as well as the disorganization symptoms that lead to disordered language) can be understood from the perspective of linguistics. However, schizophrenia research has not kept pace with technologies in computational linguistics, especially in semantics and pragmatics. As such, we examine the writings of schizophrenia patients analyzing their syntax, semantics and pragmatics. In addition, we analyze tweets of (self proclaimed) schizophrenia patients who publicly discuss their diagnoses. For writing samples dataset, syntactic features are found to be the most successful in classification whereas for the less structured Twitter dataset, a combination of features performed the best.</abstract>
    </paper>
    <paper id="29">
      <title>Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks</title>
      <author><first>Mrinmaya</first> <last>Sachan</last></author>
      <author><first>Eric</first> <last>Xing</last></author>
      <pages>251–261</pages>
      <url hash="cc51c956">S17-1029</url>
      <doi>10.18653/v1/S17-1029</doi>
      <abstract>Humans as well as animals are good at imitation. Inspired by this, the learning by demonstration view of machine learning learns to perform a task from detailed example demonstrations. In this paper, we introduce the task of question answering using natural language demonstrations where the question answering system is provided with detailed demonstrative solutions to questions in natural language. As a case study, we explore the task of learning to solve geometry problems using demonstrative solutions available in textbooks. We collect a new dataset of demonstrative geometry solutions from textbooks and explore approaches that learn to interpret these demonstrations as well as to use these interpretations to solve geometry problems. Our approaches show improvements over the best previously published system for solving geometry problems.</abstract>
    </paper>
    <paper id="30">
      <title>Ways of Asking and Replying in Duplicate Question Detection</title>
      <author><first>João</first> <last>António Rodrigues</last></author>
      <author><first>Chakaveh</first> <last>Saedi</last></author>
      <author><first>Vladislav</first> <last>Maraev</last></author>
      <author><first>João</first> <last>Silva</last></author>
      <author><first>António</first> <last>Branco</last></author>
      <pages>262–270</pages>
      <url hash="ff31f4dd">S17-1030</url>
      <doi>10.18653/v1/S17-1030</doi>
      <abstract>This paper presents the results of systematic experimentation on the impact in duplicate question detection of different types of questions across both a number of established approaches and a novel, superior one used to address this language processing task. This study permits to gain a novel insight on the different levels of robustness of the diverse detection methods with respect to different conditions of their application, including the ones that approximate real usage scenarios.</abstract>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Proceedings of the 11th International Workshop on Semantic Evaluation (<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017)</booktitle>
      <url hash="d37f2cbc">S17-2</url>
      <editor><first>Steven</first><last>Bethard</last></editor>
      <editor><first>Marine</first><last>Carpuat</last></editor>
      <editor><first>Marianna</first><last>Apidianaki</last></editor>
      <editor><first>Saif M.</first><last>Mohammad</last></editor>
      <editor><first>Daniel</first><last>Cer</last></editor>
      <editor><first>David</first><last>Jurgens</last></editor>
      <doi>10.18653/v1/S17-2</doi>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vancouver, Canada</address>
      <month>August</month>
      <year>2017</year>
    </meta>
    <frontmatter>
      <url hash="d7122f80">S17-2000</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation</title>
      <author><first>Daniel</first> <last>Cer</last></author>
      <author><first>Mona</first> <last>Diab</last></author>
      <author><first>Eneko</first> <last>Agirre</last></author>
      <author><first>Iñigo</first> <last>Lopez-Gazpio</last></author>
      <author><first>Lucia</first> <last>Specia</last></author>
      <pages>1–14</pages>
      <url hash="b4bae9f8">S17-2001</url>
      <doi>10.18653/v1/S17-2001</doi>
      <abstract>Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in <i>all language tracks</i>. We summarize
      performance and review a selection of well performing methods. Analysis
      highlights common errors, providing insight into the limitations of
      existing models. To support ongoing work on semantic representations, the
      <i>STS Benchmark</i> is introduced as a new shared training and evaluation set
      carefully selected from the corpus of English STS shared task data
      (2012-2017).
    </abstract>
    </paper>
    <paper id="2">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity</title>
      <author><first>Jose</first> <last>Camacho-Collados</last></author>
      <author><first>Mohammad Taher</first> <last>Pilehvar</last></author>
      <author><first>Nigel</first> <last>Collier</last></author>
      <author><first>Roberto</first> <last>Navigli</last></author>
      <pages>15–26</pages>
      <url hash="ae94a5fa">S17-2002</url>
      <doi>10.18653/v1/S17-2002</doi>
      <abstract>This paper introduces a new task on Multilingual and Cross-lingual SemanticThis paper introduces a new task on Multilingual and Cross-lingual Semantic Word Similarity which measures the semantic similarity of word pairs within and across five languages: English, Farsi, German, Italian and Spanish. High quality datasets were manually curated for the five languages with high inter-annotator agreements (consistently in the 0.9 ballpark). These were used for semi-automatic construction of ten cross-lingual datasets. 17 teams participated in the task, submitting 24 systems in subtask 1 and 14 systems in subtask 2. Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks. More information can be found on the task website: <url>http://alt.qcri.org/semeval2017/task2/</url>
      </abstract>
    </paper>
    <paper id="3">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Community Question Answering</title>
      <author><first>Preslav</first> <last>Nakov</last></author>
      <author><first>Doris</first> <last>Hoogeveen</last></author>
      <author><first>Lluís</first> <last>Màrquez</last></author>
      <author><first>Alessandro</first> <last>Moschitti</last></author>
      <author><first>Hamdy</first> <last>Mubarak</last></author>
      <author><first>Timothy</first> <last>Baldwin</last></author>
      <author><first>Karin</first> <last>Verspoor</last></author>
      <pages>27–48</pages>
      <url hash="e72e2e5e">S17-2003</url>
      <doi>10.18653/v1/S17-2003</doi>
      <abstract>We describe SemEval–2017 Task 3 on Community Question Answering. This year, we reran the four subtasks from SemEval-2016: (A) Question–Comment Similarity, (B) Question–Question Similarity, (C) Question–External Comment Similarity, and (D) Rerank the correct answers for a new question in Arabic, providing all the data from 2015 and 2016 for training, and fresh data for testing. Additionally, we added a new subtask E in order to enable experimentation with Multi-domain Question Duplicate Detection in a larger-scale scenario, using StackExchange subforums. A total of 23 teams participated in the task, and submitted a total of 85 runs (36 primary and 49 contrastive) for subtasks A–D. Unfortunately, no teams participated in subtask E. A variety of approaches and features were used by the participating systems to address the different subtasks. The best systems achieved an official score (MAP) of 88.43, 47.22, 15.46, and 61.16 in subtasks A, B, C, and D, respectively. These scores are better than the baselines, especially for subtasks A–C.</abstract>
    </paper>
    <paper id="4">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 6: #<fixed-case>H</fixed-case>ashtag<fixed-case>W</fixed-case>ars: Learning a Sense of Humor</title>
      <author><first>Peter</first> <last>Potash</last></author>
      <author><first>Alexey</first> <last>Romanov</last></author>
      <author><first>Anna</first> <last>Rumshisky</last></author>
      <pages>49–57</pages>
      <url hash="b6a83323">S17-2004</url>
      <doi>10.18653/v1/S17-2004</doi>
      <abstract>This paper describes a new shared task for humor understanding that attempts to eschew the ubiquitous binary approach to humor detection and focus on comparative humor ranking instead. The task is based on a new dataset of funny tweets posted in response to shared hashtags, collected from the ‘Hashtag Wars’ segment of the TV show @midnight. The results are evaluated in two subtasks that require the participants to generate either the correct pairwise comparisons of tweets (subtask A), or the correct ranking of the tweets (subtask B) in terms of how funny they are. 7 teams participated in subtask A, and 5 teams participated in subtask B. The best accuracy in subtask A was 0.675. The best (lowest) rank edit distance for subtask B was 0.872.</abstract>
    </paper>
    <paper id="5">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 7: Detection and Interpretation of <fixed-case>E</fixed-case>nglish Puns</title>
      <author><first>Tristan</first> <last>Miller</last></author>
      <author><first>Christian</first> <last>Hempelmann</last></author>
      <author><first>Iryna</first> <last>Gurevych</last></author>
      <pages>58–68</pages>
      <url hash="0f60999a">S17-2005</url>
      <doi>10.18653/v1/S17-2005</doi>
      <abstract>A pun is a form of wordplay in which a word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another word, for an intended humorous or rhetorical effect. Though a recurrent and expected feature in many discourse types, puns stymie traditional approaches to computational lexical semantics because they violate their one-sense-per-context assumption. This paper describes the first competitive evaluation for the automatic detection, location, and interpretation of puns. We describe the motivation for these tasks, the evaluation methods, and the manually annotated data set. Finally, we present an overview and discussion of the participating systems’ methodologies, resources, and results.</abstract>
    </paper>
    <paper id="6">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 8: <fixed-case>R</fixed-case>umour<fixed-case>E</fixed-case>val: Determining rumour veracity and support for rumours</title>
      <author><first>Leon</first> <last>Derczynski</last></author>
      <author><first>Kalina</first> <last>Bontcheva</last></author>
      <author><first>Maria</first> <last>Liakata</last></author>
      <author><first>Rob</first> <last>Procter</last></author>
      <author><first>Geraldine</first> <last>Wong Sak Hoi</last></author>
      <author><first>Arkaitz</first> <last>Zubiaga</last></author>
      <pages>69–76</pages>
      <url hash="594c0236">S17-2006</url>
      <doi>10.18653/v1/S17-2006</doi>
      <abstract>Media is full of false claims. Even Oxford Dictionaries named “post-truth” as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the nature of the discourse around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We present an annotation scheme, a large dataset covering multiple topics – each having their own families of claims and replies – and use these to pose two concrete challenges as well as the results achieved by participants on these challenges.</abstract>
    </paper>
    <paper id="7">
      <title><fixed-case>BIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Using Semantic Information Space to Evaluate Semantic Textual Similarity</title>
      <author><first>Hao</first> <last>Wu</last></author>
      <author><first>Heyan</first> <last>Huang</last></author>
      <author><first>Ping</first> <last>Jian</last></author>
      <author><first>Yuhang</first> <last>Guo</last></author>
      <author><first>Chao</first> <last>Su</last></author>
      <pages>77–84</pages>
      <url hash="7e86d41a">S17-2007</url>
      <doi>10.18653/v1/S17-2007</doi>
      <abstract>This paper presents three systems for semantic textual similarity (STS) evaluation at SemEval-2017 STS task. One is an unsupervised system and the other two are supervised systems which simply employ the unsupervised one. All our systems mainly depend on the (SIS), which is constructed based on the semantic hierarchical taxonomy in WordNet, to compute non-overlapping information content (IC) of sentences. Our team ranked 2nd among 31 participating teams by the primary score of Pearson correlation coefficient (PCC) mean of 7 tracks and achieved the best performance on Track 1 (AR-AR) dataset.</abstract>
    </paper>
    <paper id="8">
      <title><fixed-case>C</fixed-case>oncept<fixed-case>N</fixed-case>et at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2: Extending Word Embeddings with Multilingual Relational Knowledge</title>
      <author><first>Robyn</first> <last>Speer</last></author>
      <author><first>Joanna</first> <last>Lowry-Duda</last></author>
      <pages>85–89</pages>
      <url hash="e22cb6c7">S17-2008</url>
      <doi>10.18653/v1/S17-2008</doi>
      <revision id="1" href="S17-2008v1" hash="cded96af"/>
      <revision id="2" href="S17-2008v2" hash="e22cb6c7">No description of the changes were recorded.</revision>
      <abstract>This paper describes Luminoso’s participation in SemEval 2017 Task 2, “Multilingual and Cross-lingual Semantic Word Similarity”, with a system based on ConceptNet. ConceptNet is an open, multilingual knowledge graph that focuses on general knowledge that relates the meanings of words and phrases. Our submission to SemEval was an update of previous work that builds high-quality, multilingual word embeddings from a combination of ConceptNet and distributional semantics. Our system took first place in both subtasks. It ranked first in 4 out of 5 of the separate languages, and also ranked first in all 10 of the cross-lingual language pairs.</abstract>
    </paper>
    <paper id="9">
      <title><fixed-case>IIT</fixed-case>-<fixed-case>UHH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification</title>
      <author><first>Titas</first> <last>Nandi</last></author>
      <author><first>Chris</first> <last>Biemann</last></author>
      <author><first>Seid Muhie</first> <last>Yimam</last></author>
      <author><first>Deepak</first> <last>Gupta</last></author>
      <author><first>Sarah</first> <last>Kohail</last></author>
      <author><first>Asif</first> <last>Ekbal</last></author>
      <author><first>Pushpak</first> <last>Bhattacharyya</last></author>
      <pages>90–97</pages>
      <url hash="e5637e75">S17-2009</url>
      <doi>10.18653/v1/S17-2009</doi>
      <abstract>In this paper we present the system for Answer Selection and Ranking in Community Question Answering, which we build as part of our participation in SemEval-2017 Task 3. We develop a Support Vector Machine (SVM) based system that makes use of textual, domain-specific, word-embedding and topic-modeling features. In addition, we propose a novel method for dialogue chain identification in comment threads. Our primary submission won subtask C, outperforming other systems in all the primary evaluation metrics. We performed well in other English subtasks, ranking third in subtask A and eighth in subtask B. We also developed open source toolkits for all the three English subtasks by the name cQARank [<url>https://github.com/TitasNandi/cQARank</url>].
    </abstract>
    </paper>
    <paper id="10">
      <title><fixed-case>H</fixed-case>umor<fixed-case>H</fixed-case>awk at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 6: Mixing Meaning and Sound for Humor Recognition</title>
      <author><first>David</first> <last>Donahue</last></author>
      <author><first>Alexey</first> <last>Romanov</last></author>
      <author><first>Anna</first> <last>Rumshisky</last></author>
      <pages>98–102</pages>
      <url hash="361d0c3f">S17-2010</url>
      <doi>10.18653/v1/S17-2010</doi>
      <abstract>This paper describes the winning system for SemEval-2017 Task 6: #HashtagWars: Learning a Sense of Humor. Humor detection has up until now been predominantly addressed using feature-based approaches. Our system utilizes recurrent deep learning methods with dense embeddings to predict humorous tweets from the @midnight show #HashtagWars. In order to include both meaning and sound in the analysis, GloVe embeddings are combined with a novel phonetic representation to serve as input to an LSTM component. The output is combined with a character-based CNN model, and an XGBoost component in an ensemble model which achieves 0.675 accuracy on the evaluation data.</abstract>
    </paper>
    <paper id="11">
      <title>Idiom Savant at <fixed-case>S</fixed-case>emeval-2017 Task 7: Detection and Interpretation of <fixed-case>E</fixed-case>nglish Puns</title>
      <author><first>Samuel</first> <last>Doogan</last></author>
      <author><first>Aniruddha</first> <last>Ghosh</last></author>
      <author><first>Hanyang</first> <last>Chen</last></author>
      <author><first>Tony</first> <last>Veale</last></author>
      <pages>103–108</pages>
      <url hash="c9c96f67">S17-2011</url>
      <doi>10.18653/v1/S17-2011</doi>
      <abstract>This paper describes our system, entitled Idiom Savant, for the 7th Task of the Semeval 2017 workshop, “Detection and interpretation of English Puns”. Our system consists of two probabilistic models for each type of puns using Google n-gram and Word2Vec. Our system achieved f-score of calculating, 0.663, and 0.07 in homographic puns and 0.8439, 0.6631, and 0.0806 in heterographic puns in task 1, task 2, and task 3 respectively.</abstract>
    </paper>
    <paper id="12">
      <title><fixed-case>C</fixed-case>ompi<fixed-case>LIG</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Cross-Language Plagiarism Detection Methods for Semantic Textual Similarity</title>
      <author><first>Jérémy</first> <last>Ferrero</last></author>
      <author><first>Laurent</first> <last>Besacier</last></author>
      <author><first>Didier</first> <last>Schwab</last></author>
      <author><first>Frédéric</first> <last>Agnès</last></author>
      <pages>109–114</pages>
      <url hash="daf686f9">S17-2012</url>
      <attachment type="poster" hash="cc63aa82">S17-2012.Poster.pdf</attachment>
      <doi>10.18653/v1/S17-2012</doi>
      <abstract>We present our submitted systems for Semantic Textual Similarity (STS) Track 4 at SemEval-2017. Given a pair of Spanish-English sentences, each system must estimate their semantic similarity by a score between 0 and 5. In our submission, we use syntax-based, dictionary-based, context-based, and MT-based methods. We also combine these methods in unsupervised and supervised way. Our best run ranked 1st on track 4a with a correlation of 83.02% with human annotations.</abstract>
    </paper>
    <paper id="13">
      <title><fixed-case>U</fixed-case>d<fixed-case>L</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Semantic Textual Similarity Estimation of <fixed-case>E</fixed-case>nglish Sentence Pairs Using Regression Model over Pairwise Features</title>
      <author><first>Hussein T.</first> <last>Al-Natsheh</last></author>
      <author><first>Lucie</first> <last>Martinet</last></author>
      <author><first>Fabrice</first> <last>Muhlenbach</last></author>
      <author><first>Djamel Abdelkader</first> <last>Zighed</last></author>
      <pages>115–119</pages>
      <url hash="a7f9d8d2">S17-2013</url>
      <doi>10.18653/v1/S17-2013</doi>
      <abstract>This paper describes the model UdL we proposed to solve the semantic textual similarity task of SemEval 2017 workshop. The track we participated in was estimating the semantics relatedness of a given set of sentence pairs in English. The best run out of three submitted runs of our model achieved a Pearson correlation score of 0.8004 compared to a hidden human annotation of 250 pairs. We used random forest ensemble learning to map an expandable set of extracted pairwise features into a semantic similarity estimated value bounded between 0 and 5. Most of these features were calculated using word embedding vectors similarity to align Part of Speech (PoS) and Name Entities (NE) tagged tokens of each sentence pair. Among other pairwise features, we experimented a classical tf-idf weighted Bag of Words (BoW) vector model but with character-based range of n-grams instead of words. This sentence vector BoW-based feature gave a relatively high importance value percentage in the feature importances analysis of the ensemble learning.</abstract>
    </paper>
    <paper id="14">
      <title><fixed-case>DT</fixed-case>_<fixed-case>T</fixed-case>eam at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Semantic Similarity Using Alignments, Sentence-Level Embeddings and <fixed-case>G</fixed-case>aussian Mixture Model Output</title>
      <author><first>Nabin</first> <last>Maharjan</last></author>
      <author><first>Rajendra</first> <last>Banjade</last></author>
      <author><first>Dipesh</first> <last>Gautam</last></author>
      <author><first>Lasang J.</first> <last>Tamang</last></author>
      <author><first>Vasile</first> <last>Rus</last></author>
      <pages>120–124</pages>
      <url hash="e110274b">S17-2014</url>
      <doi>10.18653/v1/S17-2014</doi>
      <abstract>We describe our system (DT Team) submitted at SemEval-2017 Task 1, Semantic Textual Similarity (STS) challenge for English (Track 5). We developed three different models with various features including similarity scores calculated using word and chunk alignments, word/sentence embeddings, and Gaussian Mixture Model(GMM). The correlation between our system’s output and the human judgments were up to 0.8536, which is more than 10% above baseline, and almost as good as the best performing system which was at 0.8547 correlation (the difference is just about 0.1%). Also, our system produced leading results when evaluated with a separate STS benchmark dataset. The word alignment and sentence embeddings based features were found to be very effective.</abstract>
    </paper>
    <paper id="15">
      <title><fixed-case>FCICU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Sense-Based Language Independent Semantic Textual Similarity Approach</title>
      <author><first>Basma</first> <last>Hassan</last></author>
      <author><first>Samir</first> <last>AbdelRahman</last></author>
      <author><first>Reem</first> <last>Bahgat</last></author>
      <author><first>Ibrahim</first> <last>Farag</last></author>
      <pages>125–129</pages>
      <url hash="5f79c614">S17-2015</url>
      <doi>10.18653/v1/S17-2015</doi>
      <abstract>This paper describes FCICU team systems that participated in SemEval-2017 Semantic Textual Similarity task (Task1) for monolingual and cross-lingual sentence pairs. A sense-based language independent textual similarity approach is presented, in which a proposed alignment similarity method coupled with new usage of a semantic network (BabelNet) is used. Additionally, a previously proposed integration between sense-based and sur-face-based semantic textual similarity approach is applied together with our proposed approach. For all the tracks in Task1, Run1 is a string kernel with alignments metric and Run2 is a sense-based alignment similarity method. The first run is ranked 10th, and the second is ranked 12th in the primary track, with correlation 0.619 and 0.617 respectively</abstract>
    </paper>
    <paper id="16">
      <title><fixed-case>HCTI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Use convolutional neural network to evaluate Semantic Textual Similarity</title>
      <author><first>Yang</first> <last>Shao</last></author>
      <pages>130–133</pages>
      <url hash="4be91351">S17-2016</url>
      <doi>10.18653/v1/S17-2016</doi>
      <abstract>This paper describes our convolutional neural network (CNN) system for Semantic Textual Similarity (STS) task. We calculated semantic similarity score between two sentences by comparing their semantic vectors. We generated semantic vector of every sentence by max pooling every dimension of their word vectors. There are mainly two trick points in our system. One is that we trained a CNN to transfer GloVe word vectors to a more proper form for STS task before pooling. Another is that we trained a fully-connected neural network (FCNN) to transfer difference of two semantic vectors to probability of every similarity score. We decided all hyper parameters empirically. In spite of the simplicity of our neural network system, we achieved a good accuracy and ranked 3rd in primary track of SemEval 2017.</abstract>
    </paper>
    <paper id="17">
      <title><fixed-case>LIM</fixed-case>-<fixed-case>LIG</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task1: Enhancing the Semantic Similarity for <fixed-case>A</fixed-case>rabic Sentences with Vectors Weighting</title>
      <author><first>El Moatez Billah</first> <last>Nagoudi</last></author>
      <author><first>Jérémy</first> <last>Ferrero</last></author>
      <author><first>Didier</first> <last>Schwab</last></author>
      <pages>134–138</pages>
      <url hash="1fa0cdae">S17-2017</url>
      <doi>10.18653/v1/S17-2017</doi>
      <abstract>This article describes our proposed system named LIM-LIG. This system is designed for SemEval 2017 Task1: Semantic Textual Similarity (Track1). LIM-LIG proposes an innovative enhancement to word embedding-based model devoted to measure the semantic similarity in Arabic sentences. The main idea is to exploit the word representations as vectors in a multidimensional space to capture the semantic and syntactic properties of words. IDF weighting and Part-of-Speech tagging are applied on the examined sentences to support the identification of words that are highly descriptive in each sentence. LIM-LIG system achieves a Pearson’s correlation of 0.74633, ranking 2nd among all participants in the Arabic monolingual pairs STS task organized within the SemEval 2017 evaluation campaign</abstract>
    </paper>
    <paper id="18">
      <title><fixed-case>OPI</fixed-case>-<fixed-case>JSA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Application of Ensemble learning for computing semantic textual similarity</title>
      <author><first>Martyna</first> <last>Śpiewak</last></author>
      <author><first>Piotr</first> <last>Sobecki</last></author>
      <author><first>Daniel</first> <last>Karaś</last></author>
      <pages>139–143</pages>
      <url hash="b54bba03">S17-2018</url>
      <doi>10.18653/v1/S17-2018</doi>
      <abstract>Semantic Textual Similarity (STS) evaluation assesses the degree to which two parts of texts are similar, based on their semantic evaluation. In this paper, we describe three models submitted to STS SemEval 2017. Given two English parts of a text, each of proposed methods outputs the assessment of their semantic similarity. We propose an approach for computing monolingual semantic textual similarity based on an ensemble of three distinct methods. Our model consists of recursive neural network (RNN) text auto-encoders ensemble with supervised a model of vectorized sentences using reduced part of speech (PoS) weighted word embeddings as well as unsupervised a method based on word coverage (TakeLab). Additionally, we enrich our model with additional features that allow disambiguation of ensemble methods based on their efficiency. We have used Multi-Layer Perceptron as an ensemble classifier basing on estimations of trained Gradient Boosting Regressors. Results of our research proves that using such ensemble leads to a higher accuracy due to a fact that each member-algorithm tends to specialize in particular type of sentences. Simple model based on PoS weighted Word2Vec word embeddings seem to improve performance of more complex RNN based auto-encoders in the ensemble. In the monolingual English-English STS subtask our Ensemble based model achieved mean Pearson correlation of .785 compared with human annotators.</abstract>
    </paper>
    <paper id="19">
      <title>Lump at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Towards an Interlingua Semantic Similarity</title>
      <author><first>Cristina</first> <last>España-Bonet</last></author>
      <author><first>Alberto</first> <last>Barrón-Cedeño</last></author>
      <pages>144–149</pages>
      <url hash="7658234e">S17-2019</url>
      <doi>10.18653/v1/S17-2019</doi>
      <abstract>This is the Lump team participation at SemEval 2017 Task 1 on Semantic Textual Similarity. Our supervised model relies on features which are multilingual or interlingual in nature. We include lexical similarities, cross-language explicit semantic analysis, internal representations of multilingual neural networks and interlingual word embeddings. Our representations allow to use large datasets in language pairs with many instances to better classify instances in smaller language pairs avoiding the necessity of translating into a single language. Hence we can deal with all the languages in the task: Arabic, English, Spanish, and Turkish.</abstract>
    </paper>
    <paper id="20">
      <title><fixed-case>QLUT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Semantic Textual Similarity Based on Word Embeddings</title>
      <author><first>Fanqing</first> <last>Meng</last></author>
      <author><first>Wenpeng</first> <last>Lu</last></author>
      <author><first>Yuteng</first> <last>Zhang</last></author>
      <author><first>Jinyong</first> <last>Cheng</last></author>
      <author><first>Yuehan</first> <last>Du</last></author>
      <author><first>Shuwang</first> <last>Han</last></author>
      <pages>150–153</pages>
      <url hash="8474fef1">S17-2020</url>
      <doi>10.18653/v1/S17-2020</doi>
      <abstract>This paper reports the details of our submissions in the task 1 of SemEval 2017. This task aims at assessing the semantic textual similarity of two sentences or texts. We submit three unsupervised systems based on word embeddings. The differences between these runs are the various preprocessing on evaluation data. The best performance of these systems on the evaluation of Pearson correlation is 0.6887. Unsurprisingly, results of our runs demonstrate that data preprocessing, such as tokenization, lemmatization, extraction of content words and removing stop words, is helpful and plays a significant role in improving the performance of models.</abstract>
    </paper>
    <paper id="21">
      <title><fixed-case>R</fixed-case>es<fixed-case>S</fixed-case>im at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Multilingual Word Representations for Semantic Textual Similarity</title>
      <author><first>Johannes</first> <last>Bjerva</last></author>
      <author><first>Robert</first> <last>Östling</last></author>
      <pages>154–158</pages>
      <url hash="b0fd75f2">S17-2021</url>
      <doi>10.18653/v1/S17-2021</doi>
      <abstract>Shared Task 1 at SemEval-2017 deals with assessing the semantic similarity between sentences, either in the same or in different languages. In our system submission, we employ multilingual word representations, in which similar words in different languages are close to one another. Using such representations is advantageous, since the increasing amount of available parallel data allows for the application of such methods to many of the languages in the world. Hence, semantic similarity can be inferred even for languages for which no annotated data exists. Our system is trained and evaluated on all language pairs included in the shared task (English, Spanish, Arabic, and Turkish). Although development results are promising, our system does not yield high performance on the shared task test sets.</abstract>
    </paper>
    <paper id="22">
      <title><fixed-case>ITNLP</fixed-case>-<fixed-case>A</fixed-case>i<fixed-case>KF</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Rich Features Based <fixed-case>SVR</fixed-case> for Semantic Textual Similarity Computing</title>
      <author><first>Wenjie</first> <last>Liu</last></author>
      <author><first>Chengjie</first> <last>Sun</last></author>
      <author><first>Lei</first> <last>Lin</last></author>
      <author><first>Bingquan</first> <last>Liu</last></author>
      <pages>159–163</pages>
      <url hash="e6b1561b">S17-2022</url>
      <doi>10.18653/v1/S17-2022</doi>
      <abstract>Semantic Textual Similarity (STS) devotes to measuring the degree of equivalence in the underlying semantic of the sentence pair. We proposed a new system, ITNLP-AiKF, which applies in the SemEval 2017 Task1 Semantic Textual Similarity track 5 English monolingual pairs. In our system, rich features are involved, including Ontology based, word embedding based, Corpus based, Alignment based and Literal based feature. We leveraged the features to predict sentence pair similarity by a Support Vector Regression (SVR) model. In the result, a Pearson Correlation of 0.8231 is achieved by our system, which is a competitive result in the contest of this track.</abstract>
    </paper>
    <paper id="23">
      <title>Neobility at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: An Attention-based Sentence Similarity Model</title>
      <author><first>WenLi</first> <last>Zhuang</last></author>
      <author><first>Ernie</first> <last>Chang</last></author>
      <pages>164–169</pages>
      <url hash="b407e6bc">S17-2023</url>
      <doi>10.18653/v1/S17-2023</doi>
      <abstract>This paper describes a neural-network model which performed competitively (top 6) at the SemEval 2017 cross-lingual Semantic Textual Similarity (STS) task. Our system employs an attention-based recurrent neural network model that optimizes the sentence similarity. In this paper, we describe our participation in the multilingual STS task which measures similarity across English, Spanish, and Arabic.</abstract>
    </paper>
    <paper id="24">
      <title><fixed-case>SEF</fixed-case>@<fixed-case>UHH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Unsupervised Knowledge-Free Semantic Textual Similarity via Paragraph Vector</title>
      <author><first>Mirela-Stefania</first> <last>Duma</last></author>
      <author><first>Wolfgang</first> <last>Menzel</last></author>
      <pages>170–174</pages>
      <url hash="94f6b012">S17-2024</url>
      <doi>10.18653/v1/S17-2024</doi>
      <abstract>This paper describes our unsupervised knowledge-free approach to the SemEval-2017 Task 1 Competition. The proposed method makes use of Paragraph Vector for assessing the semantic similarity between pairs of sentences. We experimented with various dimensions of the vector and three state-of-the-art similarity metrics. Given a cross-lingual task, we trained models corresponding to its two languages and combined the models by averaging the similarity scores. The results of our submitted runs are above the median scores for five out of seven test sets by means of Pearson Correlation. Moreover, one of our system runs performed best on the Spanish-English-WMT test set ranking first out of 53 runs submitted in total by all participants.</abstract>
    </paper>
    <paper id="25">
      <title><fixed-case>STS</fixed-case>-<fixed-case>UHH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Scoring Semantic Textual Similarity Using Supervised and Unsupervised Ensemble</title>
      <author><first>Sarah</first> <last>Kohail</last></author>
      <author><first>Amr Rekaby</first> <last>Salama</last></author>
      <author><first>Chris</first> <last>Biemann</last></author>
      <pages>175–179</pages>
      <url hash="a69fa5ee">S17-2025</url>
      <doi>10.18653/v1/S17-2025</doi>
      <abstract>This paper reports the STS-UHH participation in the SemEval 2017 shared Task 1 of Semantic Textual Similarity (STS). Overall, we submitted 3 runs covering monolingual and cross-lingual STS tracks. Our participation involves two approaches: unsupervised approach, which estimates a word alignment-based similarity score, and supervised approach, which combines dependency graph similarity and coverage features with lexical similarity measures using regression methods. We also present a way on ensembling both models. Out of 84 submitted runs, our team best multi-lingual run has been ranked 12th in overall performance with correlation of 0.61, 7th among 31 participating teams.</abstract>
    </paper>
    <paper id="26">
      <title><fixed-case>UMD</fixed-case>eep at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: End-to-End Shared Weight <fixed-case>LSTM</fixed-case> Model for Semantic Textual Similarity</title>
      <author><first>Joe</first> <last>Barrow</last></author>
      <author><first>Denis</first> <last>Peskov</last></author>
      <pages>180–184</pages>
      <url hash="3a385a7c">S17-2026</url>
      <doi>10.18653/v1/S17-2026</doi>
      <abstract>We describe a modified shared-LSTM network for the Semantic Textual Similarity (STS) task at SemEval-2017. The network builds on previously explored Siamese network architectures. We treat max sentence length as an additional hyperparameter to be tuned (beyond learning rate, regularization, and dropout). Our results demonstrate that hand-tuning max sentence training length significantly improves final accuracy. After optimizing hyperparameters, we train the network on the multilingual semantic similarity task using pre-translated sentences. We achieved a correlation of 0.4792 for all the subtasks. We achieved the fourth highest team correlation for Task 4b, which was our best relative placement.</abstract>
    </paper>
    <paper id="27">
      <title><fixed-case>MITRE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Simple Semantic Similarity</title>
      <author><first>John</first> <last>Henderson</last></author>
      <author><first>Elizabeth</first> <last>Merkhofer</last></author>
      <author><first>Laura</first> <last>Strickhart</last></author>
      <author><first>Guido</first> <last>Zarrella</last></author>
      <pages>185–190</pages>
      <url hash="04ef69ac">S17-2027</url>
      <doi>10.18653/v1/S17-2027</doi>
      <abstract>This paper describes MITRE’s participation in the Semantic Textual Similarity task (SemEval-2017 Task 1), which evaluated machine learning approaches to the identification of similar meaning among text snippets in English, Arabic, Spanish, and Turkish. We detail the techniques we explored ranging from simple bag-of-ngrams classifiers to neural architectures with varied attention and alignment mechanisms. Linear regression is used to tie the systems together into an ensemble submitted for evaluation. The resulting system is capable of matching human similarity ratings of image captions with correlations of 0.73 to 0.83 in monolingual settings and 0.68 to 0.78 in cross-lingual conditions, demonstrating the power of relatively simple approaches.</abstract>
    </paper>
    <paper id="28">
      <title><fixed-case>ECNU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Leverage Kernel-based Traditional <fixed-case>NLP</fixed-case> features and Neural Networks to Build a Universal Model for Multilingual and Cross-lingual Semantic Textual Similarity</title>
      <author><first>Junfeng</first> <last>Tian</last></author>
      <author><first>Zhiheng</first> <last>Zhou</last></author>
      <author><first>Man</first> <last>Lan</last></author>
      <author><first>Yuanbin</first> <last>Wu</last></author>
      <pages>191–197</pages>
      <url hash="3f2d4dc5">S17-2028</url>
      <doi>10.18653/v1/S17-2028</doi>
      <abstract>To address semantic similarity on multilingual and cross-lingual sentences, we firstly translate other foreign languages into English, and then feed our monolingual English system with various interactive features. Our system is further supported by combining with deep learning semantic similarity and our best run achieves the mean Pearson correlation 73.16% in primary track.</abstract>
    </paper>
    <paper id="29">
      <title><fixed-case>P</fixed-case>urdue<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Predicting Semantic Textual Similarity with Paraphrase and Event Embeddings</title>
      <author><first>I-Ta</first> <last>Lee</last></author>
      <author><first>Mahak</first> <last>Goindani</last></author>
      <author><first>Chang</first> <last>Li</last></author>
      <author><first>Di</first> <last>Jin</last></author>
      <author><first>Kristen Marie</first> <last>Johnson</last></author>
      <author><first>Xiao</first> <last>Zhang</last></author>
      <author><first>Maria Leonor</first> <last>Pacheco</last></author>
      <author><first>Dan</first> <last>Goldwasser</last></author>
      <pages>198–202</pages>
      <url hash="5238c1f0">S17-2029</url>
      <doi>10.18653/v1/S17-2029</doi>
      <abstract>This paper describes our proposed solution for SemEval 2017 Task 1: Semantic Textual Similarity (Daniel Cer and Specia, 2017). The task aims at measuring the degree of equivalence between sentences given in English. Performance is evaluated by computing Pearson Correlation scores between the predicted scores and human judgements. Our proposed system consists of two subsystems and one regression model for predicting STS scores. The two subsystems are designed to learn Paraphrase and Event Embeddings that can take the consideration of paraphrasing characteristics and sentence structures into our system. The regression model associates these embeddings to make the final predictions. The experimental result shows that our system acquires 0.8 of Pearson Correlation Scores in this task.</abstract>
    </paper>
    <paper id="30">
      <title><fixed-case>RTM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Referential Translation Machines for Predicting Semantic Similarity</title>
      <author><first>Ergun</first> <last>Biçici</last></author>
      <pages>203–207</pages>
      <url hash="0d9940d5">S17-2030</url>
      <doi>10.18653/v1/S17-2030</doi>
      <abstract>We use referential translation machines for predicting the semantic similarity of text in all STS tasks which contain Arabic, English, Spanish, and Turkish this year. RTMs pioneer a language independent approach to semantic similarity and remove the need to access any task or domain specific information or resource. RTMs become 6th out of 52 submissions in Spanish to English STS. We average prediction scores using weights based on the training performance to improve the overall performance.</abstract>
    </paper>
    <paper id="31">
      <title><fixed-case>LIPN</fixed-case>-<fixed-case>IIMAS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 1: Subword Embeddings, Attention Recurrent Neural Networks and Cross Word Alignment for Semantic Textual Similarity</title>
      <author><first>Ignacio</first> <last>Arroyo-Fernández</last></author>
      <author><first>Ivan Vladimir</first> <last>Meza Ruiz</last></author>
      <pages>208–212</pages>
      <url hash="311b7a5f">S17-2031</url>
      <doi>10.18653/v1/S17-2031</doi>
      <abstract>In this paper we report our attempt to use, on the one hand, state-of-the-art neural approaches that are proposed to measure Semantic Textual Similarity (STS). On the other hand, we propose an unsupervised cross-word alignment approach, which is linguistically motivated. The neural approaches proposed herein are divided into two main stages. The first stage deals with constructing neural word embeddings, the components of sentence embeddings. The second stage deals with constructing a semantic similarity function relating pairs of sentence embeddings. Unfortunately our competition results were poor in all tracks, therefore we concentrated our research to improve them for Track 5 (EN-EN).</abstract>
    </paper>
    <paper id="32">
      <title><fixed-case>L</fixed-case>2<fixed-case>F</fixed-case>/<fixed-case>INESC</fixed-case>-<fixed-case>ID</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Tasks 1 and 2: Lexical and semantic features in word and textual similarity</title>
      <author><first>Pedro</first> <last>Fialho</last></author>
      <author><first>Hugo</first> <last>Patinho Rodrigues</last></author>
      <author><first>Luísa</first> <last>Coheur</last></author>
      <author><first>Paulo</first> <last>Quaresma</last></author>
      <pages>213–219</pages>
      <url hash="f72880b9">S17-2032</url>
      <doi>10.18653/v1/S17-2032</doi>
      <abstract>This paper describes our approach to the SemEval-2017 “Semantic Textual Similarity” and “Multilingual Word Similarity” tasks. In the former, we test our approach in both English and Spanish, and use a linguistically-rich set of features. These move from lexical to semantic features. In particular, we try to take advantage of the recent Abstract Meaning Representation and SMATCH measure. Although without state of the art results, we introduce semantic structures in textual similarity and analyze their impact. Regarding word similarity, we target the English language and combine WordNet information with Word Embeddings. Without matching the best systems, our approach proved to be simple and effective.</abstract>
    </paper>
    <paper id="33">
      <title><fixed-case>HCCL</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2: Combining Multilingual Word Embeddings and Transliteration Model for Semantic Similarity</title>
      <author><first>Junqing</first> <last>He</last></author>
      <author><first>Long</first> <last>Wu</last></author>
      <author><first>Xuemin</first> <last>Zhao</last></author>
      <author><first>Yonghong</first> <last>Yan</last></author>
      <pages>220–225</pages>
      <url hash="be394bf6">S17-2033</url>
      <doi>10.18653/v1/S17-2033</doi>
      <abstract>In this paper, we introduce an approach to combining word embeddings and machine translation for multilingual semantic word similarity, the task2 of SemEval-2017. Thanks to the unsupervised transliteration model, our cross-lingual word embeddings encounter decreased sums of OOVs. Our results are produced using only monolingual Wikipedia corpora and a limited amount of sentence-aligned data. Although relatively little resources are utilized, our system ranked 3rd in the monolingual subtask and can be the 6th in the cross-lingual subtask.</abstract>
    </paper>
    <paper id="34">
      <title><fixed-case>C</fixed-case>itius at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2: Cross-Lingual Similarity from Comparable Corpora and Dependency-Based Contexts</title>
      <author><first>Pablo</first> <last>Gamallo</last></author>
      <pages>226–229</pages>
      <url hash="ed909af4">S17-2034</url>
      <doi>10.18653/v1/S17-2034</doi>
      <abstract>This article describes the distributional strategy submitted by the Citius team to the SemEval 2017 Task 2. Even though the team participated in two subtasks, namely monolingual and cross-lingual word similarity, the article is mainly focused on the cross-lingual subtask. Our method uses comparable corpora and syntactic dependencies to extract count-based and transparent bilingual distributional contexts. The evaluation of the results show that our method is competitive with other cross-lingual strategies, even those using aligned and parallel texts.</abstract>
    </paper>
    <paper id="35">
      <title>Jmp8 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2: A simple and general distributional approach to estimate word similarity</title>
      <author><first>Josué</first> <last>Melka</last></author>
      <author><first>Gilles</first> <last>Bernard</last></author>
      <pages>230–234</pages>
      <url hash="88b88204">S17-2035</url>
      <doi>10.18653/v1/S17-2035</doi>
      <abstract>We have built a simple corpus-based system to estimate words similarity in multiple languages with a count-based approach. After training on Wikipedia corpora, our system was evaluated on the multilingual subtask of SemEval-2017 Task 2 and achieved a good level of performance, despite its great simplicity. Our results tend to demonstrate the power of the distributional approach in semantic similarity tasks, even without knowledge of the underlying language. We also show that dimensionality reduction has a considerable impact on the results.</abstract>
    </paper>
    <paper id="36">
      <title><fixed-case>QLUT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2: Word Similarity Based on Word Embedding and Knowledge Base</title>
      <author><first>Fanqing</first> <last>Meng</last></author>
      <author><first>Wenpeng</first> <last>Lu</last></author>
      <author><first>Yuteng</first> <last>Zhang</last></author>
      <author><first>Ping</first> <last>Jian</last></author>
      <author><first>Shumin</first> <last>Shi</last></author>
      <author><first>Heyan</first> <last>Huang</last></author>
      <pages>235–238</pages>
      <url hash="a2b36efd">S17-2036</url>
      <doi>10.18653/v1/S17-2036</doi>
      <abstract>This paper shows the details of our system submissions in the task 2 of SemEval 2017. We take part in the subtask 1 of this task, which is an English monolingual subtask. This task is designed to evaluate the semantic word similarity of two linguistic items. The results of runs are assessed by standard Pearson and Spearman correlation, contrast with official gold standard set. The best performance of our runs is 0.781 (Final). The techniques of our runs mainly make use of the word embeddings and the knowledge-based method. The results demonstrate that the combined method is effective for the computation of word similarity, while the word embeddings and the knowledge-based technique, respectively, needs more deeply improvement in details.</abstract>
    </paper>
    <paper id="37">
      <title><fixed-case>RUFINO</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2: Cross-lingual lexical similarity by extending <fixed-case>PMI</fixed-case> and word embeddings systems with a <fixed-case>S</fixed-case>wadesh’s-like list</title>
      <author><first>Sergio</first> <last>Jimenez</last></author>
      <author><first>George</first> <last>Dueñas</last></author>
      <author><first>Lorena</first> <last>Gaitan</last></author>
      <author><first>Jorge</first> <last>Segura</last></author>
      <pages>239–244</pages>
      <url hash="fcc3503e">S17-2037</url>
      <doi>10.18653/v1/S17-2037</doi>
      <abstract>The RUFINO team proposed a non-supervised, conceptually-simple and low-cost approach for addressing the Multilingual and Cross-lingual Semantic Word Similarity challenge at SemEval 2017. The proposed systems were cross-lingual extensions of popular monolingual lexical similarity approaches such as PMI and word2vec. The extensions were possible by means of a small parallel list of concepts similar to the Swadesh’s list, which we obtained in a semi-automatic way. In spite of its simplicity, our approach showed to be effective obtaining statistically-significant and consistent results in all datasets proposed for the task. Besides, we provide some research directions for improving this novel and affordable approach.</abstract>
    </paper>
    <paper id="38">
      <title><fixed-case>MERALI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2 Subtask 1: a Cognitively Inspired approach</title>
      <author><first>Enrico</first> <last>Mensa</last></author>
      <author><first>Daniele P.</first> <last>Radicioni</last></author>
      <author><first>Antonio</first> <last>Lieto</last></author>
      <pages>245–249</pages>
      <url hash="6ffb5c73">S17-2038</url>
      <doi>10.18653/v1/S17-2038</doi>
      <abstract>In this paper we report on the participation of the MERALI system to the SemEval Task 2 Subtask 1. The MERALI system approaches conceptual similarity through a simple, cognitively inspired, heuristics; it builds on a linguistic resource, the TTCS-e, that relies on BabelNet, NASARI and ConceptNet. The linguistic resource in fact contains a novel mixture of common-sense and encyclopedic knowledge. The obtained results point out that there is ample room for improvement, so that they are used to elaborate on present limitations and on future steps.</abstract>
    </paper>
    <paper id="39">
      <title><fixed-case>HHU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2: Fast Hash-Based Embeddings for Semantic Word Similarity Assessment</title>
      <author><first>Behrang</first> <last>QasemiZadeh</last></author>
      <author><first>Laura</first> <last>Kallmeyer</last></author>
      <pages>250–255</pages>
      <url hash="e970d8ba">S17-2039</url>
      <doi>10.18653/v1/S17-2039</doi>
      <abstract>This paper describes the HHU system that participated in Task 2 of SemEval 2017, Multilingual and Cross-lingual Semantic Word Similarity. We introduce our unsupervised embedding learning technique and describe how it was employed and configured to address the problems of monolingual and multilingual word similarity measurement. This paper reports from empirical evaluations on the benchmark provided by the task’s organizers.</abstract>
    </paper>
    <paper id="40">
      <title>Mahtab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2: Combination of Corpus-based and Knowledge-based Methods to Measure Semantic Word Similarity</title>
      <author><first>Niloofar</first> <last>Ranjbar</last></author>
      <author><first>Fatemeh</first> <last>Mashhadirajab</last></author>
      <author><first>Mehrnoush</first> <last>Shamsfard</last></author>
      <author><first>Rayeheh</first> <last>Hosseini pour</last></author>
      <author><first>Aryan</first> <last>Vahid pour</last></author>
      <pages>256–260</pages>
      <url hash="ab2e6014">S17-2040</url>
      <doi>10.18653/v1/S17-2040</doi>
      <abstract>In this paper, we describe our proposed method for measuring semantic similarity for a given pair of words at SemEval-2017 monolingual semantic word similarity task. We use a combination of knowledge-based and corpus-based techniques. We use FarsNet, the Persian Word Net, besides deep learning techniques to extract the similarity of words. We evaluated our proposed approach on Persian (Farsi) test data at SemEval-2017. It outperformed the other participants and ranked the first in the challenge.</abstract>
    </paper>
    <paper id="41">
      <title>Sew-Embed at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2: Language-Independent Concept Representations from a Semantically Enriched <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Claudio</first> <last>Delli Bovi</last></author>
      <author><first>Alessandro</first> <last>Raganato</last></author>
      <pages>261–266</pages>
      <url hash="9915904e">S17-2041</url>
      <doi>10.18653/v1/S17-2041</doi>
      <abstract>This paper describes Sew-Embed, our language-independent approach to multilingual and cross-lingual semantic word similarity as part of the SemEval-2017 Task 2. We leverage the Wikipedia-based concept representations developed by Raganato et al. (2016), and propose an embedded augmentation of their explicit high-dimensional vectors, which we obtain by plugging in an arbitrary word (or sense) embedding representation, and computing a weighted average in the continuous vector space. We evaluate Sew-Embed with two different off-the-shelf embedding representations, and report their performances across all monolingual and cross-lingual benchmarks available for the task. Despite its simplicity, especially compared with supervised or overly tuned approaches, Sew-Embed achieves competitive results in the cross-lingual setting (3rd best result in the global ranking of subtask 2, score 0.56).</abstract>
    </paper>
    <paper id="42">
      <title>Wild Devs’ at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 2: Using Neural Networks to Discover Word Similarity</title>
      <author><first>Răzvan-Gabriel</first> <last>Rotari</last></author>
      <author><first>Ionuț</first> <last>Hulub</last></author>
      <author><first>Ștefan</first> <last>Oprea</last></author>
      <author><first>Mihaela</first> <last>Plămadă-Onofrei</last></author>
      <author><first>Alina Beatrice</first> <last>Lorenţ</last></author>
      <author><first>Raluca</first> <last>Preisler</last></author>
      <author><first>Adrian</first> <last>Iftene</last></author>
      <author><first>Diana</first> <last>Trandabăț</last></author>
      <pages>267–270</pages>
      <url hash="626ba4c6">S17-2042</url>
      <doi>10.18653/v1/S17-2042</doi>
      <abstract>This paper presents Wild Devs’ participation in the SemEval-2017 Task 2 “Multi-lingual and Cross-lingual Semantic Word Similarity”, which tries to automatically measure the semantic similarity between two words. The system was build using neural networks, having as input a collection of word pairs, whereas the output consists of a list of scores, from 0 to 4, corresponding to the degree of similarity between the word pairs.</abstract>
    </paper>
    <paper id="43">
      <title><fixed-case>T</fixed-case>rento<fixed-case>T</fixed-case>eam at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: An application of <fixed-case>G</fixed-case>rice Maxims in Ranking Community Question Answers</title>
      <author><first>Mohammed R. H.</first> <last>Qwaider</last></author>
      <author><first>Abed Alhakim</first> <last>Freihat</last></author>
      <author><first>Fausto</first> <last>Giunchiglia</last></author>
      <pages>271–274</pages>
      <url hash="526f5506">S17-2043</url>
      <doi>10.18653/v1/S17-2043</doi>
      <abstract>In this paper we present the Tren-toTeam system which participated to thetask 3 at SemEval-2017 (Nakov et al.,2017).We concentrated our work onapplying Grice Maxims(used in manystate-of-the-art Machine learning applica-tions(Vogel et al., 2013; Kheirabadiand Aghagolzadeh, 2012; Dale and Re-iter, 1995; Franke, 2011)) to ranking an-swers of a question by answers relevancy.Particularly, we created a ranker systembased on relevancy scores, assigned by 3main components: Named entity recogni-tion, similarity score, sentiment analysis.Our system obtained a comparable resultsto Machine learning systems.</abstract>
    </paper>
    <paper id="44">
      <title><fixed-case>UPC</fixed-case>-<fixed-case>USMBA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Combining multiple approaches for <fixed-case>CQA</fixed-case> for <fixed-case>A</fixed-case>rabic</title>
      <author><first>Yassine</first> <last>El Adlouni</last></author>
      <author><first>Imane</first> <last>Lahbari</last></author>
      <author><first>Horacio</first> <last>Rodríguez</last></author>
      <author><first>Mohammed</first> <last>Meknassi</last></author>
      <author><first>Said Ouatik</first> <last>El Alaoui</last></author>
      <author><first>Noureddine</first> <last>Ennahnahi</last></author>
      <pages>275–279</pages>
      <url hash="495163a8">S17-2044</url>
      <doi>10.18653/v1/S17-2044</doi>
      <abstract>This paper presents a description of the participation of the UPC-USMBA team in the SemEval 2017 Task 3, subtask D, Arabic. Our approach for facing the task is based on a combination of a set of atomic classifiers. The atomic classifiers include lexical string based, based on vectorial representations and rulebased. Several combination approaches have been tried.</abstract>
    </paper>
    <paper id="45">
      <title>Beihang-<fixed-case>MSRA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: A Ranking System with Neural Matching Features for Community Question Answering</title>
      <author><first>Wenzheng</first> <last>Feng</last></author>
      <author><first>Yu</first> <last>Wu</last></author>
      <author><first>Wei</first> <last>Wu</last></author>
      <author><first>Zhoujun</first> <last>Li</last></author>
      <author><first>Ming</first> <last>Zhou</last></author>
      <pages>280–286</pages>
      <url hash="ea9b1b96">S17-2045</url>
      <doi>10.18653/v1/S17-2045</doi>
      <abstract>This paper presents the system in SemEval-2017 Task 3, Community Question Answering (CQA). We develop a ranking system that is capable of capturing semantic relations between text pairs with little word overlap. In addition to traditional NLP features, we introduce several neural network based matching features which enable our system to measure text similarity beyond lexicons. Our system significantly outperforms baseline methods and holds the second place in Subtask A and the fifth place in Subtask B, which demonstrates its efficacy on answer selection and question retrieval.</abstract>
    </paper>
    <paper id="46">
      <title><fixed-case>M</fixed-case>o<fixed-case>RS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Easy to use <fixed-case>SVM</fixed-case> in Ranking Tasks</title>
      <author><first>Miguel J.</first> <last>Rodrigues</last></author>
      <author><first>Francisco M.</first> <last>Couto</last></author>
      <pages>287–291</pages>
      <url hash="f41a1fd8">S17-2046</url>
      <doi>10.18653/v1/S17-2046</doi>
      <abstract>This paper describes our system, dubbed MoRS (Modular Ranking System), pronounced ‘Morse’, which participated in Task 3 of SemEval-2017. We used MoRS to perform the Community Question Answering Task 3, which consisted on reordering a set of comments according to their usefulness in answering the question in the thread. This was made for a large collection of questions created by a user community. As for this challenge we wanted to go back to simple, easy-to-use, and somewhat forgotten technologies that we think, in the hands of non-expert people, could be reused in their own data sets. Some of our techniques included the annotation of text, the retrieval of meta-data for each comment, POS tagging and Named Entity Recognition, among others. These gave place to syntactical analysis and semantic measurements. Finally we show and discuss our results and the context of our approach, which is part of a more comprehensive system in development, named MoQA.</abstract>
    </paper>
    <paper id="47">
      <title><fixed-case>EICA</fixed-case> Team at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Semantic and Metadata-based Features for Community Question Answering</title>
      <author><first>Yufei</first> <last>Xie</last></author>
      <author><first>Maoquan</first> <last>Wang</last></author>
      <author><first>Jing</first> <last>Ma</last></author>
      <author><first>Jian</first> <last>Jiang</last></author>
      <author><first>Zhao</first> <last>Lu</last></author>
      <pages>292–298</pages>
      <url hash="b95f0d69">S17-2047</url>
      <doi>10.18653/v1/S17-2047</doi>
      <abstract>We describe our system for participating in SemEval-2017 Task 3 on Community Question Answering. Our approach relies on combining a rich set of various types of features: semantic and metadata. The most important group turned out to be the metadata feature and the semantic vectors trained on QatarLiving data. In the main Subtask C, our primary submission was ranked fourth, with a MAP of 13.48 and accuracy of 97.08. In Subtask A, our primary submission get into the top 50%.</abstract>
    </paper>
    <paper id="48">
      <title><fixed-case>FA</fixed-case>3<fixed-case>L</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: A <fixed-case>T</fixed-case>h<fixed-case>R</fixed-case>ee Embeddings Recurrent Neural Network for Question Answering</title>
      <author><first>Giuseppe</first> <last>Attardi</last></author>
      <author><first>Antonio</first> <last>Carta</last></author>
      <author><first>Federico</first> <last>Errica</last></author>
      <author><first>Andrea</first> <last>Madotto</last></author>
      <author><first>Ludovica</first> <last>Pannitto</last></author>
      <pages>299–304</pages>
      <url hash="c4a4cd18">S17-2048</url>
      <doi>10.18653/v1/S17-2048</doi>
      <abstract>In this paper we present ThReeNN, a model for Community Question Answering, Task 3, of SemEval-2017. The proposed model exploits both syntactic and semantic information to build a single and meaningful embedding space. Using a dependency parser in combination with word embeddings, the model creates sequences of inputs for a Recurrent Neural Network, which are then used for the ranking purposes of the Task. The score obtained on the official test data shows promising results.</abstract>
    </paper>
    <paper id="49">
      <title><fixed-case>SCIR</fixed-case>-<fixed-case>QA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: <fixed-case>CNN</fixed-case> Model Based on Similar and Dissimilar Information between Keywords for Question Similarity</title>
      <author><first>Le</first> <last>Qi</last></author>
      <author><first>Yu</first> <last>Zhang</last></author>
      <author><first>Ting</first> <last>Liu</last></author>
      <pages>305–309</pages>
      <url hash="b79fd6e2">S17-2049</url>
      <doi>10.18653/v1/S17-2049</doi>
      <abstract>We describe a method of calculating the similarity of questions in community QA. Question in cQA are usually very long and there are a lot of useless information about calculating the similarity of questions. Therefore,we implement a CNN model based on similar and dissimilar information between question’s keywords. We extract the keywords of questions, and then model the similar and dissimilar information between the keywords, and use the CNN model to calculate the similarity.</abstract>
    </paper>
    <paper id="50">
      <title><fixed-case>L</fixed-case>earning<fixed-case>T</fixed-case>o<fixed-case>Q</fixed-case>uestion at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2017 Task 3: Ranking Similar Questions by Learning to Rank Using Rich Features</title>
      <author><first>Naman</first> <last>Goyal</last></author>
      <pages>310–314</pages>
      <url hash="ddbdf000">S17-2050</url>
      <doi>10.18653/v1/S17-2050</doi>
      <abstract>This paper describes our official entry LearningToQuestion for SemEval 2017 task 3 community question answer, subtask B. The objective is to rerank questions obtained in web forum as per their similarity to original question. Our system uses pairwise learning to rank methods on rich set of hand designed and representation learning features. We use various semantic features that help our system to achieve promising results on the task. The system achieved second highest results on official metrics MAP and good results on other search metrics.</abstract>
    </paper>
    <paper id="51">
      <title><fixed-case>S</fixed-case>im<fixed-case>B</fixed-case>ow at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Soft-Cosine Semantic Similarity between Questions for Community Question Answering</title>
      <author><first>Delphine</first> <last>Charlet</last></author>
      <author><first>Géraldine</first> <last>Damnati</last></author>
      <pages>315–319</pages>
      <url hash="1243c0bd">S17-2051</url>
      <doi>10.18653/v1/S17-2051</doi>
      <abstract>This paper describes the SimBow system submitted at SemEval2017-Task3, for the question-question similarity subtask B. The proposed approach is a supervised combination of different unsupervised textual similarities. These textual similarities rely on the introduction of a relation matrix in the classical cosine similarity between bag-of-words, so as to get a soft-cosine that takes into account relations between words. According to the type of relation matrix embedded in the soft-cosine, semantic or lexical relations can be considered. Our system ranked first among the official submissions of subtask B.</abstract>
    </paper>
    <paper id="52">
      <title><fixed-case>F</fixed-case>u<fixed-case>R</fixed-case>ong<fixed-case>W</fixed-case>ang at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Deep Neural Networks for Selecting Relevant Answers in Community Question Answering</title>
      <author><first>Sheng</first> <last>Zhang</last></author>
      <author><first>Jiajun</first> <last>Cheng</last></author>
      <author><first>Hui</first> <last>Wang</last></author>
      <author><first>Xin</first> <last>Zhang</last></author>
      <author><first>Pei</first> <last>Li</last></author>
      <author><first>Zhaoyun</first> <last>Ding</last></author>
      <pages>320–325</pages>
      <url hash="561993e3">S17-2052</url>
      <doi>10.18653/v1/S17-2052</doi>
      <abstract>We describes deep neural networks frameworks in this paper to address the community question answering (cQA) ranking task (SemEval-2017 task 3). Convolutional neural networks and bi-directional long-short term memory networks are applied in our methods to extract semantic information from questions and answers (comments). In addition, in order to take the full advantage of question-comment semantic relevance, we deploy interaction layer and augmented features before calculating the similarity. The results show that our methods have the great effectiveness for both subtask A and subtask C.</abstract>
    </paper>
    <paper id="53">
      <title><fixed-case>K</fixed-case>e<fixed-case>LP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Learning Pairwise Patterns in Community Question Answering</title>
      <author><first>Simone</first> <last>Filice</last></author>
      <author><first>Giovanni</first> <last>Da San Martino</last></author>
      <author><first>Alessandro</first> <last>Moschitti</last></author>
      <pages>326–333</pages>
      <url hash="2934b7d1">S17-2053</url>
      <doi>10.18653/v1/S17-2053</doi>
      <abstract>This paper describes the KeLP system participating in the SemEval-2017 community Question Answering (cQA) task. The system is a refinement of the kernel-based sentence pair modeling we proposed for the previous year challenge. It is implemented within the Kernel-based Learning Platform called KeLP, from which we inherit the team’s name. Our primary submission ranked first in subtask A, and third in subtasks B and C, being the only systems appearing in the top-3 ranking for all the English subtasks. This shows that the proposed framework, which has minor variations among the three subtasks, is extremely flexible and effective in tackling learning tasks defined on sentence pairs.</abstract>
    </paper>
    <paper id="54">
      <title><fixed-case>S</fixed-case>wiss<fixed-case>A</fixed-case>lps at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Attention-based Convolutional Neural Network for Community Question Answering</title>
      <author><first>Jan Milan</first> <last>Deriu</last></author>
      <author><first>Mark</first> <last>Cieliebak</last></author>
      <pages>334–338</pages>
      <url hash="f96125b2">S17-2054</url>
      <doi>10.18653/v1/S17-2054</doi>
      <abstract>In this paper we propose a system for reranking answers for a given question. Our method builds on a siamese CNN architecture which is extended by two attention mechanisms. The approach was evaluated on the datasets of the SemEval-2017 competition for Community Question Answering (cQA), where it achieved 7th place obtaining a MAP score of 86:24 points on the Question-Comment Similarity subtask.</abstract>
    </paper>
    <paper id="55">
      <title><fixed-case>T</fixed-case>ake<fixed-case>L</fixed-case>ab-<fixed-case>QA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Classification Experiments for Answer Retrieval in Community <fixed-case>QA</fixed-case></title>
      <author><first>Filip</first> <last>Šaina</last></author>
      <author><first>Toni</first> <last>Kukurin</last></author>
      <author><first>Lukrecija</first> <last>Puljić</last></author>
      <author><first>Mladen</first> <last>Karan</last></author>
      <author><first>Jan</first> <last>Šnajder</last></author>
      <pages>339–343</pages>
      <url hash="47030f0e">S17-2055</url>
      <doi>10.18653/v1/S17-2055</doi>
      <abstract>In this paper we present the TakeLab-QA entry to SemEval 2017 task 3, which is a question-comment re-ranking problem. We present a classification based approach, including two supervised learning models – Support Vector Machines (SVM) and Convolutional Neural Networks (CNN). We use features based on different semantic similarity models (e.g., Latent Dirichlet Allocation), as well as features based on several types of pre-trained word embeddings. Moreover, we also use some hand-crafted task-specific features. For training, our system uses no external labeled data apart from that provided by the organizers. Our primary submission achieves a MAP-score of 81.14 and F1-score of 66.99 – ranking us 10th on the SemEval 2017 task 3, subtask A.</abstract>
    </paper>
    <paper id="56">
      <title><fixed-case>GW</fixed-case>_<fixed-case>QA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Question Answer Re-ranking on <fixed-case>A</fixed-case>rabic Fora</title>
      <author><first>Nada</first> <last>Almarwani</last></author>
      <author><first>Mona</first> <last>Diab</last></author>
      <pages>344–348</pages>
      <url hash="1fada65a">S17-2056</url>
      <doi>10.18653/v1/S17-2056</doi>
      <abstract>This paper describes our submission to SemEval-2017 Task 3 Subtask D, “Question Answer Ranking in Arabic Community Question Answering”. In this work, we applied a supervised machine learning approach to automatically re-rank a set of QA pairs according to their relevance to a given question. We employ features based on latent semantic models, namely WTMF, as well as a set of lexical features based on string lengths and surface level matching. The proposed system ranked first out of 3 submissions, with a MAP score of 61.16%.</abstract>
    </paper>
    <paper id="57">
      <title><fixed-case>NLM</fixed-case>_<fixed-case>NIH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: from Question Entailment to Question Similarity for Community Question Answering</title>
      <author><first>Asma</first> <last>Ben Abacha</last></author>
      <author><first>Dina</first> <last>Demner-Fushman</last></author>
      <pages>349–352</pages>
      <url hash="389dd280">S17-2057</url>
      <doi>10.18653/v1/S17-2057</doi>
      <abstract>This paper describes our participation in SemEval-2017 Task 3 on Community Question Answering (cQA). The Question Similarity subtask (B) aims to rank a set of related questions retrieved by a search engine according to their similarity to the original question. We adapted our feature-based system for Recognizing Question Entailment (RQE) to the question similarity task. Tested on cQA-B-2016 test data, our RQE system outperformed the best system of the 2016 challenge in all measures with 77.47 MAP and 80.57 Accuracy. On cQA-B-2017 test data, performances of all systems dropped by around 30 points. Our primary system obtained 44.62 MAP, 67.27 Accuracy and 47.25 F1 score. The cQA-B-2017 best system achieved 47.22 MAP and 42.37 F1 score. Our system is ranked sixth in terms of MAP and third in terms of F1 out of 13 participating teams.</abstract>
    </paper>
    <paper id="58">
      <title>bunji at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Combination of Neural Similarity Features and Comment Plausibility Features</title>
      <author><first>Yuta</first> <last>Koreeda</last></author>
      <author><first>Takuya</first> <last>Hashito</last></author>
      <author><first>Yoshiki</first> <last>Niwa</last></author>
      <author><first>Misa</first> <last>Sato</last></author>
      <author><first>Toshihiko</first> <last>Yanase</last></author>
      <author><first>Kenzo</first> <last>Kurotsuchi</last></author>
      <author><first>Kohsuke</first> <last>Yanai</last></author>
      <pages>353–359</pages>
      <url hash="aed13d6c">S17-2058</url>
      <doi>10.18653/v1/S17-2058</doi>
      <abstract>This paper describes a text-ranking system developed by bunji team in SemEval-2017 Task 3: Community Question Answering, Subtask A and C. The goal of the task is to re-rank the comments in a question-and-answer forum such that useful comments for answering the question are ranked high. We proposed a method that combines neural similarity features and hand-crafted comment plausibility features, and we modeled inter-comments relationship using conditional random field. Our approach obtained the fifth place in the Subtask A and the second place in the Subtask C.</abstract>
    </paper>
    <paper id="59">
      <title><fixed-case>QU</fixed-case>-<fixed-case>BIGIR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2017 Task 3: Using Similarity Features for <fixed-case>A</fixed-case>rabic Community Question Answering Forums</title>
      <author><first>Marwan</first> <last>Torki</last></author>
      <author><first>Maram</first> <last>Hasanain</last></author>
      <author><first>Tamer</first> <last>Elsayed</last></author>
      <pages>360–364</pages>
      <url hash="b1735c28">S17-2059</url>
      <doi>10.18653/v1/S17-2059</doi>
      <abstract>In this paper we describe our QU-BIGIR system for the Arabic subtask D of the SemEval 2017 Task 3. Our approach builds on our participation in the past version of the same subtask. This year, our system uses different similarity measures that encodes lexical and semantic pairwise similarity of text pairs. In addition to well known similarity measures such as cosine similarity, we use other measures based on the summary statistics of word embedding representation for a given text. To rank a list of candidate question answer pairs for a given question, we learn a linear SVM classifier over our similarity features. Our best resulting run came second in subtask D with a very competitive performance to the first-ranking system.</abstract>
    </paper>
    <paper id="60">
      <title><fixed-case>ECNU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Using Traditional and Deep Learning Methods to Address Community Question Answering Task</title>
      <author><first>Guoshun</first> <last>Wu</last></author>
      <author><first>Yixuan</first> <last>Sheng</last></author>
      <author><first>Man</first> <last>Lan</last></author>
      <author><first>Yuanbin</first> <last>Wu</last></author>
      <pages>365–369</pages>
      <url hash="3d02fa97">S17-2060</url>
      <doi>10.18653/v1/S17-2060</doi>
      <abstract>This paper describes the systems we submitted to the task 3 (Community Question Answering) in SemEval 2017 which contains three subtasks on English corpora, i.e., subtask A: Question-Comment Similarity, subtask B: Question-Question Similarity, and subtask C: Question-External Comment Similarity. For subtask A, we combined two different methods to represent question-comment pair, i.e., supervised model using traditional features and Convolutional Neural Network. For subtask B, we utilized the information of snippets returned from Search Engine with question subject as query. For subtask C, we ranked the comments by multiplying the probability of the pair related question comment being Good by the reciprocal rank of the related question.</abstract>
    </paper>
    <paper id="61">
      <title><fixed-case>UINSUSKA</fixed-case>-<fixed-case>T</fixed-case>i<fixed-case>T</fixed-case>ech at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Exploiting Word Importance Levels for Similarity Features for <fixed-case>CQA</fixed-case></title>
      <author><first>Surya</first> <last>Agustian</last></author>
      <author><first>Hiroya</first> <last>Takamura</last></author>
      <pages>370–374</pages>
      <url hash="2c241890">S17-2061</url>
      <doi>10.18653/v1/S17-2061</doi>
      <abstract>The majority of core techniques to solve many problems in Community Question Answering (CQA) task rely on similarity computation. This work focuses on similarity between two sentences (or questions in subtask B) based on word embeddings. We exploit words importance levels in sentences or questions for similarity features, for classification and ranking with machine learning. Using only 2 types of similarity metric, our proposed method has shown comparable results with other complex systems. This method on subtask B 2017 dataset is ranked on position 7 out of 13 participants. Evaluation on 2016 dataset is on position 8 of 12, outperforms some complex systems. Further, this finding is explorable and potential to be used as baseline and extensible for many tasks in CQA and other textual similarity based system.</abstract>
    </paper>
    <paper id="62">
      <title>Talla at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 3: Identifying Similar Questions Through Paraphrase Detection</title>
      <author><first>Byron</first> <last>Galbraith</last></author>
      <author><first>Bhanu</first> <last>Pratap</last></author>
      <author><first>Daniel</first> <last>Shank</last></author>
      <pages>375–379</pages>
      <url hash="03f0a89e">S17-2062</url>
      <doi>10.18653/v1/S17-2062</doi>
      <abstract>This paper describes our approach to the SemEval-2017 shared task of determining question-question similarity in a community question-answering setting (Task 3B). We extracted both syntactic and semantic similarity features between candidate questions, performed pairwise-preference learning to optimize for ranking order, and then trained a random forest classifier to predict whether the candidate questions are paraphrases of each other. This approach achieved a MAP of 45.7% out of max achievable 67.0% on the test set.</abstract>
    </paper>
    <paper id="63">
      <title><fixed-case>QUB</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 6: Cascaded Imbalanced Classification for Humor Analysis in <fixed-case>T</fixed-case>witter</title>
      <author><first>Xiwu</first> <last>Han</last></author>
      <author><first>Gregory</first> <last>Toner</last></author>
      <pages>380–384</pages>
      <url hash="ee933d15">S17-2063</url>
      <doi>10.18653/v1/S17-2063</doi>
      <abstract>This paper presents our submission to SemEval-2017 Task 6: #HashtagWars: Learning a Sense of Humor. There are two subtasks: A. Pairwise Comparison, and B. Semi-Ranking. Our assumption is that the distribution of humorous and non-humorous texts in real life language is naturally imbalanced. Using Naïve Bayes Multinomial with standard text-representation features, we approached Subtask B as a sequence of imbalanced classification problems, and optimized our system per the macro-average recall. Subtask A was then solved via the Semi-Ranking results. On the final test, our system was ranked 10th for Subtask A, and 3rd for Subtask B.</abstract>
    </paper>
    <paper id="64">
      <title><fixed-case>D</fixed-case>uluth at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 6: Language Models in Humor Detection</title>
      <author><first>Xinru</first> <last>Yan</last></author>
      <author><first>Ted</first> <last>Pedersen</last></author>
      <pages>385–389</pages>
      <url hash="f4c0bd40">S17-2064</url>
      <doi>10.18653/v1/S17-2064</doi>
      <abstract>This paper describes the Duluth system that participated in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks A and B using N-gram language models, ranking highly in the task evaluation. This paper discusses the results of our system in the development and evaluation stages and from two post-evaluation runs.</abstract>
    </paper>
    <paper id="65">
      <title><fixed-case>D</fixed-case>ata<fixed-case>S</fixed-case>tories at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 6: <fixed-case>S</fixed-case>iamese <fixed-case>LSTM</fixed-case> with Attention for Humorous Text Comparison</title>
      <author><first>Christos</first> <last>Baziotis</last></author>
      <author><first>Nikos</first> <last>Pelekis</last></author>
      <author><first>Christos</first> <last>Doulkeridis</last></author>
      <pages>390–395</pages>
      <url hash="17fa6ad7">S17-2065</url>
      <doi>10.18653/v1/S17-2065</doi>
      <abstract>In this paper we present a deep-learning system that competed at SemEval-2017 Task 6 "#HashtagWars: Learning a Sense of Humor”. We participated in Subtask A, in which the goal was, given two Twitter messages, to identify which one is funnier. We propose a Siamese architecture with bidirectional Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism. Our system works on the token-level, leveraging word embeddings trained on a big collection of unlabeled Twitter messages. We ranked 2nd in 7 teams. A post-completion improvement of our model, achieves state-of-the-art results on #HashtagWars dataset.</abstract>
    </paper>
    <paper id="66">
      <title><fixed-case>T</fixed-case>ake<fixed-case>L</fixed-case>ab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 6: #<fixed-case>R</fixed-case>anking<fixed-case>H</fixed-case>umor<fixed-case>I</fixed-case>n4<fixed-case>P</fixed-case>ages</title>
      <author><first>Marin</first> <last>Kukovačec</last></author>
      <author><first>Juraj</first> <last>Malenica</last></author>
      <author><first>Ivan</first> <last>Mršić</last></author>
      <author><first>Antonio</first> <last>Šajatović</last></author>
      <author><first>Domagoj</first> <last>Alagić</last></author>
      <author><first>Jan</first> <last>Šnajder</last></author>
      <pages>396–400</pages>
      <url hash="2f649ca3">S17-2066</url>
      <doi>10.18653/v1/S17-2066</doi>
      <abstract>This paper describes our system for humor ranking in tweets within the SemEval 2017 Task 6: #HashtagWars (6A and 6B). For both subtasks, we use an off-the-shelf gradient boosting model built on a rich set of features, handcrafted to provide the model with the external knowledge needed to better predict the humor in the text. The features capture various cultural references and specific humor patterns. Our system ranked 2nd (officially 7th) among 10 submissions on the Subtask A and 2nd among 9 submissions on the Subtask B.</abstract>
    </paper>
    <paper id="67">
      <title><fixed-case>SRHR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 6: Word Associations for Humour Recognition</title>
      <author><first>Andrew</first> <last>Cattle</last></author>
      <author><first>Xiaojuan</first> <last>Ma</last></author>
      <pages>401–406</pages>
      <url hash="e1f9893a">S17-2067</url>
      <doi>10.18653/v1/S17-2067</doi>
      <abstract>This paper explores the role of semantic relatedness features, such as word associations, in humour recognition. Specifically, we examine the task of inferring pairwise humour judgments in Twitter hashtag wars. We examine a variety of word association features derived from University of Southern Florida Free Association Norms (USF) and the Edinburgh Associative Thesaurus (EAT) and find that word association-based features outperform Word2Vec similarity, a popular semantic relatedness measure. Our system achieves an accuracy of 56.42% using a combination of unigram perplexity, bigram perplexity, EAT difference (tweet-avg), USF forward (max), EAT difference (word-avg), USF difference (word-avg), EAT forward (min), USF difference (tweet-max), and EAT backward (min).</abstract>
    </paper>
    <paper id="68">
      <title>#<fixed-case>W</fixed-case>ar<fixed-case>T</fixed-case>eam at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 6: Using Neural Networks for Discovering Humorous Tweets</title>
      <author><first>Iuliana Alexandra</first> <last>Fleșcan-Lovin-Arseni</last></author>
      <author><first>Ramona Andreea</first> <last>Turcu</last></author>
      <author><first>Cristina</first> <last>Sîrbu</last></author>
      <author><first>Larisa</first> <last>Alexa</last></author>
      <author><first>Sandra Maria</first> <last>Amarandei</last></author>
      <author><first>Nichita</first> <last>Herciu</last></author>
      <author><first>Constantin</first> <last>Scutaru</last></author>
      <author><first>Diana</first> <last>Trandabăț</last></author>
      <author><first>Adrian</first> <last>Iftene</last></author>
      <pages>407–410</pages>
      <url hash="f7e9e878">S17-2068</url>
      <doi>10.18653/v1/S17-2068</doi>
      <abstract>This paper presents the participation of #WarTeam in Task 6 of SemEval2017 with a system classifying humor by comparing and ranking tweets. The training data consists of annotated tweets from the @midnight TV show. #WarTeam’s system uses a neural network (TensorFlow) having inputs from a Naïve Bayes humor classifier and a sentiment analyzer.</abstract>
    </paper>
    <paper id="69">
      <title><fixed-case>SVNIT</fixed-case> @ <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2017 Task-6: Learning a Sense of Humor Using Supervised Approach</title>
      <author><first>Rutal</first> <last>Mahajan</last></author>
      <author><first>Mukesh</first> <last>Zaveri</last></author>
      <pages>411–415</pages>
      <url hash="c2fad171">S17-2069</url>
      <doi>10.18653/v1/S17-2069</doi>
      <abstract>This paper describes the system devel-oped for SemEval 2017 task 6: #HashTagWars -Learning a Sense of Hu-mor. Learning to recognize sense of hu-mor is the important task for language understanding applications. Different set of features based on frequency of words, structure of tweets and semantics are used in this system to identify the presence of humor in tweets. Supervised machine learning approaches, Multilayer percep-tron and Naïve Bayes are used to classify the tweets in to three level of sense of humor. For given Hashtag, the system finds the funniest tweet and predicts the amount of funniness of all the other tweets. In official submitted runs, we have achieved 0.506 accuracy using mul-tilayer perceptron in subtask-A and 0.938 distance in subtask-B. Using Naïve bayes in subtask-B, the system achieved 0.949 distance. Apart from official runs, this system have scored 0.751 accuracy in subtask-A using SVM. But still there is a wide room for improvement in system.</abstract>
    </paper>
    <paper id="70">
      <title><fixed-case>D</fixed-case>uluth at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 7 : Puns Upon a Midnight Dreary, Lexical Semantics for the Weak and Weary</title>
      <author><first>Ted</first> <last>Pedersen</last></author>
      <pages>416–420</pages>
      <url hash="a045db91">S17-2070</url>
      <doi>10.18653/v1/S17-2070</doi>
      <abstract>This paper describes the Duluth systems that participated in SemEval-2017 Task 7 : Detection and Interpretation of English Puns. The Duluth systems participated in all three subtasks, and relied on methods that included word sense disambiguation and measures of semantic relatedness.</abstract>
    </paper>
    <paper id="71">
      <title><fixed-case>UW</fixed-case>aterloo at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 7: Locating the Pun Using Syntactic Characteristics and Corpus-based Metrics</title>
      <author><first>Olga</first> <last>Vechtomova</last></author>
      <pages>421–425</pages>
      <url hash="cdebc4ac">S17-2071</url>
      <doi>10.18653/v1/S17-2071</doi>
      <abstract>The paper presents a system for locating a pun word. The developed method calculates a score for each word in a pun, using a number of components, including its Inverse Document Frequency (IDF), Normalized Pointwise Mutual Information (NPMI) with other words in the pun text, its position in the text, part-of-speech and some syntactic features. The method achieved the best performance in the Heterographic category and the second best in the Homographic. Further analysis showed that IDF is the most useful characteristic, whereas the count of words with which the given word has high NPMI has a negative effect on performance.</abstract>
    </paper>
    <paper id="72">
      <title><fixed-case>P</fixed-case>un<fixed-case>F</fixed-case>ields at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 7: Employing <fixed-case>R</fixed-case>oget’s Thesaurus in Automatic Pun Recognition and Interpretation</title>
      <author><first>Elena</first> <last>Mikhalkova</last></author>
      <author><first>Yuri</first> <last>Karyakin</last></author>
      <pages>426–431</pages>
      <url hash="1fbedc04">S17-2072</url>
      <doi>10.18653/v1/S17-2072</doi>
      <abstract>The article describes a model of automatic interpretation of English puns, based on Roget’s Thesaurus, and its implementation, PunFields. In a pun, the algorithm discovers two groups of words that belong to two main semantic fields. The fields become a semantic vector based on which an SVM classifier learns to recognize puns. A rule-based model is then applied for recognition of intentionally ambiguous (target) words and their definitions. In SemEval Task 7 PunFields shows a considerably good result in pun classification, but requires improvement in searching for the target word and its definition.</abstract>
    </paper>
    <paper id="73">
      <title><fixed-case>JU</fixed-case> <fixed-case>CSE</fixed-case> <fixed-case>NLP</fixed-case> @ <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2017 Task 7: Employing Rules to Detect and Interpret <fixed-case>E</fixed-case>nglish Puns</title>
      <author><first>Aniket</first> <last>Pramanick</last></author>
      <author><first>Dipankar</first> <last>Das</last></author>
      <pages>432–435</pages>
      <url hash="1832731c">S17-2073</url>
      <doi>10.18653/v1/S17-2073</doi>
      <abstract>System description. Implementation of HMM and Cyclic Dependency Network.</abstract>
    </paper>
    <paper id="74">
      <title>N-Hance at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 7: A Computational Approach using Word Association for Puns</title>
      <author><first>Özge</first> <last>Sevgili</last></author>
      <author><first>Nima</first> <last>Ghotbi</last></author>
      <author><first>Selma</first> <last>Tekir</last></author>
      <pages>436–439</pages>
      <url hash="8c074ef1">S17-2074</url>
      <doi>10.18653/v1/S17-2074</doi>
      <abstract>This paper presents a system developed for SemEval-2017 Task 7, Detection and Interpretation of English Puns consisting of three subtasks; pun detection, pun location, and pun interpretation, respectively. The system stands on recognizing a distinctive word which has a high association with the pun in the given sentence. The intended humorous meaning of pun is identified through the use of this word. Our official results confirm the potential of this approach.</abstract>
    </paper>
    <paper id="75">
      <title><fixed-case>EL</fixed-case>i<fixed-case>RF</fixed-case>-<fixed-case>UPV</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 7: Pun Detection and Interpretation</title>
      <author><first>Lluís-F.</first> <last>Hurtado</last></author>
      <author><first>Encarna</first> <last>Segarra</last></author>
      <author><first>Ferran</first> <last>Pla</last></author>
      <author><first>Pascual</first> <last>Carrasco</last></author>
      <author><first>José-Ángel</first> <last>González</last></author>
      <pages>440–443</pages>
      <url hash="4ccef11f">S17-2075</url>
      <doi>10.18653/v1/S17-2075</doi>
      <abstract>This paper describes the participation of ELiRF-UPV team at task 7 (subtask 2: homographic pun detection and subtask 3: homographic pun interpretation) of SemEval2017. Our approach is based on the use of word embeddings to find related words in a sentence and a version of the Lesk algorithm to establish relationships between synsets. The results obtained are in line with those obtained by the other participants and they encourage us to continue working on this problem.</abstract>
    </paper>
    <paper id="76">
      <title><fixed-case>B</fixed-case>uzz<fixed-case>S</fixed-case>aw at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 7: Global vs. Local Context for Interpreting and Locating Homographic <fixed-case>E</fixed-case>nglish Puns with Sense Embeddings</title>
      <author><first>Dieke</first> <last>Oele</last></author>
      <author><first>Kilian</first> <last>Evang</last></author>
      <pages>444–448</pages>
      <url hash="6b71a2f6">S17-2076</url>
      <doi>10.18653/v1/S17-2076</doi>
      <abstract>This paper describes our system participating in the SemEval-2017 Task 7, for the subtasks of homographic pun location and homographic pun interpretation. For pun interpretation, we use a knowledge-based Word Sense Disambiguation (WSD) method based on sense embeddings. Pun-based jokes can be divided into two parts, each containing information about the two distinct senses of the pun. To exploit this structure we split the context that is input to the WSD system into two local contexts and find the best sense for each of them. We use the output of pun interpretation for pun location. As we expect the two meanings of a pun to be very dissimilar, we compute sense embedding cosine distances for each sense-pair and select the word that has the highest distance. We describe experiments on different methods of splitting the context and compare our method to several baselines. We find evidence supporting our hypotheses and obtain competitive results for pun interpretation.</abstract>
    </paper>
    <paper id="77">
      <title><fixed-case>UWAV</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 7: Automated feature-based system for locating puns</title>
      <author><first>Ankit</first> <last>Vadehra</last></author>
      <pages>449–452</pages>
      <url hash="735d1ea6">S17-2077</url>
      <doi>10.18653/v1/S17-2077</doi>
      <abstract>In this paper we describe our system created for SemEval-2017 Task 7: Detection and Interpretation of English Puns. We tackle subtask 1, pun detection, by leveraging features selected from sentences to design a classifier that can disambiguate between the presence or absence of a pun. We address subtask 2, pun location, by utilizing a decision flow structure that uses presence or absence of certain features to decide the next action. The results obtained by our system are encouraging, considering the simplicity of the system. We consider this system as a precursor for deeper exploration on efficient feature selection for pun detection.</abstract>
    </paper>
    <paper id="78">
      <title><fixed-case>ECNU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 7: Using Supervised and Unsupervised Methods to Detect and Locate <fixed-case>E</fixed-case>nglish Puns</title>
      <author><first>Yuhuan</first> <last>Xiu</last></author>
      <author><first>Man</first> <last>Lan</last></author>
      <author><first>Yuanbin</first> <last>Wu</last></author>
      <pages>453–456</pages>
      <url hash="96a48049">S17-2078</url>
      <doi>10.18653/v1/S17-2078</doi>
      <abstract>This paper describes our submissions to task 7 in SemEval 2017, i.e., Detection and Interpretation of English Puns. We participated in the first two subtasks, which are to detect and locate English puns respectively. For subtask 1, we presented a supervised system to determine whether or not a sentence contains a pun using similarity features calculated on sense vectors or cluster center vectors. For subtask 2, we established an unsupervised system to locate the pun by scoring each word in the sentence and we assumed that the word with the smallest score is the pun.</abstract>
    </paper>
    <paper id="79">
      <title>Fermi at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 7: Detection and Interpretation of Homographic puns in <fixed-case>E</fixed-case>nglish Language</title>
      <author><first>Vijayasaradhi</first> <last>Indurthi</last></author>
      <author><first>Subba Reddy</first> <last>Oota</last></author>
      <pages>457–460</pages>
      <url hash="1c103890">S17-2079</url>
      <doi>10.18653/v1/S17-2079</doi>
      <abstract>This paper describes our system for detection and interpretation of English puns. We participated in 2 subtasks related to homographic puns achieve comparable results for these tasks. Through the paper we provide detailed description of the approach, as well as the results obtained in the task. Our models achieved a F1-score of 77.65% for Subtask 1 and 52.15% for Subtask 2.</abstract>
    </paper>
    <paper id="80">
      <title><fixed-case>UW</fixed-case>aterloo at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 8: Detecting Stance towards Rumours with Topic Independent Features</title>
      <author><first>Hareesh</first> <last>Bahuleyan</last></author>
      <author><first>Olga</first> <last>Vechtomova</last></author>
      <pages>461–464</pages>
      <url hash="445ce7ab">S17-2080</url>
      <doi>10.18653/v1/S17-2080</doi>
      <abstract>This paper describes our system for subtask-A: SDQC for RumourEval, task-8 of SemEval 2017. Identifying rumours, especially for breaking news events as they unfold, is a challenging task due to the absence of sufficient information about the exact rumour stories circulating on social media. Determining the stance of Twitter users towards rumourous messages could provide an indirect way of identifying potential rumours. The proposed approach makes use of topic independent features from two categories, namely cue features and message specific features to fit a gradient boosting classifier. With an accuracy of 0.78, our system achieved the second best performance on subtask-A of RumourEval.</abstract>
    </paper>
    <paper id="81">
      <title><fixed-case>IKM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 8: Convolutional Neural Networks for stance detection and rumor verification</title>
      <author><first>Yi-Chin</first> <last>Chen</last></author>
      <author><first>Zhao-Yang</first> <last>Liu</last></author>
      <author><first>Hung-Yu</first> <last>Kao</last></author>
      <pages>465–469</pages>
      <url hash="5d8f7c39">S17-2081</url>
      <doi>10.18653/v1/S17-2081</doi>
      <abstract>This paper describes our approach for SemEval-2017 Task 8. We aim at detecting the stance of tweets and determining the veracity of the given rumor. We utilize a convolutional neural network for short text categorization using multiple filter sizes. Our approach beats the baseline classifiers on different event data with good F1 scores. The best of our submitted runs achieves rank 1st among all scores on subtask B.</abstract>
    </paper>
    <paper id="82">
      <title><fixed-case>N</fixed-case>ile<fixed-case>TMRG</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 8: Determining Rumour and Veracity Support for Rumours on <fixed-case>T</fixed-case>witter.</title>
      <author><first>Omar</first> <last>Enayet</last></author>
      <author><first>Samhaa R.</first> <last>El-Beltagy</last></author>
      <pages>470–474</pages>
      <url hash="dade7f3a">S17-2082</url>
      <doi>10.18653/v1/S17-2082</doi>
      <abstract>Final submission for NileTMRG on RumourEval 2017.</abstract>
    </paper>
    <paper id="83">
      <title><fixed-case>T</fixed-case>uring at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-<fixed-case>LSTM</fixed-case></title>
      <author><first>Elena</first> <last>Kochkina</last></author>
      <author><first>Maria</first> <last>Liakata</last></author>
      <author><first>Isabelle</first> <last>Augenstein</last></author>
      <pages>475–480</pages>
      <url hash="b9e7b2cb">S17-2083</url>
      <doi>10.18653/v1/S17-2083</doi>
      <abstract>This paper describes team Turing’s submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A). Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Subtask A.</abstract>
    </paper>
    <paper id="84">
      <title>Mama Edha at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 8: Stance Classification with <fixed-case>CNN</fixed-case> and Rules</title>
      <author><first>Marianela</first> <last>García Lozano</last></author>
      <author><first>Hanna</first> <last>Lilja</last></author>
      <author><first>Edward</first> <last>Tjörnhammar</last></author>
      <author><first>Maja</first> <last>Karasalo</last></author>
      <pages>481–485</pages>
      <url hash="3033e64d">S17-2084</url>
      <doi>10.18653/v1/S17-2084</doi>
      <abstract>For the competition SemEval-2017 we investigated the possibility of performing stance classification (support, deny, query or comment) for messages in Twitter conversation threads related to rumours. Stance classification is interesting since it can provide a basis for rumour veracity assessment. Our ensemble classification approach of combining convolutional neural networks with both automatic rule mining and manually written rules achieved a final accuracy of 74.9% on the competition’s test data set for Task 8A. To improve classification we also experimented with data relabeling and using the grammatical structure of the tweet contents for classification.</abstract>
    </paper>
    <paper id="85">
      <title><fixed-case>DFKI</fixed-case>-<fixed-case>DKT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 8: Rumour Detection and Classification using Cascading Heuristics</title>
      <author><first>Ankit</first> <last>Srivastava</last></author>
      <author><first>Georg</first> <last>Rehm</last></author>
      <author><first>Julian</first> <last>Moreno Schneider</last></author>
      <pages>486–490</pages>
      <url hash="1cfe9c40">S17-2085</url>
      <doi>10.18653/v1/S17-2085</doi>
      <abstract>We describe our submissions for SemEval-2017 Task 8, Determining Rumour Veracity and Support for Rumours. The Digital Curation Technologies (DKT) team at the German Research Center for Artificial Intelligence (DFKI) participated in two subtasks: Subtask A (determining the stance of a message) and Subtask B (determining veracity of a message, closed variant). In both cases, our implementation consisted of a Multivariate Logistic Regression (Maximum Entropy) classifier coupled with hand-written patterns and rules (heuristics) applied in a post-process cascading fashion. We provide a detailed analysis of the system performance and report on variants of our systems that were not part of the official submission.</abstract>
    </paper>
    <paper id="86">
      <title><fixed-case>ECNU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 8: Rumour Evaluation Using Effective Features and Supervised Ensemble Models</title>
      <author><first>Feixiang</first> <last>Wang</last></author>
      <author><first>Man</first> <last>Lan</last></author>
      <author><first>Yuanbin</first> <last>Wu</last></author>
      <pages>491–496</pages>
      <url hash="8d3d67ed">S17-2086</url>
      <doi>10.18653/v1/S17-2086</doi>
      <abstract>This paper describes our submissions to task 8 in SemEval 2017, i.e., Determining rumour veracity and support for rumours. Given a rumoured tweet and a lot of reply tweets, the subtask A is to label whether these tweets are support, deny, query or comment, and the subtask B aims to predict the veracity (i.e., true, false, and unverified) with a confidence (in range of 0-1) of the given rumoured tweet. For both subtasks, we adopted supervised machine learning methods, incorporating rich features. Since training data is imbalanced, we specifically designed a two-step classifier to address subtask A .</abstract>
    </paper>
    <paper id="87">
      <title><fixed-case>IITP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 8 : A Supervised Approach for Rumour Evaluation</title>
      <author><first>Vikram</first> <last>Singh</last></author>
      <author><first>Sunny</first> <last>Narayan</last></author>
      <author><first>Md Shad</first> <last>Akhtar</last></author>
      <author><first>Asif</first> <last>Ekbal</last></author>
      <author><first>Pushpak</first> <last>Bhattacharyya</last></author>
      <pages>497–501</pages>
      <url hash="5948af08">S17-2087</url>
      <doi>10.18653/v1/S17-2087</doi>
      <abstract>This paper describes our system participation in the SemEval-2017 Task 8 ‘RumourEval: Determining rumour veracity and support for rumours’. The objective of this task was to predict the stance and veracity of the underlying rumour. We propose a supervised classification approach employing several lexical, content and twitter specific features for learning. Evaluation shows promising results for both the problems.</abstract>
    </paper>
    <paper id="88">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Sentiment Analysis in <fixed-case>T</fixed-case>witter</title>
      <author><first>Sara</first> <last>Rosenthal</last></author>
      <author><first>Noura</first> <last>Farra</last></author>
      <author><first>Preslav</first> <last>Nakov</last></author>
      <pages>502–518</pages>
      <url hash="8bb3710d">S17-2088</url>
      <doi>10.18653/v1/S17-2088</doi>
      <abstract>This paper describes the fifth year of the Sentiment Analysis in Twitter task. SemEval-2017 Task 4 continues with a rerun of the subtasks of SemEval-2016 Task 4, which include identifying the overall sentiment of the tweet, sentiment towards a topic with classification on a two-point and on a five-point ordinal scale, and quantification of the distribution of sentiment towards a topic across a number of tweets: again on a two-point and on a five-point ordinal scale. Compared to 2016, we made two changes: (i) we introduced a new language, Arabic, for all subtasks, and (ii) we made available information from the profiles of the Twitter users who posted the target tweets. The task continues to be very popular, with a total of 48 teams participating this year.</abstract>
    </paper>
    <paper id="89">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News</title>
      <author><first>Keith</first> <last>Cortis</last></author>
      <author><first>André</first> <last>Freitas</last></author>
      <author><first>Tobias</first> <last>Daudert</last></author>
      <author><first>Manuela</first> <last>Huerlimann</last></author>
      <author><first>Manel</first> <last>Zarrouk</last></author>
      <author><first>Siegfried</first> <last>Handschuh</last></author>
      <author><first>Brian</first> <last>Davis</last></author>
      <pages>519–535</pages>
      <url hash="7f510d82">S17-2089</url>
      <doi>10.18653/v1/S17-2089</doi>
      <abstract>This paper discusses the “Fine-Grained Sentiment Analysis on Financial Microblogs and News” task as part of SemEval-2017, specifically under the “Detecting sentiment, humour, and truth” theme. This task contains two tracks, where the first one concerns Microblog messages and the second one covers News Statements and Headlines. The main goal behind both tracks was to predict the sentiment score for each of the mentioned companies/stocks. The sentiment scores for each text instance adopted floating point values in the range of -1 (very negative/bearish) to 1 (very positive/bullish), with 0 designating neutral sentiment. This task attracted a total of 32 participants, with 25 participating in Track 1 and 29 in Track 2.</abstract>
    </paper>
    <paper id="90">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 9: <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation Parsing and Generation</title>
      <author><first>Jonathan</first> <last>May</last></author>
      <author><first>Jay</first> <last>Priyadarshi</last></author>
      <pages>536–545</pages>
      <url hash="963ef0fc">S17-2090</url>
      <doi>10.18653/v1/S17-2090</doi>
      <abstract>In this report we summarize the results of the 2017 AMR SemEval shared task. The task consisted of two separate yet related subtasks. In the parsing subtask, participants were asked to produce Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the biomedical domain. In the generation subtask, participants were asked to generate English sentences given AMR graphs in the news/forum domain. A total of five sites participated in the parsing subtask, and four participated in the generation subtask. Along with a description of the task and the participants’ systems, we show various score ablations and some sample outputs.</abstract>
    </paper>
    <paper id="91">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2017 Task 10: <fixed-case>S</fixed-case>cience<fixed-case>IE</fixed-case> - Extracting Keyphrases and Relations from Scientific Publications</title>
      <author><first>Isabelle</first> <last>Augenstein</last></author>
      <author><first>Mrinal</first> <last>Das</last></author>
      <author><first>Sebastian</first> <last>Riedel</last></author>
      <author><first>Lakshmi</first> <last>Vikraman</last></author>
      <author><first>Andrew</first> <last>McCallum</last></author>
      <pages>546–555</pages>
      <url hash="cc657c1d">S17-2091</url>
      <doi>10.18653/v1/S17-2091</doi>
      <abstract>We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.</abstract>
    </paper>
    <paper id="92">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 11: End-User Development using Natural Language</title>
      <author><first>Juliano</first> <last>Sales</last></author>
      <author><first>Siegfried</first> <last>Handschuh</last></author>
      <author><first>André</first> <last>Freitas</last></author>
      <pages>556–564</pages>
      <url hash="9faed52d">S17-2092</url>
      <doi>10.18653/v1/S17-2092</doi>
      <abstract>This task proposes a challenge to support the interaction between users and applications, micro-services and software APIs using natural language. The task aims for supporting the evaluation and evolution of the discussions surrounding the natural language processing approaches within the context of end-user natural language programming, under scenarios of high semantic heterogeneity/gap.</abstract>
    </paper>
    <paper id="93">
      <title><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 12: Clinical <fixed-case>T</fixed-case>emp<fixed-case>E</fixed-case>val</title>
      <author><first>Steven</first> <last>Bethard</last></author>
      <author><first>Guergana</first> <last>Savova</last></author>
      <author><first>Martha</first> <last>Palmer</last></author>
      <author><first>James</first> <last>Pustejovsky</last></author>
      <pages>565–572</pages>
      <url hash="be47f47c">S17-2093</url>
      <doi>10.18653/v1/S17-2093</doi>
      <abstract>Clinical TempEval 2017 aimed to answer the question: how well do systems trained on annotated timelines for one medical condition (colon cancer) perform in predicting timelines on another medical condition (brain cancer)? Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were evaluated on clinical and pathology notes from Mayo Clinic cancer patients, annotated with an extension of TimeML for the clinical domain. 11 teams participated in the tasks, with the best systems achieving F1 scores above 0.55 for time expressions, above 0.70 for event expressions, and above 0.40 for temporal relations. Most tasks observed about a 20 point drop over Clinical TempEval 2016, where systems were trained and evaluated on the same domain (colon cancer).</abstract>
    </paper>
    <paper id="94">
      <title><fixed-case>BB</fixed-case>_twtr at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: <fixed-case>T</fixed-case>witter Sentiment Analysis with <fixed-case>CNN</fixed-case>s and <fixed-case>LSTM</fixed-case>s</title>
      <author><first>Mathieu</first> <last>Cliche</last></author>
      <pages>573–580</pages>
      <url hash="a77b442b">S17-2094</url>
      <doi>10.18653/v1/S17-2094</doi>
      <abstract>In this paper we describe our attempt at producing a state-of-the-art Twitter sentiment classifier using Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTMs) networks. Our system leverages a large amount of unlabeled data to pre-train word embeddings. We then use a subset of the unlabeled data to fine tune the embeddings using distant supervision. The final CNNs and LSTMs are trained on the SemEval-2017 Twitter dataset where the embeddings are fined tuned again. To boost performances we ensemble several CNNs and LSTMs together. Our approach achieved first rank on all of the five English subtasks amongst 40 teams.</abstract>
    </paper>
    <paper id="95">
      <title><fixed-case>L</fixed-case>ancaster A at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines</title>
      <author><first>Andrew</first> <last>Moore</last></author>
      <author><first>Paul</first> <last>Rayson</last></author>
      <pages>581–585</pages>
      <url hash="dc8632a3">S17-2095</url>
      <doi>10.18653/v1/S17-2095</doi>
      <abstract>This paper describes our participation in Task 5 track 2 of SemEval 2017 to predict the sentiment of financial news headlines for a specific company on a continuous scale between -1 and 1. We tackled the problem using a number of approaches, utilising a Support Vector Regression (SVR) and a Bidirectional Long Short-Term Memory (BLSTM). We found an improvement of 4-6% using the LSTM model over the SVR and came fourth in the track. We report a number of different evaluations using a finance specific word embedding model and reflect on the effects of using different evaluation metrics.</abstract>
      <attachment type="poster" hash="7120a76d">S17-2095.Poster.pdf</attachment>
      <attachment type="presentation" hash="bd5a6b49">S17-2095.Presentation.pdf</attachment>
    </paper>
    <paper id="96">
      <title><fixed-case>S</fixed-case>heffield at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 9: Transition-based language generation from <fixed-case>AMR</fixed-case>.</title>
      <author><first>Gerasimos</first> <last>Lampouras</last></author>
      <author><first>Andreas</first> <last>Vlachos</last></author>
      <pages>586–591</pages>
      <url hash="7061435e">S17-2096</url>
      <doi>10.18653/v1/S17-2096</doi>
      <abstract>This paper describes the submission by the University of Sheffield to the SemEval 2017 Abstract Meaning Representation Parsing and Generation task (SemEval 2017 Task 9, Subtask 2). We cast language generation from AMR as a sequence of actions (e.g., insert/remove/rename edges and nodes) that progressively transform the AMR graph into a dependency parse tree. This transition-based approach relies on the fact that an AMR graph can be considered structurally similar to a dependency tree, with a focus on content rather than function words. An added benefit to this approach is the greater amount of data we can take advantage of to train the parse-to-text linearizer. Our submitted run on the test data achieved a BLEU score of 3.32 and a Trueskill score of -22.04 on automatic and human evaluation respectively.</abstract>
    </paper>
    <paper id="97">
      <title>The <fixed-case>AI</fixed-case>2 system at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10 (<fixed-case>S</fixed-case>cience<fixed-case>IE</fixed-case>): semi-supervised end-to-end entity and relation extraction</title>
      <author><first>Waleed</first> <last>Ammar</last></author>
      <author><first>Matthew</first> <last>Peters</last></author>
      <author><first>Chandra</first> <last>Bhagavatula</last></author>
      <author><first>Russell</first> <last>Power</last></author>
      <pages>592–596</pages>
      <url hash="3eb5ac51">S17-2097</url>
      <doi>10.18653/v1/S17-2097</doi>
      <abstract>This paper describes our submission for the ScienceIE shared task (SemEval- 2017 Task 10) on entity and relation extraction from scientific papers. Our model is based on the end-to-end relation extraction model of Miwa and Bansal (2016) with several enhancements such as semi-supervised learning via neural language models, character-level encoding, gazetteers extracted from existing knowledge bases, and model ensembles. Our official submission ranked first in end-to-end entity and relation extraction (scenario 1), and second in the relation-only extraction (scenario 3).</abstract>
    </paper>
    <paper id="98">
      <title><fixed-case>LIMSI</fixed-case>-<fixed-case>COT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 12: Neural Architecture for Temporal Information Extraction from Clinical Narratives</title>
      <author><first>Julien</first> <last>Tourille</last></author>
      <author><first>Olivier</first> <last>Ferret</last></author>
      <author><first>Xavier</first> <last>Tannier</last></author>
      <author><first>Aurélie</first> <last>Névéol</last></author>
      <pages>597–602</pages>
      <url hash="eb2ff9c5">S17-2098</url>
      <doi>10.18653/v1/S17-2098</doi>
      <abstract>In this paper we present our participation to SemEval 2017 Task 12. We used a neural network based approach for entity and temporal relation extraction, and experimented with two domain adaptation strategies. We achieved competitive performance for both tasks.</abstract>
    </paper>
    <paper id="99">
      <title><fixed-case>OMAM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Evaluation of <fixed-case>E</fixed-case>nglish State-of-the-Art Sentiment Analysis Models for <fixed-case>A</fixed-case>rabic and a New Topic-based Model</title>
      <author><first>Ramy</first> <last>Baly</last></author>
      <author><first>Gilbert</first> <last>Badaro</last></author>
      <author><first>Ali</first> <last>Hamdi</last></author>
      <author><first>Rawan</first> <last>Moukalled</last></author>
      <author><first>Rita</first> <last>Aoun</last></author>
      <author><first>Georges</first> <last>El-Khoury</last></author>
      <author><first>Ahmad</first> <last>Al Sallab</last></author>
      <author><first>Hazem</first> <last>Hajj</last></author>
      <author><first>Nizar</first> <last>Habash</last></author>
      <author><first>Khaled</first> <last>Shaban</last></author>
      <author><first>Wassim</first> <last>El-Hajj</last></author>
      <pages>603–610</pages>
      <url hash="53516a59">S17-2099</url>
      <doi>10.18653/v1/S17-2099</doi>
      <abstract>While sentiment analysis in English has achieved significant progress, it remains a challenging task in Arabic given the rich morphology of the language. It becomes more challenging when applied to Twitter data that comes with additional sources of noise including dialects, misspellings, grammatical mistakes, code switching and the use of non-textual objects to express sentiments. This paper describes the “OMAM” systems that we developed as part of SemEval-2017 task 4. We evaluate English state-of-the-art methods on Arabic tweets for subtask A. As for the remaining subtasks, we introduce a topic-based approach that accounts for topic specificities by predicting topics or domains of upcoming tweets, and then using this information to predict their sentiment. Results indicate that applying the English state-of-the-art method to Arabic has achieved solid results without significant enhancements. Furthermore, the topic-based method ranked 1st in subtasks C and E, and 2nd in subtask D.</abstract>
    </paper>
    <paper id="100">
      <title><fixed-case>NILC</fixed-case>-<fixed-case>USP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: A Multi-view Ensemble for <fixed-case>T</fixed-case>witter Sentiment Analysis</title>
      <author><first>Edilson Anselmo</first> <last>Corrêa Júnior</last></author>
      <author><first>Vanessa Queiroz</first> <last>Marinho</last></author>
      <author><first>Leandro Borges</first> <last>dos Santos</last></author>
      <pages>611–615</pages>
      <url hash="473174c9">S17-2100</url>
      <doi>10.18653/v1/S17-2100</doi>
      <abstract>This paper describes our multi-view ensemble approach to SemEval-2017 Task 4 on Sentiment Analysis in Twitter, specifically, the Message Polarity Classification subtask for English (subtask A). Our system is a voting ensemble, where each base classifier is trained in a different feature space. The first space is a bag-of-words model and has a Linear SVM as base classifier. The second and third spaces are two different strategies of combining word embeddings to represent sentences and use a Linear SVM and a Logistic Regressor as base classifiers. The proposed system was ranked 18th out of 38 systems considering F1 score and 20th considering recall.</abstract>
    </paper>
    <paper id="101">
      <title>deep<fixed-case>SA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Interpolated Deep Neural Networks for Sentiment Analysis in <fixed-case>T</fixed-case>witter</title>
      <author><first>Tzu-Hsuan</first> <last>Yang</last></author>
      <author><first>Tzu-Hsuan</first> <last>Tseng</last></author>
      <author><first>Chia-Ping</first> <last>Chen</last></author>
      <pages>616–620</pages>
      <url hash="732519d1">S17-2101</url>
      <doi>10.18653/v1/S17-2101</doi>
      <abstract>In this paper, we describe our system implementation for sentiment analysis in Twitter. This system combines two models based on deep neural networks, namely a convolutional neural network (CNN) and a long short-term memory (LSTM) recurrent neural network, through interpolation. Distributed representation of words as vectors are input to the system, and the output is a sentiment class. The neural network models are trained exclusively with the data sets provided by the organizers of SemEval-2017 Task 4 Subtask A. Overall, this system has achieved 0.618 for the average recall rate, 0.587 for the average F1 score, and 0.618 for accuracy.</abstract>
    </paper>
    <paper id="102">
      <title><fixed-case>NNEMB</fixed-case>s at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Neural <fixed-case>T</fixed-case>witter Sentiment Classification: a Simple Ensemble Method with Different Embeddings</title>
      <author><first>Yichun</first> <last>Yin</last></author>
      <author><first>Yangqiu</first> <last>Song</last></author>
      <author><first>Ming</first> <last>Zhang</last></author>
      <pages>621–625</pages>
      <url hash="83cc1c1d">S17-2102</url>
      <doi>10.18653/v1/S17-2102</doi>
      <abstract>Recently, neural twitter sentiment classification has become one of state-of-thearts, which relies less feature engineering work compared with traditional methods. In this paper, we propose a simple and effective ensemble method to further boost the performances of neural models. We collect several word embedding sets which are publicly released (often are learned on different corpus) or constructed by running Skip-gram on released large-scale corpus. We make an assumption that different word embeddings cover different words and encode different semantic knowledge, thus using them together can improve the generalizations and performances of neural models. In the SemEval 2017, our method ranks 1st in Accuracy, 5th in AverageR. Meanwhile, the additional comparisons demonstrate the superiority of our model over these ones based on only one word embedding set. We release our code for the method duplicability.</abstract>
    </paper>
    <paper id="103">
      <title><fixed-case>C</fixed-case>rystal<fixed-case>N</fixed-case>est at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Using Sarcasm Detection for Enhancing Sentiment Classification and Quantification</title>
      <author><first>Raj Kumar</first> <last>Gupta</last></author>
      <author><first>Yinping</first> <last>Yang</last></author>
      <pages>626–633</pages>
      <url hash="c7b48b40">S17-2103</url>
      <doi>10.18653/v1/S17-2103</doi>
      <abstract>This paper describes a system developed for a shared sentiment analysis task and its subtasks organized by SemEval-2017. A key feature of our system is the embedded ability to detect sarcasm in order to enhance the performance of sentiment classification. We first constructed an affect-cognition-sociolinguistics sarcasm features model and trained a SVM-based classifier for detecting sarcastic expressions from general tweets. For sentiment prediction, we developed CrystalNest– a two-level cascade classification system using features combining sarcasm score derived from our sarcasm classifier, sentiment scores from Alchemy, NRC lexicon, n-grams, word embedding vectors, and part-of-speech features. We found that the sarcasm detection derived features consistently benefited key sentiment analysis evaluation metrics, in different degrees, across four subtasks A-D.</abstract>
    </paper>
    <paper id="104">
      <title><fixed-case>SINAI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: User based classification</title>
      <author><first>Salud María</first> <last>Jiménez-Zafra</last></author>
      <author><first>Arturo</first> <last>Montejo-Ráez</last></author>
      <author><first>Maite</first> <last>Martin</last></author>
      <author><first>L. Alfonso</first> <last>Ureña-López</last></author>
      <pages>634–639</pages>
      <url hash="7338f5c4">S17-2104</url>
      <doi>10.18653/v1/S17-2104</doi>
      <abstract>This document describes our participation in SemEval-2017 Task 4: Sentiment Analysis in Twitter. We have only reported results for subtask B - English, determining the polarity towards a topic on a two point scale (positive or negative sentiment). Our main contribution is the integration of user information in the classification process. A SVM model is trained with Word2Vec vectors from user’s tweets extracted from his timeline. The obtained results show that user-specific classifiers trained on tweets from user timeline can introduce noise as they are error prone because they are classified by an imperfect system. This encourages us to explore further integration of user information for author-based Sentiment Analysis.</abstract>
    </paper>
    <paper id="105">
      <title><fixed-case>HLP</fixed-case>@<fixed-case>UP</fixed-case>enn at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4<fixed-case>A</fixed-case>: A simple, self-optimizing text classification system combining dense and sparse vectors</title>
      <author><first>Abeed</first> <last>Sarker</last></author>
      <author><first>Graciela</first> <last>Gonzalez</last></author>
      <pages>640–643</pages>
      <url hash="31a2d64c">S17-2105</url>
      <doi>10.18653/v1/S17-2105</doi>
      <abstract>We present a simple supervised text classification system that combines sparse and dense vector representations of words, and generalized representations of words via clusters. The sparse vectors are generated from word n-gram sequences (1-3). The dense vector representations of words (embeddings) are learned by training a neural network to predict neighboring words in a large unlabeled dataset. To classify a text segment, the different representations of it are concatenated, and the classification is performed using Support Vector Machines (SVM). Our system is particularly intended for use by non-experts of natural language processing and machine learning, and, therefore, the system does not require any manual tuning of parameters or weights. Given a training set, the system automatically generates the training vectors, optimizes the relevant hyper-parameters for the SVM classifier, and trains the classification model. We evaluated this system on the SemEval-2017 English sentiment analysis task. In terms of average F1-score, our system obtained 8th position out of 39 submissions (F1-score: 0.632, average recall: 0.637, accuracy: 0.646).</abstract>
    </paper>
    <paper id="106">
      <title>ej-sa-2017 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Experiments for Target oriented Sentiment Analysis in <fixed-case>T</fixed-case>witter</title>
      <author><first>Enkhzol</first> <last>Dovdon</last></author>
      <author><first>José</first> <last>Saias</last></author>
      <pages>644–647</pages>
      <url hash="b8de344f">S17-2106</url>
      <doi>10.18653/v1/S17-2106</doi>
      <abstract>This paper describes the system we have used for participating in Subtasks A (Message Polarity Classification) and B (Topic-Based Message Polarity Classification according to a two-point scale) of SemEval-2017 Task 4 Sentiment Analysis in Twitter. We used several features with a sentiment lexicon and NLP techniques, Maximum Entropy as a classifier for our system.</abstract>
    </paper>
    <paper id="107">
      <title><fixed-case>S</fixed-case>enti<fixed-case>ME</fixed-case>++ at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Stacking State-of-the-Art Classifiers to Enhance Sentiment Classification</title>
      <author><first>Raphaël</first> <last>Troncy</last></author>
      <author><first>Enrico</first> <last>Palumbo</last></author>
      <author><first>Efstratios</first> <last>Sygkounas</last></author>
      <author><first>Giuseppe</first> <last>Rizzo</last></author>
      <pages>648–652</pages>
      <url hash="ebfa87be">S17-2107</url>
      <doi>10.18653/v1/S17-2107</doi>
      <abstract>In this paper, we describe the participation of the SentiME++ system to the SemEval 2017 Task 4A “Sentiment Analysis in Twitter” that aims to classify whether English tweets are of positive, neutral or negative sentiment. SentiME++ is an ensemble approach to sentiment analysis that leverages stacked generalization to automatically combine the predictions of five state-of-the-art sentiment classifiers. SentiME++ achieved officially 61.30% F1-score, ranking 12th out of 38 participants.</abstract>
    </paper>
    <paper id="108">
      <title><fixed-case>A</fixed-case>mobee at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Deep Learning System for Sentiment Detection on <fixed-case>T</fixed-case>witter</title>
      <author><first>Alon</first> <last>Rozental</last></author>
      <author><first>Daniel</first> <last>Fleischer</last></author>
      <pages>653–658</pages>
      <url hash="19d90653">S17-2108</url>
      <doi>10.18653/v1/S17-2108</doi>
      <abstract>This paper describes the Amobee sentiment analysis system, adapted to compete in SemEval 2017 task 4. The system consists of two parts: a supervised training of RNN models based on a Twitter sentiment treebank, and the use of feedforward NN, Naive Bayes and logistic regression classifiers to produce predictions for the different sub-tasks. The algorithm reached the 3rd place on the 5-label classification task (sub-task C).</abstract>
    </paper>
    <paper id="109">
      <title><fixed-case>TWINA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: <fixed-case>T</fixed-case>witter Sentiment Analysis with Ensemble Gradient Boost Tree Classifier</title>
      <author><first>Naveen Kumar</first> <last>Laskari</last></author>
      <author><first>Suresh Kumar</first> <last>Sanampudi</last></author>
      <pages>659–663</pages>
      <url hash="7274d312">S17-2109</url>
      <doi>10.18653/v1/S17-2109</doi>
      <abstract>This paper describes the TWINA system, with which we participated in SemEval-2017 Task 4B (Topic Based Message Polarity Classification – Two point scale) and 4D (two-point scale Tweet quantification). We implemented ensemble based Gradient Boost Trees classification method for both the tasks. Our system could perform well for the task 4D and ranked 13th among 15 teams, for the task 4B our model ranked 23rd position.</abstract>
    </paper>
    <paper id="110">
      <title>Tw-<fixed-case>S</fixed-case>t<fixed-case>AR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Sentiment Classification of <fixed-case>A</fixed-case>rabic Tweets</title>
      <author><first>Hala</first> <last>Mulki</last></author>
      <author><first>Hatem</first> <last>Haddad</last></author>
      <author><first>Mourad</first> <last>Gridach</last></author>
      <author><first>Ismail</first> <last>Babaoglu</last></author>
      <pages>664–669</pages>
      <url hash="653fe424">S17-2110</url>
      <doi>10.18653/v1/S17-2110</doi>
      <abstract>In this paper, we present our contribution in SemEval 2017 international workshop. We have tackled task 4 entitled “Sentiment analysis in Twitter”, specifically subtask 4A-Arabic. We propose two Arabic sentiment classification models implemented using supervised and unsupervised learning strategies. In both models, Arabic tweets were preprocessed first then various schemes of bag-of-N-grams were extracted to be used as features. The final submission was selected upon the best performance achieved by the supervised learning-based model. However, the results obtained by the unsupervised learning-based model are considered promising and evolvable if more rich lexica are adopted in further work.</abstract>
    </paper>
    <paper id="111">
      <title><fixed-case>OMAM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: <fixed-case>E</fixed-case>nglish Sentiment Analysis with Conditional Random Fields</title>
      <author><first>Chukwuyem</first> <last>Onyibe</last></author>
      <author><first>Nizar</first> <last>Habash</last></author>
      <pages>670–674</pages>
      <url hash="97c1b87e">S17-2111</url>
      <doi>10.18653/v1/S17-2111</doi>
      <abstract>We describe a supervised system that uses optimized Condition Random Fields and lexical features to predict the sentiment of a tweet. The system was submitted to the English version of all subtasks in SemEval-2017 Task 4.</abstract>
    </paper>
    <paper id="112">
      <title>Tweester at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Fusion of Semantic-Affective and pairwise classification models for sentiment analysis in <fixed-case>T</fixed-case>witter</title>
      <author><first>Athanasia</first> <last>Kolovou</last></author>
      <author><first>Filippos</first> <last>Kokkinos</last></author>
      <author><first>Aris</first> <last>Fergadis</last></author>
      <author><first>Pinelopi</first> <last>Papalampidi</last></author>
      <author><first>Elias</first> <last>Iosif</last></author>
      <author><first>Nikolaos</first> <last>Malandrakis</last></author>
      <author><first>Elisavet</first> <last>Palogiannidi</last></author>
      <author><first>Haris</first> <last>Papageorgiou</last></author>
      <author><first>Shrikanth</first> <last>Narayanan</last></author>
      <author><first>Alexandros</first> <last>Potamianos</last></author>
      <pages>675–682</pages>
      <url hash="7a8e6837">S17-2112</url>
      <doi>10.18653/v1/S17-2112</doi>
      <abstract>In this paper, we describe our submission to SemEval2017 Task 4: Sentiment Analysis in Twitter. Specifically the proposed system participated both to tweet polarity classification (two-, three- and five class) and tweet quantification (two and five-class) tasks.</abstract>
    </paper>
    <paper id="113">
      <title><fixed-case>NRU</fixed-case>-<fixed-case>HSE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Tweet Quantification Using Deep Learning Architecture</title>
      <author><first>Nikolay</first> <last>Karpov</last></author>
      <pages>683–688</pages>
      <url hash="bf45cf68">S17-2113</url>
      <doi>10.18653/v1/S17-2113</doi>
      <abstract>In many areas, such as social science, politics or market research, people need to deal with dataset shifting over time. Distribution drift phenomenon usually appears in the field of sentiment analysis, when proportions of instances are changing over time. In this case, the task is to correctly estimate proportions of each sentiment expressed in the set of documents (quantification task). Basically, our study was aimed to analyze the effectiveness of a mixture of quantification technique with one of deep learning architecture. All the techniques are evaluated using the SemEval-2017 Task4 dataset and source code, mentioned in this paper and available online in the Python programming language. The results of an application of the quantification techniques are discussed.</abstract>
    </paper>
    <paper id="114">
      <title><fixed-case>MI</fixed-case>&amp;<fixed-case>T</fixed-case> Lab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 task 4: An Integrated Training Method of Word Vector for Sentiment Classification</title>
      <author><first>Jingjing</first> <last>Zhao</last></author>
      <author><first>Yan</first> <last>Yang</last></author>
      <author><first>Bing</first> <last>Xu</last></author>
      <pages>689–693</pages>
      <url hash="5e6ccf89">S17-2114</url>
      <doi>10.18653/v1/S17-2114</doi>
      <abstract>A CNN method for sentiment classification task in Task 4A of SemEval 2017 is presented. To solve the problem of word2vec training word vector slowly, a method of training word vector by integrating word2vec and Convolutional Neural Network (CNN) is proposed. This training method not only improves the training speed of word2vec, but also makes the word vector more effective for the target task. Furthermore, the word2vec adopts a full connection between the input layer and the projection layer of the Continuous Bag-of-Words (CBOW) for acquiring the semantic information of the original sentence.</abstract>
    </paper>
    <paper id="115">
      <title><fixed-case>S</fixed-case>i<fixed-case>TAKA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Sentiment Analysis in <fixed-case>T</fixed-case>witter Based on a Rich Set of Features</title>
      <author><first>Mohammed</first> <last>Jabreel</last></author>
      <author id="antonio-moreno-ribas"><first>Antonio</first> <last>Moreno</last></author>
      <pages>694–699</pages>
      <url hash="9783816c">S17-2115</url>
      <doi>10.18653/v1/S17-2115</doi>
      <abstract>This paper describes SiTAKA, our system that has been used in task 4A, English and Arabic languages, Sentiment Analysis in Twitter of SemEval2017. The system proposes the representation of tweets using a novel set of features, which include a bag of negated words and the information provided by some lexicons. The polarity of tweets is determined by a classifier based on a Support Vector Machine. Our system ranks 2nd among 8 systems in the Arabic language tweets and ranks 8th among 38 systems in the English-language tweets.</abstract>
    </paper>
    <paper id="116">
      <title><fixed-case>S</fixed-case>enti17 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification</title>
      <author><first>Hussam</first> <last>Hamdan</last></author>
      <pages>700–703</pages>
      <url hash="a6b66fe9">S17-2116</url>
      <doi>10.18653/v1/S17-2116</doi>
      <abstract>This paper presents Senti17 system which uses ten convolutional neural networks (ConvNet) to assign a sentiment label to a tweet. The network consists of a convolutional layer followed by a fully-connected layer and a Softmax on top. Ten instances of this network are initialized with the same word embeddings as inputs but with different initializations for the network weights. We combine the results of all instances by selecting the sentiment label given by the majority of the ten voters. This system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4% average recall.</abstract>
    </paper>
    <paper id="117">
      <title><fixed-case>DUTH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: A Voting Classification Approach for <fixed-case>T</fixed-case>witter Sentiment Analysis</title>
      <author><first>Symeon</first> <last>Symeonidis</last></author>
      <author><first>Dimitrios</first> <last>Effrosynidis</last></author>
      <author><first>John</first> <last>Kordonis</last></author>
      <author><first>Avi</first> <last>Arampatzis</last></author>
      <pages>704–708</pages>
      <url hash="391beb04">S17-2117</url>
      <doi>10.18653/v1/S17-2117</doi>
      <abstract>This report describes our participation to SemEval-2017 Task 4: Sentiment Analysis in Twitter, specifically in subtasks A, B, and C. The approach for text sentiment classification is based on a Majority Vote scheme and combined supervised machine learning methods with classical linguistic resources, including bag-of-words and sentiment lexicon features.</abstract>
    </paper>
    <paper id="118">
      <title><fixed-case>SSN</fixed-case>_<fixed-case>MLRG</fixed-case>1 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Sentiment Analysis in <fixed-case>T</fixed-case>witter Using Multi-Kernel <fixed-case>G</fixed-case>aussian Process Classifier</title>
      <author><first>Angel Deborah</first> <last>S</last></author>
      <author><first>S Milton</first> <last>Rajendram</last></author>
      <author><first>T T</first> <last>Mirnalinee</last></author>
      <pages>709–712</pages>
      <url hash="d5fe7f2f">S17-2118</url>
      <doi>10.18653/v1/S17-2118</doi>
      <abstract>The SSN MLRG1 team for Semeval-2017 task 4 has applied Gaussian Process, with bag of words feature vectors and fixed rule multi-kernel learning, for sentiment analysis of tweets. Since tweets on the same topic, made at different times, may exhibit different emotions, their properties such as smoothness and periodicity also vary with time. Our experiments show that, compared to single kernel, multiple kernels are effective in learning the simultaneous presence of multiple properties.</abstract>
    </paper>
    <paper id="119">
      <title><fixed-case>YNUDLG</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: A <fixed-case>GRU</fixed-case>-<fixed-case>SVM</fixed-case> Model for Sentiment Classification and Quantification in <fixed-case>T</fixed-case>witter</title>
      <author><first>Ming</first> <last>Wang</last></author>
      <author><first>Biao</first> <last>Chu</last></author>
      <author><first>Qingxun</first> <last>Liu</last></author>
      <author><first>Xiaobing</first> <last>Zhou</last></author>
      <pages>713–717</pages>
      <url hash="8884a080">S17-2119</url>
      <doi>10.18653/v1/S17-2119</doi>
      <abstract>Sentiment analysis is one of the central issues in Natural Language Processing and has become more and more important in many fields. Typical sentiment analysis classifies the sentiment of sentences into several discrete classes (e.g.,positive or negative). In this paper we describe our deep learning system(combining GRU and SVM) to solve both two-, three- and five-tweet polarity classifications. We first trained a gated recurrent neural network using pre-trained word embeddings, then we extracted features from GRU layer and input these features into support vector machine to fulfill both the classification and quantification subtasks. The proposed approach achieved 37th, 19th, and 14rd places in subtasks A, B and C, respectively.</abstract>
    </paper>
    <paper id="120">
      <title><fixed-case>LSIS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Using Adapted Sentiment Similarity Seed Words For <fixed-case>E</fixed-case>nglish and <fixed-case>A</fixed-case>rabic Tweet Polarity Classification</title>
      <author><first>Amal</first> <last>Htait</last></author>
      <author><first>Sébastien</first> <last>Fournier</last></author>
      <author><first>Patrice</first> <last>Bellot</last></author>
      <pages>718–722</pages>
      <url hash="75b44678">S17-2120</url>
      <doi>10.18653/v1/S17-2120</doi>
      <abstract>We present, in this paper, our contribution in SemEval2017 task 4 : “Sentiment Analysis in Twitter”, subtask A: “Message Polarity Classification”, for English and Arabic languages. Our system is based on a list of sentiment seed words adapted for tweets. The sentiment relations between seed words and other terms are captured by cosine similarity between the word embedding representations (word2vec). These seed words are extracted from datasets of annotated tweets available online. Our tests, using these seed words, show significant improvement in results compared to the use of Turney and Littman’s (2003) seed words, on polarity classification of tweet messages.</abstract>
    </paper>
    <paper id="121">
      <title><fixed-case>EL</fixed-case>i<fixed-case>RF</fixed-case>-<fixed-case>UPV</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Sentiment Analysis using Deep Learning</title>
      <author><first>José-Ángel</first> <last>González</last></author>
      <author><first>Ferran</first> <last>Pla</last></author>
      <author><first>Lluís-F.</first> <last>Hurtado</last></author>
      <pages>723–727</pages>
      <url hash="b4c38016">S17-2121</url>
      <doi>10.18653/v1/S17-2121</doi>
      <abstract>This paper describes the participation of ELiRF-UPV team at task 4 of SemEval2017. Our approach is based on the use of convolutional and recurrent neural networks and the combination of general and specific word embeddings with polarity lexicons. We participated in all of the proposed subtasks both for English and Arabic languages using the same system with small variations.</abstract>
    </paper>
    <paper id="122">
      <title><fixed-case>XJSA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: A Deep System for Sentiment Classification in <fixed-case>T</fixed-case>witter</title>
      <author><first>Yazhou</first> <last>Hao</last></author>
      <author><first>YangYang</first> <last>Lan</last></author>
      <author><first>Yufei</first> <last>Li</last></author>
      <author><first>Chen</first> <last>Li</last></author>
      <pages>728–731</pages>
      <url hash="207ccb67">S17-2122</url>
      <doi>10.18653/v1/S17-2122</doi>
      <abstract>This paper describes the XJSA System submission from XJTU. Our system was created for SemEval2017 Task 4 – subtask A which is very popular and fundamental. The system is based on convolutional neural network and word embedding. We used two pre-trained word vectors and adopt a dynamic strategy for k-max pooling.</abstract>
    </paper>
    <paper id="123">
      <title>Adullam at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Sentiment Analyzer Using Lexicon Integrated Convolutional Neural Networks with Attention</title>
      <author><first>Joosung</first> <last>Yoon</last></author>
      <author><first>Kigon</first> <last>Lyu</last></author>
      <author><first>Hyeoncheol</first> <last>Kim</last></author>
      <pages>732–736</pages>
      <url hash="ea929615">S17-2123</url>
      <doi>10.18653/v1/S17-2123</doi>
      <abstract>We propose a sentiment analyzer for the prediction of document-level sentiments of English micro-blog messages from Twitter. The proposed method is based on lexicon integrated convolutional neural networks with attention (LCA). Its performance was evaluated using the datasets provided by SemEval competition (Task 4). The proposed sentiment analyzer obtained an average F1 of 55.2%, an average recall of 58.9% and an accuracy of 61.4%.</abstract>
    </paper>
    <paper id="124">
      <title><fixed-case>EICA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: A Simple Convolutional Neural Network for Topic-based Sentiment Classification</title>
      <author><last>Wang</last> <first>Maoquan</first></author>
      <author><last>Chen</last> <first>Shiyun</first></author>
      <author><last>Xie</last> <first>Yufei</first></author>
      <author><last>Zhao</last> <first>Lu</first></author>
      <pages>737–740</pages>
      <url hash="53668ae8">S17-2124</url>
      <doi>10.18653/v1/S17-2124</doi>
      <abstract>This paper describes our approach for SemEval-2017 Task 4 - Sentiment Analysis in Twitter (SAT). Its five subtasks are divided into two categories: (1) sentiment classification, i.e., predicting topic-based tweet sentiment polarity, and (2) sentiment quantification, that is, estimating the sentiment distributions of a set of given tweets. We build a convolutional sentence classification system for the task of SAT. Official results show that the experimental results of our system are comparative.</abstract>
    </paper>
    <paper id="125">
      <title>fun<fixed-case>S</fixed-case>entiment at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Topic-Based Message Sentiment Classification by Exploiting Word Embeddings, Text Features and Target Contexts</title>
      <author><first>Quanzhi</first> <last>Li</last></author>
      <author><first>Armineh</first> <last>Nourbakhsh</last></author>
      <author><first>Xiaomo</first> <last>Liu</last></author>
      <author><first>Rui</first> <last>Fang</last></author>
      <author><first>Sameena</first> <last>Shah</last></author>
      <pages>741–746</pages>
      <url hash="88f798b1">S17-2125</url>
      <doi>10.18653/v1/S17-2125</doi>
      <abstract>This paper describes the approach we used for SemEval-2017 Task 4: Sentiment Analysis in Twitter. Topic-based (target-dependent) sentiment analysis has become attractive and been used in some applications recently, but it is still a challenging research task. In our approach, we take the left and right context of a target into consideration when generating polarity classification features. We use two types of word embeddings in our classifiers: the general word embeddings learned from 200 million tweets, and sentiment-specific word embeddings learned from 10 million tweets using distance supervision. We also incorporate a text feature model in our algorithm. This model produces features based on text negation, tf.idf weighting scheme, and a Rocchio text classification method. We participated in four subtasks (B, C, D &amp; E for English), all of which are about topic-based message polarity classification. Our team is ranked #6 in subtask B, #3 by MAEu and #9 by MAEm in subtask C, #3 using RAE and #6 using KLD in subtask D, and #3 in subtask E.</abstract>
    </paper>
    <paper id="126">
      <title><fixed-case>D</fixed-case>ata<fixed-case>S</fixed-case>tories at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Deep <fixed-case>LSTM</fixed-case> with Attention for Message-level and Topic-based Sentiment Analysis</title>
      <author><first>Christos</first> <last>Baziotis</last></author>
      <author><first>Nikos</first> <last>Pelekis</last></author>
      <author><first>Christos</first> <last>Doulkeridis</last></author>
      <pages>747–754</pages>
      <url hash="e2e20f77">S17-2126</url>
      <doi>10.18653/v1/S17-2126</doi>
      <abstract>In this paper we present two deep-learning systems that competed at SemEval-2017 Task 4 “Sentiment Analysis in Twitter”. We participated in all subtasks for English tweets, involving message-level and topic-based sentiment polarity classification and quantification. We use Long Short-Term Memory (LSTM) networks augmented with two kinds of attention mechanisms, on top of word embeddings pre-trained on a big collection of Twitter messages. Also, we present a text processing tool suitable for social network messages, which performs tokenization, word normalization, segmentation and spell correction. Moreover, our approach uses no hand-crafted features or sentiment lexicons. We ranked 1st (tie) in Subtask A, and achieved very competitive results in the rest of the Subtasks. Both the word embeddings and our text processing tool are available to the research community.</abstract>
    </paper>
    <paper id="127">
      <title><fixed-case>T</fixed-case>wi<fixed-case>S</fixed-case>e at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Five-point <fixed-case>T</fixed-case>witter Sentiment Classification and Quantification</title>
      <author><first>Georgios</first> <last>Balikas</last></author>
      <pages>755–759</pages>
      <url hash="2a0a5cca">S17-2127</url>
      <doi>10.18653/v1/S17-2127</doi>
      <abstract>The paper describes the participation of the team “TwiSE” in the SemEval-2017 challenge. Specifically, I participated at Task 4 entitled “Sentiment Analysis in Twitter” for which I implemented systems for five-point tweet classification (Subtask C) and five-point tweet quantification (Subtask E) for English tweets. In the feature extraction steps the systems rely on the vector space model, morpho-syntactic analysis of the tweets and several sentiment lexicons. The classification step of Subtask C uses a Logistic Regression trained with the one-versus-rest approach. Another instance of Logistic Regression combined with the classify-and-count approach is trained for the quantification task of Subtask E. In the official leaderboard the system is ranked <i>5/15</i> in Subtask C and <i>2/12</i> in Subtask E.
    </abstract>
    </paper>
    <paper id="128">
      <title><fixed-case>LIA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: An Ensemble of Neural Networks for Sentiment Classification</title>
      <author><first>Mickael</first> <last>Rouvier</last></author>
      <pages>760–765</pages>
      <url hash="9f3c7527">S17-2128</url>
      <doi>10.18653/v1/S17-2128</doi>
      <abstract>This paper describes the system developed at LIA for the SemEval-2017 evaluation campaign. The goal of Task 4.A was to identify sentiment polarity in tweets. The system is an ensemble of Deep Neural Network (DNN) models: Convolutional Neural Network (CNN) and Recurrent Neural Network Long Short-Term Memory (RNN-LSTM). We initialize the input representation of DNN with different sets of embeddings trained on large datasets. The ensemble of DNNs are combined using a score-level fusion approach. The system ranked 2nd at SemEval-2017 and obtained an average recall of 67.6%.</abstract>
    </paper>
    <paper id="129">
      <title><fixed-case>T</fixed-case>opic<fixed-case>T</fixed-case>hunder at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Sentiment Classification Using a Convolutional Neural Network with Distant Supervision</title>
      <author><first>Simon</first> <last>Müller</last></author>
      <author><first>Tobias</first> <last>Huonder</last></author>
      <author><first>Jan</first> <last>Deriu</last></author>
      <author><first>Mark</first> <last>Cieliebak</last></author>
      <pages>766–770</pages>
      <url hash="f0ccdaf7">S17-2129</url>
      <doi>10.18653/v1/S17-2129</doi>
      <abstract>In this paper, we propose a classifier for predicting topic-specific sentiments of English Twitter messages. Our method is based on a 2-layer CNN.With a distant supervised phase we leverage a large amount of weakly-labelled training data. Our system was evaluated on the data provided by the SemEval-2017 competition in the Topic-Based Message Polarity Classification subtask, where it ranked 4th place.</abstract>
    </paper>
    <paper id="130">
      <title><fixed-case>INGEOTEC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2017 Task 4: A <fixed-case>B</fixed-case>4<fixed-case>MSA</fixed-case> Ensemble based on Genetic Programming for <fixed-case>T</fixed-case>witter Sentiment Analysis</title>
      <author><first>Sabino</first> <last>Miranda-Jiménez</last></author>
      <author><first>Mario</first> <last>Graff</last></author>
      <author><first>Eric Sadit</first> <last>Tellez</last></author>
      <author><first>Daniela</first> <last>Moctezuma</last></author>
      <pages>771–776</pages>
      <url hash="4e4367e5">S17-2130</url>
      <doi>10.18653/v1/S17-2130</doi>
      <abstract>This paper describes the system used in SemEval-2017 Task 4 (Subtask A): Message Polarity Classification for both English and Arabic languages. Our proposed system is an ensemble of two layers, the first one uses our generic framework for multilingual polarity classification (B4MSA) and the second layer combines all the decision function values predicted by B4MSA systems using a non-linear function evolved using a Genetic Programming system, EvoDAG. With this approach, the best performances reached by our system were macro-recall 0.68 (English) and 0.477 (Arabic) which set us in sixth and fourth positions in the results table, respectively.</abstract>
    </paper>
    <paper id="131">
      <title><fixed-case>BUSEM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4<fixed-case>A</fixed-case> Sentiment Analysis with Word Embedding and Long Short Term Memory <fixed-case>RNN</fixed-case> Approaches</title>
      <author><first>Deger</first> <last>Ayata</last></author>
      <author><first>Murat</first> <last>Saraclar</last></author>
      <author><first>Arzucan</first> <last>Ozgur</last></author>
      <pages>777–783</pages>
      <url hash="e5871d56">S17-2131</url>
      <doi>10.18653/v1/S17-2131</doi>
      <abstract>This paper describes our approach for SemEval-2017 Task 4: Sentiment Analysis in Twitter. We have participated in Subtask A: Message Polarity Classification subtask and developed two systems. The first system uses word embeddings for feature representation and Support Vector Machine, Random Forest and Naive Bayes algorithms for classification of Twitter messages into negative, neutral and positive polarity. The second system is based on Long Short Term Memory Recurrent Neural Networks and uses word indexes as sequence of inputs for feature representation.</abstract>
    </paper>
    <paper id="132">
      <title><fixed-case>T</fixed-case>ake<fixed-case>L</fixed-case>ab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Recent Deaths and the Power of Nostalgia in Sentiment Analysis in <fixed-case>T</fixed-case>witter</title>
      <author><first>David</first> <last>Lozić</last></author>
      <author><first>Doria</first> <last>Šarić</last></author>
      <author><first>Ivan</first> <last>Tokić</last></author>
      <author><first>Zoran</first> <last>Medić</last></author>
      <author><first>Jan</first> <last>Šnajder</last></author>
      <pages>784–789</pages>
      <url hash="89e75ac4">S17-2132</url>
      <doi>10.18653/v1/S17-2132</doi>
      <abstract>This paper describes the system we submitted to SemEval-2017 Task 4 (Sentiment Analysis in Twitter), specifically subtasks A, B, and D. Our main focus was topic-based message polarity classification on a two-point scale (subtask B). The system we submitted uses a Support Vector Machine classifier with rich set of features, ranging from standard to more creative, task-specific features, including a series of rating-based features as well as features that account for sentimental reminiscence of past topics and deceased famous people. Our system ranked 14th out of 39 submissions in subtask A, 5th out of 24 submissions in subtask B, and 3rd out of 16 submissions in subtask D.</abstract>
    </paper>
    <paper id="133">
      <title><fixed-case>N</fixed-case>ile<fixed-case>TMRG</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: <fixed-case>A</fixed-case>rabic Sentiment Analysis</title>
      <author><first>Samhaa R.</first> <last>El-Beltagy</last></author>
      <author><first>Mona</first> <last>El Kalamawy</last></author>
      <author><first>Abu Bakr</first> <last>Soliman</last></author>
      <pages>790–795</pages>
      <url hash="3edd98f3">S17-2133</url>
      <doi>10.18653/v1/S17-2133</doi>
      <abstract>This paper describes two systems that were used by the NileTMRG for addressing Arabic Sentiment Analysis as part of SemEval-2017, task 4. NileTMRG participated in three Arabic related subtasks which are: Subtask A (Message Polarity Classification), Subtask B (Topic-Based Message Polarity classification) and Subtask D (Tweet quantification). For subtask A, we made use of NU’s sentiment analyzer which we augmented with a scored lexicon. For subtasks B and D, we used an ensemble of three different classifiers. The first classifier was a convolutional neural network that used trained (word2vec) word embeddings. The second classifier consisted of a MultiLayer Perceptron while the third classifier was a Logistic regression model that takes the same input as the second classifier. Voting between the three classifiers was used to determine the final outcome. In all three Arabic related tasks in which NileTMRG participated, the team ranked at number one.</abstract>
    </paper>
    <paper id="134">
      <title><fixed-case>YNU</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2017 Task 4: Using A Multi-Channel <fixed-case>CNN</fixed-case>-<fixed-case>LSTM</fixed-case> Model for Sentiment Classification</title>
      <author><first>Haowei</first> <last>Zhang</last></author>
      <author><first>Jin</first> <last>Wang</last></author>
      <author><first>Jixian</first> <last>Zhang</last></author>
      <author><first>Xuejie</first> <last>Zhang</last></author>
      <pages>796–801</pages>
      <url hash="bbbc2915">S17-2134</url>
      <doi>10.18653/v1/S17-2134</doi>
      <abstract>In this paper, we propose a multi-channel convolutional neural network-long short-term memory (CNN-LSTM) model that consists of two parts: multi-channel CNN and LSTM to analyze the sentiments of short English messages from Twitter. Un-like a conventional CNN, the proposed model applies a multi-channel strategy that uses several filters of different length to extract active local n-gram features in different scales. This information is then sequentially composed using LSTM. By combining both CNN and LSTM, we can consider both local information within tweets and long-distance dependency across tweets in the classification process. Officially released results show that our system outperforms the baseline algo-rithm.</abstract>
    </paper>
    <paper id="135">
      <title><fixed-case>TSA</fixed-case>-<fixed-case>INF</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: An Ensemble of Deep Learning Architectures Including Lexicon Features for <fixed-case>T</fixed-case>witter Sentiment Analysis</title>
      <author><first>Amit Ajit</first> <last>Deshmane</last></author>
      <author><first>Jasper</first> <last>Friedrichs</last></author>
      <pages>802–806</pages>
      <url hash="32fd39ee">S17-2135</url>
      <doi>10.18653/v1/S17-2135</doi>
      <abstract>This paper describes the submission of team TSA-INF to SemEval-2017 Task 4 Subtask A. The submitted system is an ensemble of three varying deep learning architectures for sentiment analysis. The core of the architecture is a convolutional neural network that performs well on text classification as is. The second subsystem is a gated recurrent neural network implementation. Additionally, the third system integrates opinion lexicons directly into a convolution neural network architecture. The resulting ensemble of the three architectures achieved a top ten ranking with a macro-averaged recall of 64.3%. Additional results comparing variations of the submitted system are not conclusive enough to determine a best architecture, but serve as a benchmark for further implementations.</abstract>
    </paper>
    <paper id="136">
      <title><fixed-case>UCSC</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Sense n-grams for Sentiment Analysis in <fixed-case>T</fixed-case>witter</title>
      <author><first>José</first> <last>Abreu</last></author>
      <author><first>Iván</first> <last>Castro</last></author>
      <author><first>Claudia</first> <last>Martínez</last></author>
      <author><first>Sebastián</first> <last>Oliva</last></author>
      <author><first>Yoan</first> <last>Gutiérrez</last></author>
      <pages>807–811</pages>
      <url hash="4a268340">S17-2136</url>
      <doi>10.18653/v1/S17-2136</doi>
      <abstract>This paper describes the system submitted to SemEval-2017 Task 4-A Sentiment Analysis in Twitter developed by the UCSC-NLP team. We studied how relationships between sense n-grams and sentiment polarities can contribute to this task, i.e. co-occurrences of WordNet senses in the tweet, and the polarity. Furthermore, we evaluated the effect of discarding a large set of features based on char-grams reported in preceding works. Based on these elements, we developed a SVM system, which exploring SentiWordNet as a polarity lexicon. It achieves an <tex-math>F_1=0.624</tex-math>

of average. Among 39 submissions to this task, we ranked 10th. </abstract>
    </paper>
    <paper id="137">
      <title><fixed-case>ECNU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 4: Evaluating Effective Features on Machine Learning Methods for <fixed-case>T</fixed-case>witter Message Polarity Classification</title>
      <author><first>Yunxiao</first> <last>Zhou</last></author>
      <author><first>Man</first> <last>Lan</last></author>
      <author><first>Yuanbin</first> <last>Wu</last></author>
      <pages>812–816</pages>
      <url hash="79dbabf5">S17-2137</url>
      <doi>10.18653/v1/S17-2137</doi>
      <abstract>This paper reports our submission to subtask A of task 4 (Sentiment Analysis in Twitter, SAT) in SemEval 2017, i.e., Message Polarity Classification. We investigated several traditional Natural Language Processing (NLP) features, domain specific features and word embedding features together with supervised machine learning methods to address this task. Officially released results showed that our system ranked above average.</abstract>
    </paper>
    <paper id="138">
      <title>Fortia-<fixed-case>FBK</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines</title>
      <author><first>Youness</first> <last>Mansar</last></author>
      <author><first>Lorenzo</first> <last>Gatti</last></author>
      <author><first>Sira</first> <last>Ferradans</last></author>
      <author><first>Marco</first> <last>Guerini</last></author>
      <author><first>Jacopo</first> <last>Staiano</last></author>
      <pages>817–822</pages>
      <url hash="6bb7751a">S17-2138</url>
      <attachment type="poster" hash="3e43f7b8">S17-2138.Poster.pdf</attachment>
      <doi>10.18653/v1/S17-2138</doi>
      <abstract>In this paper, we describe a methodology to infer Bullish or Bearish sentiment towards companies/brands. More specifically, our approach leverages affective lexica and word embeddings in combination with convolutional neural networks to infer the sentiment of financial news headlines towards a target company. Such architecture was used and evaluated in the context of the SemEval 2017 challenge (task 5, subtask 2), in which it obtained the best performance.</abstract>
    </paper>
    <paper id="139">
      <title><fixed-case>SSN</fixed-case>_<fixed-case>MLRG</fixed-case>1 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Fine-Grained Sentiment Analysis Using Multiple Kernel <fixed-case>G</fixed-case>aussian Process Regression Model</title>
      <author><first>Angel Deborah</first> <last>S</last></author>
      <author><first>S Milton</first> <last>Rajendram</last></author>
      <author><first>T T</first> <last>Mirnalinee</last></author>
      <pages>823–826</pages>
      <url hash="aeb9940a">S17-2139</url>
      <doi>10.18653/v1/S17-2139</doi>
      <abstract>The system developed by the SSN_MLRG1 team for Semeval-2017 task 5 on fine-grained sentiment analysis uses Multiple Kernel Gaussian Process for identifying the optimistic and pessimistic sentiments associated with companies and stocks. Since the comments made at different times about the same companies and stocks may display different emotions, their properties such as smoothness and periodicity may vary. Our experiments show that while single kernel Gaussian Process can learn certain properties well, Multiple Kernel Gaussian Process are effective in learning the presence of different properties simultaneously.</abstract>
    </paper>
    <paper id="140">
      <title><fixed-case>IBA</fixed-case>-Sys at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News</title>
      <author><first>Zarmeen</first> <last>Nasim</last></author>
      <pages>827–831</pages>
      <url hash="328cb5e5">S17-2140</url>
      <doi>10.18653/v1/S17-2140</doi>
      <abstract>This paper presents the details of our system IBA-Sys that participated in SemEval Task: Fine-grained sentiment analysis on Financial Microblogs and News. Our system participated in both tracks. For microblogs track, a supervised learning approach was adopted and the regressor was trained using XgBoost regression algorithm on lexicon features. For news headlines track, an ensemble of regressors was used to predict sentiment score. One regressor was trained using TF-IDF features and another was trained using the n-gram features. The source code is available at Github.</abstract>
    </paper>
    <paper id="141">
      <title><fixed-case>HHU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Data using Machine Learning Methods</title>
      <author><first>Tobias</first> <last>Cabanski</last></author>
      <author><first>Julia</first> <last>Romberg</last></author>
      <author><first>Stefan</first> <last>Conrad</last></author>
      <pages>832–836</pages>
      <url hash="f96dda3d">S17-2141</url>
      <doi>10.18653/v1/S17-2141</doi>
      <abstract>In this Paper a system for solving SemEval-2017 Task 5 is presented. This task is divided into two tracks where the sentiment of microblog messages and news headlines has to be predicted. Since two submissions were allowed, two different machine learning methods were developed to solve this task, a support vector machine approach and a recurrent neural network approach. To feed in data for these approaches, different feature extraction methods are used, mainly word representations and lexica. The best submissions for both tracks are provided by the recurrent neural network which achieves a F1-score of 0.729 in track 1 and 0.702 in track 2.</abstract>
    </paper>
    <paper id="142">
      <title><fixed-case>INF</fixed-case>-<fixed-case>UFRGS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: A Supervised Identification of Sentiment Score in Tweets and Headlines</title>
      <author><first>Tiago</first> <last>Zini</last></author>
      <author><first>Karin</first> <last>Becker</last></author>
      <author><first>Marcelo</first> <last>Dias</last></author>
      <pages>837–841</pages>
      <url hash="97d07a71">S17-2142</url>
      <doi>10.18653/v1/S17-2142</doi>
      <abstract>This paper describes a supervised solution for detecting the polarity scores of tweets or headline news in the financial domain, submitted to the SemEval 2017 Fine-Grained Sentiment Analysis on Financial Microblogs and News Task. The premise is that it is possible to understand market reaction over a company stock by measuring the positive/negative sentiment contained in the financial tweets and news headlines, where polarity is measured in a continuous scale ranging from -1.0 (very bearish) to 1.0 (very bullish). Our system receives as input the textual content of tweets or news headlines, together with their ids, stock cashtag or name of target company, and the polarity score gold standard for the training dataset. Our solution retrieves features from these text instances using n-gram, hashtags, sentiment score calculated by a external APIs and others features to train a regression model capable to detect continuous score of these sentiments with precision.</abstract>
    </paper>
    <paper id="143">
      <title><fixed-case>HCS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Polarity detection in business news using convolutional neural networks</title>
      <author><first>Lidia</first> <last>Pivovarova</last></author>
      <author><first>Llorenç</first> <last>Escoter</last></author>
      <author><first>Arto</first> <last>Klami</last></author>
      <author><first>Roman</first> <last>Yangarber</last></author>
      <pages>842–846</pages>
      <url hash="b19c8a6c">S17-2143</url>
      <doi>10.18653/v1/S17-2143</doi>
      <abstract>Task 5 of SemEval-2017 involves fine-grained sentiment analysis on financial microblogs and news. Our solution for determining the sentiment score extends an earlier convolutional neural network for sentiment analysis in several ways. We explicitly encode a focus on a particular company, we apply a data augmentation scheme, and use a larger data collection to complement the small training data provided by the task organizers. The best results were achieved by training a model on an external dataset and then tuning it using the provided training dataset.</abstract>
    </paper>
    <paper id="144">
      <title><fixed-case>NLG</fixed-case>301 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News</title>
      <author><first>Chung-Chi</first> <last>Chen</last></author>
      <author><first>Hen-Hsen</first> <last>Huang</last></author>
      <author><first>Hsin-Hsi</first> <last>Chen</last></author>
      <pages>847–851</pages>
      <url hash="6c3fd57f">S17-2144</url>
      <doi>10.18653/v1/S17-2144</doi>
      <abstract>Short length, multi-targets, target relation-ship, monetary expressions, and outside reference are characteristics of financial tweets. This paper proposes methods to extract target spans from a tweet and its referencing web page. Total 15 publicly available sentiment dictionaries and one sentiment dictionary constructed from training set, containing sentiment scores in binary or real numbers, are used to compute the sentiment scores of text spans. Moreover, the correlation coeffi-cients of the price return between any two stocks are learned with the price data from Bloomberg. They are used to capture the relationships between the interesting tar-get and other stocks mentioned in a tweet. The best result of our method in both sub-task are 56.68% and 55.43%, evaluated by evaluation method 2.</abstract>
    </paper>
    <paper id="145">
      <title>fun<fixed-case>S</fixed-case>entiment at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs Using Word Vectors Built from <fixed-case>S</fixed-case>tock<fixed-case>T</fixed-case>wits and <fixed-case>T</fixed-case>witter</title>
      <author><first>Quanzhi</first> <last>Li</last></author>
      <author><first>Sameena</first> <last>Shah</last></author>
      <author><first>Armineh</first> <last>Nourbakhsh</last></author>
      <author><first>Rui</first> <last>Fang</last></author>
      <author><first>Xiaomo</first> <last>Liu</last></author>
      <pages>852–856</pages>
      <url hash="b26d5553">S17-2145</url>
      <doi>10.18653/v1/S17-2145</doi>
      <abstract>This paper describes the approach we used for SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs. We use three types of word embeddings in our algorithm: word embeddings learned from 200 million tweets, sentiment-specific word embeddings learned from 10 million tweets using distance supervision, and word embeddings learned from 20 million StockTwits messages. In our approach, we also take the left and right context of the target company into consideration when generating polarity prediction features. All the features generated from different word embeddings and contexts are integrated together to train our algorithm</abstract>
    </paper>
    <paper id="146">
      <title><fixed-case>S</fixed-case>enti<fixed-case>H</fixed-case>eros at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: An application of Sentiment Analysis on Financial Tweets</title>
      <author><first>Narges</first> <last>Tabari</last></author>
      <author><first>Armin</first> <last>Seyeditabari</last></author>
      <author><first>Wlodek</first> <last>Zadrozny</last></author>
      <pages>857–860</pages>
      <url hash="ea6322d7">S17-2146</url>
      <doi>10.18653/v1/S17-2146</doi>
      <abstract>Sentiment analysis is the process of identifying the opinion expressed in text. Recently it has been used to study behavioral finance, and in particular the effect of opinions and emotions on economic or financial decisions. SemEval-2017 task 5 focuses on the financial market as the domain for sentiment analysis of text; specifically, task 5, subtask 1 focuses on financial tweets about stock symbols. In this paper, we describe a machine learning classifier for binary classification of financial tweets. We used natural language processing techniques and the random forest algorithm to train our model, and tuned it for the training dataset of Task 5, subtask 1. Our system achieves the 7th rank on the leaderboard of the task.</abstract>
    </paper>
    <paper id="147">
      <title><fixed-case>DUTH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Sentiment Predictability in Financial Microblogging and News Articles</title>
      <author><first>Symeon</first> <last>Symeonidis</last></author>
      <author><first>John</first> <last>Kordonis</last></author>
      <author><first>Dimitrios</first> <last>Effrosynidis</last></author>
      <author><first>Avi</first> <last>Arampatzis</last></author>
      <pages>861–865</pages>
      <url hash="2c5ecee1">S17-2147</url>
      <doi>10.18653/v1/S17-2147</doi>
      <abstract>We present the system developed by the team DUTH for the participation in Semeval-2017 task 5 - Fine-Grained Sentiment Analysis on Financial Microblogs and News, in subtasks A and B. Our approach to determine the sentiment of Microblog Messages and News Statements &amp; Headlines is based on linguistic preprocessing, feature engineering, and supervised machine learning techniques. To train our model, we used Neural Network Regression, Linear Regression, Boosted Decision Tree Regression and Decision Forrest Regression classifiers to forecast sentiment scores. At the end, we present an error measure, so as to improve the performance about forecasting methods of the system.</abstract>
    </paper>
    <paper id="148">
      <title><fixed-case>T</fixed-case>ake<fixed-case>L</fixed-case>ab at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Linear aggregation of word embeddings for fine-grained sentiment analysis of financial news</title>
      <author><first>Leon</first> <last>Rotim</last></author>
      <author><first>Martin</first> <last>Tutek</last></author>
      <author><first>Jan</first> <last>Šnajder</last></author>
      <pages>866–871</pages>
      <url hash="70776738">S17-2148</url>
      <doi>10.18653/v1/S17-2148</doi>
      <abstract>This paper describes our system for fine-grained sentiment scoring of news headlines submitted to SemEval 2017 task 5–subtask 2. Our system uses a feature-light method that consists of a Support Vector Regression (SVR) with various kernels and word vectors as features. Our best-performing submission scored 3rd on the task out of 29 teams and 4th out of 45 submissions with a cosine score of 0.733.</abstract>
    </paper>
    <paper id="149">
      <title><fixed-case>UW</fixed-case>-<fixed-case>F</fixed-case>in<fixed-case>S</fixed-case>ent at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Sentiment Analysis on Financial News Headlines using Training Dataset Augmentation</title>
      <author><first>Vineet</first> <last>John</last></author>
      <author><first>Olga</first> <last>Vechtomova</last></author>
      <pages>872–876</pages>
      <url hash="31f0e311">S17-2149</url>
      <doi>10.18653/v1/S17-2149</doi>
      <abstract>This paper discusses the approach taken by the UWaterloo team to arrive at a solution for the Fine-Grained Sentiment Analysis problem posed by Task 5 of SemEval 2017. The paper describes the document vectorization and sentiment score prediction techniques used, as well as the design and implementation decisions taken while building the system for this task. The system uses text vectorization models, such as N-gram, TF-IDF and paragraph embeddings, coupled with regression model variants to predict the sentiment scores. Amongst the methods examined, unigrams and bigrams coupled with simple linear regression obtained the best baseline accuracy. The paper also explores data augmentation methods to supplement the training dataset. This system was designed for Subtask 2 (News Statements and Headlines).</abstract>
    </paper>
    <paper id="150">
      <title><fixed-case>R</fixed-case>i<fixed-case>TUAL</fixed-case>-<fixed-case>UH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Sentiment Analysis on Financial Data Using Neural Networks</title>
      <author><first>Sudipta</first> <last>Kar</last></author>
      <author><first>Suraj</first> <last>Maharjan</last></author>
      <author><first>Thamar</first> <last>Solorio</last></author>
      <pages>877–882</pages>
      <url hash="8ab635d0">S17-2150</url>
      <doi>10.18653/v1/S17-2150</doi>
      <abstract>In this paper, we present our systems for the “SemEval-2017 Task-5 on Fine-Grained Sentiment Analysis on Financial Microblogs and News”. In our system, we combined hand-engineered lexical, sentiment and metadata features, the representations learned from Convolutional Neural Networks (CNN) and Bidirectional Gated Recurrent Unit (Bi-GRU) with Attention model applied on top. With this architecture we obtained weighted cosine similarity scores of 0.72 and 0.74 for subtask-1 and subtask-2, respectively. Using the official scoring system, our system ranked the second place for subtask-2 and eighth place for the subtask-1. It ranked first for both of the subtasks by the scores achieved by an alternate scoring system.</abstract>
    </paper>
    <paper id="151">
      <title><fixed-case>COMMIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Ontology-based Method for Sentiment Analysis of Financial Headlines</title>
      <author><first>Kim</first> <last>Schouten</last></author>
      <author><first>Flavius</first> <last>Frasincar</last></author>
      <author><first>Franciska</first> <last>de Jong</last></author>
      <pages>883–887</pages>
      <url hash="19264abb">S17-2151</url>
      <doi>10.18653/v1/S17-2151</doi>
      <abstract>This paper describes our submission to Task 5 of SemEval 2017, Fine-Grained Sentiment Analysis on Financial Microblogs and News, where we limit ourselves to performing sentiment analysis on news headlines only (track 2). The approach presented in this paper uses a Support Vector Machine to do the required regression, and besides unigrams and a sentiment tool, we use various ontology-based features. To this end we created a domain ontology that models various concepts from the financial domain. This allows us to model the sentiment of actions depending on which entity they are affecting (e.g., ‘decreasing debt’ is positive, but ‘decreasing profit’ is negative). The presented approach yielded a cosine distance of 0.6810 on the official test data, resulting in the 12th position.</abstract>
    </paper>
    <paper id="152">
      <title><fixed-case>ECNU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: An Ensemble of Regression Algorithms with Effective Features for Fine-Grained Sentiment Analysis in Financial Domain</title>
      <author><first>Mengxiao</first> <last>Jiang</last></author>
      <author><first>Man</first> <last>Lan</last></author>
      <author><first>Yuanbin</first> <last>Wu</last></author>
      <pages>888–893</pages>
      <url hash="ea185a8b">S17-2152</url>
      <doi>10.18653/v1/S17-2152</doi>
      <abstract>This paper describes our systems submitted to the Fine-Grained Sentiment Analysis on Financial Microblogs and News task (i.e., Task 5) in SemEval-2017. This task includes two subtasks in microblogs and news headline domain respectively. To settle this problem, we extract four types of effective features, including linguistic features, sentiment lexicon features, domain-specific features and word embedding features. Then we employ these features to construct models by using ensemble regression algorithms. Our submissions rank 1st and rank 5th in subtask 1 and subtask 2 respectively.</abstract>
    </paper>
    <paper id="153">
      <title><fixed-case>IITPB</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Sentiment Prediction in Financial Text</title>
      <author><first>Abhishek</first> <last>Kumar</last></author>
      <author><first>Abhishek</first> <last>Sethi</last></author>
      <author><first>Md Shad</first> <last>Akhtar</last></author>
      <author><first>Asif</first> <last>Ekbal</last></author>
      <author><first>Chris</first> <last>Biemann</last></author>
      <author><first>Pushpak</first> <last>Bhattacharyya</last></author>
      <pages>894–898</pages>
      <url hash="41de481e">S17-2153</url>
      <doi>10.18653/v1/S17-2153</doi>
      <abstract>This paper reports team IITPB’s participation in the SemEval 2017 Task 5 on ‘Fine-grained sentiment analysis on financial microblogs and news’. We developed 2 systems for the two tracks. One system was based on an ensemble of Support Vector Classifier and Logistic Regression. This system relied on Distributional Thesaurus (DT), word embeddings and lexicon features to predict a floating sentiment value between -1 and +1. The other system was based on Support Vector Regression using word embeddings, lexicon features, and PMI scores as features. The system was ranked 5th in track 1 and 8th in track 2.</abstract>
    </paper>
    <paper id="154">
      <title><fixed-case>IITP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: An Ensemble of Deep Learning and Feature Based Models for Financial Sentiment Analysis</title>
      <author><first>Deepanway</first> <last>Ghosal</last></author>
      <author><first>Shobhit</first> <last>Bhatnagar</last></author>
      <author><first>Md Shad</first> <last>Akhtar</last></author>
      <author><first>Asif</first> <last>Ekbal</last></author>
      <author><first>Pushpak</first> <last>Bhattacharyya</last></author>
      <pages>899–903</pages>
      <url hash="fc9e6ea2">S17-2154</url>
      <doi>10.18653/v1/S17-2154</doi>
      <abstract>In this paper we propose an ensemble based model which combines state of the art deep learning sentiment analysis algorithms like Convolution Neural Network (CNN) and Long Short Term Memory (LSTM) along with feature based models to identify optimistic or pessimistic sentiments associated with companies and stocks in financial texts. We build our system to participate in a competition organized by Semantic Evaluation 2017 International Workshop. We combined predictions from various models using an artificial neural network to determine the opinion towards an entity in (a) Microblog Messages and (b) News Headlines data. Our models achieved a cosine similarity score of 0.751 and 0.697 for the above two tracks giving us the rank of 2nd and 7th best team respectively.</abstract>
    </paper>
    <paper id="155">
      <title><fixed-case>FEUP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 5: Predicting Sentiment Polarity and Intensity with Financial Word Embeddings</title>
      <author><first>Pedro</first> <last>Saleiro</last></author>
      <author><first>Eduarda</first> <last>Mendes Rodrigues</last></author>
      <author><first>Carlos</first> <last>Soares</last></author>
      <author><first>Eugénio</first> <last>Oliveira</last></author>
      <pages>904–908</pages>
      <url hash="71470cc4">S17-2155</url>
      <doi>10.18653/v1/S17-2155</doi>
      <abstract>This paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News. The task consisted in predicting a real continuous variable from -1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies/stocks mentioned in short texts. We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-of-embeddings. We used an external collection of tweets and news headlines mentioning companies/stocks from S&amp;P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities. The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1 - Microblogs and 0.68 in sub-task 5.2 - News Headlines.</abstract>
    </paper>
    <paper id="156">
      <title><fixed-case>UIT</fixed-case>-<fixed-case>DANGNT</fixed-case>-<fixed-case>CLNLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 9: Building Scientific Concept Fixing Patterns for Improving <fixed-case>CAMR</fixed-case></title>
      <author><first>Khoa</first> <last>Nguyen</last></author>
      <author><first>Dang</first> <last>Nguyen</last></author>
      <pages>909–913</pages>
      <url hash="c269f6c8">S17-2156</url>
      <doi>10.18653/v1/S17-2156</doi>
      <abstract>This paper describes the improvements that we have applied on CAMR baseline parser (Wang et al., 2016) at Task 8 of SemEval-2016. Our objective is to increase the performance of CAMR when parsing sentences from scientific articles, especially articles of biology domain more accurately. To achieve this goal, we built two wrapper layers for CAMR. The first layer, which covers the input data, will normalize, add necessary information to the input sentences to make the input dependency parser and the aligner better handle reference citations, scientific figures, formulas, etc. The second layer, which covers the output data, will modify and standardize output data based on a list of scientific concept fixing patterns. This will help CAMR better handle biological concepts which are not in the training dataset. Finally, after applying our approach, CAMR has scored 0.65 F-score on the test set of Biomedical training data and 0.61 F-score on the official blind test dataset.</abstract>
    </paper>
    <paper id="157">
      <title><fixed-case>O</fixed-case>xford at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 9: Neural <fixed-case>AMR</fixed-case> Parsing with Pointer-Augmented Attention</title>
      <author><first>Jan</first> <last>Buys</last></author>
      <author><first>Phil</first> <last>Blunsom</last></author>
      <pages>914–919</pages>
      <url hash="f742db82">S17-2157</url>
      <doi>10.18653/v1/S17-2157</doi>
      <abstract>We present a neural encoder-decoder AMR parser that extends an attention-based model by predicting the alignment between graph nodes and sentence tokens explicitly with a pointer mechanism. Candidate lemmas are predicted as a pre-processing step so that the lemmas of lexical concepts, as well as constant strings, are factored out of the graph linearization and recovered through the predicted alignments. The approach does not rely on syntactic parses or extensive external resources. Our parser obtained 59% Smatch on the SemEval test set.</abstract>
    </paper>
    <paper id="158">
      <title><fixed-case>FORG</fixed-case>e at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 9: Deep sentence generation based on a sequence of graph transducers</title>
      <author><first>Simon</first> <last>Mille</last></author>
      <author><first>Roberto</first> <last>Carlini</last></author>
      <author><first>Alicia</first> <last>Burga</last></author>
      <author><first>Leo</first> <last>Wanner</last></author>
      <pages>920–923</pages>
      <url hash="48868d56">S17-2158</url>
      <doi>10.18653/v1/S17-2158</doi>
      <abstract>We present the contribution of Universitat Pompeu Fabra’s NLP group to the SemEval Task 9.2 (AMR-to-English Generation). The proposed generation pipeline comprises: (i) a series of rule-based graph-transducers for the syntacticization of the input graphs and the resolution of morphological agreements, and (ii) an off-the-shelf statistical linearization component.</abstract>
    </paper>
    <paper id="159">
      <title><fixed-case>RIGOTRIO</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 9: Combining Machine Learning and Grammar Engineering for <fixed-case>AMR</fixed-case> Parsing and Generation</title>
      <author><first>Normunds</first> <last>Gruzitis</last></author>
      <author><first>Didzis</first> <last>Gosko</last></author>
      <author><first>Guntis</first> <last>Barzdins</last></author>
      <pages>924–928</pages>
      <url hash="3112dd59">S17-2159</url>
      <doi>10.18653/v1/S17-2159</doi>
      <abstract>By addressing both text-to-AMR parsing and AMR-to-text generation, SemEval-2017 Task 9 established AMR as a powerful semantic interlingua. We strengthen the interlingual aspect of AMR by applying the multilingual Grammatical Framework (GF) for AMR-to-text generation. Our current rule-based GF approach completely covered only 12.3% of the test AMRs, therefore we combined it with state-of-the-art JAMR Generator to see if the combination increases or decreases the overall performance. The combined system achieved the automatic BLEU score of 18.82 and the human Trueskill score of 107.2, to be compared to the plain JAMR Generator results. As for AMR parsing, we added NER extensions to our SemEval-2016 general-domain AMR parser to handle the biomedical genre, rich in organic compound names, achieving Smatch F1=54.0%.</abstract>
    </paper>
    <paper id="160">
      <title>The Meaning Factory at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 9: Producing <fixed-case>AMR</fixed-case>s with Neural Semantic Parsing</title>
      <author><first>Rik</first> <last>van Noord</last></author>
      <author><first>Johan</first> <last>Bos</last></author>
      <pages>929–933</pages>
      <url hash="12b0a1db">S17-2160</url>
      <doi>10.18653/v1/S17-2160</doi>
      <abstract>We evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval-2017 shared task on semantic parsing for AMRs. With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline character-level model. Although we improve on previous character-based neural semantic parsing models, the overall accuracy is still lower than a state-of-the-art AMR parser. An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance.</abstract>
    </paper>
    <paper id="161">
      <title><fixed-case>PKU</fixed-case>_<fixed-case>ICL</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Keyphrase Extraction with Model Ensemble and External Knowledge</title>
      <author><first>Liang</first> <last>Wang</last></author>
      <author><first>Sujian</first> <last>Li</last></author>
      <pages>934–937</pages>
      <url hash="123ec515">S17-2161</url>
      <doi>10.18653/v1/S17-2161</doi>
      <abstract>This paper presents a system that participated in SemEval 2017 Task 10 (subtask A and subtask B): Extracting Keyphrases and Relations from Scientific Publications (Augenstein et al., 2017). Our proposed approach utilizes external knowledge to enrich feature representation of candidate keyphrase, including Wikipedia, IEEE taxonomy and pre-trained word embeddings etc. Ensemble of unsupervised models, random forest and linear models are used for candidate keyphrase ranking and keyphrase type classification. Our system achieves the 3rd place in subtask A and 4th place in subtask B.</abstract>
    </paper>
    <paper id="162">
      <title><fixed-case>NTNU</fixed-case>-1@<fixed-case>S</fixed-case>cience<fixed-case>IE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Identifying and Labelling Keyphrases with Conditional Random Fields</title>
      <author><first>Erwin</first> <last>Marsi</last></author>
      <author><first>Utpal Kumar</first> <last>Sikdar</last></author>
      <author><first>Cristina</first> <last>Marco</last></author>
      <author><first>Biswanath</first> <last>Barik</last></author>
      <author><first>Rune</first> <last>Sætre</last></author>
      <pages>938–941</pages>
      <url hash="d9873bf8">S17-2162</url>
      <doi>10.18653/v1/S17-2162</doi>
      <abstract>We present NTNU’s systems for Task A (prediction of keyphrases) and Task B (labelling as Material, Process or Task) at SemEval 2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications (Augenstein et al., 2017). Our approach relies on supervised machine learning using Conditional Random Fields. Our system yields a micro F-score of 0.34 for Tasks A and B combined on the test data. For Task C (relation extraction), we relied on an independently developed system described in (Barik and Marsi, 2017). For the full Scenario 1 (including relations), our approach reaches a micro F-score of 0.33 (5th place). Here we describe our systems, report results and discuss errors.</abstract>
    </paper>
    <paper id="163">
      <title><fixed-case>EELECTION</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Ensemble of n<fixed-case>E</fixed-case>ural Learners for k<fixed-case>E</fixed-case>yphrase <fixed-case>C</fixed-case>lassifica<fixed-case>TION</fixed-case></title>
      <author><first>Steffen</first> <last>Eger</last></author>
      <author><first>Erik-Lân</first> <last>Do Dinh</last></author>
      <author><first>Ilia</first> <last>Kuznetsov</last></author>
      <author><first>Masoud</first> <last>Kiaeeha</last></author>
      <author><first>Iryna</first> <last>Gurevych</last></author>
      <pages>942–946</pages>
      <url hash="bf768535">S17-2163</url>
      <doi>10.18653/v1/S17-2163</doi>
      <abstract>This paper describes our approach to the SemEval 2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications, specifically to Subtask (B): Classification of identified keyphrases. We explored three different deep learning approaches: a character-level convolutional neural network (CNN), a stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM. From these approaches, we created an ensemble of differently hyper-parameterized systems, achieving a micro-<tex-math>F_1</tex-math>-score of 0.63 on the test data. Our approach ranks 2nd (score of 1st placed system: 0.64) out of four according to this official score. However, we erroneously trained 2 out of 3 neural nets (the stacker and the CNN) on only roughly 15% of the full data, namely, the original development set. When trained on the full data (training+development), our ensemble has a micro-<tex-math>F_{1}</tex-math>-score of 0.69. Our code is available from <url>https://github.com/UKPLab/semeval2017-scienceie</url>. </abstract>
    </paper>
    <paper id="164">
      <title><fixed-case>LABDA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Extracting Keyphrases from Scientific Publications by combining the <fixed-case>BANNER</fixed-case> tool and the <fixed-case>UMLS</fixed-case> Semantic Network</title>
      <author><first>Isabel</first> <last>Segura-Bedmar</last></author>
      <author><first>Cristóbal</first> <last>Colón-Ruiz</last></author>
      <author><first>Paloma</first> <last>Martínez</last></author>
      <pages>947–950</pages>
      <url hash="265b9d52">S17-2164</url>
      <doi>10.18653/v1/S17-2164</doi>
      <abstract>This paper describes the system presented by the LABDA group at SemEval 2017 Task 10 ScienceIE, specifically for the subtasks of identification and classification of keyphrases from scientific articles. For the task of identification, we use the BANNER tool, a named entity recognition system, which is based on conditional random fields (CRF) and has obtained successful results in the biomedical domain. To classify keyphrases, we study the UMLS semantic network and propose a possible linking between the keyphrase types and the UMLS semantic groups. Based on this semantic linking, we create a dictionary for each keyphrase type. Then, a feature indicating if a token is found in one of these dictionaries is incorporated to feature set used by the BANNER tool. The final results on the test dataset show that our system still needs to be improved, but the conditional random fields and, consequently, the BANNER system can be used as a first approximation to identify and classify keyphrases.</abstract>
    </paper>
    <paper id="165">
      <title>The <fixed-case>NTNU</fixed-case> System at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications Using Multiple Conditional Random Fields</title>
      <author><first>Lung-Hao</first> <last>Lee</last></author>
      <author><first>Kuei-Ching</first> <last>Lee</last></author>
      <author><first>Yuen-Hsien</first> <last>Tseng</last></author>
      <pages>951–955</pages>
      <url hash="17363b85">S17-2165</url>
      <doi>10.18653/v1/S17-2165</doi>
      <abstract>This study describes the design of the NTNU system for the ScienceIE task at the SemEval 2017 workshop. We use self-defined feature templates and multiple conditional random fields with extracted features to identify keyphrases along with categorized labels and their relations from scientific publications. A total of 16 teams participated in evaluation scenario 1 (subtasks A, B, and C), with only 7 teams competing in all sub-tasks. Our best micro-averaging F1 across the three subtasks is 0.23, ranking in the middle among all 16 submissions.</abstract>
    </paper>
    <paper id="166">
      <title><fixed-case>M</fixed-case>ayo<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2017 Task 10: Word Embedding Distance Pattern for Keyphrase Classification in Scientific Publications</title>
      <author><first>Sijia</first> <last>Liu</last></author>
      <author><first>Feichen</first> <last>Shen</last></author>
      <author><first>Vipin</first> <last>Chaudhary</last></author>
      <author><first>Hongfang</first> <last>Liu</last></author>
      <pages>956–960</pages>
      <url hash="5ce93e38">S17-2166</url>
      <doi>10.18653/v1/S17-2166</doi>
      <abstract>In this paper, we present MayoNLP’s results from the participation in the ScienceIE share task at SemEval 2017. We focused on the keyphrase classification task (Subtask B). We explored semantic similarities and patterns of keyphrases in scientific publications using pre-trained word embedding models. Word Embedding Distance Pattern, which uses the head noun word embedding to generate distance patterns based on labeled keyphrases, is proposed as an incremental feature set to enhance the conventional Named Entity Recognition feature sets. Support vector machine is used as the supervised classifier for keyphrase classification. Our system achieved an overall F1 score of 0.67 for keyphrase classification and 0.64 for keyphrase classification and relation detection.</abstract>
    </paper>
    <paper id="167">
      <title>Know-Center at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Sequence Classification with the <fixed-case>CODE</fixed-case> Annotator</title>
      <author><first>Roman</first> <last>Kern</last></author>
      <author><first>Stefan</first> <last>Falk</last></author>
      <author><first>Andi</first> <last>Rexha</last></author>
      <pages>961–964</pages>
      <url hash="a684d04d">S17-2167</url>
      <doi>10.18653/v1/S17-2167</doi>
      <abstract>This paper describes our participation in SemEval-2017 Task 10. We competed in Subtask 1 and 2 which consist respectively in identifying all the key phrases in scientific publications and label them with one of the three categories: Task, Process, and Material. These scientific publications are selected from Computer Science, Material Sciences, and Physics domains. We followed a supervised approach for both subtasks by using a sequential classifier (CRF - Conditional Random Fields). For generating our solution we used a web-based application implemented in the EU-funded research project, named CODE. Our system achieved an F1 score of 0.39 for the Subtask 1 and 0.28 for the Subtask 2.</abstract>
    </paper>
    <paper id="168">
      <title><fixed-case>NTNU</fixed-case>-2 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Identifying Synonym and Hyponym Relations among Keyphrases in Scientific Documents</title>
      <author><first>Biswanath</first> <last>Barik</last></author>
      <author><first>Erwin</first> <last>Marsi</last></author>
      <pages>965–968</pages>
      <url hash="a31925c2">S17-2168</url>
      <doi>10.18653/v1/S17-2168</doi>
      <abstract>This paper presents our relation extraction system for subtask C of SemEval-2017 Task 10: ScienceIE. Assuming that the keyphrases are already annotated in the input data, our work explores a wide range of linguistic features, applies various feature selection techniques, optimizes the hyper parameters and class weights and experiments with different problem formulations (single classification model vs individual classifiers for each keyphrase type, single-step classifier vs pipeline classifier for hyponym relations). Performance of five popular classification algorithms are evaluated for each problem formulation along with feature selection. The best setting achieved an F1 score of 71.0% for synonym and 30.0% for hyponym relation on the test data.</abstract>
    </paper>
    <paper id="169">
      <title><fixed-case>LABDA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Relation Classification between keyphrases via Convolutional Neural Network</title>
      <author><first>Víctor</first> <last>Suárez-Paniagua</last></author>
      <author><first>Isabel</first> <last>Segura-Bedmar</last></author>
      <author><first>Paloma</first> <last>Martínez</last></author>
      <pages>969–972</pages>
      <url hash="22eb314b">S17-2169</url>
      <doi>10.18653/v1/S17-2169</doi>
      <abstract>In this paper, we describe our participation at the subtask of extraction of relationships between two identified keyphrases. This task can be very helpful in improving search engines for scientific articles. Our approach is based on the use of a convolutional neural network (CNN) trained on the training dataset. This deep learning model has already achieved successful results for the extraction relationships between named entities. Thus, our hypothesis is that this model can be also applied to extract relations between keyphrases. The official results of the task show that our architecture obtained an F1-score of 0.38% for Keyphrases Relation Classification. This performance is lower than the expected due to the generic preprocessing phase and the basic configuration of the CNN model, more complex architectures are proposed as future work to increase the classification rate.</abstract>
    </paper>
    <paper id="170">
      <title><fixed-case>WING</fixed-case>-<fixed-case>NUS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Keyphrase Extraction and Classification as Joint Sequence Labeling</title>
      <author><first>Animesh</first> <last>Prasad</last></author>
      <author><first>Min-Yen</first> <last>Kan</last></author>
      <pages>973–977</pages>
      <url hash="0032030f">S17-2170</url>
      <doi>10.18653/v1/S17-2170</doi>
      <abstract>We describe an end-to-end pipeline processing approach for SemEval 2017’s Task 10 to extract keyphrases and their relations from scientific publications. We jointly identify and classify keyphrases by modeling the subtasks as sequential labeling. Our system utilizes standard, surface-level features along with the adjacent word features, and performs conditional decoding on whole text to extract keyphrases. We focus only on the identification and typing of keyphrases (Subtasks A and B, together referred as extraction), but provide an end-to-end system inclusive of keyphrase relation identification (Subtask C) for completeness. Our top performing configuration achieves an <tex-math>F_1</tex-math> of 0.27 for the end-to-end keyphrase extraction and relation identification scenario on the final test data, and compares on par to other top ranked systems for keyphrase extraction. Our system outperforms other techniques that do not employ global decoding and hence do not account for dependencies between keyphrases. We believe this is crucial for keyphrase classification in the given context of scientific document mining. </abstract>
    </paper>
    <paper id="171">
      <title><fixed-case>MIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Relation Extraction with Convolutional Neural Networks</title>
      <author><first>Ji Young</first> <last>Lee</last></author>
      <author><first>Franck</first> <last>Dernoncourt</last></author>
      <author><first>Peter</first> <last>Szolovits</last></author>
      <pages>978–984</pages>
      <url hash="0cf35b5d">S17-2171</url>
      <doi>10.18653/v1/S17-2171</doi>
      <abstract>Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts. Artificial neural networks have recently been explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C).</abstract>
    </paper>
    <paper id="172">
      <title><fixed-case>TTI</fixed-case>-<fixed-case>COIN</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Investigating Embeddings for End-to-End Relation Extraction from Scientific Papers</title>
      <author><first>Tomoki</first> <last>Tsujimura</last></author>
      <author><first>Makoto</first> <last>Miwa</last></author>
      <author><first>Yutaka</first> <last>Sasaki</last></author>
      <pages>985–989</pages>
      <url hash="290046cc">S17-2172</url>
      <doi>10.18653/v1/S17-2172</doi>
      <abstract>This paper describes our TTI-COIN system that participated in SemEval-2017 Task 10. We investigated appropriate embeddings to adapt a neural end-to-end entity and relation extraction system LSTM-ER to this task. We participated in the full task setting of the entity segmentation, entity classification and relation classification (scenario 1) and the setting of relation classification only (scenario 3). The system was directly applied to the scenario 1 without modifying the codes thanks to its generality and flexibility. Our evaluation results show that the choice of appropriate pre-trained embeddings affected the performance significantly. With the best embeddings, our system was ranked third in the scenario 1 with the micro F1 score of 0.38. We also confirm that our system can produce the micro F1 score of 0.48 for the scenario 3 on the test data, and this score is close to the score of the 3rd ranked system in the task.</abstract>
    </paper>
    <paper id="173">
      <title><fixed-case>SZTE</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: A High Precision Sequence Model for Keyphrase Extraction Utilizing Sparse Coding for Feature Generation</title>
      <author><first>Gábor</first> <last>Berend</last></author>
      <pages>990–994</pages>
      <url hash="dfd07fa3">S17-2173</url>
      <doi>10.18653/v1/S17-2173</doi>
      <abstract>In this paper we introduce our system participating at the 2017 SemEval shared task on keyphrase extraction from scientific documents. We aimed at the creation of a keyphrase extraction approach which relies on as little external resources as possible. Without applying any hand-crafted external resources, and only utilizing a transformed version of word embeddings trained at Wikipedia, our proposed system manages to perform among the best participating systems in terms of precision.</abstract>
    </paper>
    <paper id="174">
      <title><fixed-case>LIPN</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 10: Filtering Candidate Keyphrases from Scientific Publications with Part-of-Speech Tag Sequences to Train a Sequence Labeling Model</title>
      <author><first>Simon David</first> <last>Hernandez</last></author>
      <author><first>Davide</first> <last>Buscaldi</last></author>
      <author><first>Thierry</first> <last>Charnois</last></author>
      <pages>995–999</pages>
      <url hash="ad502833">S17-2174</url>
      <doi>10.18653/v1/S17-2174</doi>
      <abstract>This paper describes the system used by the team LIPN in SemEval 2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications. The team participated in Scenario 1, that includes three subtasks, Identification of keyphrases (Subtask A), Classification of identified keyphrases (Subtask B) and Extraction of relationships between two identified keyphrases (Subtask C). The presented system was mainly focused on the use of part-of-speech tag sequences to filter candidate keyphrases for Subtask A. Subtasks A and B were addressed as a sequence labeling problem using Conditional Random Fields (CRFs) and even though Subtask C was out of the scope of this approach, one rule was included to identify synonyms.</abstract>
    </paper>
    <paper id="175">
      <title><fixed-case>EUDAMU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 11: Action Ranking and Type Matching for End-User Development</title>
      <author><first>Marek</first> <last>Kubis</last></author>
      <author><first>Paweł</first> <last>Skórzewski</last></author>
      <author><first>Tomasz</first> <last>Ziętkiewicz</last></author>
      <pages>1000–1004</pages>
      <url hash="5dbfdea7">S17-2175</url>
      <doi>10.18653/v1/S17-2175</doi>
      <abstract>The paper describes a system for end-user development using natural language. Our approach uses a ranking model to identify the actions to be executed followed by reference and parameter matching models to select parameter values that should be set for the given commands. We discuss the results of evaluation and possible improvements for future work.</abstract>
    </paper>
    <paper id="176">
      <title>Hitachi at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 12: System for temporal information extraction from clinical notes</title>
      <author><first>Sarath</first> <last>P R</last></author>
      <author><first>Manikandan</first> <last>R</last></author>
      <author><first>Yoshiki</first> <last>Niwa</last></author>
      <pages>1005–1009</pages>
      <url hash="0337987a">S17-2176</url>
      <doi>10.18653/v1/S17-2176</doi>
      <abstract>This paper describes the system developed for the task of temporal information extraction from clinical narratives in the context of the 2017 Clinical TempEval challenge. Clinical TempEval 2017 addressed the problem of temporal reasoning in the clinical domain by providing annotated clinical notes, pathology and radiology reports in line with Clinical TempEval challenges 2015/16, across two different evaluation phases focusing on cross domain adaptation. Our team focused on subtasks involving extractions of temporal spans and relations for which the developed systems showed average F-score of 0.45 and 0.47 across the two phases of evaluations.</abstract>
    </paper>
    <paper id="177">
      <title><fixed-case>NTU</fixed-case>-1 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 12: Detection and classification of temporal events in clinical data with domain adaptation</title>
      <author><first>Po-Yu</first> <last>Huang</last></author>
      <author><first>Hen-Hsen</first> <last>Huang</last></author>
      <author><first>Yu-Wun</first> <last>Wang</last></author>
      <author><first>Ching</first> <last>Huang</last></author>
      <author><first>Hsin-Hsi</first> <last>Chen</last></author>
      <pages>1010–1013</pages>
      <url hash="a0a73d95">S17-2177</url>
      <doi>10.18653/v1/S17-2177</doi>
      <abstract>This study proposes a system to participate in the Clinical TempEval 2017 shared task, a part of the SemEval 2017 Tasks. Domain adaptation was the main challenge this year. We took part in the supervised domain adaption where data of 591 records of colon cancer patients and 30 records of brain cancer patients from Mayo clinic were given and we are asked to analyze the records from brain cancer patients. Based on the THYME corpus released by the organizer of Clinical TempEval, we propose a framework that automatically analyzes clinical temporal events in a fine-grained level. Support vector machine (SVM) and conditional random field (CRF) were implemented in our system for different subtasks, including detecting clinical relevant events and time expression, determining their attributes, and identifying their relations with each other within the document. The results showed the capability of domain adaptation of our system.</abstract>
    </paper>
    <paper id="178">
      <title><fixed-case>XJNLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 12: Clinical temporal information ex-traction with a Hybrid Model</title>
      <author><first>Yu</first> <last>Long</last></author>
      <author><first>Zhijing</first> <last>Li</last></author>
      <author><first>Xuan</first> <last>Wang</last></author>
      <author><first>Chen</first> <last>Li</last></author>
      <pages>1014–1018</pages>
      <url hash="5e54ac13">S17-2178</url>
      <doi>10.18653/v1/S17-2178</doi>
      <revision id="1" href="S17-2178v1" hash="fd45a75d"/>
      <revision id="2" href="S17-2178v2" hash="5e54ac13">No description of the changes were recorded.</revision>
      <abstract>Temporality is crucial in understanding the course of clinical events from a patient’s electronic health recordsand temporal processing is becoming more and more important for improving access to content.SemEval 2017 Task 12 (Clinical TempEval) addressed this challenge using the THYME corpus, a corpus of clinical narratives annotated with a schema based on TimeML2 guidelines. We developed and evaluated approaches for: extraction of temporal expressions (TIMEX3) and EVENTs; EVENT attributes; document-time relations. Our approach is a hybrid model which is based on rule based methods, semi-supervised learning, and semantic features with addition of manually crafted rules.</abstract>
    </paper>
    <paper id="179">
      <title><fixed-case>ULISBOA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 12: Extraction and classification of temporal expressions and events</title>
      <author><first>Andre</first> <last>Lamurias</last></author>
      <author><first>Diana</first> <last>Sousa</last></author>
      <author><first>Sofia</first> <last>Pereira</last></author>
      <author><first>Luka</first> <last>Clarke</last></author>
      <author><first>Francisco M.</first> <last>Couto</last></author>
      <pages>1019–1023</pages>
      <url hash="fc6eaffe">S17-2179</url>
      <doi>10.18653/v1/S17-2179</doi>
      <abstract>This paper presents our approach to participate in the SemEval 2017 Task 12: Clinical TempEval challenge, specifically in the event and time expressions span and attribute identification subtasks (ES, EA, TS, TA). Our approach consisted in training Conditional Random Fields (CRF) classifiers using the provided annotations, and in creating manually curated rules to classify the attributes of each event and time expression. We used a set of common features for the event and time CRF classifiers, and a set of features specific to each type of entity, based on domain knowledge. Training only on the source domain data, our best F-scores were 0.683 and 0.485 for event and time span identification subtasks. When adding target domain annotations to the training data, the best F-scores obtained were 0.729 and 0.554, for the same subtasks. We obtained the second highest F-score of the challenge on the event polarity subtask (0.708). The source code of our system, Clinical Timeline Annotation (CiTA), is available at <url>https://github.com/lasigeBioTM/CiTA</url>.
    </abstract>
    </paper>
    <paper id="180">
      <title><fixed-case>GUIR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 12: A Framework for Cross-Domain Clinical Temporal Information Extraction</title>
      <author><first>Sean</first> <last>MacAvaney</last></author>
      <author><first>Arman</first> <last>Cohan</last></author>
      <author><first>Nazli</first> <last>Goharian</last></author>
      <pages>1024–1029</pages>
      <url hash="fa4afa8f">S17-2180</url>
      <doi>10.18653/v1/S17-2180</doi>
      <abstract>Clinical TempEval 2017 (SemEval 2017 Task 12) addresses the task of cross-domain temporal extraction from clinical text. We present a system for this task that uses supervised learning for the extraction of temporal expression and event spans with corresponding attributes and narrative container relations. Approaches include conditional random fields and decision tree ensembles, using lexical, syntactic, semantic, distributional, and rule-based features. Our system received best or second best scores in TIMEX3 span, EVENT span, and CONTAINS relation extraction.</abstract>
    </paper>
    <paper id="181">
      <title><fixed-case>KUL</fixed-case>euven-<fixed-case>LIIR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2017 Task 12: Cross-Domain Temporal Information Extraction from Clinical Records</title>
      <author><first>Artuur</first> <last>Leeuwenberg</last></author>
      <author><first>Marie-Francine</first> <last>Moens</last></author>
      <pages>1030–1034</pages>
      <url hash="fa5ec7e5">S17-2181</url>
      <doi>10.18653/v1/S17-2181</doi>
      <abstract>In this paper, we describe the system of the KULeuven-LIIR submission for Clinical TempEval 2017. We participated in all six subtasks, using a combination of Support Vector Machines (SVM) for event and temporal expression detection, and a structured perceptron for extracting temporal relations. Moreover, we present and analyze the results from our submissions, and verify the effectiveness of several system components. Our system performed above average for all subtasks in both phases.</abstract>
    </paper>
  </volume>
</collection>
