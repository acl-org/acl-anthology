<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.wnu">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the The 7th Workshop on Narrative Understanding</booktitle>
      <editor><first>Elizabeth</first><last>Clark</last></editor>
      <editor><first>Yash Kumar</first><last>Lal</last></editor>
      <editor><first>Snigdha</first><last>Chaturvedi</last></editor>
      <editor><first>Mohit</first><last>Iyyer</last></editor>
      <editor><first>Anneliese</first><last>Brei</last></editor>
      <editor><first>Ashutosh</first><last>Modi</last></editor>
      <editor><first>Khyathi Raghavi</first><last>Chandu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>May</month>
      <year>2025</year>
      <url hash="d53c0480">2025.wnu-1</url>
      <venue>wnu</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-247-3</isbn>
    </meta>
    <frontmatter>
      <url hash="6fd888f6">2025.wnu-1.0</url>
      <bibkey>wnu-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>N</fixed-case>arra<fixed-case>D</fixed-case>etect: An annotated dataset for the task of narrative detection</title>
      <author><first>Andrew</first><last>Piper</last><affiliation>McGill University</affiliation></author>
      <author><first>Sunyam</first><last>Bagga</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <pages>1-7</pages>
      <abstract>Narrative detection is an important task across diverse research domains where storytelling serves as a key mechanism for explaining human beliefs and behavior. However, the task faces three significant challenges: (1) inter-narrative heterogeneity, or the variation in narrative communication across social contexts; (2) intra-narrative heterogeneity, or the dynamic variation of narrative features within a single text over time; and (3) the lack of theoretical consensus regarding the concept of narrative. This paper introduces the NarraDetect dataset, a comprehensive resource comprising over 13,000 passages from 18 distinct narrative and non-narrative genres. Through a manually annotated subset of ~400 passages, we also introduce a novel theoretical framework for annotating for a scalar concept of “narrativity.” Our findings indicate that while supervised models outperform large language models (LLMs) on this dataset, LLMs exhibit stronger generalization and alignment with the scalar concept of narrativity.</abstract>
      <url hash="53e03008">2025.wnu-1.1</url>
      <bibkey>piper-bagga-2025-narradetect</bibkey>
    </paper>
    <paper id="3">
      <title>On the Transferability of Causal Knowledge for Language Models</title>
      <author><first>Gourab</first><last>Dey</last><affiliation>Stony Brook University, New York</affiliation></author>
      <author><first>Yash Kumar</first><last>Lal</last><affiliation>Stony Brook University</affiliation></author>
      <pages>8-14</pages>
      <abstract>Language understanding includes identifying logical connections between events in a discourse, such as news and instructional text. We study the transferability of causal knowledge across these two domains by analyzing the extent to which understanding preconditions in narratives such as news articles can help models reason about cooking recipes, and vice-versa. Our experiments show that using instructions to pretrain small models on one domain before similarly finetuning it on the other shows a slight improvement over just finetuning it. We also find that finetuning the models on a mix of both types of data is better (~3-7%) for understanding causal relations in instructional text. While we find that the improvements do not translate to larger or already instruction tuned models, our analysis highlights the aspects of a plan that are better captured through the interoperability of causal knowledge.</abstract>
      <url hash="4bac5009">2025.wnu-1.3</url>
      <bibkey>dey-lal-2025-transferability</bibkey>
    </paper>
    <paper id="6">
      <title>Finding Common Patterns in Domestic Violence Stories Posted on <fixed-case>R</fixed-case>eddit</title>
      <author><first>Mohammad</first><last>Shokri</last><affiliation>Graduate Center, City University of New York</affiliation></author>
      <author><first>Emily</first><last>Klapper</last><affiliation>Hunter College (CUNY)</affiliation></author>
      <author><first>Jason</first><last>Shan</last><affiliation>Hunter College (CUNY)</affiliation></author>
      <author><first>Sarah Ita</first><last>Levitan</last><affiliation>Hunter College (CUNY)</affiliation></author>
      <pages>15-25</pages>
      <abstract>Domestic violence survivors often share their experiences in online spaces, offering valuable insights into common abuse patterns. This study analyzes a dataset of personal narratives about domestic violence from Reddit, focusing on event extraction and topic modeling to uncover recurring themes. We evaluate GPT-4 and LLaMA-3.1 for extracting key sentences, finding that GPT-4 exhibits higher precision, while LLaMA-3.1 achieves better recall. Using LLM-based topic assignment, we identify dominant themes such as psychological aggression, financial abuse, and physical assault which align with previously published psychology findings. A co-occurrence and PMI analysis further reveals the interdependencies among different abuse types, emphasizing the multifaceted nature of domestic violence. Our findings provide a structured approach to analyzing survivor narratives, with implications for social support systems and policy interventions.</abstract>
      <url hash="b57fc9bd">2025.wnu-1.6</url>
      <bibkey>shokri-etal-2025-finding</bibkey>
    </paper>
    <paper id="7">
      <title>A Theoretical Framework for Evaluating Narrative Surprise in Large Language Models</title>
      <author><first>Annaliese</first><last>Bissell</last><affiliation>McGill University</affiliation></author>
      <author><first>Ella</first><last>Paulin</last><affiliation>McGill University</affiliation></author>
      <author><first>Andrew</first><last>Piper</last><affiliation>McGill University</affiliation></author>
      <pages>26-35</pages>
      <abstract>Narrative surprise is a core element of storytelling for engaging audiences, and yet it remains underexplored in the context of large language models (LLMs) and narrative generation. While surprise arises from events that deviate from expectations while maintaining retrospective coherence, current computational approaches lack comprehensive frameworks to evaluate this phenomenon. This paper presents a novel framework for assessing narrative surprise, drawing on psychological theories of narrative comprehension and surprise intensity. We operationalize six criteria—initiatoriness, immutability violation, predictability, post-dictability, importance, and valence—to measure narrative surprise in story endings. Our study evaluates 120 story endings, generated by both human authors and LLMs, across 30 mystery narratives. Through a ranked-choice voting methodology, we identify significant correlations between reader preferences and four of the six criteria. Results underscore the continuing advantage of human-authored endings in achieving compelling narrative surprise, while also revealing significant progress in LLM-generated narratives.</abstract>
      <url hash="525b9a58">2025.wnu-1.7</url>
      <bibkey>bissell-etal-2025-theoretical</bibkey>
    </paper>
    <paper id="10">
      <title>Beyond <fixed-case>LLM</fixed-case>s A Linguistic Approach to Causal Graph Generation from Narrative Texts</title>
      <author><first>Zehan</first><last>Li</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ruhua</first><last>Pan</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Xinyu</first><last>Pi</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>36-51</pages>
      <url hash="e8a09cd1">2025.wnu-1.10</url>
      <bibkey>li-etal-2025-beyond-llms</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>CHATTER</fixed-case>: A character-attribution dataset for narrative understanding</title>
      <author><first>Sabyasachee</first><last>Baruah</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Shrikanth</first><last>Narayanan</last><affiliation>University of Southern California</affiliation></author>
      <pages>52-63</pages>
      <abstract>Computational narrative understanding studies the identification, description, and interaction of the elements of a narrative: characters, attributes, events, and relations.Narrative research has given considerable attention to defining and classifying character types.However, these character-type taxonomies do not generalize well because they are small, too simple, or specific to a domain.We require robust and reliable benchmarks to test whether narrative models truly understand the nuances of the character’s development in the story.Our work addresses this by curating the CHATTER dataset that labels whether a character portrays some attribute for 88124 character-attribute pairs, encompassing 2998 characters, 12967 attributes and 660 movies.We validate a subset of CHATTER, called CHATTEREVAL, using human annotations to serve as an evaluation benchmark for the character attribution task in movie scripts.CHATTEREVAL also assesses narrative understanding and the long-context modeling capacity of language models.</abstract>
      <url hash="54bb0b29">2025.wnu-1.11</url>
      <bibkey>baruah-narayanan-2025-chatter</bibkey>
    </paper>
    <paper id="12">
      <title>Tracking Evolving Relationship Between Characters in Books in the Era of Large Language Models</title>
      <author><first>Abhilasha</first><last>Sancheti</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Rachel</first><last>Rudinger</last><affiliation>University of Maryland</affiliation></author>
      <pages>64-82</pages>
      <abstract>This work aims to assess the zero-shot social reasoning capabilities of LLMs by proposing various strategies based on the granularity of information used to track the fine-grained evolution in the relationship between characters in a book. Without gold annotations, we thoroughly analyze the agreements between predictions from multiple LLMs and manually examine their consensus at a local and global level via the task of trope prediction. Our findings reveal low-to-moderate agreement among LLMs and humans, reflecting the complexity of the task. Analysis shows that LLMs are sensitive to subtle contextual changes and often rely on surface-level cues. Humans, too, may interpret relationships differently, leading to disagreements in annotations.</abstract>
      <url hash="c4f67bd9">2025.wnu-1.12</url>
      <bibkey>sancheti-rudinger-2025-tracking</bibkey>
    </paper>
    <paper id="16">
      <title>Narrative Studio: Visual narrative exploration using <fixed-case>LLM</fixed-case>s and <fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Tree Search</title>
      <author><first>Parsa</first><last>Ghaffari</last><affiliation>Independent researcher</affiliation></author>
      <author><first>Chris</first><last>Hokamp</last><affiliation>Aylien Ltd.</affiliation></author>
      <pages>83-96</pages>
      <abstract>Interactive storytelling benefits from planning and exploring multiple “what if” scenarios. Modern LLMs are useful tools for ideation and exploration, but current chat-based user interfaces restrict users to a single linear flow. To address this limitation, we propose Narrative Studio – a novel in-browser narrative exploration environment featuring a tree-like interface that allows branching exploration from user-defined points in a story. Each branch is extended via iterative LLM inference guided by system and user-defined prompts. Additionally, we employ Monte Carlo Tree Search (MCTS) to automatically expand promising narrative paths based on user-specified criteria, enabling more diverse and robust story development. We also allow users to enhance narrative coherence by grounding the generated text in a graph that represents the actors and environment of the story.</abstract>
      <url hash="af22a37b">2025.wnu-1.16</url>
      <bibkey>ghaffari-hokamp-2025-narrative</bibkey>
    </paper>
    <paper id="17">
      <title>Speaker Identification and Dataset Construction Using <fixed-case>LLM</fixed-case>s: A Case Study on <fixed-case>J</fixed-case>apanese Narratives</title>
      <author><first>Seiji</first><last>Gobara</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>97-119</pages>
      <abstract>Speaker identification in narrative analysis is a challenging task due to complex dialogues, diverse utterance patterns, and ambiguous character references. Cosly and time-intensive manual annotation limits the scalability of high-quality dataset creation.This study demonstrates a cost-efficient approach of constructing speaker identification datasets by combining small-scale manual annotation with LLM-based labeling. A subset of data is manually annotated and is used to guide LLM predictions with a few-shot approach followed by refinement through minimal human corrections. Our results show that LLMs achieve approximately 90% accuracy on challenging narratives, such as the “Three Kingdoms” dataset, underscoring the importance of targeted human corrections. This approach proves effective for constructing scalable and cost-efficient datasets for Japanese and complex narratives.</abstract>
      <url hash="97782848">2025.wnu-1.17</url>
      <bibkey>gobara-etal-2025-speaker</bibkey>
    </paper>
  </volume>
</collection>
