<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.acl">
  <volume id="srw" ingest-date="2022-05-07">
    <meta>
      <booktitle>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</booktitle>
      <editor><first>Samuel</first><last>Louvan</last></editor>
      <editor><first>Andrea</first><last>Madotto</last></editor>
      <editor><first>Brielen</first><last>Madureira</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="a7e3d8d3">2022.acl-srw</url>
    </meta>
    <frontmatter>
      <url hash="a7e3d8d3">2022.acl-srw.0</url>
      <bibkey>acl-2022-association</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Evaluating zero-shot transfers and multilingual models for dependency parsing and <fixed-case>POS</fixed-case> tagging within the low-resource language family Tupían</title>
      <author><first>Frederic</first><last>Blum</last></author>
      <pages>1-9</pages>
      <abstract>This work presents two experiments with the goal of replicating the transferability of dependency parsers and POS taggers trained on closely related languages within the low-resource language family Tupían. The experiments include both zero-shot settings as well as multilingual models. Previous studies have found that even a comparably small treebank from a closely related language will improve sequence labelling considerably in such cases. Results from both POS tagging and dependency parsing confirm previous evidence that the closer the phylogenetic relation between two languages, the better the predictions for sequence labelling tasks get. In many cases, the results are improved if multiple languages from the same family are combined. This suggests that in addition to leveraging similarity between two related languages, the incorporation of multiple languages of the same family might lead to better results in transfer learning for NLP applications.</abstract>
      <url hash="cdab8487">2022.acl-srw.1</url>
      <bibkey>blum-2022-evaluating</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>RFBFN</fixed-case>: A Relation-First Blank Filling Network for Joint Relational Triple Extraction</title>
      <author><first>Zhe</first><last>Li</last></author>
      <author><first>Luoyi</first><last>Fu</last></author>
      <author><first>Xinbing</first><last>Wang</last></author>
      <author><first>Haisong</first><last>Zhang</last></author>
      <author><first>Chenghu</first><last>Zhou</last></author>
      <pages>10-20</pages>
      <abstract>Joint relational triple extraction from unstructured text is an important task in information extraction. However, most existing works either ignore the semantic information of relations or predict subjects and objects sequentially. To address the issues, we introduce a new blank filling paradigm for the task, and propose a relation-first blank filling network (RFBFN). Specifically, we first detect potential relations maintained in the text to aid the following entity pair extraction. Then, we transform relations into relation templates with blanks which contain the fine-grained semantic representation of the relations. Finally, corresponding subjects and objects are extracted simultaneously by filling the blanks. We evaluate the proposed model on public benchmark datasets. Experimental results show our model outperforms current state-of-the-art methods. The source code of our work is available at: https://github.com/lizhe2016/RFBFN.</abstract>
      <url hash="63be2c47">2022.acl-srw.2</url>
      <bibkey>li-etal-2022-rfbfn</bibkey>
    </paper>
    <paper id="3">
      <title>Building a Dialogue Corpus Annotated with Expressed and Experienced Emotions</title>
      <author><first>Tatsuya</first><last>Ide</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <pages>21-30</pages>
      <abstract>In communication, a human would recognize the emotion of an interlocutor and respond with an appropriate emotion, such as empathy and comfort. Toward developing a dialogue system with such a human-like ability, we propose a method to build a dialogue corpus annotated with two kinds of emotions. We collect dialogues from Twitter and annotate each utterance with the emotion that a speaker put into the utterance (expressed emotion) and the emotion that a listener felt after listening to the utterance (experienced emotion). We built a dialogue corpus in Japanese using this method, and its statistical analysis revealed the differences between expressed and experienced emotions. We conducted experiments on recognition of the two kinds of emotions. The experimental results indicated the difficulty in recognizing experienced emotions and the effectiveness of multi-task learning of the two kinds of emotions. We hope that the constructed corpus will facilitate the study on emotion recognition in a dialogue and emotion-aware dialogue response generation.</abstract>
      <url hash="b71fb36e">2022.acl-srw.3</url>
      <bibkey>ide-kawahara-2022-building</bibkey>
    </paper>
    <paper id="4">
      <title>Darkness can not drive out darkness: Investigating Bias in Hate <fixed-case>S</fixed-case>peech<fixed-case>D</fixed-case>etection Models</title>
      <author><first>Fatma</first><last>Elsafoury</last></author>
      <pages>31-43</pages>
      <abstract>It has become crucial to develop tools for automated hate speech and abuse detection. These tools would help to stop the bullies and the haters and provide a safer environment for individuals especially from marginalized groups to freely express themselves. However, recent research shows that machine learning models are biased and they might make the right decisions for the wrong reasons. In this thesis, I set out to understand the performance of hate speech and abuse detection models and the different biases that could influence them. I show that hate speech and abuse detection models are not only subject to social bias but also to other types of bias that have not been explored before. Finally, I investigate the causal effect of the social and intersectional bias on the performance and unfairness of hate speech detection models.</abstract>
      <url hash="01d28bf8">2022.acl-srw.4</url>
      <bibkey>elsafoury-2022-darkness</bibkey>
    </paper>
    <paper id="5">
      <title>Ethical Considerations for Low-resourced Machine Translation</title>
      <author><first>Levon</first><last>Haroutunian</last></author>
      <pages>44-54</pages>
      <abstract>This paper considers some ethical implications of machine translation for low-resourced languages. I use Armenian as a case study and investigate specific needs for and concerns arising from the creation and deployment of improved machine translation between English and Armenian. To do this, I conduct stakeholder interviews and construct Value Scenarios (Nathan et al., 2007) from the themes that emerge. These scenarios illustrate some of the potential harms that low-resourced language communities may face due to the deployment of improved machine translation systems. Based on these scenarios, I recommend 1) collaborating with stakeholders in order to create more useful and reliable machine translation tools, and 2) determining which other forms of language technology should be developed alongside efforts to improve machine translation in order to mitigate harms rendered to vulnerable language communities. Both of these goals require treating low-resourced machine translation as a language-specific, rather than language-agnostic, task.</abstract>
      <url hash="c2acb4a2">2022.acl-srw.5</url>
      <bibkey>haroutunian-2022-ethical</bibkey>
    </paper>
    <paper id="6">
      <title>Integrating Question Rewrites in Conversational Question Answering: A Reinforcement Learning Approach</title>
      <author><first>Etsuko</first><last>Ishii</last></author>
      <author><first>Bryan</first><last>Wilie</last></author>
      <author><first>Yan</first><last>Xu</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>55-66</pages>
      <abstract>Resolving dependencies among dialogue history is one of the main obstacles in the research on conversational question answering (QA). The conversational question rewrites (QR) task has been shown to be effective to solve this problem by reformulating questions in a self-contained form. However, QR datasets are limited and existing methods tend to depend on the assumption of the existence of corresponding QR datasets for every CQA dataset.This paper proposes a reinforcement learning approach that integrates QR and CQA tasks without corresponding labeled QR datasets. We train a QR model based on the reward signal obtained from the CQA, and the experimental results show that our approach can bring improvement over the pipeline approaches.</abstract>
      <url hash="ff8f9c0e">2022.acl-srw.6</url>
      <bibkey>ishii-etal-2022-integrating</bibkey>
    </paper>
    <paper id="7">
      <title>What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification</title>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>67-83</pages>
      <abstract>Over the last five years, research on Relation Extraction (RE) witnessed extensive progress with many new dataset releases. At the same time, setup clarity has decreased, contributing to increased difficulty of reliable empirical evaluation (Taillé et al., 2020). In this paper, we provide a comprehensive survey of RE datasets, and revisit the task definition and its adoption by the community. We find that cross-dataset and cross-domain setups are particularly lacking. We present an empirical study on scientific Relation Classification across two datasets. Despite large data overlap, our analysis reveals substantial discrepancies in annotation. Annotation discrepancies strongly impact Relation Classification performance, explaining large drops in cross-dataset evaluations. Variation within further sub-domains exists but impacts Relation Classification only to limited degrees. Overall, our study calls for more rigour in reporting setups in RE and evaluation across multiple test sets.</abstract>
      <url hash="b64e8ad5">2022.acl-srw.7</url>
      <bibkey>bassignana-plank-2022-mean</bibkey>
    </paper>
    <paper id="8">
      <title>Logical Inference for Counting on Semi-structured Tables</title>
      <author><first>Tomoya</first><last>Kurosawa</last></author>
      <author><first>Hitomi</first><last>Yanaka</last></author>
      <pages>84-96</pages>
      <abstract>Recently, the Natural Language Inference (NLI) task has been studied for semi-structured tables that do not have a strict format. Although neural approaches have achieved high performance in various types of NLI, including NLI between semi-structured tables and texts, they still have difficulty in performing a numerical type of inference, such as counting. To handle a numerical type of inference, we propose a logical inference system for reasoning between semi-structured tables and texts. We use logical representations as meaning representations for tables and texts and use model checking to handle a numerical type of inference between texts and tables. To evaluate the extent to which our system can perform inference with numerical comparatives, we make an evaluation protocol that focuses on numerical understanding between semi-structured tables and texts in English. We show that our system can more robustly perform inference between tables and texts that requires numerical understanding compared with current neural approaches.</abstract>
      <url hash="99494799">2022.acl-srw.8</url>
      <bibkey>kurosawa-yanaka-2022-logical</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>GNN</fixed-case>er: Reducing Overlapping in Span-based <fixed-case>NER</fixed-case> Using Graph Neural Networks</title>
      <author><first>Urchade</first><last>Zaratiana</last></author>
      <author><first>Nadi</first><last>Tomeh</last></author>
      <author><first>Pierre</first><last>Holat</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <pages>97-103</pages>
      <abstract>There are two main paradigms for Named Entity Recognition (NER): sequence labelling and span classification. Sequence labelling aims to assign a label to each word in an input text using, for example, BIO (Begin, Inside and Outside) tagging, while span classification involves enumerating all possible spans in a text and classifying them into their labels. In contrast to sequence labelling, unconstrained span-based methods tend to assign entity labels to overlapping spans, which is generally undesirable, especially for NER tasks without nested entities. Accordingly, we propose GNNer, a framework that uses Graph Neural Networks to enrich the span representation to reduce the number of overlapping spans during prediction. Our approach reduces the number of overlapping spans compared to strong baseline while maintaining competitive metric performance. Code is available at <url>https://github.com/urchade/GNNer</url>.</abstract>
      <url hash="d0597faa">2022.acl-srw.9</url>
      <bibkey>zaratiana-etal-2022-gnner</bibkey>
    </paper>
    <paper id="10">
      <title>Compositional Semantics and Inference System for Temporal Order based on <fixed-case>J</fixed-case>apanese <fixed-case>CCG</fixed-case></title>
      <author><first>Tomoki</first><last>Sugimoto</last></author>
      <author><first>Hitomi</first><last>Yanaka</last></author>
      <pages>104-114</pages>
      <abstract>Natural Language Inference (NLI) is the task of determining whether a premise entails a hypothesis. NLI with temporal order is a challenging task because tense and aspect are complex linguistic phenomena involving interactions with temporal adverbs and temporal connectives. To tackle this, temporal and aspectual inference has been analyzed in various ways in the field of formal semantics. However, a Japanese NLI system for temporal order based on the analysis of formal semantics has not been sufficiently developed. We present a logic-based NLI system that considers temporal order in Japanese based on compositional semantics via Combinatory Categorial Grammar (CCG) syntactic analysis. Our system performs inference involving temporal order by using axioms for temporal relations and automated theorem provers. We evaluate our system by experimenting with Japanese NLI datasets that involve temporal order. We show that our system outperforms previous logic-based systems as well as current deep learning-based models.</abstract>
      <url hash="0775c922">2022.acl-srw.10</url>
      <bibkey>sugimoto-yanaka-2022-compositional</bibkey>
    </paper>
    <paper id="11">
      <title>Combine to Describe: Evaluating Compositional Generalization in Image Captioning</title>
      <author><first>George</first><last>Pantazopoulos</last></author>
      <author><first>Alessandro</first><last>Suglia</last></author>
      <author><first>Arash</first><last>Eshghi</last></author>
      <pages>115-131</pages>
      <abstract>Compositionality – the ability to combine simpler concepts to understand &amp; generate arbitrarily more complex conceptual structures – has long been thought to be the cornerstone of human language capacity. With the recent, notable success of neural models in various NLP tasks, attention has now naturally turned to the compositional capacity of these models. In this paper, we study the compositional generalization properties of image captioning models. We perform a set experiments under controlled conditions using model and data ablations, each designed to benchmark a particular facet of compositional generalization: systematicity is the ability of a model to create novel combinations of concepts out of those observed during training, productivity is here operationalised as the capacity of a model to extend its predictions beyond the length distribution it has observed during training, and substitutivity is concerned with the robustness of the model against synonym substitutions. While previous work has focused primarily on systematicity, here we provide a more in-depth analysis of the strengths and weaknesses of state of the art captioning models. Our findings demonstrate that the models we study here do not compositionally generalize in terms of systematicity and productivity, however, they are robust to some degree to synonym substitutions</abstract>
      <url hash="fb15edb4">2022.acl-srw.11</url>
      <bibkey>pantazopoulos-etal-2022-combine</bibkey>
    </paper>
    <paper id="12">
      <title>Towards Unification of Discourse Annotation Frameworks</title>
      <author><first>Yingxue</first><last>Fu</last></author>
      <pages>132-142</pages>
      <abstract>Discourse information is difficult to represent and annotate. Among the major frameworks for annotating discourse information, RST, PDTB and SDRT are widely discussed and used, each having its own theoretical foundation and focus. Corpora annotated under different frameworks vary considerably. To make better use of the existing discourse corpora and achieve the possible synergy of different frameworks, it is worthwhile to investigate the systematic relations between different frameworks and devise methods of unifying the frameworks. Although the issue of framework unification has been a topic of discussion for a long time, there is currently no comprehensive approach which considers unifying both discourse structure and discourse relations and evaluates the unified framework intrinsically and extrinsically. We plan to use automatic means for the unification task and evaluate the result with structural complexity and downstream tasks. We will also explore the application of the unified framework in multi-task learning and graphical models.</abstract>
      <url hash="dbfb1d47">2022.acl-srw.12</url>
      <bibkey>fu-2022-towards</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>AMR</fixed-case> Alignment for Morphologically-rich and Pro-drop Languages</title>
      <author><first>K. Elif</first><last>Oral</last></author>
      <author><first>Gülşen</first><last>Eryiğit</last></author>
      <pages>143-152</pages>
      <abstract>Alignment between concepts in an abstract meaning representation (AMR) graph and the words within a sentence is one of the important stages of AMR parsing. Although there exist high performing AMR aligners for English, unfortunately, these are not well suited for many languages where many concepts appear from morpho-semantic elements.For the first time in the literature, this paper presents an AMR aligner tailored for morphologically-rich and pro-drop languages by experimenting on the Turkish language being a prominent example of this language group.Our aligner focuses on the meaning considering the rich Turkish morphology and aligns AMR concepts that emerge from morphemes using a tree traversal approach without additional resources or rules. We evaluate our aligner over a manually annotated gold data set in terms of precision, recall and F1 score. Our aligner outperforms the Turkish adaptations of the previously proposed aligners for English and Portuguese by an F1 score of 0.87 and provides a relative error reduction of up to 76%.</abstract>
      <url hash="94b71440">2022.acl-srw.13</url>
      <bibkey>oral-eryigit-2022-amr</bibkey>
    </paper>
    <paper id="14">
      <title>Sketching a Linguistically-Driven Reasoning Dialog Model for Social Talk</title>
      <author><first>Alex</first><last>Lưu</last></author>
      <pages>153-170</pages>
      <abstract>The capability of holding social talk (or casual conversation) and making sense of conversational content requires context-sensitive natural language understanding and reasoning, which cannot be handled efficiently by the current popular open-domain dialog systems and chatbots. Heavily relying on corpus-based machine learning techniques to encode and decode context-sensitive meanings, these systems focus on fitting a particular training dataset, but not tracking what is actually happening in a conversation, and therefore easily derail in a new context. This work sketches out a more linguistically-informed architecture to handle social talk in English, in which corpus-based methods form the backbone of the relatively context-insensitive components (e.g. part-of-speech tagging, approximation of lexical meaning and constituent chunking), while symbolic modeling is used for reasoning out the context-sensitive components, which do not have any consistent mapping to linguistic forms. All components are fitted into a Bayesian game-theoretic model to address the interactive and rational aspects of conversation.</abstract>
      <url hash="278f461b">2022.acl-srw.14</url>
      <bibkey>luu-2022-sketching</bibkey>
    </paper>
    <paper id="15">
      <title>Scoping natural language processing in <fixed-case>I</fixed-case>ndonesian and <fixed-case>M</fixed-case>alay for education applications</title>
      <author><first>Zara</first><last>Maxwelll-Smith</last></author>
      <author><first>Michelle</first><last>Kohler</last></author>
      <author><first>Hanna</first><last>Suominen</last></author>
      <pages>171-228</pages>
      <abstract>Indonesian and Malay are underrepresented in the development of natural language processing (NLP) technologies and available resources are difficult to find. A clear picture of existing work can invigorate and inform how researchers conceptualise worthwhile projects. Using an education sector project to motivate the study, we conducted a wide-ranging overview of Indonesian and Malay human language technologies and corpus work. We charted 657 included studies according to Hirschberg and Manning’s 2015 description of NLP, concluding that the field was dominated by exploratory corpus work, machine reading of text gathered from the Internet, and sentiment analysis. In this paper, we identify most published authors and research hubs, and make a number of recommendations to encourage future collaboration and efficiency within NLP in Indonesian and Malay.</abstract>
      <url hash="3232c55c">2022.acl-srw.15</url>
      <bibkey>maxwelll-smith-etal-2022-scoping</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>E</fixed-case>nglish-<fixed-case>M</fixed-case>alay Cross-Lingual Embedding Alignment using Bilingual Lexicon Augmentation</title>
      <author><first>Ying Hao</first><last>Lim</last></author>
      <author><first>Jasy Suet Yan</first><last>Liew</last></author>
      <pages>229-238</pages>
      <abstract>As high-quality Malay language resources are still a scarcity, cross lingual word embeddings make it possible for richer English resources to be leveraged for downstream Malay text classification tasks. This paper focuses on creating an English-Malay cross-lingual word embeddings using embedding alignment by exploiting existing language resources. We augmented the training bilingual lexicons using machine translation with the goal to improve the alignment precision of our cross-lingual word embeddings. We investigated the quality of the current state-of-the-art English-Malay bilingual lexicon and worked on improving its quality using Google Translate. We also examined the effect of Malay word coverage on the quality of cross-lingual word embeddings. Experimental results with a precision up till 28.17% show that the alignment precision of the cross-lingual word embeddings would inevitably degrade after 1-NN but a better seed lexicon and cleaner nearest neighbours can reduce the number of word pairs required to achieve satisfactory performance. As the English and Malay monolingual embeddings are pre-trained on informal language corpora, our proposed English-Malay embeddings alignment approach is also able to map non-standard Malay translations in the English nearest neighbours.</abstract>
      <url hash="8494c205">2022.acl-srw.16</url>
      <bibkey>lim-liew-2022-english</bibkey>
    </paper>
    <paper id="17">
      <title>Towards Detecting Political Bias in <fixed-case>H</fixed-case>indi News Articles</title>
      <author><first>Samyak</first><last>Agrawal</last></author>
      <author><first>Kshitij</first><last>Gupta</last></author>
      <author><first>Devansh</first><last>Gautam</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>239-244</pages>
      <abstract>Political propaganda in recent times has been amplified by media news portals through biased reporting, creating untruthful narratives on serious issues causing misinformed public opinions with interests of siding and helping a particular political party. This issue proposes a challenging NLP task of detecting political bias in news articles.We propose a transformer-based transfer learning method to fine-tune the pre-trained network on our data for this bias detection. As the required dataset for this particular task was not available, we created our dataset comprising 1388 Hindi news articles and their headlines from various Hindi news media outlets. We marked them on whether they are biased towards, against, or neutral to BJP, a political party, and the current ruling party at the centre in India.</abstract>
      <url hash="87d7d2a6">2022.acl-srw.17</url>
      <bibkey>agrawal-etal-2022-towards</bibkey>
    </paper>
    <paper id="18">
      <title>Restricted or Not: A General Training Framework for Neural Machine Translation</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>245-251</pages>
      <abstract>Restricted machine translation incorporates human prior knowledge into translation. It restricts the flexibility of the translation to satisfy the demands of translation in specific scenarios. Existing work typically imposes constraints on beam search decoding. Although this can satisfy the requirements overall, it usually requires a larger beam size and far longer decoding time than unrestricted translation, which limits the concurrent processing ability of the translation model in deployment, and thus its practicality. In this paper, we propose a general training framework that allows a model to simultaneously support both unrestricted and restricted translation by adopting an additional auxiliary training process without constraining the decoding process. This maintains the benefits of restricted translation but greatly reduces the extra time overhead of constrained decoding, thus improving its practicality. The effectiveness of our proposed training framework is demonstrated by experiments on both original (WAT21 En<tex-math>\leftrightarrow</tex-math>Ja) and simulated (WMT14 En<tex-math>\rightarrow</tex-math>De and En<tex-math>\rightarrow</tex-math>Fr) restricted translation benchmarks.</abstract>
      <url hash="bdd635b7">2022.acl-srw.18</url>
      <bibkey>li-etal-2022-restricted</bibkey>
    </paper>
    <paper id="19">
      <title>What do Models Learn From Training on More Than Text? Measuring Visual Commonsense Knowledge</title>
      <author><first>Lovisa</first><last>Hagström</last></author>
      <author><first>Richard</first><last>Johansson</last></author>
      <pages>252-261</pages>
      <abstract>There are limitations in learning language from text alone. Therefore, recent focus has been on developing multimodal models. However, few benchmarks exist that can measure what language models learn about language from multimodal training. We hypothesize that training on a visual modality should improve on the visual commonsense knowledge in language models. Therefore, we introduce two evaluation tasks for measuring visual commonsense knowledge in language models (code publicly available at: github.com/lovhag/measure-visual-commonsense-knowledge) and use them to evaluate different multimodal models and unimodal baselines. Primarily, we find that the visual commonsense knowledge is not significantly different between the multimodal models and unimodal baseline models trained on visual text data.</abstract>
      <url hash="806f797a">2022.acl-srw.19</url>
      <bibkey>hagstrom-johansson-2022-models</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>T</fixed-case>elugu<fixed-case>NER</fixed-case>: Leveraging Multi-Domain Named Entity Recognition with Deep Transformers</title>
      <author><first>Suma Reddy</first><last>Duggenpudi</last></author>
      <author><first>Subba Reddy</first><last>Oota</last></author>
      <author><first>Mounika</first><last>Marreddy</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>262-272</pages>
      <abstract>Named Entity Recognition (NER) is a successful and well-researched problem in English due to the availability of resources. The transformer models, specifically the masked-language models (MLM), have shown remarkable performance in NER during recent times. With growing data in different online platforms, there is a need for NER in other languages too. NER remains to be underexplored in Indian languages due to the lack of resources and tools. Our contributions in this paper include (i) Two annotated NER datasets for the Telugu language in multiple domains: Newswire Dataset (ND) and Medical Dataset (MD), and we combined ND and MD to form Combined Dataset (CD) (ii) Comparison of the finetuned Telugu pretrained transformer models (BERT-Te, RoBERTa-Te, and ELECTRA-Te) with other baseline models (CRF, LSTM-CRF, and BiLSTM-CRF) (iii) Further investigation of the performance of Telugu pretrained transformer models against the multilingual models mBERT, XLM-R, and IndicBERT. We find that pretrained Telugu language models (BERT-Te and RoBERTa) outperform the existing pretrained multilingual and baseline models in NER. On a large dataset (CD) of 38,363 sentences, the BERT-Te achieves a high F1-score of 0.80 (entity-level) and 0.75 (token-level). Further, these pretrained Telugu models have shown state-of-the-art performance on various existing Telugu NER datasets. We open-source our dataset, pretrained models, and code.</abstract>
      <url hash="68037939">2022.acl-srw.20</url>
      <bibkey>duggenpudi-etal-2022-teluguner</bibkey>
    </paper>
    <paper id="21">
      <title>Using Neural Machine Translation Methods for Sign Language Translation</title>
      <author><first>Galina</first><last>Angelova</last></author>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>273-284</pages>
      <abstract>We examine methods and techniques, proven to be helpful for the text-to-text translation of spoken languages in the context of gloss-to-text translation systems, where the glosses are the written representation of the signs. We present one of the first works that include experiments on both parallel corpora of the German Sign Language (PHOENIX14T and the Public DGS Corpus). We experiment with two NMT architectures with optimization of their hyperparameters, several tokenization methods and two data augmentation techniques (back-translation and paraphrasing). Through our investigation we achieve a substantial improvement of 5.0 and 2.2 BLEU scores for the models trained on the two corpora respectively. Our RNN models outperform our Transformer models, and the segmentation method we achieve best results with is BPE, whereas back-translation and paraphrasing lead to minor but not significant improvements.</abstract>
      <url hash="20d75fa4">2022.acl-srw.21</url>
      <bibkey>angelova-etal-2022-using</bibkey>
    </paper>
    <paper id="22">
      <title>Flexible Visual Grounding</title>
      <author><first>Yongmin</first><last>Kim</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>285-299</pages>
      <abstract>Existing visual grounding datasets are artificially made, where every query regarding an entity must be able to be grounded to a corresponding image region, i.e., answerable. However, in real-world multimedia data such as news articles and social media, many entities in the text cannot be grounded to the image, i.e., unanswerable, due to the fact that the text is unnecessarily directly describing the accompanying image. A robust visual grounding model should be able to flexibly deal with both answerable and unanswerable visual grounding. To study this flexible visual grounding problem, we construct a pseudo dataset and a social media dataset including both answerable and unanswerable queries. In order to handle unanswerable visual grounding, we propose a novel method by adding a pseudo image region corresponding to a query that cannot be grounded. The model is then trained to ground to ground-truth regions for answerable queries and pseudo regions for unanswerable queries. In our experiments, we show that our model can flexibly process both answerable and unanswerable queries with high accuracy on our datasets.</abstract>
      <url hash="56f4c355">2022.acl-srw.22</url>
      <bibkey>kim-etal-2022-flexible</bibkey>
    </paper>
    <paper id="23">
      <title>A large-scale computational study of content preservation measures for text style transfer and paraphrase generation</title>
      <author><first>Nikolay</first><last>Babakov</last></author>
      <author><first>David</first><last>Dale</last></author>
      <author><first>Varvara</first><last>Logacheva</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>300-321</pages>
      <abstract>Text style transfer and paraphrasing of texts are actively growing areas of NLP, dozens of methods for solving these tasks have been recently introduced. In both tasks, the system is supposed to generate a text which should be semantically similar to the input text. Therefore, these tasks are dependent on methods of measuring textual semantic similarity. However, it is still unclear which measures are the best to automatically evaluate content preservation between original and generated text. According to our observations, many researchers still use BLEU-like measures, while there exist more advanced measures including neural-based that significantly outperform classic approaches. The current problem is the lack of a thorough evaluation of the available measures. We close this gap by conducting a large-scale computational study by comparing 57 measures based on different principles on 19 annotated datasets. We show that measures based on cross-encoder models outperform alternative approaches in almost all cases.We also introduce the Mutual Implication Score (MIS), a measure that uses the idea of paraphrasing as a bidirectional entailment and outperforms all other measures on the paraphrase detection task and performs on par with the best measures in the text style transfer task.</abstract>
      <url hash="b367e784">2022.acl-srw.23</url>
      <bibkey>babakov-etal-2022-large</bibkey>
    </paper>
    <paper id="24">
      <title>Explicit Object Relation Alignment for Vision and Language Navigation</title>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>322-331</pages>
      <abstract>In this paper, we investigate the problem of vision and language navigation. To solve this problem, grounding the landmarks and spatial relations in the textual instructions into visual modality is important. We propose a neural agent named Explicit Object Relation Alignment Agent (EXOR),to explicitly align the spatial information in both instruction and the visual environment, including landmarks and spatial relationships between the agent and landmarks.Empirically, our proposed method surpasses the baseline by a large margin on the R2R dataset. We provide a comprehensive analysis to show our model’s spatial reasoning ability and explainability.</abstract>
      <url hash="2769bf94">2022.acl-srw.24</url>
      <bibkey>zhang-kordjamshidi-2022-explicit</bibkey>
    </paper>
    <paper id="25">
      <title>Mining Logical Event Schemas From Pre-Trained Language Models</title>
      <author><first>Lane</first><last>Lawley</last></author>
      <author><first>Lenhart</first><last>Schubert</last></author>
      <pages>332-345</pages>
      <abstract>We present NESL (the Neuro-Episodic Schema Learner), an event schema learning system that combines large language models, FrameNet parsing, a powerful logical representation of language, and a set of simple behavioral schemas meant to bootstrap the learning process. In lieu of a pre-made corpus of stories, our dataset is a continuous feed of “situation samples” from a pre-trained language model, which are then parsed into FrameNet frames, mapped into simple behavioral schemas, and combined and generalized into complex, hierarchical schemas for a variety of everyday scenarios. We show that careful sampling from the language model can help emphasize stereotypical properties of situations and de-emphasize irrelevant details, and that the resulting schemas specify situations more comprehensively than those learned by other systems.</abstract>
      <url hash="5952286a">2022.acl-srw.25</url>
      <bibkey>lawley-schubert-2022-mining</bibkey>
    </paper>
    <paper id="26">
      <title>Exploring Cross-lingual Text Detoxification with Large Multilingual Language Models.</title>
      <author><first>Daniil</first><last>Moskovskiy</last></author>
      <author><first>Daryna</first><last>Dementieva</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>346-354</pages>
      <abstract>Detoxification is a task of generating text in polite style while preserving meaning and fluency of the original toxic text. Existing detoxification methods are monolingual i.e. designed to work in one exact language. This work investigates multilingual and cross-lingual detoxification and the behavior of large multilingual models in this setting. Unlike previous works we aim to make large language models able to perform detoxification without direct fine-tuning in a given language. Experiments show that multilingual models are capable of performing multilingual style transfer. However, tested state-of-the-art models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is currently inevitable and motivating the need of further research in this direction.</abstract>
      <url hash="90f73db6">2022.acl-srw.26</url>
      <bibkey>moskovskiy-etal-2022-exploring</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>MEKER</fixed-case>: Memory Efficient Knowledge Embedding Representation for Link Prediction and Question Answering</title>
      <author><first>Viktoriia</first><last>Chekalina</last></author>
      <author><first>Anton</first><last>Razzhigaev</last></author>
      <author><first>Albert</first><last>Sayapin</last></author>
      <author><first>Evgeny</first><last>Frolov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>355-365</pages>
      <abstract>Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG embedding contains concise data used in NLP tasks requiring implicit information about the real world. Furthermore, the size of KGs that may be useful in actual NLP assignments is enormous, and creating embedding over it has memory cost issues. We represent KG as a 3rd-order binary tensor and move beyond the standard CP decomposition (CITATION) by using a data-specific generalized version of it (CITATION). The generalization of the standard CP-ALS algorithm allows obtaining optimization gradients without a backpropagation mechanism. It reduces the memory needed in training while providing computational benefits. We propose a MEKER, a memory-efficient KG embedding model, which yields SOTA-comparable performance on link prediction tasks and KG-based Question Answering.</abstract>
      <url hash="9a2fd942">2022.acl-srw.27</url>
      <bibkey>chekalina-etal-2022-meker</bibkey>
    </paper>
    <paper id="28">
      <title>Discourse on <fixed-case>ASR</fixed-case> Measurement: Introducing the <fixed-case>ARPOCA</fixed-case> Assessment Tool</title>
      <author><first>Megan</first><last>Merz</last></author>
      <author><first>Olga</first><last>Scrivner</last></author>
      <pages>366-372</pages>
      <abstract>Automatic speech recognition (ASR) has evolved from a pipeline architecture with pronunciation dictionaries, phonetic features and language models to the end-to-end systems performing a direct translation from a raw waveform into a word sequence. With the increase in accuracy and the availability of pre-trained models, the ASR systems are now omnipresent in our daily applications. On the other hand, the models’ interpretability and their computational cost have become more challenging, particularly when dealing with less-common languages or identifying regional variations of speakers. This research proposal will follow a four-stage process: 1) Proving an overview of acoustic features and feature extraction algorithms; 2) Exploring current ASR models, tools, and performance assessment techniques; 3) Aligning features with interpretable phonetic transcripts; and 4) Designing a prototype ARPOCA to increase awareness of regional language variation and improve models feedback by developing a semi-automatic acoustic features extraction using PRAAT in conjunction with phonetic transcription.</abstract>
      <url hash="b5deadad">2022.acl-srw.28</url>
      <bibkey>merz-scrivner-2022-discourse</bibkey>
    </paper>
    <paper id="29">
      <title>Pretrained Knowledge Base Embeddings for improved Sentential Relation Extraction</title>
      <author><first>Andrea</first><last>Papaluca</last></author>
      <author><first>Daniel</first><last>Krefl</last></author>
      <author><first>Hanna</first><last>Suominen</last></author>
      <author><first>Artem</first><last>Lenskiy</last></author>
      <pages>373-382</pages>
      <abstract>In this work we put forward to combine pretrained knowledge base graph embeddings with transformer based language models to improve performance on the sentential Relation Extraction task in natural language processing. Our proposed model is based on a simple variation of existing models to incorporate off-task pretrained graph embeddings with an on-task finetuned BERT encoder. We perform a detailed statistical evaluation of the model on standard datasets. We provide evidence that the added graph embeddings improve the performance, making such a simple approach competitive with the state-of-the-art models that perform explicit on-task training of the graph embeddings. Furthermore, we ob- serve for the underlying BERT model an interesting power-law scaling behavior between the variance of the F1 score obtained for a relation class and its support in terms of training examples.</abstract>
      <url hash="d1712ba5">2022.acl-srw.29</url>
      <bibkey>papaluca-etal-2022-pretrained</bibkey>
    </paper>
    <paper id="30">
      <title>Improving Cross-domain, Cross-lingual and Multi-modal Deception Detection</title>
      <author><first>Subhadarshi</first><last>Panda</last></author>
      <author><first>Sarah Ita</first><last>Levitan</last></author>
      <pages>383-390</pages>
      <abstract>With the increase of deception and misinformation especially in social media, it has become crucial to be able to develop machine learning methods to automatically identify deceptive language. In this proposal, we identify key challenges underlying deception detection in cross-domain, cross-lingual and multi-modal settings. To improve cross-domain deception classification, we propose to use inter-domain distance to identify a suitable source domain for a given target domain. We propose to study the efficacy of multilingual classification models vs translation for cross-lingual deception classification. Finally, we propose to better understand multi-modal deception detection and explore methods to weight and combine information from multiple modalities to improve multi-modal deception classification.</abstract>
      <url hash="60ec8626">2022.acl-srw.30</url>
      <bibkey>panda-levitan-2022-improving</bibkey>
    </paper>
    <paper id="31">
      <title>Automatic Generation of Distractors for Fill-in-the-Blank Exercises with Round-Trip Neural Machine Translation</title>
      <author><first>Subhadarshi</first><last>Panda</last></author>
      <author><first>Frank</first><last>Palma Gomez</last></author>
      <author><first>Michael</first><last>Flor</last></author>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <pages>391-401</pages>
      <abstract>In a fill-in-the-blank exercise, a student is presented with a carrier sentence with one word hidden, and a multiple-choice list that includes the correct answer and several inappropriate options, called distractors. We propose to automatically generate distractors using round-trip neural machine translation: the carrier sentence is translated from English into another (pivot) language and back, and distractors are produced by aligning the original sentence and its round-trip translation. We show that using hundreds of translations for a given sentence allows us to generate a rich set of challenging distractors. Further, using multiple pivot languages produces a diverse set of candidates. The distractors are evaluated against a real corpus of cloze exercises and checked manually for validity. We demonstrate that the proposed method significantly outperforms two strong baselines.</abstract>
      <url hash="f5f038b9">2022.acl-srw.31</url>
      <bibkey>panda-etal-2022-automatic</bibkey>
    </paper>
    <paper id="32">
      <title>On the Locality of Attention in Direct Speech Translation</title>
      <author><first>Belen</first><last>Alastruey</last></author>
      <author><first>Javier</first><last>Ferrando</last></author>
      <author><first>Gerard I.</first><last>Gállego</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <pages>402-412</pages>
      <abstract>Transformers have achieved state-of-the-art results across multiple NLP tasks. However, the self-attention mechanism complexity scales quadratically with the sequence length, creating an obstacle for tasks involving long sequences, like in the speech domain. In this paper, we discuss the usefulness of self-attention for Direct Speech Translation. First, we analyze the layer-wise token contributions in the self-attention of the encoder, unveiling local diagonal patterns. To prove that some attention weights are avoidable, we propose to substitute the standard self-attention with a local efficient one, setting the amount of context used based on the results of the analysis. With this approach, our model matches the baseline performance, and improves the efficiency by skipping the computation of those weights that standard attention discards.</abstract>
      <url hash="8fe5f60d">2022.acl-srw.32</url>
      <bibkey>alastruey-etal-2022-locality</bibkey>
    </paper>
    <paper id="33">
      <title>Extraction of Diagnostic Reasoning Relations for Clinical Knowledge Graphs</title>
      <author><first>Vimig</first><last>Socrates</last></author>
      <pages>413-421</pages>
      <abstract>Clinical knowledge graphs lack meaningful diagnostic relations (e.g. comorbidities, sign/symptoms), limiting their ability to represent real-world diagnostic processes. Previous methods in biomedical relation extraction have focused on concept relations, such as gene-disease and disease-drug, and largely ignored clinical processes. In this thesis, we leverage a clinical reasoning ontology and propose methods to extract such relations from a physician-facing point-of-care reference wiki and consumer health resource texts. Given the lack of data labeled with diagnostic relations, we also propose new methods of evaluating the correctness of extracted triples in the zero-shot setting. We describe a process for the intrinsic evaluation of new facts by triple confidence filtering and clinician manual review, as well extrinsic evaluation in the form of a differential diagnosis prediction task.</abstract>
      <url hash="0b2a4189">2022.acl-srw.33</url>
      <bibkey>socrates-2022-extraction</bibkey>
    </paper>
    <paper id="34">
      <title>Scene-Text Aware Image and Text Retrieval with Dual-Encoder</title>
      <author><first>Shumpei</first><last>Miyawaki</last></author>
      <author><first>Taku</first><last>Hasegawa</last></author>
      <author><first>Kyosuke</first><last>Nishida</last></author>
      <author><first>Takuma</first><last>Kato</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <pages>422-433</pages>
      <abstract>We tackle the tasks of image and text retrieval using a dual-encoder model in which images and text are encoded independently. This model has attracted attention as an approach that enables efficient offline inferences by connecting both vision and language in the same semantic space; however, whether an image encoder as part of a dual-encoder model can interpret scene-text (i.e., the textual information in images) is unclear.We propose pre-training methods that encourage a joint understanding of the scene-text and surrounding visual information.The experimental results demonstrate that our methods improve the retrieval performances of the dual-encoder models.</abstract>
      <url hash="d4ae3b6e">2022.acl-srw.34</url>
      <bibkey>miyawaki-etal-2022-scene</bibkey>
    </paper>
    <paper id="35">
      <title>Towards Fine-grained Classification of Climate Change related Social Media Text</title>
      <author><first>Roopal</first><last>Vaid</last></author>
      <author><first>Kartikey</first><last>Pant</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>434-443</pages>
      <abstract>With climate change becoming a cause of concern worldwide, it becomes essential to gauge people’s reactions. This can help educate and spread awareness about it and help leaders improve decision-making. This work explores the fine-grained classification and Stance detection of climate change-related social media text. Firstly, we create two datasets, ClimateStance and ClimateEng, consisting of 3777 tweets each, posted during the 2019 United Nations Framework Convention on Climate Change and comprehensively outline the dataset collection, annotation methodology, and dataset composition. Secondly, we propose the task of Climate Change stance detection based on our proposed ClimateStance dataset. Thirdly, we propose a fine-grained classification based on the ClimateEng dataset, classifying social media text into five categories: Disaster, Ocean/Water, Agriculture/Forestry, Politics, and General. We benchmark both the datasets for climate change stance detection and fine-grained classification using state-of-the-art methods in text classification. We also create a Reddit-based dataset for both the tasks, ClimateReddit, consisting of 6262 pseudo-labeled comments along with 329 manually annotated comments for the label. We then perform semi-supervised experiments for both the tasks and benchmark their results using the best-performing model for the supervised experiments. Lastly, we provide insights into the ClimateStance and ClimateReddit using part-of-speech tagging and named-entity recognition.</abstract>
      <url hash="285efec6">2022.acl-srw.35</url>
      <bibkey>vaid-etal-2022-towards</bibkey>
    </paper>
    <paper id="36">
      <title>Deep Neural Representations for Multiword Expressions Detection</title>
      <author><first>Kamil</first><last>Kanclerz</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <pages>444-453</pages>
      <abstract>Effective methods for multiword expressions detection are important for many technologies related to Natural Language Processing. Most contemporary methods are based on the sequence labeling scheme applied to an annotated corpus, while traditional methods use statistical measures. In our approach, we want to integrate the concepts of those two approaches. We present a novel weakly supervised multiword expressions extraction method which focuses on their behaviour in various contexts. Our method uses a lexicon of English multiword lexical units acquired from The Oxford Dictionary of English as a reference knowledge base and leverages neural language modelling with deep learning architectures. In our approach, we do not need a corpus annotated specifically for the task. The only required components are: a lexicon of multiword units, a large corpus, and a general contextual embeddings model. We propose a method for building a silver dataset by spotting multiword expression occurrences and acquiring statistical collocations as negative samples. Sample representation has been inspired by representations used in Natural Language Inference and relation recognition. Very good results (F1=0.8) were obtained with CNN network applied to individual occurrences followed by weighted voting used to combine results from the whole corpus.The proposed method can be quite easily applied to other languages.</abstract>
      <url hash="d8d0df12">2022.acl-srw.36</url>
      <bibkey>kanclerz-piasecki-2022-deep</bibkey>
    </paper>
    <paper id="37">
      <title>A Checkpoint on Multilingual Misogyny Identification</title>
      <author><first>Arianna</first><last>Muti</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <pages>454-460</pages>
      <abstract>We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian, and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream taskto explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data, and multilingual transformers with both monolingual and multilingual data.Our models reach state-of-the-art performance on all three languages. The single-language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero-shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes.</abstract>
      <url hash="d0b4fc41">2022.acl-srw.37</url>
      <bibkey>muti-barron-cedeno-2022-checkpoint</bibkey>
    </paper>
    <paper id="38">
      <title>Using dependency parsing for few-shot learning in distributional semantics</title>
      <author><first>Stefania</first><last>Preda</last></author>
      <author><first>Guy</first><last>Emerson</last></author>
      <pages>461-466</pages>
      <abstract>In this work, we explore the novel idea of employing dependency parsing information in the context of few-shot learning, the task of learning the meaning of a rare word based on a limited amount of context sentences. Firstly, we use dependency-based word embedding models as background spaces for few-shot learning. Secondly, we introduce two few-shot learning methods which enhance the additive baseline model by using dependencies.</abstract>
      <url hash="24d1ac78">2022.acl-srw.38</url>
      <bibkey>preda-emerson-2022-using</bibkey>
    </paper>
    <paper id="39">
      <title>A Dataset and <fixed-case>BERT</fixed-case>-based Models for Targeted Sentiment Analysis on <fixed-case>T</fixed-case>urkish Texts</title>
      <author><first>Mustafa Melih</first><last>Mutlu</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <pages>467-472</pages>
      <abstract>Targeted Sentiment Analysis aims to extract sentiment towards a particular target from a given text. It is a field that is attracting attention due to the increasing accessibility of the Internet, which leads people to generate an enormous amount of data. Sentiment analysis, which in general requires annotated data for training, is a well-researched area for widely studied languages such as English. For low-resource languages such as Turkish, there is a lack of such annotated data. We present an annotated Turkish dataset suitable for targeted sentiment analysis. We also propose BERT-based models with different architectures to accomplish the task of targeted sentiment analysis. The results demonstrate that the proposed models outperform the traditional sentiment analysis models for the targeted sentiment analysis task.</abstract>
      <url hash="4ac36fe8">2022.acl-srw.39</url>
      <bibkey>mutlu-ozgur-2022-dataset</bibkey>
    </paper>
  </volume>
</collection>
