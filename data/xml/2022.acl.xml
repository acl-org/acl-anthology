<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.acl">
  <volume id="short" ingest-date="2022-05-10">
    <meta>
      <booktitle>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</booktitle>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Aline</first><last>Villavicencio</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="45a05b97">2022.acl-short</url>
    </meta>
    <frontmatter>
      <url hash="45a05b97">2022.acl-short.0</url>
      <bibkey>acl-2022-association-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>B</fixed-case>it<fixed-case>F</fixed-case>it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</title>
      <author><first>Elad</first><last>Ben Zaken</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Shauli</first><last>Ravfogel</last></author>
      <pages>1-9</pages>
      <abstract>We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods.Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.</abstract>
      <url hash="920a4a2b">2022.acl-short.1</url>
      <attachment type="software" hash="0e45fd56">2022.acl-short.1.software.zip</attachment>
      <bibkey>ben-zaken-etal-2022-bitfit</bibkey>
    </paper>
    <paper id="2">
      <title>Are Shortest Rationales the Best Explanations for Human Understanding?</title>
      <author><first>Hua</first><last>Shen</last></author>
      <author><first>Tongshuang</first><last>Wu</last></author>
      <author><first>Wenbo</first><last>Guo</last></author>
      <author><first>Ting-Hao</first><last>Huang</last></author>
      <pages>10-19</pages>
      <abstract>Existing self-explaining models typically favor extracting the shortest possible rationales — snippets of an input text “responsible for” corresponding output — to explain the model prediction, with the assumption that shorter rationales are more intuitive to humans. However, this assumption has yet to be validated. Is the shortest rationale indeed the most human-understandable? To answer this question, we design a self-explaining model, LimitedInk, which allows users to extract rationales at any target length. Compared to existing baselines, LimitedInk achieves compatible end-task performance and human-annotated rationale agreement, making it a suitable representation of the recent class of self-explaining models. We use LimitedInk to conduct a user study on the impact of rationale length, where we ask human judges to predict the sentiment label of documents based only on LimitedInk-generated rationales with different lengths. We show rationales that are too short do not help humans predict labels better than randomly masked text, suggesting the need for more careful design of the best human rationales.</abstract>
      <url hash="96b76aec">2022.acl-short.2</url>
      <bibkey>shen-etal-2022-shortest</bibkey>
    </paper>
    <paper id="3">
      <title>Analyzing Wrap-Up Effects through an Information-Theoretic Lens</title>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Thomas</first><last>Clark</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>20-28</pages>
      <abstract>Numerous analyses of reading time (RT) data have been undertaken in the effort to learn more about the internal processes that occur during reading comprehension. However, data measured on words at the end of a sentence–or even clause–is often omitted due to the confounding factors introduced by so-called “wrap-up effects,” which manifests as a skewed distribution of RTs for these words. Consequently, the understanding of the cognitive processes that might be involved in these effects is limited. In this work, we attempt to learn more about these processes by looking for the existence–or absence–of a link between wrap-up effects and information theoretic quantities, such as word and context information content. We find that the information distribution of prior context is often predictive of sentence- and clause-final RTs (while not of sentence-medial RTs), which lends support to several prior hypotheses about the processes involved in wrap-up effects.</abstract>
      <url hash="05df18a1">2022.acl-short.3</url>
      <bibkey>meister-etal-2022-analyzing</bibkey>
    </paper>
    <paper id="4">
      <title>Have my arguments been replied to? Argument Pair Extraction as Machine Reading Comprehension</title>
      <author><first>Jianzhu</first><last>Bao</last></author>
      <author><first>Jingyi</first><last>Sun</last></author>
      <author><first>Qinglin</first><last>Zhu</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>29-35</pages>
      <abstract>Argument pair extraction (APE) aims to automatically mine argument pairs from two interrelated argumentative documents. Existing studies typically identify argument pairs indirectly by predicting sentence-level relations between two documents, neglecting the modeling of the holistic argument-level interactions. Towards this issue, we propose to address APE via a machine reading comprehension (MRC) framework with two phases. The first phase employs an argument mining (AM) query to identify all arguments in two documents. The second phase considers each identified argument as an APE query to extract its paired arguments from another document, allowing to better capture the argument-level interactions. Also, this framework enables these two phases to be jointly trained in a single MRC model, thereby maximizing the mutual benefits of them. Experimental results demonstrate that our approach achieves the best performance, outperforming the state-of-the-art method by 7.11% in F1 score.</abstract>
      <url hash="3b0f716e">2022.acl-short.4</url>
      <attachment type="software" hash="9485565d">2022.acl-short.4.software.zip</attachment>
      <bibkey>bao-etal-2022-arguments</bibkey>
    </paper>
    <paper id="5">
      <title>High probability or low information? The probability–quality paradox in language generation</title>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Gian</first><last>Wiher</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>36-45</pages>
      <abstract>When generating natural language from neural probabilistic models, high probability does not always coincide with high quality. Rather, mode-seeking decoding methods can lead to incredibly unnatural language, while stochastic methods produce text perceived as much more human-like. In this note, we offer an explanation for this phenomenon by analyzing language as a means of communication in the information-theoretic sense. We posit that human-like language usually contains an expected amount of information—quantified as negative log-probability—and that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence for this hypothesis using quality ratings for both human and machine-generated text, covering multiple tasks and common decoding schemes.</abstract>
      <url hash="5f6f5fc5">2022.acl-short.5</url>
      <bibkey>meister-etal-2022-high</bibkey>
    </paper>
    <paper id="6">
      <title>Disentangled Knowledge Transfer for <fixed-case>OOD</fixed-case> Intent Discovery with Unified Contrastive Learning</title>
      <author><first>Yutao</first><last>Mou</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>Zhiyuan</first><last>Zeng</last></author>
      <author><first>Hong</first><last>Xu</last></author>
      <author><first>Huixing</first><last>Jiang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>46-53</pages>
      <abstract>Discovering Out-of-Domain(OOD) intents is essential for developing new skills in a task-oriented dialogue system. The key challenge is how to transfer prior IND knowledge to OOD clustering. Different from existing work based on shared intent representation, we propose a novel disentangled knowledge transfer method via a unified multi-head contrastive learning framework. We aim to bridge the gap between IND pre-training and OOD clustering. Experiments and analysis on two benchmark datasets show the effectiveness of our method.</abstract>
      <url hash="0033f1f0">2022.acl-short.6</url>
      <bibkey>mou-etal-2022-disentangled</bibkey>
    </paper>
    <paper id="7">
      <title>Voxel-informed Language Grounding</title>
      <author><first>Rodolfo</first><last>Corona</last></author>
      <author><first>Shizhan</first><last>Zhu</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <author><first>Trevor</first><last>Darrell</last></author>
      <pages>54-60</pages>
      <abstract>Natural language applied to natural 2D images describes a fundamentally 3D world. We present the Voxel-informed Language Grounder (VLG), a language grounding model that leverages 3D geometric information in the form of voxel maps derived from the visual input using a volumetric reconstruction model. We show that VLG significantly improves grounding accuracy on SNARE, an object reference game task.At the time of writing, VLG holds the top place on the SNARE leaderboard, achieving SOTA results with a 2.0% absolute improvement.</abstract>
      <url hash="db00b958">2022.acl-short.7</url>
      <bibkey>corona-etal-2022-voxel</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>P</fixed-case>-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks</title>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Kaixuan</first><last>Ji</last></author>
      <author><first>Yicheng</first><last>Fu</last></author>
      <author><first>Weng</first><last>Tam</last></author>
      <author><first>Zhengxiao</first><last>Du</last></author>
      <author><first>Zhilin</first><last>Yang</last></author>
      <author><first>Jie</first><last>Tang</last></author>
      <pages>61-68</pages>
      <abstract>Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.</abstract>
      <url hash="f2409bb4">2022.acl-short.8</url>
      <bibkey>liu-etal-2022-p</bibkey>
    </paper>
    <paper id="9">
      <title>On Efficiently Acquiring Annotations for Multilingual Models</title>
      <author><first>Joel</first><last>Moniz</last></author>
      <author><first>Barun</first><last>Patra</last></author>
      <author><first>Matthew</first><last>Gormley</last></author>
      <pages>69-85</pages>
      <abstract>When tasked with supporting multiple languages for a given problem, two approaches have arisen: training a model for each language with the annotation budget divided equally among them, and training on a high-resource language followed by zero-shot transfer to the remaining languages. In this work, we show that the strategy of joint learning across multiple languages using a single model performs substantially better than the aforementioned alternatives. We also demonstrate that active learning provides additional, complementary benefits. We show that this simple approach enables the model to be data efficient by allowing it to arbitrate its annotation budget to query languages it is less certain on. We illustrate the effectiveness of our proposed method on a diverse set of tasks: a classification task with 4 languages, a sequence tagging task with 4 languages and a dependency parsing task with 5 languages. Our proposed method, whilst simple, substantially outperforms the other viable alternatives for building a model in a multilingual setting under constrained budgets.</abstract>
      <url hash="11bf2504">2022.acl-short.9</url>
      <bibkey>moniz-etal-2022-efficiently</bibkey>
    </paper>
    <paper id="10">
      <title>Automatic Detection of Entity-Manipulated Text using Factual Knowledge</title>
      <author><first>Ganesh</first><last>Jawahar</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>Laks</first><last>Lakshmanan</last></author>
      <pages>86-93</pages>
      <abstract>In this work, we focus on the problem of distinguishing a human written news article from a news article that is created by manipulating entities in a human written news article (e.g., replacing entities with factually incorrect entities). Such manipulated articles can mislead the reader by posing as a human written news article. We propose a neural network based detector that detects manipulated news articles by reasoning about the facts mentioned in the article. Our proposed detector exploits factual knowledge via graph convolutional neural network along with the textual information in the news article. We also create challenging datasets for this task by considering various strategies to generate the new replacement entity (e.g., entity generation from GPT-2). In all the settings, our proposed model either matches or outperforms the state-of-the-art detector in terms of accuracy. Our code and data are available at https://github.com/UBC-NLP/manipulated_entity_detection.</abstract>
      <url hash="48a0eebb">2022.acl-short.10</url>
      <bibkey>jawahar-etal-2022-automatic</bibkey>
    </paper>
    <paper id="11">
      <title>Does <fixed-case>BERT</fixed-case> Know that the <fixed-case>IS</fixed-case>-A Relation Is Transitive?</title>
      <author><first>Ruixi</first><last>Lin</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <pages>94-99</pages>
      <abstract>The success of a natural language processing (NLP) system on a task does not amount to fully understanding the complexity of the task, typified by many deep learning models. One such question is: can a black-box model make logically consistent predictions for transitive relations? Recent studies suggest that pre-trained BERT can capture lexico-semantic clues from words in the context. However, to what extent BERT captures the transitive nature of some lexical relations is unclear. From a probing perspective, we examine WordNet word senses and the IS-A relation, which is a transitive relation. That is, for senses A, B, and C, A is-a B and B is-a C entail A is-a C. We aim to quantify how much BERT agrees with the transitive property of IS-A relations, via a minimalist probing setting. Our investigation reveals that BERT’s predictions do not fully obey the transitivity property of the IS-A relation.</abstract>
      <url hash="bf4e392a">2022.acl-short.11</url>
      <attachment type="software" hash="93a7eb0a">2022.acl-short.11.software.zip</attachment>
      <bibkey>lin-ng-2022-bert</bibkey>
    </paper>
    <paper id="12">
      <title>Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models</title>
      <author><first>Chengyu</first><last>Chuang</last></author>
      <author><first>Yi</first><last>Yang</last></author>
      <pages>100-105</pages>
      <abstract>Pretrained language models such as BERT have achieved remarkable success in several NLP tasks. With the wide adoption of BERT in real-world applications, researchers begin to investigate the implicit biases encoded in the BERT. In this paper, we assess the implicit stock market preferences in BERT and its finance domain-specific model FinBERT. We find some interesting patterns. For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector. Given the prevalence of NLP models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets. Awareness of such problems can help practitioners improve robustness and accountability of their financial NLP pipelines .</abstract>
      <url hash="7501e9c4">2022.acl-short.12</url>
      <bibkey>chuang-yang-2022-buy</bibkey>
    </paper>
    <paper id="13">
      <title>Pixie: Preference in Implicit and Explicit Comparisons</title>
      <author><first>Amanul</first><last>Haque</last></author>
      <author><first>Vaibhav</first><last>Garg</last></author>
      <author><first>Hui</first><last>Guo</last></author>
      <author><first>Munindar</first><last>Singh</last></author>
      <pages>106-112</pages>
      <abstract>We present Pixie, a manually annotated dataset for preference classification comprising 8,890 sentences drawn from app reviews. Unlike previous studies on preference classification, Pixie contains implicit (omitting an entity being compared) and indirect (lacking comparative linguistic cues) comparisons. We find that transformer-based pretrained models, finetuned on Pixie, achieve a weighted average F1 score of 83.34% and outperform the existing state-of-the-art preference classification model (73.99%).</abstract>
      <url hash="c7e895be">2022.acl-short.13</url>
      <bibkey>haque-etal-2022-pixie</bibkey>
    </paper>
    <paper id="14">
      <title>Counterfactual Explanations for Natural Language Interfaces</title>
      <author><first>George</first><last>Tolkachev</last></author>
      <author><first>Stephen</first><last>Mell</last></author>
      <author><first>Stephan</first><last>Zdancewic</last></author>
      <author><first>Osbert</first><last>Bastani</last></author>
      <pages>113-118</pages>
      <abstract>A key challenge facing natural language interfaces is enabling users to understand the capabilities of the underlying system. We propose a novel approach for generating explanations of a natural language interface based on semantic parsing. We focus on counterfactual explanations, which are post-hoc explanations that describe to the user how they could have minimally modified their utterance to achieve their desired goal. In particular, the user provides an utterance along with a demonstration of their desired goal; then, our algorithm synthesizes a paraphrase of their utterance that is guaranteed to achieve their goal. In two user studies, we demonstrate that our approach substantially improves user performance, and that it generates explanations that more closely match the user’s intent compared to two ablations.</abstract>
      <url hash="e35adf1c">2022.acl-short.14</url>
      <attachment type="software" hash="275e13c7">2022.acl-short.14.software.zip</attachment>
      <bibkey>tolkachev-etal-2022-counterfactual</bibkey>
    </paper>
    <paper id="15">
      <title>Predicting Difficulty and Discrimination of Natural Language Questions</title>
      <author><first>Matthew</first><last>Byrd</last></author>
      <author><first>Shashank</first><last>Srivastava</last></author>
      <pages>119-130</pages>
      <abstract>Item Response Theory (IRT) has been extensively used to numerically characterize question difficulty and discrimination for human subjects in domains including cognitive psychology and education (Primi et al., 2014; Downing, 2003). More recently, IRT has been used to similarly characterize item difficulty and discrimination for natural language models across various datasets (Lalor et al., 2019; Vania et al., 2021; Rodriguez et al., 2021). In this work, we explore predictive models for directly estimating and explaining these traits for natural language questions in a question-answering context. We use HotpotQA for illustration. Our experiments show that it is possible to predict both difficulty and discrimination parameters for new questions, and these traits are correlated with features of questions, answers, and associated contexts. Our findings can have significant implications for the creation of new datasets and tests on the one hand and strategies such as active learning and curriculum learning on the other.</abstract>
      <url hash="6ded83f5">2022.acl-short.15</url>
      <attachment type="software" hash="55174984">2022.acl-short.15.software.zip</attachment>
      <bibkey>byrd-srivastava-2022-predicting</bibkey>
    </paper>
    <paper id="16">
      <title>How does the pre-training objective affect what large language models learn about linguistic properties?</title>
      <author><first>Ahmed</first><last>Alajrami</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>131-147</pages>
      <abstract>Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.</abstract>
      <url hash="3c1f4c83">2022.acl-short.16</url>
      <bibkey>alajrami-aletras-2022-pre</bibkey>
    </paper>
    <paper id="17">
      <title>The Power of Prompt Tuning for Low-Resource Semantic Parsing</title>
      <author><first>Nathan</first><last>Schucher</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <author><first>Harm</first><last>de Vries</last></author>
      <pages>148-156</pages>
      <abstract>Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks. In this paper, we investigate prompt tuning for semantic parsing—the task of mapping natural language utterances onto formal meaning representations. On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines. We also conduct ablation studies across different model scales and target representations, finding that, with increasing model scale, prompt tuned T5 models improve at generating target representations that are far from the pre-training distribution.</abstract>
      <url hash="76d22c4d">2022.acl-short.17</url>
      <bibkey>schucher-etal-2022-power</bibkey>
    </paper>
    <paper id="18">
      <title>Data Contamination: From Memorization to Exploitation</title>
      <author><first>Inbal</first><last>Magar</last></author>
      <author><first>Roy</first><last>Schwartz</last></author>
      <pages>157-165</pages>
      <abstract>Pretrained language models are typically trained on massive web-based datasets, which are often “contaminated” with downstream test sets. It is not clear to what extent models exploit the contaminated data for downstream tasks. We present a principled method to study this question. We pretrain BERT models on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune them on the relevant task. Comparing performance between samples seen and unseen during pretraining enables us to define and quantify levels of memorization and exploitation.Experiments with two models and three downstream tasks show that exploitation exists in some cases, but in others the models memorize the contaminated data, but do not exploit it. We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size. Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation.</abstract>
      <url hash="e0837ac7">2022.acl-short.18</url>
      <bibkey>magar-schwartz-2022-data</bibkey>
    </paper>
    <paper id="19">
      <title>Detecting Annotation Errors in Morphological Data with the Transformer</title>
      <author><first>Ling</first><last>Liu</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>166-174</pages>
      <abstract>Annotation errors that stem from various sources are usually unavoidable when performing large-scale annotation of linguistic data. In this paper, we evaluate the feasibility of using the Transformer model to detect various types of annotator errors in morphological data sets that contain inflected word forms. We evaluate our error detection model on four languages by introducing three different types of artificial errors in the data: (1) typographic errors, where single characters in the data are inserted, replaced, or deleted; (2) linguistic confusion errors where two inflected forms are systematically swapped; and (3) self-adversarial errors where the Transformer model itself is used to generate plausible-looking, but erroneous forms by retrieving high-scoring predictions from the search beam. Results show that the Transformer model can with perfect, or near-perfect recall detect errors in all three scenarios, even when significant amounts of the annotated data (5%-30%) are corrupted on all languages tested. Precision varies across the languages and types of errors, but is high enough that the model can be very effectively used to flag suspicious entries in large data sets for further scrutiny by human annotators.</abstract>
      <url hash="396005e1">2022.acl-short.19</url>
      <bibkey>liu-hulden-2022-detecting</bibkey>
    </paper>
    <paper id="20">
      <title>Estimating the Entropy of Linguistic Distributions</title>
      <author><first>Aryaman</first><last>Arora</last></author>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>175-195</pages>
      <abstract>Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropymust typically be estimated from observed data because researchers do not have access to the underlying probability distribution. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data. In this work, we fill this void, studying the empirical effectiveness of different entropy estimators for linguistic distributions. In a replication of two recent information-theoretic linguistic studies, we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators. We end this paper with a concrete recommendation for the entropy estimators that should be used in future linguistic studies.</abstract>
      <url hash="1b4eaa7d">2022.acl-short.20</url>
      <attachment type="software" hash="9d122214">2022.acl-short.20.software.zip</attachment>
      <bibkey>arora-etal-2022-estimating</bibkey>
    </paper>
    <paper id="21">
      <title>Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a <fixed-case>G</fixed-case>eorgian Case Study</title>
      <author><first>David</first><last>Guriel</last></author>
      <author><first>Omer</first><last>Goldman</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <pages>196-202</pages>
      <abstract>In recent years, a flurry of morphological datasets had emerged, most notably UniMorph, aa multi-lingual repository of inflection tables. However, the flat structure of the current morphological annotation makes the treatment of some languages quirky, if not impossible, specifically in cases of polypersonal agreement. In this paper we propose a general solution for such cases and expand the UniMorph annotation schema to naturally address this phenomenon, in which verbs agree with multiple arguments using true affixes. We apply this extended schema to one such language, Georgian, and provide a human-verified, accurate and balanced morphological dataset for Georgian verbs. The dataset has 4 times more tables and 6 times more verb forms compared to the existing UniMorph dataset, covering all possible variants of argument marking, demonstrating the adequacy of our proposed scheme. Experiments on a reinflection task show that generalization is easy when the data is split at the form level, but extremely hard when splitting along lemma lines. Expanding the other languages in UniMorph according to this schema is expected to improve both the coverage, consistency and interpretability of this benchmark.</abstract>
      <url hash="c79b31a7">2022.acl-short.21</url>
      <bibkey>guriel-etal-2022-morphological</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>DQ</fixed-case>-<fixed-case>BART</fixed-case>: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization</title>
      <author><first>Zheng</first><last>Li</last></author>
      <author><first>Zijian</first><last>Wang</last></author>
      <author><first>Ming</first><last>Tan</last></author>
      <author><first>Ramesh</first><last>Nallapati</last></author>
      <author><first>Parminder</first><last>Bhatia</last></author>
      <author><first>Andrew</first><last>Arnold</last></author>
      <author><first>Bing</first><last>Xiang</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>203-211</pages>
      <abstract>Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks.</abstract>
      <url hash="fc29ebf9">2022.acl-short.22</url>
      <attachment type="software" hash="0d9ca22d">2022.acl-short.22.software.zip</attachment>
      <bibkey>li-etal-2022-dq</bibkey>
    </paper>
    <paper id="23">
      <title>Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension</title>
      <author><first>Chao</first><last>Zhao</last></author>
      <author><first>Wenlin</first><last>Yao</last></author>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Kaiqiang</first><last>Song</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Jianshu</first><last>Chen</last></author>
      <pages>212-218</pages>
      <abstract>Comprehending a dialogue requires a model to capture diverse kinds of key information in the utterances, which are either scattered around or implicitly implied in different turns of conversations. Therefore, dialogue comprehension requires diverse capabilities such as paraphrasing, summarizing, and commonsense reasoning. Towards the objective of pre-training a zero-shot dialogue comprehension model, we develop a novel narrative-guided pre-training strategy that learns by narrating the key information from a dialogue input. However, the dialogue-narrative parallel corpus for such a pre-training strategy is currently unavailable. For this reason, we first construct a dialogue-narrative parallel corpus by automatically aligning movie subtitles and their synopses. We then pre-train a BART model on the data and evaluate its performance on four dialogue-based tasks that require comprehension. Experimental results show that our model not only achieves superior zero-shot performance but also exhibits stronger fine-grained dialogue comprehension capabilities. The data and code are available at https://github.com/zhaochaocs/Diana.</abstract>
      <url hash="8c20bb93">2022.acl-short.23</url>
      <bibkey>zhao-etal-2022-learning</bibkey>
    </paper>
    <paper id="24">
      <title>Kronecker Decomposition for <fixed-case>GPT</fixed-case> Compression</title>
      <author><first>Ali</first><last>Edalati</last></author>
      <author><first>Marzieh</first><last>Tahaei</last></author>
      <author><first>Ahmad</first><last>Rashid</last></author>
      <author><first>Vahid</first><last>Nia</last></author>
      <author><first>James</first><last>Clark</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <pages>219-226</pages>
      <abstract>GPT is an auto-regressive Transformer-based pre-trained language model which has attracted a lot of attention in the natural language processing (NLP) domain. The success of GPT is mostly attributed to its pre-training on huge amount of data and its large number of parameters. Despite the superior performance of GPT, this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory. This problem can be mitigated using model compression techniques; however, compressing GPT models has not been investigated much in the literature. In this work, we use Kronecker decomposition to compress the linear mappings of the GPT-2 model. Our Kronecker GPT-2 model (KnGPT2) is initialized based on the Kronecker decomposed version of the GPT-2 model and then is undergone a very light pre- training on only a small portion of the training data with intermediate layer knowledge distillation (ILKD). Finally, our KnGPT2 is fine-tuned on downstream tasks using ILKD as well. We evaluate our model on both language modeling and General Language Understanding Evaluation benchmark tasks and show that with more efficient pre-training and similar number of parameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly.</abstract>
      <url hash="7d071f30">2022.acl-short.24</url>
      <bibkey>edalati-etal-2022-kronecker</bibkey>
    </paper>
    <paper id="25">
      <title>Simple and Effective Knowledge-Driven Query Expansion for <fixed-case>QA</fixed-case>-Based Product Attribute Extraction</title>
      <author><first>Keiji</first><last>Shinzato</last></author>
      <author><first>Naoki</first><last>Yoshinaga</last></author>
      <author><first>Yandi</first><last>Xia</last></author>
      <author><first>Wei-Te</first><last>Chen</last></author>
      <pages>227-234</pages>
      <abstract>A key challenge in attribute value extraction (AVE) from e-commerce sites is how to handle a large number of attributes for diverse products. Although this challenge is partially addressed by a question answering (QA) approach which finds a value in product data for a given query (attribute), it does not work effectively for rare and ambiguous queries. We thus propose simple knowledge-driven query expansion based on possible answers (values) of a query (attribute) for QA-based AVE. We retrieve values of a query (attribute) from the training data to expand the query. We train a model with two tricks, knowledge dropout and knowledge token mixing, which mimic the imperfection of the value knowledge in testing. Experimental results on our cleaned version of AliExpress dataset show that our method improves the performance of AVE (+6.08 macro F1), especially for rare and ambiguous attributes (+7.82 and +6.86 macro F1, respectively).</abstract>
      <url hash="649f922d">2022.acl-short.25</url>
      <bibkey>shinzato-etal-2022-simple</bibkey>
    </paper>
    <paper id="26">
      <title>Event-Event Relation Extraction using Probabilistic Box Embedding</title>
      <author><first>EunJeong</first><last>Hwang</last></author>
      <author><first>Jay-Yoon</first><last>Lee</last></author>
      <author><first>Tianyi</first><last>Yang</last></author>
      <author><first>Dhruvesh</first><last>Patel</last></author>
      <author><first>Dongxu</first><last>Zhang</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>235-244</pages>
      <abstract>To understand a story with multiple events, it is important to capture the proper relations across these events. However, existing event relation extraction (ERE) framework regards it as a multi-class classification task and do not guarantee any coherence between different relation types, such as anti-symmetry. If a phone line “died” after “storm”, then it is obvious that the “storm” happened before the “died”. Current framework of event relation extraction do not guarantee this coherence and thus enforces it via constraint loss function (Wang et al., 2020). In this work, we propose to modify the underlying ERE model to guarantee coherence by representing each event as a box representation (BERE) without applying explicit constraints. From our experiments, BERE also shows stronger conjunctive constraint satisfaction while performing on par or better in F1 compared to previous models with constraint injection.</abstract>
      <url hash="81741f2c">2022.acl-short.26</url>
      <bibkey>hwang-etal-2022-event</bibkey>
    </paper>
    <paper id="27">
      <title>Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation</title>
      <author><first>Tsz Kin</first><last>Lam</last></author>
      <author><first>Shigehiko</first><last>Schamoni</last></author>
      <author><first>Stefan</first><last>Riezler</last></author>
      <pages>245-254</pages>
      <abstract>End-to-end speech translation relies on data that pair source-language speech inputs with corresponding translations into a target language. Such data are notoriously scarce, making synthetic data augmentation by back-translation or knowledge distillation a necessary ingredient of end-to-end training. In this paper, we present a novel approach to data augmentation that leverages audio alignments, linguistic properties, and translation. First, we augment a transcription by sampling from a suffix memory that stores text and audio data. Second, we translate the augmented transcript. Finally, we recombine concatenated audio segments and the generated translation. Our method delivers consistent improvements of up to 0.9 and 1.1 BLEU points on top of augmentation with knowledge distillation on five language pairs on CoVoST 2 and on two language pairs on Europarl-ST, respectively.</abstract>
      <url hash="261e92a1">2022.acl-short.27</url>
      <attachment type="software" hash="76f29c2f">2022.acl-short.27.software.tgz</attachment>
      <bibkey>lam-etal-2022-sample</bibkey>
    </paper>
    <paper id="28">
      <title>Predicting Sentence Deletions for Text Simplification Using a Functional Discourse Structure</title>
      <author><first>Bohan</first><last>Zhang</last></author>
      <author><first>Prafulla Kumar</first><last>Choubey</last></author>
      <author><first>Ruihong</first><last>Huang</last></author>
      <pages>255-261</pages>
      <abstract>Document-level text simplification often deletes some sentences besides performing lexical, grammatical or structural simplification to reduce text complexity. In this work, we focus on sentence deletions for text simplification and use a news genre-specific functional discourse structure, which categorizes sentences based on their contents and their function roles in telling a news story, for predicting sentence deletion. We incorporate sentence categories into a neural net model in two ways for predicting sentence deletions, either as additional features or by jointly predicting sentence deletions and sentence categories. Experimental results using human-annotated data show that incorporating the functional structure improves the recall of sentence deletion prediction by 6.5% and 10.7% respectively using the two methods, and improves the overall F1-score by 3.6% and 4.3% respectively.</abstract>
      <url hash="5fe5a282">2022.acl-short.28</url>
      <bibkey>zhang-etal-2022-predicting</bibkey>
    </paper>
    <paper id="29">
      <title>Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer</title>
      <author><first>Huiyuan</first><last>Lai</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>262-271</pages>
      <abstract>We exploit the pre-trained seq2seq model mBART for multilingual text style transfer. Using machine translated data as well as gold aligned English sentences yields state-of-the-art results in the three target languages we consider. Besides, in view of the general scarcity of parallel data, we propose a modular approach for multilingual formality transfer, which consists of two training strategies that target adaptation to both language and task. Our approach achieves competitive performance without monolingual task-specific parallel data and can be applied to other style transfer tasks as well as to other languages.</abstract>
      <url hash="75993540">2022.acl-short.29</url>
      <bibkey>lai-etal-2022-multilingual</bibkey>
    </paper>
    <paper id="30">
      <title>When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Kevin</first><last>Seppi</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>272-282</pages>
      <abstract>Transfer learning (TL) in natural language processing (NLP) has seen a surge of interest in recent years, as pre-trained models have shown an impressive ability to transfer to novel tasks. Three main strategies have emerged for making use of multiple supervised datasets during fine-tuning: training on an intermediate task before training on the target task (STILTs), using multi-task learning (MTL) to train jointly on a supplementary task and the target task (pairwise MTL), or simply using MTL to train jointly on all available datasets (MTL-ALL). In this work, we compare all three TL methods in a comprehensive analysis on the GLUE dataset suite. We find that there is a simple heuristic for when to use one of these techniques over the other: pairwise MTL is better than STILTs when the target task has fewer instances than the supporting task and vice versa. We show that this holds true in more than 92% of applicable cases on the GLUE dataset and validate this hypothesis with experiments varying dataset size. The simplicity and effectiveness of this heuristic is surprising and warrants additional exploration by the TL community. Furthermore, we find that MTL-ALL is worse than the pairwise methods in almost every case. We hope this study will aid others as they choose between TL methods for NLP tasks.</abstract>
      <url hash="b3db0099">2022.acl-short.30</url>
      <bibkey>weller-etal-2022-use</bibkey>
    </paper>
    <paper id="31">
      <title>Leveraging Explicit Lexico-logical Alignments in Text-to-<fixed-case>SQL</fixed-case> Parsing</title>
      <author><first>Runxin</first><last>Sun</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Chong</first><last>Zhu</last></author>
      <author><first>Yaohan</first><last>He</last></author>
      <author><first>Jinlong</first><last>Li</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <pages>283-289</pages>
      <abstract>Text-to-SQL aims to parse natural language questions into SQL queries, which is valuable in providing an easy interface to access large databases. Previous work has observed that leveraging lexico-logical alignments is very helpful to improve parsing performance. However, current attention-based approaches can only model such alignments at the token level and have unsatisfactory generalization capability. In this paper, we propose a new approach to leveraging explicit lexico-logical alignments. It first identifies possible phrase-level alignments and injects them as additional contexts to guide the parsing procedure. Experimental results on <tex-math>\textsc{Squall}</tex-math> show that our approach can make better use of such alignments and obtains an absolute improvement of 3.4% compared with the current state-of-the-art.</abstract>
      <url hash="db96344a">2022.acl-short.31</url>
      <attachment type="software" hash="2537fdd8">2022.acl-short.31.software.zip</attachment>
      <bibkey>sun-etal-2022-leveraging</bibkey>
    </paper>
    <paper id="32">
      <title>Complex Evolutional Pattern Learning for Temporal Knowledge Graph Reasoning</title>
      <author><first>Zixuan</first><last>Li</last></author>
      <author><first>Saiping</first><last>Guan</last></author>
      <author><first>Xiaolong</first><last>Jin</last></author>
      <author><first>Weihua</first><last>Peng</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <author><first>Yong</first><last>Zhu</last></author>
      <author><first>Long</first><last>Bai</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>290-296</pages>
      <abstract>A Temporal Knowledge Graph (TKG) is a sequence of KGs corresponding to different timestamps. TKG reasoning aims to predict potential facts in the future given the historical KG sequences. One key of this task is to mine and understand evolutional patterns of facts from these sequences. The evolutional patterns are complex in two aspects, length-diversity and time-variability. Existing models for TKG reasoning focus on modeling fact sequences of a fixed length, which cannot discover complex evolutional patterns that vary in length. Furthermore, these models are all trained offline, which cannot well adapt to the changes of evolutional patterns from then on. Thus, we propose a new model, called Complex Evolutional Network (CEN), which uses a length-aware Convolutional Neural Network (CNN) to handle evolutional patterns of different lengths via an easy-to-difficult curriculum learning strategy. Besides, we propose to learn the model under the online setting so that it can adapt to the changes of evolutional patterns over time. Extensive experiments demonstrate that CEN obtains substantial performance improvement under both the traditional offline and the proposed online settings.</abstract>
      <url hash="372e4bfd">2022.acl-short.32</url>
      <bibkey>li-etal-2022-complex</bibkey>
    </paper>
    <paper id="33">
      <title>Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking</title>
      <author><first>Takyoung</first><last>Kim</last></author>
      <author><first>Hoonsang</first><last>Yoon</last></author>
      <author><first>Yukyung</first><last>Lee</last></author>
      <author><first>Pilsung</first><last>Kang</last></author>
      <author><first>Misuk</first><last>Kim</last></author>
      <pages>297-309</pages>
      <abstract>Dialogue state tracking (DST) aims to extract essential information from multi-turn dialog situations and take appropriate actions. A belief state, one of the core pieces of information, refers to the subject and its specific content, and appears in the form of domain-slot-value. The trained model predicts “accumulated” belief states in every turn, and joint goal accuracy and slot accuracy are mainly used to evaluate the prediction; however, we specify that the current evaluation metrics have a critical limitation when evaluating belief states accumulated as the dialogue proceeds, especially in the most used MultiWOZ dataset. Additionally, we propose <b>relative slot accuracy</b> to complement existing metrics. Relative slot accuracy does not depend on the number of predefined slots, and allows intuitive evaluation by assigning relative scores according to the turn of each dialog. This study also encourages not solely the reporting of joint goal accuracy, but also various complementary metrics in DST tasks for the sake of a realistic evaluation.</abstract>
      <url hash="5b9d4929">2022.acl-short.33</url>
      <bibkey>kim-etal-2022-mismatch</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>LM</fixed-case>-<fixed-case>BFF</fixed-case>-<fixed-case>MS</fixed-case>: Improving Few-Shot Fine-tuning of Language Models based on Multiple Soft Demonstration Memory</title>
      <author><first>Eunhwan</first><last>Park</last></author>
      <author><first>Donghyeon</first><last>Jeon</last></author>
      <author><first>Seonhoon</first><last>Kim</last></author>
      <author><first>Inho</first><last>Kang</last></author>
      <author><first>Seung-Hoon</first><last>Na</last></author>
      <pages>310-317</pages>
      <abstract>LM-BFF (CITATION) achieves significant few-shot performance by using auto-generated prompts and adding demonstrations similar to an input example. To improve the approach of LM-BFF, this paper proposes <b>LM-BFF-MS</b>—<b>b</b>etter <b>f</b>ew-shot <b>f</b>ine-tuning of <b>l</b>anguage <b>m</b>odels with <b>m</b>ultiple <b>s</b>oft demonstrations by making its further extensions, which include 1) prompts with <i>multiple demonstrations</i> based on automatic generation of multiple label words; and 2) <i>soft demonstration memory</i> which consists of multiple sequences of <i>globally shared</i> word embeddings for a similar context. Experiments conducted on eight NLP tasks show that LM-BFF-MS leads to improvements over LM-BFF on five tasks, particularly achieving 94.0 and 90.4 on SST-2 and MRPC, respectively.</abstract>
      <url hash="627f1839">2022.acl-short.34</url>
      <attachment type="software" hash="86fbf2bb">2022.acl-short.34.software.tgz</attachment>
      <bibkey>park-etal-2022-lm</bibkey>
    </paper>
    <paper id="35">
      <title>Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances</title>
      <author><first>Suvodip</first><last>Dey</last></author>
      <author><first>Ramamohan</first><last>Kummara</last></author>
      <author><first>Maunendra</first><last>Desarkar</last></author>
      <pages>318-324</pages>
      <abstract>Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction. Generally in DST, the dialogue state or belief state for a given turn contain all the intents shown by the user till that turn. Due to this cumulative nature of the belief state, it is difficult to get a correct prediction once a misprediction has occurred. Thus, although being a useful metric, it can be harsh at times and underestimate the true potential of a DST model. Moreover, an improvement in JGA can sometimes decrease the performance of turn-level or non-cumulative belief state prediction due to inconsistency in annotations. So, using JGA as the only metric for model selection may not be ideal for all scenarios. In this work, we discuss various evaluation metrics used for DST along with their shortcomings. To address the existing issues, we propose a new evaluation metric named Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike JGA, it tries to give penalized rewards to mispredictions that are locally correct i.e. the root cause of the error is an earlier turn. By doing so, FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics. We also show that FGA is a better discriminator of DST model performance.</abstract>
      <url hash="1dfde773">2022.acl-short.35</url>
      <attachment type="software" hash="a4f7ce29">2022.acl-short.35.software.zip</attachment>
      <bibkey>dey-etal-2022-towards</bibkey>
    </paper>
    <paper id="36">
      <title>Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task</title>
      <author><first>Mohsen</first><last>Tabasi</last></author>
      <author><first>Kiamehr</first><last>Rezaee</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>325-332</pages>
      <abstract>As a recent development in few-shot learning, prompt-based techniques have demonstrated promising potential in a variety of natural language processing tasks. However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset. Specifically, none of the existing few-shot approaches (including the in-context learning of GPT-3) can attain a performance that is meaningfully different from the random baseline.Trying to fill this gap, we propose a new prompting technique, based on similarity metrics, which boosts few-shot performance to the level of fully supervised methods. Our simple adaptation shows that the failure of existing prompt-based techniques in semantic distinction is due to their improper configuration, rather than lack of relevant knowledge in the representations. We also show that this approach can be effectively extended to other downstream tasks for which a single prompt is sufficient.</abstract>
      <url hash="72b0dd77">2022.acl-short.36</url>
      <bibkey>tabasi-etal-2022-exploiting</bibkey>
    </paper>
    <paper id="37">
      <title>Hierarchical Curriculum Learning for <fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Liang</first><last>Chen</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Damai</first><last>Dai</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <pages>333-339</pages>
      <abstract>Abstract Meaning Representation (AMR) parsing aims to translate sentences to semantic representation with a hierarchical structure, and is recently empowered by pretrained sequence-to-sequence models. However, there exists a gap between their flat training objective (i.e., equally treats all output tokens) and the hierarchical AMR structure, which limits the model generalization. To bridge this gap, we propose a Hierarchical Curriculum Learning (HCL) framework with Structure-level (SC) and Instance-level Curricula (IC). SC switches progressively from core to detail AMR semantic elements while IC transits from structure-simple to -complex AMR instances during training. Through these two warming-up processes, HCL reduces the difficulty of learning complex structures, thus the flat model can better adapt to the AMR hierarchy. Extensive experiments on AMR2.0, AMR3.0, structure-complex and out-of-distribution situations verify the effectiveness of HCL.</abstract>
      <url hash="1675c555">2022.acl-short.37</url>
      <attachment type="software" hash="000755dd">2022.acl-short.37.software.zip</attachment>
      <bibkey>wang-etal-2022-hierarchical</bibkey>
    </paper>
    <paper id="38">
      <title><fixed-case>PARE</fixed-case>: A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction</title>
      <author><first>Vipul</first><last>Rathore</last></author>
      <author><first>Kartikeya</first><last>Badola</last></author>
      <author><first>Parag</first><last>Singla</last></author>
      <author><first>Mausam</first><last>.</last></author>
      <pages>340-354</pages>
      <abstract>Neural models for distantly supervised relation extraction (DS-RE) encode each sentence in an entity-pair bag separately. These are then aggregated for bag-level relation prediction. Since, at encoding time, these approaches do not allow information to flow from other sentences in the bag, we believe that they do not utilize the available bag data to the fullest. In response, we explore a simple baseline approach (PARE) in which all sentences of a bag are concatenated into a passage of sentences, and encoded jointly using BERT. The contextual embeddings of tokens are aggregated using attention with the candidate relation as query – this summary of whole passage predicts the candidate relation. We find that our simple baseline solution outperforms existing state-of-the-art DS-RE models in both monolingual and multilingual DS-RE datasets.</abstract>
      <url hash="f0c2d9f7">2022.acl-short.38</url>
      <attachment type="software" hash="36cab2db">2022.acl-short.38.software.zip</attachment>
      <bibkey>rathore-etal-2022-pare</bibkey>
    </paper>
    <paper id="39">
      <title>To Find Waldo You Need Contextual Cues: Debiasing Who’s Waldo</title>
      <author><first>Yiran</first><last>Luo</last></author>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Tejas</first><last>Gokhale</last></author>
      <author><first>Yezhou</first><last>Yang</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>355-361</pages>
      <abstract>We present a debiased dataset for the Person-centric Visual Grounding (PCVG) task first proposed by Cui et al. (2021) in the Who’s Waldo dataset. Given an image and a caption, PCVG requires pairing up a person’s name mentioned in a caption with a bounding box that points to the person in the image. We find that the original Who’s Waldo dataset compiled for this task contains a large number of biased samples that are solvable simply by heuristic methods; for instance, in many cases the first name in the sentence corresponds to the largest bounding box, or the sequence of names in the sentence corresponds to an exact left-to-right order in the image. Naturally, models trained on these biased data lead to over-estimation of performance on the benchmark. To enforce models being correct for the correct reasons, we design automated tools to filter and debias the original dataset by ruling out all examples of insufficient context, such as those with no verb or with a long chain of conjunct names in their captions. Our experiments show that our new sub-sampled dataset contains less bias with much lowered heuristic performances and widened gaps between heuristic and supervised methods. We also demonstrate the same benchmark model trained on our debiased training set outperforms that trained on the original biased (and larger) training set on our debiased test set. We argue our debiased dataset offers the PCVG task a more practical baseline for reliable benchmarking and future improvements.</abstract>
      <url hash="66f49b92">2022.acl-short.39</url>
      <bibkey>luo-etal-2022-find</bibkey>
    </paper>
    <paper id="40">
      <title>Translate-Train Embracing Translationese Artifacts</title>
      <author><first>Sicheng</first><last>Yu</last></author>
      <author><first>Qianru</first><last>Sun</last></author>
      <author><first>Hao</first><last>Zhang</last></author>
      <author><first>Jing</first><last>Jiang</last></author>
      <pages>362-370</pages>
      <abstract>Translate-train is a general training approach to multilingual tasks. The key idea is to use the translator of the target language to generate training data to mitigate the gap between the source and target languages. However, its performance is often hampered by the artifacts in the translated texts (translationese). We discover that such artifacts have common patterns in different languages and can be modeled by deep learning, and subsequently propose an approach to conduct translate-train using Translationese Embracing the effect of Artifacts (TEA). TEA learns to mitigate such effect on the training data of a source language (whose original and translationese are both available), and applies the learned module to facilitate the inference on the target language. Extensive experiments on the multilingual QA dataset TyDiQA demonstrate that TEA outperforms strong baselines.</abstract>
      <url hash="127f70c5">2022.acl-short.40</url>
      <bibkey>yu-etal-2022-translate</bibkey>
    </paper>
    <paper id="41">
      <title><fixed-case>C</fixed-case>-<fixed-case>MORE</fixed-case>: Pretraining to Answer Open-Domain Questions by Consulting Millions of References</title>
      <author><first>Xiang</first><last>Yue</last></author>
      <author><first>Xiaoman</first><last>Pan</last></author>
      <author><first>Wenlin</first><last>Yao</last></author>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Jianshu</first><last>Chen</last></author>
      <pages>371-377</pages>
      <abstract>We consider the problem of pretraining a two-stage open-domain question answering (QA) system (retriever + reader) with strong transfer capabilities. The key challenge is how to construct a large amount of high-quality question-answer-context triplets without task-specific annotations. Specifically, the triplets should align well with downstream tasks by: (i) covering a wide range of domains (for open-domain applications), (ii) linking a question to its semantically relevant context with supporting evidence (for training the retriever), and (iii) identifying the correct answer in the context (for training the reader). Previous pretraining approaches generally fall short of one or more of these requirements. In this work, we automatically construct a large-scale corpus that meets all three criteria by consulting millions of references cited within Wikipedia. The well-aligned pretraining signals benefit both the retriever and the reader significantly. Our pretrained retriever leads to 2%-10% absolute gains in top-20 accuracy. And with our pretrained reader, the entire system improves by up to 4% in exact match.</abstract>
      <url hash="0e03c484">2022.acl-short.41</url>
      <bibkey>yue-etal-2022-c</bibkey>
    </paper>
    <paper id="42">
      <title>k-<fixed-case>R</fixed-case>ater <fixed-case>R</fixed-case>eliability: <fixed-case>T</fixed-case>he Correct Unit of Reliability for Aggregated Human Annotations</title>
      <author><first>Ka</first><last>Wong</last></author>
      <author><first>Praveen</first><last>Paritosh</last></author>
      <pages>378-384</pages>
      <abstract>Since the inception of crowdsourcing, aggregation has been a common strategy for dealing with unreliable data. Aggregate ratings are more reliable than individual ones. However, many Natural Language Processing (NLP) applications that rely on aggregate ratings only report the reliability of individual ratings, which is the incorrect unit of analysis. In these instances, the data reliability is under-reported, and a proposed <tex-math>k</tex-math>-rater reliability (kRR) should be used as the correct data reliability for aggregated datasets. It is a multi-rater generalization of inter-rater reliability (IRR). We conducted two replications of the WordSim-353 benchmark, and present empirical, analytical, and bootstrap-based methods for computing kRR on WordSim-353. These methods produce very similar results. We hope this discussion will nudge researchers to report kRR in addition to IRR.</abstract>
      <url hash="82a59fdf">2022.acl-short.42</url>
      <bibkey>wong-paritosh-2022-k</bibkey>
    </paper>
    <paper id="43">
      <title>An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers</title>
      <author><first>Valentin</first><last>Hofmann</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <author><first>Janet</first><last>Pierrehumbert</last></author>
      <pages>385-393</pages>
      <abstract>We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.</abstract>
      <url hash="b614856f">2022.acl-short.43</url>
      <attachment type="software" hash="f9bf618d">2022.acl-short.43.software.zip</attachment>
      <bibkey>hofmann-etal-2022-embarrassingly</bibkey>
    </paper>
    <paper id="44">
      <title><fixed-case>SCD</fixed-case>: Self-Contrastive Decorrelation of Sentence Embeddings</title>
      <author><first>Tassilo</first><last>Klein</last></author>
      <author><first>Moin</first><last>Nabi</last></author>
      <pages>394-400</pages>
      <abstract>In this paper, we propose Self-Contrastive Decorrelation (SCD), a self-supervised approach. Given an input sentence, it optimizes a joint self-contrastive and decorrelation objective. Learning a representation is facilitated by leveraging the contrast arising from the instantiation of standard dropout at different rates. The proposed method is conceptually simple yet empirically powerful. It achieves comparable results with state-of-the-art methods on multiple benchmarks without using contrastive pairs. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods.</abstract>
      <url hash="1d3150cc">2022.acl-short.44</url>
      <bibkey>klein-nabi-2022-scd</bibkey>
    </paper>
    <paper id="45">
      <title>Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words</title>
      <author><first>Kaitlyn</first><last>Zhou</last></author>
      <author><first>Kawin</first><last>Ethayarajh</last></author>
      <author><first>Dallas</first><last>Card</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>401-423</pages>
      <abstract>Cosine similarity of contextual embeddings is used in many NLP tasks (e.g., QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in which word similarities estimated by cosine over BERT embeddings are understated and trace this effect to training data frequency. We find that relative to human judgements, cosine similarity underestimates the similarity of frequent words with other instances of the same word or other words across contexts, even after controlling for polysemy and other factors. We conjecture that this underestimation of similarity for high frequency words is due to differences in the representational geometry of high and low frequency words and provide a formal argument for the two-dimensional case.</abstract>
      <url hash="18aba160">2022.acl-short.45</url>
      <bibkey>zhou-etal-2022-problems</bibkey>
    </paper>
    <paper id="46">
      <title>Revisiting the Compositional Generalization Abilities of Neural Sequence Models</title>
      <author><first>Arkil</first><last>Patel</last></author>
      <author><first>Satwik</first><last>Bhattamishra</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <author><first>Navin</first><last>Goyal</last></author>
      <pages>424-434</pages>
      <abstract>Compositional generalization is a fundamental trait in humans, allowing us to effortlessly combine known phrases to form novel sentences. Recent works have claimed that standard seq-to-seq models severely lack the ability to compositionally generalize. In this paper, we focus on one-shot primitive generalization as introduced by the popular SCAN benchmark. We demonstrate that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance, thereby showing that their compositional generalization abilities were previously underestimated. We perform detailed empirical analysis of this phenomenon. Our results indicate that the generalization performance of models is highly sensitive to the characteristics of the training data which should be carefully considered while designing such benchmarks in future.</abstract>
      <url hash="200b96fe">2022.acl-short.46</url>
      <attachment type="software" hash="d670a08a">2022.acl-short.46.software.zip</attachment>
      <bibkey>patel-etal-2022-revisiting</bibkey>
    </paper>
    <paper id="47">
      <title>A Copy-Augmented Generative Model for Open-Domain Question Answering</title>
      <author><first>Shuang</first><last>Liu</last></author>
      <author><first>Dong</first><last>Wang</last></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Minghui</first><last>Huang</last></author>
      <author><first>Meizhen</first><last>Ding</last></author>
      <pages>435-441</pages>
      <abstract>Open-domain question answering is a challenging task with a wide variety of practical applications. Existing modern approaches mostly follow a standard two-stage paradigm: retriever then reader. In this article, we focus on improving the effectiveness of the reader module and propose a novel copy-augmented generative approach that integrates the merits of both extractive and generative readers. In particular, our model is built upon the powerful generative model FiD (CITATION). We enhance the original generative reader by incorporating a pointer network to encourage the model to directly copy words from the retrieved passages. We conduct experiments on the two benchmark datasets, Natural Questions and TriviaQA, and the empirical results demonstrate the performance gains of our proposed approach.</abstract>
      <url hash="33e8e5d9">2022.acl-short.47</url>
      <bibkey>liu-etal-2022-copy</bibkey>
    </paper>
    <paper id="48">
      <title>Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation</title>
      <author><first>Soyeong</first><last>Jeong</last></author>
      <author><first>Jinheon</first><last>Baek</last></author>
      <author><first>Sukmin</first><last>Cho</last></author>
      <author><first>Sung Ju</first><last>Hwang</last></author>
      <author><first>Jong</first><last>Park</last></author>
      <pages>442-452</pages>
      <abstract>Dense retrieval models, which aim at retrieving the most relevant document for an input query on a dense representation space, have gained considerable attention for their remarkable success. Yet, dense models require a vast amount of labeled training data for notable performance, whereas it is often challenging to acquire query-document pairs annotated by humans. To tackle this problem, we propose a simple but effective Document Augmentation for dense Retrieval (DAR) framework, which augments the representations of documents with their interpolation and perturbation. We validate the performance of DAR on retrieval tasks with two benchmark datasets, showing that the proposed DAR significantly outperforms relevant baselines on the dense retrieval of both the labeled and unlabeled documents.</abstract>
      <url hash="0cce5cd7">2022.acl-short.48</url>
      <bibkey>jeong-etal-2022-augmenting</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>WLASL</fixed-case>-<fixed-case>LEX</fixed-case>: a Dataset for Recognising Phonological Properties in <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Federico</first><last>Tavella</last></author>
      <author><first>Viktor</first><last>Schlegel</last></author>
      <author><first>Marta</first><last>Romeo</last></author>
      <author><first>Aphrodite</first><last>Galata</last></author>
      <author><first>Angelo</first><last>Cangelosi</last></author>
      <pages>453-463</pages>
      <abstract>Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far.In this paper, we bring to attention the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training.</abstract>
      <url hash="f607cd90">2022.acl-short.49</url>
      <attachment type="software" hash="d671aab0">2022.acl-short.49.software.zip</attachment>
      <bibkey>tavella-etal-2022-wlasl</bibkey>
    </paper>
    <paper id="50">
      <title>Investigating person-specific errors in chat-oriented dialogue systems</title>
      <author><first>Koh</first><last>Mitsuda</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <author><first>Tingxuan</first><last>Li</last></author>
      <author><first>Sen</first><last>Yoshida</last></author>
      <pages>464-469</pages>
      <abstract>Creating chatbots to behave like real people is important in terms of believability. Errors in general chatbots and chatbots that follow a rough persona have been studied, but those in chatbots that behave like real people have not been thoroughly investigated. We collected a large amount of user interactions of a generation-based chatbot trained from large-scale dialogue data of a specific character, i.e., target person, and analyzed errors related to that person. We found that person-specific errors can be divided into two types: errors in attributes and those in relations, each of which can be divided into two levels: self and other. The correspondence with an existing taxonomy of errors was also investigated, and person-specific errors that should be addressed in the future were clarified.</abstract>
      <url hash="5ef2eb15">2022.acl-short.50</url>
      <bibkey>mitsuda-etal-2022-investigating</bibkey>
    </paper>
    <paper id="51">
      <title>Direct parsing to sentiment graphs</title>
      <author><first>David</first><last>Samuel</last></author>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Robin</first><last>Kurtz</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>470-478</pages>
      <abstract>This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text. We advance the state of the art on 4 out of 5 standard benchmark sets. We release the source code, models and predictions.</abstract>
      <url hash="dd5bf40e">2022.acl-short.51</url>
      <attachment type="software" hash="b7223ee1">2022.acl-short.51.software.zip</attachment>
      <bibkey>samuel-etal-2022-direct</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>XDBERT</fixed-case>: <fixed-case>D</fixed-case>istilling Visual Information to <fixed-case>BERT</fixed-case> from Cross-Modal Systems to Improve Language Understanding</title>
      <author><first>Chan-Jan</first><last>Hsu</last></author>
      <author><first>Hung-yi</first><last>Lee</last></author>
      <author><first>Yu</first><last>Tsao</last></author>
      <pages>479-489</pages>
      <abstract>Transformer-based models are widely used in natural language understanding (NLU) tasks, and multimodal transformers have been effective in visual-language tasks. This study explores distilling visual information from pretrained multimodal transformers to pretrained language encoders. Our framework is inspired by cross-modal encoders’ success in visual-language tasks while we alter the learning objective to cater to the language-heavy characteristics of NLU. After training with a small number of extra adapting steps and finetuned, the proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in general language understanding evaluation (GLUE), situations with adversarial generations (SWAG) benchmarks, and readability benchmarks. We analyze the performance of XDBERT on GLUE to show that the improvement is likely visually grounded.</abstract>
      <url hash="a79234a2">2022.acl-short.52</url>
      <attachment type="software" hash="6d3019f3">2022.acl-short.52.software.zip</attachment>
      <bibkey>hsu-etal-2022-xdbert</bibkey>
    </paper>
    <paper id="53">
      <title>As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning</title>
      <author><first>Jannis</first><last>Vamvas</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>490-500</pages>
      <abstract>Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation. The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model.</abstract>
      <url hash="6bd3ec3e">2022.acl-short.53</url>
      <attachment type="software" hash="ae53cad9">2022.acl-short.53.software.zip</attachment>
      <bibkey>vamvas-sennrich-2022-little</bibkey>
    </paper>
    <paper id="54">
      <title>How Distributed are Distributed Representations? An Observation on the Locality of Syntactic Information in Verb Agreement Tasks</title>
      <author><first>Bingzhi</first><last>Li</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>Benoit</first><last>Crabbé</last></author>
      <pages>501-507</pages>
      <abstract>This work addresses the question of the localization of syntactic information encoded in the transformers representations. We tackle this question from two perspectives, considering the object-past participle agreement in French, by identifying, first, in which part of the sentence and, second, in which part of the representation the syntactic information is encoded. The results of our experiments, using probing, causal analysis and feature selection method, show that syntactic information is encoded locally in a way consistent with the French grammar.</abstract>
      <url hash="9ffdcdc9">2022.acl-short.54</url>
      <bibkey>li-etal-2022-distributed</bibkey>
    </paper>
    <paper id="55">
      <title>Machine Translation for <fixed-case>L</fixed-case>ivonian: Catering to 20 Speakers</title>
      <author><first>Matīss</first><last>Rikters</last></author>
      <author><first>Marili</first><last>Tomingas</last></author>
      <author><first>Tuuli</first><last>Tuisk</last></author>
      <author><first>Valts</first><last>Ernštreits</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <pages>508-514</pages>
      <abstract>Livonian is one of the most endangered languages in Europe with just a tiny handful of speakers and virtually no publicly available corpora. In this paper we tackle the task of developing neural machine translation (NMT) between Livonian and English, with a two-fold aim: on one hand, preserving the language and on the other – enabling access to Livonian folklore, lifestories and other textual intangible heritage as well as making it easier to create further parallel corpora. We rely on Livonian’s linguistic similarity to Estonian and Latvian and collect parallel and monolingual data for the four languages for translation experiments. We combine different low-resource NMT techniques like zero-shot translation, cross-lingual transfer and synthetic data creation to reach the highest possible translation quality as well as to find which base languages are empirically more helpful for transfer to Livonian. The resulting NMT systems and the collected monolingual and parallel data, including a manually translated and verified translation benchmark, are publicly released via OPUS and Huggingface repositories.</abstract>
      <url hash="708a7e49">2022.acl-short.55</url>
      <bibkey>rikters-etal-2022-machine</bibkey>
    </paper>
    <paper id="56">
      <title>Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text-based Games</title>
      <author><first>Dongwon</first><last>Ryu</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <author><first>Meng</first><last>Fang</last></author>
      <author><first>Yunqiu</first><last>Xu</last></author>
      <author><first>Shirui</first><last>Pan</last></author>
      <author><first>Reza</first><last>Haf</last></author>
      <pages>515-522</pages>
      <abstract>Text-based games (TGs) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action spaces. In these games, the agent learns to explore the environment via natural language interactions with the game simulator. A fundamental challenge in TGs is the efficient exploration of the large action space when the agent has not yet acquired enough knowledge about the environment. We propose CommExpl, an exploration technique that injects external commonsense knowledge, via a pretrained language model (LM), into the agent during training when the agent is the most uncertain about its next action. Our method exhibits improvement on the collected game scores during the training in four out of nine games from Jericho. Additionally, the produced trajectory of actions exhibit lower perplexity, when tested with a pretrained LM, indicating better closeness to human language.</abstract>
      <url hash="3b2f68ea">2022.acl-short.56</url>
      <bibkey>ryu-etal-2022-fire</bibkey>
    </paper>
    <paper id="57">
      <title>A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models</title>
      <author><first>Deming</first><last>Ye</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <pages>523-529</pages>
      <abstract>Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities. In this paper, we propose to build a simple but effective Pluggable Entity Lookup Table (PELT) on demand by aggregating the entity’s output representations of multiple occurrences in the corpora. PELT can be compatibly plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation with capability of acquiring knowledge from out-of-domain corpora for domain adaptation scenario. The experiments on knowledge-related tasks demonstrate that our method, PELT, can flexibly and effectively transfer entity knowledge from related corpora into PLMs with different architectures. Our code and models are publicly available at https://github.com/thunlp/PELT</abstract>
      <url hash="1d575286">2022.acl-short.57</url>
      <attachment type="software" hash="fdae91cf">2022.acl-short.57.software.zip</attachment>
      <bibkey>ye-etal-2022-simple</bibkey>
    </paper>
    <paper id="58">
      <title>S<tex-math>^4</tex-math>-Tuning: A Simple Cross-lingual Sub-network Tuning Method</title>
      <author><first>Runxin</first><last>Xu</last></author>
      <author><first>Fuli</first><last>Luo</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <pages>530-537</pages>
      <abstract>The emergence of multilingual pre-trained language models makes it possible to adapt to target languages with only few labeled examples.However, vanilla fine-tuning tends to achieve degenerated and unstable results, owing to the Language Interference among different languages, and Parameter Overload under the few-sample transfer learning scenarios.To address two problems elegantly, we propose S<tex-math>^4</tex-math>-Tuning, a Simple Cross-lingual Sub-network Tuning method. S<tex-math>^4</tex-math>-Tuning first detects the most essential sub-network for each target language, and only updates it during fine-tuning.In this way, the language sub-networks lower the scale of trainable parameters, and hence better suit the low-resource scenarios.Meanwhile, the commonality and characteristics across languages are modeled by the overlapping and non-overlapping parts to ease the interference among languages.Simple but effective, S<tex-math>^4</tex-math>-Tuning gains consistent improvements over vanilla fine-tuning on three multi-lingual tasks involving 37 different languages in total (XNLI, PAWS-X, and Tatoeba).</abstract>
      <url hash="29f1b446">2022.acl-short.58</url>
      <bibkey>xu-etal-2022-s4</bibkey>
    </paper>
    <paper id="59">
      <title>Region-dependent temperature scaling for certainty calibration and application to class-imbalanced token classification</title>
      <author><first>Hillary</first><last>Dawkins</last></author>
      <author><first>Isar</first><last>Nejadgholi</last></author>
      <pages>538-544</pages>
      <abstract>Certainty calibration is an important goal on the path to interpretability and trustworthy AI. Particularly in the context of human-in-the-loop systems, high-quality low to mid-range certainty estimates are essential. In the presence of a dominant high-certainty class, for instance the non-entity class in NER problems, existing calibration error measures are completely insensitive to potentially large errors in this certainty region of interest. We introduce a region-balanced calibration error metric that weights all certainty regions equally. When low and mid certainty estimates are taken into account, calibration error is typically larger than previously reported. We introduce a simple extension of temperature scaling, requiring no additional computation, that can reduce both traditional and region-balanced notions of calibration error over existing baselines.</abstract>
      <url hash="d2e90b55">2022.acl-short.59</url>
      <bibkey>dawkins-nejadgholi-2022-region</bibkey>
    </paper>
    <paper id="60">
      <title>Developmental Negation Processing in Transformer Language Models</title>
      <author><first>Antonio</first><last>Laverghetta Jr.</last></author>
      <author><first>John</first><last>Licato</last></author>
      <pages>545-551</pages>
      <abstract>Reasoning using negation is known to be difficult for transformer-based language models. While previous studies have used the tools of psycholinguistics to probe a transformer’s ability to reason over negation, none have focused on the types of negation studied in developmental psychology. We explore how well transformers can process such categories of negation, by framing the problem as a natural language inference (NLI) task. We curate a set of diagnostic questions for our target categories from popular NLI datasets and evaluate how well a suite of models reason over them. We find that models perform consistently better only on certain categories, suggesting clear distinctions in how they are processed.</abstract>
      <url hash="a6b9e565">2022.acl-short.60</url>
      <attachment type="software" hash="457bf561">2022.acl-short.60.software.zip</attachment>
      <bibkey>laverghetta-jr-licato-2022-developmental</bibkey>
    </paper>
    <paper id="61">
      <title>Canary Extraction in Natural Language Understanding Models</title>
      <author><first>Rahil</first><last>Parikh</last></author>
      <author><first>Christophe</first><last>Dupuy</last></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <pages>552-560</pages>
      <abstract>Natural Language Understanding (NLU) models can be trained on sensitive information such as phone numbers, zip-codes etc. Recent literature has focused on Model Inversion Attacks (ModIvA) that can extract training data from model parameters. In this work, we present a version of such an attack by extracting canaries inserted in NLU training data. In the attack, an adversary with open-box access to the model reconstructs the canaries contained in the model’s training set. We evaluate our approach by performing text completion on canaries and demonstrate that by using the prefix (non-sensitive) tokens of the canary, we can generate the full canary. As an example, our attack is able to reconstruct a four digit code in the training dataset of the NLU model with a probability of 0.5 in its best configuration. As countermeasures, we identify several defense mechanisms that, when combined, effectively eliminate the risk of ModIvA in our experiments.</abstract>
      <url hash="182c70ff">2022.acl-short.61</url>
      <bibkey>parikh-etal-2022-canary</bibkey>
    </paper>
    <paper id="62">
      <title>On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations</title>
      <author><first>Yang</first><last>Cao</last></author>
      <author><first>Yada</first><last>Pruksachatkun</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Varun</first><last>Kumar</last></author>
      <author><first>Jwala</first><last>Dhamala</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <pages>561-570</pages>
      <abstract>Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.</abstract>
      <url hash="f397ddf9">2022.acl-short.62</url>
      <bibkey>cao-etal-2022-intrinsic</bibkey>
    </paper>
    <paper id="63">
      <title>Sequence-to-sequence <fixed-case>AMR</fixed-case> Parsing with Ancestor Information</title>
      <author><first>Chen</first><last>Yu</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <pages>571-577</pages>
      <abstract>AMR parsing is the task that maps a sentence to an AMR semantic graph automatically. The difficulty comes from generating the complex graph structure. The previous state-of-the-art method translates the AMR graph into a sequence, then directly fine-tunes a pretrained sequence-to-sequence Transformer model (BART). However, purely treating the graph as a sequence does not take advantage of structural information about the graph. In this paper, we design several strategies to add the important <i>ancestor information</i> into the Transformer Decoder. Our experiments show that we can improve the performance for both AMR 2.0 and AMR 3.0 dataset and achieve new state-of-the-art results.</abstract>
      <url hash="c530c41d">2022.acl-short.63</url>
      <bibkey>yu-gildea-2022-sequence</bibkey>
    </paper>
    <paper id="64">
      <title>Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning</title>
      <author><first>Miryam</first><last>De Lhoneux</last></author>
      <author><first>Sheng</first><last>Zhang</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>578-587</pages>
      <abstract>Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models Wu and Dredze (2019), but only between related languages. However, source and training languages are rarely related, when parsing truly low-resource languages. To close this gap, we adopt a method from multi-task learning, which relies on automated curriculum learning, to dynamically optimize for parsing performance on <i>outlier</i> languages. We show that this approach is significantly better than uniform and size-proportional sampling in the zero-shot setting. </abstract>
      <url hash="ea8a1474">2022.acl-short.64</url>
      <bibkey>de-lhoneux-etal-2022-zero</bibkey>
    </paper>
    <paper id="65">
      <title><fixed-case>P</fixed-case>ri<fixed-case>M</fixed-case>ock57: A Dataset Of Primary Care Mock Consultations</title>
      <author><first>Alex</first><last>Papadopoulos Korfiatis</last></author>
      <author><first>Francesco</first><last>Moramarco</last></author>
      <author><first>Radmila</first><last>Sarac</last></author>
      <author><first>Aleksandar</first><last>Savkov</last></author>
      <pages>588-598</pages>
      <abstract>Recent advances in Automatic Speech Recognition (ASR) have made it possible to reliably produce automatic transcripts of clinician-patient conversations. However, access to clinical datasets is heavily restricted due to patient privacy, thus slowing down normal research practices. We detail the development of a public access, high quality dataset comprising of 57 mocked primary care consultations, including audio recordings, their manual utterance-level transcriptions, and the associated consultation notes. Our work illustrates how the dataset can be used as a benchmark for conversational medical ASR as well as consultation note generation from transcripts.</abstract>
      <url hash="7ec3cb08">2022.acl-short.65</url>
      <bibkey>papadopoulos-korfiatis-etal-2022-primock57</bibkey>
    </paper>
    <paper id="66">
      <title><fixed-case>U</fixed-case>ni<fixed-case>GDD</fixed-case>: <fixed-case>A</fixed-case> Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue</title>
      <author><first>Chang</first><last>Gao</last></author>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>599-605</pages>
      <abstract>The goal-oriented document-grounded dialogue aims at responding to the user query based on the dialogue context and supporting document. Existing studies tackle this problem by decomposing it into two sub-tasks: knowledge identification and response generation. However, such pipeline methods would unavoidably suffer from the error propagation issue. This paper proposes to unify these two sub-tasks via sequentially generating the grounding knowledge and the response. We further develop a prompt-connected multi-task learning strategy to model the characteristics and connections of different tasks and introduce linear temperature scheduling to reduce the negative effect of irrelevant document information. Experimental results demonstrate the effectiveness of our framework.</abstract>
      <url hash="24faa9f4">2022.acl-short.66</url>
      <bibkey>gao-etal-2022-unigdd</bibkey>
    </paper>
    <paper id="67">
      <title><fixed-case>DM</fixed-case>ix: Adaptive Distance-aware Interpolative Mixup</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Megh</first><last>Thakkar</last></author>
      <author><first>Shrey</first><last>Pandit</last></author>
      <author><first>Ritesh</first><last>Soun</last></author>
      <author><first>Di</first><last>Jin</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>606-612</pages>
      <abstract>Interpolation-based regularisation methods such as Mixup, which generate virtual training samples, have proven to be effective for various tasks and modalities.We extend Mixup and propose DMix, an adaptive distance-aware interpolative Mixup that selects samples based on their diversity in the embedding space. DMix leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation.DMix achieves state-of-the-art results on sentence classification over existing data augmentation methods on 8 benchmark datasets across English, Arabic, Turkish, and Hindi languages while achieving benchmark F1 scores in 3 times less number of iterations.We probe the effectiveness of DMix in conjunction with various similarity measures and qualitatively analyze the different components.DMix being generalizable, can be applied to various tasks, models and modalities.</abstract>
      <url hash="41ab0cea">2022.acl-short.67</url>
      <attachment type="software" hash="d9088b45">2022.acl-short.67.software.zip</attachment>
      <bibkey>sawhney-etal-2022-dmix</bibkey>
    </paper>
    <paper id="68">
      <title>Sub-Word Alignment is Still Useful: A Vest-Pocket Method for Enhancing Low-Resource Machine Translation</title>
      <author><first>Minhan</first><last>Xu</last></author>
      <author><first>Yu</first><last>Hong</last></author>
      <pages>613-619</pages>
      <abstract>We leverage embedding duplication between aligned sub-words to extend the Parent-Child transfer learning method, so as to improve low-resource machine translation. We conduct experiments on benchmark datasets of My-En, Id-En and Tr-En translation scenarios. The test results show that our method produces substantial improvements, achieving the BLEU scores of 22.5, 28.0 and 18.1 respectively. In addition, the method is computationally efficient which reduces the consumption of training time by 63.8%, reaching the duration of 1.6 hours when training on a Tesla 16GB P100 GPU. All the models and source codes in the experiments will be made publicly available to support reproducible research.</abstract>
      <url hash="b42c4b78">2022.acl-short.68</url>
      <attachment type="software" hash="e33424fe">2022.acl-short.68.software.zip</attachment>
      <bibkey>xu-hong-2022-sub</bibkey>
    </paper>
    <paper id="69">
      <title><fixed-case>HYPHEN</fixed-case>: Hyperbolic <fixed-case>H</fixed-case>awkes Attention For Text Streams</title>
      <author><first>Shivam</first><last>Agarwal</last></author>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Sanchit</first><last>Ahuja</last></author>
      <author><first>Ritesh</first><last>Soun</last></author>
      <author><first>Sudheer</first><last>Chava</last></author>
      <pages>620-627</pages>
      <abstract>Analyzing the temporal sequence of texts from sources such as social media, news, and parliamentary debates is a challenging problem as it exhibits time-varying scale-free properties and fine-grained timing irregularities. We propose a Hyperbolic Hawkes Attention Network (HYPHEN), which learns a data-driven hyperbolic space and models irregular powerlaw excitations using a hyperbolic Hawkes process. Through quantitative and exploratory experiments over financial NLP, suicide ideation detection, and political debate analysis we demonstrate HYPHEN’s practical applicability for modeling online text sequences in a geometry agnostic manner.</abstract>
      <url hash="8646d58c">2022.acl-short.69</url>
      <attachment type="software" hash="6fdc6607">2022.acl-short.69.software.zip</attachment>
      <bibkey>agarwal-etal-2022-hyphen</bibkey>
    </paper>
    <paper id="70">
      <title>A Risk-Averse Mechanism for Suicidality Assessment on Social Media</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Atula</first><last>Neerkaje</last></author>
      <author><first>Manas</first><last>Gaur</last></author>
      <pages>628-635</pages>
      <abstract>Recent studies have shown that social media has increasingly become a platform for users to express suicidal thoughts outside traditional clinical settings. With advances in Natural Language Processing strategies, it is now possible to design automated systems to assess suicide risk. However, such systems may generate uncertain predictions, leading to severe consequences. We hence reformulate suicide risk assessment as a selective prioritized prediction problem over the Columbia Suicide Severity Risk Scale (C-SSRS). We propose SASI, a risk-averse and self-aware transformer-based hierarchical attention classifier, augmented to refrain from making uncertain predictions. We show that SASI is able to refrain from 83% of incorrect predictions on real-world Reddit data. Furthermore, we discuss the qualitative, practical, and ethical aspects of SASI for suicide risk assessment as a human-in-the-loop framework.</abstract>
      <url hash="09250925">2022.acl-short.70</url>
      <bibkey>sawhney-etal-2022-risk</bibkey>
    </paper>
    <paper id="71">
      <title>When classifying grammatical role, <fixed-case>BERT</fixed-case> doesn’t care about word order... except when it matters</title>
      <author><first>Isabel</first><last>Papadimitriou</last></author>
      <author><first>Richard</first><last>Futrell</last></author>
      <author><first>Kyle</first><last>Mahowald</last></author>
      <pages>636-643</pages>
      <abstract>Because meaning can often be inferred from lexical semantics alone, word order is often a redundant cue in natural language. For example, the words chopped, chef, and onion are more likely used to convey “The chef chopped the onion,” not “The onion chopped the chef.” Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations. To overcome this confound, we probe grammatical role representation in English BERT and GPT-2, on instances where lexical expectations are not sufficient, and word order knowledge is necessary for correct classification. Such non-prototypical instances are naturally occurring English sentences with inanimate subjects or animate objects, or sentences where we systematically swap the arguments to make sentences like “The onion chopped the chef”. We find that, while early layer embeddings are largely lexical, word order is in fact crucial in defining the later-layer representations of words in semantically non-prototypical positions. Our experiments isolate the effect of word order on the contextualization process, and highlight how models use context in the uncommon, but critical, instances where it matters.</abstract>
      <url hash="56c28312">2022.acl-short.71</url>
      <attachment type="software" hash="b23260cc">2022.acl-short.71.software.tgz</attachment>
      <bibkey>papadimitriou-etal-2022-classifying-grammatical</bibkey>
    </paper>
    <paper id="72">
      <title>Triangular Transfer: Freezing the Pivot for Triangular Machine Translation</title>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>644-650</pages>
      <abstract>Triangular machine translation is a special case of low-resource machine translation where the language pair of interest has limited parallel data, but both languages have abundant parallel data with a pivot language. Naturally, the key to triangular machine translation is the successful exploitation of such auxiliary data. In this work, we propose a transfer-learning-based approach that utilizes all types of auxiliary data. As we train auxiliary source-pivot and pivot-target translation models, we initialize some parameters of the pivot side with a pre-trained language model and freeze them to encourage both translation models to work in the same pivot language space, so that they can be smoothly transferred to the source-target translation model. Experiments show that our approach can outperform previous ones.</abstract>
      <url hash="daaebfca">2022.acl-short.72</url>
      <bibkey>zhang-etal-2022-triangular</bibkey>
    </paper>
    <paper id="73">
      <title>Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge</title>
      <author><first>Brielen</first><last>Madureira</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>651-664</pages>
      <abstract>Cognitively plausible visual dialogue models should keep a mental scoreboard of shared established facts in the dialogue context. We propose a theory-based evaluation method for investigating to what degree models pretrained on the VisDial dataset incrementally build representations that appropriately do scorekeeping. Our conclusion is that the ability to make the distinction between shared and privately known statements along the dialogue is moderately present in the analysed models, but not always incrementally consistent, which may partially be due to the limited need for grounding interactions in the original task.</abstract>
      <url hash="fe2ec2d0">2022.acl-short.73</url>
      <attachment type="software" hash="496421c2">2022.acl-short.73.software.zip</attachment>
      <bibkey>madureira-schlangen-2022-visual</bibkey>
    </paper>
    <paper id="74">
      <title>Focus on the Target’s Vocabulary: Masked Label Smoothing for Machine Translation</title>
      <author><first>Liang</first><last>Chen</last></author>
      <author><first>Runxin</first><last>Xu</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <pages>665-671</pages>
      <abstract>Label smoothing and vocabulary sharing are two widely used techniques in neural machine translation models. However, we argue that simply applying both techniques can be conflicting and even leads to sub-optimal performance. When allocating smoothed probability, original label smoothing treats the source-side words that would never appear in the target language equally to the real target-side words, which could bias the translation model. To address this issue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the soft label probability of source-side words to zero. Simple yet effective, MLS manages to better integrate label smoothing with vocabulary sharing. Our extensive experiments show that MLS consistently yields improvement over original label smoothing on different datasets, including bilingual and multilingual translation from both translation quality and model’s calibration. Our code is released at https://github.com/PKUnlp-icler/MLS</abstract>
      <url hash="bc308565">2022.acl-short.74</url>
      <attachment type="software" hash="8d6fa45e">2022.acl-short.74.software.zip</attachment>
      <bibkey>chen-etal-2022-focus</bibkey>
    </paper>
    <paper id="75">
      <title>Contrastive Learning-Enhanced Nearest Neighbor Mechanism for Multi-Label Text Classification</title>
      <author><first>Xi’ao</first><last>Su</last></author>
      <author><first>Ran</first><last>Wang</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <pages>672-679</pages>
      <abstract>Multi-Label Text Classification (MLTC) is a fundamental and challenging task in natural language processing. Previous studies mainly focus on learning text representation and modeling label correlation but neglect the rich knowledge from the existing similar instances when predicting labels of a specific text. To make up for this oversight, we propose a k nearest neighbor (kNN) mechanism which retrieves several neighbor instances and interpolates the model output with their labels. Moreover, we design a multi-label contrastive learning objective that makes the model aware of the kNN classification process and improves the quality of the retrieved neighbors while inference. Extensive experiments show that our method can bring consistent and significant performance improvement to multiple MLTC models including the state-of-the-art pretrained and non-pretrained ones.</abstract>
      <url hash="838db0b6">2022.acl-short.75</url>
      <bibkey>su-etal-2022-contrastive</bibkey>
    </paper>
    <paper id="76">
      <title><fixed-case>N</fixed-case>oisy<fixed-case>T</fixed-case>une: A Little Noise Can Help You Finetune Pretrained Language Models Better</title>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Tao</first><last>Qi</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>680-685</pages>
      <abstract>Effectively finetuning pretrained language models (PLMs) is critical for their success in downstream tasks. However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks. Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance. In this paper, we propose a very simple yet effective method named NoisyTune to help better finetune PLMs on downstream tasks by adding some noise to the parameters of PLMs before fine-tuning. More specifically, we propose a matrix-wise perturbing method which adds different uniform noises to different parameter matrices based on their standard deviations. In this way, the varied characteristics of different types of parameters in PLMs can be considered. Extensive experiments on both GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can consistently empower the finetuning of different PLMs on different downstream tasks.</abstract>
      <url hash="90c8a9ff">2022.acl-short.76</url>
      <bibkey>wu-etal-2022-noisytune</bibkey>
    </paper>
    <paper id="77">
      <title>Adjusting the Precision-Recall Trade-Off with Align-and-Predict Decoding for Grammatical Error Correction</title>
      <author><first>Xin</first><last>Sun</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>686-693</pages>
      <abstract>Modern writing assistance applications are always equipped with a Grammatical Error Correction (GEC) model to correct errors in user-entered sentences. Different scenarios have varying requirements for correction behavior, e.g., performing more precise corrections (high precision) or providing more candidates for users (high recall). However, previous works adjust such trade-off only for sequence labeling approaches. In this paper, we propose a simple yet effective counterpart – Align-and-Predict Decoding (APD) for the most popular sequence-to-sequence models to offer more flexibility for the precision-recall trade-off. During inference, APD aligns the already generated sequence with input and adjusts scores of the following tokens. Experiments in both English and Chinese GEC benchmarks show that our approach not only adapts a single model to precision-oriented and recall-oriented inference, but also maximizes its potential to achieve state-of-the-art results. Our code is available at https://github.com/AutoTemp/Align-and-Predict.</abstract>
      <url hash="d8918ea2">2022.acl-short.77</url>
      <bibkey>sun-wang-2022-adjusting</bibkey>
    </paper>
    <paper id="78">
      <title>On the Effect of Isotropy on <fixed-case>VAE</fixed-case> Representations of Text</title>
      <author><first>Lan</first><last>Zhang</last></author>
      <author><first>Wray</first><last>Buntine</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <pages>694-701</pages>
      <abstract>Injecting desired geometric properties into text representations has attracted a lot of attention. A property that has been argued for, due to its better utilisation of representation space, is isotropy. In parallel, VAEs have been successful in areas of NLP, but are known for their sub-optimal utilisation of the representation space. To address an aspect of this, we investigate the impact of injecting isotropy during training of VAEs. We achieve this by using an isotropic Gaussian posterior (IGP) instead of the ellipsoidal Gaussian posterior. We illustrate that IGP effectively encourages isotropy in the representations, inducing a more discriminative latent space. Compared to vanilla VAE, this translates into a much better classification performance, robustness to input perturbation, and generative behavior. Additionally, we offer insights about the representational properties encouraged by IGP.</abstract>
      <url hash="b18885d3">2022.acl-short.78</url>
      <attachment type="software" hash="97f5638a">2022.acl-short.78.software.zip</attachment>
      <bibkey>zhang-etal-2022-effect</bibkey>
    </paper>
    <paper id="79">
      <title>Efficient Classification of Long Documents Using Transformers</title>
      <author><first>Hyunji</first><last>Park</last></author>
      <author><first>Yogarshi</first><last>Vyas</last></author>
      <author><first>Kashif</first><last>Shah</last></author>
      <pages>702-709</pages>
      <abstract>Several methods have been proposed for classifying long textual documents using Transformers. However, there is a lack of consensus on a benchmark to enable a fair comparison among different approaches. In this paper, we provide a comprehensive evaluation of the relative efficacy measured against various baselines and diverse datasets — both in terms of accuracy as well as time and space overheads. Our datasets cover binary, multi-class, and multi-label classification tasks and represent various ways information is organized in a long text (e.g. information that is critical to making the classification decision is at the beginning or towards the end of the document). Our results show that more complex models often fail to outperform simple baselines and yield inconsistent performance across datasets. These findings emphasize the need for future studies to consider comprehensive baselines and datasets that better represent the task of long document classification to develop robust models.</abstract>
      <url hash="b83bf9bb">2022.acl-short.79</url>
      <bibkey>park-etal-2022-efficient</bibkey>
    </paper>
    <paper id="80">
      <title>Rewarding Semantic Similarity under Optimized Alignments for <fixed-case>AMR</fixed-case>-to-Text Generation</title>
      <author><first>Lisa</first><last>Jin</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <pages>710-715</pages>
      <abstract>A common way to combat exposure bias is by applying scores from evaluation metrics as rewards in reinforcement learning (RL). Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards. However, metrics such as BERTScore greedily align candidate and reference tokens, which can allow system outputs to receive excess credit relative to a reference. Furthermore, past approaches featuring semantic similarity rewards suffer from repetitive outputs and overfitting. We address these issues by proposing metrics that replace the greedy alignments in BERTScore with optimized ones. We compute them on a model’s trained token embeddings to prevent domain mismatch. Our model optimizing discrete alignment metrics consistently outperforms cross-entropy and BLEU reward baselines on AMR-to-text generation. In addition, we find that this approach enjoys stable training compared to a non-RL setting.</abstract>
      <url hash="55460c69">2022.acl-short.80</url>
      <bibkey>jin-gildea-2022-rewarding</bibkey>
    </paper>
    <paper id="81">
      <title>An Analysis of Negation in Natural Language Understanding Corpora</title>
      <author><first>Md Mosharaf</first><last>Hossain</last></author>
      <author><first>Dhivya</first><last>Chinnappa</last></author>
      <author><first>Eduardo</first><last>Blanco</last></author>
      <pages>716-723</pages>
      <abstract>This paper analyzes negation in eight popular corpora spanning six natural language understanding tasks. We show that these corpora have few negations compared to general-purpose English, and that the few negations in them are often unimportant. Indeed, one can often ignore negations and still make the right predictions. Additionally, experimental results show that state-of-the-art transformers trained with these corpora obtain substantially worse results with instances that contain negation, especially if the negations are important. We conclude that new corpora accounting for negation are needed to solve natural language understanding tasks when negation is present.</abstract>
      <url hash="dfecc69d">2022.acl-short.81</url>
      <attachment type="software" hash="3a2e892c">2022.acl-short.81.software.zip</attachment>
      <bibkey>hossain-etal-2022-analysis</bibkey>
    </paper>
    <paper id="82">
      <title><fixed-case>P</fixed-case>rimum <fixed-case>N</fixed-case>on <fixed-case>N</fixed-case>ocere: <fixed-case>B</fixed-case>efore working with <fixed-case>I</fixed-case>ndigenous data, the <fixed-case>ACL</fixed-case> must confront ongoing colonialism</title>
      <author><first>Lane</first><last>Schwartz</last></author>
      <pages>724-731</pages>
      <abstract>In this paper, we challenge the ACL community to reckon with historical and ongoing colonialism by adopting a set of ethical obligations and best practices drawn from the Indigenous studies literature. While the vast majority of NLP research focuses on a very small number of very high resource languages (English, Chinese, etc), some work has begun to engage with Indigenous languages. No research involving Indigenous language data can be considered ethical without first acknowledging that Indigenous languages are not merely very low resource languages. The toxic legacy of colonialism permeates every aspect of interaction between Indigenous communities and outside researchers. To this end, we propose that the ACL draft and adopt an ethical framework for NLP researchers and computational linguists wishing to engage in research involving Indigenous languages.</abstract>
      <url hash="8c03cfb2">2022.acl-short.82</url>
      <bibkey>schwartz-2022-primum</bibkey>
    </paper>
    <paper id="83">
      <title>Unsupervised multiple-choice question generation for out-of-domain <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case> fine-tuning</title>
      <author><first>Guillaume</first><last>Le Berre</last></author>
      <author><first>Christophe</first><last>Cerisara</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <pages>732-738</pages>
      <abstract>Pre-trained models have shown very good performances on a number of question answering benchmarks especially when fine-tuned on multiple question answering datasets at once. In this work, we propose an approach for generating a fine-tuning dataset thanks to a rule-based algorithm that generates questions and answers from unannotated sentences. We show that the state-of-the-art model UnifiedQA can greatly benefit from such a system on a multiple-choice benchmark about physics, biology and chemistry it has never been trained on. We further show that improved performances may be obtained by selecting the most challenging distractors (wrong answers), with a dedicated ranker based on a pretrained RoBERTa model.</abstract>
      <url hash="e81acf4d">2022.acl-short.83</url>
      <attachment type="software" hash="f57c82e4">2022.acl-short.83.software.zip</attachment>
      <bibkey>le-berre-etal-2022-unsupervised</bibkey>
    </paper>
    <paper id="84">
      <title>Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models</title>
      <author><first>Ling</first><last>Liu</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>739-749</pages>
      <abstract>Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmata—i.e. under “wug test”-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.</abstract>
      <url hash="d8eb612c">2022.acl-short.84</url>
      <attachment type="software" hash="881c783e">2022.acl-short.84.software.zip</attachment>
      <bibkey>liu-hulden-2022-transformer</bibkey>
    </paper>
    <paper id="85">
      <title>Probing the Robustness of Trained Metrics for Conversational Dialogue Systems</title>
      <author><first>Jan</first><last>Deriu</last></author>
      <author><first>Don</first><last>Tuggener</last></author>
      <author><first>Pius</first><last>Von Däniken</last></author>
      <author><first>Mark</first><last>Cieliebak</last></author>
      <pages>750-761</pages>
      <abstract>This paper introduces an adversarial method to stress-test trained metrics for the evaluation of conversational dialogue systems. The method leverages Reinforcement Learning to find response strategies that elicit optimal scores from the trained metrics. We apply our method to test recently proposed trained metrics. We find that they all are susceptible to giving high scores to responses generated by rather simple and obviously flawed strategies that our method converges on. For instance, simply copying parts of the conversation context to form a response yields competitive scores or even outperforms responses written by humans.</abstract>
      <url hash="52c7c112">2022.acl-short.85</url>
      <attachment type="software" hash="6156e3b3">2022.acl-short.85.software.zip</attachment>
      <bibkey>deriu-etal-2022-probing</bibkey>
    </paper>
    <paper id="86">
      <title>Rethinking and Refining the Distinct Metric</title>
      <author><first>Siyang</first><last>Liu</last></author>
      <author><first>Sahand</first><last>Sabour</last></author>
      <author><first>Yinhe</first><last>Zheng</last></author>
      <author><first>Pei</first><last>Ke</last></author>
      <author><first>Xiaoyan</first><last>Zhu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>762-770</pages>
      <abstract>Distinct is a widely used automatic metric for evaluating diversity in language generation tasks.However, we observed that the original approach to calculating distinct scores has evident biases that tend to assign higher penalties to longer sequences. We refine the calculation of distinct scores by scaling the number of distinct tokens based on their expectations. We provide both empirical and theoretical evidence to show that our method effectively removes the biases existing in the original distinct score. Our experiments show that our proposed metric, <i>Expectation-Adjusted Distinct (EAD)</i>, correlates better with human judgment in evaluating response diversity.To assist future research, we provide an example implementation at <url>https://github.com/lsy641/Expectation-Adjusted-Distinct</url>.</abstract>
      <url hash="e6441f69">2022.acl-short.86</url>
      <bibkey>liu-etal-2022-rethinking</bibkey>
    </paper>
    <paper id="87">
      <title>How reparametrization trick broke differentially-private text representation learning</title>
      <author><first>Ivan</first><last>Habernal</last></author>
      <pages>771-777</pages>
      <abstract>As privacy gains traction in the NLP community, researchers have started adopting various approaches to privacy-preserving methods. One of the favorite privacy frameworks, differential privacy (DP), is perhaps the most compelling thanks to its fundamental theoretical guarantees. Despite the apparent simplicity of the general concept of differential privacy, it seems non-trivial to get it right when applying it to NLP. In this short paper, we formally analyze several recent NLP papers proposing text representation learning using DPText (Beigi et al., 2019a,b; Alnasser et al., 2021; Beigi et al., 2021) and reveal their false claims of being differentially private. Furthermore, we also show a simple yet general empirical sanity check to determine whether a given implementation of a DP mechanism almost certainly violates the privacy loss guarantees. Our main goal is to raise awareness and help the community understand potential pitfalls of applying differential privacy to text representation learning.</abstract>
      <url hash="510b5c9a">2022.acl-short.87</url>
      <attachment type="software" hash="4ef18285">2022.acl-short.87.software.zip</attachment>
      <bibkey>habernal-2022-reparametrization</bibkey>
    </paper>
    <paper id="88">
      <title>Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution</title>
      <author><first>Klim</first><last>Zaporojets</last></author>
      <author><first>Johannes</first><last>Deleu</last></author>
      <author><first>Yiwei</first><last>Jiang</last></author>
      <author><first>Thomas</first><last>Demeester</last></author>
      <author><first>Chris</first><last>Develder</last></author>
      <pages>778-784</pages>
      <abstract>We consider the task of document-level entity linking (EL), where it is important to make consistent decisions for entity mentions over the full document jointly. We aim to leverage explicit “connections” among mentions within the document itself: we propose to join EL and coreference resolution (coref) in a single structured prediction task over directed trees and use a globally normalized model to solve it. This contrasts with related works where two separate models are trained for each of the tasks and additional logic is required to merge the outputs. Experimental results on two datasets show a boost of up to +5% F1-score on both coref and EL tasks, compared to their standalone counterparts. For a subset of hard cases, with individual mentions lacking the correct EL in their candidate entity list, we obtain a +50% increase in accuracy.</abstract>
      <url hash="90149246">2022.acl-short.88</url>
      <bibkey>zaporojets-etal-2022-towards</bibkey>
    </paper>
    <paper id="89">
      <title>A Flexible Multi-Task Model for <fixed-case>BERT</fixed-case> Serving</title>
      <author><first>Tianwen</first><last>Wei</last></author>
      <author><first>Jianwei</first><last>Qi</last></author>
      <author><first>Shenghuan</first><last>He</last></author>
      <pages>785-796</pages>
      <abstract>We present an efficient BERT-based multi-task (MT) framework that is particularly suitable for iterative and incremental development of the tasks. The proposed framework is based on the idea of partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the other layers frozen. For each task, we train independently a single-task (ST) model using partial fine-tuning. Then we compress the task-specific layers in each ST model using knowledge distillation. Those compressed ST models are finally merged into one MT model so that the frozen layers of the former are shared across the tasks. We exemplify our approach on eight GLUE tasks, demonstrating that it is able to achieve 99.6% of the performance of the full fine-tuning method, while reducing up to two thirds of its overhead.</abstract>
      <url hash="6afc23c7">2022.acl-short.89</url>
      <bibkey>wei-etal-2022-flexible</bibkey>
    </paper>
    <paper id="90">
      <title>Understanding Game-Playing Agents with Natural Language Annotations</title>
      <author><first>Nicholas</first><last>Tomlin</last></author>
      <author><first>Andre</first><last>He</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>797-807</pages>
      <abstract>We present a new dataset containing 10K human-annotated games of Go and show how these natural language annotations can be used as a tool for model interpretability. Given a board state and its associated comment, our approach uses linear probing to predict mentions of domain-specific terms (e.g., ko, atari) from the intermediate state representations of game-playing agents like AlphaGo Zero. We find these game concepts are nontrivially encoded in two distinct policy networks, one trained via imitation learning and another trained via reinforcement learning. Furthermore, mentions of domain-specific terms are most easily predicted from the later layers of both models, suggesting that these policy networks encode high-level abstractions similar to those used in the natural language annotations.</abstract>
      <url hash="68fe6fa1">2022.acl-short.90</url>
      <attachment type="software" hash="af80e601">2022.acl-short.90.software.zip</attachment>
      <bibkey>tomlin-etal-2022-understanding</bibkey>
    </paper>
    <paper id="91">
      <title>Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic <fixed-case>ICD</fixed-case> Coding</title>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>Chuanqi</first><last>Tan</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <pages>808-814</pages>
      <abstract>Automatic ICD coding is defined as assigning disease codes to electronic medical records (EMRs).Existing methods usually apply label attention with code representations to match related text snippets.Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD. By aligning codes to concepts in UMLS, we collect synonyms of every code. Then, we propose a multiple synonyms matching network to leverage synonyms for better code representation learning, and finally help the code classification. Experiments on the MIMIC-III dataset show that our proposed method outperforms previous state-of-the-art methods.</abstract>
      <url hash="251d4fa3">2022.acl-short.91</url>
      <attachment type="software" hash="55e42695">2022.acl-short.91.software.zip</attachment>
      <bibkey>yuan-etal-2022-code</bibkey>
    </paper>
    <paper id="92">
      <title><fixed-case>C</fixed-case>o<fixed-case>DA</fixed-case>21: Evaluating Language Understanding Capabilities of <fixed-case>NLP</fixed-case> Models With Context-Definition Alignment</title>
      <author><first>Lütfi Kerem</first><last>Senel</last></author>
      <author><first>Timo</first><last>Schick</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>815-824</pages>
      <abstract>Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task is to align the k definitions with the k contexts. CoDA21 requires a deep understanding of contexts and definitions, including complex inference and world knowledge. We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.</abstract>
      <url hash="0abc883d">2022.acl-short.92</url>
      <bibkey>senel-etal-2022-coda21</bibkey>
    </paper>
    <paper id="93">
      <title>On the Importance of Effectively Adapting Pretrained Language Models for Active Learning</title>
      <author><first>Katerina</first><last>Margatina</last></author>
      <author><first>Loic</first><last>Barrault</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>825-836</pages>
      <abstract>Recent active learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL. We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for AL.</abstract>
      <url hash="05388ab7">2022.acl-short.93</url>
      <bibkey>margatina-etal-2022-importance</bibkey>
    </paper>
    <paper id="94">
      <title>A Recipe for Arbitrary Text Style Transfer with Large Language Models</title>
      <author><first>Emily</first><last>Reif</last></author>
      <author><first>Daphne</first><last>Ippolito</last></author>
      <author><first>Ann</first><last>Yuan</last></author>
      <author><first>Andy</first><last>Coenen</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Jason</first><last>Wei</last></author>
      <pages>837-848</pages>
      <abstract>In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as ‘make this melodramatic’ or ‘insert a metaphor.’</abstract>
      <url hash="8079f5f3">2022.acl-short.94</url>
      <attachment type="software" hash="46605c32">2022.acl-short.94.software.zip</attachment>
      <bibkey>reif-etal-2022-recipe</bibkey>
    </paper>
    <paper id="95">
      <title><fixed-case>D</fixed-case>i<fixed-case>S</fixed-case>-<fixed-case>R</fixed-case>e<fixed-case>X</fixed-case>: A Multilingual Dataset for Distantly Supervised Relation Extraction</title>
      <author><first>Abhyuday</first><last>Bhartiya</last></author>
      <author><first>Kartikeya</first><last>Badola</last></author>
      <author><first>Mausam</first><last>.</last></author>
      <pages>849-863</pages>
      <abstract>Our goal is to study the novel task of distant supervision for multilingual relation extraction (Multi DS-RE). Research in Multi DS-RE has remained limited due to the absence of a reliable benchmarking dataset. The only available dataset for this task, RELX-Distant (Köksal and Özgür, 2020), displays several unrealistic characteristics, leading to a systematic overestimation of model performance. To alleviate these concerns, we release a new benchmark dataset for the task, named DiS-ReX. We also modify the widely-used bag attention models using an mBERT encoder and provide the first baseline results on the proposed task. We show that DiS-ReX serves as a more challenging dataset than RELX-Distant, leaving ample room for future research in this domain.</abstract>
      <url hash="31ce4f49">2022.acl-short.95</url>
      <attachment type="software" hash="1f5e4a5c">2022.acl-short.95.software.zip</attachment>
      <bibkey>bhartiya-etal-2022-dis</bibkey>
    </paper>
    <paper id="96">
      <title>(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models’ Performance</title>
      <author><first>Omer</first><last>Goldman</last></author>
      <author><first>David</first><last>Guriel</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <pages>864-870</pages>
      <abstract>In the domain of Morphology, Inflection is a fundamental and important task that gained a lot of traction in recent years, mostly via SIGMORPHON’s shared-tasks.With average accuracy above 0.9 over the scores of all languages, the task is considered mostly solved using relatively generic neural seq2seq models, even with little data provided.In this work, we propose to re-evaluate morphological inflection models by employing harder train-test splits that will challenge the generalization capacity of the models. In particular, as opposed to the naïve split-by-form, we propose a split-by-lemma method to challenge the performance on existing benchmarks.Our experiments with the three top-ranked systems on the SIGMORPHON’s 2020 shared-task show that the lemma-split presents an average drop of 30 percentage points in macro-average for the 90 languages included. The effect is most significant for low-resourced languages with a drop as high as 95 points, but even high-resourced languages lose about 10 points on average. Our results clearly show that generalizing inflection to unseen lemmas is far from being solved, presenting a simple yet effective means to promote more sophisticated models.</abstract>
      <url hash="c3d8c442">2022.acl-short.96</url>
      <bibkey>goldman-etal-2022-un</bibkey>
    </paper>
    <paper id="97">
      <title>Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks</title>
      <author><first>Xing</first><last>Wu</last></author>
      <author><first>Chaochen</first><last>Gao</last></author>
      <author><first>Meng</first><last>Lin</last></author>
      <author><first>Liangjun</first><last>Zang</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>871-875</pages>
      <abstract>Before entering the neural network, a token needs to be converted to its one-hot representation, which is a discrete distribution of the vocabulary. Smoothed representation is the probability of candidate tokens obtained from the pre-trained masked language model, which can be seen as a more informative augmented substitution to the one-hot representation. We propose an efficient data augmentation method, dub as text smoothing, by converting a sentence from its one-hot representation to controllable smoothed representation.We evaluate text smoothing on different datasets in a low-resource regime. Experimental results show that text smoothing outperforms various mainstream data augmentation methods by a substantial margin. Moreover, text smoothing can be combined with these data augmentation methods to achieve better performance.</abstract>
      <url hash="4d08fc9c">2022.acl-short.97</url>
      <bibkey>wu-etal-2022-text</bibkey>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2022-05-10">
    <meta>
      <booktitle>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</booktitle>
      <editor><first>Samuel</first><last>Louvan</last></editor>
      <editor><first>Andrea</first><last>Madotto</last></editor>
      <editor><first>Brielen</first><last>Madureira</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="a7e3d8d3">2022.acl-srw</url>
    </meta>
    <frontmatter>
      <url hash="a7e3d8d3">2022.acl-srw.0</url>
      <bibkey>acl-2022-association</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Evaluating zero-shot transfers and multilingual models for dependency parsing and <fixed-case>POS</fixed-case> tagging within the low-resource language family Tupían</title>
      <author><first>Frederic</first><last>Blum</last></author>
      <pages>1-9</pages>
      <abstract>This work presents two experiments with the goal of replicating the transferability of dependency parsers and POS taggers trained on closely related languages within the low-resource language family Tupían. The experiments include both zero-shot settings as well as multilingual models. Previous studies have found that even a comparably small treebank from a closely related language will improve sequence labelling considerably in such cases. Results from both POS tagging and dependency parsing confirm previous evidence that the closer the phylogenetic relation between two languages, the better the predictions for sequence labelling tasks get. In many cases, the results are improved if multiple languages from the same family are combined. This suggests that in addition to leveraging similarity between two related languages, the incorporation of multiple languages of the same family might lead to better results in transfer learning for NLP applications.</abstract>
      <url hash="cdab8487">2022.acl-srw.1</url>
      <bibkey>blum-2022-evaluating</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>RFBFN</fixed-case>: A Relation-First Blank Filling Network for Joint Relational Triple Extraction</title>
      <author><first>Zhe</first><last>Li</last></author>
      <author><first>Luoyi</first><last>Fu</last></author>
      <author><first>Xinbing</first><last>Wang</last></author>
      <author><first>Haisong</first><last>Zhang</last></author>
      <author><first>Chenghu</first><last>Zhou</last></author>
      <pages>10-20</pages>
      <abstract>Joint relational triple extraction from unstructured text is an important task in information extraction. However, most existing works either ignore the semantic information of relations or predict subjects and objects sequentially. To address the issues, we introduce a new blank filling paradigm for the task, and propose a relation-first blank filling network (RFBFN). Specifically, we first detect potential relations maintained in the text to aid the following entity pair extraction. Then, we transform relations into relation templates with blanks which contain the fine-grained semantic representation of the relations. Finally, corresponding subjects and objects are extracted simultaneously by filling the blanks. We evaluate the proposed model on public benchmark datasets. Experimental results show our model outperforms current state-of-the-art methods. The source code of our work is available at: https://github.com/lizhe2016/RFBFN.</abstract>
      <url hash="63be2c47">2022.acl-srw.2</url>
      <bibkey>li-etal-2022-rfbfn</bibkey>
    </paper>
    <paper id="3">
      <title>Building a Dialogue Corpus Annotated with Expressed and Experienced Emotions</title>
      <author><first>Tatsuya</first><last>Ide</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <pages>21-30</pages>
      <abstract>In communication, a human would recognize the emotion of an interlocutor and respond with an appropriate emotion, such as empathy and comfort. Toward developing a dialogue system with such a human-like ability, we propose a method to build a dialogue corpus annotated with two kinds of emotions. We collect dialogues from Twitter and annotate each utterance with the emotion that a speaker put into the utterance (expressed emotion) and the emotion that a listener felt after listening to the utterance (experienced emotion). We built a dialogue corpus in Japanese using this method, and its statistical analysis revealed the differences between expressed and experienced emotions. We conducted experiments on recognition of the two kinds of emotions. The experimental results indicated the difficulty in recognizing experienced emotions and the effectiveness of multi-task learning of the two kinds of emotions. We hope that the constructed corpus will facilitate the study on emotion recognition in a dialogue and emotion-aware dialogue response generation.</abstract>
      <url hash="b71fb36e">2022.acl-srw.3</url>
      <bibkey>ide-kawahara-2022-building</bibkey>
    </paper>
    <paper id="4">
      <title>Darkness can not drive out darkness: Investigating Bias in Hate <fixed-case>S</fixed-case>peech<fixed-case>D</fixed-case>etection Models</title>
      <author><first>Fatma</first><last>Elsafoury</last></author>
      <pages>31-43</pages>
      <abstract>It has become crucial to develop tools for automated hate speech and abuse detection. These tools would help to stop the bullies and the haters and provide a safer environment for individuals especially from marginalized groups to freely express themselves. However, recent research shows that machine learning models are biased and they might make the right decisions for the wrong reasons. In this thesis, I set out to understand the performance of hate speech and abuse detection models and the different biases that could influence them. I show that hate speech and abuse detection models are not only subject to social bias but also to other types of bias that have not been explored before. Finally, I investigate the causal effect of the social and intersectional bias on the performance and unfairness of hate speech detection models.</abstract>
      <url hash="01d28bf8">2022.acl-srw.4</url>
      <bibkey>elsafoury-2022-darkness</bibkey>
    </paper>
    <paper id="5">
      <title>Ethical Considerations for Low-resourced Machine Translation</title>
      <author><first>Levon</first><last>Haroutunian</last></author>
      <pages>44-54</pages>
      <abstract>This paper considers some ethical implications of machine translation for low-resourced languages. I use Armenian as a case study and investigate specific needs for and concerns arising from the creation and deployment of improved machine translation between English and Armenian. To do this, I conduct stakeholder interviews and construct Value Scenarios (Nathan et al., 2007) from the themes that emerge. These scenarios illustrate some of the potential harms that low-resourced language communities may face due to the deployment of improved machine translation systems. Based on these scenarios, I recommend 1) collaborating with stakeholders in order to create more useful and reliable machine translation tools, and 2) determining which other forms of language technology should be developed alongside efforts to improve machine translation in order to mitigate harms rendered to vulnerable language communities. Both of these goals require treating low-resourced machine translation as a language-specific, rather than language-agnostic, task.</abstract>
      <url hash="c2acb4a2">2022.acl-srw.5</url>
      <bibkey>haroutunian-2022-ethical</bibkey>
    </paper>
    <paper id="6">
      <title>Integrating Question Rewrites in Conversational Question Answering: A Reinforcement Learning Approach</title>
      <author><first>Etsuko</first><last>Ishii</last></author>
      <author><first>Bryan</first><last>Wilie</last></author>
      <author><first>Yan</first><last>Xu</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>55-66</pages>
      <abstract>Resolving dependencies among dialogue history is one of the main obstacles in the research on conversational question answering (QA). The conversational question rewrites (QR) task has been shown to be effective to solve this problem by reformulating questions in a self-contained form. However, QR datasets are limited and existing methods tend to depend on the assumption of the existence of corresponding QR datasets for every CQA dataset.This paper proposes a reinforcement learning approach that integrates QR and CQA tasks without corresponding labeled QR datasets. We train a QR model based on the reward signal obtained from the CQA, and the experimental results show that our approach can bring improvement over the pipeline approaches.</abstract>
      <url hash="ff8f9c0e">2022.acl-srw.6</url>
      <bibkey>ishii-etal-2022-integrating</bibkey>
    </paper>
    <paper id="7">
      <title>What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification</title>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>67-83</pages>
      <abstract>Over the last five years, research on Relation Extraction (RE) witnessed extensive progress with many new dataset releases. At the same time, setup clarity has decreased, contributing to increased difficulty of reliable empirical evaluation (Taillé et al., 2020). In this paper, we provide a comprehensive survey of RE datasets, and revisit the task definition and its adoption by the community. We find that cross-dataset and cross-domain setups are particularly lacking. We present an empirical study on scientific Relation Classification across two datasets. Despite large data overlap, our analysis reveals substantial discrepancies in annotation. Annotation discrepancies strongly impact Relation Classification performance, explaining large drops in cross-dataset evaluations. Variation within further sub-domains exists but impacts Relation Classification only to limited degrees. Overall, our study calls for more rigour in reporting setups in RE and evaluation across multiple test sets.</abstract>
      <url hash="b64e8ad5">2022.acl-srw.7</url>
      <bibkey>bassignana-plank-2022-mean</bibkey>
    </paper>
    <paper id="8">
      <title>Logical Inference for Counting on Semi-structured Tables</title>
      <author><first>Tomoya</first><last>Kurosawa</last></author>
      <author><first>Hitomi</first><last>Yanaka</last></author>
      <pages>84-96</pages>
      <abstract>Recently, the Natural Language Inference (NLI) task has been studied for semi-structured tables that do not have a strict format. Although neural approaches have achieved high performance in various types of NLI, including NLI between semi-structured tables and texts, they still have difficulty in performing a numerical type of inference, such as counting. To handle a numerical type of inference, we propose a logical inference system for reasoning between semi-structured tables and texts. We use logical representations as meaning representations for tables and texts and use model checking to handle a numerical type of inference between texts and tables. To evaluate the extent to which our system can perform inference with numerical comparatives, we make an evaluation protocol that focuses on numerical understanding between semi-structured tables and texts in English. We show that our system can more robustly perform inference between tables and texts that requires numerical understanding compared with current neural approaches.</abstract>
      <url hash="99494799">2022.acl-srw.8</url>
      <bibkey>kurosawa-yanaka-2022-logical</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>GNN</fixed-case>er: Reducing Overlapping in Span-based <fixed-case>NER</fixed-case> Using Graph Neural Networks</title>
      <author><first>Urchade</first><last>Zaratiana</last></author>
      <author><first>Nadi</first><last>Tomeh</last></author>
      <author><first>Pierre</first><last>Holat</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <pages>97-103</pages>
      <abstract>There are two main paradigms for Named Entity Recognition (NER): sequence labelling and span classification. Sequence labelling aims to assign a label to each word in an input text using, for example, BIO (Begin, Inside and Outside) tagging, while span classification involves enumerating all possible spans in a text and classifying them into their labels. In contrast to sequence labelling, unconstrained span-based methods tend to assign entity labels to overlapping spans, which is generally undesirable, especially for NER tasks without nested entities. Accordingly, we propose GNNer, a framework that uses Graph Neural Networks to enrich the span representation to reduce the number of overlapping spans during prediction. Our approach reduces the number of overlapping spans compared to strong baseline while maintaining competitive metric performance. Code is available at <url>https://github.com/urchade/GNNer</url>.</abstract>
      <url hash="d0597faa">2022.acl-srw.9</url>
      <bibkey>zaratiana-etal-2022-gnner</bibkey>
    </paper>
    <paper id="10">
      <title>Compositional Semantics and Inference System for Temporal Order based on <fixed-case>J</fixed-case>apanese <fixed-case>CCG</fixed-case></title>
      <author><first>Tomoki</first><last>Sugimoto</last></author>
      <author><first>Hitomi</first><last>Yanaka</last></author>
      <pages>104-114</pages>
      <abstract>Natural Language Inference (NLI) is the task of determining whether a premise entails a hypothesis. NLI with temporal order is a challenging task because tense and aspect are complex linguistic phenomena involving interactions with temporal adverbs and temporal connectives. To tackle this, temporal and aspectual inference has been analyzed in various ways in the field of formal semantics. However, a Japanese NLI system for temporal order based on the analysis of formal semantics has not been sufficiently developed. We present a logic-based NLI system that considers temporal order in Japanese based on compositional semantics via Combinatory Categorial Grammar (CCG) syntactic analysis. Our system performs inference involving temporal order by using axioms for temporal relations and automated theorem provers. We evaluate our system by experimenting with Japanese NLI datasets that involve temporal order. We show that our system outperforms previous logic-based systems as well as current deep learning-based models.</abstract>
      <url hash="0775c922">2022.acl-srw.10</url>
      <bibkey>sugimoto-yanaka-2022-compositional</bibkey>
    </paper>
    <paper id="11">
      <title>Combine to Describe: Evaluating Compositional Generalization in Image Captioning</title>
      <author><first>George</first><last>Pantazopoulos</last></author>
      <author><first>Alessandro</first><last>Suglia</last></author>
      <author><first>Arash</first><last>Eshghi</last></author>
      <pages>115-131</pages>
      <abstract>Compositionality – the ability to combine simpler concepts to understand &amp; generate arbitrarily more complex conceptual structures – has long been thought to be the cornerstone of human language capacity. With the recent, notable success of neural models in various NLP tasks, attention has now naturally turned to the compositional capacity of these models. In this paper, we study the compositional generalization properties of image captioning models. We perform a set experiments under controlled conditions using model and data ablations, each designed to benchmark a particular facet of compositional generalization: systematicity is the ability of a model to create novel combinations of concepts out of those observed during training, productivity is here operationalised as the capacity of a model to extend its predictions beyond the length distribution it has observed during training, and substitutivity is concerned with the robustness of the model against synonym substitutions. While previous work has focused primarily on systematicity, here we provide a more in-depth analysis of the strengths and weaknesses of state of the art captioning models. Our findings demonstrate that the models we study here do not compositionally generalize in terms of systematicity and productivity, however, they are robust to some degree to synonym substitutions</abstract>
      <url hash="fb15edb4">2022.acl-srw.11</url>
      <bibkey>pantazopoulos-etal-2022-combine</bibkey>
    </paper>
    <paper id="12">
      <title>Towards Unification of Discourse Annotation Frameworks</title>
      <author><first>Yingxue</first><last>Fu</last></author>
      <pages>132-142</pages>
      <abstract>Discourse information is difficult to represent and annotate. Among the major frameworks for annotating discourse information, RST, PDTB and SDRT are widely discussed and used, each having its own theoretical foundation and focus. Corpora annotated under different frameworks vary considerably. To make better use of the existing discourse corpora and achieve the possible synergy of different frameworks, it is worthwhile to investigate the systematic relations between different frameworks and devise methods of unifying the frameworks. Although the issue of framework unification has been a topic of discussion for a long time, there is currently no comprehensive approach which considers unifying both discourse structure and discourse relations and evaluates the unified framework intrinsically and extrinsically. We plan to use automatic means for the unification task and evaluate the result with structural complexity and downstream tasks. We will also explore the application of the unified framework in multi-task learning and graphical models.</abstract>
      <url hash="dbfb1d47">2022.acl-srw.12</url>
      <bibkey>fu-2022-towards</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>AMR</fixed-case> Alignment for Morphologically-rich and Pro-drop Languages</title>
      <author><first>K. Elif</first><last>Oral</last></author>
      <author><first>Gülşen</first><last>Eryiğit</last></author>
      <pages>143-152</pages>
      <abstract>Alignment between concepts in an abstract meaning representation (AMR) graph and the words within a sentence is one of the important stages of AMR parsing. Although there exist high performing AMR aligners for English, unfortunately, these are not well suited for many languages where many concepts appear from morpho-semantic elements.For the first time in the literature, this paper presents an AMR aligner tailored for morphologically-rich and pro-drop languages by experimenting on the Turkish language being a prominent example of this language group.Our aligner focuses on the meaning considering the rich Turkish morphology and aligns AMR concepts that emerge from morphemes using a tree traversal approach without additional resources or rules. We evaluate our aligner over a manually annotated gold data set in terms of precision, recall and F1 score. Our aligner outperforms the Turkish adaptations of the previously proposed aligners for English and Portuguese by an F1 score of 0.87 and provides a relative error reduction of up to 76%.</abstract>
      <url hash="94b71440">2022.acl-srw.13</url>
      <bibkey>oral-eryigit-2022-amr</bibkey>
    </paper>
    <paper id="14">
      <title>Sketching a Linguistically-Driven Reasoning Dialog Model for Social Talk</title>
      <author><first>Alex</first><last>Lưu</last></author>
      <pages>153-170</pages>
      <abstract>The capability of holding social talk (or casual conversation) and making sense of conversational content requires context-sensitive natural language understanding and reasoning, which cannot be handled efficiently by the current popular open-domain dialog systems and chatbots. Heavily relying on corpus-based machine learning techniques to encode and decode context-sensitive meanings, these systems focus on fitting a particular training dataset, but not tracking what is actually happening in a conversation, and therefore easily derail in a new context. This work sketches out a more linguistically-informed architecture to handle social talk in English, in which corpus-based methods form the backbone of the relatively context-insensitive components (e.g. part-of-speech tagging, approximation of lexical meaning and constituent chunking), while symbolic modeling is used for reasoning out the context-sensitive components, which do not have any consistent mapping to linguistic forms. All components are fitted into a Bayesian game-theoretic model to address the interactive and rational aspects of conversation.</abstract>
      <url hash="278f461b">2022.acl-srw.14</url>
      <bibkey>luu-2022-sketching</bibkey>
    </paper>
    <paper id="15">
      <title>Scoping natural language processing in <fixed-case>I</fixed-case>ndonesian and <fixed-case>M</fixed-case>alay for education applications</title>
      <author><first>Zara</first><last>Maxwelll-Smith</last></author>
      <author><first>Michelle</first><last>Kohler</last></author>
      <author><first>Hanna</first><last>Suominen</last></author>
      <pages>171-228</pages>
      <abstract>Indonesian and Malay are underrepresented in the development of natural language processing (NLP) technologies and available resources are difficult to find. A clear picture of existing work can invigorate and inform how researchers conceptualise worthwhile projects. Using an education sector project to motivate the study, we conducted a wide-ranging overview of Indonesian and Malay human language technologies and corpus work. We charted 657 included studies according to Hirschberg and Manning’s 2015 description of NLP, concluding that the field was dominated by exploratory corpus work, machine reading of text gathered from the Internet, and sentiment analysis. In this paper, we identify most published authors and research hubs, and make a number of recommendations to encourage future collaboration and efficiency within NLP in Indonesian and Malay.</abstract>
      <url hash="3232c55c">2022.acl-srw.15</url>
      <bibkey>maxwelll-smith-etal-2022-scoping</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>E</fixed-case>nglish-<fixed-case>M</fixed-case>alay Cross-Lingual Embedding Alignment using Bilingual Lexicon Augmentation</title>
      <author><first>Ying Hao</first><last>Lim</last></author>
      <author><first>Jasy Suet Yan</first><last>Liew</last></author>
      <pages>229-238</pages>
      <abstract>As high-quality Malay language resources are still a scarcity, cross lingual word embeddings make it possible for richer English resources to be leveraged for downstream Malay text classification tasks. This paper focuses on creating an English-Malay cross-lingual word embeddings using embedding alignment by exploiting existing language resources. We augmented the training bilingual lexicons using machine translation with the goal to improve the alignment precision of our cross-lingual word embeddings. We investigated the quality of the current state-of-the-art English-Malay bilingual lexicon and worked on improving its quality using Google Translate. We also examined the effect of Malay word coverage on the quality of cross-lingual word embeddings. Experimental results with a precision up till 28.17% show that the alignment precision of the cross-lingual word embeddings would inevitably degrade after 1-NN but a better seed lexicon and cleaner nearest neighbours can reduce the number of word pairs required to achieve satisfactory performance. As the English and Malay monolingual embeddings are pre-trained on informal language corpora, our proposed English-Malay embeddings alignment approach is also able to map non-standard Malay translations in the English nearest neighbours.</abstract>
      <url hash="8494c205">2022.acl-srw.16</url>
      <bibkey>lim-liew-2022-english</bibkey>
    </paper>
    <paper id="17">
      <title>Towards Detecting Political Bias in <fixed-case>H</fixed-case>indi News Articles</title>
      <author><first>Samyak</first><last>Agrawal</last></author>
      <author><first>Kshitij</first><last>Gupta</last></author>
      <author><first>Devansh</first><last>Gautam</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>239-244</pages>
      <abstract>Political propaganda in recent times has been amplified by media news portals through biased reporting, creating untruthful narratives on serious issues causing misinformed public opinions with interests of siding and helping a particular political party. This issue proposes a challenging NLP task of detecting political bias in news articles.We propose a transformer-based transfer learning method to fine-tune the pre-trained network on our data for this bias detection. As the required dataset for this particular task was not available, we created our dataset comprising 1388 Hindi news articles and their headlines from various Hindi news media outlets. We marked them on whether they are biased towards, against, or neutral to BJP, a political party, and the current ruling party at the centre in India.</abstract>
      <url hash="87d7d2a6">2022.acl-srw.17</url>
      <bibkey>agrawal-etal-2022-towards</bibkey>
    </paper>
    <paper id="18">
      <title>Restricted or Not: A General Training Framework for Neural Machine Translation</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>245-251</pages>
      <abstract>Restricted machine translation incorporates human prior knowledge into translation. It restricts the flexibility of the translation to satisfy the demands of translation in specific scenarios. Existing work typically imposes constraints on beam search decoding. Although this can satisfy the requirements overall, it usually requires a larger beam size and far longer decoding time than unrestricted translation, which limits the concurrent processing ability of the translation model in deployment, and thus its practicality. In this paper, we propose a general training framework that allows a model to simultaneously support both unrestricted and restricted translation by adopting an additional auxiliary training process without constraining the decoding process. This maintains the benefits of restricted translation but greatly reduces the extra time overhead of constrained decoding, thus improving its practicality. The effectiveness of our proposed training framework is demonstrated by experiments on both original (WAT21 En<tex-math>\leftrightarrow</tex-math>Ja) and simulated (WMT14 En<tex-math>\rightarrow</tex-math>De and En<tex-math>\rightarrow</tex-math>Fr) restricted translation benchmarks.</abstract>
      <url hash="bdd635b7">2022.acl-srw.18</url>
      <bibkey>li-etal-2022-restricted</bibkey>
    </paper>
    <paper id="19">
      <title>What do Models Learn From Training on More Than Text? Measuring Visual Commonsense Knowledge</title>
      <author><first>Lovisa</first><last>Hagström</last></author>
      <author><first>Richard</first><last>Johansson</last></author>
      <pages>252-261</pages>
      <abstract>There are limitations in learning language from text alone. Therefore, recent focus has been on developing multimodal models. However, few benchmarks exist that can measure what language models learn about language from multimodal training. We hypothesize that training on a visual modality should improve on the visual commonsense knowledge in language models. Therefore, we introduce two evaluation tasks for measuring visual commonsense knowledge in language models (code publicly available at: github.com/lovhag/measure-visual-commonsense-knowledge) and use them to evaluate different multimodal models and unimodal baselines. Primarily, we find that the visual commonsense knowledge is not significantly different between the multimodal models and unimodal baseline models trained on visual text data.</abstract>
      <url hash="806f797a">2022.acl-srw.19</url>
      <bibkey>hagstrom-johansson-2022-models</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>T</fixed-case>elugu<fixed-case>NER</fixed-case>: Leveraging Multi-Domain Named Entity Recognition with Deep Transformers</title>
      <author><first>Suma Reddy</first><last>Duggenpudi</last></author>
      <author><first>Subba Reddy</first><last>Oota</last></author>
      <author><first>Mounika</first><last>Marreddy</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>262-272</pages>
      <abstract>Named Entity Recognition (NER) is a successful and well-researched problem in English due to the availability of resources. The transformer models, specifically the masked-language models (MLM), have shown remarkable performance in NER during recent times. With growing data in different online platforms, there is a need for NER in other languages too. NER remains to be underexplored in Indian languages due to the lack of resources and tools. Our contributions in this paper include (i) Two annotated NER datasets for the Telugu language in multiple domains: Newswire Dataset (ND) and Medical Dataset (MD), and we combined ND and MD to form Combined Dataset (CD) (ii) Comparison of the finetuned Telugu pretrained transformer models (BERT-Te, RoBERTa-Te, and ELECTRA-Te) with other baseline models (CRF, LSTM-CRF, and BiLSTM-CRF) (iii) Further investigation of the performance of Telugu pretrained transformer models against the multilingual models mBERT, XLM-R, and IndicBERT. We find that pretrained Telugu language models (BERT-Te and RoBERTa) outperform the existing pretrained multilingual and baseline models in NER. On a large dataset (CD) of 38,363 sentences, the BERT-Te achieves a high F1-score of 0.80 (entity-level) and 0.75 (token-level). Further, these pretrained Telugu models have shown state-of-the-art performance on various existing Telugu NER datasets. We open-source our dataset, pretrained models, and code.</abstract>
      <url hash="68037939">2022.acl-srw.20</url>
      <bibkey>duggenpudi-etal-2022-teluguner</bibkey>
    </paper>
    <paper id="21">
      <title>Using Neural Machine Translation Methods for Sign Language Translation</title>
      <author><first>Galina</first><last>Angelova</last></author>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>273-284</pages>
      <abstract>We examine methods and techniques, proven to be helpful for the text-to-text translation of spoken languages in the context of gloss-to-text translation systems, where the glosses are the written representation of the signs. We present one of the first works that include experiments on both parallel corpora of the German Sign Language (PHOENIX14T and the Public DGS Corpus). We experiment with two NMT architectures with optimization of their hyperparameters, several tokenization methods and two data augmentation techniques (back-translation and paraphrasing). Through our investigation we achieve a substantial improvement of 5.0 and 2.2 BLEU scores for the models trained on the two corpora respectively. Our RNN models outperform our Transformer models, and the segmentation method we achieve best results with is BPE, whereas back-translation and paraphrasing lead to minor but not significant improvements.</abstract>
      <url hash="20d75fa4">2022.acl-srw.21</url>
      <bibkey>angelova-etal-2022-using</bibkey>
    </paper>
    <paper id="22">
      <title>Flexible Visual Grounding</title>
      <author><first>Yongmin</first><last>Kim</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>285-299</pages>
      <abstract>Existing visual grounding datasets are artificially made, where every query regarding an entity must be able to be grounded to a corresponding image region, i.e., answerable. However, in real-world multimedia data such as news articles and social media, many entities in the text cannot be grounded to the image, i.e., unanswerable, due to the fact that the text is unnecessarily directly describing the accompanying image. A robust visual grounding model should be able to flexibly deal with both answerable and unanswerable visual grounding. To study this flexible visual grounding problem, we construct a pseudo dataset and a social media dataset including both answerable and unanswerable queries. In order to handle unanswerable visual grounding, we propose a novel method by adding a pseudo image region corresponding to a query that cannot be grounded. The model is then trained to ground to ground-truth regions for answerable queries and pseudo regions for unanswerable queries. In our experiments, we show that our model can flexibly process both answerable and unanswerable queries with high accuracy on our datasets.</abstract>
      <url hash="56f4c355">2022.acl-srw.22</url>
      <bibkey>kim-etal-2022-flexible</bibkey>
    </paper>
    <paper id="23">
      <title>A large-scale computational study of content preservation measures for text style transfer and paraphrase generation</title>
      <author><first>Nikolay</first><last>Babakov</last></author>
      <author><first>David</first><last>Dale</last></author>
      <author><first>Varvara</first><last>Logacheva</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>300-321</pages>
      <abstract>Text style transfer and paraphrasing of texts are actively growing areas of NLP, dozens of methods for solving these tasks have been recently introduced. In both tasks, the system is supposed to generate a text which should be semantically similar to the input text. Therefore, these tasks are dependent on methods of measuring textual semantic similarity. However, it is still unclear which measures are the best to automatically evaluate content preservation between original and generated text. According to our observations, many researchers still use BLEU-like measures, while there exist more advanced measures including neural-based that significantly outperform classic approaches. The current problem is the lack of a thorough evaluation of the available measures. We close this gap by conducting a large-scale computational study by comparing 57 measures based on different principles on 19 annotated datasets. We show that measures based on cross-encoder models outperform alternative approaches in almost all cases.We also introduce the Mutual Implication Score (MIS), a measure that uses the idea of paraphrasing as a bidirectional entailment and outperforms all other measures on the paraphrase detection task and performs on par with the best measures in the text style transfer task.</abstract>
      <url hash="b367e784">2022.acl-srw.23</url>
      <bibkey>babakov-etal-2022-large</bibkey>
    </paper>
    <paper id="24">
      <title>Explicit Object Relation Alignment for Vision and Language Navigation</title>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>322-331</pages>
      <abstract>In this paper, we investigate the problem of vision and language navigation. To solve this problem, grounding the landmarks and spatial relations in the textual instructions into visual modality is important. We propose a neural agent named Explicit Object Relation Alignment Agent (EXOR),to explicitly align the spatial information in both instruction and the visual environment, including landmarks and spatial relationships between the agent and landmarks.Empirically, our proposed method surpasses the baseline by a large margin on the R2R dataset. We provide a comprehensive analysis to show our model’s spatial reasoning ability and explainability.</abstract>
      <url hash="2769bf94">2022.acl-srw.24</url>
      <bibkey>zhang-kordjamshidi-2022-explicit</bibkey>
    </paper>
    <paper id="25">
      <title>Mining Logical Event Schemas From Pre-Trained Language Models</title>
      <author><first>Lane</first><last>Lawley</last></author>
      <author><first>Lenhart</first><last>Schubert</last></author>
      <pages>332-345</pages>
      <abstract>We present NESL (the Neuro-Episodic Schema Learner), an event schema learning system that combines large language models, FrameNet parsing, a powerful logical representation of language, and a set of simple behavioral schemas meant to bootstrap the learning process. In lieu of a pre-made corpus of stories, our dataset is a continuous feed of “situation samples” from a pre-trained language model, which are then parsed into FrameNet frames, mapped into simple behavioral schemas, and combined and generalized into complex, hierarchical schemas for a variety of everyday scenarios. We show that careful sampling from the language model can help emphasize stereotypical properties of situations and de-emphasize irrelevant details, and that the resulting schemas specify situations more comprehensively than those learned by other systems.</abstract>
      <url hash="5952286a">2022.acl-srw.25</url>
      <bibkey>lawley-schubert-2022-mining</bibkey>
    </paper>
    <paper id="26">
      <title>Exploring Cross-lingual Text Detoxification with Large Multilingual Language Models.</title>
      <author><first>Daniil</first><last>Moskovskiy</last></author>
      <author><first>Daryna</first><last>Dementieva</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>346-354</pages>
      <abstract>Detoxification is a task of generating text in polite style while preserving meaning and fluency of the original toxic text. Existing detoxification methods are monolingual i.e. designed to work in one exact language. This work investigates multilingual and cross-lingual detoxification and the behavior of large multilingual models in this setting. Unlike previous works we aim to make large language models able to perform detoxification without direct fine-tuning in a given language. Experiments show that multilingual models are capable of performing multilingual style transfer. However, tested state-of-the-art models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is currently inevitable and motivating the need of further research in this direction.</abstract>
      <url hash="90f73db6">2022.acl-srw.26</url>
      <bibkey>moskovskiy-etal-2022-exploring</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>MEKER</fixed-case>: Memory Efficient Knowledge Embedding Representation for Link Prediction and Question Answering</title>
      <author><first>Viktoriia</first><last>Chekalina</last></author>
      <author><first>Anton</first><last>Razzhigaev</last></author>
      <author><first>Albert</first><last>Sayapin</last></author>
      <author><first>Evgeny</first><last>Frolov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>355-365</pages>
      <abstract>Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG embedding contains concise data used in NLP tasks requiring implicit information about the real world. Furthermore, the size of KGs that may be useful in actual NLP assignments is enormous, and creating embedding over it has memory cost issues. We represent KG as a 3rd-order binary tensor and move beyond the standard CP decomposition (CITATION) by using a data-specific generalized version of it (CITATION). The generalization of the standard CP-ALS algorithm allows obtaining optimization gradients without a backpropagation mechanism. It reduces the memory needed in training while providing computational benefits. We propose a MEKER, a memory-efficient KG embedding model, which yields SOTA-comparable performance on link prediction tasks and KG-based Question Answering.</abstract>
      <url hash="9a2fd942">2022.acl-srw.27</url>
      <bibkey>chekalina-etal-2022-meker</bibkey>
    </paper>
    <paper id="28">
      <title>Discourse on <fixed-case>ASR</fixed-case> Measurement: Introducing the <fixed-case>ARPOCA</fixed-case> Assessment Tool</title>
      <author><first>Megan</first><last>Merz</last></author>
      <author><first>Olga</first><last>Scrivner</last></author>
      <pages>366-372</pages>
      <abstract>Automatic speech recognition (ASR) has evolved from a pipeline architecture with pronunciation dictionaries, phonetic features and language models to the end-to-end systems performing a direct translation from a raw waveform into a word sequence. With the increase in accuracy and the availability of pre-trained models, the ASR systems are now omnipresent in our daily applications. On the other hand, the models’ interpretability and their computational cost have become more challenging, particularly when dealing with less-common languages or identifying regional variations of speakers. This research proposal will follow a four-stage process: 1) Proving an overview of acoustic features and feature extraction algorithms; 2) Exploring current ASR models, tools, and performance assessment techniques; 3) Aligning features with interpretable phonetic transcripts; and 4) Designing a prototype ARPOCA to increase awareness of regional language variation and improve models feedback by developing a semi-automatic acoustic features extraction using PRAAT in conjunction with phonetic transcription.</abstract>
      <url hash="b5deadad">2022.acl-srw.28</url>
      <bibkey>merz-scrivner-2022-discourse</bibkey>
    </paper>
    <paper id="29">
      <title>Pretrained Knowledge Base Embeddings for improved Sentential Relation Extraction</title>
      <author><first>Andrea</first><last>Papaluca</last></author>
      <author><first>Daniel</first><last>Krefl</last></author>
      <author><first>Hanna</first><last>Suominen</last></author>
      <author><first>Artem</first><last>Lenskiy</last></author>
      <pages>373-382</pages>
      <abstract>In this work we put forward to combine pretrained knowledge base graph embeddings with transformer based language models to improve performance on the sentential Relation Extraction task in natural language processing. Our proposed model is based on a simple variation of existing models to incorporate off-task pretrained graph embeddings with an on-task finetuned BERT encoder. We perform a detailed statistical evaluation of the model on standard datasets. We provide evidence that the added graph embeddings improve the performance, making such a simple approach competitive with the state-of-the-art models that perform explicit on-task training of the graph embeddings. Furthermore, we ob- serve for the underlying BERT model an interesting power-law scaling behavior between the variance of the F1 score obtained for a relation class and its support in terms of training examples.</abstract>
      <url hash="d1712ba5">2022.acl-srw.29</url>
      <bibkey>papaluca-etal-2022-pretrained</bibkey>
    </paper>
    <paper id="30">
      <title>Improving Cross-domain, Cross-lingual and Multi-modal Deception Detection</title>
      <author><first>Subhadarshi</first><last>Panda</last></author>
      <author><first>Sarah Ita</first><last>Levitan</last></author>
      <pages>383-390</pages>
      <abstract>With the increase of deception and misinformation especially in social media, it has become crucial to be able to develop machine learning methods to automatically identify deceptive language. In this proposal, we identify key challenges underlying deception detection in cross-domain, cross-lingual and multi-modal settings. To improve cross-domain deception classification, we propose to use inter-domain distance to identify a suitable source domain for a given target domain. We propose to study the efficacy of multilingual classification models vs translation for cross-lingual deception classification. Finally, we propose to better understand multi-modal deception detection and explore methods to weight and combine information from multiple modalities to improve multi-modal deception classification.</abstract>
      <url hash="60ec8626">2022.acl-srw.30</url>
      <bibkey>panda-levitan-2022-improving</bibkey>
    </paper>
    <paper id="31">
      <title>Automatic Generation of Distractors for Fill-in-the-Blank Exercises with Round-Trip Neural Machine Translation</title>
      <author><first>Subhadarshi</first><last>Panda</last></author>
      <author><first>Frank</first><last>Palma Gomez</last></author>
      <author><first>Michael</first><last>Flor</last></author>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <pages>391-401</pages>
      <abstract>In a fill-in-the-blank exercise, a student is presented with a carrier sentence with one word hidden, and a multiple-choice list that includes the correct answer and several inappropriate options, called distractors. We propose to automatically generate distractors using round-trip neural machine translation: the carrier sentence is translated from English into another (pivot) language and back, and distractors are produced by aligning the original sentence and its round-trip translation. We show that using hundreds of translations for a given sentence allows us to generate a rich set of challenging distractors. Further, using multiple pivot languages produces a diverse set of candidates. The distractors are evaluated against a real corpus of cloze exercises and checked manually for validity. We demonstrate that the proposed method significantly outperforms two strong baselines.</abstract>
      <url hash="f5f038b9">2022.acl-srw.31</url>
      <bibkey>panda-etal-2022-automatic</bibkey>
    </paper>
    <paper id="32">
      <title>On the Locality of Attention in Direct Speech Translation</title>
      <author><first>Belen</first><last>Alastruey</last></author>
      <author><first>Javier</first><last>Ferrando</last></author>
      <author><first>Gerard I.</first><last>Gállego</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <pages>402-412</pages>
      <abstract>Transformers have achieved state-of-the-art results across multiple NLP tasks. However, the self-attention mechanism complexity scales quadratically with the sequence length, creating an obstacle for tasks involving long sequences, like in the speech domain. In this paper, we discuss the usefulness of self-attention for Direct Speech Translation. First, we analyze the layer-wise token contributions in the self-attention of the encoder, unveiling local diagonal patterns. To prove that some attention weights are avoidable, we propose to substitute the standard self-attention with a local efficient one, setting the amount of context used based on the results of the analysis. With this approach, our model matches the baseline performance, and improves the efficiency by skipping the computation of those weights that standard attention discards.</abstract>
      <url hash="8fe5f60d">2022.acl-srw.32</url>
      <bibkey>alastruey-etal-2022-locality</bibkey>
    </paper>
    <paper id="33">
      <title>Extraction of Diagnostic Reasoning Relations for Clinical Knowledge Graphs</title>
      <author><first>Vimig</first><last>Socrates</last></author>
      <pages>413-421</pages>
      <abstract>Clinical knowledge graphs lack meaningful diagnostic relations (e.g. comorbidities, sign/symptoms), limiting their ability to represent real-world diagnostic processes. Previous methods in biomedical relation extraction have focused on concept relations, such as gene-disease and disease-drug, and largely ignored clinical processes. In this thesis, we leverage a clinical reasoning ontology and propose methods to extract such relations from a physician-facing point-of-care reference wiki and consumer health resource texts. Given the lack of data labeled with diagnostic relations, we also propose new methods of evaluating the correctness of extracted triples in the zero-shot setting. We describe a process for the intrinsic evaluation of new facts by triple confidence filtering and clinician manual review, as well extrinsic evaluation in the form of a differential diagnosis prediction task.</abstract>
      <url hash="0b2a4189">2022.acl-srw.33</url>
      <bibkey>socrates-2022-extraction</bibkey>
    </paper>
    <paper id="34">
      <title>Scene-Text Aware Image and Text Retrieval with Dual-Encoder</title>
      <author><first>Shumpei</first><last>Miyawaki</last></author>
      <author><first>Taku</first><last>Hasegawa</last></author>
      <author><first>Kyosuke</first><last>Nishida</last></author>
      <author><first>Takuma</first><last>Kato</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <pages>422-433</pages>
      <abstract>We tackle the tasks of image and text retrieval using a dual-encoder model in which images and text are encoded independently. This model has attracted attention as an approach that enables efficient offline inferences by connecting both vision and language in the same semantic space; however, whether an image encoder as part of a dual-encoder model can interpret scene-text (i.e., the textual information in images) is unclear.We propose pre-training methods that encourage a joint understanding of the scene-text and surrounding visual information.The experimental results demonstrate that our methods improve the retrieval performances of the dual-encoder models.</abstract>
      <url hash="d4ae3b6e">2022.acl-srw.34</url>
      <bibkey>miyawaki-etal-2022-scene</bibkey>
    </paper>
    <paper id="35">
      <title>Towards Fine-grained Classification of Climate Change related Social Media Text</title>
      <author><first>Roopal</first><last>Vaid</last></author>
      <author><first>Kartikey</first><last>Pant</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>434-443</pages>
      <abstract>With climate change becoming a cause of concern worldwide, it becomes essential to gauge people’s reactions. This can help educate and spread awareness about it and help leaders improve decision-making. This work explores the fine-grained classification and Stance detection of climate change-related social media text. Firstly, we create two datasets, ClimateStance and ClimateEng, consisting of 3777 tweets each, posted during the 2019 United Nations Framework Convention on Climate Change and comprehensively outline the dataset collection, annotation methodology, and dataset composition. Secondly, we propose the task of Climate Change stance detection based on our proposed ClimateStance dataset. Thirdly, we propose a fine-grained classification based on the ClimateEng dataset, classifying social media text into five categories: Disaster, Ocean/Water, Agriculture/Forestry, Politics, and General. We benchmark both the datasets for climate change stance detection and fine-grained classification using state-of-the-art methods in text classification. We also create a Reddit-based dataset for both the tasks, ClimateReddit, consisting of 6262 pseudo-labeled comments along with 329 manually annotated comments for the label. We then perform semi-supervised experiments for both the tasks and benchmark their results using the best-performing model for the supervised experiments. Lastly, we provide insights into the ClimateStance and ClimateReddit using part-of-speech tagging and named-entity recognition.</abstract>
      <url hash="285efec6">2022.acl-srw.35</url>
      <bibkey>vaid-etal-2022-towards</bibkey>
    </paper>
    <paper id="36">
      <title>Deep Neural Representations for Multiword Expressions Detection</title>
      <author><first>Kamil</first><last>Kanclerz</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <pages>444-453</pages>
      <abstract>Effective methods for multiword expressions detection are important for many technologies related to Natural Language Processing. Most contemporary methods are based on the sequence labeling scheme applied to an annotated corpus, while traditional methods use statistical measures. In our approach, we want to integrate the concepts of those two approaches. We present a novel weakly supervised multiword expressions extraction method which focuses on their behaviour in various contexts. Our method uses a lexicon of English multiword lexical units acquired from The Oxford Dictionary of English as a reference knowledge base and leverages neural language modelling with deep learning architectures. In our approach, we do not need a corpus annotated specifically for the task. The only required components are: a lexicon of multiword units, a large corpus, and a general contextual embeddings model. We propose a method for building a silver dataset by spotting multiword expression occurrences and acquiring statistical collocations as negative samples. Sample representation has been inspired by representations used in Natural Language Inference and relation recognition. Very good results (F1=0.8) were obtained with CNN network applied to individual occurrences followed by weighted voting used to combine results from the whole corpus.The proposed method can be quite easily applied to other languages.</abstract>
      <url hash="d8d0df12">2022.acl-srw.36</url>
      <bibkey>kanclerz-piasecki-2022-deep</bibkey>
    </paper>
    <paper id="37">
      <title>A Checkpoint on Multilingual Misogyny Identification</title>
      <author><first>Arianna</first><last>Muti</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <pages>454-460</pages>
      <abstract>We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian, and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream taskto explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data, and multilingual transformers with both monolingual and multilingual data.Our models reach state-of-the-art performance on all three languages. The single-language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero-shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes.</abstract>
      <url hash="d0b4fc41">2022.acl-srw.37</url>
      <bibkey>muti-barron-cedeno-2022-checkpoint</bibkey>
    </paper>
    <paper id="38">
      <title>Using dependency parsing for few-shot learning in distributional semantics</title>
      <author><first>Stefania</first><last>Preda</last></author>
      <author><first>Guy</first><last>Emerson</last></author>
      <pages>461-466</pages>
      <abstract>In this work, we explore the novel idea of employing dependency parsing information in the context of few-shot learning, the task of learning the meaning of a rare word based on a limited amount of context sentences. Firstly, we use dependency-based word embedding models as background spaces for few-shot learning. Secondly, we introduce two few-shot learning methods which enhance the additive baseline model by using dependencies.</abstract>
      <url hash="24d1ac78">2022.acl-srw.38</url>
      <bibkey>preda-emerson-2022-using</bibkey>
    </paper>
    <paper id="39">
      <title>A Dataset and <fixed-case>BERT</fixed-case>-based Models for Targeted Sentiment Analysis on <fixed-case>T</fixed-case>urkish Texts</title>
      <author><first>Mustafa Melih</first><last>Mutlu</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <pages>467-472</pages>
      <abstract>Targeted Sentiment Analysis aims to extract sentiment towards a particular target from a given text. It is a field that is attracting attention due to the increasing accessibility of the Internet, which leads people to generate an enormous amount of data. Sentiment analysis, which in general requires annotated data for training, is a well-researched area for widely studied languages such as English. For low-resource languages such as Turkish, there is a lack of such annotated data. We present an annotated Turkish dataset suitable for targeted sentiment analysis. We also propose BERT-based models with different architectures to accomplish the task of targeted sentiment analysis. The results demonstrate that the proposed models outperform the traditional sentiment analysis models for the targeted sentiment analysis task.</abstract>
      <url hash="4ac36fe8">2022.acl-srw.39</url>
      <bibkey>mutlu-ozgur-2022-dataset</bibkey>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2022-05-10">
    <meta>
      <booktitle>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</booktitle>
      <editor><first>Valerio</first><last>Basile</last></editor>
      <editor><first>Zornitsa</first><last>Kozareva</last></editor>
      <editor><first>Sanja</first><last>Stajner</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="d92e3f4d">2022.acl-demo</url>
    </meta>
    <frontmatter>
      <url hash="d92e3f4d">2022.acl-demo.0</url>
      <bibkey>acl-2022-association-linguistics-system</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>D</fixed-case>o<fixed-case>TAT</fixed-case>: A Domain-oriented Text Annotation Tool</title>
      <author><first>Yupian</first><last>Lin</last></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <author><first>Ming</first><last>Liang</last></author>
      <author><first>Tingting</first><last>Cai</last></author>
      <author><first>Wen</first><last>Du</last></author>
      <author><first>Yi</first><last>Wang</last></author>
      <pages>1-8</pages>
      <abstract>We propose DoTAT, a domain-oriented text annotation tool. The tool designs and implements functions heavily in need in domain-oriented information extraction. Firstly, the tool supports a multi-person collaborative process with automatically merging and review, which can greatly improve the annotation accuracy. Secondly, the tool provides annotation of events, nested event and nested entity, which are frequently required in domain-related text structuring tasks. Finally, DoTAT provides visual annotation specification definition, automatic batch annotation and iterative annotation to improve annotation efficiency. Experiments on the ACE2005 dataset show that DoTAT can reduce the event annotation time by 19.7% compared with existing annotation tools. The accuracy without review is 84.09%, 1.35% higher than Brat and 2.59% higher than Webanno. The accuracy of DoTAT even reaches 93.76% with review. The demonstration video can be accessed from https://ecust-nlp-docker.oss-cn-shanghai.aliyuncs.com/dotat_demo.mp4. A live demo website is available at https://github.com/FXLP/MarkTool.</abstract>
      <url hash="e9f55c41">2022.acl-demo.1</url>
      <bibkey>lin-etal-2022-dotat</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>UKP</fixed-case>-<fixed-case>SQUARE</fixed-case>: An Online Platform for Question Answering Research</title>
      <author><first>Tim</first><last>Baumgärtner</last></author>
      <author><first>Kexin</first><last>Wang</last></author>
      <author><first>Rachneet</first><last>Sachdeva</last></author>
      <author><first>Gregor</first><last>Geigle</last></author>
      <author><first>Max</first><last>Eichler</last></author>
      <author><first>Clifton</first><last>Poth</last></author>
      <author><first>Hannah</first><last>Sterz</last></author>
      <author><first>Haritz</first><last>Puerto</last></author>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Nils</first><last>Reimers</last></author>
      <author><first>Gözde</first><last>Şahin</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>9-22</pages>
      <abstract>Recent advances in NLP and information retrieval have given rise to a diverse set of question answering tasks that are of different formats (e.g., extractive, abstractive), require different model architectures (e.g., generative, discriminative), and setups (e.g., with or without retrieval). Despite having a large number of powerful, specialized QA pipelines (which we refer to as Skills) that consider a single domain, model or setup, there exists no framework where users can easily explore and compare such pipelines and can extend them according to their needs. To address this issue, we present UKP-SQuARE, an extensible online QA platform for researchers which allows users to query and analyze a large collection of modern Skills via a user-friendly web interface and integrated behavioural tests. In addition, QA researchers can develop, manage, and share their custom Skills using our microservices that support a wide range of models (Transformers, Adapters, ONNX), datastores and retrieval techniques (e.g., sparse and dense). UKP-SQuARE is available on https://square.ukp-lab.de</abstract>
      <url hash="d7023ccc">2022.acl-demo.2</url>
      <bibkey>baumgartner-etal-2022-ukp</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>V</fixed-case>i<fixed-case>LM</fixed-case>edic: a framework for research at the intersection of vision and language in medical <fixed-case>AI</fixed-case></title>
      <author><first>Jean-benoit</first><last>Delbrouck</last></author>
      <author><first>Khaled</first><last>Saab</last></author>
      <author><first>Maya</first><last>Varma</last></author>
      <author><first>Sabri</first><last>Eyuboglu</last></author>
      <author><first>Pierre</first><last>Chambon</last></author>
      <author><first>Jared</first><last>Dunnmon</last></author>
      <author><first>Juan</first><last>Zambrano</last></author>
      <author><first>Akshay</first><last>Chaudhari</last></author>
      <author><first>Curtis</first><last>Langlotz</last></author>
      <pages>23-34</pages>
      <abstract>There is a growing need to model interactions between data modalities (e.g., vision, language) — both to improve AI predictions on existing tasks and to enable new applications. In the recent field of multimodal medical AI, integrating multiple modalities has gained widespread popularity as multimodal models have proven to improve performance, robustness, require less training samples and add complementary information. To improve technical reproducibility and transparency for multimodal medical tasks as well as speed up progress across medical AI, we present ViLMedic, a Vision-and-Language medical library. As of 2022, the library contains a dozen reference implementations replicating the state-of-the-art results for problems that range from medical visual question answering and radiology report generation to multimodal representation learning on widely adopted medical datasets. In addition, ViLMedic hosts a model-zoo with more than twenty pretrained models for the above tasks designed to be extensible by researchers but also simple for practitioners. Ultimately, we hope our reproducible pipelines can enable clinical translation and create real impact.The library is available at https://github.com/jbdel/vilmedic.</abstract>
      <url hash="8e1f32be">2022.acl-demo.3</url>
      <bibkey>delbrouck-etal-2022-vilmedic</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>T</fixed-case>ext<fixed-case>P</fixed-case>runer: A Model Pruning Toolkit for Pre-Trained Language Models</title>
      <author><first>Ziqing</first><last>Yang</last></author>
      <author><first>Yiming</first><last>Cui</last></author>
      <author><first>Zhigang</first><last>Chen</last></author>
      <pages>35-43</pages>
      <abstract>Pre-trained language models have been prevailed in natural language processing and become the backbones of many NLP tasks, but the demands for computational resources have limited their applications. In this paper, we introduce TextPruner, an open-source model pruning toolkit designed for pre-trained language models, targeting fast and easy model compression. TextPruner offers structured post-training pruning methods, including vocabulary pruning and transformer pruning, and can be applied to various models and tasks. We also propose a self-supervised pruning method that can be applied without the labeled data. Our experiments with several NLP tasks demonstrate the ability of TextPruner to reduce the model size without re-training the model.</abstract>
      <url hash="7e4f5bbf">2022.acl-demo.4</url>
      <bibkey>yang-etal-2022-textpruner</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>A</fixed-case>nn<fixed-case>IE</fixed-case>: An Annotation Platform for Constructing Complete Open Information Extraction Benchmark</title>
      <author><first>Niklas</first><last>Friedrich</last></author>
      <author><first>Kiril</first><last>Gashteovski</last></author>
      <author><first>Mingying</first><last>Yu</last></author>
      <author><first>Bhushan</first><last>Kotnis</last></author>
      <author><first>Carolin</first><last>Lawrence</last></author>
      <author><first>Mathias</first><last>Niepert</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>44-60</pages>
      <abstract>Open Information Extraction (OIE) is the task of extracting facts from sentences in the form of relations and their corresponding arguments in schema-free manner. Intrinsic performance of OIE systems is difficult to measure due to the incompleteness of existing OIE benchmarks: ground truth extractions do not group all acceptable surface realizations of the same fact that can be extracted from a sentence. To measure performance of OIE systems more realistically, it is necessary to manually annotate complete facts (i.e., clusters of all acceptable surface realizations of the same fact) from input sentences. We propose AnnIE: an interactive annotation platform that facilitates such challenging annotation tasks and supports creation of complete fact-oriented OIE evaluation benchmarks. AnnIE is modular and flexible in order to support different use case scenarios (i.e., benchmarks covering different types of facts) and different languages. We use AnnIE to build two complete OIE benchmarks: one with verb-mediated facts and another with facts encompassing named entities. We evaluate several OIE systems on our complete benchmarks created with AnnIE. We publicly release AnnIE (and all gold datasets generated with it) under non-restrictive license.</abstract>
      <url hash="4cb2acf6">2022.acl-demo.5</url>
      <bibkey>friedrich-etal-2022-annie</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>A</fixed-case>dapter<fixed-case>H</fixed-case>ub Playground: Simple and Flexible Few-Shot Learning with Adapters</title>
      <author><first>Tilman</first><last>Beck</last></author>
      <author><first>Bela</first><last>Bohlender</last></author>
      <author><first>Christina</first><last>Viehmann</last></author>
      <author><first>Vincent</first><last>Hane</last></author>
      <author><first>Yanik</first><last>Adamson</last></author>
      <author><first>Jaber</first><last>Khuri</last></author>
      <author><first>Jonas</first><last>Brossmann</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>61-75</pages>
      <abstract>The open-access dissemination of pretrained language models through online repositories has led to a democratization of state-of-the-art natural language processing (NLP) research.This also allows people outside of NLP to use such models and adapt them to specific use-cases.However, a certain amount of technical proficiency is still required which is an entry barrier for users who want to apply these models to a certain task but lack the necessary knowledge or resources.In this work, we aim to overcome this gap by providing a tool which allows researchers to leverage pretrained models without writing a single line of code.Built upon the parameter-efficient adapter modules for transfer learning, our AdapterHub Playground provides an intuitive interface, allowing the usage of adapters for prediction, training and analysis of textual data for a variety of NLP tasks.We present the tool’s architecture and demonstrate its advantages with prototypical use-cases, where we show that predictive performance can easily be increased in a few-shot learning scenario.Finally, we evaluate its usability in a user study.We provide the code and a live interface at https://adapter-hub.github.io/playground.</abstract>
      <url hash="33aba80a">2022.acl-demo.6</url>
      <bibkey>beck-etal-2022-adapterhub</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>Q</fixed-case>iu<fixed-case>N</fixed-case>iu: A <fixed-case>C</fixed-case>hinese Lyrics Generation System with Passage-Level Input</title>
      <author><first>Le</first><last>Zhang</last></author>
      <author><first>Rongsheng</first><last>Zhang</last></author>
      <author><first>Xiaoxi</first><last>Mao</last></author>
      <author><first>Yongzhu</first><last>Chang</last></author>
      <pages>76-82</pages>
      <abstract>Lyrics generation has been a very popular application of natural language generation. Previous works mainly focused on generating lyrics based on a couple of attributes or keywords, rendering very limited control over the content of the lyrics. In this paper, we demonstrate the QiuNiu, a Chinese lyrics generation system which is conditioned on passage-level text rather than a few attributes or keywords. By using the passage-level text as input, the content of generated lyrics is expected to reflect the nuances of users’ needs. The QiuNiu system supports various forms of passage-level input, such as short stories, essays, poetry. The training of it is conducted under the framework of unsupervised machine translation, due to the lack of aligned passage-level text-to-lyrics corpus. We initialize the parameters of QiuNiu with a custom pretrained Chinese GPT-2 model and adopt a two-step process to finetune the model for better alignment between passage-level text and lyrics. Additionally, a postprocess module is used to filter and rerank the generated lyrics to select the ones of highest quality. The demo video of the system is available at https://youtu.be/OCQNzahqWgM.</abstract>
      <url hash="14cdb00f">2022.acl-demo.7</url>
      <bibkey>zhang-etal-2022-qiuniu</bibkey>
    </paper>
    <paper id="8">
      <title>Automatic Gloss Dictionary for Sign Language Learners</title>
      <author><first>Chenchen</first><last>Xu</last></author>
      <author><first>Dongxu</first><last>Li</last></author>
      <author><first>Hongdong</first><last>Li</last></author>
      <author><first>Hanna</first><last>Suominen</last></author>
      <author><first>Ben</first><last>Swift</last></author>
      <pages>83-92</pages>
      <abstract>A multi-language dictionary is a fundamental tool for language learning, allowing the learner to look up unfamiliar words. Searching an unrecognized word in the dictionary does not usually require deep knowledge of the target language. However, this is not true for sign language, where gestural elements preclude this type of easy lookup. This paper introduces GlossFinder, an online tool supporting 2, 000 signs to assist language learners in determining the meaning of given signs. Unlike alternative systems of complex inputs, our system requires only that learners imitate the sign in front of a standard webcam. A user study conducted among sign language speakers of varying ability compared our system against existing alternatives and the interviews indicated a clear preference for our new system. This implies that GlossFinder can lower the barrier in sign language learning by addressing the common problem of sign finding and make it accessible to the wider community.</abstract>
      <url hash="eb23e040">2022.acl-demo.8</url>
      <bibkey>xu-etal-2022-automatic</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>S</fixed-case>ource: An Integrated Development Environment and Repository for Natural Language Prompts</title>
      <author><first>Stephen</first><last>Bach</last></author>
      <author><first>Victor</first><last>Sanh</last></author>
      <author><first>Zheng Xin</first><last>Yong</last></author>
      <author><first>Albert</first><last>Webson</last></author>
      <author><first>Colin</first><last>Raffel</last></author>
      <author><first>Nihal V.</first><last>Nayak</last></author>
      <author><first>Abheesht</first><last>Sharma</last></author>
      <author><first>Taewoon</first><last>Kim</last></author>
      <author><first>M Saiful</first><last>Bari</last></author>
      <author><first>Thibault</first><last>Fevry</last></author>
      <author><first>Zaid</first><last>Alyafeai</last></author>
      <author><first>Manan</first><last>Dey</last></author>
      <author><first>Andrea</first><last>Santilli</last></author>
      <author><first>Zhiqing</first><last>Sun</last></author>
      <author><first>Srulik</first><last>Ben-david</last></author>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Gunjan</first><last>Chhablani</last></author>
      <author><first>Han</first><last>Wang</last></author>
      <author><first>Jason</first><last>Fries</last></author>
      <author><first>Maged</first><last>Al-shaibani</last></author>
      <author><first>Shanya</first><last>Sharma</last></author>
      <author><first>Urmish</first><last>Thakker</last></author>
      <author><first>Khalid</first><last>Almubarak</last></author>
      <author><first>Xiangru</first><last>Tang</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Mike Tian-jian</first><last>Jiang</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>93-104</pages>
      <abstract>PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github.com/bigscience-workshop/promptsource.</abstract>
      <url hash="1e9e1db2">2022.acl-demo.9</url>
      <bibkey>bach-etal-2022-promptsource</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>O</fixed-case>pen<fixed-case>P</fixed-case>rompt: An Open-source Framework for Prompt-learning</title>
      <author><first>Ning</first><last>Ding</last></author>
      <author><first>Shengding</first><last>Hu</last></author>
      <author><first>Weilin</first><last>Zhao</last></author>
      <author><first>Yulin</first><last>Chen</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Haitao</first><last>Zheng</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>105-113</pages>
      <abstract>Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing prompt- learning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, verbalizing strategy, etc., that need to be considered in prompt-learning, practitioners face impediments to quickly adapting the de-sired prompt learning methods to their applications. In this paper, we present Open- Prompt, a unified easy-to-use toolkit to conduct prompt-learning over PLMs. OpenPrompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task for- mats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints.</abstract>
      <url hash="26d9deed">2022.acl-demo.10</url>
      <bibkey>ding-etal-2022-openprompt</bibkey>
    </paper>
    <paper id="11">
      <title>Guided K-best Selection for Semantic Parsing Annotation</title>
      <author><first>Anton</first><last>Belyy</last></author>
      <author><first>Chieh-yang</first><last>Huang</last></author>
      <author><first>Jacob</first><last>Andreas</last></author>
      <author><first>Emmanouil Antonios</first><last>Platanios</last></author>
      <author><first>Sam</first><last>Thomson</last></author>
      <author><first>Richard</first><last>Shin</last></author>
      <author><first>Subhro</first><last>Roy</last></author>
      <author><first>Aleksandr</first><last>Nisnevich</last></author>
      <author><first>Charles</first><last>Chen</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>114-126</pages>
      <abstract>Collecting data for conversational semantic parsing is a time-consuming and demanding process. In this paper we consider, given an incomplete dataset with only a small amount of data, how to build an AI-powered human-in-the-loop process to enable efficient data collection. A guided K-best selection process is proposed, which (i) generates a set of possible valid candidates; (ii) allows users to quickly traverse the set and filter incorrect parses; and (iii) asks users to select the correct parse, with minimal modification when necessary. We investigate how to best support users in efficiently traversing the candidate set and locating the correct parse, in terms of speed and accuracy. In our user study, consisting of five annotators labeling 300 instances each, we find that combining keyword searching, where keywords can be used to query relevant candidates, and keyword suggestion, where representative keywords are automatically generated, enables fast and accurate annotation.</abstract>
      <url hash="7d835507">2022.acl-demo.11</url>
      <bibkey>belyy-etal-2022-guided</bibkey>
    </paper>
    <paper id="12">
      <title>Hard and Soft Evaluation of <fixed-case>NLP</fixed-case> models with <fixed-case>BOO</fixed-case>t<fixed-case>ST</fixed-case>rap <fixed-case>SA</fixed-case>mpling - <fixed-case>B</fixed-case>oo<fixed-case>S</fixed-case>t<fixed-case>S</fixed-case>a</title>
      <author><first>Tommaso</first><last>Fornaciari</last></author>
      <author><first>Alexandra</first><last>Uma</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>127-134</pages>
      <abstract>Natural Language Processing (NLP) ‘s applied nature makes it necessary to select the most effective and robust models. Producing slightly higher performance is insufficient; we want to know whether this advantage will carry over to other data sets. Bootstrapped significance tests can indicate that ability.So while necessary, computing the significance of models’ performance differences has many levels of complexity. It can be tedious, especially when the experimental design has many conditions to compare and several runs of experiments.We present BooStSa, a tool that makes it easy to compute significance levels with the BOOtSTrap SAmpling procedure to evaluate models that predict not only standard hard labels but soft-labels (i.e., probability distributions over different classes) as well.</abstract>
      <url hash="d1bd1c89">2022.acl-demo.12</url>
      <bibkey>fornaciari-etal-2022-hard</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>COVID</fixed-case>-19 Claim Radar: A Structured Claim Extraction and Tracking System</title>
      <author><first>Manling</first><last>Li</last></author>
      <author><first>Revanth</first><last>Gangi Reddy</last></author>
      <author><first>Ziqi</first><last>Wang</last></author>
      <author><first>Yi-shyuan</first><last>Chiang</last></author>
      <author><first>Tuan</first><last>Lai</last></author>
      <author><first>Pengfei</first><last>Yu</last></author>
      <author><first>Zixuan</first><last>Zhang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>135-144</pages>
      <abstract>To tackle the challenge of accurate and timely communication regarding the COVID-19 pandemic, we present a COVID-19 Claim Radar to automatically extract supporting and refuting claims on a daily basis. We provide a comprehensive structured view of claims, including rich claim attributes (such as claimers and claimer affiliations) and associated knowledge elements as claim semantics (such as events, relations and entities), enabling users to explore equivalent, refuting, or supporting claims with structural evidence, such as shared claimers, similar centroid events and arguments. In order to consolidate claim structures at the corpus-level, we leverage Wikidata as the hub to merge coreferential knowledge elements. The system automatically provides users a comprehensive exposure to COVID-19 related claims, their importance, and their interconnections. The system is publicly available at GitHub and DockerHub, with complete documentation.</abstract>
      <url hash="7ea6244d">2022.acl-demo.13</url>
      <bibkey>li-etal-2022-covid</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>TS</fixed-case>-<fixed-case>ANNO</fixed-case>: An Annotation Tool to Build, Annotate and Evaluate Text Simplification Corpora</title>
      <author><first>Regina</first><last>Stodden</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <pages>145-155</pages>
      <abstract>We introduce TS-ANNO, an open-source web application for manual creation and for evaluation of parallel corpora for text simplification. TS-ANNO can be used for i) sentence–wise alignment, ii) rating alignment pairs (e.g., w.r.t. grammaticality, meaning preservation, ...), iii) annotating alignment pairs w.r.t. simplification transformations (e.g., lexical substitution, sentence splitting, ...), and iv) manual simplification of complex documents. For evaluation, TS-ANNO calculates inter-annotator agreement of alignments (i) and annotations (ii).</abstract>
      <url hash="a8b4ae72">2022.acl-demo.14</url>
      <bibkey>stodden-kallmeyer-2022-ts</bibkey>
    </paper>
    <paper id="15">
      <title>Language Diversity: Visible to Humans, Exploitable by Machines</title>
      <author><first>Gábor</first><last>Bella</last></author>
      <author><first>Erdenebileg</first><last>Byambadorj</last></author>
      <author><first>Yamini</first><last>Chandrashekar</last></author>
      <author><first>Khuyagbaatar</first><last>Batsuren</last></author>
      <author><first>Danish</first><last>Cheema</last></author>
      <author><first>Fausto</first><last>Giunchiglia</last></author>
      <pages>156-165</pages>
      <abstract>The Universal Knowledge Core (UKC) is a large multilingual lexical database with a focus on language diversity and covering over two thousand languages. The aim of the database, as well as its tools and data catalogue, is to make the abstract notion of linguistic diversity visually understandable for humans and formally exploitable by machines. The UKC website lets users explore millions of individual words and their meanings, but also phenomena of cross-lingual convergence and divergence, such as shared interlingual meanings, lexicon similarities, cognate clusters, or lexical gaps. The UKC LiveLanguage Catalogue, in turn, provides access to the underlying lexical data in a computer-processable form, ready to be reused in cross-lingual applications.</abstract>
      <url hash="3d19ef5d">2022.acl-demo.15</url>
      <bibkey>bella-etal-2022-language</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>C</fixed-case>og<fixed-case>KGE</fixed-case>: A Knowledge Graph Embedding Toolkit and Benchmark for Representing Multi-source and Heterogeneous Knowledge</title>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Tianyi</first><last>Men</last></author>
      <author><first>Hongbang</first><last>Yuan</last></author>
      <author><first>Zhitao</first><last>He</last></author>
      <author><first>Dianbo</first><last>Sui</last></author>
      <author><first>Chenhao</first><last>Wang</last></author>
      <author><first>Zhipeng</first><last>Xue</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>166-173</pages>
      <abstract>In this paper, we propose CogKGE, a knowledge graph embedding (KGE) toolkit, which aims to represent multi-source and heterogeneous knowledge. For multi-source knowledge, unlike existing methods that mainly focus on entity-centric knowledge, CogKGE also supports the representations of event-centric, commonsense and linguistic knowledge. For heterogeneous knowledge, besides structured triple facts, CogKGE leverages additional unstructured information, such as text descriptions, node types and temporal information, to enhance the meaning of embeddings. Designing CogKGE aims to provide a unified programming framework for KGE tasks and a series of knowledge representations for downstream tasks. As a research framework, CogKGE consists of five parts, including core, data, model, knowledge and adapter module. As a knowledge discovery toolkit, CogKGE provides pre-trained embedders to discover new facts, cluster entities and check facts. Furthermore, we construct two benchmark datasets for further research on multi-source heterogeneous KGE tasks: EventKG240K and CogNet360K. We also release an online system to discover knowledge visually. Source code, datasets and pre-trained embeddings are publicly available at GitHub, with a short instruction video.</abstract>
      <url hash="51dd6617">2022.acl-demo.16</url>
      <attachment type="OptionalSupplementaryMaterial" hash="7e447955">2022.acl-demo.16.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>jin-etal-2022-cogkge</bibkey>
    </paper>
    <paper id="17">
      <title>Dynatask: A Framework for Creating Dynamic <fixed-case>AI</fixed-case> Benchmark Tasks</title>
      <author><first>Tristan</first><last>Thrush</last></author>
      <author><first>Kushal</first><last>Tirumala</last></author>
      <author><first>Anmol</first><last>Gupta</last></author>
      <author><first>Max</first><last>Bartolo</last></author>
      <author><first>Pedro</first><last>Rodriguez</last></author>
      <author><first>Tariq</first><last>Kane</last></author>
      <author><first>William</first><last>Gaviria Rojas</last></author>
      <author><first>Peter</first><last>Mattson</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <pages>174-181</pages>
      <abstract>We introduce Dynatask: an open source system for setting up custom NLP tasks that aims to greatly lower the technical knowledge and effort required for hosting and evaluating state-of-the-art NLP models, as well as for conducting model in the loop data collection with crowdworkers. Dynatask is integrated with Dynabench, a research platform for rethinking benchmarking in AI that facilitates human and model in the loop data collection and evaluation. To create a task, users only need to write a short task configuration file from which the relevant web interfaces and model hosting infrastructure are automatically generated. The system is available at https://dynabench.org/ and the full library can be found at https://github.com/facebookresearch/dynabench.</abstract>
      <url hash="1a2dedca">2022.acl-demo.17</url>
      <bibkey>thrush-etal-2022-dynatask</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>D</fixed-case>ata<fixed-case>L</fixed-case>ab: A Platform for Data Analysis and Intervention</title>
      <author><first>Yang</first><last>Xiao</last></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Weizhe</first><last>Yuan</last></author>
      <author><first>Vijay</first><last>Viswanathan</last></author>
      <author><first>Zhoumianze</first><last>Liu</last></author>
      <author><first>Yixin</first><last>Liu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>182-195</pages>
      <abstract>Despite data’s crucial role in machine learning, most existing tools and research tend to focus on systems on top of existing data rather than how to interpret and manipulate data.In this paper, we propose DataLab, a unified data-oriented platform that not only allows users to interactively analyze the characteristics of data but also provides a standardized interface so that many data processing operations can be provided within a unified interface. Additionally, in view of the ongoing surge in the proliferation of datasets, DataLab has features for dataset recommendation and global vision analysis that help researchers form a better view of the data ecosystem. So far, DataLab covers 1,300 datasets and 3,583 of its transformed version, where 313 datasets support different types of analysis (e.g., with respect to gender bias) with the help of 119M samples annotated by 318 feature functions. DataLab is under active development and will be supported going forward. We have released a web platform, web API, Python SDK, and PyPI published package, which hopefully, can meet the diverse needs of researchers.</abstract>
      <url hash="02223ab3">2022.acl-demo.18</url>
      <bibkey>xiao-etal-2022-datalab</bibkey>
    </paper>
    <paper id="19">
      <title>Cue-bot: A Conversational Agent for Assistive Technology</title>
      <author><first>Shachi</first><last>H Kumar</last></author>
      <author><first>Hsuan</first><last>Su</last></author>
      <author><first>Ramesh</first><last>Manuvinakurike</last></author>
      <author><first>Maximilian</first><last>C Pinaroc</last></author>
      <author><first>Sai</first><last>Prasad</last></author>
      <author><first>Saurav</first><last>Sahay</last></author>
      <author><first>Lama</first><last>Nachman</last></author>
      <pages>196-203</pages>
      <abstract>Intelligent conversational assistants have become an integral part of our lives for performing simple tasks. However, such agents, for example, Google bots, Alexa and others are yet to have any social impact on minority population, for example, for people with neurological disorders and people with speech, language and social communication disorders, sometimes with locked-in states where speaking or typing is a challenge. Language model technologies can be very powerful tools in enabling these users to carry out daily communication and social interactions. In this work, we present a system that users with varied levels of disabilties can use to interact with the world, supported by eye-tracking, mouse controls and an intelligent agent Cue-bot, that can represent the user in a conversation. The agent provides relevant controllable ‘cues’ to generate desirable responses quickly for an ongoing dialog context. In the context of usage of such systems for people with degenerative disorders, we present automatic and human evaluation of our cue/keyword predictor and the controllable dialog system and show that our models perform significantly better than models without control and can also reduce user effort (fewer keystrokes) and speed up communication (typing time) significantly.</abstract>
      <url hash="4f4bc591">2022.acl-demo.19</url>
      <bibkey>h-kumar-etal-2022-cue</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>M</fixed-case>-<fixed-case>SENA</fixed-case>: An Integrated Platform for Multimodal Sentiment Analysis</title>
      <author><first>Huisheng</first><last>Mao</last></author>
      <author><first>Ziqi</first><last>Yuan</last></author>
      <author><first>Hua</first><last>Xu</last></author>
      <author><first>Wenmeng</first><last>Yu</last></author>
      <author><first>Yihe</first><last>Liu</last></author>
      <author><first>Kai</first><last>Gao</last></author>
      <pages>204-213</pages>
      <abstract>M-SENA is an open-sourced platform for Multimodal Sentiment Analysis. It aims to facilitate advanced research by providing flexible toolkits, reliable benchmarks, and intuitive demonstrations. The platform features a fully modular video sentiment analysis framework consisting of data management, feature extraction, model training, and result analysis modules. In this paper, we first illustrate the overall architecture of the M-SENA platform and then introduce features of the core modules. Reliable baseline results of different modality features and MSA benchmarks are also reported. Moreover, we use model evaluation and analysis tools provided by M-SENA to present intermediate representation visualization, on-the-fly instance test, and generalization ability test results. The source code of the platform is publicly available at https://github.com/thuiar/M-SENA.</abstract>
      <url hash="cf94fa6b">2022.acl-demo.20</url>
      <bibkey>mao-etal-2022-sena</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>HOSMEL</fixed-case>: A Hot-Swappable Modularized Entity Linking Toolkit for <fixed-case>C</fixed-case>hinese</title>
      <author><first>Daniel</first><last>Zhang-li</last></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Xiaokang</first><last>Zhang</last></author>
      <author><first>Peng</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Tang</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>214-223</pages>
      <abstract>We investigate the usage of entity linking (EL)in downstream tasks and present the first modularized EL toolkit for easy task adaptation. Different from the existing EL methods that dealwith all the features simultaneously, we modularize the whole model into separate parts witheach feature. This decoupled design enablesflexibly adding new features without retraining the whole model as well as flow visualization with better interpretability of the ELresult. We release the corresponding toolkit,HOSMEL, for Chinese, with three flexible usage modes, a live demo, and a demonstrationvideo. Experiments on two benchmarks forthe question answering task demonstrate thatHOSMEL achieves much less time and spaceconsumption as well as significantly better accuracy performance compared with existingSOTA EL methods. We hope the release ofHOSMEL will call for more attention to studyEL for downstream tasks in non-English languages.</abstract>
      <url hash="e8117316">2022.acl-demo.21</url>
      <bibkey>zhang-li-etal-2022-hosmel</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>BMI</fixed-case>nf: An Efficient Toolkit for Big Model Inference and Tuning</title>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Guoyang</first><last>Zeng</last></author>
      <author><first>Weilin</first><last>Zhao</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Zhengyan</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Jun</first><last>Zhang</last></author>
      <author><first>Jia</first><last>Chao</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>224-230</pages>
      <abstract>In recent years, large-scale pre-trained language models (PLMs) containing billions of parameters have achieved promising results on various NLP tasks. Although we can pre-train these big models by stacking computing clusters at any cost, it is impractical to use such huge computing resources to apply big models for each downstream task. To address the computation bottleneck encountered in deploying big models in real-world scenarios, we introduce an open-source toolkit for big model inference and tuning (BMInf), which can support big model inference and tuning at extremely low computation cost. More specifically, at the algorithm level, we introduce model quantization and parameter-efficient tuning for efficient model inference and tuning. At the implementation level, we apply model offloading, model checkpointing, and CPU-GPU scheduling optimization to further reduce the computation and memory cost of big models. Based on above efforts, we can efficiently perform big model inference and tuning with a single GPU (even a consumer-level GPU like GTX 1060) instead of computing clusters, which is difficult for existing distributed learning toolkits for PLMs. BMInf is publicly released at https://github.com/OpenBMB/BMInf.</abstract>
      <url hash="6d02c126">2022.acl-demo.22</url>
      <bibkey>han-etal-2022-bminf</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>MMEKG</fixed-case>: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities</title>
      <author><first>Yubo</first><last>Ma</last></author>
      <author><first>Zehao</first><last>Wang</last></author>
      <author><first>Mukai</first><last>Li</last></author>
      <author><first>Yixin</first><last>Cao</last></author>
      <author><first>Meiqi</first><last>Chen</last></author>
      <author><first>Xinze</first><last>Li</last></author>
      <author><first>Wenqi</first><last>Sun</last></author>
      <author><first>Kunquan</first><last>Deng</last></author>
      <author><first>Kun</first><last>Wang</last></author>
      <author><first>Aixin</first><last>Sun</last></author>
      <author><first>Jing</first><last>Shao</last></author>
      <pages>231-239</pages>
      <abstract>Events are fundamental building blocks of real-world happenings. In this paper, we present a large-scale, multi-modal event knowledge graph named MMEKG. MMEKG unifies different modalities of knowledge via events, which complement and disambiguate each other.Specifically, MMEKG incorporates (i) over 990 thousand concept events with 644 relation types to cover most types of happenings, and (ii) over 863 million instance events connected through 934 million relations, which provide rich contextual information in texts and/or images. To collect billion-scale instance events and relations among them, we additionally develop an efficient yet effective pipeline for textual/visual knowledge extraction system. We also develop an induction strategy to create million-scale concept events and a schema organizing all events and relations in MMEKG. To this end, we also provide a pipeline enabling our system to seamlessly parse texts/images to event graphs and to retrieve multi-modal knowledge at both concept- and instance-levels.</abstract>
      <url hash="174b5faa">2022.acl-demo.23</url>
      <bibkey>ma-etal-2022-mmekg</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>S</fixed-case>ocio<fixed-case>F</fixed-case>illmore: A Tool for Discovering Perspectives</title>
      <author><first>Gosse</first><last>Minnema</last></author>
      <author><first>Sara</first><last>Gemelli</last></author>
      <author><first>Chiara</first><last>Zanchi</last></author>
      <author><first>Tommaso</first><last>Caselli</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>240-250</pages>
      <abstract>SOCIOFILLMORE is a multilingual tool which helps to bring to the fore the focus or the perspective that a text expresses in depicting an event. Our tool, whose rationale we also support through a large collection of human judgements, is theoretically grounded on frame semantics and cognitive linguistics, and implemented using the LOME frame semantic parser. We describe SOCIOFILLMORE’s development and functionalities, show how non-NLP researchers can easily interact with the tool, and present some example case studies which are already incorporated in the system, together with the kind of analysis that can be visualised.</abstract>
      <url hash="e9bb9382">2022.acl-demo.24</url>
      <bibkey>minnema-etal-2022-sociofillmore</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>T</fixed-case>ime<fixed-case>LM</fixed-case>s: Diachronic Language Models from <fixed-case>T</fixed-case>witter</title>
      <author><first>Daniel</first><last>Loureiro</last></author>
      <author><first>Francesco</first><last>Barbieri</last></author>
      <author><first>Leonardo</first><last>Neves</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <author><first>Jose</first><last>Camacho-collados</last></author>
      <pages>251-260</pages>
      <abstract>Despite its importance, the time variable has been largely neglected in the NLP and language model literature. In this paper, we present TimeLMs, a set of language models specialized on diachronic Twitter data. We show that a continual learning strategy contributes to enhancing Twitter-based language models’ capacity to deal with future and out-of-distribution tweets, while making them competitive with standardized and more monolithic benchmarks. We also perform a number of qualitative analyses showing how they cope with trends and peaks in activity involving specific named entities or concept drift. TimeLMs is available at github.com/cardiffnlp/timelms.</abstract>
      <url hash="f6ee4ecd">2022.acl-demo.25</url>
      <bibkey>loureiro-etal-2022-timelms</bibkey>
    </paper>
    <paper id="26">
      <title>Adaptor: Objective-Centric Adaptation Framework for Language Models</title>
      <author><first>Michal</first><last>Štefánik</last></author>
      <author><first>Vít</first><last>Novotný</last></author>
      <author><first>Nikola</first><last>Groverová</last></author>
      <author><first>Petr</first><last>Sojka</last></author>
      <pages>261-269</pages>
      <abstract>This paper introduces Adaptor library, which transposes traditional model-centric approach composed of pre-training + fine-tuning steps to objective-centric approach, composing the training process by applications of selected objectives.We survey research directions that can benefit from enhanced objective-centric experimentation in multitask training, custom objectives development, dynamic training curricula, or domain adaptation.Adaptor aims to ease reproducibility of these research directions in practice. Finally, we demonstrate the practical applicability of Adaptor in selected unsupervised domain adaptation scenarios.</abstract>
      <url hash="48760f05">2022.acl-demo.26</url>
      <bibkey>stefanik-etal-2022-adaptor</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>Q</fixed-case>uick<fixed-case>G</fixed-case>raph: A Rapid Annotation Tool for Knowledge Graph Extraction from Technical Text</title>
      <author><first>Tyler</first><last>Bikaun</last></author>
      <author><first>Michael</first><last>Stewart</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <pages>270-278</pages>
      <abstract>Acquiring high-quality annotated corpora for complex multi-task information extraction (MT-IE) is an arduous and costly process for human-annotators. Adoption of unsupervised techniques for automated annotation have thus become popular. However, these techniques rely heavily on dictionaries, gazetteers, and knowledge bases. While such resources are abundant for general domains, they are scarce for specialised technical domains. To tackle this challenge, we present QuickGraph, the first collaborative MT-IE annotation tool built with indirect weak supervision and clustering to maximise annotator productivity.QuickGraph’s main contribution is a set of novel features that enable knowledge graph extraction through rapid and consistent complex multi-task entity and relation annotation. In this paper, we discuss these key features and qualitatively compare QuickGraph to existing annotation tools.</abstract>
      <url hash="fb186997">2022.acl-demo.27</url>
      <bibkey>bikaun-etal-2022-quickgraph</bibkey>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2022-05-11">
    <meta>
      <booktitle>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</booktitle>
      <editor><first>Luciana</first><last>Benotti</last></editor>
      <editor><first>Naoaki</first><last>Okazaki</last></editor>
      <editor><first>Yves</first><last>Scherrer</last></editor>
      <editor><first>Marcos</first><last>Zampieri</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="b9934aae">2022.acl-tutorials</url>
    </meta>
    <frontmatter>
      <url hash="b9934aae">2022.acl-tutorials.0</url>
      <bibkey>acl-2022-association-linguistics-tutorial</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Gentle Introduction to Deep Nets and Opportunities for the Future</title>
      <author><first>Kenneth</first><last>Church</last></author>
      <author><first>Valia</first><last>Kordoni</last></author>
      <author><first>Gary</first><last>Marcus</last></author>
      <author><first>Ernest</first><last>Davis</last></author>
      <author><first>Yanjun</first><last>Ma</last></author>
      <author><first>Zeyu</first><last>Chen</last></author>
      <pages>1-6</pages>
      <abstract>The first half of this tutorial will make deep nets more accessible to a broader audience, following “Deep Nets for Poets” and “A Gentle Introduction to Fine-Tuning.” We will also introduce GFT (general fine tuning), a little language for fine tuning deep nets with short (one line) programs that are as easy to code as regression in statistics packages such as R using glm (general linear models). Based on the success of these methods on a number of benchmarks, one might come away with the impression that deep nets are all we need. However, we believe the glass is half-full: while there is much that can be done with deep nets, there is always more to do. The second half of this tutorial will discuss some of these opportunities.</abstract>
      <url hash="d991f392">2022.acl-tutorials.1</url>
      <bibkey>church-etal-2022-gentle</bibkey>
    </paper>
    <paper id="2">
      <title>Towards Reproducible Machine Learning Research in Natural Language Processing</title>
      <author><first>Ana</first><last>Lucic</last></author>
      <author><first>Maurits</first><last>Bleeker</last></author>
      <author><first>Samarth</first><last>Bhargav</last></author>
      <author><first>Jessica</first><last>Forde</last></author>
      <author><first>Koustuv</first><last>Sinha</last></author>
      <author><first>Jesse</first><last>Dodge</last></author>
      <author><first>Sasha</first><last>Luccioni</last></author>
      <author><first>Robert</first><last>Stojnic</last></author>
      <pages>7-11</pages>
      <abstract>While recent progress in the field of ML has been significant, the reproducibility of these cutting-edge results is often lacking, with many submissions lacking the necessary information in order to ensure subsequent reproducibility. Despite proposals such as the Reproducibility Checklist and reproducibility criteria at several major conferences, the reflex for carrying out research with reproducibility in mind is lacking in the broader ML community. We propose this tutorial as a gentle introduction to ensuring reproducible research in ML, with a specific emphasis on computational linguistics and NLP. We also provide a framework for using reproducibility as a teaching tool in university-level computer science programs.</abstract>
      <url hash="06b2f824">2022.acl-tutorials.2</url>
      <bibkey>lucic-etal-2022-towards</bibkey>
    </paper>
    <paper id="3">
      <title>Knowledge-Augmented Methods for Natural Language Processing</title>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Bill</first><last>Lin</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <author><first>Wenhao</first><last>Yu</last></author>
      <pages>12-20</pages>
      <abstract>Knowledge in natural language processing (NLP) has been a rising trend especially after the advent of large scale pre-trained models. NLP models with attention to knowledge can i) access unlimited amount of external information; ii) delegate the task of storing knowledge from its parameter space to knowledge sources; iii) obtain up-to-date information; iv) make prediction results more explainable via selected knowledge. In this tutorial, we will introduce the key steps in integrating knowledge into NLP, including knowledge grounding from text, knowledge representation and fusing. In addition, we will introduce recent state-of-the-art applications in fusing knowledge into language understanding, language generation and commonsense reasoning.</abstract>
      <url hash="7b24bb5a">2022.acl-tutorials.3</url>
      <bibkey>zhu-etal-2022-knowledge</bibkey>
    </paper>
    <paper id="4">
      <title>Non-Autoregressive Sequence Generation</title>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Xu</first><last>Tan</last></author>
      <pages>21-27</pages>
      <abstract>Non-autoregressive sequence generation (NAR) attempts to generate the entire or partial output sequences in parallel to speed up the generation process and avoid potential issues (e.g., label bias, exposure bias) in autoregressive generation. While it has received much research attention and has been applied in many sequence generation tasks in natural language and speech, naive NAR models still face many challenges to close the performance gap between state-of-the-art autoregressive models because of a lack of modeling power. In this tutorial, we will provide a thorough introduction and review of non-autoregressive sequence generation, in four sections: 1) Background, which covers the motivation of NAR generation, the problem definition, the evaluation protocol, and the comparison with standard autoregressive generation approaches. 2) Method, which includes different aspects: model architecture, objective function, training data, learning paradigm, and additional inference tricks. 3) Application, which covers different tasks in text and speech generation, and some advanced topics in applications. 4) Conclusion, in which we describe several research challenges and discuss the potential future research directions. We hope this tutorial can serve both academic researchers and industry practitioners working on non-autoregressive sequence generation.</abstract>
      <url hash="a91a6086">2022.acl-tutorials.4</url>
      <bibkey>gu-tan-2022-non</bibkey>
    </paper>
    <paper id="5">
      <title>Learning with Limited Text Data</title>
      <author><first>Diyi</first><last>Yang</last></author>
      <author><first>Ankur</first><last>Parikh</last></author>
      <author><first>Colin</first><last>Raffel</last></author>
      <pages>28-31</pages>
      <abstract>Natural Language Processing (NLP) has achieved great progress in the past decade on the basis of neural models, which often make use of large amounts of labeled data to achieve state-of-the-art performance. The dependence on labeled data prevents NLP models from being applied to low-resource settings and languages because of the time, money, and expertise that is often required to label massive amounts of textual data. Consequently, the ability to learn with limited labeled data is crucial for deploying neural systems to real-world NLP applications. Recently, numerous approaches have been explored to alleviate the need for labeled data in NLP such as data augmentation and semi-supervised learning. This tutorial aims to provide a systematic and up-to-date overview of these methods in order to help researchers and practitioners understand the landscape of approaches and the challenges associated with learning from limited labeled data, an emerging topic in the computational linguistics community. We will consider applications to a wide variety of NLP tasks (including text classification, generation, and structured prediction) and will highlight current challenges and future directions.</abstract>
      <url hash="fe4ec2d3">2022.acl-tutorials.5</url>
      <bibkey>yang-etal-2022-learning</bibkey>
    </paper>
    <paper id="6">
      <title>Zero- and Few-Shot <fixed-case>NLP</fixed-case> with Pretrained Language Models</title>
      <author><first>Iz</first><last>Beltagy</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <author><first>Robert</first><last>Logan IV</last></author>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>32-37</pages>
      <abstract>The ability to efficiently learn from little-to-no data is critical to applying NLP to tasks where data collection is costly or otherwise difficult. This is a challenging setting both academically and practically—particularly because training neutral models typically require large amount of labeled data. More recently, advances in pretraining on unlabelled data have brought up the potential of better zero-shot or few-shot learning (Devlin et al., 2019; Brown et al., 2020). In particular, over the past year, a great deal of research has been conducted to better learn from limited data using large-scale language models. In this tutorial, we aim at bringing interested NLP researchers up to speed about the recent and ongoing techniques for zero- and few-shot learning with pretrained language models. Additionally, our goal is to reveal new research opportunities to the audience, which will hopefully bring us closer to address existing challenges in this domain.</abstract>
      <url hash="2e1a3511">2022.acl-tutorials.6</url>
      <bibkey>beltagy-etal-2022-zero</bibkey>
    </paper>
    <paper id="7">
      <title>Vision-Language Pretraining: Current Trends and the Future</title>
      <author><first>Aishwarya</first><last>Agrawal</last></author>
      <author><first>Damien</first><last>Teney</last></author>
      <author><first>Aida</first><last>Nematzadeh</last></author>
      <pages>38-43</pages>
      <abstract>In the last few years, there has been an increased interest in building multimodal (vision-language) models that are pretrained on larger but noisier datasets where the two modalities (e.g., image and text) loosely correspond to each other (e.g., Lu et al., 2019; Radford et al., 2021). Given a task (such as visual question answering), these models are then often fine-tuned on task-specific supervised datasets. (e.g., Lu et al., 2019; Chen et al.,2020; Tan and Bansal, 2019; Li et al., 2020a,b). In addition to the larger pretraining datasets, the transformer architecture (Vaswani et al., 2017) and in particular self-attention applied to two modalities are responsible for the impressive performance of the recent pretrained models on downstream tasks (Hendricks et al., 2021). In this tutorial, we focus on recent vision-language pretraining paradigms. Our goal is to first provide the background on image–language datasets, benchmarks, and modeling innovations before the multimodal pretraining area. Next we discuss the different family of models used for vision-language pretraining, highlighting their strengths and shortcomings. Finally, we discuss the limits of vision-language pretraining through statistical learning, and the need for alternative approaches such as causal representation learning.</abstract>
      <url hash="883f682f">2022.acl-tutorials.7</url>
      <bibkey>agrawal-etal-2022-vision</bibkey>
    </paper>
    <paper id="8">
      <title>Natural Language Processing for Multilingual Task-Oriented Dialogue</title>
      <author><first>Evgeniia</first><last>Razumovskaia</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <author><first>Olga</first><last>Majewska</last></author>
      <author><first>Edoardo</first><last>Ponti</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <pages>44-50</pages>
      <abstract>Recent advances in deep learning have also enabled fast progress in the research of task-oriented dialogue (ToD) systems. However, the majority of ToD systems are developed for English and merely a handful of other widely spoken languages, e.g., Chinese and German. This hugely limits the global reach and, consequently, transformative socioeconomic potential of such systems. In this tutorial, we will thus discuss and demonstrate the importance of (building) multilingual ToD systems, and then provide a systematic overview of current research gaps, challenges and initiatives related to multilingual ToD systems, with a particular focus on their connections to current research and challenges in multilingual and low-resource NLP. The tutorial will aim to provide answers or shed new light to the following questions: a) Why are multilingual dialogue systems so hard to build: what makes multilinguality for dialogue more challenging than for other NLP applications and tasks? b) What are the best existing methods and datasets for multilingual and cross-lingual (task-oriented) dialog systems? How are (multilingual) ToD systems usually evaluated? c) What are the promising future directions for multilingual ToD research: where can one draw inspiration from related NLP areas and tasks?</abstract>
      <url hash="c8809ed6">2022.acl-tutorials.8</url>
      <bibkey>razumovskaia-etal-2022-natural</bibkey>
    </paper>
  </volume>
</collection>
