<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.challengehml">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)</booktitle>
      <editor><first>Amir</first><last>Zadeh</last></editor>
      <editor><first>Louis-Philippe</first><last>Morency</last></editor>
      <editor><first>Paul Pu</first><last>Liang</last></editor>
      <editor><first>Soujanya</first><last>Poria</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, USA</address>
      <month>July</month>
      <year>2020</year>
      <url hash="ce85a8c6">2020.challengehml-1</url>
      <venue>challengehml</venue>
    </meta>
    <frontmatter>
      <url hash="1197a09f">2020.challengehml-1.0</url>
      <bibkey>challenge-hml-2020-grand</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis</title>
      <author><first>Jean-Benoit</first><last>Delbrouck</last></author>
      <author><first>Noé</first><last>Tits</last></author>
      <author><first>Mathilde</first><last>Brousmiche</last></author>
      <author><first>Stéphane</first><last>Dupont</last></author>
      <pages>1–7</pages>
      <abstract>Understanding expressed sentiment and emotions are two crucial factors in human multimodal language. This paper describes a Transformer-based joint-encoding (TBJE) for the task of Emotion Recognition and Sentiment Analysis. In addition to use the Transformer architecture, our approach relies on a modular co-attention and a glimpse layer to jointly encode one or more modalities. The proposed solution has also been submitted to the ACL20: Second Grand-Challenge on Multimodal Language to be evaluated on the CMU-MOSEI dataset. The code to replicate the presented experiments is open-source .</abstract>
      <url hash="f77d0be9">2020.challengehml-1.1</url>
      <doi>10.18653/v1/2020.challengehml-1.1</doi>
      <video href="http://slideslive.com/38931263"/>
      <bibkey>delbrouck-etal-2020-transformer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
    </paper>
    <paper id="2">
      <title>A Multi-modal Approach to Fine-grained Opinion Mining on Video Reviews</title>
      <author><first>Edison</first><last>Marrese-Taylor</last></author>
      <author><first>Cristian</first><last>Rodriguez</last></author>
      <author><first>Jorge</first><last>Balazs</last></author>
      <author><first>Stephen</first><last>Gould</last></author>
      <author><first>Yutaka</first><last>Matsuo</last></author>
      <pages>8–18</pages>
      <abstract>Despite the recent advances in opinion mining for written reviews, few works have tackled the problem on other sources of reviews. In light of this issue, we propose a multi-modal approach for mining fine-grained opinions from video reviews that is able to determine the aspects of the item under review that are being discussed and the sentiment orientation towards them. Our approach works at the sentence level without the need for time annotations and uses features derived from the audio, video and language transcriptions of its contents.We evaluate our approach on two datasets and show that leveraging the video and audio modalities consistently provides increased performance over text-only baselines, providing evidence these extra modalities are key in better understanding video reviews.</abstract>
      <url hash="40d762b5">2020.challengehml-1.2</url>
      <doi>10.18653/v1/2020.challengehml-1.2</doi>
      <video href="http://slideslive.com/38931259"/>
      <bibkey>marrese-taylor-etal-2020-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/youtubean">Youtubean</pwcdataset>
    </paper>
    <paper id="3">
      <title>Multilogue-Net: A Context-Aware <fixed-case>RNN</fixed-case> for Multi-modal Emotion Detection and Sentiment Analysis in Conversation</title>
      <author><first>Aman</first><last>Shenoy</last></author>
      <author><first>Ashish</first><last>Sardana</last></author>
      <pages>19–28</pages>
      <abstract>Sentiment Analysis and Emotion Detection in conversation is key in several real-world applications, with an increase in modalities available aiding a better understanding of the underlying emotions. Multi-modal Emotion Detection and Sentiment Analysis can be particularly useful, as applications will be able to use specific subsets of available modalities, as per the available data. Current systems dealing with Multi-modal functionality fail to leverage and capture - the context of the conversation through all modalities, the dependency between the listener(s) and speaker emotional states, and the relevance and relationship between the available modalities. In this paper, we propose an end to end RNN architecture that attempts to take into account all the mentioned drawbacks. Our proposed model, at the time of writing, out-performs the state of the art on a benchmark dataset on a variety of accuracy and regression metrics.</abstract>
      <url hash="0d7550f4">2020.challengehml-1.3</url>
      <doi>10.18653/v1/2020.challengehml-1.3</doi>
      <video href="http://slideslive.com/38931258"/>
      <bibkey>shenoy-sardana-2020-multilogue</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity">Multimodal Opinionlevel Sentiment Intensity</pwcdataset>
    </paper>
    <paper id="4">
      <title>Low Rank Fusion based Transformers for Multimodal Sequences</title>
      <author><first>Saurav</first><last>Sahay</last></author>
      <author><first>Eda</first><last>Okur</last></author>
      <author><first>Shachi</first><last>H Kumar</last></author>
      <author><first>Lama</first><last>Nachman</last></author>
      <pages>29–34</pages>
      <abstract>Our senses individually work in a coordinated fashion to express our emotional intentions. In this work, we experiment with modeling modality-specific sensory signals to attend to our latent multimodal emotional intentions and vice versa expressed via low-rank multimodal fusion and multimodal transformers. The low-rank factorization of multimodal fusion amongst the modalities helps represent approximate multiplicative latent signal interactions. Motivated by the work of~(CITATION) and~(CITATION), we present our transformer-based cross-fusion architecture without any over-parameterization of the model. The low-rank fusion helps represent the latent signal interactions while the modality-specific attention helps focus on relevant parts of the signal. We present two methods for the Multimodal Sentiment and Emotion Recognition results on CMU-MOSEI, CMU-MOSI, and IEMOCAP datasets and show that our models have lesser parameters, train faster and perform comparably to many larger fusion-based architectures.</abstract>
      <url hash="943959bc">2020.challengehml-1.4</url>
      <doi>10.18653/v1/2020.challengehml-1.4</doi>
      <video href="http://slideslive.com/38931264"/>
      <bibkey>sahay-etal-2020-low</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="5">
      <title>Unsupervised Online Grounding of Natural Language during Human-Robot Interactions</title>
      <author><first>Oliver</first><last>Roesler</last></author>
      <pages>35–45</pages>
      <abstract>Allowing humans to communicate through natural language with robots requires connections between words and percepts. The process of creating these connections is called symbol grounding and has been studied for nearly three decades. Although many studies have been conducted, not many considered grounding of synonyms and the employed algorithms either work only offline or in a supervised manner. In this paper, a cross-situational learning based grounding framework is proposed that allows grounding of words and phrases through corresponding percepts without human supervision and online, i.e. it does not require any explicit training phase, but instead updates the obtained mappings for every new encountered situation. The proposed framework is evaluated through an interaction experiment between a human tutor and a robot, and compared to an existing unsupervised grounding framework. The results show that the proposed framework is able to ground words through their corresponding percepts online and in an unsupervised manner, while outperforming the baseline framework.</abstract>
      <url hash="adf980b7">2020.challengehml-1.5</url>
      <doi>10.18653/v1/2020.challengehml-1.5</doi>
      <video href="http://slideslive.com/38931262"/>
      <bibkey>roesler-2020-unsupervised</bibkey>
    </paper>
    <paper id="6">
      <title>Leveraging Multimodal Behavioral Analytics for Automated Job Interview Performance Assessment and Feedback</title>
      <author><first>Anumeha</first><last>Agrawal</last></author>
      <author><first>Rosa</first><last>Anil George</last></author>
      <author><first>Selvan Sunitha</first><last>Ravi</last></author>
      <author><first>Sowmya</first><last>Kamath S</last></author>
      <author><first>Anand</first><last>Kumar</last></author>
      <pages>46–54</pages>
      <abstract>Behavioral cues play a significant part in human communication and cognitive perception. In most professional domains, employee recruitment policies are framed such that both professional skills and personality traits are adequately assessed. Hiring interviews are structured to evaluate expansively a potential employee’s suitability for the position - their professional qualifications, interpersonal skills, ability to perform in critical and stressful situations, in the presence of time and resource constraints, etc. Candidates, therefore, need to be aware of their positive and negative attributes and be mindful of behavioral cues that might have adverse effects on their success. We propose a multimodal analytical framework that analyzes the candidate in an interview scenario and provides feedback for predefined labels such as engagement, speaking rate, eye contact, etc. We perform a comprehensive analysis that includes the interviewee’s facial expressions, speech, and prosodic information, using the video, audio, and text transcripts obtained from the recorded interview. We use these multimodal data sources to construct a composite representation, which is used for training machine learning classifiers to predict the class labels. Such analysis is then used to provide constructive feedback to the interviewee for their behavioral cues and body language. Experimental validation showed that the proposed methodology achieved promising results.</abstract>
      <url hash="f576ea53">2020.challengehml-1.6</url>
      <doi>10.18653/v1/2020.challengehml-1.6</doi>
      <video href="http://slideslive.com/38931260"/>
      <bibkey>agrawal-etal-2020-leveraging</bibkey>
    </paper>
    <paper id="7">
      <title>Audio-Visual Understanding of Passenger Intents for In-Cabin Conversational Agents</title>
      <author><first>Eda</first><last>Okur</last></author>
      <author><first>Shachi</first><last>H Kumar</last></author>
      <author><first>Saurav</first><last>Sahay</last></author>
      <author><first>Lama</first><last>Nachman</last></author>
      <pages>55–59</pages>
      <abstract>Building multimodal dialogue understanding capabilities situated in the in-cabin context is crucial to enhance passenger comfort in autonomous vehicle (AV) interaction systems. To this end, understanding passenger intents from spoken interactions and vehicle vision systems is an important building block for developing contextual and visually grounded conversational agents for AV. Towards this goal, we explore AMIE (Automated-vehicle Multimodal In-cabin Experience), the in-cabin agent responsible for handling multimodal passenger-vehicle interactions. In this work, we discuss the benefits of multimodal understanding of in-cabin utterances by incorporating verbal/language input together with the non-verbal/acoustic and visual input from inside and outside the vehicle. Our experimental results outperformed text-only baselines as we achieved improved performances for intent detection with multimodal approach.</abstract>
      <url hash="f8016049">2020.challengehml-1.7</url>
      <doi>10.18653/v1/2020.challengehml-1.7</doi>
      <video href="http://slideslive.com/38931265"/>
      <bibkey>okur-etal-2020-audio</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>AI</fixed-case> <fixed-case>S</fixed-case>ensing for Robotics using Deep Learning based Visual and Language Modeling</title>
      <author><first>Yuvaram</first><last>Singh</last></author>
      <author><first>Kameshwar Rao</first><last>JV</last></author>
      <pages>60–63</pages>
      <abstract>An artificial intelligence(AI) system should be capable of processing the sensory inputs to extract both task-specific and general information about its environment. However, most of the existing algorithms extract only task specific information. In this work, an innovative approach to address the problem of processing visual sensory data is presented by utilizing convolutional neural network (CNN). It recognizes and represents the physical and semantic nature of the surrounding in both human readable and machine processable format. This work utilizes the image captioning model to capture the semantics of the input image and a modular design to generate a probability distribution for semantic topics. It gives any autonomous system the ability to process visual information in a human-like way and generates more insights which are hardly possible with a conventional algorithm. Here a model and data collection method are proposed.</abstract>
      <url hash="2e4886a0">2020.challengehml-1.8</url>
      <doi>10.18653/v1/2020.challengehml-1.8</doi>
      <video href="http://slideslive.com/38931257"/>
      <bibkey>singh-jv-2020-ai</bibkey>
    </paper>
    <paper id="9">
      <title>Exploring Weaknesses of <fixed-case>VQA</fixed-case> Models through Attribution Driven Insights</title>
      <author><first>Shaunak</first><last>Halbe</last></author>
      <pages>64–68</pages>
      <abstract>Deep Neural Networks have been successfully used for the task of Visual Question Answering for the past few years owing to the availability of relevant large scale datasets. However these datasets are created in artificial settings and rarely reflect the real world scenario. Recent research effectively applies these VQA models for answering visual questions for the blind. Despite achieving high accuracy these models appear to be susceptible to variation in input questions.We analyze popular VQA models through the lens of attribution (input’s influence on predictions) to gain valuable insights. Further, We use these insights to craft adversarial attacks which inflict significant damage to these systems with negligible change in meaning of the input questions. We believe this will enhance development of systems more robust to the possible variations in inputs when deployed to assist the visually impaired.</abstract>
      <url hash="1d5e491e">2020.challengehml-1.9</url>
      <doi>10.18653/v1/2020.challengehml-1.9</doi>
      <bibkey>halbe-2020-exploring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vizwiz">VizWiz</pwcdataset>
    </paper>
  </volume>
</collection>
