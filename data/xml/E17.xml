<?xml version='1.0' encoding='UTF-8'?>
<collection id="E17">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 15th Conference of the <fixed-case>E</fixed-case>uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</booktitle>
      <url hash="5a53410a">E17-1</url>
      <editor><first>Mirella</first><last>Lapata</last></editor>
      <editor><first>Phil</first><last>Blunsom</last></editor>
      <editor><first>Alexander</first><last>Koller</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Valencia, Spain</address>
      <month>April</month>
      <year>2017</year>
      <venue>eacl</venue>
    </meta>
    <frontmatter>
      <url hash="d66936a7">E17-1000</url>
      <bibkey>eacl-2017-european</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Gated End-to-End Memory Networks</title>
      <author id="fei-liu-unimelb"><first>Fei</first><last>Liu</last></author>
      <author><first>Julien</first><last>Perez</last></author>
      <pages>1–10</pages>
      <url hash="ab4ecb33">E17-1001</url>
      <abstract>Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks (MemN2N) have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture (GMemN2N). From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the Dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art.</abstract>
      <bibkey>liu-perez-2017-gated</bibkey>
    </paper>
    <paper id="2">
      <title>Neural Tree Indexers for Text Understanding</title>
      <author><first>Tsendsuren</first><last>Munkhdalai</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>11–21</pages>
      <url hash="c518d088">E17-1002</url>
      <abstract>Recurrent neural networks (RNNs) process input text sequentially and model the conditional transition between word tokens. In contrast, the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language. However, the current recursive architecture is limited by its dependence on syntactic tree. In this paper, we introduce a robust syntactic parsing-independent tree structured model, Neural Tree Indexers (NTI) that provides a middle ground between the sequential RNNs and the syntactic treebased recursive models. NTI constructs a full n-ary tree by processing the input text with its node function in a bottom-up fashion. Attention mechanism can then be applied to both structure and node function. We implemented and evaluated a binary tree model of NTI, showing the model achieved the state-of-the-art performance on three different NLP tasks: natural language inference, answer sentence selection, and sentence classification, outperforming state-of-the-art recurrent and recursive neural networks.</abstract>
      <bibkey>munkhdalai-yu-2017-neural</bibkey>
      <pwccode url="https://bitbucket.org/tsendeemts/nti" additional="false">tsendeemts/nti</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="3">
      <title>Exploring Different Dimensions of Attention for Uncertainty Detection</title>
      <author><first>Heike</first><last>Adel</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>22–34</pages>
      <url hash="d8c66eaa">E17-1003</url>
      <abstract>Neural networks with attention have proven effective for many natural language processing tasks. In this paper, we develop attention mechanisms for uncertainty detection. In particular, we generalize standardly used attention mechanisms by introducing external attention and sequence-preserving attention. These novel architectures differ from standard approaches in that they use external resources to compute attention weights and preserve sequence information. We compare them to other configurations along different dimensions of attention. Our novel architectures set the new state of the art on a Wikipedia benchmark dataset and perform similar to the state-of-the-art model on a biomedical benchmark which uses a large set of linguistic features.</abstract>
      <bibkey>adel-schutze-2017-exploring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="4">
      <title>Classifying Illegal Activities on Tor Network Based on Web Textual Contents</title>
      <author><first>Mhd Wesam</first><last>Al Nabki</last></author>
      <author><first>Eduardo</first><last>Fidalgo</last></author>
      <author><first>Enrique</first><last>Alegre</last></author>
      <author><first>Ivan</first><last>de Paz</last></author>
      <pages>35–43</pages>
      <url hash="f5d465e6">E17-1004</url>
      <abstract>The freedom of the Deep Web offers a safe place where people can express themselves anonymously but they also can conduct illegal activities. In this paper, we present and make publicly available a new dataset for Darknet active domains, which we call ”Darknet Usage Text Addresses” (DUTA). We built DUTA by sampling the Tor network during two months and manually labeled each address into 26 classes. Using DUTA, we conducted a comparison between two well-known text representation techniques crossed by three different supervised classifiers to categorize the Tor hidden services. We also fixed the pipeline elements and identified the aspects that have a critical influence on the classification results. We found that the combination of TFIDF words representation with Logistic Regression classifier achieves 96.6% of 10 folds cross-validation accuracy and a macro F1 score of 93.7% when classifying a subset of illegal activities from DUTA. The good performance of the classifier might support potential tools to help the authorities in the detection of these activities.</abstract>
      <bibkey>al-nabki-etal-2017-classifying</bibkey>
    </paper>
    <paper id="5">
      <title>When is multitask learning effective? Semantic sequence prediction under varying data conditions</title>
      <author><first>Héctor</first><last>Martínez Alonso</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>44–53</pages>
      <url hash="d6595583">E17-1005</url>
      <abstract>Multitask learning has been applied successfully to a range of tasks, mostly morphosyntactic. However, little is known on <i>when</i> MTL works and whether there are data characteristics that help to determine the success of MTL. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary task configurations, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 tasks. When successful, auxiliary tasks with compact and more uniform label distributions are preferable.</abstract>
      <bibkey>martinez-alonso-plank-2017-multitask</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="6">
      <title>Learning Compositionality Functions on Word Embeddings for Modelling Attribute Meaning in Adjective-Noun Phrases</title>
      <author><first>Matthias</first><last>Hartung</last></author>
      <author><first>Fabian</first><last>Kaupmann</last></author>
      <author><first>Soufian</first><last>Jebbara</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <pages>54–64</pages>
      <url hash="4797bd38">E17-1006</url>
      <abstract>Word embeddings have been shown to be highly effective in a variety of lexical semantic tasks. They tend to capture meaningful relational similarities between individual words, at the expense of lacking the capabilty of making the underlying semantic relation explicit. In this paper, we investigate the attribute relation that often holds between the constituents of adjective-noun phrases. We use CBOW word embeddings to represent word meaning and learn a compositionality function that combines the individual constituents into a phrase representation, thus capturing the compositional attribute meaning. The resulting embedding model, while being fully interpretable, outperforms count-based distributional vector space models that are tailored to attribute meaning in the two tasks of attribute selection and phrase similarity prediction. Moreover, as the model captures a generalized layer of attribute meaning, it bears the potential to be used for predictions over various attribute inventories without re-training.</abstract>
      <bibkey>hartung-etal-2017-learning</bibkey>
    </paper>
    <paper id="7">
      <title>Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection</title>
      <author><first>Vered</first><last>Shwartz</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <author><first>Dominik</first><last>Schlechtweg</last></author>
      <pages>65–75</pages>
      <url hash="0fccde2e">E17-1007</url>
      <abstract>The fundamental role of hypernymy in NLP has motivated the development of many methods for the automatic identification of this relation, most of which rely on word distribution. We investigate an extensive number of such unsupervised measures, using several distributional semantic models that differ by context type and feature weighting. We analyze the performance of the different methods based on their linguistic motivation. Comparison to the state-of-the-art supervised methods shows that while supervised methods generally outperform the unsupervised ones, the former are sensitive to the distribution of training instances, hurting their reliability. Being based on general linguistic hypotheses and independent from training data, unsupervised measures are more robust, and therefore are still useful artillery for hypernymy detection.</abstract>
      <bibkey>shwartz-etal-2017-hypernyms</bibkey>
      <pwccode url="https://github.com/vered1986/UnsupervisedHypernymy" additional="false">vered1986/UnsupervisedHypernymy</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2018-task-9-hypernym-discovery">SemEval-2018 Task 9: Hypernym Discovery</pwcdataset>
    </paper>
    <paper id="8">
      <title>Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network</title>
      <author><first>Kim Anh</first><last>Nguyen</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>76–85</pages>
      <url hash="6d017ffe">E17-1008</url>
      <abstract>Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In addition to the lexical and syntactic information, we successfully integrate the distance between the related words along the syntactic path as a new pattern feature. The results from classification experiments show that AntSynNET improves the performance over prior pattern-based methods.</abstract>
      <bibkey>nguyen-etal-2017-distinguishing</bibkey>
      <pwccode url="https://github.com/nguyenkh/AntSynNET" additional="false">nguyenkh/AntSynNET</pwccode>
    </paper>
    <paper id="9">
      <title>Unsupervised Does Not Mean Uninterpretable: The Case for Word Sense Induction and Disambiguation</title>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Eugen</first><last>Ruppert</last></author>
      <author><first>Stefano</first><last>Faralli</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>86–98</pages>
      <url hash="d2414a23">E17-1009</url>
      <abstract>The current trend in NLP is the use of highly opaque models, e.g. neural networks and word embeddings. While these models yield state-of-the-art results on a range of tasks, their drawback is poor interpretability. On the example of word sense induction and disambiguation (WSID), we show that it is possible to develop an interpretable model that matches the state-of-the-art models in accuracy. Namely, we present an unsupervised, knowledge-free WSID approach, which is interpretable at three levels: word sense inventory, sense feature representations, and disambiguation procedure. Experiments show that our model performs on par with state-of-the-art word sense embeddings and other unsupervised systems while offering the possibility to justify its decisions in human-readable form.</abstract>
      <bibkey>panchenko-etal-2017-unsupervised-mean</bibkey>
    </paper>
    <paper id="10">
      <title>Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison</title>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>99–110</pages>
      <url hash="51adaa36">E17-1010</url>
      <abstract>Word Sense Disambiguation is a long-standing task in Natural Language Processing, lying at the core of human language understanding. However, the evaluation of automatic systems has been problematic, mainly due to the lack of a reliable evaluation framework. In this paper we develop a unified evaluation framework and analyze the performance of various Word Sense Disambiguation systems in a fair setup. The results show that supervised systems clearly outperform knowledge-based models. Among the supervised systems, a linear classifier trained on conventional local features still proves to be a hard baseline to beat. Nonetheless, recent approaches exploiting neural networks on unlabeled corpora achieve promising results, surpassing this hard baseline in most test sets.</abstract>
      <bibkey>raganato-etal-2017-word</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senseval-2-1">Senseval-2</pwcdataset>
    </paper>
    <paper id="11">
      <title>Which is the Effective Way for <fixed-case>G</fixed-case>aokao: Information Retrieval or Neural Networks?</title>
      <author><first>Shangmin</first><last>Guo</last></author>
      <author><first>Xiangrong</first><last>Zeng</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>111–120</pages>
      <url hash="66236688">E17-1011</url>
      <abstract>As one of the most important test of China, Gaokao is designed to be difficult enough to distinguish the excellent high school students. In this work, we detailed the Gaokao History Multiple Choice Questions(GKHMC) and proposed two different approaches to address them using various resources. One approach is based on entity search technique (IR approach), the other is based on text entailment approach where we specifically employ deep neural networks(NN approach). The result of experiment on our collected real Gaokao questions showed that they are good at different categories of questions, that is IR approach performs much better at entity questions(EQs) while NN approach shows its advantage on sentence questions(SQs). We achieve state-of-the-art performance and show that it’s indispensable to apply hybrid method when participating in the real-world tests.</abstract>
      <bibkey>guo-etal-2017-effective</bibkey>
      <pwccode url="https://github.com/IACASNLPIR/GKHMC" additional="false">IACASNLPIR/GKHMC</pwccode>
    </paper>
    <paper id="12">
      <title>If You Can’t Beat Them Join Them: Handcrafted Features Complement Neural Nets for Non-Factoid Answer Reranking</title>
      <author><first>Dasha</first><last>Bogdanova</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <author><first>Daria</first><last>Dzendzik</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>121–131</pages>
      <url hash="8986409c">E17-1012</url>
      <abstract>We show that a neural approach to the task of non-factoid answer reranking can benefit from the inclusion of tried-and-tested handcrafted features. We present a neural network architecture based on a combination of recurrent neural networks that are used to encode questions and answers, and a multilayer perceptron. We show how this approach can be combined with additional features, in particular, the discourse features used by previous research. Our neural approach achieves state-of-the-art performance on a public dataset from Yahoo! Answers and its performance is further improved by incorporating the discourse features. Additionally, we present a new dataset of Ask Ubuntu questions where the hybrid approach also achieves good results.</abstract>
      <bibkey>bogdanova-etal-2017-cant</bibkey>
    </paper>
    <paper id="13">
      <title>Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks</title>
      <author><first>Rajarshi</first><last>Das</last></author>
      <author><first>Arvind</first><last>Neelakantan</last></author>
      <author><first>David</first><last>Belanger</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>132–141</pages>
      <url hash="da573e9a">E17-1013</url>
      <abstract>Our goal is to combine the rich multi-step inference of symbolic logical reasoning with the generalization capabilities of neural networks. We are particularly interested in complex reasoning about entities and relations in text and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs to compose the distributed semantics of multi-hop paths in KBs; however for multiple reasons, the approach lacks accuracy and practicality. This paper proposes three significant modeling advances: (1) we learn to jointly reason about relations, <i>entities, and entity-types</i>; (2) we use neural attention modeling to incorporate <i>multiple paths</i>; (3) we learn to <i>share strength in a single RNN</i> that represents logical composition across all relations. On a large-scale Freebase+ClueWeb prediction task, we achieve 25% error reduction, and a 53% error reduction on sparse relations due to shared strength. On chains of reasoning in WordNet we reduce error in mean quantile by 84% versus previous state-of-the-art.</abstract>
      <bibkey>das-etal-2017-chains</bibkey>
      <pwccode url="" additional="true"/>
    </paper>
    <paper id="14">
      <title>Recognizing Mentions of Adverse Drug Reaction in Social Media Using Knowledge-Infused Recurrent Models</title>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <author><first>Daniel</first><last>Gruhl</last></author>
      <author><first>Pablo</first><last>Mendes</last></author>
      <pages>142–151</pages>
      <url hash="5346b745">E17-1014</url>
      <abstract>Recognizing mentions of Adverse Drug Reactions (ADR) in social media is challenging: ADR mentions are context-dependent and include long, varied and unconventional descriptions as compared to more formal medical symptom terminology. We use the CADEC corpus to train a recurrent neural network (RNN) transducer, integrated with knowledge graph embeddings of DBpedia, and show the resulting model to be highly accurate (93.4 F1). Furthermore, even when lacking high quality expert annotations, we show that by employing an active learning technique and using purpose built annotation tools, we can train the RNN to perform well (83.9 F1).</abstract>
      <bibkey>stanovsky-etal-2017-recognizing</bibkey>
    </paper>
    <paper id="15">
      <title>Multitask Learning for Mental Health Conditions with Limited Social Media Data</title>
      <author><first>Adrian</first><last>Benton</last></author>
      <author><first>Margaret</first><last>Mitchell</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>152–162</pages>
      <url hash="a2eb94d9">E17-1015</url>
      <revision id="1" href="E17-1015v1" hash="2b4866f2"/>
      <revision id="2" href="E17-1015v2" hash="a2eb94d9">No description of the changes were recorded.</revision>
      <abstract>Language contains information about the author’s demographic attributes as well as their mental state, and has been successfully leveraged in NLP to predict either one alone. However, demographic attributes and mental states also interact with each other, and we are the first to demonstrate how to use them jointly to improve the prediction of mental health conditions across the board. We model the different conditions as tasks in a multitask learning (MTL) framework, and establish for the first time the potential of deep learning in the prediction of mental health from online user-generated text. The framework we propose significantly improves over all baselines and single-task models for predicting mental health conditions, with particularly significant gains for conditions with limited data. In addition, our best MTL model can predict the presence of conditions (neuroatypicality) more generally, further reducing the error of the strong feed-forward baseline.</abstract>
      <bibkey>benton-etal-2017-multitask</bibkey>
    </paper>
    <paper id="16">
      <title>Evaluation by Association: A Systematic Study of Quantitative Word Association Evaluation</title>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>163–175</pages>
      <url hash="8b6dcbf7">E17-1016</url>
      <abstract>Recent work on evaluating representation learning architectures in NLP has established a need for evaluation protocols based on subconscious cognitive measures rather than manually tailored intrinsic similarity and relatedness tasks. In this work, we propose a novel evaluation framework that enables large-scale evaluation of such architectures in the free word association (WA) task, which is firmly grounded in cognitive theories of human semantic representation. This evaluation is facilitated by the existence of large manually constructed repositories of word association data. In this paper, we (1) present a detailed analysis of the new quantitative WA evaluation protocol, (2) suggest new evaluation metrics for the WA task inspired by its direct analogy with information retrieval problems, (3) evaluate various state-of-the-art representation models on this task, and (4) discuss the relationship between WA and prior evaluations of semantic representation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available.</abstract>
      <bibkey>vulic-etal-2017-evaluation</bibkey>
    </paper>
    <paper id="17">
      <title>Computational Argumentation Quality Assessment in Natural Language</title>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Nona</first><last>Naderi</last></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Yonatan</first><last>Bilu</last></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <author><first>Tim Alberdingk</first><last>Thijm</last></author>
      <author><first>Graeme</first><last>Hirst</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>176–187</pages>
      <url hash="d1570681">E17-1017</url>
      <abstract>Research on computational argumentation faces the problem of how to automatically assess the quality of an argument or argumentation. While different quality dimensions have been approached in natural language processing, a common understanding of argumentation quality is still missing. This paper presents the first holistic work on computational argumentation quality in natural language. We comprehensively survey the diverse existing theories and approaches to assess logical, rhetorical, and dialectical quality dimensions, and we derive a systematic taxonomy from these. In addition, we provide a corpus with 320 arguments, annotated for all 15 dimensions in the taxonomy. Our results establish a common ground for research on computational argumentation quality assessment.</abstract>
      <bibkey>wachsmuth-etal-2017-computational</bibkey>
    </paper>
    <paper id="18">
      <title>A method for in-depth comparative evaluation: How (dis)similar are outputs of pos taggers, dependency parsers and coreference resolvers really?</title>
      <author><first>Don</first><last>Tuggener</last></author>
      <pages>188–198</pages>
      <url hash="01563cee">E17-1018</url>
      <abstract>This paper proposes a generic method for the comparative evaluation of system outputs. The approach is able to quantify the pairwise differences between two outputs and to unravel in detail what the differences consist of. We apply our approach to three tasks in Computational Linguistics, i.e. POS tagging, dependency parsing, and coreference resolution. We find that system outputs are more distinct than the (often) small differences in evaluation scores seem to suggest.</abstract>
      <bibkey>tuggener-2017-method</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="19">
      <title>Re-evaluating Automatic Metrics for Image Captioning</title>
      <author><first>Mert</first><last>Kilickaya</last></author>
      <author><first>Aykut</first><last>Erdem</last></author>
      <author><first>Nazli</first><last>Ikizler-Cinbis</last></author>
      <author><first>Erkut</first><last>Erdem</last></author>
      <pages>199–209</pages>
      <url hash="5e138e53">E17-1019</url>
      <abstract>The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover’s Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.</abstract>
      <bibkey>kilickaya-etal-2017-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="20">
      <title>Integrating Meaning into Quality Evaluation of Machine Translation</title>
      <author><first>Osman</first><last>Başkaya</last></author>
      <author><first>Eray</first><last>Yildiz</last></author>
      <author><first>Doruk</first><last>Tunaoğlu</last></author>
      <author><first>Mustafa Tolga</first><last>Eren</last></author>
      <author><first>A. Seza</first><last>Doğruöz</last></author>
      <pages>210–219</pages>
      <url hash="7d939d80">E17-1020</url>
      <abstract>Machine translation (MT) quality is evaluated through comparisons between MT outputs and the human translations (HT). Traditionally, this evaluation relies on form related features (e.g. lexicon and syntax) and ignores the transfer of meaning reflected in HT outputs. Instead, we evaluate the quality of MT outputs through meaning related features (e.g. polarity, subjectivity) with two experiments. In the first experiment, the meaning related features are compared to human rankings individually. In the second experiment, combinations of meaning related features and other quality metrics are utilized to predict the same human rankings. The results of our experiments confirm the benefit of these features in predicting human evaluation of translation quality in addition to traditional metrics which focus mainly on form.</abstract>
      <bibkey>baskaya-etal-2017-integrating</bibkey>
    </paper>
    <paper id="21">
      <title>Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages</title>
      <author><first>Michael</first><last>Schlichtkrull</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>220–229</pages>
      <url hash="46dc313f">E17-1021</url>
      <abstract>In cross-lingual dependency annotation projection, information is often lost during transfer because of early decoding. We present an end-to-end graph-based neural network dependency parser that can be trained to reproduce matrices of edge scores, which can be directly projected across word alignments. We show that our approach to cross-lingual dependency parsing is not only simpler, but also achieves an absolute improvement of 2.25% averaged across 10 languages compared to the previous state of the art.</abstract>
      <bibkey>schlichtkrull-sogaard-2017-cross</bibkey>
      <pwccode url="https://github.com/MichSchli/Tensor-LSTM" additional="false">MichSchli/Tensor-LSTM</pwccode>
    </paper>
    <paper id="22">
      <title>Parsing <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies without training</title>
      <author><first>Héctor</first><last>Martínez Alonso</last></author>
      <author><first>Željko</first><last>Agić</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>230–240</pages>
      <url hash="90db81de">E17-1022</url>
      <abstract>We present UDP, the first training-free parser for Universal Dependencies (UD). Our algorithm is based on PageRank and a small set of specific dependency head rules. UDP features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD. The parser has very few parameters and distinctly robust to domain change across languages.</abstract>
      <bibkey>martinez-alonso-etal-2017-parsing</bibkey>
      <pwccode url="https://github.com/hectormartinez/ud_unsup_parser" additional="false">hectormartinez/ud_unsup_parser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="23">
      <title>Delexicalized Word Embeddings for Cross-lingual Dependency Parsing</title>
      <author><first>Mathieu</first><last>Dehouck</last></author>
      <author><first>Pascal</first><last>Denis</last></author>
      <pages>241–250</pages>
      <url hash="44950e11">E17-1023</url>
      <abstract>This paper presents a new approach to the problem of cross-lingual dependency parsing, aiming at leveraging training data from different source languages to learn a parser in a target language. Specifically, this approach first constructs word vector representations that exploit structural (i.e., dependency-based) contexts but only considering the morpho-syntactic information associated with each word and its contexts. These delexicalized word embeddings, which can be trained on any set of languages and capture features shared across languages, are then used in combination with standard language-specific features to train a lexicalized parser in the target language. We evaluate our approach through experiments on a set of eight different languages that are part the Universal Dependencies Project. Our main results show that using such delexicalized embeddings, either trained in a monolingual or multilingual fashion, achieves significant improvements over monolingual baselines.</abstract>
      <bibkey>dehouck-denis-2017-delexicalized</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="24">
      <title>Stance Classification of Context-Dependent Claims</title>
      <author><first>Roy</first><last>Bar-Haim</last></author>
      <author><first>Indrajit</first><last>Bhattacharya</last></author>
      <author><first>Francesco</first><last>Dinuzzo</last></author>
      <author><first>Amrita</first><last>Saha</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>251–261</pages>
      <url hash="5a59a8ee">E17-1024</url>
      <abstract>Recent work has addressed the problem of detecting relevant claims for a given controversial topic. We introduce the complementary task of Claim Stance Classification, along with the first benchmark dataset for this task. We decompose this problem into: (a) open-domain target identification for topic and claim (b) sentiment classification for each target, and (c) open-domain contrast detection between the topic and the claim targets. Manual annotation of the dataset confirms the applicability and validity of our model. We describe an implementation of our model, focusing on a novel algorithm for contrast detection. Our approach achieves promising results, and is shown to outperform several baselines, which represent the common practice of applying a single, monolithic classifier for stance classification.</abstract>
      <bibkey>bar-haim-etal-2017-stance</bibkey>
    </paper>
    <paper id="25">
      <title>Exploring the Impact of Pragmatic Phenomena on Irony Detection in Tweets: A Multilingual Corpus Study</title>
      <author><first>Jihen</first><last>Karoui</last></author>
      <author><first>Farah</first><last>Benamara</last></author>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Viviana</first><last>Patti</last></author>
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Nathalie</first><last>Aussenac-Gilles</last></author>
      <pages>262–272</pages>
      <url hash="53f5c9d5">E17-1025</url>
      <abstract>This paper provides a linguistic and pragmatic analysis of the phenomenon of irony in order to represent how Twitter’s users exploit irony devices within their communication strategies for generating textual contents. We aim to measure the impact of a wide-range of pragmatic phenomena in the interpretation of irony, and to investigate how these phenomena interact with contexts local to the tweet. Informed by linguistic theories, we propose for the first time a multi-layered annotation schema for irony and its application to a corpus of French, English and Italian tweets. We detail each layer, explore their interactions, and discuss our results according to a qualitative and quantitative perspective.</abstract>
      <bibkey>karoui-etal-2017-exploring</bibkey>
    </paper>
    <paper id="26">
      <title>A Multi-View Sentiment Corpus</title>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <author><first>Enza</first><last>Messina</last></author>
      <pages>273–280</pages>
      <url hash="d72938a1">E17-1026</url>
      <abstract>Sentiment Analysis is a broad task that involves the analysis of various aspect of the natural language text. However, most of the approaches in the state of the art usually investigate independently each aspect, i.e. Subjectivity Classification, Sentiment Polarity Classification, Emotion Recognition, Irony Detection. In this paper we present a Multi-View Sentiment Corpus (MVSC), which comprises 3000 English microblog posts related the movie domain. Three independent annotators manually labelled MVSC, following a broad annotation schema about different aspects that can be grasped from natural language text coming from social networks. The contribution is therefore a corpus that comprises five different views for each message, i.e. subjective/objective, sentiment polarity, implicit/explicit, irony, emotion. In order to allow a more detailed investigation on the human labelling behaviour, we provide the annotations of each human annotator involved.</abstract>
      <bibkey>nozza-etal-2017-multi</bibkey>
    </paper>
    <paper id="27">
      <title>A Systematic Study of Neural Discourse Models for Implicit Discourse Relation</title>
      <author><first>Attapol</first><last>Rutherford</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <pages>281–291</pages>
      <url hash="971bbe06">E17-1027</url>
      <abstract>Inferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Many neural network models have been proposed to tackle this problem. However, the comparison for this task is not unified, so we could hardly draw clear conclusions about the effectiveness of various architectures. Here, we propose neural network models that are based on feedforward and long-short term memory architecture and systematically study the effects of varying structures. To our surprise, the best-configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Further, we compare our best feedforward system with competitive convolutional and recurrent networks and find that feedforward can actually be more effective. For the first time for this task, we compile and publish outputs from previous neural and non-neural systems to establish the standard for further comparison.</abstract>
      <bibkey>rutherford-etal-2017-systematic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="28">
      <title>Cross-lingual <fixed-case>RST</fixed-case> Discourse Parsing</title>
      <author><first>Chloé</first><last>Braud</last></author>
      <author><first>Maximin</first><last>Coavoux</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>292–304</pages>
      <url hash="10ca80de">E17-1028</url>
      <abstract>Discourse parsing is an integral part of understanding information flow and argumentative structure in documents. Most previous research has focused on inducing and evaluating models from the English RST Discourse Treebank. However, discourse treebanks for other languages exist, including Spanish, German, Basque, Dutch and Brazilian Portuguese. The treebanks share the same underlying linguistic theory, but differ slightly in the way documents are annotated. In this paper, we present (a) a new discourse parser which is simpler, yet competitive (significantly better on 2/3 metrics) to state of the art for English, (b) a harmonization of discourse treebanks across languages, enabling us to present (c) what to the best of our knowledge are the first experiments on cross-lingual discourse parsing.</abstract>
      <bibkey>braud-etal-2017-cross</bibkey>
      <pwccode url="https://bitbucket.org/chloebt/discourse" additional="false">chloebt/discourse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rst-dt">RST-DT</pwcdataset>
    </paper>
    <paper id="29">
      <title>Dialog state tracking, a machine reading approach using Memory Network</title>
      <author><first>Julien</first><last>Perez</last></author>
      <author id="fei-liu-unimelb"><first>Fei</first><last>Liu</last></author>
      <pages>305–314</pages>
      <url hash="637327fc">E17-1029</url>
      <abstract>In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the speech recognition and the natural language understanding modules. This paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an End-to-End Memory Network, MemN2N, a memory-enhanced neural network architecture. We evaluate the proposed approach on the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has been converted for the occasion in order to frame the hidden state variable inference as a question-answering task based on a sequence of utterances extracted from a dialog. We show that the proposed tracker gives encouraging results. Then, we propose to extend the DSTC-2 dataset with specific reasoning capabilities requirement like counting, list maintenance, yes-no question answering and indefinite knowledge management. Finally, we present encouraging results using our proposed MemN2N based tracking model.</abstract>
      <bibkey>perez-liu-2017-dialog</bibkey>
    </paper>
    <paper id="30">
      <title>Sentence Segmentation in Narrative Transcripts from Neuropsychological Tests using Recurrent Convolutional Neural Networks</title>
      <author><first>Marcos</first><last>Treviso</last></author>
      <author><first>Christopher</first><last>Shulby</last></author>
      <author><first>Sandra</first><last>Aluísio</last></author>
      <pages>315–325</pages>
      <url hash="b3ab9d33">E17-1030</url>
      <abstract>Automated discourse analysis tools based on Natural Language Processing (NLP) aiming at the diagnosis of language-impairing dementias generally extract several textual metrics of narrative transcripts. However, the absence of sentence boundary segmentation in the transcripts prevents the direct application of NLP methods which rely on these marks in order to function properly, such as taggers and parsers. We present the first steps taken towards automatic neuropsychological evaluation based on narrative discourse analysis, presenting a new automatic sentence segmentation method for impaired speech. Our model uses recurrent convolutional neural networks with prosodic, Part of Speech (PoS) features, and word embeddings. It was evaluated intrinsically on impaired, spontaneous speech as well as normal, prepared speech and presents better results for healthy elderly (CTL) (F1 = 0.74) and Mild Cognitive Impairment (MCI) patients (F1 = 0.70) than the Conditional Random Fields method (F1 = 0.55 and 0.53, respectively) used in the same context of our study. The results suggest that our model is robust for impaired speech and can be used in automated discourse analysis tools to differentiate narratives produced by MCI and CTL.</abstract>
      <bibkey>treviso-etal-2017-sentence</bibkey>
    </paper>
    <paper id="31">
      <title>Joint, Incremental Disfluency Detection and Utterance Segmentation from Speech</title>
      <author><first>Julian</first><last>Hough</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>326–336</pages>
      <url hash="d4e517c5">E17-1031</url>
      <abstract>We present the joint task of incremental disfluency detection and utterance segmentation and a simple deep learning system which performs it on transcripts and ASR results. We show how the constraints of the two tasks interact. Our joint-task system outperforms the equivalent individual task systems, provides competitive results and is suitable for future use in conversation agents in the psychiatric domain.</abstract>
      <bibkey>hough-schlangen-2017-joint</bibkey>
    </paper>
    <paper id="32">
      <title>From Segmentation to Analyses: a Probabilistic Model for Unsupervised Morphology Induction</title>
      <author><first>Toms</first><last>Bergmanis</last></author>
      <author><first>Sharon</first><last>Goldwater</last></author>
      <pages>337–346</pages>
      <url hash="cc8fa385">E17-1032</url>
      <abstract>A major motivation for unsupervised morphological analysis is to reduce the sparse data problem in under-resourced languages. Most previous work focus on segmenting surface forms into their constituent morphs (taking: tak +ing), but surface form segmentation does not solve the sparse data problem as the analyses of take and taking are not connected to each other. We present a system that adapts the MorphoChains system (Narasimhan et al., 2015) to provide morphological analyses that aim to abstract over spelling differences in functionally similar morphs. This results in analyses that are not compelled to use all the orthographic material of a word (stopping: stop +ing) or limited to only that material (acidified: acid +ify +ed). On average across six typologically varied languages our system has a similar or better F-score on EMMA (a measure of underlying morpheme accuracy) than three strong baselines; moreover, the total number of distinct morphemes identified by our system is on average 12.8% lower than for Morfessor (Virpioja et al., 2013), a state-of-the-art surface segmentation system.</abstract>
      <bibkey>bergmanis-goldwater-2017-segmentation</bibkey>
    </paper>
    <paper id="33">
      <title>Creating <fixed-case>POS</fixed-case> Tagging and Dependency Parsing Experts via Topic Modeling</title>
      <author><first>Atreyee</first><last>Mukherjee</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <author><first>Matthias</first><last>Scheutz</last></author>
      <pages>347–355</pages>
      <url hash="9c1ab824">E17-1033</url>
      <abstract>Part of speech (POS) taggers and dependency parsers tend to work well on homogeneous datasets but their performance suffers on datasets containing data from different genres. In our current work, we investigate how to create POS tagging and dependency parsing experts for heterogeneous data by employing topic modeling. We create topic models (using Latent Dirichlet Allocation) to determine genres from a heterogeneous dataset and then train an expert for each of the genres. Our results show that the topic modeling experts reach substantial improvements when compared to the general versions. For dependency parsing, the improvement reaches 2 percent points over the full training baseline when we use two topics.</abstract>
      <bibkey>mukherjee-etal-2017-creating</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies and Morphology for <fixed-case>H</fixed-case>ungarian - and on the Price of Universality</title>
      <author><first>Veronika</first><last>Vincze</last></author>
      <author><first>Katalin</first><last>Simkó</last></author>
      <author><first>Zsolt</first><last>Szántó</last></author>
      <author><first>Richárd</first><last>Farkas</last></author>
      <pages>356–365</pages>
      <url hash="c7bdab0a">E17-1034</url>
      <abstract>In this paper, we present how the principles of universal dependencies and morphology have been adapted to Hungarian. We report the most challenging grammatical phenomena and our solutions to those. On the basis of the adapted guidelines, we have converted and manually corrected 1,800 sentences from the Szeged Treebank to universal dependency format. We also introduce experiments on this manually annotated corpus for evaluating automatic conversion and the added value of language-specific, i.e. non-universal, annotations. Our results reveal that converting to universal dependencies is not necessarily trivial, moreover, using language-specific morphological features may have an impact on overall performance.</abstract>
      <bibkey>vincze-etal-2017-universal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="35">
      <title>Addressing the Data Sparsity Issue in Neural <fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Xiaochang</first><last>Peng</last></author>
      <author><first>Chuan</first><last>Wang</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <pages>366–375</pages>
      <url hash="0e1a5ce0">E17-1035</url>
      <abstract>Neural attention models have achieved great success in different NLP tasks. However, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources.</abstract>
      <bibkey>peng-etal-2017-addressing</bibkey>
    </paper>
    <paper id="36">
      <title>Generating Natural Language Question-Answer Pairs from a Knowledge Graph Using a <fixed-case>RNN</fixed-case> Based Question Generation Model</title>
      <author><first>Sathish</first><last>Reddy</last></author>
      <author><first>Dinesh</first><last>Raghu</last></author>
      <author><first>Mitesh M.</first><last>Khapra</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <pages>376–385</pages>
      <url hash="52616ccd">E17-1036</url>
      <abstract>In recent years, knowledge graphs such as Freebase that capture facts about entities and relationships between them have been used actively for answering factoid questions. In this paper, we explore the problem of automatically generating question answer pairs from a given knowledge graph. The generated question answer (QA) pairs can be used in several downstream applications. For example, they could be used for training better QA systems. To generate such QA pairs, we first extract a set of keywords from entities and relationships expressed in a triple stored in the knowledge graph. From each such set, we use a subset of keywords to generate a natural language question that has a unique answer. We treat this subset of keywords as a sequence and propose a sequence to sequence model using RNN to generate a natural language question from it. Our RNN based model generates QA pairs with an accuracy of 33.61 percent and performs 110.47 percent (relative) better than a state-of-the-art template based method for generating natural language question from keywords. We also do an extrinsic evaluation by using the generated QA pairs to train a QA system and observe that the F1-score of the QA system improves by 5.5 percent (relative) when using automatically generated QA pairs in addition to manually generated QA pairs available for training.</abstract>
      <bibkey>reddy-etal-2017-generating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="37">
      <title>Enumeration of Extractive Oracle Summaries</title>
      <author><first>Tsutomu</first><last>Hirao</last></author>
      <author><first>Masaaki</first><last>Nishino</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>386–396</pages>
      <url hash="2cf2147e">E17-1037</url>
      <abstract>To analyze the limitations and the future directions of the extractive summarization paradigm, this paper proposes an Integer Linear Programming (ILP) formulation to obtain extractive oracle summaries in terms of ROUGE-N. We also propose an algorithm that enumerates all of the oracle summaries for a set of reference summaries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries.</abstract>
      <bibkey>hirao-etal-2017-enumeration</bibkey>
    </paper>
    <paper id="38">
      <title>Neural Semantic Encoders</title>
      <author><first>Tsendsuren</first><last>Munkhdalai</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>397–407</pages>
      <url hash="3f81c0aa">E17-1038</url>
      <abstract>We present a memory augmented neural network for natural language understanding: Neural Semantic Encoders. NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves over time and maintains the understanding of input sequences through read, compose and write operations. NSE can also access 1 multiple and shared memories. In this paper, we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks: natural language inference, question answering, sentence classification, document sentiment analysis and machine translation where NSE achieved state-of-the-art performance when evaluated on publically available benchmarks. For example, our shared-memory model showed an encouraging result on neural machine translation, improving an attention-based baseline by approximately 1.0 BLEU.</abstract>
      <bibkey>munkhdalai-yu-2017-neural-semantic</bibkey>
      <pwccode url="https://bitbucket.org/tsendeemts/nse" additional="true">tsendeemts/nse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="39">
      <title>Efficient Benchmarking of <fixed-case>NLP</fixed-case> <fixed-case>API</fixed-case>s using Multi-armed Bandits</title>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Tuan Dung</first><last>Tran</last></author>
      <author><first>Mark</first><last>Carman</last></author>
      <pages>408–416</pages>
      <url hash="6cf4d183">E17-1039</url>
      <abstract>Comparing NLP systems to select the best one for a task of interest, such as named entity recognition, is critical for practitioners and researchers. A rigorous approach involves setting up a hypothesis testing scenario using the performance of the systems on query documents. However, often the hypothesis testing approach needs to send a lot of document queries to the systems, which can be problematic. In this paper, we present an effective alternative based on the multi-armed bandit (MAB). We propose a hierarchical generative model to represent the uncertainty in the performance measures of the competing systems, to be used by Thompson Sampling to solve the resulting MAB. Experimental results on both synthetic and real data show that our approach requires significantly fewer queries compared to the standard benchmarking technique to identify the best system according to F-measure.</abstract>
      <bibkey>haffari-etal-2017-efficient</bibkey>
    </paper>
    <paper id="40">
      <title>Character-Word <fixed-case>LSTM</fixed-case> Language Models</title>
      <author><first>Lyan</first><last>Verwimp</last></author>
      <author><first>Joris</first><last>Pelemans</last></author>
      <author><first>Hugo</first><last>Van hamme</last></author>
      <author><first>Patrick</first><last>Wambacq</last></author>
      <pages>417–427</pages>
      <url hash="9fcd655d">E17-1040</url>
      <abstract>We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level models with a larger number of parameters.</abstract>
      <bibkey>verwimp-etal-2017-character</bibkey>
    </paper>
    <paper id="41">
      <title>A Hierarchical Neural Model for Learning Sequences of Dialogue Acts</title>
      <author><first>Quan Hung</first><last>Tran</last></author>
      <author><first>Ingrid</first><last>Zukerman</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>428–437</pages>
      <url hash="95b5f0a8">E17-1041</url>
      <abstract>We propose a novel hierarchical Recurrent Neural Network (RNN) for learning sequences of Dialogue Acts (DAs). The input in this task is a sequence of utterances (i.e., conversational contributions) comprising a sequence of tokens, and the output is a sequence of DA labels (one label per utterance). Our model leverages the hierarchical nature of dialogue data by using two nested RNNs that capture long-range dependencies at the dialogue level and the utterance level. This model is combined with an attention mechanism that focuses on salient tokens in utterances. Our experimental results show that our model outperforms strong baselines on two popular datasets, Switchboard and MapTask; and our detailed empirical analysis highlights the impact of each aspect of our model.</abstract>
      <bibkey>tran-etal-2017-hierarchical</bibkey>
    </paper>
    <paper id="42">
      <title>A Network-based End-to-End Trainable Task-oriented Dialogue System</title>
      <author><first>Tsung-Hsien</first><last>Wen</last></author>
      <author><first>David</first><last>Vandyke</last></author>
      <author><first>Nikola</first><last>Mrkšić</last></author>
      <author><first>Milica</first><last>Gašić</last></author>
      <author><first>Lina M.</first><last>Rojas-Barahona</last></author>
      <author><first>Pei-Hao</first><last>Su</last></author>
      <author><first>Stefan</first><last>Ultes</last></author>
      <author><first>Steve</first><last>Young</last></author>
      <pages>438–449</pages>
      <url hash="9d3e4f48">E17-1042</url>
      <abstract>Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.</abstract>
      <bibkey>wen-etal-2017-network</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-oz">Wizard-of-Oz</pwcdataset>
    </paper>
    <paper id="43">
      <title>May <fixed-case>I</fixed-case> take your order? A Neural Model for Extracting Structured Information from Conversations</title>
      <author><first>Baolin</first><last>Peng</last></author>
      <author><first>Michael</first><last>Seltzer</last></author>
      <author><first>Y.C.</first><last>Ju</last></author>
      <author><first>Geoffrey</first><last>Zweig</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>450–459</pages>
      <url hash="579f467c">E17-1043</url>
      <abstract>In this paper we tackle a unique and important problem of extracting a structured order from the conversation a customer has with an order taker at a restaurant. This is motivated by an actual system under development to assist in the order taking process. We develop a sequence-to-sequence model that is able to map from unstructured conversational input to the structured form that is conveyed to the kitchen and appears on the customer receipt. This problem is critically different from other tasks like machine translation where sequence-to-sequence models have been used: the input includes two sides of a conversation; the output is highly structured; and logical manipulations must be performed, for example when the customer changes his mind while ordering. We present a novel sequence-to-sequence model that incorporates a special attention-memory gating mechanism and conversational role markers. The proposed model improves performance over both a phrase-based machine translation approach and a standard sequence-to-sequence model.</abstract>
      <bibkey>peng-etal-2017-may</bibkey>
    </paper>
    <paper id="44">
      <title>A Two-stage Sieve Approach for Quote Attribution</title>
      <author><first>Grace</first><last>Muzny</last></author>
      <author><first>Michael</first><last>Fang</last></author>
      <author><first>Angel</first><last>Chang</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>460–470</pages>
      <url hash="4515b09e">E17-1044</url>
      <abstract>We present a deterministic sieve-based system for attributing quotations in literary text and a new dataset: QuoteLi3. Quote attribution, determining who said what in a given text, is important for tasks like creating dialogue systems, and in newer areas like computational literary studies, where it creates opportunities to analyze novels at scale rather than only a few at a time. We release QuoteLi3, which contains more than 6,000 annotations linking quotes to speaker mentions and quotes to speaker entities, and introduce a new algorithm for quote attribution. Our two-stage algorithm first links quotes to mentions, then mentions to entities. Using two stages encapsulates difficult sub-problems and improves system performance. The modular design allows us to tune for overall performance or higher precision, which is useful for many real-world use cases. Our system achieves an average F-score of 87.5 across three novels, outperforming previous systems, and can be tuned for precision of 90.4 at a recall of 65.1.</abstract>
      <bibkey>muzny-etal-2017-two</bibkey>
    </paper>
    <paper id="45">
      <title>Out-of-domain <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et Semantic Role Labeling</title>
      <author><first>Silvana</first><last>Hartmann</last></author>
      <author><first>Ilia</first><last>Kuznetsov</last></author>
      <author><first>Teresa</first><last>Martin</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>471–482</pages>
      <url hash="fc190b2d">E17-1045</url>
      <abstract>Domain dependence of NLP systems is one of the major obstacles to their application in large-scale text analysis, also restricting the applicability of FrameNet semantic role labeling (SRL) systems. Yet, current FrameNet SRL systems are still only evaluated on a single in-domain test set. For the first time, we study the domain dependence of FrameNet SRL on a wide range of benchmark sets. We create a novel test set for FrameNet SRL based on user-generated web text and find that the major bottleneck for out-of-domain FrameNet SRL is the frame identification step. To address this problem, we develop a simple, yet efficient system based on distributed word representations. Our system closely approaches the state-of-the-art in-domain while outperforming the best available frame identification system out-of-domain. We publish our system and test data for research purposes.</abstract>
      <bibkey>hartmann-etal-2017-domain</bibkey>
    </paper>
    <paper id="46">
      <title><fixed-case>TDP</fixed-case>arse: Multi-target-specific sentiment recognition on <fixed-case>T</fixed-case>witter</title>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <author><first>Arkaitz</first><last>Zubiaga</last></author>
      <author><first>Rob</first><last>Procter</last></author>
      <pages>483–493</pages>
      <url hash="fc6da506">E17-1046</url>
      <abstract>Existing target-specific sentiment recognition methods consider only a single target per tweet, and have been shown to miss nearly half of the actual targets mentioned. We present a corpus of UK election tweets, with an average of 3.09 entities per tweet and more than one type of sentiment in half of the tweets. This requires a method for multi-target specific sentiment recognition, which we develop by using the context around a target as well as syntactic dependencies involving the target. We present results of our method on both a benchmark corpus of single targets and the multi-target election corpus, showing state-of-the art performance in both corpora and outperforming previous approaches to multi-target sentiment task as well as deep learning models for single-target sentiment.</abstract>
      <bibkey>wang-etal-2017-tdparse</bibkey>
    </paper>
    <paper id="47">
      <title>Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems</title>
      <author><first>Shyam</first><last>Upadhyay</last></author>
      <author><first>Ming-Wei</first><last>Chang</last></author>
      <pages>494–504</pages>
      <url hash="83b146a6">E17-1047</url>
      <abstract>We propose a new evaluation for automatic solvers for algebra word problems, which can identify mistakes that existing evaluations overlook. Our proposal is to evaluate such solvers using derivations, which reflect how an equation system was constructed from the word problem. To accomplish this, we develop an algorithm for checking the equivalence between two derivations, and show how derivation annotations can be semi-automatically added to existing datasets. To make our experiments more comprehensive, we include the derivation annotation for DRAW-1K, a new dataset containing 1000 general algebra word problems. In our experiments, we found that the annotated derivations enable a more accurate evaluation of automatic solvers than previously used metrics. We release derivation annotations for over 2300 algebra word problems for future evaluations.</abstract>
      <bibkey>upadhyay-chang-2017-annotating</bibkey>
    </paper>
    <paper id="48">
      <title>An Extensive Empirical Evaluation of Character-Based Morphological Tagging for 14 Languages</title>
      <author><first>Georg</first><last>Heigold</last></author>
      <author><first>Guenter</first><last>Neumann</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>505–513</pages>
      <url hash="34950c22">E17-1048</url>
      <abstract>This paper investigates neural character-based morphological tagging for languages with complex morphology and large tag sets. Character-based approaches are attractive as they can handle rarely- and unseen words gracefully. We evaluate on 14 languages and observe consistent gains over a state-of-the-art morphological tagger across all languages except for English and French, where we match the state-of-the-art. We compare two architectures for computing character-based word vectors using recurrent (RNN) and convolutional (CNN) nets. We show that the CNN based approach performs slightly worse and less consistently than the RNN based approach. Small but systematic gains are observed when combining the two architectures by ensembling.</abstract>
      <bibkey>heigold-etal-2017-extensive</bibkey>
    </paper>
    <paper id="49">
      <title>Neural Multi-Source Morphological Reinflection</title>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>514–524</pages>
      <url hash="a7b17f05">E17-1049</url>
      <abstract>We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one source form since different source forms can provide complementary information, e.g., different stems. We further present a novel extension to the encoder-decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.</abstract>
      <bibkey>kann-etal-2017-neural</bibkey>
    </paper>
    <paper id="50">
      <title>Online Automatic Post-editing for <fixed-case>MT</fixed-case> in a Multi-Domain Translation Environment</title>
      <author><first>Rajen</first><last>Chatterjee</last></author>
      <author><first>Gebremedhen</first><last>Gebremelak</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>525–535</pages>
      <url hash="bc02472a">E17-1050</url>
      <abstract>Automatic post-editing (APE) for machine translation (MT) aims to fix recurrent errors made by the MT decoder by learning from correction examples. In controlled evaluation scenarios, the representativeness of the training set with respect to the test data is a key factor to achieve good performance. Real-life scenarios, however, do not guarantee such favorable learning conditions. Ideally, to be integrated in a real professional translation workflow (e.g. to play a role in computer-assisted translation framework), APE tools should be flexible enough to cope with continuous streams of diverse data coming from different domains/genres. To cope with this problem, we propose an online APE framework that is: i) robust to data diversity (i.e. capable to learn and apply correction rules in the right contexts) and ii) able to evolve over time (by continuously extending and refining its knowledge). In a comparative evaluation, with English-German test data coming in random order from two different domains, we show the effectiveness of our approach, which outperforms a strong batch system and the state of the art in online APE.</abstract>
      <bibkey>chatterjee-etal-2017-online</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="51">
      <title>An Incremental Parser for <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation</title>
      <author><first>Marco</first><last>Damonte</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <author><first>Giorgio</first><last>Satta</last></author>
      <pages>536–546</pages>
      <url hash="244b81fb">E17-1051</url>
      <abstract>Abstract Meaning Representation (AMR) is a semantic representation for natural language that embeds annotations related to traditional tasks such as named entity recognition, semantic role labeling, word sense disambiguation and co-reference resolution. We describe a transition-based parser for AMR that parses sentences left-to-right, in linear time. We further propose a test-suite that assesses specific subtasks that are helpful in comparing AMR parsers, and show that our parser is competitive with the state of the art on the LDC2015E86 dataset and that it outperforms state-of-the-art parsers for recovering named entities and handling polarity.</abstract>
      <bibkey>damonte-etal-2017-incremental</bibkey>
      <pwccode url="https://github.com/mdtux89/amr-evaluation" additional="true">mdtux89/amr-evaluation</pwccode>
    </paper>
    <paper id="52">
      <title>Integrated Learning of Dialog Strategies and Semantic Parsing</title>
      <author><first>Aishwarya</first><last>Padmakumar</last></author>
      <author><first>Jesse</first><last>Thomason</last></author>
      <author><first>Raymond J.</first><last>Mooney</last></author>
      <pages>547–557</pages>
      <url hash="7b235d12">E17-1052</url>
      <abstract>Natural language understanding and dialog management are two integral components of interactive dialog systems. Previous research has used machine learning techniques to individually optimize these components, with different forms of direct and indirect supervision. We present an approach to integrate the learning of both a dialog strategy using reinforcement learning, and a semantic parser for robust natural language understanding, using only natural dialog interaction for supervision. Experimental results on a simulated task of robot instruction demonstrate that joint learning of both components improves dialog performance over learning either of these components alone.</abstract>
      <bibkey>padmakumar-etal-2017-integrated</bibkey>
    </paper>
    <paper id="53">
      <title>Unsupervised <fixed-case>AMR</fixed-case>-Dependency Parse Alignment</title>
      <author><first>Wei-Te</first><last>Chen</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <pages>558–567</pages>
      <url hash="8d5f2231">E17-1053</url>
      <abstract>In this paper, we introduce an Abstract Meaning Representation (AMR) to Dependency Parse aligner. Alignment is a preliminary step for AMR parsing, and our aligner improves current AMR parser performance. Our aligner involves several different features, including named entity tags and semantic role labels, and uses Expectation-Maximization training. Results show that our aligner reaches an 87.1% F-Score score with the experimental data, and enhances AMR parsing.</abstract>
      <bibkey>chen-palmer-2017-unsupervised</bibkey>
    </paper>
    <paper id="54">
      <title>Improving <fixed-case>C</fixed-case>hinese Semantic Role Labeling using High-quality Surface and Deep Case Frames</title>
      <author><first>Gongye</first><last>Jin</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>568–577</pages>
      <url hash="85d716b2">E17-1054</url>
      <abstract>This paper presents a method for applying automatically acquired knowledge to semantic role labeling (SRL). We use a large amount of automatically extracted knowledge to improve the performance of SRL. We present two varieties of knowledge, which we call surface case frames and deep case frames. Although the surface case frames are compiled from syntactic parses and can be used as rich syntactic knowledge, they have limited capability for resolving semantic ambiguity. To compensate the deficiency of the surface case frames, we compile deep case frames from automatic semantic roles. We also consider quality management for both types of knowledge in order to get rid of the noise brought from the automatic analyses. The experimental results show that Chinese SRL can be improved using automatically acquired knowledge and the quality management shows a positive effect on this task.</abstract>
      <bibkey>jin-etal-2017-improving</bibkey>
    </paper>
    <paper id="55">
      <title>Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities</title>
      <author><first>Yadollah</first><last>Yaghoobzadeh</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>578–589</pages>
      <url hash="b7e7b5b8">E17-1055</url>
      <abstract>Entities are essential elements of natural language. In this paper, we present methods for learning multi-level representations of entities on three complementary levels: character (character patterns in entity names extracted, e.g., by neural networks), word (embeddings of words in entity names) and entity (entity embeddings). We investigate state-of-the-art learning methods on each level and find large differences, e.g., for deep learning models, traditional ngram features and the subword model of fasttext (Bojanowski et al., 2016) on the character level; for word2vec (Mikolov et al., 2013) on the word level; and for the order-aware model wang2vec (Ling et al., 2015a) on the entity level. We confirm experimentally that each level of representation contributes complementary information and a joint representation of all three levels improves the existing embedding based baseline for fine-grained entity typing by a large margin. Additionally, we show that adding information from entity descriptions further improves multi-level representations of entities.</abstract>
      <bibkey>yaghoobzadeh-schutze-2017-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/figment">Figment</pwcdataset>
    </paper>
    <paper id="56">
      <title>The <fixed-case>C</fixed-case>ontrast<fixed-case>M</fixed-case>edium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links</title>
      <author><first>Stefano</first><last>Faralli</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <pages>590–600</pages>
      <url hash="e2a5a0bb">E17-1056</url>
      <abstract>In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manually selected input root and leaf concepts. This is achieved by leveraging structural information from a companion reference taxonomy, to which the input knowledge graph is linked (either automatically or manually). When used in conjunction with methods for hypernym acquisition and knowledge base linking, our methodology provides a complete solution for end-to-end taxonomy induction. We conduct experiments using automatically acquired knowledge graphs, as well as a SemEval benchmark, and show that our method is able to achieve high performance on the task of taxonomy induction.</abstract>
      <bibkey>faralli-etal-2017-contrastmedium</bibkey>
    </paper>
    <paper id="57">
      <title>Probabilistic Inference for Cold Start Knowledge Base Population with Prior World Knowledge</title>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Marjorie</first><last>Freedman</last></author>
      <author><first>Talya</first><last>Meltzer</last></author>
      <pages>601–612</pages>
      <url hash="5388d126">E17-1057</url>
      <abstract>Building knowledge bases (KB) automatically from text corpora is crucial for many applications such as question answering and web search. The problem is very challenging and has been divided into sub-problems such as mention and named entity recognition, entity linking and relation extraction. However, combining these components has shown to be under-constrained and often produces KBs with supersize entities and common-sense errors in relations (a person has multiple birthdates). The errors are difficult to resolve solely with IE tools but become obvious with world knowledge at the corpus level. By analyzing Freebase and a large text collection, we found that per-relation cardinality and the popularity of entities follow the power-law distribution favoring flat long tails with low-frequency instances. We present a probabilistic joint inference algorithm to incorporate this world knowledge during KB construction. Our approach yields state-of-the-art performance on the TAC Cold Start task, and 42% and 19.4% relative improvements in F1 over our baseline on Cold Start hop-1 and all-hop queries respectively.</abstract>
      <bibkey>min-etal-2017-probabilistic</bibkey>
    </paper>
    <paper id="58">
      <title>Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema</title>
      <author><first>Patrick</first><last>Verga</last></author>
      <author><first>Arvind</first><last>Neelakantan</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>613–622</pages>
      <url hash="a12a6697">E17-1058</url>
      <abstract>Universal schema predicts the types of entities and relations in a knowledge base (KB) by jointly embedding the union of all available schema types—not only types from multiple structured databases (such as Freebase or Wikipedia infoboxes), but also types expressed as textual patterns from raw text. This prediction is typically modeled as a matrix completion problem, with one type per column, and either one or two entities per row (in the case of entity types or binary relation types, respectively). Factorizing this sparsely observed matrix yields a learned vector embedding for each row and each column. In this paper we explore the problem of making predictions for entities or entity-pairs unseen at training time (and hence without a pre-learned row embedding). We propose an approach having no per-row parameters at all; rather we produce a row vector on the fly using a learned aggregation function of the vectors of the observed columns for that row. We experiment with various aggregation functions, including neural network attention models. Our approach can be understood as a natural language database, in that questions about KB entities are answered by attending to textual or database evidence. In experiments predicting both relations and entity types, we demonstrate that despite having an order of magnitude fewer parameters than traditional universal schema, we can match the accuracy of the traditional model, and more importantly, we can now make predictions about unseen rows with nearly the same accuracy as rows available at training time.</abstract>
      <bibkey>verga-etal-2017-generalizing</bibkey>
      <pwccode url="https://github.com/patverga/torch-relation-extraction" additional="false">patverga/torch-relation-extraction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k">FB15k</pwcdataset>
    </paper>
    <paper id="59">
      <title>Learning to Generate Product Reviews from Attributes</title>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Ke</first><last>Xu</last></author>
      <pages>623–632</pages>
      <url hash="c2aab7d3">E17-1059</url>
      <abstract>Automatically generating product reviews is a meaningful, yet not well-studied task in sentiment analysis. Traditional natural language generation methods rely extensively on hand-crafted rules and predefined templates. This paper presents an attention-enhanced attribute-to-sequence model to generate product reviews for given attribute information, such as user, product, and rating. The attribute encoder learns to represent input attributes as vectors. Then, the sequence decoder generates reviews by conditioning its output on these vectors. We also introduce an attention mechanism to jointly generate reviews and align words with input attributes. The proposed model is trained end-to-end to maximize the likelihood of target product reviews given the attributes. We build a publicly available dataset for the review generation task by leveraging the Amazon book reviews and their metadata. Experiments on the dataset show that our approach outperforms baseline methods and the attention mechanism significantly improves the performance of our model.</abstract>
      <bibkey>dong-etal-2017-learning-generate</bibkey>
    </paper>
    <paper id="60">
      <title>Learning to generate one-sentence biographies from <fixed-case>W</fixed-case>ikidata</title>
      <author><first>Andrew</first><last>Chisholm</last></author>
      <author><first>Will</first><last>Radford</last></author>
      <author><first>Ben</first><last>Hachey</last></author>
      <pages>633–642</pages>
      <url hash="0b54ba37">E17-1060</url>
      <abstract>We investigate the generation of one-sentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our model incorporates a novel secondary objective that helps ensure it generates sentences that contain the input facts. The model achieves a BLEU score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the model is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information.</abstract>
      <bibkey>chisholm-etal-2017-learning</bibkey>
      <pwccode url="https://github.com/andychisholm/mimo" additional="false">andychisholm/mimo</pwccode>
    </paper>
    <paper id="61">
      <title>Transition-Based Deep Input Linearization</title>
      <author><first>Ratish</first><last>Puduppully</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>643–654</pages>
      <url hash="c182746b">E17-1061</url>
      <abstract>Traditional methods for deep NLG adopt pipeline approaches comprising stages such as constructing syntactic input, predicting function words, linearizing the syntactic input and generating the surface forms. Though easier to visualize, pipeline approaches suffer from error propagation. In addition, information available across modules cannot be leveraged by all modules. We construct a transition-based model to jointly perform linearization, function word prediction and morphological generation, which considerably improves upon the accuracy compared to a pipelined baseline system. On a standard deep input linearization shared task, our system achieves the best results reported so far.</abstract>
      <bibkey>puduppully-etal-2017-transition</bibkey>
      <pwccode url="https://github.com/SUTDNLP/ZGen" additional="false">SUTDNLP/ZGen</pwccode>
    </paper>
    <paper id="62">
      <title>Generating flexible proper name references in text: Data, models and evaluation</title>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <author><first>Sander</first><last>Wubben</last></author>
      <pages>655–664</pages>
      <url hash="5053ae0f">E17-1062</url>
      <abstract>This study introduces a statistical model able to generate variations of a proper name by taking into account the person to be mentioned, the discourse context and variation. The model relies on the REGnames corpus, a dataset with 53,102 proper name references to 1,000 people in different discourse contexts. We evaluate the versions of our model from the perspective of how human writers produce proper names, and also how human readers process them. The corpus and the model are publicly available.</abstract>
      <bibkey>castro-ferreira-etal-2017-generating</bibkey>
    </paper>
    <paper id="63">
      <title>Dependency Parsing as Head Selection</title>
      <author><first>Xingxing</first><last>Zhang</last></author>
      <author><first>Jianpeng</first><last>Cheng</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>665–676</pages>
      <url hash="3bf88324">E17-1063</url>
      <abstract>Conventional graph-based dependency parsers guarantee a tree structure both during training and inference. Instead, we formalize dependency parsing as the problem of independently selecting the head of each word in a sentence. Our model which we call DENSE (as shorthand for <b>De</b>pendency <b>N</b>eural <b>Se</b>lection) produces a distribution over possible heads for each word using features obtained from a bidirectional recurrent neural network. Without enforcing structural constraints during training, DeNSe generates (at inference time) trees for the overwhelming majority of sentences, while non-tree outputs can be adjusted with a maximum spanning tree algorithm.  We evaluate DeNSe on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of the approach, our parsers are on par with the state of the art.</abstract>
      <bibkey>zhang-etal-2017-dependency</bibkey>
      <pwccode url="https://github.com/XingxingZhang/dense_parser" additional="false">XingxingZhang/dense_parser</pwccode>
    </paper>
    <paper id="64">
      <title>Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing</title>
      <author><first>Minh</first><last>Lê</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <pages>677–687</pages>
      <url hash="60b8b802">E17-1064</url>
      <abstract>Error propagation is a common problem in NLP. Reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes are made early in a process. In this paper, we apply reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation.</abstract>
      <bibkey>le-fokkens-2017-tackling</bibkey>
      <pwccode url="https://bitbucket.org/cltl/redep-java" additional="false">cltl/redep-java</pwccode>
    </paper>
    <paper id="65">
      <title>Noisy-context surprisal as a human sentence processing cost model</title>
      <author><first>Richard</first><last>Futrell</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>688–698</pages>
      <url hash="11906a5e">E17-1065</url>
      <abstract>We use the noisy-channel theory of human sentence comprehension to develop an incremental processing cost model that unifies and extends key features of expectation-based and memory-based models. In this model, which we call noisy-context surprisal, the processing cost of a word is the surprisal of the word given a noisy representation of the preceding context. We show that this model accounts for an outstanding puzzle in sentence comprehension, language-dependent structural forgetting effects (Gibson and Thomas, 1999; Vasishth et al., 2010; Frank et al., 2016), which are previously not well modeled by either expectation-based or memory-based approaches. Additionally, we show that this model derives and generalizes locality effects (Gibson, 1998; Demberg and Keller, 2008), a signature prediction of memory-based models. We give corpus-based evidence for a key assumption in this derivation.</abstract>
      <bibkey>futrell-levy-2017-noisy</bibkey>
    </paper>
    <paper id="66">
      <title>Task-Specific Attentive Pooling of Phrase Alignments Contributes to Sentence Matching</title>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>699–709</pages>
      <url hash="4c9d276d">E17-1066</url>
      <abstract>This work studies comparatively two typical sentence matching tasks: textual entailment (TE) and answer selection (AS), observing that weaker phrase alignments are more critical in TE, while stronger phrase alignments deserve more attention in AS. The key to reach this observation lies in phrase detection, phrase representation, phrase alignment, and more importantly how to connect those aligned phrases of different matching degrees with the final classifier. Prior work (i) has limitations in phrase generation and representation, or (ii) conducts alignment at word and phrase levels by handcrafted features or (iii) utilizes a single framework of alignment without considering the characteristics of specific tasks, which limits the framework’s effectiveness across tasks. We propose an architecture based on Gated Recurrent Unit that supports (i) representation learning of phrases of arbitrary granularity and (ii) task-specific attentive pooling of phrase alignments between two sentences. Experimental results on TE and AS match our observation and show the effectiveness of our approach.</abstract>
      <bibkey>yin-schutze-2017-task</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="67">
      <title>On-demand Injection of Lexical Knowledge for Recognising Textual Entailment</title>
      <author><first>Pascual</first><last>Martínez-Gómez</last></author>
      <author><first>Koji</first><last>Mineshima</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <author><first>Daisuke</first><last>Bekki</last></author>
      <pages>710–720</pages>
      <url hash="5adc44c3">E17-1067</url>
      <abstract>We approach the recognition of textual entailment using logical semantic representations and a theorem prover. In this setup, lexical divergences that preserve semantic entailment between the source and target texts need to be explicitly stated. However, recognising subsentential semantic relations is not trivial. We address this problem by monitoring the proof of the theorem and detecting unprovable sub-goals that share predicate arguments with logical premises. If a linguistic relation exists, then an appropriate axiom is constructed on-demand and the theorem proving continues. Experiments show that this approach is effective and precise, producing a system that outperforms other logic-based systems and is competitive with state-of-the-art statistical methods.</abstract>
      <bibkey>martinez-gomez-etal-2017-demand</bibkey>
      <pwccode url="https://github.com/mynlp/ccg2lambda" additional="false">mynlp/ccg2lambda</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
    </paper>
    <paper id="68">
      <title>Learning to Predict Denotational Probabilities For Modeling Entailment</title>
      <author><first>Alice</first><last>Lai</last></author>
      <author><first>Julia</first><last>Hockenmaier</last></author>
      <pages>721–730</pages>
      <url hash="f535c671">E17-1068</url>
      <abstract>We propose a framework that captures the denotational probabilities of words and phrases by embedding them in a vector space, and present a method to induce such an embedding from a dataset of denotational probabilities. We show that our model successfully predicts denotational probabilities for unseen phrases, and that its predictions are useful for textual entailment datasets such as SICK and SNLI.</abstract>
      <bibkey>lai-hockenmaier-2017-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="69">
      <title>A Societal Sentiment Analysis: Predicting the Values and Ethics of Individuals by Analysing Social Media Content</title>
      <author><first>Tushar</first><last>Maheshwari</last></author>
      <author><first>Aishwarya N.</first><last>Reganti</last></author>
      <author><first>Samiksha</first><last>Gupta</last></author>
      <author><first>Anupam</first><last>Jamatia</last></author>
      <author><first>Upendra</first><last>Kumar</last></author>
      <author><first>Björn</first><last>Gambäck</last></author>
      <author><first>Amitava</first><last>Das</last></author>
      <pages>731–741</pages>
      <url hash="e21872f3">E17-1069</url>
      <abstract>To find out how users’ social media behaviour and language are related to their ethical practices, the paper investigates applying Schwartz’ psycholinguistic model of societal sentiment to social media text. The analysis is based on corpora collected from user essays as well as social media (Facebook and Twitter). Several experiments were carried out on the corpora to classify the ethical values of users, incorporating Linguistic Inquiry Word Count analysis, n-grams, topic models, psycholinguistic lexica, speech-acts, and non-linguistic information, while applying a range of machine learners (Support Vector Machines, Logistic Regression, and Random Forests) to identify the best linguistic and non-linguistic features for automatic classification of values and ethics.</abstract>
      <bibkey>maheshwari-etal-2017-societal</bibkey>
    </paper>
    <paper id="70">
      <title>Argument Strength is in the Eye of the Beholder: Audience Effects in Persuasion</title>
      <author><first>Stephanie</first><last>Lukin</last></author>
      <author><first>Pranav</first><last>Anand</last></author>
      <author><first>Marilyn</first><last>Walker</last></author>
      <author><first>Steve</first><last>Whittaker</last></author>
      <pages>742–753</pages>
      <url hash="9dcb34e6">E17-1070</url>
      <abstract>Americans spend about a third of their time online, with many participating in online conversations on social and political issues. We hypothesize that social media arguments on such issues may be more engaging and persuasive than traditional media summaries, and that particular types of people may be more or less convinced by particular styles of argument, e.g. emotional arguments may resonate with some personalities while factual arguments resonate with others. We report a set of experiments testing at large scale how audience variables interact with argument style to affect the persuasiveness of an argument, an under-researched topic within natural language processing. We show that belief change is affected by personality factors, with conscientious, open and agreeable people being more convinced by emotional arguments.</abstract>
      <bibkey>lukin-etal-2017-argument</bibkey>
    </paper>
    <paper id="71">
      <title>A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts</title>
      <author id="fei-liu-unimelb"><first>Fei</first><last>Liu</last></author>
      <author><first>Julien</first><last>Perez</last></author>
      <author><first>Scott</first><last>Nowson</last></author>
      <pages>754–764</pages>
      <url hash="de04c706">E17-1071</url>
      <abstract>There have been many attempts at automatically recognising author personality traits from text, typically incorporating linguistic features with conventional machine learning models, e.g. linear regression or Support Vector Machines. In this work, we propose to use deep-learning-based models with atomic features of text – the characters – to build hierarchical, vectorial word and sentence representations for the task of trait inference. On a corpus of tweets, this method shows state-of-the-art performance across five traits and three languages (English, Spanish and Italian) compared with prior work in author profiling. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits.</abstract>
      <bibkey>liu-etal-2017-language</bibkey>
    </paper>
    <paper id="72">
      <title>A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments</title>
      <author><first>Omer</first><last>Levy</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>765–774</pages>
      <url hash="cc355432">E17-1072</url>
      <abstract>While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remain vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account.</abstract>
      <bibkey>levy-etal-2017-strong</bibkey>
    </paper>
    <paper id="73">
      <title>Online Learning of Task-specific Word Representations with a Joint Biconvex Passive-Aggressive Algorithm</title>
      <author><first>Pascal</first><last>Denis</last></author>
      <author><first>Liva</first><last>Ralaivola</last></author>
      <pages>775–784</pages>
      <url hash="05466654">E17-1073</url>
      <abstract>This paper presents a new, efficient method for learning task-specific word vectors using a variant of the Passive-Aggressive algorithm. Specifically, this algorithm learns a word embedding matrix in tandem with the classifier parameters in an online fashion, solving a bi-convex constrained optimization at each iteration. We provide a theoretical analysis of this new algorithm in terms of regret bounds, and evaluate it on both synthetic data and NLP classification problems, including text classification and sentiment analysis. In the latter case, we compare various pre-trained word vectors to initialize our word embedding matrix, and show that the matrix learned by our algorithm vastly outperforms the initial matrix, with performance results comparable or above the state-of-the-art on these tasks.</abstract>
      <bibkey>denis-ralaivola-2017-online</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="74">
      <title>Nonsymbolic Text Representation</title>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>785–796</pages>
      <url hash="c9e16ee2">E17-1074</url>
      <abstract>We introduce the first generic text representation model that is completely nonsymbolic, i.e., it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text. This applies to training the parameters of the model on a training corpus as well as to applying it when computing the representation of a new text. We show that our model performs better than prior work on an information extraction and a text denoising task.</abstract>
      <bibkey>schutze-2017-nonsymbolic</bibkey>
    </paper>
    <paper id="75">
      <title>Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings</title>
      <author><first>Abhishek</first><last>Abhishek</last></author>
      <author><first>Ashish</first><last>Anand</last></author>
      <author><first>Amit</first><last>Awekar</last></author>
      <pages>797–807</pages>
      <url hash="fc58a51f">E17-1075</url>
      <abstract>Fine-grained entity type classification (FETC) is the task of classifying an entity mention to a broad set of types. Distant supervision paradigm is extensively used to generate training data for this task. However, generated training data assigns same set of labels to every mention of an entity without considering its local context. Existing FETC systems have two major drawbacks: assuming training data to be noise free and use of hand crafted features. Our work overcomes both drawbacks. We propose a neural network model that jointly learns entity mentions and their context representation to eliminate use of hand crafted features. Our model treats training data as noisy and uses non-parametric variant of hinge loss function. Experiments show that the proposed model outperforms previous state-of-the-art methods on two publicly available datasets, namely FIGER (GOLD) and BBN with an average relative improvement of 2.69% in micro-F1 score. Knowledge learnt by our model on one dataset can be transferred to other datasets while using same model or other FETC systems. These approaches of transferring knowledge further improve the performance of respective models.</abstract>
      <bibkey>abhishek-etal-2017-fine</bibkey>
      <pwccode url="https://github.com/abhipec/fnet" additional="false">abhipec/fnet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="76">
      <title>Event extraction from <fixed-case>T</fixed-case>witter using Non-Parametric <fixed-case>B</fixed-case>ayesian Mixture Model with Word Embeddings</title>
      <author><first>Deyu</first><last>Zhou</last></author>
      <author><first>Xuan</first><last>Zhang</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>808–817</pages>
      <url hash="c78baba2">E17-1076</url>
      <abstract>To extract structured representations of newsworthy events from Twitter, unsupervised models typically assume that tweets involving the same named entities and expressed using similar words are likely to belong to the same event. Hence, they group tweets into clusters based on the co-occurrence patterns of named entities and topical keywords. However, there are two main limitations. First, they require the number of events to be known beforehand, which is not realistic in practical applications. Second, they don’t recognise that the same named entity might be referred to by multiple mentions and tweets using different mentions would be wrongly assigned to different events. To overcome these limitations, we propose a non-parametric Bayesian mixture model with word embeddings for event extraction, in which the number of events can be inferred automatically and the issue of lexical variations for the same named entity can be dealt with properly. Our model has been evaluated on three datasets with sizes ranging between 2,499 and over 60 million tweets. Experimental results show that our model outperforms the baseline approach on all datasets by 5-8% in F-measure.</abstract>
      <bibkey>zhou-etal-2017-event</bibkey>
    </paper>
    <paper id="77">
      <title>End-to-end Relation Extraction using Neural Networks and <fixed-case>M</fixed-case>arkov <fixed-case>L</fixed-case>ogic <fixed-case>N</fixed-case>etworks</title>
      <author><first>Sachin</first><last>Pawar</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Girish</first><last>Palshikar</last></author>
      <pages>818–827</pages>
      <url hash="7f2043a3">E17-1077</url>
      <abstract>End-to-end relation extraction refers to identifying boundaries of entity mentions, entity types of these mentions and appropriate semantic relation for each pair of mentions. Traditionally, separate predictive models were trained for each of these tasks and were used in a “pipeline” fashion where output of one model is fed as input to another. But it was observed that addressing some of these tasks jointly results in better performance. We propose a single, joint neural network based model to carry out all the three tasks of boundary identification, entity type classification and relation type classification. This model is referred to as “All Word Pairs” model (AWP-NN) as it assigns an appropriate label to each word pair in a given sentence for performing end-to-end relation extraction. We also propose to refine output of the AWP-NN model by using inference in Markov Logic Networks (MLN) so that additional domain knowledge can be effectively incorporated. We demonstrate effectiveness of our approach by achieving better end-to-end relation extraction performance than all 4 previous joint modelling approaches, on the standard dataset of ACE 2004.</abstract>
      <bibkey>pawar-etal-2017-end</bibkey>
    </paper>
    <paper id="78">
      <title>Trust, but Verify! Better Entity Linking through Automatic Verification</title>
      <author><first>Benjamin</first><last>Heinzerling</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <author><first>Chin-Yew</first><last>Lin</last></author>
      <pages>828–838</pages>
      <url hash="d5f5781e">E17-1078</url>
      <abstract>We introduce automatic verification as a post-processing step for entity linking (EL). The proposed method trusts EL system results collectively, by assuming entity mentions are mostly linked correctly, in order to create a semantic profile of the given text using geospatial and temporal information, as well as fine-grained entity types. This profile is then used to automatically verify each linked mention individually, i.e., to predict whether it has been linked correctly or not. Verification allows leveraging a rich set of global and pairwise features that would be prohibitively expensive for EL systems employing global inference. Evaluation shows consistent improvements across datasets and systems. In particular, when applied to state-of-the-art systems, our method yields an absolute improvement in linking performance of up to 1.7 F1 on AIDA/CoNLL’03 and up to 2.4 F1 on the English TAC KBP 2015 TEDL dataset.</abstract>
      <bibkey>heinzerling-etal-2017-trust</bibkey>
    </paper>
    <paper id="79">
      <title>Named Entity Recognition in the Medical Domain with Constrained <fixed-case>CRF</fixed-case> Models</title>
      <author><first>Charles</first><last>Jochim</last></author>
      <author><first>Léa</first><last>Deleris</last></author>
      <pages>839–849</pages>
      <url hash="cdde40fa">E17-1079</url>
      <abstract>This paper investigates how to improve performance on information extraction tasks by constraining and sequencing CRF-based approaches. We consider two different relation extraction tasks, both from the medical literature: dependence relations and probability statements. We explore whether adding constraints can lead to an improvement over standard CRF decoding. Results on our relation extraction tasks are promising, showing significant increases in performance from both (i) adding constraints to post-process the output of a baseline CRF, which captures “domain knowledge”, and (ii) further allowing flexibility in the application of those constraints by leveraging a binary classifier as a pre-processing step.</abstract>
      <bibkey>jochim-deleris-2017-named</bibkey>
    </paper>
    <paper id="80">
      <title>Learning and Knowledge Transfer with Memory Networks for Machine Comprehension</title>
      <author><first>Mohit</first><last>Yadav</last></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <author><first>Gautam</first><last>Shroff</last></author>
      <pages>850–859</pages>
      <url hash="fa3a87f2">E17-1080</url>
      <abstract>Enabling machines to read and comprehend unstructured text remains an unfulfilled goal for NLP research. Recent research efforts on the “machine comprehension” task have managed to achieve close to ideal performance on simulated data. However, achieving similar levels of performance on small real world datasets has proved difficult; major challenges stem from the large vocabulary size, complex grammar, and, the frequent ambiguities in linguistic structure. On the other hand, the requirement of human generated annotations for training, in order to ensure a sufficiently diverse set of questions is prohibitively expensive. Motivated by these practical issues, we propose a novel curriculum inspired training procedure for Memory Networks to improve the performance for machine comprehension with relatively small volumes of training data. Additionally, we explore various training regimes for Memory Networks to allow knowledge transfer from a closely related domain having larger volumes of labelled data. We also suggest the use of a loss function to incorporate the asymmetric nature of knowledge transfer. Our experiments demonstrate improvements on Dailymail, CNN, and MCTest datasets.</abstract>
      <bibkey>yadav-etal-2017-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mctest">MCTest</pwcdataset>
    </paper>
    <paper id="81">
      <title>If No Media Were Allowed inside the Venue, Was Anybody Allowed?</title>
      <author><first>Zahra</first><last>Sarabi</last></author>
      <author><first>Eduardo</first><last>Blanco</last></author>
      <pages>860–869</pages>
      <url hash="0cef6d01">E17-1081</url>
      <abstract>This paper presents a framework to understand negation in positive terms. Specifically, we extract positive meaning from negation when the negation cue syntactically modifies a noun or adjective. Our approach is grounded on generating potential positive interpretations automatically, and then scoring them. Experimental results show that interpretations scored high can be reliably identified.</abstract>
      <bibkey>sarabi-blanco-2017-media</bibkey>
    </paper>
    <paper id="82">
      <title>Metaheuristic Approaches to Lexical Substitution and Simplification</title>
      <author><first>Sallam</first><last>Abualhaija</last></author>
      <author><first>Tristan</first><last>Miller</last></author>
      <author><first>Judith</first><last>Eckle-Kohler</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <author><first>Karl-Heinz</first><last>Zimmermann</last></author>
      <pages>870–880</pages>
      <url hash="a55bdc11">E17-1082</url>
      <abstract>In this paper, we propose using metaheuristics—in particular, simulated annealing and the new D-Bees algorithm—to solve word sense disambiguation as an optimization problem within a knowledge-based lexical substitution system. We are the first to perform such an extrinsic evaluation of metaheuristics, for which we use two standard lexical substitution datasets, one English and one German. We find that D-Bees has robust performance for both languages, and performs better than simulated annealing, though both achieve good results. Moreover, the D-Bees–based lexical substitution system outperforms state-of-the-art systems on several evaluation metrics. We also show that D-Bees achieves competitive performance in lexical simplification, a variant of lexical substitution.</abstract>
      <bibkey>abualhaija-etal-2017-metaheuristic</bibkey>
    </paper>
    <paper id="83">
      <title>Paraphrasing Revisited with Neural Machine Translation</title>
      <author><first>Jonathan</first><last>Mallinson</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>881–893</pages>
      <url hash="7e819361">E17-1083</url>
      <abstract>Recognizing and generating paraphrases is an important component in many natural language processing applications. A well-established technique for automatically extracting paraphrases leverages bilingual corpora to find meaning-equivalent phrases in a single language by “pivoting” over a shared translation in another language. In this paper we revisit bilingual pivoting in the context of neural machine translation and present a paraphrasing model based purely on neural networks. Our model represents paraphrases in a continuous space, estimates the degree of semantic relatedness between text segments of arbitrary length, and generates candidate paraphrases for any source input. Experimental results across tasks and datasets show that neural paraphrases outperform those obtained with conventional phrase-based pivoting approaches.</abstract>
      <bibkey>mallinson-etal-2017-paraphrasing</bibkey>
    </paper>
    <paper id="84">
      <title>Multilingual Training of Crosslingual Word Embeddings</title>
      <author><first>Long</first><last>Duong</last></author>
      <author><first>Hiroshi</first><last>Kanayama</last></author>
      <author><first>Tengfei</first><last>Ma</last></author>
      <author><first>Steven</first><last>Bird</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <pages>894–904</pages>
      <url hash="749c13f3">E17-1084</url>
      <abstract>Crosslingual word embeddings represent lexical items from different languages using the same vector space, enabling crosslingual transfer. Most prior work constructs embeddings for a pair of languages, with English on one side. We investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space.In this way, we can exploit and combine strength of many languages. We obtained high performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks.</abstract>
      <bibkey>duong-etal-2017-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/panlex">Panlex</pwcdataset>
    </paper>
    <paper id="85">
      <title>Building Lexical Vector Representations from Concept Definitions</title>
      <author><first>Danilo</first><last>Silva de Carvalho</last></author>
      <author><first>Minh Le</first><last>Nguyen</last></author>
      <pages>905–915</pages>
      <url hash="c54fcb75">E17-1085</url>
      <abstract>The use of distributional language representations have opened new paths in solving a variety of NLP problems. However, alternative approaches can take advantage of information unavailable through pure statistical means. This paper presents a method for building vector representations from meaning unit blocks called concept definitions, which are obtained by extracting information from a curated linguistic resource (Wiktionary). The representations obtained in this way can be compared through conventional cosine similarity and are also interpretable by humans. Evaluation was conducted in semantic similarity and relatedness test sets, with results indicating a performance comparable to other methods based on single linguistic resource extraction. The results also indicate noticeable performance gains when combining distributional similarity scores with the ones obtained using this approach. Additionally, a discussion on the proposed method’s shortcomings is provided in the analysis of error cases.</abstract>
      <bibkey>silva-de-carvalho-nguyen-2017-building</bibkey>
    </paper>
    <paper id="86">
      <title><fixed-case>S</fixed-case>hotgun<fixed-case>WSD</fixed-case>: An unsupervised algorithm for global word sense disambiguation inspired by <fixed-case>DNA</fixed-case> sequencing</title>
      <author><first>Andrei</first><last>Butnaru</last></author>
      <author><first>Radu Tudor</first><last>Ionescu</last></author>
      <author><first>Florentina</first><last>Hristea</last></author>
      <pages>916–926</pages>
      <url hash="8ad8d8df">E17-1086</url>
      <abstract>In this paper, we present a novel unsupervised algorithm for word sense disambiguation (WSD) at the document level. Our algorithm is inspired by a widely-used approach in the field of genetics for whole genome sequencing, known as the Shotgun sequencing technique. The proposed WSD algorithm is based on three main steps. First, a brute-force WSD algorithm is applied to short context windows (up to 10 words) selected from the document in order to generate a short list of likely sense configurations for each window. In the second step, these local sense configurations are assembled into longer composite configurations based on suffix and prefix matching. The resulted configurations are ranked by their length, and the sense of each word is chosen based on a voting scheme that considers only the top k configurations in which the word appears. We compare our algorithm with other state-of-the-art unsupervised WSD algorithms and demonstrate better performance, sometimes by a very large margin. We also show that our algorithm can yield better performance than the Most Common Sense (MCS) baseline on one data set. Moreover, our algorithm has a very small number of parameters, is robust to parameter tuning, and, unlike other bio-inspired methods, it gives a deterministic solution (it does not involve random choices).</abstract>
      <bibkey>butnaru-etal-2017-shotgunwsd</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2013">SemEval 2013</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senseval-2-1">Senseval-2</pwcdataset>
    </paper>
    <paper id="87">
      <title><fixed-case>L</fixed-case>anide<fixed-case>NN</fixed-case>: Multilingual Language Identification on Character Window</title>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>927–936</pages>
      <url hash="a1feec36">E17-1087</url>
      <abstract>In language identification, a common first step in natural language processing, we want to automatically determine the language of some input text. Monolingual language identification assumes that the given document is written in one language. In multilingual language identification, the document is usually in two or three languages and we just want their names. We aim one step further and propose a method for textual language identification where languages can change arbitrarily and the goal is to identify the spans of each of the languages. Our method is based on Bidirectional Recurrent Neural Networks and it performs well in monolingual and multilingual language identification tasks on six datasets covering 131 languages. The method keeps the accuracy also for short documents and across domains, so it is ideal for off-the-shelf use without preparation of training data.</abstract>
      <bibkey>kocmi-bojar-2017-lanidenn</bibkey>
      <pwccode url="https://github.com/tomkocmi/LanideNN" additional="false">tomkocmi/LanideNN</pwccode>
    </paper>
    <paper id="88">
      <title>Cross-Lingual Word Embeddings for Low-Resource Language Modeling</title>
      <author><first>Oliver</first><last>Adams</last></author>
      <author><first>Adam</first><last>Makarucha</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Steven</first><last>Bird</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <pages>937–947</pages>
      <url hash="c4c89420">E17-1088</url>
      <abstract>Most languages have no established writing system and minimal written records. However, textual data is essential for natural language processing, and particularly important for training language models to support speech recognition. Even in cases where text data is missing, there are some languages for which bilingual lexicons are available, since creating lexicons is a fundamental task of documentary linguistics. We investigate the use of such lexicons to improve language models when textual training data is limited to as few as a thousand sentences. The method involves learning cross-lingual word embeddings as a preliminary step in training monolingual language models. Results across a number of languages show that language models are improved by this pre-training. Application to Yongning Na, a threatened language, highlights challenges in deploying the approach in real low-resource environments.</abstract>
      <bibkey>adams-etal-2017-cross</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/panlex">Panlex</pwcdataset>
    </paper>
    <paper id="89">
      <title>Consistent Translation of Repeated Nouns using Syntactic and Semantic Cues</title>
      <author><first>Xiao</first><last>Pu</last></author>
      <author><first>Laura</first><last>Mascarell</last></author>
      <author><first>Andrei</first><last>Popescu-Belis</last></author>
      <pages>948–957</pages>
      <url hash="ad8e85f3">E17-1089</url>
      <abstract>We propose a method to decide whether two occurrences of the same noun in a source text should be translated consistently, i.e. using the same noun in the target text as well. We train and test classifiers that predict consistent translations based on lexical, syntactic, and semantic features. We first evaluate the accuracy of our classifiers intrinsically, in terms of the accuracy of consistency predictions, over a subset of the UN Corpus. Then, we also evaluate them in combination with phrase-based statistical MT systems for Chinese-to-English and German-to-English. We compare the automatic post-editing of noun translations with the re-ranking of the translation hypotheses based on the classifiers’ output, and also use these methods in combination. This improves over the baseline and closes up to 50% of the gap in BLEU scores between the baseline and an oracle classifier.</abstract>
      <bibkey>pu-etal-2017-consistent</bibkey>
    </paper>
    <paper id="90">
      <title>Psycholinguistic Models of Sentence Processing Improve Sentence Readability Ranking</title>
      <author><first>David M.</first><last>Howcroft</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>958–968</pages>
      <url hash="854173db">E17-1090</url>
      <abstract>While previous research on readability has typically focused on document-level measures, recent work in areas such as natural language generation has pointed out the need of sentence-level readability measures. Much of psycholinguistics has focused for many years on processing measures that provide difficulty estimates on a word-by-word basis. However, these psycholinguistic measures have not yet been tested on sentence readability ranking tasks. In this paper, we use four psycholinguistic measures: idea density, surprisal, integration cost, and embedding depth to test whether these features are predictive of readability levels. We find that psycholinguistic features significantly improve performance by up to 3 percentage points over a standard document-level readability metric baseline.</abstract>
      <bibkey>howcroft-demberg-2017-psycholinguistic</bibkey>
    </paper>
    <paper id="91">
      <title>Web-Scale Language-Independent Cataloging of Noisy Product Listings for <fixed-case>E</fixed-case>-Commerce</title>
      <author><first>Pradipto</first><last>Das</last></author>
      <author><first>Yandi</first><last>Xia</last></author>
      <author><first>Aaron</first><last>Levine</last></author>
      <author><first>Giuseppe</first><last>Di Fabbrizio</last></author>
      <author><first>Ankur</first><last>Datta</last></author>
      <pages>969–979</pages>
      <url hash="f2a2e944">E17-1091</url>
      <abstract>The cataloging of product listings through taxonomy categorization is a fundamental problem for any e-commerce marketplace, with applications ranging from personalized search recommendations to query understanding. However, manual and rule based approaches to categorization are not scalable. In this paper, we compare several classifiers for categorizing listings in both English and Japanese product catalogs. We show empirically that a combination of words from product titles, navigational breadcrumbs, and list prices, when available, improves results significantly. We outline a novel method using correspondence topic models and a lightweight manual process to reduce noise from mis-labeled data in the training set. We contrast linear models, gradient boosted trees (GBTs) and convolutional neural networks (CNNs), and show that GBTs and CNNs yield the highest gains in error reduction. Finally, we show GBTs applied in a language-agnostic way on a large-scale Japanese e-commerce dataset have improved taxonomy categorization performance over current state-of-the-art based on deep belief network models.</abstract>
      <bibkey>das-etal-2017-web</bibkey>
    </paper>
    <paper id="92">
      <title>Recognizing Insufficiently Supported Arguments in Argumentative Essays</title>
      <author><first>Christian</first><last>Stab</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>980–990</pages>
      <url hash="8da794c8">E17-1092</url>
      <abstract>In this paper, we propose a new task for assessing the quality of natural language arguments. The premises of a well-reasoned argument should provide enough evidence for accepting or rejecting its claim. Although this criterion, known as sufficiency, is widely adopted in argumentation theory, there are no empirical studies on its applicability to real arguments. In this work, we show that human annotators substantially agree on the sufficiency criterion and introduce a novel annotated corpus. Furthermore, we experiment with feature-rich SVMs and Convolutional Neural Networks and achieve 84% accuracy for automatically identifying insufficiently supported arguments. The final corpus as well as the annotation guideline are freely available for encouraging future research on argument quality.</abstract>
      <bibkey>stab-gurevych-2017-recognizing</bibkey>
    </paper>
    <paper id="93">
      <title>Distributed Document and Phrase Co-embeddings for Descriptive Clustering</title>
      <author><first>Motoki</first><last>Sato</last></author>
      <author><first>Austin J.</first><last>Brockmeier</last></author>
      <author><first>Georgios</first><last>Kontonatsios</last></author>
      <author><first>Tingting</first><last>Mu</last></author>
      <author><first>John Y.</first><last>Goulermas</last></author>
      <author><first>Jun’ichi</first><last>Tsujii</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <pages>991–1001</pages>
      <url hash="662b298b">E17-1093</url>
      <abstract>Descriptive document clustering aims to automatically discover groups of semantically related documents and to assign a meaningful label to characterise the content of each cluster. In this paper, we present a descriptive clustering approach that employs a distributed representation model, namely the paragraph vector model, to capture semantic similarities between documents and phrases. The proposed method uses a joint representation of phrases and documents (i.e., a co-embedding) to automatically select a descriptive phrase that best represents each document cluster. We evaluate our method by comparing its performance to an existing state-of-the-art descriptive clustering method that also uses co-embedding but relies on a bag-of-words representation. Results obtained on benchmark datasets demonstrate that the paragraph vector-based method obtains superior performance over the existing approach in both identifying clusters and assigning appropriate descriptive labels to them.</abstract>
      <bibkey>sato-etal-2017-distributed</bibkey>
    </paper>
    <paper id="94">
      <title><fixed-case>SMART</fixed-case>ies: Sentiment Models for <fixed-case>A</fixed-case>rabic Target entities</title>
      <author><first>Noura</first><last>Farra</last></author>
      <author><first>Kathy</first><last>McKeown</last></author>
      <pages>1002–1013</pages>
      <url hash="5d10f530">E17-1094</url>
      <abstract>We consider entity-level sentiment analysis in Arabic, a morphologically rich language with increasing resources. We present a system that is applied to complex posts written in response to Arabic newspaper articles. Our goal is to identify important entity “targets” within the post along with the polarity expressed about each target. We achieve significant improvements over multiple baselines, demonstrating that the use of specific morphological representations improves the performance of identifying both important targets and their sentiment, and that the use of distributional semantic clusters further boosts performances for these representations, especially when richer linguistic resources are not available.</abstract>
      <bibkey>farra-mckeown-2017-smarties</bibkey>
    </paper>
    <paper id="95">
      <title>Exploring Convolutional Neural Networks for Sentiment Analysis of <fixed-case>S</fixed-case>panish tweets</title>
      <author><first>Isabel</first><last>Segura-Bedmar</last></author>
      <author><first>Antonio</first><last>Quirós</last></author>
      <author><first>Paloma</first><last>Martínez</last></author>
      <pages>1014–1022</pages>
      <url hash="8a138394">E17-1095</url>
      <abstract>Spanish is the third-most used language on the internet, after English and Chinese, with a total of 7.7% (more than 277 million of users) and a huge internet growth of more than 1,400%. However, most work on sentiment analysis has been focused on English. This paper describes a deep learning system for Spanish sentiment analysis. To the best of our knowledge, this is the first work that explores the use of a convolutional neural network to polarity classification of Spanish tweets.</abstract>
      <bibkey>segura-bedmar-etal-2017-exploring</bibkey>
    </paper>
    <paper id="96">
      <title>Contextual Bidirectional Long Short-Term Memory Recurrent Neural Network Language Models: A Generative Approach to Sentiment Analysis</title>
      <author><first>Amr</first><last>Mousa</last></author>
      <author><first>Björn</first><last>Schuller</last></author>
      <pages>1023–1032</pages>
      <url hash="2b8e170b">E17-1096</url>
      <abstract>Traditional learning-based approaches to sentiment analysis of written text use the concept of bag-of-words or bag-of-n-grams, where a document is viewed as a set of terms or short combinations of terms disregarding grammar rules or word order. Novel approaches de-emphasize this concept and view the problem as a sequence classification problem. In this context, recurrent neural networks (RNNs) have achieved significant success. The idea is to use RNNs as discriminative binary classifiers to predict a positive or negative sentiment label at every word position then perform a type of pooling to get a sentence-level polarity. Here, we investigate a novel generative approach in which a separate probability distribution is estimated for every sentiment using language models (LMs) based on long short-term memory (LSTM) RNNs. We introduce a novel type of LM using a modified version of bidirectional LSTM (BLSTM) called contextual BLSTM (cBLSTM), where the probability of a word is estimated based on its full left and right contexts. Our approach is compared with a BLSTM binary classifier. Significant improvements are observed in classifying the IMDB movie review dataset. Further improvements are achieved via model combination.</abstract>
      <bibkey>mousa-schuller-2017-contextual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="97">
      <title>Large-scale Opinion Relation Extraction with Distantly Supervised Neural Network</title>
      <author><first>Changzhi</first><last>Sun</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Man</first><last>Lan</last></author>
      <author><first>Shiliang</first><last>Sun</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>1033–1043</pages>
      <url hash="8781c3ba">E17-1097</url>
      <abstract>We investigate the task of open domain opinion relation extraction. Different from works on manually labeled corpus, we propose an efficient distantly supervised framework based on pattern matching and neural network classifiers. The patterns are designed to automatically generate training data, and the deep learning model is design to capture various lexical and syntactic features. The result algorithm is fast and scalable on large-scale corpus. We test the system on the Amazon online review dataset. The result shows that our model is able to achieve promising performances without any human annotations.</abstract>
      <bibkey>sun-etal-2017-large</bibkey>
    </paper>
    <paper id="98">
      <title>Decoding with Finite-State Transducers on <fixed-case>GPU</fixed-case>s</title>
      <author><first>Arturo</first><last>Argueta</last></author>
      <author><first>David</first><last>Chiang</last></author>
      <pages>1044–1052</pages>
      <url hash="345e451f">E17-1098</url>
      <abstract>Weighted finite automata and transducers (including hidden Markov models and conditional random fields) are widely used in natural language processing (NLP) to perform tasks such as morphological analysis, part-of-speech tagging, chunking, named entity recognition, speech recognition, and others. Parallelizing finite state algorithms on graphics processing units (GPUs) would benefit many areas of NLP. Although researchers have implemented GPU versions of basic graph algorithms, no work, to our knowledge, has been done on GPU algorithms for weighted finite automata. We introduce a GPU implementation of the Viterbi and forward-backward algorithm, achieving speedups of up to 4x over our serial implementations running on different computer architectures and 3335x over widely used tools such as OpenFST.</abstract>
      <bibkey>argueta-chiang-2017-decoding</bibkey>
    </paper>
    <paper id="99">
      <title>Learning to Translate in Real-time with Neural Machine Translation</title>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Victor O.K.</first><last>Li</last></author>
      <pages>1053–1062</pages>
      <url hash="92d57ae8">E17-1099</url>
      <abstract>Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.</abstract>
      <bibkey>gu-etal-2017-learning</bibkey>
      <pwccode url="https://github.com/nyu-dl/dl4mt-simul-trans" additional="false">nyu-dl/dl4mt-simul-trans</pwccode>
    </paper>
    <paper id="100">
      <title>A Multifaceted Evaluation of Neural versus Phrase-Based Machine Translation for 9 Language Directions</title>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Víctor M.</first><last>Sánchez-Cartagena</last></author>
      <pages>1063–1073</pages>
      <url hash="de089a13">E17-1100</url>
      <abstract>We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art neural machine translation and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of sentence length and performance across different error categories. We find out that translations produced by neural machine translation systems are considerably different, more fluent and more accurate in terms of word order compared to those produced by phrase-based systems. Neural machine translation systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences.</abstract>
      <bibkey>toral-sanchez-cartagena-2017-multifaceted</bibkey>
      <pwccode url="https://github.com/antot/neural_vs_-phrasebased_smt_eacl17" additional="false">antot/neural_vs_-phrasebased_smt_eacl17</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="101">
      <title>Personalized Machine Translation: Preserving Original Author Traits</title>
      <author><first>Ella</first><last>Rabinovich</last></author>
      <author><first>Raj Nath</first><last>Patel</last></author>
      <author><first>Shachar</first><last>Mirkin</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Shuly</first><last>Wintner</last></author>
      <pages>1074–1084</pages>
      <url hash="1930aabc">E17-1101</url>
      <abstract>The language that we produce reflects our personality, and various personal and demographic characteristics can be detected in natural language texts. We focus on one particular personal trait of the author, gender, and study how it is manifested in original texts and in translations. We show that author’s gender has a powerful, clear signal in originals texts, but this signal is obfuscated in human and machine translation. We then propose simple domain-adaptation techniques that help retain the original gender traits in the translation, without harming the quality of the translation, thereby creating more personalized machine translation systems.</abstract>
      <attachment type="presentation" hash="2aae1ad2">E17-1101.Presentation.pdf</attachment>
      <bibkey>rabinovich-etal-2017-personalized</bibkey>
    </paper>
    <paper id="102">
      <title>Bilingual Lexicon Induction by Learning to Combine Word-Level and Character-Level Representations</title>
      <author><first>Geert</first><last>Heyman</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>1085–1095</pages>
      <url hash="b315519e">E17-1102</url>
      <abstract>We study the problem of bilingual lexicon induction (BLI) in a setting where some translation resources are available, but unknown translations are sought for certain, possibly domain-specific terminology. We frame BLI as a classification problem for which we design a neural network based classification architecture composed of recurrent long short-term memory and deep feed forward networks. The results show that word- and character-level representations each improve state-of-the-art results for BLI, and the best results are obtained by exploiting the synergy between these word- and character-level representations in the classification model.</abstract>
      <bibkey>heyman-etal-2017-bilingual</bibkey>
    </paper>
    <paper id="103">
      <title>Grouping business news stories based on salience of named entities</title>
      <author><first>Llorenç</first><last>Escoter</last></author>
      <author><first>Lidia</first><last>Pivovarova</last></author>
      <author><first>Mian</first><last>Du</last></author>
      <author><first>Anisia</first><last>Katinskaia</last></author>
      <author><first>Roman</first><last>Yangarber</last></author>
      <pages>1096–1106</pages>
      <url hash="fc293ee7">E17-1103</url>
      <abstract>In news aggregation systems focused on broad news domains, certain stories may appear in multiple articles. Depending on the relative importance of the story, the number of versions can reach dozens or hundreds within a day. The text in these versions may be nearly identical or quite different. Linking multiple versions of a story into a single group brings several important benefits to the end-user–reducing the cognitive load on the reader, as well as signaling the relative importance of the story. We present a grouping algorithm, and explore several vector-based representations of input documents: from a baseline using keywords, to a method using salience–a measure of importance of named entities in the text. We demonstrate that features beyond keywords yield substantial improvements, verified on a manually-annotated corpus of business news stories.</abstract>
      <bibkey>escoter-etal-2017-grouping</bibkey>
    </paper>
    <paper id="104">
      <title>Very Deep Convolutional Networks for Text Classification</title>
      <author><first>Alexis</first><last>Conneau</last></author>
      <author><first>Holger</first><last>Schwenk</last></author>
      <author><first>Loïc</first><last>Barrault</last></author>
      <author><first>Yann</first><last>Lecun</last></author>
      <pages>1107–1116</pages>
      <url hash="12484645">E17-1104</url>
      <abstract>The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.</abstract>
      <bibkey>conneau-etal-2017-deep</bibkey>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/yahoo-answers">Yahoo! Answers</pwcdataset>
    </paper>
    <paper id="105">
      <title>“<fixed-case>P</fixed-case>age<fixed-case>R</fixed-case>ank” for Argument Relevance</title>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <author><first>Yamen</first><last>Ajjour</last></author>
      <pages>1117–1127</pages>
      <url hash="69dd1d67">E17-1105</url>
      <abstract>Future search engines are expected to deliver pro and con arguments in response to queries on controversial topics. While argument mining is now in the focus of research, the question of how to retrieve the relevant arguments remains open. This paper proposes a radical model to assess relevance objectively at web scale: the relevance of an argument’s conclusion is decided by what other arguments reuse it as a premise. We build an argument graph for this model that we analyze with a recursive weighting scheme, adapting key ideas of PageRank. In experiments on a large ground-truth argument graph, the resulting relevance scores correlate with human average judgments. We outline what natural language challenges must be faced at web scale in order to stepwise bring argument relevance to web search engines.</abstract>
      <bibkey>wachsmuth-etal-2017-pagerank</bibkey>
    </paper>
    <paper id="106">
      <title>Predicting Counselor Behaviors in Motivational Interviewing Encounters</title>
      <author><first>Verónica</first><last>Pérez-Rosas</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <author><first>Kenneth</first><last>Resnicow</last></author>
      <author><first>Satinder</first><last>Singh</last></author>
      <author><first>Lawrence</first><last>An</last></author>
      <author><first>Kathy J.</first><last>Goggin</last></author>
      <author><first>Delwyn</first><last>Catley</last></author>
      <pages>1128–1137</pages>
      <url hash="f78d3125">E17-1106</url>
      <abstract>As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors’ language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.</abstract>
      <bibkey>perez-rosas-etal-2017-predicting</bibkey>
    </paper>
    <paper id="107">
      <title>Authorship Attribution Using Text Distortion</title>
      <author><first>Efstathios</first><last>Stamatatos</last></author>
      <pages>1138–1149</pages>
      <url hash="6cedf6ed">E17-1107</url>
      <abstract>Authorship attribution is associated with important applications in forensics and humanities research. A crucial point in this field is to quantify the personal style of writing, ideally in a way that is not affected by changes in topic or genre. In this paper, we present a novel method that enhances authorship attribution effectiveness by introducing a text distortion step before extracting stylometric measures. The proposed method attempts to mask topic-specific information that is not related to the personal style of authors. Based on experiments on two main tasks in authorship attribution, closed-set attribution and authorship verification, we demonstrate that the proposed approach can enhance existing methods especially under cross-topic conditions, where the training and test corpora do not match in topic.</abstract>
      <bibkey>stamatatos-2017-authorship</bibkey>
    </paper>
    <paper id="108">
      <title>Structured Learning for Temporal Relation Extraction from Clinical Records</title>
      <author><first>Artuur</first><last>Leeuwenberg</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>1150–1158</pages>
      <url hash="b5d807cf">E17-1108</url>
      <abstract>We propose a scalable structured learning model that jointly predicts temporal relations between events and temporal expressions (TLINKS), and the relation between these events and the document creation time (DCTR). We employ a structured perceptron, together with integer linear programming constraints for document-level inference during training and prediction to exploit relational properties of temporality, together with global learning of the relations at the document level. Moreover, this study gives insights in the results of integrating constraints for temporal relation extraction when using structured learning and prediction. Our best system outperforms the state-of-the art on both the CONTAINS TLINK task, and the DCTR task.</abstract>
      <bibkey>leeuwenberg-moens-2017-structured</bibkey>
      <pwccode url="https://github.com/tuur/SPTempRels" additional="false">tuur/SPTempRels</pwccode>
    </paper>
    <paper id="109">
      <title>Entity Extraction in Biomedical Corpora: An Approach to Evaluate Word Embedding Features with <fixed-case>PSO</fixed-case> based Feature Selection</title>
      <author><first>Shweta</first><last>Yadav</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>1159–1170</pages>
      <url hash="180339e4">E17-1109</url>
      <abstract>Text mining has drawn significant attention in recent past due to the rapid growth in biomedical and clinical records. Entity extraction is one of the fundamental components for biomedical text mining. In this paper, we propose a novel approach of feature selection for entity extraction that exploits the concept of deep learning and Particle Swarm Optimization (PSO). The system utilizes word embedding features along with several other features extracted by studying the properties of the datasets. We obtain an interesting observation that compact word embedding features as determined by PSO are more effective compared to the entire word embedding feature set for entity extraction. The proposed system is evaluated on three benchmark biomedical datasets such as GENIA, GENETAG, and AiMed. The effectiveness of the proposed approach is evident with significant performance gains over the baseline models as well as the other existing systems. We observe improvements of 7.86%, 5.27% and 7.25% F-measure points over the baseline models for GENIA, GENETAG, and AiMed dataset respectively.</abstract>
      <bibkey>yadav-etal-2017-entity</bibkey>
    </paper>
    <paper id="110">
      <title>Distant Supervision for Relation Extraction beyond the Sentence Boundary</title>
      <author><first>Chris</first><last>Quirk</last></author>
      <author><first>Hoifung</first><last>Poon</last></author>
      <pages>1171–1182</pages>
      <url hash="f9024ad9">E17-1110</url>
      <abstract>The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross-sentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach.</abstract>
      <bibkey>quirk-poon-2017-distant</bibkey>
    </paper>
    <paper id="111">
      <title>Noise Mitigation for Neural Entity Typing and Relation Extraction</title>
      <author><first>Yadollah</first><last>Yaghoobzadeh</last></author>
      <author><first>Heike</first><last>Adel</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>1183–1194</pages>
      <url hash="d91e363c">E17-1111</url>
      <abstract>In this paper, we address two different types of noise in information extraction models: noise from distant supervision and noise from pipeline input features. Our target tasks are entity typing and relation extraction. For the first noise type, we introduce multi-instance multi-label learning algorithms using neural network models, and apply them to fine-grained entity typing for the first time. Our model outperforms the state-of-the-art supervised approach which uses global embeddings of entities. For the second noise type, we propose ways to improve the integration of noisy entity type predictions into relation extraction. Our experiments show that probabilistic predictions are more robust than discrete predictions and that joint training of the two tasks performs best.</abstract>
      <bibkey>yaghoobzadeh-etal-2017-noise</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/figment">Figment</pwcdataset>
    </paper>
    <paper id="112">
      <title>Analyzing Semantic Change in <fixed-case>J</fixed-case>apanese Loanwords</title>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Ryo</first><last>Nagata</last></author>
      <author><first>Yoshifumi</first><last>Kawasaki</last></author>
      <pages>1195–1204</pages>
      <url hash="58e35517">E17-1112</url>
      <abstract>We analyze semantic changes in loanwords from English that are used in Japanese (Japanese loanwords). Specifically, we create word embeddings of English and Japanese and map the Japanese embeddings into the English space so that we can calculate the similarity of each Japanese word and each English word. We then attempt to find loanwords that are semantically different from their original, see if known meaning changes are correctly captured, and show the possibility of using our methodology in language education.</abstract>
      <bibkey>takamura-etal-2017-analyzing</bibkey>
    </paper>
    <paper id="113">
      <title>Using support vector machines and state-of-the-art algorithms for phonetic alignment to identify cognates in multi-lingual wordlists</title>
      <author><first>Gerhard</first><last>Jäger</last></author>
      <author><first>Johann-Mattis</first><last>List</last></author>
      <author><first>Pavel</first><last>Sofroniev</last></author>
      <pages>1205–1216</pages>
      <url hash="1797d28d">E17-1113</url>
      <abstract>Most current approaches in phylogenetic linguistics require as input multilingual word lists partitioned into sets of etymologically related words (cognates). Cognate identification is so far done manually by experts, which is time consuming and as of yet only available for a small number of well-studied language families. Automatizing this step will greatly expand the empirical scope of phylogenetic methods in linguistics, as raw wordlists (in phonetic transcription) are much easier to obtain than wordlists in which cognate words have been fully identified and annotated, even for under-studied languages. A couple of different methods have been proposed in the past, but they are either disappointing regarding their performance or not applicable to larger datasets. Here we present a new approach that uses support vector machines to unify different state-of-the-art methods for phonetic alignment and cognate detection within a single framework. Training and evaluating these method on a typologically broad collection of gold-standard data shows it to be superior to the existing state of the art.</abstract>
      <bibkey>jager-etal-2017-using</bibkey>
    </paper>
    <paper id="114">
      <title>A Multi-task Approach to Predict Likability of Books</title>
      <author><first>Suraj</first><last>Maharjan</last></author>
      <author><first>John</first><last>Arevalo</last></author>
      <author><first>Manuel</first><last>Montes</last></author>
      <author><first>Fabio A.</first><last>González</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>1217–1227</pages>
      <url hash="ffe9394f">E17-1114</url>
      <abstract>We investigate the value of feature engineering and neural network models for predicting successful writing. Similar to previous work, we treat this as a binary classification task and explore new strategies to automatically learn representations from book contents. We evaluate our feature set on two different corpora created from Project Gutenberg books. The first presents a novel approach for generating the gold standard labels for the task and the other is based on prior research. Using a combination of hand-crafted and recurrent neural network learned representations in a dual learning setting, we obtain the best performance of 73.50% weighted F1-score.</abstract>
      <bibkey>maharjan-etal-2017-multi</bibkey>
    </paper>
    <paper id="115">
      <title>A Data-Oriented Model of Literary Language</title>
      <author><first>Andreas</first><last>van Cranenburgh</last></author>
      <author><first>Rens</first><last>Bod</last></author>
      <pages>1228–1238</pages>
      <url hash="682e7f1f">E17-1115</url>
      <abstract>We consider the task of predicting how literary a text is, with a gold standard from human ratings. Aside from a standard bigram baseline, we apply rich syntactic tree fragments, mined from the training set, and a series of hand-picked features. Our model is the first to distinguish degrees of highly and less literary novels using a variety of lexical and syntactic features, and explains 76.0 % of the variation in literary ratings.</abstract>
      <bibkey>van-cranenburgh-bod-2017-data</bibkey>
    </paper>
    <paper id="116">
      <title>Aye or naw, whit dae ye hink? <fixed-case>S</fixed-case>cottish independence and linguistic identity on social media</title>
      <author><first>Philippa</first><last>Shoemark</last></author>
      <author><first>Debnil</first><last>Sur</last></author>
      <author><first>Luke</first><last>Shrimpton</last></author>
      <author><first>Iain</first><last>Murray</last></author>
      <author><first>Sharon</first><last>Goldwater</last></author>
      <pages>1239–1248</pages>
      <url hash="30f63e7f">E17-1116</url>
      <abstract>Political surveys have indicated a relationship between a sense of Scottish identity and voting decisions in the 2014 Scottish Independence Referendum. Identity is often reflected in language use, suggesting the intuitive hypothesis that individuals who support Scottish independence are more likely to use distinctively Scottish words than those who oppose it. In the first large-scale study of sociolinguistic variation on social media in the UK, we identify distinctively Scottish terms in a data-driven way, and find that these terms are indeed used at a higher rate by users of pro-independence hashtags than by users of anti-independence hashtags. However, we also find that in general people are less likely to use distinctively Scottish words in tweets with referendum-related hashtags than in their general Twitter activity. We attribute this difference to style shifting relative to audience, aligning with previous work showing that Twitter users tend to use fewer local variants when addressing a broader audience.</abstract>
      <bibkey>shoemark-etal-2017-aye</bibkey>
    </paper>
    <paper id="117">
      <title>What Do Recurrent Neural Network Grammars Learn About Syntax?</title>
      <author><first>Adhiguna</first><last>Kuncoro</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <author><first>Lingpeng</first><last>Kong</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>1249–1258</pages>
      <url hash="c5bf3168">E17-1117</url>
      <abstract>Recurrent neural network grammars (RNNG) are a recently proposed probablistic generative modeling family for natural language. They show state-of-the-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model’s latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis.</abstract>
      <bibkey>kuncoro-etal-2017-recurrent</bibkey>
      <pwccode url="https://github.com/clab/rnng" additional="false">clab/rnng</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="118">
      <title>Incremental Discontinuous Phrase Structure Parsing with the <fixed-case>GAP</fixed-case> Transition</title>
      <author><first>Maximin</first><last>Coavoux</last></author>
      <author><first>Benoît</first><last>Crabbé</last></author>
      <pages>1259–1270</pages>
      <url hash="e5679dd8">E17-1118</url>
      <abstract>This article introduces a novel transition system for discontinuous lexicalized constituent parsing called SR-GAP. It is an extension of the shift-reduce algorithm with an additional gap transition. Evaluation on two German treebanks shows that SR-GAP outperforms the previous best transition-based discontinuous parser (Maier, 2015) by a large margin (it is notably twice as accurate on the prediction of discontinuous constituents), and is competitive with the state of the art (Fernández-González and Martins, 2015). As a side contribution, we adapt span features (Hall et al., 2014) to discontinuous parsing.</abstract>
      <bibkey>coavoux-crabbe-2017-incremental</bibkey>
      <pwccode url="https://github.com/mcoavoux/mtg" additional="false">mcoavoux/mtg</pwccode>
    </paper>
    <paper id="119">
      <title>Neural Architectures for Fine-grained Entity Type Classification</title>
      <author><first>Sonse</first><last>Shimaoka</last></author>
      <author><first>Pontus</first><last>Stenetorp</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <pages>1271–1280</pages>
      <url hash="a446c065">E17-1119</url>
      <abstract>In this work, we investigate several neural network architectures for fine-grained entity type classification and make three key contributions. Despite being a natural comparison and addition, previous work on attentive neural architectures have not considered hand-crafted features and we combine these with learnt features and establish that they complement each other. Additionally, through quantitative analysis we establish that the attention mechanism learns to attend over syntactic heads and the phrase containing the mention, both of which are known to be strong hand-crafted features for our task. We introduce parameter sharing between labels through a hierarchical encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare models trained using different data. We demonstrate that the choice of training data has a drastic impact on performance, which decreases by as much as 9.85% loose micro F1 score for a previously proposed method. Despite this discrepancy, our best model achieves state-of-the-art results with 75.36% loose micro F1 score on the well-established Figer (GOLD) dataset and we report the best results for models trained using publicly available data for the OntoNotes dataset with 64.93% loose micro F1 score.</abstract>
      <bibkey>shimaoka-etal-2017-neural</bibkey>
      <pwccode url="https://github.com/shimaokasonse/NFGEC" additional="false">shimaokasonse/NFGEC</pwccode>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Proceedings of the 15th Conference of the <fixed-case>E</fixed-case>uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</booktitle>
      <url hash="aa8810b7">E17-2</url>
      <editor><first>Mirella</first><last>Lapata</last></editor>
      <editor><first>Phil</first><last>Blunsom</last></editor>
      <editor><first>Alexander</first><last>Koller</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Valencia, Spain</address>
      <month>April</month>
      <year>2017</year>
      <venue>eacl</venue>
    </meta>
    <frontmatter>
      <url hash="9e41232c">E17-2000</url>
      <bibkey>eacl-2017-european-chapter</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Multilingual Back-and-Forth Conversion between Content and Function Head for Easy Dependency Parsing</title>
      <author><first>Ryosuke</first><last>Kohita</last></author>
      <author><first>Hiroshi</first><last>Noji</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <pages>1–7</pages>
      <url hash="6d54bba4">E17-2001</url>
      <abstract>Universal Dependencies (UD) is becoming a standard annotation scheme cross-linguistically, but it is argued that this scheme centering on content words is harder to parse than the conventional one centering on function words. To improve the parsability of UD, we propose a back-and-forth conversion algorithm, in which we preprocess the training treebank to increase parsability, and reconvert the parser outputs to follow the UD scheme as a postprocess. We show that this technique consistently improves LAS across languages even with a state-of-the-art parser, in particular on core dependency arcs such as nominal modifier. We also provide an in-depth analysis to understand why our method increases parsability.</abstract>
      <bibkey>kohita-etal-2017-multilingual</bibkey>
      <pwccode url="https://github.com/kohilin/MultiBFConv" additional="false">kohilin/MultiBFConv</pwccode>
    </paper>
    <paper id="2">
      <title><fixed-case>URIEL</fixed-case> and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors</title>
      <author><first>Patrick</first><last>Littell</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <author><first>Ke</first><last>Lin</last></author>
      <author><first>Katherine</first><last>Kairis</last></author>
      <author><first>Carlisle</first><last>Turner</last></author>
      <author><first>Lori</first><last>Levin</last></author>
      <pages>8–14</pages>
      <url hash="0482031a">E17-2002</url>
      <abstract>We introduce the URIEL knowledge base for massively multilingual NLP and the lang2vec utility, which provides information-rich vector identifications of languages drawn from typological, geographical, and phylogenetic databases and normalized to have straightforward and consistent formats, naming, and semantics. The goal of URIEL and lang2vec is to enable multilingual NLP, especially on less-resourced languages and make possible types of experiments (especially but not exclusively related to NLP tasks) that are otherwise difficult or impossible due to the sparsity and incommensurability of the data sources. lang2vec vectors have been shown to reduce perplexity in multilingual language modeling, when compared to one-hot language identification vectors.</abstract>
      <bibkey>littell-etal-2017-uriel</bibkey>
    </paper>
    <paper id="3">
      <title>An experimental analysis of Noise-Contrastive Estimation: the noise distribution matters</title>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <pages>15–20</pages>
      <url hash="c4385ae4">E17-2003</url>
      <abstract>Noise Contrastive Estimation (NCE) is a learning procedure that is regularly used to train neural language models, since it avoids the computational bottleneck caused by the output softmax. In this paper, we attempt to explain some of the weaknesses of this objective function, and to draw directions for further developments. Experiments on a small task show the issues raised by an unigram noise distribution, and that a context dependent noise distribution, such as the bigram distribution, can solve these issues and provide stable and data-efficient learning.</abstract>
      <bibkey>labeau-allauzen-2017-experimental</bibkey>
    </paper>
    <paper id="4">
      <title>Robust Training under Linguistic Adversity</title>
      <author><first>Yitong</first><last>Li</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>21–27</pages>
      <url hash="6f7e9d02">E17-2004</url>
      <abstract>Deep neural networks have achieved remarkable results across many language processing tasks, however they have been shown to be susceptible to overfitting and highly sensitive to noise, including adversarial attacks. In this work, we propose a linguistically-motivated approach for training robust models based on exposing the model to corrupted text examples at training time. We consider several flavours of linguistically plausible corruption, include lexical semantic and syntactic methods. Empirically, we evaluate our method with a convolutional neural model across a range of sentiment analysis datasets. Compared with a baseline and the dropout method, our method achieves better overall performance.</abstract>
      <bibkey>li-etal-2017-robust</bibkey>
      <pwccode url="https://github.com/lrank/Linguistic_adversity" additional="false">lrank/Linguistic_adversity</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="5">
      <title>Using <fixed-case>T</fixed-case>witter Language to Predict the Real Estate Market</title>
      <author><first>Mohammadzaman</first><last>Zamani</last></author>
      <author><first>H. Andrew</first><last>Schwartz</last></author>
      <pages>28–33</pages>
      <url hash="5730cc74">E17-2005</url>
      <abstract>We explore whether social media can provide a window into community real estate -foreclosure rates and price changes- beyond that of traditional economic and demographic variables. We find language use in Twitter not only predicts real estate outcomes as well as traditional variables across counties, but that including Twitter language in traditional models leads to a significant improvement (e.g. from Pearson r = :50 to r = :59 for price changes). We overcome the challenge of the relative sparsity and noise in Twitter language variables by showing that training on the residual error of the traditional models leads to more accurate overall assessments. Finally, we discover that it is Twitter language related to business (e.g. ‘company’, ‘marketing’) and technology (e.g. ‘technology’, ‘internet’), among others, that yield predictive power over economics.</abstract>
      <bibkey>zamani-schwartz-2017-using</bibkey>
    </paper>
    <paper id="6">
      <title>Lexical Simplification with Neural Ranking</title>
      <author><first>Gustavo</first><last>Paetzold</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>34–40</pages>
      <url hash="8c2e55aa">E17-2006</url>
      <abstract>We present a new Lexical Simplification approach that exploits Neural Networks to learn substitutions from the Newsela corpus - a large set of professionally produced simplifications. We extract candidate substitutions by combining the Newsela corpus with a retrofitted context-aware word embeddings model and rank them using a new neural regression model that learns rankings from annotated data. This strategy leads to the highest Accuracy, Precision and F1 scores to date in standard datasets for the task.</abstract>
      <bibkey>paetzold-specia-2017-lexical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="7">
      <title>The limits of automatic summarisation according to <fixed-case>ROUGE</fixed-case></title>
      <author><first>Natalie</first><last>Schluter</last></author>
      <pages>41–45</pages>
      <url hash="28da8509">E17-2007</url>
      <abstract>This paper discusses some central caveats of summarisation, incurred in the use of the ROUGE metric for evaluation, with respect to optimal solutions. The task is NP-hard, of which we give the first proof. Still, as we show empirically for three central benchmark datasets for the task, greedy algorithms empirically seem to perform optimally according to the metric. Additionally, overall quality assurance is problematic: there is no natural upper bound on the quality of summarisation systems, and even humans are excluded from performing optimal summarisation.</abstract>
      <bibkey>schluter-2017-limits</bibkey>
    </paper>
    <paper id="8">
      <title>Crowd-Sourced Iterative Annotation for Narrative Summarization Corpora</title>
      <author><first>Jessica</first><last>Ouyang</last></author>
      <author><first>Serina</first><last>Chang</last></author>
      <author><first>Kathy</first><last>McKeown</last></author>
      <pages>46–51</pages>
      <url hash="8ae8c040">E17-2008</url>
      <abstract>We present an iterative annotation process for producing aligned, parallel corpora of abstractive and extractive summaries for narrative. Our approach uses a combination of trained annotators and crowd-sourcing, allowing us to elicit human-generated summaries and alignments quickly and at low cost. We use crowd-sourcing to annotate aligned phrases with the text-to-text generation techniques needed to transform each phrase into the other. We apply this process to a corpus of 476 personal narratives, which we make available on the Web.</abstract>
      <bibkey>ouyang-etal-2017-crowd</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sentence-compression">Sentence Compression</pwcdataset>
    </paper>
    <paper id="9">
      <title>Broad Context Language Modeling as Reading Comprehension</title>
      <author><first>Zewei</first><last>Chu</last></author>
      <author><first>Hai</first><last>Wang</last></author>
      <author><first>Kevin</first><last>Gimpel</last></author>
      <author><first>David</first><last>McAllester</last></author>
      <pages>52–57</pages>
      <url hash="ed1b2312">E17-2009</url>
      <abstract>Progress in text understanding has been driven by large datasets that test particular capabilities, like recent datasets for reading comprehension (Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence. We view LAMBADA as a reading comprehension problem and apply comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 49%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed.</abstract>
      <bibkey>chu-etal-2017-broad</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lambada">LAMBADA</pwcdataset>
    </paper>
    <paper id="10">
      <title>Detecting negation scope is easy, except when it isn’t</title>
      <author><first>Federico</first><last>Fancellu</last></author>
      <author><first>Adam</first><last>Lopez</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <author><first>Hangfeng</first><last>He</last></author>
      <pages>58–63</pages>
      <url hash="54797b3f">E17-2010</url>
      <abstract>Several corpora have been annotated with negation scope—the set of words whose meaning is negated by a cue like the word “not”—leading to the development of classifiers that detect negation scope with high accuracy. We show that for nearly all of these corpora, this high accuracy can be attributed to a single fact: they frequently annotate negation scope as a single span of text delimited by punctuation. For negation scopes not of this form, detection accuracy is low and under-sampling the easy training examples does not substantially improve accuracy. We demonstrate that this is partly an artifact of annotation guidelines, and we argue that future negation scope annotation efforts should focus on these more difficult cases.</abstract>
      <bibkey>fancellu-etal-2017-detecting</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>MT</fixed-case>/<fixed-case>IE</fixed-case>: Cross-lingual Open Information Extraction with Neural Sequence-to-Sequence Models</title>
      <author><first>Sheng</first><last>Zhang</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>64–70</pages>
      <url hash="6aeef027">E17-2011</url>
      <abstract>Cross-lingual information extraction is the task of distilling facts from foreign language (e.g. Chinese text) into representations in another language that is preferred by the user (e.g. English tuples). Conventional pipeline solutions decompose the task as machine translation followed by information extraction (or vice versa). We propose a joint solution with a neural sequence model, and show that it outperforms the pipeline in a cross-lingual open information extraction setting by 1-4 BLEU and 0.5-0.8 F1.</abstract>
      <bibkey>zhang-etal-2017-mt</bibkey>
    </paper>
    <paper id="12">
      <title>Learning to Negate Adjectives with Bilinear Models</title>
      <author><first>Laura</first><last>Rimell</last></author>
      <author><first>Amandla</first><last>Mabona</last></author>
      <author><first>Luana</first><last>Bulat</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <pages>71–78</pages>
      <url hash="d0453511">E17-2012</url>
      <abstract>We learn a mapping that negates adjectives by predicting an adjective’s antonym in an arbitrary word embedding model. We show that both linear models and neural networks improve on this task when they have access to a vector representing the semantic domain of the input word, e.g. a centroid of temperature words when predicting the antonym of ‘cold’. We introduce a continuous class-conditional bilinear neural network which is able to negate adjectives with high precision.</abstract>
      <bibkey>rimell-etal-2017-learning</bibkey>
    </paper>
    <paper id="13">
      <title>Instances and concepts in distributional space</title>
      <author><first>Gemma</first><last>Boleda</last></author>
      <author><first>Abhijeet</first><last>Gupta</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>79–85</pages>
      <url hash="622aa9da">E17-2013</url>
      <abstract>Instances (“Mozart”) are ontologically distinct from concepts or classes (“composer”). Natural language encompasses both, but instances have received comparatively little attention in distributional semantics. Our results show that instances and concepts differ in their distributional properties. We also establish that instantiation detection (“Mozart – composer”) is generally easier than hypernymy detection (“chemist – scientist”), and that results on the influence of input representation do not transfer from hyponymy to instantiation.</abstract>
      <bibkey>boleda-etal-2017-instances</bibkey>
    </paper>
    <paper id="14">
      <title>Is this a Child, a Girl or a Car? Exploring the Contribution of Distributional Similarity to Learning Referential Word Meanings</title>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>86–91</pages>
      <url hash="ce10e0e2">E17-2014</url>
      <abstract>There has recently been a lot of work trying to use images of referents of words for improving vector space meaning representations derived from text. We investigate the opposite direction, as it were, trying to improve visual word predictors that identify objects in images, by exploiting distributional similarity information during training. We show that for certain words (such as entry-level nouns or hypernyms), we can indeed learn better referential word meanings by taking into account their semantic similarity to other words. For other words, there is no or even a detrimental effect, compared to a learning setup that presents even semantically related objects as negative instances.</abstract>
      <bibkey>zarriess-schlangen-2017-child</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/refcoco">RefCOCO</pwcdataset>
    </paper>
    <paper id="15">
      <title>The Semantic Proto-Role Linking Model</title>
      <author><first>Aaron Steven</first><last>White</last></author>
      <author><first>Kyle</first><last>Rawlins</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>92–98</pages>
      <url hash="e4df1638">E17-2015</url>
      <abstract>We propose the semantic proto-role linking model, which jointly induces both predicate-specific semantic roles and predicate-general semantic proto-roles based on semantic proto-role property likelihood judgments. We use this model to empirically evaluate Dowty’s thematic proto-role linking theory.</abstract>
      <bibkey>white-etal-2017-semantic</bibkey>
    </paper>
    <paper id="16">
      <title>The Language of Place: Semantic Value from Geospatial Context</title>
      <author><first>Anne</first><last>Cocos</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>99–104</pages>
      <url hash="fed22c6d">E17-2016</url>
      <abstract>There is a relationship between what we say and where we say it. Word embeddings are usually trained assuming that semantically-similar words occur within the same textual contexts. We investigate the extent to which semantically-similar words occur within the same geospatial contexts. We enrich a corpus of geolocated Twitter posts with physical data derived from Google Places and OpenStreetMap, and train word embeddings using the resulting geospatial contexts. Intrinsic evaluation of the resulting vectors shows that geographic context alone does provide useful information about semantic relatedness.</abstract>
      <bibkey>cocos-callison-burch-2017-language</bibkey>
    </paper>
    <paper id="17">
      <title>Are Emojis Predictable?</title>
      <author><first>Francesco</first><last>Barbieri</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>105–111</pages>
      <url hash="563f1047">E17-2017</url>
      <abstract>Emojis are ideograms which are naturally combined with plain text to visually complement or condense the meaning of a message. Despite being widely used in social media, their underlying semantics have received little attention from a Natural Language Processing standpoint. In this paper, we investigate the relation between words and emojis, studying the novel task of predicting which emojis are evoked by text-based tweet messages. We train several models based on Long Short-Term Memory networks (LSTMs) in this task. Our experimental results show that our neural model outperforms a baseline as well as humans solving the same task, suggesting that computational models are able to better capture the underlying semantics of emojis.</abstract>
      <bibkey>barbieri-etal-2017-emojis</bibkey>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multimodal-emoji-prediction">Multimodal Emoji Prediction</pwcdataset>
    </paper>
    <paper id="18">
      <title>A Rich Morphological Tagger for <fixed-case>E</fixed-case>nglish: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax</title>
      <author><first>Christo</first><last>Kirov</last></author>
      <author><first>John</first><last>Sylak-Glassman</last></author>
      <author><first>Rebecca</first><last>Knowles</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <pages>112–117</pages>
      <url hash="f9bee6ea">E17-2018</url>
      <abstract>A traditional claim in linguistics is that all human languages are equally expressive—able to convey the same wide range of meanings. Morphologically rich languages, such as Czech, rely on overt inflectional and derivational morphology to convey many semantic distinctions. Languages with comparatively limited morphology, such as English, should be able to accomplish the same using a combination of syntactic and contextual cues. We capitalize on this idea by training a tagger for English that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from Czech. The high accuracy of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including machine translation.</abstract>
      <bibkey>kirov-etal-2017-rich</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="19">
      <title>Context-Aware Prediction of Derivational Word-forms</title>
      <author><first>Ekaterina</first><last>Vylomova</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <pages>118–124</pages>
      <url hash="e41bf20d">E17-2019</url>
      <abstract>Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose a new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder-decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under lexicon agnostic setting.</abstract>
      <bibkey>vylomova-etal-2017-context</bibkey>
      <pwccode url="https://github.com/ivri/dmorph" additional="false">ivri/dmorph</pwccode>
    </paper>
    <paper id="20">
      <title>Comparing Character-level Neural Language Models Using a Lexical Decision Task</title>
      <author><first>Gaël</first><last>Le Godais</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <author><first>Emmanuel</first><last>Dupoux</last></author>
      <pages>125–130</pages>
      <url hash="aac2d7b5">E17-2020</url>
      <abstract>What is the information captured by neural network models of language? We address this question in the case of character-level recurrent neural language models. These models do not have explicit word representations; do they acquire implicit ones? We assess the lexical capacity of a network using the lexical decision task common in psycholinguistics: the system is required to decide whether or not a string of characters forms a word. We explore how accuracy on this task is affected by the architecture of the network, focusing on cell type (LSTM vs. SRN), depth and width. We also compare these architectural properties to a simple count of the parameters of the network. The overall number of parameters in the network turns out to be the most important predictor of accuracy; in particular, there is little evidence that deeper networks are beneficial for this task.</abstract>
      <bibkey>le-godais-etal-2017-comparing</bibkey>
    </paper>
    <paper id="21">
      <title>Optimal encoding! - Information Theory constrains article omission in newspaper headlines</title>
      <author><first>Robin</first><last>Lemke</last></author>
      <author><first>Eva</first><last>Horch</last></author>
      <author><first>Ingo</first><last>Reich</last></author>
      <pages>131–135</pages>
      <url hash="fb6aefe2">E17-2021</url>
      <abstract>In this paper we pursue the hypothesis that the distribution of article omission specifically is constrained by principles of Information Theory (Shannon 1948). In particular, Information Theory predicts a stronger preference for article omission before nouns which are relatively unpredictable in context of the preceding words. We investigated article omission in German newspaper headlines with a corpus and acceptability rating study. Both support our hypothesis: Articles are inserted more often before unpredictable nouns and subjects perceive article omission before predictable nouns as more well-formed than before unpredictable ones. This suggests that information theoretic principles constrain the distribution of article omission in headlines.</abstract>
      <bibkey>lemke-etal-2017-optimal</bibkey>
    </paper>
    <paper id="22">
      <title>A Computational Analysis of the Language of Drug Addiction</title>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>136–142</pages>
      <url hash="0310e0a5">E17-2022</url>
      <abstract>We present a computational analysis of the language of drug users when talking about their drug experiences. We introduce a new dataset of over 4,000 descriptions of experiences reported by users of four main drug types, and show that we can predict with an F1-score of up to 88% the drug behind a certain experience. We also perform an analysis of the dominant psycholinguistic processes and dominant emotions associated with each drug type, which sheds light on the characteristics of drug users.</abstract>
      <bibkey>strapparava-mihalcea-2017-computational</bibkey>
    </paper>
    <paper id="23">
      <title>A Practical Perspective on Latent Structured Prediction for Coreference Resolution</title>
      <author><first>Iryna</first><last>Haponchyk</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>143–149</pages>
      <url hash="9817fe86">E17-2023</url>
      <abstract>Latent structured prediction theory proposes powerful methods such as Latent Structural SVM (LSSVM), which can potentially be very appealing for coreference resolution (CR). In contrast, only small work is available, mainly targeting the latent structured perceptron (LSP). In this paper, we carried out a practical study comparing for the first time online learning with LSSVM. We analyze the intricacies that may have made initial attempts to use LSSVM fail, i.e., a huge training time and much lower accuracy produced by Kruskal’s spanning tree algorithm. In this respect, we also propose a new effective feature selection approach for improving system efficiency. The results show that LSP, if correctly parameterized, produces the same performance as LSSVM, being much more efficient.</abstract>
      <bibkey>haponchyk-moschitti-2017-practical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
    </paper>
    <paper id="24">
      <title>On the Need of Cross Validation for Discourse Relation Classification</title>
      <author><first>Wei</first><last>Shi</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>150–156</pages>
      <url hash="e09cb49f">E17-2024</url>
      <abstract>The task of implicit discourse relation classification has received increased attention in recent years, including two CoNNL shared tasks on the topic. Existing machine learning models for the task train on sections 2-21 of the PDTB and test on section 23, which includes a total of 761 implicit discourse relations. In this paper, we’d like to make a methodological point, arguing that the standard test set is too small to draw conclusions about whether the inclusion of certain features constitute a genuine improvement, or whether one got lucky with some properties of the test set, and argue for the adoption of cross validation for the discourse relation classification task by the community.</abstract>
      <bibkey>shi-demberg-2017-need</bibkey>
    </paper>
    <paper id="25">
      <title>Using the Output Embedding to Improve Language Models</title>
      <author><first>Ofir</first><last>Press</last></author>
      <author><first>Lior</first><last>Wolf</last></author>
      <pages>157–163</pages>
      <url hash="091485ef">E17-2025</url>
      <abstract>We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.</abstract>
      <bibkey>press-wolf-2017-using</bibkey>
      <pwccode url="https://github.com/ofirpress/UsingTheOutputEmbedding" additional="true">ofirpress/UsingTheOutputEmbedding</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="26">
      <title>Identifying beneficial task relations for multi-task learning in deep neural networks</title>
      <author><first>Joachim</first><last>Bingel</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>164–169</pages>
      <url hash="0da6d2d4">E17-2026</url>
      <abstract>Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.</abstract>
      <bibkey>bingel-sogaard-2017-identifying</bibkey>
      <pwccode url="https://github.com/jbingel/eacl2017_mtl" additional="false">jbingel/eacl2017_mtl</pwccode>
    </paper>
    <paper id="27">
      <title>Effective search space reduction for spell correction using character neural embeddings</title>
      <author><first>Harshit</first><last>Pande</last></author>
      <pages>170–174</pages>
      <url hash="6f6d4f9c">E17-2027</url>
      <abstract>We present a novel, unsupervised, and distance measure agnostic method for search space reduction in spell correction using neural character embeddings. The embeddings are learned by skip-gram word2vec training on sequences generated from dictionary words in a phonetic information-retentive manner. We report a very high performance in terms of both success rates and reduction of search space on the Birkbeck spelling error corpus. To the best of our knowledge, this is the first application of word2vec to spell correction.</abstract>
      <bibkey>pande-2017-effective</bibkey>
    </paper>
    <paper id="28">
      <title>Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis</title>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Adam</first><last>Poliak</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <author><first>Jason</first><last>Eisner</last></author>
      <pages>175–181</pages>
      <url hash="f8e2bbcd">E17-2028</url>
      <abstract>The popular skip-gram model induces word embeddings by exploiting the signal from word-context coocurrence. We offer a new interpretation of skip-gram based on exponential family PCA-a form of matrix factorization to generalize the skip-gram model to tensor factorization. In turn, this lets us train embeddings through richer higher-order coocurrences, e.g., triples that include positional information (to incorporate syntax) or morphological information (to share parameters across related words). We experiment on 40 languages and show our model improves upon skip-gram.</abstract>
      <bibkey>cotterell-etal-2017-explaining</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="29">
      <title>Latent Variable Dialogue Models and their Diversity</title>
      <author><first>Kris</first><last>Cao</last></author>
      <author><first>Stephen</first><last>Clark</last></author>
      <pages>182–187</pages>
      <url hash="be1ffa47">E17-2029</url>
      <abstract>We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the ‘boring output’ issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.</abstract>
      <bibkey>cao-clark-2017-latent</bibkey>
    </paper>
    <paper id="30">
      <title>Age Group Classification with Speech and Metadata Multimodality Fusion</title>
      <author><first>Denys</first><last>Katerenchuk</last></author>
      <pages>188–193</pages>
      <url hash="4913ddc6">E17-2030</url>
      <abstract>Children comprise a significant proportion of TV viewers and it is worthwhile to customize the experience for them. However, identifying who is a child in the audience can be a challenging task. We present initial studies of a novel method which combines utterances with user metadata. In particular, we develop an ensemble of different machine learning techniques on different subsets of data to improve child detection. Our initial results show an 9.2% absolute improvement over the baseline, leading to a state-of-the-art performance.</abstract>
      <bibkey>katerenchuk-2017-age</bibkey>
    </paper>
    <paper id="31">
      <title>Automatically augmenting an emotion dataset improves classification using audio</title>
      <author><first>Egor</first><last>Lakomkin</last></author>
      <author><first>Cornelius</first><last>Weber</last></author>
      <author><first>Stefan</first><last>Wermter</last></author>
      <pages>194–197</pages>
      <url hash="e2450fd4">E17-2031</url>
      <abstract>In this work, we tackle a problem of speech emotion classification. One of the issues in the area of affective computation is that the amount of annotated data is very limited. On the other hand, the number of ways that the same emotion can be expressed verbally is enormous due to variability between speakers. This is one of the factors that limits performance and generalization. We propose a simple method that extracts audio samples from movies using textual sentiment analysis. As a result, it is possible to automatically construct a larger dataset of audio samples with positive, negative emotional and neutral speech. We show that pretraining recurrent neural network on such a dataset yields better results on the challenging EmotiW corpus. This experiment shows a potential benefit of combining textual sentiment analysis with vocal information.</abstract>
      <bibkey>lakomkin-etal-2017-automatically</bibkey>
    </paper>
    <paper id="32">
      <title>On-line Dialogue Policy Learning with Companion Teaching</title>
      <author><first>Lu</first><last>Chen</last></author>
      <author><first>Runzhe</first><last>Yang</last></author>
      <author><first>Cheng</first><last>Chang</last></author>
      <author><first>Zihao</first><last>Ye</last></author>
      <author><first>Xiang</first><last>Zhou</last></author>
      <author><first>Kai</first><last>Yu</last></author>
      <pages>198–204</pages>
      <url hash="ae974800">E17-2032</url>
      <abstract>On-line dialogue policy learning is the key for building evolvable conversational agent in real world scenarios. Poor initial policy can easily lead to bad user experience and consequently fail to attract sufficient users for policy training. A novel framework, companion teaching, is proposed to include a human teacher in the dialogue policy training loop to address the cold start problem. Here, dialogue policy is trained using not only user’s reward, but also teacher’s example action as well as estimated immediate reward at turn level. Simulation experiments showed that, with small number of human teaching dialogues, the proposed approach can effectively improve user experience at the beginning and smoothly lead to good performance with more user interaction data.</abstract>
      <bibkey>chen-etal-2017-line</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dialogue-state-tracking-challenge">Dialogue State Tracking Challenge</pwcdataset>
    </paper>
    <paper id="33">
      <title>Hybrid Dialog State Tracker with <fixed-case>ASR</fixed-case> Features</title>
      <author><first>Miroslav</first><last>Vodolán</last></author>
      <author><first>Rudolf</first><last>Kadlec</last></author>
      <author><first>Jan</first><last>Kleindienst</last></author>
      <pages>205–210</pages>
      <url hash="209a394a">E17-2033</url>
      <abstract>This paper presents a hybrid dialog state tracker enhanced by trainable Spoken Language Understanding (SLU) for slot-filling dialog systems. Our architecture is inspired by previously proposed neural-network-based belief-tracking systems. In addition, we extended some parts of our modular architecture with differentiable rules to allow end-to-end training. We hypothesize that these rules allow our tracker to generalize better than pure machine-learning based systems. For evaluation, we used the Dialog State Tracking Challenge (DSTC) 2 dataset - a popular belief tracking testbed with dialogs from restaurant information system. To our knowledge, our hybrid tracker sets a new state-of-the-art result in three out of four categories within the DSTC2.</abstract>
      <bibkey>vodolan-etal-2017-hybrid</bibkey>
    </paper>
    <paper id="34">
      <title>Morphological Analysis without Expert Annotation</title>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <pages>211–216</pages>
      <url hash="36ce50e5">E17-2034</url>
      <abstract>The task of morphological analysis is to produce a complete list of lemma+tag analyses for a given word-form. We propose a discriminative string transduction approach which exploits plain inflection tables and raw text corpora, thus obviating the need for expert annotation. Experiments on four languages demonstrate that our system has much higher coverage than a hand-engineered FST analyzer, and is more accurate than a state-of-the-art morphological tagger.</abstract>
      <bibkey>nicolai-kondrak-2017-morphological</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/celex">CELEX</pwcdataset>
    </paper>
    <paper id="35">
      <title>Morphological Analysis of the <fixed-case>D</fixed-case>ravidian Language Family</title>
      <author><first>Arun</first><last>Kumar</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Lluís</first><last>Padró</last></author>
      <author><first>Antoni</first><last>Oliver</last></author>
      <pages>217–222</pages>
      <url hash="390f4b39">E17-2035</url>
      <abstract>The Dravidian languages are one of the most widely spoken language families in the world, yet there are very few annotated resources available to NLP researchers. To remedy this, we create DravMorph, a corpus annotated for morphological segmentation and part-of-speech. Additionally, we exploit novel features and higher-order models to set state-of-the-art results on these corpora on both tasks, beating techniques proposed in the literature by as much as 4 points in segmentation F1.</abstract>
      <bibkey>kumar-etal-2017-morphological</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>B</fixed-case>abel<fixed-case>D</fixed-case>omains: Large-Scale Domain Labeling of Lexical Resources</title>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>223–228</pages>
      <url hash="73376b61">E17-2036</url>
      <abstract>In this paper we present BabelDomains, a unified resource which provides lexical items with information about domains of knowledge. We propose an automatic method that uses knowledge from various lexical resources, exploiting both distributional and graph-based clues, to accurately propagate domain information. We evaluate our methodology intrinsically on two lexical resources (WordNet and BabelNet), achieving a precision over 80% in both cases. Finally, we show the potential of BabelDomains in a supervised learning setting, clustering training data by domain for hypernym discovery.</abstract>
      <bibkey>camacho-collados-navigli-2017-babeldomains</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>JFLEG</fixed-case>: A Fluency Corpus and Benchmark for Grammatical Error Correction</title>
      <author><first>Courtney</first><last>Napoles</last></author>
      <author><first>Keisuke</first><last>Sakaguchi</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <pages>229–234</pages>
      <url hash="7a69e260">E17-2037</url>
      <abstract>We present a new parallel corpus, JHU FLuency-Extended GUG corpus (JFLEG) for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. We describe the types of corrections made and benchmark four leading GEC systems on this corpus, identifying specific areas in which they do well and how they can improve. JFLEG fulfills the need for a new gold standard to properly assess the current state of GEC.</abstract>
      <bibkey>napoles-etal-2017-jfleg</bibkey>
      <pwccode url="https://github.com/keisks/jfleg" additional="false">keisks/jfleg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="38">
      <title>A Parallel Corpus for Evaluating Machine Translation between <fixed-case>A</fixed-case>rabic and <fixed-case>E</fixed-case>uropean Languages</title>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Nasser</first><last>Zalmout</last></author>
      <author><first>Dima</first><last>Taji</last></author>
      <author><first>Hieu</first><last>Hoang</last></author>
      <author><first>Maverick</first><last>Alzate</last></author>
      <pages>235–241</pages>
      <url hash="291442a7">E17-2038</url>
      <abstract>We present Arab-Acquis, a large publicly available dataset for evaluating machine translation between 22 European languages and Arabic. Arab-Acquis consists of over 12,000 sentences from the JRC-Acquis (Acquis Communautaire) corpus translated twice by professional translators, once from English and once from French, and totaling over 600,000 words. The corpus follows previous data splits in the literature for tuning, development, and testing. We describe the corpus and how it was created. We also present the first benchmarking results on translating to and from Arabic for 22 European languages.</abstract>
      <bibkey>habash-etal-2017-parallel</bibkey>
    </paper>
    <paper id="39">
      <title>The <fixed-case>P</fixed-case>arallel <fixed-case>M</fixed-case>eaning <fixed-case>B</fixed-case>ank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations</title>
      <author><first>Lasha</first><last>Abzianidze</last></author>
      <author><first>Johannes</first><last>Bjerva</last></author>
      <author><first>Kilian</first><last>Evang</last></author>
      <author><first>Hessel</first><last>Haagsma</last></author>
      <author><first>Rik</first><last>van Noord</last></author>
      <author><first>Pierre</first><last>Ludmann</last></author>
      <author><first>Duc-Duy</first><last>Nguyen</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <pages>242–247</pages>
      <url hash="7d68a950">E17-2039</url>
      <abstract>The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four languages (English, German, Italian, and Dutch). Our approach is based on cross-lingual projection: automatically produced (and manually corrected) semantic annotations for English sentences are mapped onto their word-aligned translations, assuming that the translations are meaning-preserving. The semantic annotation consists of five main steps: (i) segmentation of the text in sentences and lexical items; (ii) syntactic parsing with Combinatory Categorial Grammar; (iii) universal semantic tagging; (iv) symbolization; and (v) compositional semantic analysis based on Discourse Representation Theory. These steps are performed using statistical models trained in a semi-supervised manner. The employed annotation models are all language-neutral. Our first results are promising.</abstract>
      <bibkey>abzianidze-etal-2017-parallel</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/groningen-meaning-bank">Groningen Meaning Bank</pwcdataset>
    </paper>
    <paper id="40">
      <title>Cross-lingual tagger evaluation without test data</title>
      <author><first>Željko</first><last>Agić</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>248–253</pages>
      <url hash="ffeb845f">E17-2040</url>
      <abstract>We address the challenge of cross-lingual POS tagger evaluation in absence of manually annotated test data. We put forth and evaluate two dictionary-based metrics. On the tasks of accuracy prediction and system ranking, we reveal that these metrics are reliable enough to approximate test set-based evaluation, and at the same time lean enough to support assessment for truly low-resource languages.</abstract>
      <bibkey>agic-etal-2017-cross</bibkey>
    </paper>
    <paper id="41">
      <title>Legal <fixed-case>NERC</fixed-case> with ontologies, <fixed-case>W</fixed-case>ikipedia and curriculum learning</title>
      <author><first>Cristian</first><last>Cardellino</last></author>
      <author><first>Milagro</first><last>Teruel</last></author>
      <author><first>Laura</first><last>Alonso Alemany</last></author>
      <author><first>Serena</first><last>Villata</last></author>
      <pages>254–259</pages>
      <url hash="de74019e">E17-2041</url>
      <abstract>In this paper, we present a Wikipedia-based approach to develop resources for the legal domain. We establish a mapping between a legal domain ontology, LKIF (Hoekstra et al. 2007), and a Wikipedia-based ontology, YAGO (Suchanek et al. 2007), and through that we populate LKIF. Moreover, we use the mentions of those entities in Wikipedia text to train a specific Named Entity Recognizer and Classifier. We find that this classifier works well in the Wikipedia, but, as could be expected, performance decreases in a corpus of judgments of the European Court of Human Rights. However, this tool will be used as a preprocess for human annotation. We resort to a technique called “curriculum learning” aimed to overcome problems of overfitting by learning increasingly more complex concepts. However, we find that in this particular setting, the method works best by learning from most specific to most general concepts, not the other way round.</abstract>
      <bibkey>cardellino-etal-2017-legal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/yago">YAGO</pwcdataset>
    </paper>
    <paper id="42">
      <title>The Content Types Dataset: a New Resource to Explore Semantic and Functional Characteristics of Texts</title>
      <author><first>Rachele</first><last>Sprugnoli</last></author>
      <author><first>Tommaso</first><last>Caselli</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <author><first>Giovanni</first><last>Moretti</last></author>
      <pages>260–266</pages>
      <url hash="a4bf8583">E17-2042</url>
      <abstract>This paper presents a new resource, called Content Types Dataset, to promote the analysis of texts as a composition of units with specific semantic and functional roles. By developing this dataset, we also introduce a new NLP task for the automatic classification of Content Types. The annotation scheme and the dataset are described together with two sets of classification experiments.</abstract>
      <bibkey>sprugnoli-etal-2017-content</bibkey>
    </paper>
    <paper id="43">
      <title>Continuous N-gram Representations for Authorship Attribution</title>
      <author><first>Yunita</first><last>Sari</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <author><first>Mark</first><last>Stevenson</last></author>
      <pages>267–273</pages>
      <url hash="f89a455a">E17-2043</url>
      <abstract>This paper presents work on using continuous representations for authorship attribution. In contrast to previous work, which uses discrete feature representations, our model learns continuous representations for n-gram features via a neural network jointly with the classification layer. Experimental results demonstrate that the proposed model outperforms the state-of-the-art on two datasets, while producing comparable results on the remaining two.</abstract>
      <bibkey>sari-etal-2017-continuous</bibkey>
    </paper>
    <paper id="44">
      <title>Reconstructing the house from the ad: Structured prediction on real estate classifieds</title>
      <author><first>Giannis</first><last>Bekoulis</last></author>
      <author><first>Johannes</first><last>Deleu</last></author>
      <author><first>Thomas</first><last>Demeester</last></author>
      <author><first>Chris</first><last>Develder</last></author>
      <pages>274–279</pages>
      <url hash="e19d0b37">E17-2044</url>
      <abstract>In this paper, we address the (to the best of our knowledge) new problem of extracting a structured description of real estate properties from their natural language descriptions in classifieds. We survey and present several models to (a) identify important entities of a property (e.g.,rooms) from classifieds and (b) structure them into a tree format, with the entities as nodes and edges representing a part-of relation. Experiments show that a graph-based system deriving the tree from an initially fully connected entity graph, outperforms a transition-based system starting from only the entity nodes, since it better reconstructs the tree.</abstract>
      <bibkey>bekoulis-etal-2017-reconstructing</bibkey>
      <pwccode url="https://github.com/bekou/ad_data" additional="false">bekou/ad_data</pwccode>
    </paper>
    <paper id="45">
      <title>Neural vs. Phrase-Based Machine Translation in a Multi-Domain Scenario</title>
      <author><first>M. Amin</first><last>Farajian</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Nicola</first><last>Bertoldi</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <pages>280–284</pages>
      <url hash="dc3d22cd">E17-2045</url>
      <abstract>State-of-the-art neural machine translation (NMT) systems are generally trained on specific domains by carefully selecting the training sets and applying proper domain adaptation techniques. In this paper we consider the real world scenario in which the target domain is not predefined, hence the system should be able to translate text from multiple domains. We compare the performance of a generic NMT system and phrase-based statistical machine translation (PBMT) system by training them on a generic parallel corpus composed of data from different domains. Our results on multi-domain English-French data show that, in these realistic conditions, PBMT outperforms its neural counterpart. This raises the question: is NMT ready for deployment as a generic/multi-purpose MT backbone in real-world settings?</abstract>
      <bibkey>farajian-etal-2017-neural</bibkey>
    </paper>
    <paper id="46">
      <title>Improving <fixed-case>ROUGE</fixed-case> for Timeline Summarization</title>
      <author><first>Sebastian</first><last>Martschat</last></author>
      <author><first>Katja</first><last>Markert</last></author>
      <pages>285–290</pages>
      <url hash="ccb0906d">E17-2046</url>
      <abstract>Current evaluation metrics for timeline summarization either ignore the temporal aspect of the task or require strict date matching. We introduce variants of ROUGE that allow alignment of daily summaries via temporal distance or semantic similarity. We argue for the suitability of these variants in a theoretical analysis and demonstrate it in a battery of task-specific tests.</abstract>
      <bibkey>martschat-markert-2017-improving</bibkey>
    </paper>
    <paper id="47">
      <title>Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization</title>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>291–297</pages>
      <url hash="b68f5013">E17-2047</url>
      <abstract>This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark.</abstract>
      <bibkey>suzuki-nagata-2017-cutting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/duc-2004">DUC 2004</pwcdataset>
    </paper>
    <paper id="48">
      <title>To Sing like a Mockingbird</title>
      <author><first>Lorenzo</first><last>Gatti</last></author>
      <author><first>Gözde</first><last>Özbal</last></author>
      <author><first>Oliviero</first><last>Stock</last></author>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <pages>298–304</pages>
      <url hash="8b05bd6e">E17-2048</url>
      <abstract>Musical parody, i.e. the act of changing the lyrics of an existing and very well-known song, is a commonly used technique for creating catchy advertising tunes and for mocking people or events. Here we describe a system for automatically producing a musical parody, starting from a corpus of songs. The system can automatically identify characterizing words and concepts related to a novel text, which are taken from the daily news. These concepts are then used as seeds to appropriately replace part of the original lyrics of a song, using metrical, rhyming and lexical constraints. Finally, the parody can be sung with a singing speech synthesizer, with no intervention from the user.</abstract>
      <bibkey>gatti-etal-2017-sing</bibkey>
    </paper>
    <paper id="49">
      <title>K-best Iterative <fixed-case>V</fixed-case>iterbi Parsing</title>
      <author><first>Katsuhiko</first><last>Hayashi</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>305–310</pages>
      <url hash="74340737">E17-2049</url>
      <abstract>This paper presents an efficient and optimal parsing algorithm for probabilistic context-free grammars (PCFGs). To achieve faster parsing, our proposal employs a pruning technique to reduce unnecessary edges in the search space. The key is to conduct repetitively Viterbi inside and outside parsing, while gradually expanding the search space to efficiently compute heuristic bounds used for pruning. Our experimental results using the English Penn Treebank corpus show that the proposed algorithm is faster than the standard CKY parsing algorithm. In addition, we also show how to extend this algorithm to extract k-best Viterbi parse trees.</abstract>
      <bibkey>hayashi-nagata-2017-k</bibkey>
    </paper>
    <paper id="50">
      <title><fixed-case>PP</fixed-case> Attachment: Where do We Stand?</title>
      <author><first>Daniël</first><last>de Kok</last></author>
      <author><first>Jianqiang</first><last>Ma</last></author>
      <author><first>Corina</first><last>Dima</last></author>
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <pages>311–317</pages>
      <url hash="3e96f603">E17-2050</url>
      <abstract>Prepostitional phrase (PP) attachment is a well known challenge to parsing. In this paper, we combine the insights of different works, namely: (1) treating PP attachment as a classification task with an arbitrary number of attachment candidates; (2) using auxiliary distributions to augment the data beyond the hand-annotated training set; (3) using topological fields to get information about the distribution of PP attachment throughout clauses and (4) using state-of-the-art techniques such as word embeddings and neural networks. We show that jointly using these techniques leads to substantial improvements. We also conduct a qualitative analysis to gauge where the ceiling of the task is in a realistic setup.</abstract>
      <bibkey>de-kok-etal-2017-pp</bibkey>
    </paper>
    <paper id="51">
      <title>Don’t Stop Me Now! Using Global Dynamic Oracles to Correct Training Biases of Transition-Based Dependency Parsers</title>
      <author><first>Lauriane</first><last>Aufrant</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>318–323</pages>
      <url hash="10363d62">E17-2051</url>
      <abstract>This paper formalizes a sound extension of dynamic oracles to global training, in the frame of transition-based dependency parsers. By dispensing with the pre-computation of references, this extension widens the training strategies that can be entertained for such parsers; we show this by revisiting two standard training procedures, early-update and max-violation, to correct some of their search space sampling biases. Experimentally, on the SPMRL treebanks, this improvement increases the similarity between the train and test distributions and yields performance improvements up to 0.7 UAS, without any computation overhead.</abstract>
      <bibkey>aufrant-etal-2017-dont</bibkey>
    </paper>
    <paper id="52">
      <title>Joining Hands: Exploiting Monolingual Treebanks for Parsing of Code-mixing Data</title>
      <author><first>Irshad</first><last>Bhat</last></author>
      <author><first>Riyaz A.</first><last>Bhat</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>324–330</pages>
      <url hash="e2770613">E17-2052</url>
      <abstract>In this paper, we propose efficient and less resource-intensive strategies for parsing of code-mixed data. These strategies are not constrained by in-domain annotations, rather they leverage pre-existing monolingual annotated resources for training. We show that these methods can produce significantly better results as compared to an informed baseline. Due to lack of an evaluation set for code-mixed structures, we also present a data set of 450 Hindi and English code-mixed tweets of Hindi multilingual speakers for evaluation.</abstract>
      <bibkey>bhat-etal-2017-joining</bibkey>
    </paper>
    <paper id="53">
      <title>Multilingual Lexicalized Constituency Parsing with Word-Level Auxiliary Tasks</title>
      <author><first>Maximin</first><last>Coavoux</last></author>
      <author><first>Benoît</first><last>Crabbé</last></author>
      <pages>331–336</pages>
      <url hash="a436d2b1">E17-2053</url>
      <abstract>We introduce a constituency parser based on a bi-LSTM encoder adapted from recent work (Cross and Huang, 2016b; Kiperwasser and Goldberg, 2016), which can incorporate a lower level character biLSTM (Ballesteros et al., 2015; Plank et al., 2016). We model two important interfaces of constituency parsing with auxiliary tasks supervised at the word level: (i) part-of-speech (POS) and morphological tagging, (ii) functional label prediction. On the SPMRL dataset, our parser obtains above state-of-the-art results on constituency parsing without requiring either predicted POS or morphological tags, and outputs labelled dependency trees.</abstract>
      <bibkey>coavoux-crabbe-2017-multilingual</bibkey>
      <pwccode url="https://github.com/mcoavoux/mtg" additional="false">mcoavoux/mtg</pwccode>
    </paper>
    <paper id="54">
      <title>Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision</title>
      <author><first>Sandro</first><last>Pezzelle</last></author>
      <author><first>Marco</first><last>Marelli</last></author>
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <pages>337–342</pages>
      <url hash="8c596dd9">E17-2054</url>
      <abstract>People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few, most, all). In humans, these two processes underlie fairly different cognitive and neural mechanisms. Inspired by this evidence, the present study proposes two models for learning the objective meaning of cardinals and quantifiers from visual scenes containing multiple objects. We show that a model capitalizing on a ‘fuzzy’ measure of similarity is effective for learning quantifiers, whereas the learning of exact cardinals is better accomplished when information about number is provided.</abstract>
      <bibkey>pezzelle-etal-2017-precise</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="55">
      <title>Improving a Strong Neural Parser with Conjunction-Specific Features</title>
      <author><first>Jessica</first><last>Ficler</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>343–348</pages>
      <url hash="eb9fdc07">E17-2055</url>
      <abstract>While dependency parsers reach very high overall accuracy, some dependency relations are much harder than others. In particular, dependency parsers perform poorly in coordination construction (i.e., correctly attaching the conj relation). We extend a state-of-the-art dependency parser with conjunction-specific features, focusing on the similarity between the conjuncts head words. Training the extended parser yields an improvement in conj attachment as well as in overall dependency parsing accuracy on the Stanford dependency conversion of the Penn TreeBank.</abstract>
      <bibkey>ficler-goldberg-2017-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="56">
      <title>Neural Automatic Post-Editing Using Prior Alignment and Reranking</title>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Sudip Kumar</first><last>Naskar</last></author>
      <author><first>Mihaela</first><last>Vela</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>349–355</pages>
      <url hash="7847b5b8">E17-2056</url>
      <abstract>We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a first-stage MT system. Our APE system (APE_Sym) is an extended version of an attention based NMT model with bilingual symmetry employing bidirectional models, mt–pe and pe–mt. APE translations produced by our system show statistically significant improvements over the first-stage MT, phrase-based APE and the best reported score on the WMT 2016 APE dataset by a previous neural APE system. Re-ranking (APE_Rerank) of the n-best translations from the phrase-based APE and APE_Sym systems provides further substantial improvements over the symmetric neural APE model. Human evaluation confirms that the APE_Rerank generated PE translations improve on the previous best neural APE system at WMT 2016.</abstract>
      <bibkey>pal-etal-2017-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="57">
      <title>Improving Evaluation of Document-level Machine Translation Quality Estimation</title>
      <author><first>Yvette</first><last>Graham</last></author>
      <author><first>Qingsong</first><last>Ma</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Carla</first><last>Parra</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <pages>356–361</pages>
      <url hash="d91571f4">E17-2057</url>
      <abstract>Meaningful conclusions about the relative performance of NLP systems are only possible if the gold standard employed in a given evaluation is both valid and reliable. In this paper, we explore the validity of human annotations currently employed in the evaluation of document-level quality estimation for machine translation (MT). We demonstrate the degree to which MT system rankings are dependent on weights employed in the construction of the gold standard, before proposing direct human assessment as a valid alternative. Experiments show direct assessment (DA) scores for documents to be highly reliable, achieving a correlation of above 0.9 in a self-replication experiment, in addition to a substantial estimated cost reduction through quality controlled crowd-sourcing. The original gold standard based on post-edits incurs a 10–20 times greater cost than DA.</abstract>
      <bibkey>graham-etal-2017-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="58">
      <title>Neural Machine Translation by Minimising the <fixed-case>B</fixed-case>ayes-risk with Respect to Syntactic Translation Lattices</title>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author><first>Adrià</first><last>de Gispert</last></author>
      <author><first>Eva</first><last>Hasler</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <pages>362–368</pages>
      <url hash="12e0359b">E17-2058</url>
      <abstract>We present a novel scheme to combine neural machine translation (NMT) with traditional statistical machine translation (SMT). Our approach borrows ideas from linearised lattice minimum Bayes-risk decoding for SMT. The NMT score is combined with the Bayes-risk of the translation according the SMT lattice. This makes our approach much more flexible than n-best list or lattice rescoring as the neural decoder is not restricted to the SMT search space. We show an efficient and simple way to integrate risk estimation into the NMT decoder which is suitable for word-level as well as subword-unit-level NMT. We test our method on English-German and Japanese-English and report significant gains over lattice rescoring on several data sets for both single and ensembled NMT. The MBR decoder produces entirely new hypotheses far beyond simply rescoring the SMT search space or fixing UNKs in the NMT output.</abstract>
      <bibkey>stahlberg-etal-2017-neural</bibkey>
    </paper>
    <paper id="59">
      <title>Producing Unseen Morphological Variants in Statistical Machine Translation</title>
      <author><first>Matthias</first><last>Huck</last></author>
      <author><first>Aleš</first><last>Tamchyna</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>369–375</pages>
      <url hash="d18b23be">E17-2059</url>
      <abstract>Translating into morphologically rich languages is difficult. Although the coverage of lemmas may be reasonable, many morphological variants cannot be learned from the training data. We present a statistical translation system that is able to produce these inflected word forms. Different from most previous work, we do not separate morphological prediction from lexical choice into two consecutive steps. Our approach is novel in that it is integrated in decoding and takes advantage of context information from both the source language and the target language sides.</abstract>
      <bibkey>huck-etal-2017-producing</bibkey>
    </paper>
    <paper id="60">
      <title>How Grammatical is Character-level Neural Machine Translation? Assessing <fixed-case>MT</fixed-case> Quality with Contrastive Translation Pairs</title>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>376–382</pages>
      <url hash="1614bc0a">E17-2060</url>
      <abstract>Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as agreement over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of error. We present LingEval97, a large-scale data set of 97000 contrastive translation pairs based on the WMT English-&gt;German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at transliteration than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.</abstract>
      <bibkey>sennrich-2017-grammatical</bibkey>
      <pwccode url="https://github.com/rsennrich/lingeval97" additional="false">rsennrich/lingeval97</pwccode>
    </paper>
    <paper id="61">
      <title>Neural Machine Translation with Recurrent Attention Modeling</title>
      <author><first>Zichao</first><last>Yang</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <author><first>Yuntian</first><last>Deng</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <author><first>Alex</first><last>Smola</last></author>
      <pages>383–387</pages>
      <url hash="cf0130ae">E17-2061</url>
      <abstract>Knowing which words have been attended to in previous time steps while generating a translation is a rich source of information for predicting what words will be attended to in the future. We improve upon the attention model of Bahdanau et al. (2014) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one recurrent network per input word. This architecture easily captures informative features, such as fertility and regularities in relative distortion. In experiments, we show our parameterization of attention improves translation quality.</abstract>
      <bibkey>yang-etal-2017-neural</bibkey>
    </paper>
    <paper id="62">
      <title>Inducing Embeddings for Rare and Unseen Words by Leveraging Lexical Resources</title>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>388–393</pages>
      <url hash="293cb21e">E17-2062</url>
      <abstract>We put forward an approach that exploits the knowledge encoded in lexical resources in order to induce representations for words that were not encountered frequently during training. Our approach provides an advantage over the past work in that it enables vocabulary expansion not only for morphological variations, but also for infrequent domain specific terms. We performed evaluations in different settings, showing that the technique can provide consistent improvements on multiple benchmarks across domains.</abstract>
      <bibkey>pilehvar-collier-2017-inducing</bibkey>
    </paper>
    <paper id="63">
      <title>Large-scale evaluation of dependency-based <fixed-case>DSM</fixed-case>s: Are they worth the effort?</title>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <author><first>Stefan</first><last>Evert</last></author>
      <pages>394–400</pages>
      <url hash="163566e5">E17-2063</url>
      <abstract>This paper presents a large-scale evaluation study of dependency-based distributional semantic models. We evaluate dependency-filtered and dependency-structured DSMs in a number of standard semantic similarity tasks, systematically exploring their parameter space in order to give them a “fair shot” against window-based models. Our results show that properly tuned window-based DSMs still outperform the dependency-based models in most tasks. There appears to be little need for the language-dependent resources and computational cost associated with syntactic analysis.</abstract>
      <bibkey>lapesa-evert-2017-large</bibkey>
    </paper>
    <paper id="64">
      <title>How Well Can We Predict Hypernyms from Word Embeddings? A Dataset-Centric Analysis</title>
      <author><first>Ivan</first><last>Sanchez</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <pages>401–407</pages>
      <url hash="30261370">E17-2064</url>
      <abstract>One key property of word embeddings currently under study is their capacity to encode hypernymy. Previous works have used supervised models to recover hypernymy structures from embeddings. However, the overall results do not clearly show how well we can recover such structures. We conduct the first dataset-centric analysis that shows how only the Baroni dataset provides consistent results. We empirically show that a possible reason for its good performance is its alignment to dimensions specific of hypernymy: generality and similarity</abstract>
      <bibkey>sanchez-riedel-2017-well</bibkey>
    </paper>
    <paper id="65">
      <title>Cross-Lingual Syntactically Informed Distributed Word Representations</title>
      <author><first>Ivan</first><last>Vulić</last></author>
      <pages>408–414</pages>
      <url hash="a1eff5a2">E17-2065</url>
      <abstract>We develop a novel cross-lingual word representation model which injects syntactic information through dependency-based contexts into a shared cross-lingual word vector space. The model, termed CL-DepEmb, is based on the following assumptions: (1) dependency relations are largely language-independent, at least for related languages and prominent dependency links such as direct objects, as evidenced by the Universal Dependencies project; (2) word translation equivalents take similar grammatical roles in a sentence and are therefore substitutable within their syntactic contexts. Experiments with several language pairs on word similarity and bilingual lexicon induction, two fundamental semantic tasks emphasising semantic similarity, suggest the usefulness of the proposed syntactically informed cross-lingual word vector spaces. Improvements are observed in both tasks over standard cross-lingual “offline mapping” baselines trained using the same setup and an equal level of bilingual supervision.</abstract>
      <bibkey>vulic-2017-cross</bibkey>
    </paper>
    <paper id="66">
      <title>Using Word Embedding for Cross-Language Plagiarism Detection</title>
      <author><first>Jérémy</first><last>Ferrero</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Frédéric</first><last>Agnès</last></author>
      <pages>415–421</pages>
      <url hash="3e0bf0c3">E17-2066</url>
      <abstract>This paper proposes to use distributed representation of words (word embeddings) in cross-language textual similarity detection. The main contributions of this paper are the following: (a) we introduce new cross-language similarity detection methods based on distributed representation of words; (b) we combine the different methods proposed to verify their complementarity and finally obtain an overall F1 score of 89.15% for English-French similarity detection at chunk level (88.5% at sentence level) on a very challenging corpus.</abstract>
      <attachment type="presentation" hash="37c62a21">E17-2066.Presentation.pdf</attachment>
      <bibkey>ferrero-etal-2017-using</bibkey>
    </paper>
    <paper id="67">
      <title>The Interplay of Semantics and Morphology in Word Embeddings</title>
      <author><first>Oded</first><last>Avraham</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>422–426</pages>
      <url hash="3dc2dbb7">E17-2067</url>
      <abstract>We explore the ability of word embeddings to capture both semantic and morphological similarity, as affected by the different types of linguistic properties (surface form, lemma, morphological tag) used to compose the representation of each word. We train several models, where each uses a different subset of these properties to compose its representations. By evaluating the models on semantic and morphological measures, we reveal some useful insights on the relationship between semantics and morphology.</abstract>
      <bibkey>avraham-goldberg-2017-interplay</bibkey>
      <pwccode url="https://github.com/oavraham1/prop2vec" additional="false">oavraham1/prop2vec</pwccode>
    </paper>
    <paper id="68">
      <title>Bag of Tricks for Efficient Text Classification</title>
      <author><first>Armand</first><last>Joulin</last></author>
      <author><first>Edouard</first><last>Grave</last></author>
      <author><first>Piotr</first><last>Bojanowski</last></author>
      <author><first>Tomas</first><last>Mikolov</last></author>
      <pages>427–431</pages>
      <url hash="0e826023">E17-2068</url>
      <abstract>This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.</abstract>
      <bibkey>joulin-etal-2017-bag</bibkey>
      <pwccode url="https://github.com/facebookresearch/fastText" additional="true">facebookresearch/fastText</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cped">CPED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/yfcc100m">YFCC100M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/yahoo-answers">Yahoo! Answers</pwcdataset>
    </paper>
    <paper id="69">
      <title>Pulling Out the Stops: Rethinking Stopword Removal for Topic Models</title>
      <author><first>Alexandra</first><last>Schofield</last></author>
      <author><first>Måns</first><last>Magnusson</last></author>
      <author><first>David</first><last>Mimno</last></author>
      <pages>432–436</pages>
      <url hash="3ab2b20e">E17-2069</url>
      <abstract>It is often assumed that topic models benefit from the use of a manually curated stopword list. Constructing this list is time-consuming and often subject to user judgments about what kinds of words are important to the model and the application. Although stopword removal clearly affects which word types appear as most probable terms in topics, we argue that this improvement is superficial, and that topic inference benefits little from the practice of removing stopwords beyond very frequent terms. Removing corpus-specific stopwords after model inference is more transparent and produces similar results to removing those words prior to inference.</abstract>
      <bibkey>schofield-etal-2017-pulling</bibkey>
    </paper>
    <paper id="70">
      <title>Measuring Topic Coherence through Optimal Word Buckets</title>
      <author><first>Nitin</first><last>Ramrakhiyani</last></author>
      <author><first>Sachin</first><last>Pawar</last></author>
      <author><first>Swapnil</first><last>Hingmire</last></author>
      <author><first>Girish</first><last>Palshikar</last></author>
      <pages>437–442</pages>
      <url hash="71fe0ec8">E17-2070</url>
      <abstract>Measuring topic quality is essential for scoring the learned topics and their subsequent use in Information Retrieval and Text classification. To measure quality of Latent Dirichlet Allocation (LDA) based topics learned from text, we propose a novel approach based on grouping of topic words into buckets (TBuckets). A single large bucket signifies a single coherent theme, in turn indicating high topic coherence. TBuckets uses word embeddings of topic words and employs singular value decomposition (SVD) and Integer Linear Programming based optimization to create coherent word buckets. TBuckets outperforms the state-of-the-art techniques when evaluated using 3 publicly available datasets and on another one proposed in this paper.</abstract>
      <bibkey>ramrakhiyani-etal-2017-measuring</bibkey>
    </paper>
    <paper id="71">
      <title>A Hybrid <fixed-case>CNN</fixed-case>-<fixed-case>RNN</fixed-case> Alignment Model for Phrase-Aware Sentence Classification</title>
      <author><first>Shiou Tian</first><last>Hsu</last></author>
      <author><first>Changsung</first><last>Moon</last></author>
      <author><first>Paul</first><last>Jones</last></author>
      <author><first>Nagiza</first><last>Samatova</last></author>
      <pages>443–449</pages>
      <url hash="1e235c3a">E17-2071</url>
      <abstract>The success of sentence classification often depends on understanding both the syntactic and semantic properties of word-phrases. Recent progress on this task has been based on exploiting the grammatical structure of sentences but often this structure is difficult to parse and noisy. In this paper, we propose a structure-independent ‘Gated Representation Alignment’ (GRA) model that blends a phrase-focused Convolutional Neural Network (CNN) approach with sequence-oriented Recurrent Neural Network (RNN). Our novel alignment mechanism allows the RNN to selectively include phrase information in a word-by-word sentence representation, and to do this without awareness of the syntactic structure. An empirical evaluation of GRA shows higher prediction accuracy (up to 4.6%) of fine-grained sentiment ratings, when compared to other structure-independent baselines. We also show comparable results to several structure-dependent methods. Finally, we analyzed the effect of our alignment mechanism and found that this is critical to the effectiveness of the CNN-RNN hybrid.</abstract>
      <bibkey>hsu-etal-2017-hybrid</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="72">
      <title>Multivariate <fixed-case>G</fixed-case>aussian Document Representation from Word Embeddings for Text Categorization</title>
      <author><first>Giannis</first><last>Nikolentzos</last></author>
      <author><first>Polykarpos</first><last>Meladianos</last></author>
      <author><first>François</first><last>Rousseau</last></author>
      <author><first>Yannis</first><last>Stavrakas</last></author>
      <author><first>Michalis</first><last>Vazirgiannis</last></author>
      <pages>450–455</pages>
      <url hash="4f7dd334">E17-2072</url>
      <abstract>Recently, there has been a lot of activity in learning distributed representations of words in vector spaces. Although there are models capable of learning high-quality distributed representations of words, how to generate vector representations of the same quality for phrases or documents still remains a challenge. In this paper, we propose to model each document as a multivariate Gaussian distribution based on the distributed representations of its words. We then measure the similarity between two documents based on the similarity of their distributions. Experiments on eight standard text categorization datasets demonstrate the effectiveness of the proposed approach in comparison with state-of-the-art methods.</abstract>
      <bibkey>nikolentzos-etal-2017-multivariate</bibkey>
    </paper>
    <paper id="73">
      <title>Derivation of Document Vectors from Adaptation of <fixed-case>LSTM</fixed-case> Language Model</title>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Brian</first><last>Mak</last></author>
      <pages>456–461</pages>
      <url hash="1a14fffe">E17-2073</url>
      <abstract>In many natural language processing (NLP) tasks, a document is commonly modeled as a bag of words using the term frequency-inverse document frequency (TF-IDF) vector. One major shortcoming of the frequency-based TF-IDF feature vector is that it ignores word orders that carry syntactic and semantic relationships among the words in a document. This paper proposes a novel distributed vector representation of a document, which will be labeled as DV-LSTM, and is derived from the result of adapting a long short-term memory recurrent neural network language model by the document. DV-LSTM is expected to capture some high-level sequential information in the document, which other current document representations fail to do. It was evaluated in document genre classification in the Brown Corpus and the BNC Baby Corpus. The results show that DV-LSTM significantly outperforms TF-IDF vector and paragraph vector (PV-DM) in most cases, and their combinations may further improve the classification performance.</abstract>
      <bibkey>li-mak-2017-derivation</bibkey>
    </paper>
    <paper id="74">
      <title>Real-Time Keyword Extraction from Conversations</title>
      <author><first>Polykarpos</first><last>Meladianos</last></author>
      <author><first>Antoine</first><last>Tixier</last></author>
      <author><first>Ioannis</first><last>Nikolentzos</last></author>
      <author><first>Michalis</first><last>Vazirgiannis</last></author>
      <pages>462–467</pages>
      <url hash="4a36f322">E17-2074</url>
      <abstract>We introduce a novel method to extract keywords from meeting speech in real-time. Our approach builds on the graph-of-words representation of text and leverages the k-core decomposition algorithm and properties of submodular functions. We outperform multiple baselines in a real-time scenario emulated from the AMI and ICSI meeting corpora. Evaluation is conducted against both extractive and abstractive gold standard using two standard performance metrics and a newer one based on word embeddings.</abstract>
      <bibkey>meladianos-etal-2017-real</bibkey>
    </paper>
    <paper id="75">
      <title>A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue</title>
      <author><first>Mihail</first><last>Eric</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>468–473</pages>
      <url hash="6e9ec070">E17-2075</url>
      <abstract>Task-oriented dialogue focuses on conversational agents that participate in dialogues with user goals on domain-specific topics. In contrast to chatbots, which simply seek to sustain open-ended meaningful discourse, existing task-oriented agents usually explicitly model user intent and belief states. This paper examines bypassing such an explicit representation by depending on a latent neural embedding of state and learning selective attention to dialogue history together with copying to incorporate relevant prior context. We complement recent work by showing the effectiveness of simple sequence-to-sequence neural architectures with a copy mechanism. Our model outperforms more complex memory-augmented models by 7% in per-response generation and is on par with the current state-of-the-art on DSTC2, a real-world task-oriented dialogue dataset.</abstract>
      <bibkey>eric-manning-2017-copy</bibkey>
    </paper>
    <paper id="76">
      <title>Towards speech-to-text translation without speech recognition</title>
      <author><first>Sameer</first><last>Bansal</last></author>
      <author><first>Herman</first><last>Kamper</last></author>
      <author><first>Adam</first><last>Lopez</last></author>
      <author><first>Sharon</first><last>Goldwater</last></author>
      <pages>474–479</pages>
      <url hash="fb120902">E17-2076</url>
      <abstract>We explore the problem of translating speech to text in low-resource scenarios where neither automatic speech recognition (ASR) nor machine translation (MT) are available, but we have training data in the form of audio paired with text translations. We present the first system for this problem applied to a realistic multi-speaker dataset, the CALLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data.</abstract>
      <bibkey>bansal-etal-2017-towards</bibkey>
    </paper>
    <paper id="77">
      <title>Evaluating Persuasion Strategies and Deep Reinforcement Learning methods for Negotiation Dialogue agents</title>
      <author><first>Simon</first><last>Keizer</last></author>
      <author><first>Markus</first><last>Guhe</last></author>
      <author><first>Heriberto</first><last>Cuayáhuitl</last></author>
      <author><first>Ioannis</first><last>Efstathiou</last></author>
      <author><first>Klaus-Peter</first><last>Engelbrecht</last></author>
      <author><first>Mihai</first><last>Dobre</last></author>
      <author><first>Alex</first><last>Lascarides</last></author>
      <author><first>Oliver</first><last>Lemon</last></author>
      <pages>480–484</pages>
      <url hash="03f5791f">E17-2077</url>
      <abstract>In this paper we present a comparative evaluation of various negotiation strategies within an online version of the game “Settlers of Catan”. The comparison is based on human subjects playing games against artificial game-playing agents (‘bots’) which implement different negotiation dialogue strategies, using a chat dialogue interface to negotiate trades. Our results suggest that a negotiation strategy that uses persuasion, as well as a strategy that is trained from data using Deep Reinforcement Learning, both lead to an improved win rate against humans, compared to previous rule-based and supervised learning baseline dialogue negotiators.</abstract>
      <bibkey>keizer-etal-2017-evaluating</bibkey>
    </paper>
    <paper id="78">
      <title>Unsupervised Dialogue Act Induction using <fixed-case>G</fixed-case>aussian Mixtures</title>
      <author><first>Tomáš</first><last>Brychcín</last></author>
      <author><first>Pavel</first><last>Král</last></author>
      <pages>485–490</pages>
      <url hash="bdccb948">E17-2078</url>
      <abstract>This paper introduces a new unsupervised approach for dialogue act induction. Given the sequence of dialogue utterances, the task is to assign them the labels representing their function in the dialogue. Utterances are represented as real-valued vectors encoding their meaning. We model the dialogue as Hidden Markov model with emission probabilities estimated by Gaussian mixtures. We use Gibbs sampling for posterior inference. We present the results on the standard Switchboard-DAMSL corpus. Our algorithm achieves promising results compared with strong supervised baselines and outperforms other unsupervised algorithms.</abstract>
      <bibkey>brychcin-kral-2017-unsupervised</bibkey>
    </paper>
    <paper id="79">
      <title>Grounding Language by Continuous Observation of Instruction Following</title>
      <author><first>Ting</first><last>Han</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>491–496</pages>
      <url hash="d7ccfaa1">E17-2079</url>
      <abstract>Grounded semantics is typically learnt from utterance-level meaning representations (e.g., successful database retrievals, denoted objects in images, moves in a game). We explore learning word and utterance meanings by continuous observation of the actions of an instruction follower (IF). While an instruction giver (IG) provided a verbal description of a configuration of objects, IF recreated it using a GUI. Aligning these GUI actions to sub-utterance chunks allows a simple maximum entropy model to associate them as chunk meaning better than just providing it with the utterance-final configuration. This shows that semantics useful for incremental (word-by-word) application, as required in natural dialogue, might also be better acquired from incremental settings.</abstract>
      <bibkey>han-schlangen-2017-grounding</bibkey>
    </paper>
    <paper id="80">
      <title>Mapping the Perfect via Translation Mining</title>
      <author><first>Martijn</first><last>van der Klis</last></author>
      <author><first>Bert</first><last>Le Bruyn</last></author>
      <author><first>Henriëtte</first><last>de Swart</last></author>
      <pages>497–502</pages>
      <url hash="a3125e80">E17-2080</url>
      <abstract>Semantic analyses of the Perfect often defeat their own purpose: by restricting their attention to ‘real’ perfects (like the English one), they implicitly assume the Perfect has predefined meanings and usages. We turn the tables and focus on form, using data extracted from multilingual parallel corpora to automatically generate semantic maps (Haspelmath, 1997) of the sequence ‘Have/Be + past participle’ in five European languages (German, English, Spanish, French, Dutch). This technique, which we dub Translation Mining, has been applied before in the lexical domain (Wälchli and Cysouw, 2012) but we showcase its application at the level of the grammar.</abstract>
      <bibkey>van-der-klis-etal-2017-mapping</bibkey>
    </paper>
    <paper id="81">
      <title>Efficient, Compositional, Order-sensitive n-gram Embeddings</title>
      <author><first>Adam</first><last>Poliak</last></author>
      <author><first>Pushpendre</first><last>Rastogi</last></author>
      <author><first>M. Patrick</first><last>Martin</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>503–508</pages>
      <url hash="6a3da8bf">E17-2081</url>
      <abstract>We propose ECO: a new way to generate embeddings for phrases that is Efficient, Compositional, and Order-sensitive. Our method creates decompositional embeddings for words offline and combines them to create new embeddings for phrases in real time. Unlike other approaches, ECO can create embeddings for phrases not seen during training. We evaluate ECO on supervised and unsupervised tasks and demonstrate that creating phrase embeddings that are sensitive to word order can help downstream tasks.</abstract>
      <bibkey>poliak-etal-2017-efficient</bibkey>
      <pwccode url="https://github.com/azpoliak/eco" additional="false">azpoliak/eco</pwccode>
    </paper>
    <paper id="82">
      <title>Integrating Semantic Knowledge into Lexical Embeddings Based on Information Content Measurement</title>
      <author><first>Hsin-Yang</first><last>Wang</last></author>
      <author><first>Wei-Yun</first><last>Ma</last></author>
      <pages>509–515</pages>
      <url hash="f143f7e4">E17-2082</url>
      <abstract>Distributional word representations are widely used in NLP tasks. These representations are based on an assumption that words with a similar context tend to have a similar meaning. To improve the quality of the context-based embeddings, many researches have explored how to make full use of existing lexical resources. In this paper, we argue that while we incorporate the prior knowledge with context-based embeddings, words with different occurrences should be treated differently. Therefore, we propose to rely on the measurement of information content to control the degree of applying prior knowledge into context-based embeddings - different words would have different learning rates when adjusting their embeddings. In the result, we demonstrate that our embeddings get significant improvements on two different tasks: Word Similarity and Analogical Reasoning.</abstract>
      <bibkey>wang-ma-2017-integrating</bibkey>
      <pwccode url="https://github.com/hywangntut/KBE" additional="false">hywangntut/KBE</pwccode>
    </paper>
    <paper id="83">
      <title>Improving Neural Knowledge Base Completion with Cross-Lingual Projections</title>
      <author><first>Patrick</first><last>Klein</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>516–522</pages>
      <url hash="3518bd5d">E17-2083</url>
      <abstract>In this paper we present a cross-lingual extension of a neural tensor network model for knowledge base completion. We exploit multilingual synsets from BabelNet to translate English triples to other languages and then augment the reference knowledge base with cross-lingual triples. We project monolingual embeddings of different languages to a shared multilingual space and use them for network initialization (i.e., as initial concept embeddings). We then train the network with triples from the cross-lingually augmented knowledge base. Results on WordNet link prediction show that leveraging cross-lingual information yields significant gains over exploiting only monolingual triples.</abstract>
      <bibkey>klein-etal-2017-improving</bibkey>
    </paper>
    <paper id="84">
      <title>Modelling metaphor with attribute-based semantics</title>
      <author><first>Luana</first><last>Bulat</last></author>
      <author><first>Stephen</first><last>Clark</last></author>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <pages>523–528</pages>
      <url hash="cd989416">E17-2084</url>
      <abstract>One of the key problems in computational metaphor modelling is finding the optimal level of abstraction of semantic representations, such that these are able to capture and generalise metaphorical mechanisms. In this paper we present the first metaphor identification method that uses representations constructed from property norms. Such norms have been previously shown to provide a cognitively plausible representation of concepts in terms of semantic properties. Our results demonstrate that such property-based semantic representations provide a suitable model of cross-domain knowledge projection in metaphors, outperforming standard distributional models on a metaphor identification task.</abstract>
      <bibkey>bulat-etal-2017-modelling</bibkey>
    </paper>
    <paper id="85">
      <title>When a Red Herring in Not a Red Herring: Using Compositional Methods to Detect Non-Compositional Phrases</title>
      <author><first>Julie</first><last>Weeds</last></author>
      <author><first>Thomas</first><last>Kober</last></author>
      <author><first>Jeremy</first><last>Reffin</last></author>
      <author><first>David</first><last>Weir</last></author>
      <pages>529–534</pages>
      <url hash="b6430d35">E17-2085</url>
      <abstract>Non-compositional phrases such as <i>red herring</i> and weakly compositional phrases such as <i>spelling bee</i> are an integral part of natural language (Sag, 2002).  They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics. Compositionality detection therefore provides a good testbed for compositional methods. We compare an integrated compositional distributional approach, using sparse high dimensional representations, with the ad-hoc compositional approach of applying simple composition operations to state-of-the-art neural embeddings.</abstract>
      <bibkey>weeds-etal-2017-red</bibkey>
    </paper>
    <paper id="86">
      <title>Applying Multi-Sense Embeddings for <fixed-case>G</fixed-case>erman Verbs to Determine Semantic Relatedness and to Detect Non-Literal Language</title>
      <author><first>Maximilian</first><last>Köper</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>535–542</pages>
      <url hash="1c6d8693">E17-2086</url>
      <abstract>Up to date, the majority of computational models still determines the semantic relatedness between words (or larger linguistic units) on the type level. In this paper, we compare and extend multi-sense embeddings, in order to model and utilise word senses on the token level. We focus on the challenging class of complex verbs, and evaluate the model variants on various semantic tasks: semantic classification; predicting compositionality; and detecting non-literal language usage. While there is no overall best model, all models significantly outperform a word2vec single-sense skip baseline, thus demonstrating the need to distinguish between word senses in a distributional semantic model.</abstract>
      <bibkey>koper-schulte-im-walde-2017-applying</bibkey>
    </paper>
    <paper id="87">
      <title>Negative Sampling Improves Hypernymy Extraction Based on Projection Learning</title>
      <author><first>Dmitry</first><last>Ustalov</last></author>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>543–550</pages>
      <url hash="56b46ff5">E17-2087</url>
      <abstract>We present a new approach to extraction of hypernyms based on projection learning and word embeddings. In contrast to classification-based approaches, projection-based methods require no candidate hyponym-hypernym pairs. While it is natural to use both positive and negative training examples in supervised relation extraction, the impact of positive examples on hypernym prediction was not studied so far. In this paper, we show that explicit negative examples used for regularization of the model significantly improve performance compared to the state-of-the-art approach of Fu et al. (2014) on three datasets from different languages.</abstract>
      <bibkey>ustalov-etal-2017-negative</bibkey>
      <pwccode url="https://github.com/nlpub/projlearn" additional="false">nlpub/projlearn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/evalution">EVALution</pwcdataset>
    </paper>
    <paper id="88">
      <title>A Dataset for Multi-Target Stance Detection</title>
      <author><first>Parinaz</first><last>Sobhani</last></author>
      <author><first>Diana</first><last>Inkpen</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <pages>551–557</pages>
      <url hash="d2fae70b">E17-2088</url>
      <abstract>Current models for stance classification often treat each target independently, but in many applications, there exist natural dependencies among targets, e.g., stance towards two or more politicians in an election or towards several brands of the same product. In this paper, we focus on the problem of multi-target stance detection. We present a new dataset that we built for this task. Furthermore, We experiment with several neural models on the dataset and show that they are more effective in jointly modeling the overall position towards two related targets compared to independent predictions and other models of joint learning, such as cascading classification. We make the new dataset publicly available, in order to facilitate further research in multi-target stance classification.</abstract>
      <bibkey>sobhani-etal-2017-dataset</bibkey>
    </paper>
    <paper id="89">
      <title>Single and Cross-domain Polarity Classification using String Kernels</title>
      <author><first>Rosa M.</first><last>Giménez-Pérez</last></author>
      <author><first>Marc</first><last>Franco-Salvador</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>558–563</pages>
      <url hash="f25d71e4">E17-2089</url>
      <abstract>The polarity classification task aims at automatically identifying whether a subjective text is positive or negative. When the target domain is different from those where a model was trained, we refer to a cross-domain setting. That setting usually implies the use of a domain adaptation method. In this work, we study the single and cross-domain polarity classification tasks from the string kernels perspective. Contrary to classical domain adaptation methods, which employ texts from both domains to detect pivot features, we do not use the target domain for training. Our approach detects the lexical peculiarities that characterise the text polarity and maps them into a domain independent space by means of kernel discriminant analysis. Experimental results show state-of-the-art performance in single and cross-domain polarity classification.</abstract>
      <bibkey>gimenez-perez-etal-2017-single</bibkey>
    </paper>
    <paper id="90">
      <title>Predicting Emotional Word Ratings using Distributional Representations and Signed Clustering</title>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Daniel</first><last>Preoţiuc-Pietro</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <pages>564–571</pages>
      <url hash="e5e3dda5">E17-2090</url>
      <abstract>Inferring the emotional content of words is important for text-based sentiment analysis, dialogue systems and psycholinguistics, but word ratings are expensive to collect at scale and across languages or domains. We develop a method that automatically extends word-level ratings to unrated words using signed clustering of vector space word representations along with affect ratings. We use our method to determine a word’s valence and arousal, which determine its position on the circumplex model of affect, the most popular dimensional model of emotion. Our method achieves superior out-of-sample word rating prediction on both affective dimensions across three different languages when compared to state-of-the-art word similarity based methods. Our method can assist building word ratings for new languages and improve downstream tasks such as sentiment analysis and emotion detection.</abstract>
      <bibkey>sedoc-etal-2017-predicting</bibkey>
    </paper>
    <paper id="91">
      <title>Attention Modeling for Targeted Sentiment</title>
      <author><first>Jiangming</first><last>Liu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>572–577</pages>
      <url hash="3d6d0242">E17-2091</url>
      <abstract>Neural network models have been used for target-dependent sentiment analysis. Previous work focus on learning a target specific representation for a given input sentence which is used for classification. However, they do not explicitly model the contribution of each word in a sentence with respect to targeted sentiment polarities. We investigate an attention model to this end. In particular, a vanilla LSTM model is used to induce an attention value of the whole sentence. The model is further extended to differentiate left and right contexts given a certain target following previous work. Results show that by using attention to model the contribution of each word with respect to the target, our model gives significantly improved results over two standard benchmarks. We report the best accuracy for this task.</abstract>
      <bibkey>liu-zhang-2017-attention</bibkey>
    </paper>
    <paper id="92">
      <title><fixed-case>E</fixed-case>mo<fixed-case>B</fixed-case>ank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis</title>
      <author><first>Sven</first><last>Buechel</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <pages>578–585</pages>
      <url hash="c5365d3a">E17-2092</url>
      <abstract>We describe EmoBank, a corpus of 10k English sentences balancing multiple genres, which we annotated with dimensional emotion metadata in the Valence-Arousal-Dominance (VAD) representation format. EmoBank excels with a bi-perspectival and bi-representational design. On the one hand, we distinguish between writer’s and reader’s emotions, on the other hand, a subset of the corpus complements dimensional VAD annotations with categorical ones based on Basic Emotions. We find evidence for the supremacy of the reader’s perspective in terms of IAA and rating intensity, and achieve close-to-human performance when mapping between dimensional and categorical formats.</abstract>
      <bibkey>buechel-hahn-2017-emobank</bibkey>
      <pwccode url="https://github.com/JULIELab/EmoBank" additional="false">JULIELab/EmoBank</pwccode>
    </paper>
    <paper id="93">
      <title>Structural Attention Neural Networks for improved sentiment analysis</title>
      <author><first>Filippos</first><last>Kokkinos</last></author>
      <author><first>Alexandros</first><last>Potamianos</last></author>
      <pages>586–591</pages>
      <url hash="f67ed8a9">E17-2093</url>
      <abstract>We introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottom-up and top-down information propagation. Also, the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree.</abstract>
      <bibkey>kokkinos-potamianos-2017-structural</bibkey>
    </paper>
    <paper id="94">
      <title>Ranking Convolutional Recurrent Neural Networks for Purchase Stage Identification on Imbalanced <fixed-case>T</fixed-case>witter Data</title>
      <author><first>Heike</first><last>Adel</last></author>
      <author><first>Francine</first><last>Chen</last></author>
      <author><first>Yan-Ying</first><last>Chen</last></author>
      <pages>592–598</pages>
      <url hash="28daf44d">E17-2094</url>
      <abstract>Users often use social media to share their interest in products. We propose to identify purchase stages from Twitter data following the AIDA model (Awareness, Interest, Desire, Action). In particular, we define the task of classifying the purchase stage of each tweet in a user’s tweet sequence. We introduce RCRNN, a Ranking Convolutional Recurrent Neural Network which computes tweet representations using convolution over word embeddings and models a tweet sequence with gated recurrent units. Also, we consider various methods to cope with the imbalanced label distribution in our data and show that a ranking layer outperforms class weights.</abstract>
      <bibkey>adel-etal-2017-ranking</bibkey>
    </paper>
    <paper id="95">
      <title>Context-Aware Graph Segmentation for Graph-Based Translation</title>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>599–604</pages>
      <url hash="16c2e034">E17-2095</url>
      <abstract>In this paper, we present an improved graph-based translation model which segments an input graph into node-induced subgraphs by taking source context into consideration. Translations are generated by combining subgraph translations left-to-right using beam search. Experiments on Chinese–English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model.</abstract>
      <bibkey>li-etal-2017-context</bibkey>
    </paper>
    <paper id="96">
      <title>Reranking Translation Candidates Produced by Several Bilingual Word Similarity Sources</title>
      <author><first>Laurent</first><last>Jakubina</last></author>
      <author><first>Phillippe</first><last>Langlais</last></author>
      <pages>605–611</pages>
      <url hash="7bd5e73c">E17-2096</url>
      <abstract>We investigate the reranking of the output of several distributional approaches on the Bilingual Lexicon Induction task. We show that reranking an n-best list produced by any of those approaches leads to very substantial improvements. We further demonstrate that combining several n-best lists by reranking is an effective way of further boosting performance.</abstract>
      <bibkey>jakubina-langlais-2017-reranking</bibkey>
    </paper>
    <paper id="97">
      <title>Lexicalized Reordering for Left-to-Right Hierarchical Phrase-based Translation</title>
      <author><first>Maryam</first><last>Siahbani</last></author>
      <author><first>Anoop</first><last>Sarkar</last></author>
      <pages>612–618</pages>
      <url hash="d924f526">E17-2097</url>
      <abstract>Phrase-based and hierarchical phrase-based (Hiero) translation models differ radically in the way reordering is modeled. Lexicalized reordering models play an important role in phrase-based MT and such models have been added to CKY-based decoders for Hiero. Watanabe et al. (2006) proposed a promising decoding algorithm for Hiero (LR-Hiero) that visits input spans in arbitrary order and produces the translation in left to right (LR) order which leads to far fewer language model calls and leads to a considerable speedup in decoding. We introduce a novel shift-reduce algorithm to LR-Hiero to decode with our lexicalized reordering model (LRM) and show that it improves translation quality for Czech-English, Chinese-English and German-English.</abstract>
      <bibkey>siahbani-sarkar-2017-lexicalized</bibkey>
    </paper>
    <paper id="98">
      <title>Bootstrapping Unsupervised Bilingual Lexicon Induction</title>
      <author><first>Bradley</first><last>Hauer</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <pages>619–624</pages>
      <url hash="bf1b7f20">E17-2098</url>
      <abstract>The task of unsupervised lexicon induction is to find translation pairs across monolingual corpora. We develop a novel method that creates seed lexicons by identifying cognates in the vocabularies of related languages on the basis of their frequency and lexical similarity. We apply bidirectional bootstrapping to a method which learns a linear mapping between context-based vector spaces. Experimental results on three language pairs show consistent improvement over prior work.</abstract>
      <bibkey>hauer-etal-2017-bootstrapping</bibkey>
    </paper>
    <paper id="99">
      <title>Addressing Problems across Linguistic Levels in <fixed-case>SMT</fixed-case>: Combining Approaches to Model Morphology, Syntax and Lexical Choice</title>
      <author><first>Marion</first><last>Weller-Di Marco</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>625–630</pages>
      <url hash="c012935f">E17-2099</url>
      <abstract>Many errors in phrase-based SMT can be attributed to problems on three linguistic levels: morphological complexity in the target language, structural differences and lexical choice. We explore combinations of linguistically motivated approaches to address these problems in English-to-German SMT and show that they are complementary to one another, but also that the popular verbal pre-ordering can cause problems on the morphological and lexical level. A discriminative classifier can overcome these problems, in particular when enriching standard lexical features with features geared towards verbal inflection.</abstract>
      <bibkey>weller-di-marco-etal-2017-addressing</bibkey>
    </paper>
    <paper id="100">
      <title>Machine Translation of <fixed-case>S</fixed-case>panish Personal and Possessive Pronouns Using Anaphora Probabilities</title>
      <author><first>Ngoc Quang</first><last>Luong</last></author>
      <author><first>Andrei</first><last>Popescu-Belis</last></author>
      <author><first>Annette</first><last>Rios Gonzales</last></author>
      <author><first>Don</first><last>Tuggener</last></author>
      <pages>631–636</pages>
      <url hash="0b7f627f">E17-2100</url>
      <abstract>We implement a fully probabilistic model to combine the hypotheses of a Spanish anaphora resolution system with those of a Spanish-English machine translation system. The probabilities over antecedents are converted into probabilities for the features of translated pronouns, and are integrated with phrase-based MT using an additional translation model for pronouns. The system improves the translation of several Spanish personal and possessive pronouns into English, by solving translation divergencies such as ‘ella’ vs. ‘she’/‘it’ or ‘su’ vs. ‘his’/‘her’/‘its’/‘their’. On a test set with 2,286 pronouns, a baseline system correctly translates 1,055 of them, while ours improves this by 41. Moreover, with oracle antecedents, possessives are translated with an accuracy of 83%.</abstract>
      <bibkey>luong-etal-2017-machine</bibkey>
      <pwccode url="https://github.com/a-rios/CorefMT" additional="false">a-rios/CorefMT</pwccode>
    </paper>
    <paper id="101">
      <title>Using Images to Improve Machine-Translating <fixed-case>E</fixed-case>-Commerce Product Listings.</title>
      <author><first>Iacer</first><last>Calixto</last></author>
      <author><first>Daniel</first><last>Stein</last></author>
      <author><first>Evgeny</first><last>Matusov</last></author>
      <author><first>Pintu</first><last>Lohar</last></author>
      <author><first>Sheila</first><last>Castilho</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>637–643</pages>
      <url hash="aa16b521">E17-2101</url>
      <abstract>In this paper we study the impact of using images to machine-translate user-generated e-commerce product listings. We study how a multi-modal Neural Machine Translation (NMT) model compares to two text-only approaches: a conventional state-of-the-art attentional NMT and a Statistical Machine Translation (SMT) model. User-generated product listings often do not constitute grammatical or well-formed sentences. More often than not, they consist of the juxtaposition of short phrases or keywords. We train our models end-to-end as well as use text-only and multi-modal NMT models for re-ranking <tex-math>n</tex-math>-best lists generated by an SMT model. We qualitatively evaluate our user-generated training data also analyse how adding synthetic data impacts the results. We evaluate our models quantitatively using BLEU and TER and find that (i) additional synthetic data has a general positive impact on text-only and multi-modal NMT models, and that (ii) using a multi-modal NMT model for re-ranking n-best lists improves TER significantly across different n-best list sizes.</abstract>
      <bibkey>calixto-etal-2017-using</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2015">WMT 2015</pwcdataset>
    </paper>
    <paper id="102">
      <title>Continuous multilinguality with language vectors</title>
      <author><first>Robert</first><last>Östling</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>644–649</pages>
      <url hash="e687c26d">E17-2102</url>
      <abstract>Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages.</abstract>
      <bibkey>ostling-tiedemann-2017-continuous</bibkey>
    </paper>
    <paper id="103">
      <title>Unsupervised Training for Large Vocabulary Translation Using Sparse Lexicon and Word Classes</title>
      <author><first>Yunsu</first><last>Kim</last></author>
      <author><first>Julian</first><last>Schamper</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>650–656</pages>
      <url hash="aa00ab22">E17-2103</url>
      <abstract>We address for the first time unsupervised training for a translation task with hundreds of thousands of vocabulary words. We scale up the expectation-maximization (EM) algorithm to learn a large translation table without any parallel text or seed lexicon. First, we solve the memory bottleneck and enforce the sparsity with a simple thresholding scheme for the lexicon. Second, we initialize the lexicon training with word classes, which efficiently boosts the performance. Our methods produced promising results on two large-scale unsupervised translation tasks.</abstract>
      <bibkey>kim-etal-2017-unsupervised</bibkey>
    </paper>
    <paper id="104">
      <title>Co-reference Resolution of Elided Subjects and Possessive Pronouns in <fixed-case>S</fixed-case>panish-<fixed-case>E</fixed-case>nglish Statistical Machine Translation</title>
      <author><first>Annette</first><last>Rios Gonzales</last></author>
      <author><first>Don</first><last>Tuggener</last></author>
      <pages>657–662</pages>
      <url hash="a696c26b">E17-2104</url>
      <abstract>This paper presents a straightforward method to integrate co-reference information into phrase-based machine translation to address the problems of i) elided subjects and ii) morphological underspecification of pronouns when translating from pro-drop languages. We evaluate the method for the language pair Spanish-English and find that translation quality improves with the addition of co-reference information.</abstract>
      <bibkey>rios-gonzales-tuggener-2017-co</bibkey>
    </paper>
    <paper id="105">
      <title>Large-Scale Categorization of <fixed-case>J</fixed-case>apanese Product Titles Using Neural Attention Models</title>
      <author><first>Yandi</first><last>Xia</last></author>
      <author><first>Aaron</first><last>Levine</last></author>
      <author><first>Pradipto</first><last>Das</last></author>
      <author><first>Giuseppe</first><last>Di Fabbrizio</last></author>
      <author><first>Keiji</first><last>Shinzato</last></author>
      <author><first>Ankur</first><last>Datta</last></author>
      <pages>663–668</pages>
      <url hash="de1c01ed">E17-2105</url>
      <abstract>We propose a variant of Convolutional Neural Network (CNN) models, the Attention CNN (ACNN); for large-scale categorization of millions of Japanese items into thirty-five product categories. Compared to a state-of-the-art Gradient Boosted Tree (GBT) classifier, the proposed model reduces training time from three weeks to three days while maintaining more than 96% accuracy. Additionally, our proposed model characterizes products by imputing attentive focus on word tokens in a language agnostic way. The attention words have been observed to be semantically highly correlated with the predicted categories and give us a choice of automatic feature extraction for downstream processing.</abstract>
      <bibkey>xia-etal-2017-large</bibkey>
    </paper>
    <paper id="106">
      <title>Convolutional Neural Networks for Authorship Attribution of Short Texts</title>
      <author><first>Prasha</first><last>Shrestha</last></author>
      <author><first>Sebastian</first><last>Sierra</last></author>
      <author><first>Fabio</first><last>González</last></author>
      <author><first>Manuel</first><last>Montes</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>669–674</pages>
      <url hash="abf8ebe2">E17-2106</url>
      <abstract>We present a model to perform authorship attribution of tweets using Convolutional Neural Networks (CNNs) over character n-grams. We also present a strategy that improves model interpretability by estimating the importance of input text fragments in the predicted classification. The experimental evaluation shows that text CNNs perform competitively and are able to outperform previous methods.</abstract>
      <bibkey>shrestha-etal-2017-convolutional</bibkey>
    </paper>
    <paper id="107">
      <title>Aspect Extraction from Product Reviews Using Category Hierarchy Information</title>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Cen</first><last>Chen</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Forrest</first><last>Bao</last></author>
      <pages>675–680</pages>
      <url hash="4b848305">E17-2107</url>
      <abstract>Aspect extraction abstracts the common properties of objects from corpora discussing them, such as reviews of products. Recent work on aspect extraction is leveraging the hierarchical relationship between products and their categories. However, such effort focuses on the aspects of child categories but ignores those from parent categories. Hence, we propose an LDA-based generative topic model inducing the two-layer categorical information (CAT-LDA), to balance the aspects of both a parent category and its child categories. Our hypothesis is that child categories inherit aspects from parent categories, controlled by the hierarchy between them. Experimental results on 5 categories of Amazon.com products show that both common aspects of parent category and the individual aspects of sub-categories can be extracted to align well with the common sense. We further evaluate the manually extracted aspects of 16 products, resulting in an average hit rate of 79.10%.</abstract>
      <bibkey>yang-etal-2017-aspect</bibkey>
    </paper>
    <paper id="108">
      <title>On the Relevance of Syntactic and Discourse Features for Author Profiling and Identification</title>
      <author><first>Juan</first><last>Soler-Company</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <pages>681–687</pages>
      <url hash="59224b48">E17-2108</url>
      <abstract>The majority of approaches to author profiling and author identification focus mainly on lexical features, i.e., on the content of a text. We argue that syntactic and discourse features play a significantly more prominent role than they were given in the past. We show that they achieve state-of-the-art performance in author and gender identification on a literary corpus while keeping the feature set small: the used feature set is composed of only 188 features and still outperforms the winner of the PAN 2014 shared task on author verification in the literary genre.</abstract>
      <bibkey>soler-company-wanner-2017-relevance</bibkey>
    </paper>
    <paper id="109">
      <title>Unsupervised Cross-Lingual Scaling of Political Texts</title>
      <author><first>Goran</first><last>Glavaš</last></author>
      <author><first>Federico</first><last>Nanni</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <pages>688–693</pages>
      <url hash="73a998cc">E17-2109</url>
      <abstract>Political text scaling aims to linearly order parties and politicians across political dimensions (e.g., left-to-right ideology) based on textual content (e.g., politician speeches or party manifestos). Existing models scale texts based on relative word usage and cannot be used for cross-lingual analyses. Additionally, there is little quantitative evidence that the output of these models correlates with common political dimensions like left-to-right orientation. Experimental results show that the semantically-informed scaling models better predict the party positions than the existing word-based models in two different political dimensions. Furthermore, the proposed models exhibit no drop in performance in the cross-lingual compared to monolingual setting.</abstract>
      <bibkey>glavas-etal-2017-unsupervised</bibkey>
      <pwccode url="https://bitbucket.org/gg42554/cl-scaling" additional="false">gg42554/cl-scaling</pwccode>
    </paper>
    <paper id="110">
      <title>Neural Networks for Joint Sentence Classification in Medical Paper Abstracts</title>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Ji Young</first><last>Lee</last></author>
      <author><first>Peter</first><last>Szolovits</last></author>
      <pages>694–700</pages>
      <url hash="d30b2e6c">E17-2110</url>
      <abstract>Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model outperforms the state-of-the-art results on two different datasets for sequential sentence classification in medical abstracts.</abstract>
      <bibkey>dernoncourt-etal-2017-neural</bibkey>
      <pwccode url="" additional="true"/>
    </paper>
    <paper id="111">
      <title>Multimodal Topic Labelling</title>
      <author><first>Ionut</first><last>Sorodoc</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>701–706</pages>
      <url hash="937a9d0f">E17-2111</url>
      <abstract>Topics generated by topic models are typically presented as a list of topic terms. Automatic topic labelling is the task of generating a succinct label that summarises the theme or subject of a topic, with the intention of reducing the cognitive load of end-users when interpreting these topics. Traditionally, topic label systems focus on a single label modality, e.g. textual labels. In this work we propose a multimodal approach to topic labelling using a simple feedforward neural network. Given a topic and a candidate image or textual label, our method automatically generates a rating for the label, relative to the topic. Experiments show that this multimodal approach outperforms single-modality topic labelling systems.</abstract>
      <bibkey>sorodoc-etal-2017-multimodal</bibkey>
    </paper>
    <paper id="112">
      <title>Detecting (Un)Important Content for Single-Document News Summarization</title>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Forrest</first><last>Bao</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <pages>707–712</pages>
      <url hash="28c45c10">E17-2112</url>
      <abstract>We present a robust approach for detecting intrinsic sentence importance in news, by training on two corpora of document-summary pairs. When used for single-document summarization, our approach, combined with the “beginning of document” heuristic, outperforms a state-of-the-art summarizer and the beginning-of-article baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for news have not been able to consistently outperform the strong beginning-of-article baseline.</abstract>
      <bibkey>yang-etal-2017-detecting</bibkey>
    </paper>
    <paper id="113">
      <title><fixed-case>F</fixed-case>-Score Driven Max Margin Neural Network for Named Entity Recognition in <fixed-case>C</fixed-case>hinese Social Media</title>
      <author><first>Hangfeng</first><last>He</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>713–718</pages>
      <url hash="bf5d73b2">E17-2113</url>
      <abstract>We focus on named entity recognition (NER) for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a semi-supervised learning model based on B-LSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine transition probability with deep learning in our model. To bridge the gap between label accuracy and F-score of NER, we construct a model which can be directly trained on F-score. When considering the instability of F-score driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields 7.44% improvement over previous state-of-the-art result.</abstract>
      <bibkey>he-sun-2017-f</bibkey>
    </paper>
    <paper id="114">
      <title>Discriminative Information Retrieval for Question Answering Sentence Selection</title>
      <author><first>Tongfei</first><last>Chen</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>719–725</pages>
      <url hash="3d5590d5">E17-2114</url>
      <abstract>We propose a framework for discriminative IR atop linguistic features, trained to improve the recall of answer candidate passage retrieval, the initial step in text-based question answering. We formalize this as an instance of linear feature-based IR, demonstrating a 34%-43% improvement in recall for candidate triage for QA.</abstract>
      <bibkey>chen-van-durme-2017-discriminative</bibkey>
      <pwccode url="https://github.com/ctongfei/probe" additional="false">ctongfei/probe</pwccode>
    </paper>
    <paper id="115">
      <title>Effective shared representations with Multitask Learning for Community Question Answering</title>
      <author><first>Daniele</first><last>Bonadiman</last></author>
      <author><first>Antonio</first><last>Uva</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>726–732</pages>
      <url hash="f38f774f">E17-2115</url>
      <abstract>An important asset of using Deep Neural Networks (DNNs) for text applications is their ability to automatically engineering features. Unfortunately, DNNs usually require a lot of training data, especially for highly semantic tasks such as community Question Answering (cQA). In this paper, we tackle the problem of data scarcity by learning the target DNN together with two auxiliary tasks in a multitask learning setting. We exploit the strong semantic connection between selection of comments relevant to (i) new questions and (ii) forum questions. This enables a global representation for comments, new and previous questions. The experiments of our model on a SemEval challenge dataset for cQA show a 20% of relative improvement over standard DNNs.</abstract>
      <bibkey>bonadiman-etal-2017-effective</bibkey>
    </paper>
    <paper id="116">
      <title>Learning User Embeddings from Emails</title>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Chia-Jung</first><last>Lee</last></author>
      <pages>733–738</pages>
      <url hash="658ff436">E17-2116</url>
      <abstract>Many important email-related tasks, such as email classification or search, highly rely on building quality document representations (e.g., bag-of-words or key phrases) to assist matching and understanding. Despite prior success on representing textual messages, creating quality user representations from emails was overlooked. In this paper, we propose to represent users using embeddings that are trained to reflect the email communication network. Our experiments on Enron dataset suggest that the resulting embeddings capture the semantic distance between users. To assess the quality of embeddings in a real-world application, we carry out auto-foldering task where the lexical representation of an email is enriched with user embedding features. Our results show that folder prediction accuracy is improved when embedding features are present across multiple settings.</abstract>
      <bibkey>song-lee-2017-learning</bibkey>
    </paper>
    <paper id="117">
      <title>Temporal information extraction from clinical text</title>
      <author><first>Julien</first><last>Tourille</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <pages>739–745</pages>
      <url hash="bbda1185">E17-2117</url>
      <abstract>In this paper, we present a method for temporal relation extraction from clinical narratives in French and in English. We experiment on two comparable corpora, the MERLOT corpus and the THYME corpus, and show that a common approach can be used for both languages.</abstract>
      <bibkey>tourille-etal-2017-temporal</bibkey>
    </paper>
    <paper id="118">
      <title>Neural Temporal Relation Extraction</title>
      <author><first>Dmitriy</first><last>Dligach</last></author>
      <author><first>Timothy</first><last>Miller</last></author>
      <author><first>Chen</first><last>Lin</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <author><first>Guergana</first><last>Savova</last></author>
      <pages>746–751</pages>
      <url hash="97737a29">E17-2118</url>
      <abstract>We experiment with neural architectures for temporal relation extraction and establish a new state-of-the-art for several scenarios. We find that neural models with only tokens as input outperform state-of-the-art hand-engineered feature-based models, that convolutional neural networks outperform LSTM models, and that encoding relation arguments with XML tags outperforms a traditional position-based encoding.</abstract>
      <bibkey>dligach-etal-2017-neural</bibkey>
    </paper>
    <paper id="119">
      <title>End-to-End Trainable Attentive Decoder for Hierarchical Entity Classification</title>
      <author><first>Sanjeev</first><last>Karn</last></author>
      <author><first>Ulli</first><last>Waltinger</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>752–758</pages>
      <url hash="10d48bea">E17-2119</url>
      <abstract>We address fine-grained entity classification and propose a novel attention-based recurrent neural network (RNN) encoder-decoder that generates paths in the type hierarchy and can be trained end-to-end. We show that our model performs better on fine-grained entity classification than prior work that relies on flat or local classifiers that do not directly model hierarchical structure.</abstract>
      <bibkey>karn-etal-2017-end</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="120">
      <title>Neural Graphical Models over Strings for Principal Parts Morphological Paradigm Completion</title>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>John</first><last>Sylak-Glassman</last></author>
      <author><first>Christo</first><last>Kirov</last></author>
      <pages>759–765</pages>
      <url hash="086c52a6">E17-2120</url>
      <abstract>Many of the world’s languages contain an abundance of inflected forms for each lexeme. A critical task in processing such languages is predicting these inflected forms. We develop a novel statistical model for the problem, drawing on graphical modeling techniques and recent advances in deep learning. We derive a Metropolis-Hastings algorithm to jointly decode the model. Our Bayesian network draws inspiration from principal parts morphological analysis. We demonstrate improvements on 5 languages.</abstract>
      <bibkey>cotterell-etal-2017-neural</bibkey>
    </paper>
  </volume>
  <volume id="3">
    <meta>
      <booktitle>Proceedings of the Software Demonstrations of the 15th Conference of the <fixed-case>E</fixed-case>uropean Chapter of the Association for Computational Linguistics</booktitle>
      <url hash="e7754cd9">E17-3</url>
      <editor><first>André</first><last>Martins</last></editor>
      <editor><first>Anselmo</first><last>Peñas</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Valencia, Spain</address>
      <month>April</month>
      <year>2017</year>
      <venue>eacl</venue>
    </meta>
    <frontmatter>
      <url hash="90a3c31b">E17-3000</url>
      <bibkey>eacl-2017-software</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>COVER</fixed-case>: Covering the Semantically Tractable Questions</title>
      <author><first>Michael</first><last>Minock</last></author>
      <pages>1–4</pages>
      <url hash="15a9b77e">E17-3001</url>
      <abstract>In semantic parsing, natural language questions map to expressions in a meaning representation language (MRL) over some fixed vocabulary of predicates. To do this reliably, one must guarantee that for a wide class of natural language questions (the so called semantically tractable questions), correct interpretations are always in the mapped set of possibilities. In this demonstration, we introduce the system COVER which significantly clarifies, revises and extends the basic notion of semantic tractability. COVER achieves coverage of 89% while the earlier PRECISE system achieved coverage of 77% on the well known GeoQuery corpus. Like PRECISE, COVER requires only a simple domain lexicon and integrates off-the-shelf syntactic parsers. Beyond PRECISE, COVER also integrates off-the-shelf theorem provers to provide more accurate results. COVER is written in Python and uses the NLTK.</abstract>
      <bibkey>minock-2017-cover</bibkey>
    </paper>
    <paper id="2">
      <title>Common Round: Application of Language Technologies to Large-Scale Web Debates</title>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <author><first>Aleksandra</first><last>Gabryszak</last></author>
      <author><first>Leonhard</first><last>Hennig</last></author>
      <author><first>Jörg</first><last>Steffen</last></author>
      <author><first>Renlong</first><last>Ai</last></author>
      <author><first>Stephan</first><last>Busemann</last></author>
      <author><first>Jon</first><last>Dehdari</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <author><first>Georg</first><last>Heigold</last></author>
      <author><first>Nils</first><last>Rethmeier</last></author>
      <author><first>Raphael</first><last>Rubino</last></author>
      <author><first>Sven</first><last>Schmeier</last></author>
      <author><first>Philippe</first><last>Thomas</last></author>
      <author><first>He</first><last>Wang</last></author>
      <author><first>Feiyu</first><last>Xu</last></author>
      <pages>5–8</pages>
      <url hash="ff3a619b">E17-3002</url>
      <abstract>Web debates play an important role in enabling broad participation of constituencies in social, political and economic decision-taking. However, it is challenging to organize, structure, and navigate a vast number of diverse argumentations and comments collected from many participants over a long time period. In this paper we demonstrate Common Round, a next generation platform for large-scale web debates, which provides functions for eliciting the semantic content and structures from the contributions of participants. In particular, Common Round applies language technologies for the extraction of semantic essence from textual input, aggregation of the formulated opinions and arguments. The platform also provides a cross-lingual access to debates using machine translation.</abstract>
      <bibkey>uszkoreit-etal-2017-common</bibkey>
    </paper>
    <paper id="3">
      <title>A Web-Based Interactive Tool for Creating, Inspecting, Editing, and Publishing Etymological Datasets</title>
      <author><first>Johann-Mattis</first><last>List</last></author>
      <pages>9–12</pages>
      <url hash="21dbc813">E17-3003</url>
      <abstract>The paper presents the Etymological DICtionary ediTOR (EDICTOR), a free, interactive, web-based tool designed to aid historical linguists in creating, editing, analysing, and publishing etymological datasets. The EDICTOR offers interactive solutions for important tasks in historical linguistics, including facilitated input and segmentation of phonetic transcriptions, quantitative and qualitative analyses of phonetic and morphological data, enhanced interfaces for cognate class assignment and multiple word alignment, and automated evaluation of regular sound correspondences. As a web-based tool written in JavaScript, the EDICTOR can be used in standard web browsers across all major platforms.</abstract>
      <bibkey>list-2017-web</bibkey>
      <pwccode url="https://github.com/digling/edictor" additional="false">digling/edictor</pwccode>
    </paper>
    <paper id="4">
      <title><fixed-case>WAT</fixed-case>-<fixed-case>SL</fixed-case>: A Customizable Web Annotation Tool for Segment Labeling</title>
      <author><first>Johannes</first><last>Kiesel</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Khalid</first><last>Al-Khatib</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>13–16</pages>
      <url hash="08698d8a">E17-3004</url>
      <abstract>A frequent type of annotations in text corpora are labeled text segments. General-purpose annotation tools tend to be overly comprehensive, often making the annotation process slower and more error-prone. We present WAT-SL, a new web-based tool that is dedicated to segment labeling and highly customizable to the labeling task at hand. We outline its main features and exemplify how we used it for a crowdsourced corpus with labeled argument units.</abstract>
      <bibkey>kiesel-etal-2017-wat</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>T</fixed-case>ext<fixed-case>I</fixed-case>mager as a Generic Interface to <fixed-case>R</fixed-case></title>
      <author><first>Tolga</first><last>Uslu</last></author>
      <author><first>Wahed</first><last>Hemati</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <author><first>Daniel</first><last>Baumartz</last></author>
      <pages>17–20</pages>
      <url hash="02585ffb">E17-3005</url>
      <abstract>R is a very powerful framework for statistical modeling. Thus, it is of high importance to integrate R with state-of-the-art tools in NLP. In this paper, we present the functionality and architecture of such an integration by means of TextImager. We use the OpenCPU API to integrate R based on our own R-Server. This allows for communicating with R-packages and combining them with TextImager’s NLP-components.</abstract>
      <bibkey>uslu-etal-2017-textimager</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>G</fixed-case>ra<fixed-case>W</fixed-case>i<fixed-case>T</fixed-case>as: a Grammar-based <fixed-case>W</fixed-case>ikipedia Talk Page Parser</title>
      <author><first>Benjamin</first><last>Cabrera</last></author>
      <author><first>Laura</first><last>Steinert</last></author>
      <author><first>Björn</first><last>Ross</last></author>
      <pages>21–24</pages>
      <url hash="9a9c16ef">E17-3006</url>
      <abstract>Wikipedia offers researchers unique insights into the collaboration and communication patterns of a large self-regulating community of editors. The main medium of direct communication between editors of an article is the article’s talk page. However, a talk page file is unstructured and therefore difficult to analyse automatically. A few parsers exist that enable its transformation into a structured data format. However, they are rarely open source, support only a limited subset of the talk page syntax – resulting in the loss of content – and usually support only one export format. Together with this article we offer a very fast, lightweight, open source parser with support for various output formats. In a preliminary evaluation it achieved a high accuracy. The parser uses a grammar-based approach – offering a transparent implementation and easy extensibility.</abstract>
      <bibkey>cabrera-etal-2017-grawitas</bibkey>
      <pwccode url="https://github.com/ace7k3/grawitas" additional="false">ace7k3/grawitas</pwccode>
    </paper>
    <paper id="7">
      <title><fixed-case>TWINE</fixed-case>: A real-time system for <fixed-case>TW</fixed-case>eet analysis via <fixed-case>IN</fixed-case>formation Extraction</title>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Fausto</first><last>Ristagno</last></author>
      <author><first>Matteo</first><last>Palmonari</last></author>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <author><first>Pikakshi</first><last>Manchanda</last></author>
      <author><first>Enza</first><last>Messina</last></author>
      <pages>25–28</pages>
      <url hash="fbd32e9e">E17-3007</url>
      <abstract>In the recent years, the amount of user generated contents shared on the Web has significantly increased, especially in social media environment, e.g. Twitter, Facebook, Google+. This large quantity of data has generated the need of reactive and sophisticated systems for capturing and understanding the underlying information enclosed in them. In this paper we present TWINE, a real-time system for the big data analysis and exploration of information extracted from Twitter streams. The proposed system based on a Named Entity Recognition and Linking pipeline and a multi-dimensional spatial geo-localization is managed by a scalable and flexible architecture for an interactive visualization of micropost streams insights. The demo is available at <url>http://twine-mind.cloudapp.net/streaming</url>.</abstract>
      <bibkey>nozza-etal-2017-twine</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ipm-nel">IPM NEL</pwcdataset>
    </paper>
    <paper id="8">
      <title><fixed-case>A</fixed-case>lto: Rapid Prototyping for Parsing and Translation</title>
      <author><first>Johannes</first><last>Gontrum</last></author>
      <author><first>Jonas</first><last>Groschwitz</last></author>
      <author><first>Alexander</first><last>Koller</last></author>
      <author><first>Christoph</first><last>Teichmann</last></author>
      <pages>29–32</pages>
      <url hash="f348ca97">E17-3008</url>
      <abstract>We present Alto, a rapid prototyping tool for new grammar formalisms. Alto implements generic but efficient algorithms for parsing, translation, and training for a range of monolingual and synchronous grammar formalisms. It can easily be extended to new formalisms, which makes all of these algorithms immediately available for the new formalism.</abstract>
      <bibkey>gontrum-etal-2017-alto</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>CASSANDRA</fixed-case>: A multipurpose configurable voice-enabled human-computer-interface</title>
      <author><first>Tiberiu</first><last>Boros</last></author>
      <author><first>Stefan Daniel</first><last>Dumitrescu</last></author>
      <author><first>Sonia</first><last>Pipa</last></author>
      <pages>33–36</pages>
      <url hash="f53bafdb">E17-3009</url>
      <abstract>Voice enabled human computer interfaces (HCI) that integrate automatic speech recognition, text-to-speech synthesis and natural language understanding have become a commodity, introduced by the immersion of smart phones and other gadgets in our daily lives. Smart assistants are able to respond to simple queries (similar to text-based question-answering systems), perform simple tasks (call a number, reject a call etc.) and help organizing appointments. With this paper we introduce a newly created process automation platform that enables the user to control applications and home appliances and to query the system for information using a natural voice interface. We offer an overview of the technologies that enabled us to construct our system and we present different usage scenarios in home and office environments.</abstract>
      <bibkey>boros-etal-2017-cassandra</bibkey>
    </paper>
    <paper id="10">
      <title>An Extensible Framework for Verification of Numerical Claims</title>
      <author><first>James</first><last>Thorne</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>37–40</pages>
      <url hash="89edd9cd">E17-3010</url>
      <abstract>In this paper we present our automated fact checking system demonstration which we developed in order to participate in the Fast and Furious Fact Check challenge. We focused on simple numerical claims such as “population of Germany in 2015 was 80 million” which comprised a quarter of the test instances in the challenge, achieving 68% accuracy. Our system extends previous work on semantic parsing and claim identification to handle temporal expressions and knowledge bases consisting of multiple tables, while relying solely on automatically generated training data. We demonstrate the extensible nature of our system by evaluating it on relations used in previous work. We make our system publicly available so that it can be used and extended by the community.</abstract>
      <bibkey>thorne-vlachos-2017-extensible</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>AD</fixed-case>o<fixed-case>CS</fixed-case>: Automatic Designer of Conference Schedules</title>
      <author><first>Diego Fernando</first><last>Vallejo Huanga</last></author>
      <author><first>Paulina Adriana</first><last>Morillo Alcívar</last></author>
      <author><first>Cèsar</first><last>Ferri Ramírez</last></author>
      <pages>41–44</pages>
      <url hash="6df8f90b">E17-3011</url>
      <abstract>Distributing papers into sessions in scientific conferences is a task consisting in grouping papers with common topics and considering the size restrictions imposed by the conference schedule. This problem can be seen as a semi-supervised clustering of scientific papers based on their features. This paper presents a web tool called ADoCS that solves the problem of configuring conference schedules by an automatic clustering of articles by similarity using a new algorithm considering size constraints.</abstract>
      <bibkey>vallejo-huanga-etal-2017-adocs</bibkey>
    </paper>
    <paper id="12">
      <title>A Web Interface for Diachronic Semantic Search in <fixed-case>S</fixed-case>panish</title>
      <author><first>Pablo</first><last>Gamallo</last></author>
      <author><first>Iván</first><last>Rodríguez-Torres</last></author>
      <author><first>Marcos</first><last>Garcia</last></author>
      <pages>45–48</pages>
      <url hash="bc4168db">E17-3012</url>
      <abstract>This article describes a semantic system which is based on distributional models obtained from a chronologically structured language resource, namely Google Books Syntactic Ngrams.The models were created using dependency-based contexts and a strategy for reducing the vector space, which consists in selecting the more informative and relevant word contexts. The system allowslinguists to analize meaning change of Spanish words in the written language across time.</abstract>
      <bibkey>gamallo-etal-2017-web</bibkey>
    </paper>
    <paper id="13">
      <title>Multilingual <fixed-case>CALL</fixed-case> Framework for Automatic Language Exercise Generation from Free Text</title>
      <author><first>Naiara</first><last>Perez</last></author>
      <author><first>Montse</first><last>Cuadros</last></author>
      <pages>49–52</pages>
      <url hash="d82f7845">E17-3013</url>
      <abstract>This paper describes a web-based application to design and answer exercises for language learning. It is available in Basque, Spanish, English, and French. Based on open-source Natural Language Processing (NLP) technology such as word embedding models and word sense disambiguation, the application enables users to automatic create easily and in real time three types of exercises, namely, Fill-in-the-Gaps, Multiple Choice, and Shuffled Sentences questionnaires. These are generated from texts of the users’ own choice, so they can train their language skills with content of their particular interest.</abstract>
      <bibkey>perez-cuadros-2017-multilingual</bibkey>
    </paper>
    <paper id="14">
      <title>Audience Segmentation in Social Media</title>
      <author><first>Verena</first><last>Henrich</last></author>
      <author><first>Alexander</first><last>Lang</last></author>
      <pages>53–56</pages>
      <url hash="a04de36e">E17-3014</url>
      <abstract>Understanding the social media audience is becoming increasingly important for social media analysis. This paper presents an approach that detects various audience attributes, including author location, demographics, behavior and interests. It works both for a variety of social media sources and for multiple languages. The approach has been implemented within IBM Watson Analytics for Social Media and creates author profiles for more than 300 different analysis domains every day.</abstract>
      <bibkey>henrich-lang-2017-audience</bibkey>
    </paper>
    <paper id="15">
      <title>The ar<fixed-case>T</fixed-case>ext prototype: An automatic system for writing specialized texts</title>
      <author><first>Iria</first><last>da Cunha</last></author>
      <author><first>M. Amor</first><last>Montané</last></author>
      <author><first>Luis</first><last>Hysa</last></author>
      <pages>57–60</pages>
      <url hash="df4743e6">E17-3015</url>
      <abstract>This article describes an automatic system for writing specialized texts in Spanish. The arText prototype is a free online text editor that includes different types of linguistic information. It is designed for a variety of end users and domains, including specialists and university students working in the fields of medicine and tourism, and laypersons writing to the public administration. ArText provides guidance on how to structure a text, prompts users to include all necessary contents in each section, and detects lexical and discourse problems in the text.</abstract>
      <bibkey>da-cunha-etal-2017-artext</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>QCRI</fixed-case> Live Speech Translation System</title>
      <author><first>Fahim</first><last>Dalvi</last></author>
      <author><first>Yifan</first><last>Zhang</last></author>
      <author><first>Sameer</first><last>Khurana</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Ahmed</first><last>Ali</last></author>
      <author><first>Stephan</first><last>Vogel</last></author>
      <pages>61–64</pages>
      <url hash="735856f5">E17-3016</url>
      <abstract>This paper presents QCRI’s Arabic-to-English live speech translation system. It features modern web technologies to capture live audio, and broadcasts Arabic transcriptions and English translations simultaneously. Our Kaldi-based ASR system uses the Time Delay Neural Network (TDNN) architecture, while our Machine Translation (MT) system uses both phrase-based and neural frameworks. Although our neural MT system is slower than the phrase-based system, it produces significantly better translations and is memory efficient. The demo is available at <url>https://st.qcri.org/demos/livetranslation</url>.</abstract>
      <bibkey>dalvi-etal-2017-qcri</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>N</fixed-case>ematus: a Toolkit for Neural Machine Translation</title>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Julian</first><last>Hitschler</last></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <author><first>Samuel</first><last>Läubli</last></author>
      <author><first>Antonio Valerio</first><last>Miceli Barone</last></author>
      <author><first>Jozef</first><last>Mokry</last></author>
      <author><first>Maria</first><last>Nădejde</last></author>
      <pages>65–68</pages>
      <url hash="488e8266">E17-3017</url>
      <abstract>We present Nematus, a toolkit for Neural Machine Translation. The toolkit prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions to shared translation tasks at WMT and IWSLT, and has been used to train systems for production environments.</abstract>
      <bibkey>sennrich-etal-2017-nematus</bibkey>
      <pwccode url="https://github.com/rsennrich/nematus" additional="true">rsennrich/nematus</pwccode>
    </paper>
    <paper id="18">
      <title>A tool for extracting sense-disambiguated example sentences through user feedback</title>
      <author><first>Beto</first><last>Boullosa</last></author>
      <author><first>Richard</first><last>Eckart de Castilho</last></author>
      <author><first>Alexander</first><last>Geyken</last></author>
      <author><first>Lothar</first><last>Lemnitzer</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>69–72</pages>
      <url hash="dc98b9f1">E17-3018</url>
      <abstract>This paper describes an application system aimed to help lexicographers in the extraction of example sentences for a given headword based on its different senses. The tool uses classification and clustering methods and incorporates user feedback to refine its results.</abstract>
      <bibkey>boullosa-etal-2017-tool</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>L</fixed-case>ingmotif: Sentiment Analysis for the Digital Humanities</title>
      <author><first>Antonio</first><last>Moreno-Ortiz</last></author>
      <pages>73–76</pages>
      <url hash="a49328f7">E17-3019</url>
      <abstract>Lingmotif is a lexicon-based, linguistically-motivated, user-friendly, GUI-enabled, multi-platform, Sentiment Analysis desktop application. Lingmotif can perform SA on any type of input texts, regardless of their length and topic. The analysis is based on the identification of sentiment-laden words and phrases contained in the application’s rich core lexicons, and employs context rules to account for sentiment shifters. It offers easy-to-interpret visual representations of quantitative data (text polarity, sentiment intensity, sentiment profile), as well as a detailed, qualitative analysis of the text in terms of its sentiment. Lingmotif can also take user-provided plugin lexicons in order to account for domain-specific sentiment expression. Lingmotif currently analyzes English and Spanish texts.</abstract>
      <bibkey>moreno-ortiz-2017-lingmotif</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>RAMBLE</fixed-case> <fixed-case>ON</fixed-case>: Tracing Movements of Popular Historical Figures</title>
      <author><first>Stefano</first><last>Menini</last></author>
      <author><first>Rachele</first><last>Sprugnoli</last></author>
      <author><first>Giovanni</first><last>Moretti</last></author>
      <author><first>Enrico</first><last>Bignotti</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <author><first>Bruno</first><last>Lepri</last></author>
      <pages>77–80</pages>
      <url hash="de6455ee">E17-3020</url>
      <abstract>We present RAMBLE ON, an application integrating a pipeline for frame-based information extraction and an interface to track and display movement trajectories. The code of the extraction pipeline and a navigator are freely available; moreover we display in a demonstrator the outcome of a case study carried out on trajectories of notable persons of the XX Century.</abstract>
      <bibkey>menini-etal-2017-ramble</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="21">
      <title><fixed-case>A</fixed-case>utobank: a semi-automatic annotation tool for developing deep <fixed-case>M</fixed-case>inimalist <fixed-case>G</fixed-case>rammar treebanks</title>
      <author><first>John</first><last>Torr</last></author>
      <pages>81–86</pages>
      <url hash="1bb2d845">E17-3021</url>
      <abstract>This paper presents Autobank, a prototype tool for constructing a wide-coverage Minimalist Grammar (MG) (Stabler 1997), and semi-automatically converting the Penn Treebank (PTB) into a deep Minimalist treebank. The front end of the tool is a graphical user interface which facilitates the rapid development of a seed set of MG trees via manual reannotation of PTB preterminals with MG lexical categories. The system then extracts various dependency mappings between the source and target trees, and uses these in concert with a non-statistical MG parser to automatically reannotate the rest of the corpus. Autobank thus enables deep treebank conversions (and subsequent modifications) without the need for complex transduction algorithms accompanied by cascades of ad hoc rules; instead, the locus of human effort falls directly on the task of grammar construction itself.</abstract>
      <bibkey>torr-2017-autobank</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="22">
      <title>Chatbot with a Discourse Structure-Driven Dialogue Management</title>
      <author><first>Boris</first><last>Galitsky</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <pages>87–90</pages>
      <url hash="3ccb3c58">E17-3022</url>
      <abstract>We build a chat bot with iterative content exploration that leads a user through a personalized knowledge acquisition session. The chat bot is designed as an automated customer support or product recommendation agent assisting a user in learning product features, product usability, suitability, troubleshooting and other related tasks. To control the user navigation through content, we extend the notion of a linguistic discourse tree (DT) towards a set of documents with multiple sections covering a topic. For a given paragraph, a DT is built by DT parsers. We then combine DTs for the paragraphs of documents to form what we call extended DT, which is a basis for interactive content exploration facilitated by the chat bot. To provide cohesive answers, we use a measure of rhetoric agreement between a question and an answer by tree kernel learning of their DTs.</abstract>
      <bibkey>galitsky-ilvovsky-2017-chatbot</bibkey>
    </paper>
    <paper id="23">
      <title>Marine Variable Linker: Exploring Relations between Changing Variables in Marine Science Literature</title>
      <author><first>Erwin</first><last>Marsi</last></author>
      <author><first>Pinar</first><last>Pinar Øzturk</last></author>
      <author><first>Murat</first><last>V. Ardelan</last></author>
      <pages>91–94</pages>
      <url hash="c909a91f">E17-3023</url>
      <abstract>We report on a demonstration system for text mining of literature in marine science and related disciplines. It automatically extracts variables (“CO2”) involved in events of change/increase/decrease (“increasing CO2”), as well as co-occurrence and causal relations among these events (“increasing CO2 causes a decrease in pH in seawater”), resulting in a big knowledge graph. A web-based graphical user interface targeted at marine scientists facilitates searching, browsing and visualising events and their relations in an interactive way.</abstract>
      <bibkey>marsi-etal-2017-marine</bibkey>
    </paper>
    <paper id="24">
      <title>Neoveille, a Web Platform for Neologism Tracking</title>
      <author><first>Emmanuel</first><last>Cartier</last></author>
      <pages>95–98</pages>
      <url hash="9726c758">E17-3024</url>
      <abstract>This paper details a software designed to track neologisms in seven languages through newspapers monitor corpora. The platform combines state-of-the-art processes to track linguistic changes and a web platform for linguists to create and manage their corpora, accept or reject automatically identified neologisms, describe linguistically the accepted neologisms and follow their lifecycle on the monitor corpora. In the following, after a short state-of-the-art in Neologism Retrieval, Analysis and Life-tracking, we describe the overall architecture of the system. The platform can be freely browsed at <url>www.neoveille.org</url> where detailed presentation is given.  Access to the editing modules is available upon request.</abstract>
      <bibkey>cartier-2017-neoveille</bibkey>
    </paper>
    <paper id="25">
      <title>Building Web-Interfaces for Vector Semantic Models with the <fixed-case>W</fixed-case>eb<fixed-case>V</fixed-case>ectors Toolkit</title>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Elizaveta</first><last>Kuzmenko</last></author>
      <pages>99–103</pages>
      <url hash="cf370d6b">E17-3025</url>
      <abstract>In this demo we present WebVectors, a free and open-source toolkit helping to deploy web services which demonstrate and visualize distributional semantic models (widely known as word embeddings). WebVectors can be useful in a very common situation when one has trained a distributional semantics model for one’s particular corpus or language (tools for this are now widespread and simple to use), but then there is a need to demonstrate the results to general public over the Web. We show its abilities on the example of the living web services featuring distributional models for English, Norwegian and Russian.</abstract>
      <bibkey>kutuzov-kuzmenko-2017-building</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>I</fixed-case>n<fixed-case>T</fixed-case>o<fixed-case>E</fixed-case>vent<fixed-case>S</fixed-case>: An Interactive Toolkit for Discovering and Building Event Schemas</title>
      <author><first>Germán</first><last>Ferrero</last></author>
      <author><first>Audi</first><last>Primadhanty</last></author>
      <author><first>Ariadna</first><last>Quattoni</last></author>
      <pages>104–107</pages>
      <url hash="88765cca">E17-3026</url>
      <abstract>Event Schema Induction is the task of learning a representation of events (e.g., bombing) and the roles involved in them (e.g, victim and perpetrator). This paper presents InToEventS, an interactive tool for learning these schemas. InToEventS allows users to explore a corpus and discover which kind of events are present. We show how users can create useful event schemas using two interactive clustering steps.</abstract>
      <bibkey>ferrero-etal-2017-intoevents</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>ICE</fixed-case>: Idiom and Collocation Extractor for Research and Education</title>
      <author><first>Vasanthi</first><last>Vuppuluri</last></author>
      <author><first>Shahryar</first><last>Baki</last></author>
      <author><first>An</first><last>Nguyen</last></author>
      <author><first>Rakesh</first><last>Verma</last></author>
      <pages>108–111</pages>
      <url hash="f32aa0e5">E17-3027</url>
      <abstract>Collocation and idiom extraction are well-known challenges with many potential applications in Natural Language Processing (NLP). Our experimental, open-source software system, called ICE, is a python package for flexibly extracting collocations and idioms, currently in English. It also has a competitive POS tagger that can be used alone or as part of collocation/idiom extraction. ICE is available free of cost for research and educational uses in two user-friendly formats. This paper gives an overview of ICE and its performance, and briefly describes the research underlying the extraction algorithms.</abstract>
      <bibkey>vuppuluri-etal-2017-ice</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>B</fixed-case>ib2vec: Embedding-based Search System for Bibliographic Information</title>
      <author><first>Takuma</first><last>Yoneda</last></author>
      <author><first>Koki</first><last>Mori</last></author>
      <author><first>Makoto</first><last>Miwa</last></author>
      <author><first>Yutaka</first><last>Sasaki</last></author>
      <pages>112–115</pages>
      <url hash="5bdceb62">E17-3028</url>
      <abstract>We propose a novel embedding model that represents relationships among several elements in bibliographic information with high representation ability and flexibility. Based on this model, we present a novel search system that shows the relationships among the elements in the ACL Anthology Reference Corpus. The evaluation results show that our model can achieve a high prediction ability and produce reasonable search results.</abstract>
      <bibkey>yoneda-etal-2017-bib2vec</bibkey>
    </paper>
    <paper id="29">
      <title>The <fixed-case>SUMMA</fixed-case> Platform Prototype</title>
      <author><first>Renars</first><last>Liepins</last></author>
      <author><first>Ulrich</first><last>Germann</last></author>
      <author><first>Guntis</first><last>Barzdins</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <author><first>Steve</first><last>Renals</last></author>
      <author><first>Susanne</first><last>Weber</last></author>
      <author><first>Peggy</first><last>van der Kreeft</last></author>
      <author><first>Hervé</first><last>Bourlard</last></author>
      <author><first>João</first><last>Prieto</last></author>
      <author><first>Ondřej</first><last>Klejch</last></author>
      <author><first>Peter</first><last>Bell</last></author>
      <author><first>Alexandros</first><last>Lazaridis</last></author>
      <author><first>Alfonso</first><last>Mendes</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Mariana S. C.</first><last>Almeida</last></author>
      <author><first>Pedro</first><last>Balage</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <author><first>Tomasz</first><last>Dwojak</last></author>
      <author><first>Philip N.</first><last>Garner</last></author>
      <author><first>Andreas</first><last>Giefer</last></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <author><first>Hina</first><last>Imran</last></author>
      <author><first>David</first><last>Nogueira</last></author>
      <author><first>Ahmed</first><last>Ali</last></author>
      <author><first>Sebastião</first><last>Miranda</last></author>
      <author><first>Andrei</first><last>Popescu-Belis</last></author>
      <author><first>Lesly</first><last>Miculicich Werlen</last></author>
      <author><first>Nikos</first><last>Papasarantopoulos</last></author>
      <author><first>Abiola</first><last>Obamuyide</last></author>
      <author><first>Clive</first><last>Jones</last></author>
      <author><first>Fahim</first><last>Dalvi</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <author><first>Yang</first><last>Wang</last></author>
      <author><first>Sibo</first><last>Tong</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Nikolaos</first><last>Pappas</last></author>
      <author><first>Shashi</first><last>Narayan</last></author>
      <author><first>Marco</first><last>Damonte</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Sameer</first><last>Khurana</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <author><first>Stephan</first><last>Vogel</last></author>
      <author><first>David</first><last>Sheppey</last></author>
      <author><first>Chris</first><last>Hernon</last></author>
      <author><first>Jeff</first><last>Mitchell</last></author>
      <pages>116–119</pages>
      <url hash="e12a5730">E17-3029</url>
      <abstract>We present the first prototype of the SUMMA Platform: an integrated platform for multilingual media monitoring. The platform contains a rich suite of low-level and high-level natural language processing technologies: automatic speech recognition of broadcast media, machine translation, automated tagging and classification of named entities, semantic parsing to detect relationships between entities, and automatic construction / augmentation of factual knowledge bases. Implemented on the Docker platform, it can easily be deployed, customised, and scaled to large volumes of incoming media streams.</abstract>
      <bibkey>liepins-etal-2017-summa</bibkey>
    </paper>
  </volume>
  <volume id="4">
    <meta>
      <booktitle>Proceedings of the Student Research Workshop at the 15th Conference of the <fixed-case>E</fixed-case>uropean Chapter of the Association for Computational Linguistics</booktitle>
      <url hash="9a8c60e3">E17-4</url>
      <editor><first>Florian</first><last>Kunneman</last></editor>
      <editor><first>Uxoa</first><last>Iñurrieta</last></editor>
      <editor><first>John J.</first><last>Camilleri</last></editor>
      <editor><first>Mariona Coll</first><last>Ardanuy</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Valencia, Spain</address>
      <month>April</month>
      <year>2017</year>
      <venue>eacl</venue>
    </meta>
    <frontmatter>
      <url hash="bce4b098">E17-4000</url>
      <bibkey>eacl-2017-student</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Pragmatic descriptions of perceptual stimuli</title>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <pages>1–10</pages>
      <url hash="6745c0be">E17-4001</url>
      <abstract>This research proposal discusses pragmatic factors in image description, arguing that current automatic image description systems do not take these factors into account. I present a general model of the human image description process, and propose to study this process using corpus analysis, experiments, and computational modeling. This will lead to a better characterization of human image description behavior, providing a road map for future research in automatic image description, and the automatic description of perceptual stimuli in general.</abstract>
      <bibkey>van-miltenburg-2017-pragmatic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="2">
      <title>Detecting spelling variants in non-standard texts</title>
      <author><first>Fabian</first><last>Barteld</last></author>
      <pages>11–22</pages>
      <url hash="3fac01db">E17-4002</url>
      <abstract>Spelling variation in non-standard language, e.g. computer-mediated communication and historical texts, is usually treated as a deviation from a standard spelling, e.g. 2mr as an non-standard spelling for tomorrow. Consequently, in normalization – the standard approach of dealing with spelling variation – so-called non-standard words are mapped to their corresponding standard words. However, there is not always a corresponding standard word. This can be the case for single types (like emoticons in computer-mediated communication) or a complete language, e.g. texts from historical languages that did not develop to a standard variety. The approach presented in this thesis proposal deals with spelling variation in absence of reference to a standard. The task is to detect pairs of types that are variants of the same morphological word. An approach for spelling-variant detection is presented, where pairs of potential spelling variants are generated with Levenshtein distance and subsequently filtered by supervised machine learning. The approach is evaluated on historical Low German texts. Finally, further perspectives are discussed.</abstract>
      <bibkey>barteld-2017-detecting</bibkey>
    </paper>
    <paper id="3">
      <title>Replication issues in syntax-based aspect extraction for opinion mining</title>
      <author><first>Edison</first><last>Marrese-Taylor</last></author>
      <author><first>Yutaka</first><last>Matsuo</last></author>
      <pages>23–32</pages>
      <url hash="2959493d">E17-4003</url>
      <abstract>Reproducing experiments is an important instrument to validate previous work and build upon existing approaches. It has been tackled numerous times in different areas of science. In this paper, we introduce an empirical replicability study of three well-known algorithms for syntactic centric aspect-based opinion mining. We show that reproducing results continues to be a difficult endeavor, mainly due to the lack of details regarding preprocessing and parameter setting, as well as due to the absence of available implementations that clarify these details. We consider these are important threats to validity of the research on the field, specifically when compared to other problems in NLP where public datasets and code availability are critical validity components. We conclude by encouraging code-based research, which we think has a key role in helping researchers to understand the meaning of the state-of-the-art better and to generate continuous advances.</abstract>
      <bibkey>marrese-taylor-matsuo-2017-replication</bibkey>
      <pwccode url="https://github.com/epochx/opminreplicability" additional="false">epochx/opminreplicability</pwccode>
    </paper>
    <paper id="4">
      <title>Discourse Relations and Conjoined <fixed-case>VP</fixed-case>s: Automated Sense Recognition</title>
      <author><first>Valentina</first><last>Pyatkin</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <pages>33–42</pages>
      <url hash="7f3904e3">E17-4004</url>
      <abstract>Sense classification of discourse relations is a sub-task of shallow discourse parsing. Discourse relations can occur both across sentences (<i>inter-sentential</i>) and within sentences (<i>intra-sentential</i>), and more than one discourse relation can hold between the same units. Using a newly available corpus of discourse-annotated intra-sentential conjoined verb phrases, we demonstrate a sequential classification pipeline for their multi-label sense classification. We assess the importance of each feature used in the classification, the feature scope, and what is lost in moving from gold standard manual parses to the output of an off-the-shelf parser.</abstract>
      <bibkey>pyatkin-webber-2017-discourse</bibkey>
    </paper>
    <paper id="5">
      <title>Deception detection in <fixed-case>R</fixed-case>ussian texts</title>
      <author><first>Olga</first><last>Litvinova</last></author>
      <author><first>Pavel</first><last>Seredin</last></author>
      <author><first>Tatiana</first><last>Litvinova</last></author>
      <author><first>John</first><last>Lyell</last></author>
      <pages>43–52</pages>
      <url hash="0edda1b0">E17-4005</url>
      <abstract>Humans are known to detect deception in speech randomly and it is therefore important to develop tools to enable them to detect deception. The problem of deception detection has been studied for a significant amount of time, however the last 10-15 years have seen methods of computational linguistics being employed. Texts are processed using different NLP tools and then classified as deceptive/truthful using machine learning methods. While most research has been performed for English, Slavic languages have never been a focus of detection deception studies. The paper deals with deception detection in Russian narratives. It employs a specially designed corpus of truthful and deceptive texts on the same topic from each respondent, N = 113. The texts were processed using Linguistic Inquiry and Word Count software that is used in most studies of text-based deception detection. The list of parameters computed using the software was expanded due to the designed users’ dictionaries. A variety of text classification methods was employed. The accuracy of the model was found to depend on the author’s gender and text type (deceptive/truthful).</abstract>
      <bibkey>litvinova-etal-2017-deception</bibkey>
    </paper>
    <paper id="6">
      <title>A Computational Model of Human Preferences for Pronoun Resolution</title>
      <author><first>Olga</first><last>Seminck</last></author>
      <author><first>Pascal</first><last>Amsili</last></author>
      <pages>53–63</pages>
      <url hash="d782b028">E17-4006</url>
      <abstract>We present a cognitive computational model of pronoun resolution that reproduces the human interpretation preferences of the Subject Assignment Strategy and the Parallel Function Strategy. Our model relies on a probabilistic pronoun resolution system trained on corpus data. Factors influencing pronoun resolution are represented as features weighted by their relative importance. The importance the model gives to the preferences is in line with psycholinguistic studies. We demonstrate the cognitive plausibility of the model by running it on experimental items and simulating antecedent choice and reading times of human participants. Our model can be used as a new means to study pronoun resolution, because it captures the interaction of preferences.</abstract>
      <bibkey>seminck-amsili-2017-computational</bibkey>
    </paper>
    <paper id="7">
      <title>Automatic Extraction of News Values from Headline Text</title>
      <author><first>Alicja</first><last>Piotrkowicz</last></author>
      <author><first>Vania</first><last>Dimitrova</last></author>
      <author><first>Katja</first><last>Markert</last></author>
      <pages>64–74</pages>
      <url hash="b29fc8dd">E17-4007</url>
      <abstract>Headlines play a crucial role in attracting audiences’ attention to online artefacts (e.g. news articles, videos, blogs). The ability to carry out an automatic, large-scale analysis of headlines is critical to facilitate the selection and prioritisation of a large volume of digital content. In journalism studies news content has been extensively studied using manually annotated news values - factors used implicitly and explicitly when making decisions on the selection and prioritisation of news items. This paper presents the first attempt at a fully automatic extraction of news values from headline text. The news values extraction methods are applied on a large headlines corpus collected from The Guardian, and evaluated by comparing it with a manually annotated gold standard. A crowdsourcing survey indicates that news values affect people’s decisions to click on a headline, supporting the need for an automatic news values detection.</abstract>
      <bibkey>piotrkowicz-etal-2017-automatic</bibkey>
    </paper>
    <paper id="8">
      <title>Assessing Convincingness of Arguments in Online Debates with Limited Number of Features</title>
      <author><first>Lisa Andreevna</first><last>Chalaguine</last></author>
      <author><first>Claudia</first><last>Schulz</last></author>
      <pages>75–83</pages>
      <url hash="3ef8cb0a">E17-4008</url>
      <abstract>We propose a new method in the field of argument analysis in social media to determining convincingness of arguments in online debates, following previous research by Habernal and Gurevych (2016). Rather than using argument specific feature values, we measure feature values relative to the average value in the debate, allowing us to determine argument convincingness with fewer features (between 5 and 35) than normally used for natural language processing tasks. We use a simple forward-feeding neural network for this task and achieve an accuracy of 0.77 which is comparable to the accuracy obtained using 64k features and a support vector machine by Habernal and Gurevych.</abstract>
      <bibkey>chalaguine-schulz-2017-assessing</bibkey>
      <pwccode url="https://github.com/lisanka93/individualProject" additional="false">lisanka93/individualProject</pwccode>
    </paper>
    <paper id="9">
      <title><fixed-case>Z</fixed-case>ipf’s and <fixed-case>B</fixed-case>enford’s laws in <fixed-case>T</fixed-case>witter hashtags</title>
      <author><first>José Alberto</first><last>Pérez Melián</last></author>
      <author><first>J. Alberto</first><last>Conejero</last></author>
      <author><first>Cèsar</first><last>Ferri Ramírez</last></author>
      <pages>84–93</pages>
      <url hash="6c0d2935">E17-4009</url>
      <abstract>Social networks have transformed communication dramatically in recent years through the rise of new platforms and the development of a new language of communication. This landscape requires new forms to describe and predict the behaviour of users in networks. This paper presents an analysis of the frequency distribution of hashtag popularity in Twitter conversations. Our objective is to determine if these frequency distribution follow some well-known frequency distribution that many real-life sets of numerical data satisfy. In particular, we study the similarity of frequency distribution of hashtag popularity with respect to Zipf’s law, an empirical law referring to the phenomenon that many types of data in social sciences can be approximated with a Zipfian distribution. Additionally, we also analyse Benford’s law, is a special case of Zipf’s law, a common pattern about the frequency distribution of leading digits. In order to compute correctly the frequency distribution of hashtag popularity, we need to correct many spelling errors that Twitter’s users introduce. For this purpose we introduce a new filter to correct hashtag mistake based on string distances. The experiments obtained employing datasets of Twitter streams generated under controlled conditions show that Benford’s law and Zipf’s law can be used to model hashtag frequency distribution.</abstract>
      <bibkey>perez-melian-etal-2017-zipfs</bibkey>
    </paper>
    <paper id="10">
      <title>A Multi-aspect Analysis of Automatic Essay Scoring for <fixed-case>B</fixed-case>razilian <fixed-case>P</fixed-case>ortuguese</title>
      <author><first>Evelin</first><last>Amorim</last></author>
      <author><first>Adriano</first><last>Veloso</last></author>
      <pages>94–102</pages>
      <url hash="f7bae8b5">E17-4010</url>
      <abstract>Several methods for automatic essay scoring (AES) for English language have been proposed. However, multi-aspect AES systems for other languages are unusual. Therefore, we propose a multi-aspect AES system to apply on a dataset of Brazilian Portuguese essays, which human experts evaluated according to five aspects defined by Brazilian Government to the National Exam to High School Student (ENEM). These aspects are skills that student must master and every skill is assessed apart from each other. Besides the prediction of each aspect, the feature analysis also was performed for each aspect. The AES system proposed employs several features already employed by AES systems for English language. Our results show that predictions for some aspects performed well with the features we employed, while predictions for other aspects performed poorly. Also, it is possible to note the difference between the five aspects in the detailed feature analysis we performed. Besides these contributions, the eight millions of enrollments every year for ENEM raise some challenge issues for future directions in our research.</abstract>
      <bibkey>amorim-veloso-2017-multi</bibkey>
    </paper>
    <paper id="11">
      <title>Literal or idiomatic? Identifying the reading of single occurrences of <fixed-case>G</fixed-case>erman multiword expressions using word embeddings</title>
      <author><first>Rafael</first><last>Ehren</last></author>
      <pages>103–112</pages>
      <url hash="1b088e88">E17-4011</url>
      <abstract>Non-compositional multiword expressions (MWEs) still pose serious issues for a variety of natural language processing tasks and their ubiquity makes it impossible to get around methods which automatically identify these kind of MWEs. The method presented in this paper was inspired by Sporleder and Li (2009) and is able to discriminate between the literal and non-literal use of an MWE in an unsupervised way. It is based on the assumption that words in a text form cohesive units. If the cohesion of these units is weakened by an expression, it is classified as literal, and otherwise as idiomatic. While Sporleder an Li used <i>Normalized Google Distance</i> to modell semantic similarity, the present work examines the use of a variety of different word embeddings.</abstract>
      <bibkey>ehren-2017-literal</bibkey>
    </paper>
    <paper id="12">
      <title>Evaluating the Reliability and Interaction of Recursively Used Feature Classes for Terminology Extraction</title>
      <author><first>Anna</first><last>Hätty</last></author>
      <author><first>Michael</first><last>Dorna</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>113–121</pages>
      <url hash="d198457e">E17-4012</url>
      <abstract>Feature design and selection is a crucial aspect when treating terminology extraction as a machine learning classification problem. We designed feature classes which characterize different properties of terms based on distributions, and propose a new feature class for components of term candidates. By using random forests, we infer optimal features which are later used to build decision tree classifiers. We evaluate our method using the ACL RD-TEC dataset. We demonstrate the importance of the novel feature class for downgrading termhood which exploits properties of term components. Furthermore, our classification suggests that the identification of reliable term candidates should be performed successively, rather than just once.</abstract>
      <bibkey>hatty-etal-2017-evaluating</bibkey>
    </paper>
  </volume>
  <volume id="5">
    <meta>
      <booktitle>Proceedings of the 15th Conference of the <fixed-case>E</fixed-case>uropean Chapter of the Association for Computational Linguistics: Tutorial Abstracts</booktitle>
      <editor><first>Alexandre</first><last>Klementiev</last></editor>
      <editor><first>Lucia</first><last>Specia</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Valencia, Spain</address>
      <month>April</month>
      <year>2017</year>
      <venue>eacl</venue>
    </meta>
    <paper id="1">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Joakim</first><last>Nivre</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <url hash="4dacb20e">E17-5001</url>
      <abstract>Universal Dependencies (UD) is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages. This tutorial gives an introduction to the UD framework and resources, from basic design principles to annotation guidelines and existing treebanks. We also discuss tools for developing and exploiting UD treebanks and survey applications of UD in NLP and linguistics.</abstract>
      <bibkey>nivre-etal-2017-universal</bibkey>
    </paper>
    <paper id="2">
      <title>Practical Neural Machine Translation</title>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <url hash="ea1ce4ac">E17-5002</url>
      <abstract>Neural Machine Translation (NMT) has achieved new breakthroughs in machine translation in recent years. It has dominated recent shared translation tasks in machine translation research, and is also being quickly adopted in industry. The technical differences between NMT and the previously dominant phrase-based statistical approach require that practictioners learn new best practices for building MT systems, ranging from different hardware requirements, new techniques for handling rare words and monolingual data, to new opportunities in continued learning and domain adaptation.

This tutorial is aimed at researchers and users of machine translation interested in working with NMT. The tutorial will cover a basic theoretical introduction to NMT, discuss the components of state-of-the-art systems, and provide practical advice for building NMT systems.</abstract>
      <bibkey>sennrich-haddow-2017-practical</bibkey>
    </paper>
    <paper id="3">
      <title>Imitation learning for structured prediction in natural language processing</title>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <author><first>Gerasimos</first><last>Lampouras</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <url hash="fe5527fd">E17-5003</url>
      <abstract>Imitation learning is a learning paradigm originally developed to learn robotic controllers from demonstrations by humans, e.g. autonomous flight from pilot demonstrations. Recently, algorithms for structured prediction were proposed under this paradigm and have been applied successfully to a number of tasks including syntactic dependency parsing, information extraction, coreference resolution, dynamic feature selection, semantic parsing and natural language generation. Key advantages are the ability to handle large output search spaces and to learn with non-decomposable loss functions. Our aim in this tutorial is to have a unified presentation of the various imitation algorithms for structure prediction, and show how they can be applied to a variety of NLP tasks.

All material associated with the tutorial will be made available through https://sheffieldnlp.github.io/ImitationLearningTutorialEACL2017/.</abstract>
      <bibkey>vlachos-etal-2017-imitation</bibkey>
    </paper>
    <paper id="4">
      <title>Word Vector Space Specialisation</title>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Nikola</first><last>Mrkšić</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <url hash="830764bb">E17-5004</url>
      <abstract>Specialising vector spaces to maximise their content with respect to one key property of vector space models (e.g. semantic similarity vs. relatedness or lexical entailment) while mitigating others has become an active and attractive research topic in representation learning. Such specialised vector spaces support different classes of NLP problems. Proposed approaches fall into two broad categories: a) Unsupervised methods which learn from raw textual corpora in more sophisticated ways (e.g. using context selection, extracting co-occurrence information from word patterns, attending over contexts); and b) Knowledge-base driven approaches which exploit available resources to encode external information into distributional vector spaces, injecting knowledge from semantic lexicons (e.g., WordNet, FrameNet, PPDB). In this tutorial, we will introduce researchers to state-of-the-art methods for constructing vector spaces specialised for a broad range of downstream NLP applications. We will deliver a detailed survey of the proposed methods and discuss best practices for intrinsic and application-oriented evaluation of such vector spaces.

Throughout the tutorial, we will provide running examples reaching beyond English as the only (and probably the easiest) use-case language, in order to demonstrate the applicability and modelling challenges of current representation learning architectures in other languages.</abstract>
      <bibkey>vulic-etal-2017-word</bibkey>
    </paper>
    <paper id="5">
      <title>Integer Linear Programming formulations in Natural Language Processing</title>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <url hash="8404bafa">E17-5005</url>
      <abstract>Making decisions in natural language processing problems often involves assigning values to sets of interdependent variables where the expressive dependency structure can influence, or even dictate what assignments are possible. This setting includes a broad range of structured prediction problems such as semantic role labeling, named entity and relation recognition, co-reference resolution, dependency parsing and semantic parsing. The setting is also appropriate for cases that may require making global decisions that involve multiple components, possibly pre-designed or pre-learned, as in event recognition and analysis, summarization, paraphrasing, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain or problem specific constraints.

Over the last few years, starting with a couple of papers written by (Roth &amp; Yih, 2004, 2005), dozens of papers have been using the Integer linear programming (ILP) formulation developed there, including several award-winning papers (e.g., (Martins, Smith, &amp; Xing, 2009; Koo, Rush, Collins, Jaakkola, &amp; Sontag., 2010; Berant, Dagan, &amp; Goldberger, 2011)).

This tutorial will present the key ingredients of ILP formulations of natural language processing problems, aiming at guiding readers through the key modeling steps, explaining the learning and inference paradigms and exemplifying these by providing examples from the literature. We will cover a range of topics, from the theoretical foundations of learning and inference with ILP models, to practical modeling guides, to software packages and applications.

The goal of this tutorial is to introduce the computational framework to broader ACL community, motivate it as a generic framework for learning and inference in global NLP decision problems, present some of the key theoretical and practical issues involved and survey some of the existing applications of it as a way to promote further development of the framework and additional applications. We will also make connections with some of the “hot” topics in current NLP research and show how they can be used within the general framework proposed here. The tutorial will thus be useful for many of the senior and junior researchers that have interest in global decision problems in NLP, providing a concise overview of recent perspectives and research results.</abstract>
      <bibkey>roth-srikumar-2017-integer</bibkey>
    </paper>
    <paper id="6">
      <title>Building Multimodal Simulations for Natural Language</title>
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Nikhil</first><last>Krishnaswamy</last></author>
      <url hash="13ce8c26">E17-5006</url>
      <abstract>In this tutorial, we introduce a computational framework and modeling language (VoxML) for composing multimodal simulations of natural language expressions within a 3D simulation environment (VoxSim). We demonstrate how to construct voxemes, which are visual object representations of linguistic entities. We also show how to compose events and actions over these objects, within a restricted domain of dynamics. This gives us the building blocks to simulate narratives of multiple events or participate in a multimodal dialogue with synthetic agents in the simulation environment. To our knowledge, this is the first time such material has been presented as a tutorial within the CL community.

This will be of relevance to students and researchers interested in modeling actionable language, natural language communication with agents and robots, spatial and temporal constraint solving through language, referring expression generation, embodied cognition, as well as minimal model creation.

Multimodal simulation of language, particularly motion expressions, brings together a number of existing lines of research from the computational linguistic, semantics, robotics, and formal logic communities, including action and event representation (Di Eugenio, 1991), modeling gestural correlates to NL expressions (Kipp et al., 2007; Neff et al., 2008), and action event modeling (Kipper and Palmer, 2000; Yang et al., 2015). We combine an approach to event modeling with a scene generation approach akin to those found in work by (Coyne and Sproat, 2001; Siskind, 2011; Chang et al., 2015). Mapping natural language expressions through a formal model and a dynamic logic interpretation into a visualization of the event described provides an environment for grounding concepts and referring expressions that is interpretable by both a computer and a human user. This opens a variety of avenues for humans to communicate with computerized agents and robots, as in (Matuszek et al., 2013; Lauria et al., 2001), (Forbes et al., 2015), and (Deits et al., 2013; Walter et al., 2013; Tellex et al., 2014). Simulation and automatic visualization of events from natural language descriptions and supplementary modalities, such as gestures, allows humans to use their native capabilities as linguistic and visual interpreters to collaborate on tasks with an artificial agent or to put semantic intuitions to the test in an environment where user and agent share a common context.

In previous work (Pustejovsky and Krishnaswamy, 2014; Pustejovsky, 2013a), we introduced a method for modeling natural language expressions within a 3D simulation environment built on top of the game development platform Unity (Goldstone, 2009). The goal of that work was to evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how an object is itself situated relative to its environment; and an operational characterization of how an object changes its location or how an agent acts on an object over time, e.g., its affordance structure. The former has developed into a semantic notion of situational context, called a habitat (Pustejovsky, 2013a; McDonald and Pustejovsky, 2014), while the latter is addressed by dynamic interpretations of event structure (Pustejovsky and Moszkowicz, 2011; Pustejovsky and Krishnaswamy, 2016b; Pustejovsky, 2013b).

The requirements on building a visual simulation from language include several components. We require a rich type system for lexical items and their composition, as well as a language for modeling the dynamics of events, based on Generative Lexicon (GL). Further, a minimal embedding space (MES) for the simulation must be determined. This is the 3D region within which the state is configured or the event unfolds. Object-based attributes for participants in a situation or event also need to be specified; e.g., orientation, relative size, default position or pose, etc. The simulation establishes an epistemic condition on the object and event rendering, imposing an implicit point of view (POV). Finally, there must be some sort of agent-dependent embodiment; this determines the relative scaling of an agent and its event participants and their surroundings, as it engages in the environment.
In order to construct a robust simulation from linguistic input, an event and its participants must be embedded within an appropriate minimal embedding space. This must sufficiently enclose the event localization, while optionally including space enough for a frame of reference for the event (the viewerâ€™s perspective).

We first describe the formal multimodal foundations for the modeling language, VoxML, which creates a minimal simulation from the linguistic input interpreted by the multimodal language, DITL. We then describe VoxSim, the compositional modeling and simulation environment, which maps the minimal VoxML model of the linguistic utterance to a simulation in Unity. This knowledge includes specification of object affordances, e.g., what actions are possible or enabled by use an object.

VoxML (Pustejovsky and Krishnaswamy, 2016b; Pustejovsky and Krishnaswamy, 2016a) encodes semantic knowledge of real-world objects represented as 3D models, and of events and attributes related to and enacted over these objects. VoxML goes beyond the limitations of existing 3D visual markup languages by allowing for the encoding of a broad range of semantic knowledge that can be exploited by a simulation platform such as VoxSim.

VoxSim (Krishnaswamy and Pustejovsky, 2016a; Krishnaswamy and Pustejovsky, 2016b) uses object and event semantic knowledge to generate animated scenes in real time without a complex animation interface. It uses the Unity game engine for graphics and I/O processing and takes as input a simple natural language utterance. The parsed utterance is semantically interpreted and transformed into a hybrid dynamic logic representation (DITL), and used to generate a minimal simulation of the event when composed with VoxML knowledge. 3D assets and VoxML-modeled nominal objects and events are created with other Unity-based tools, and VoxSim uses the entirety of the composed information to render a visualization of the described event.

The tutorial participants will learn how to build simulatable objects, compose dynamic event structures, and simulate the events running over the objects. The toolkit consists of object and program (event) composers and the runtime environment, which allows for the user to directly manipulate the objects, or interact with synthetic agents in VoxSim. As a result of this tutorial, the student will acquire the following skill set: take a novel object geometry from a library and model it in VoxML; apply existing library behaviors (actions or events) to the new VoxML object; model attributes of new objects as well as introduce novel attributes; model novel behaviors over objects.

The tutorial modules will be conducted within a build image of the software. Access to libraries will be provided by the instructors. No knowledge of 3D modeling or the Unity platform will be required.</abstract>
      <bibkey>pustejovsky-krishnaswamy-2017-building</bibkey>
    </paper>
  </volume>
</collection>
