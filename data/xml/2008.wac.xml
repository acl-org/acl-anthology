<?xml version='1.0' encoding='UTF-8'?>
<collection id="2008.wac">
  <volume id="1" type="proceedings" ingest-date="2025-03-26">
    <meta>
      <booktitle>Proceedings of the 4th Web as Corpus Workshop</booktitle>
      <publisher>European Language Resources Association</publisher>
      <address>Marrakech, Morocco</address>
      <month>June</month>
      <year>2008</year>
      <editor><first>Stefan</first><last>Evert</last></editor>
      <editor><first>Adam</first><last>Kilgarriff</last></editor>
      <editor><first>Serge</first><last>Sharoff</last></editor>
      <venue>wac</venue>
      <venue>ws</venue>
      <url hash="f6e08c9e">2008.wac-1</url>
    </meta>
    <frontmatter>
      <pages>1-6</pages>
      <url hash="bc85076c">2008.wac-1.0</url>
      <bibkey>-2008-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Reranking <fixed-case>G</fixed-case>oogle with <fixed-case>GR</fixed-case>e<fixed-case>G</fixed-case></title>
      <author><first>Rodolfo</first><last>Delmonte</last></author>
      <author><first>Marco Aldo Piccolino</first><last>Boniforti</last></author>
      <pages>1-7</pages>
      <url hash="8793c0db">2008.wac-1.1</url>
      <abstract>We present an experiment evaluating the contribution of a system called GReG for reranking the snippets returned by Google’s search engine in the 10 best links presented to the user, captured by the use of Google’s API. The evaluation aims at establishing whether or not the introduction of deep linguistic information may improve the accuracy of Google or rather it is the opposite case as maintained by the majority of people working in Information Retrieval, using a Bag Of Words approach. We used 900 questions, answers taken from TREC 8, 9 competitions, execute three different types of evaluation: one without any linguistic aid; a second one with tagging, syntactic constituency contribution; another run with what we call Partial Logical Form. Even though GReG is still work in progress, it is possible to draw clearcut conclusions: adding linguistic information to the evaluation process of the best snippet that can answer a question improves enormously the performance. In another experiment we used the actual associated to the Q/A pairs distributed by one of TREC’s participant, got even higher accuracy.</abstract>
      <bibkey>delmonte-boniforti-2008-reranking</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>G</fixed-case>oogle for the Linguist on a Budget</title>
      <author><first>András</first><last>Kornai</last></author>
      <author><first>Péter</first><last>Halácsy</last></author>
      <pages>8-11</pages>
      <url hash="55c98f7b">2008.wac-1.2</url>
      <abstract>In this paper, we present GLB, yet another open source, free system to create, exploit linguistic corpora gathered from the web. A simple, robust web crawl algorithm, a multi-dimensional information retrieval tool„ a crude parallelization mechanism are proposed, especially for researchers working in resource-limited environments.</abstract>
      <bibkey>kornai-halacsy-2008-google</bibkey>
    </paper>
    <paper id="3">
      <title>Victor: the Web-Page Cleaning Tool</title>
      <author><first>Miroslav</first><last>Spousta</last></author>
      <author><first>Michal</first><last>Marek</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <pages>12-17</pages>
      <url hash="7cf7b0c4">2008.wac-1.3</url>
      <abstract>In this paper we present a complete solution for automatic cleaning of arbitrary HTML pages with a goal of using web data as a corpus in the area of natural language processing, computational linguistics. We employ a sequence-labeling approach based on Conditional Random Fields (CRF). Every block of text in analyzed web page is assigned a set of features extracted from the textual content, HTML structure of the page. The blocks are automatically labeled either as content segments containing main web page content, which should be preserved, or as noisy segments not suitable for further linguistic processing, which should be eliminated. Our solution is based on the tool introduced at the CLEANEVAL 2007 shared task workshop. In this paper, we present new CRF features, a handy annotation tool„ new evaluation metrics. Evaluation itself is performed on a random sample of web pages automatically downloaded from the Czech web domain.</abstract>
      <bibkey>spousta-etal-2008-victor</bibkey>
    </paper>
    <paper id="4">
      <title>Segmenting <fixed-case>HTML</fixed-case> pages using visual, semantic information</title>
      <author><first>Georgios</first><last>Petasis</last></author>
      <author><first>Pavlina</first><last>Fragkou</last></author>
      <author><first>Aris</first><last>Theodorakos</last></author>
      <author><first>Vangelis</first><last>Karkaletsis</last></author>
      <author><first>Constantine D.</first><last>Spyropoulos</last></author>
      <pages>18-25</pages>
      <url hash="a6e2a811">2008.wac-1.4</url>
      <abstract>The information explosion of the Web aggravates the problem of effective information retrieval. Even though linguistic approaches found in the literature perform linguistic annotation by creating metadata in the form of tokens, lemmas or part of speech tags, however, this process is insufficient. This is due to the fact that these linguistic metadata do not exploit the actual content of the page, leading to the need of performing semantic annotation based on a predefined semantic model. This paper proposes a new learning approach for performing automatic semantic annotation. This is the result of a two step procedure: the first step partitions a web page into blocks based on its visual layout, while the second, performs subsequent partitioning based on the examination of appearance of specific types of entities denoting the semantic category as well as the application of a number of simple heuristics. Preliminary experiments performed on a manually annotated corpus regarding athletics proved to be very promising.</abstract>
      <bibkey>petasis-etal-2008-segmenting</bibkey>
    </paper>
    <paper id="5">
      <title>Identification of Duplicate News Stories in Web Pages</title>
      <author><first>John</first><last>Gibson</last></author>
      <author><first>Ben</first><last>Wellner</last></author>
      <author><first>Susan</first><last>Lubar</last></author>
      <pages>26-33</pages>
      <url hash="deeb5878">2008.wac-1.5</url>
      <abstract>Identifying near duplicate documents is a challenge often faced in the field of information discovery. Unfortunately many algorithms that find near duplicate pairs of plain text documents perform poorly when used on web pages, where metadata, other extraneous information make that process much more difficult. If the content of the page (e.g., the body of a news article) can be extracted from the page, then the accuracy of the duplicate detection algorithms is greatly increased. Using machine learning techniques to identify the content portion of web pages, we achieve duplicate detection accuracy that is nearly identical to plain text, significantly better than simple heuristic approaches to content extraction. We performed these experiments on a small, but fully annotated corpus.</abstract>
      <bibkey>gibson-etal-2008-identification</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>G</fixed-case>lossa<fixed-case>N</fixed-case>et 2: a linguistic search engine for <fixed-case>RSS</fixed-case>-based corpora</title>
      <author><first>Cédrick</first><last>Fairon</last></author>
      <author><first>Kévin</first><last>Macé</last></author>
      <author><first>Hubert</first><last>Naets</last></author>
      <pages>34-39</pages>
      <url hash="358a54af">2008.wac-1.6</url>
      <abstract>This paper presents GlossaNet 2, a free online concordance service that enables users to search into dynamic Web corpora. Two steps are involved in using GlossaNet. At first, users define a corpus by selecting RSS feeds in a preselected pool of sources (they can also add their own RSS feeds). These sources will be visited on a regular basis by a crawler in order to generate a dynamic corpus. Secondly, the user can register one or more search queries on his / her dynamic corpus. Search queries will be re-applied on the corpus every time it is updated, new concordances will be recorded for the user (results can be emailed, published for the user in a privative RSS feed, or they can be viewed online). This service integrates two preexisting software: Corporator (Fairon, 2006), a program that creates corpora by downloading, filtering RSS feeds, Unitex (Paumier, 2003), an open source corpus processor that relies on linguistic resources. After a short introduction, we will briefly present the concept of “RSS corpora”, the assets of this approach to corpus development. We will then give an overview of the GlossaNet architecture, present various cases of use.</abstract>
      <bibkey>fairon-etal-2008-glossanet</bibkey>
    </paper>
    <paper id="7">
      <title>Collecting <fixed-case>B</fixed-case>asque specialized corpora from the web: language-specific performance tweaks, improving topic precision</title>
      <author><first>I.</first><last>Leturia</last></author>
      <author><first>I.</first><last>San Vicente</last></author>
      <author><first>X.</first><last>Saralegi</last></author>
      <author><first>M.</first><last>Lopez de Lacalle</last></author>
      <pages>40-46</pages>
      <url hash="89016563">2008.wac-1.7</url>
      <abstract>The de facto standard process for collecting corpora from the Internet (with a given list of words, asking APIs of search engines for random combinations of them, downloading the returned pages) does not give very good precision when searching for texts on a certain topic., this precision is much worse when searching for corpora in the Basque language, due to certain properties inherent in the language, in the Basque web. The method proposed in this paper improves topic precision by using a sample mini-corpus as a basis for the process: the words to be used in the queries are automatically extracted from it„ a final topic-filtering step is performed using document-similarity measures with this sample corpus. We also describe the changes made to the usual process to adapt it to the peculiarities of Basque, alongside other adjustments to improve the general performance of the system, quality of the collected corpora.</abstract>
      <bibkey>leturia-etal-2008-collecting</bibkey>
    </paper>
    <paper id="8">
      <title>Introducing, evaluating uk<fixed-case>W</fixed-case>a<fixed-case>C</fixed-case>, a very large web-derived corpus of <fixed-case>E</fixed-case>nglish</title>
      <author><first>Adriano</first><last>Ferraresi</last></author>
      <author><first>Eros</first><last>Zanchetta</last></author>
      <author><first>Marco</first><last>Baroni</last></author>
      <author><first>Silvia</first><last>Bernardini</last></author>
      <pages>47-54</pages>
      <url hash="195eb191">2008.wac-1.8</url>
      <abstract>In this paper we introduce ukWaC, a large corpus of English constructed by crawling the .uk Internet domain. The corpus contains more than 2 billion tokens, is one of the largest freely available linguistic resources for English. The paper describes the tools, methodology used in the construction of the corpus, provides a qualitative evaluation of its contents, carried out through a vocabulary-based comparison with the BNC. We conclude by giving practical information about availability, format of the corpus.</abstract>
      <bibkey>ferraresi-etal-2008-introducing</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>R</fixed-case>o<fixed-case>DEO</fixed-case>: Reasoning over Dependencies Extracted Online</title>
      <author><first>Reda</first><last>Siblini</last></author>
      <author><first>Leila</first><last>Kosseim</last></author>
      <pages>55-62</pages>
      <url hash="cf88c689">2008.wac-1.9</url>
      <abstract>The web is the largest available corpus, which could be enormously valuable to many natural language processing applications. However it is becoming very difficult to identify relevant information from the web. We present a system for querying dependency tree collocations from the web. We show its usefulness in identifying relevant information by evaluating its accuracy in the task of extracting classes of named entities. The task achieved a general accuracy of 70%.</abstract>
      <bibkey>siblini-kosseim-2008-rodeo</bibkey>
    </paper>
  </volume>
</collection>
