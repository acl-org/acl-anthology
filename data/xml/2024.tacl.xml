<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.tacl">
  <volume id="1" type="journal">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 12</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2024</year>
      <venue>tacl</venue>
      <journal-volume>12</journal-volume>
    </meta>
    <paper id="1">
      <title><fixed-case>A</fixed-case>mbi<fixed-case>FC</fixed-case>: Fact-Checking Ambiguous Claims with Evidence</title>
      <author><first>Max</first><last>Glockner</last></author>
      <author><first>Ieva</first><last>Staliūnaitė</last></author>
      <author><first>James</first><last>Thorne</last></author>
      <author><first>Gisela</first><last>Vallejo</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/tacl_a_00629</doi>
      <abstract>Automated fact-checking systems verify claims against evidence to predict their veracity. In real-world scenarios, the retrieved evidence may not unambiguously support or refute the claim and yield conflicting but valid interpretations. Existing fact-checking datasets assume that the models developed with them predict a single veracity label for each claim, thus discouraging the handling of such ambiguity. To address this issue we present AmbiFC,1 a fact-checking dataset with 10k claims derived from real-world information needs. It contains fine-grained evidence annotations of 50k passages from 5k Wikipedia pages. We analyze the disagreements arising from ambiguity when comparing claims against evidence in AmbiFC, observing a strong correlation of annotator disagreement with linguistic phenomena such as underspecification and probabilistic reasoning. We develop models for predicting veracity handling this ambiguity via soft labels, and find that a pipeline that learns the label distribution for sentence-level evidence selection and veracity prediction yields the best performance. We compare models trained on different subsets of AmbiFC and show that models trained on the ambiguous instances perform better when faced with the identified linguistic phenomena.</abstract>
      <pages>1–18</pages>
      <url hash="190d51f6">2024.tacl-1.1</url>
      <bibkey>glockner-etal-2024-ambifc</bibkey>
    </paper>
    <paper id="2">
      <title>Language Varieties of <fixed-case>I</fixed-case>taly: Technology Challenges and Opportunities</title>
      <author><first>Alan</first><last>Ramponi</last></author>
      <doi>10.1162/tacl_a_00631</doi>
      <abstract>Italy is characterized by a one-of-a-kind linguistic diversity landscape in Europe, which implicitly encodes local knowledge, cultural traditions, artistic expressions, and history of its speakers. However, most local languages and dialects in Italy are at risk of disappearing within a few generations. The NLP community has recently begun to engage with endangered languages, including those of Italy. Yet, most efforts assume that these varieties are under-resourced language monoliths with an established written form and homogeneous functions and needs, and thus highly interchangeable with each other and with high-resource, standardized languages. In this paper, we introduce the linguistic context of Italy and challenge the default machine-centric assumptions of NLP for Italy’s language varieties. We advocate for a shift in the paradigm from machine-centric to speaker-centric NLP, and provide recommendations and opportunities for work that prioritizes languages and their speakers over technological advances. To facilitate the process, we finally propose building a local community towards responsible, participatory efforts aimed at supporting vitality of languages and dialects of Italy.</abstract>
      <pages>19–38</pages>
      <url hash="dd061bfb">2024.tacl-1.2</url>
      <bibkey>ramponi-2024-language</bibkey>
    </paper>
    <paper id="3">
      <title>Benchmarking Large Language Models for News Summarization</title>
      <author><first>Tianyi</first><last>Zhang</last></author>
      <author><first>Faisal</first><last>Ladhak</last></author>
      <author><first>Esin</first><last>Durmus</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Tatsunori B.</first><last>Hashimoto</last></author>
      <doi>10.1162/tacl_a_00632</doi>
      <abstract>Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM’s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.</abstract>
      <pages>39–57</pages>
      <url hash="2e4604d4">2024.tacl-1.3</url>
      <bibkey>zhang-etal-2024-benchmarking</bibkey>
    </paper>
    <paper id="4">
      <title>m<fixed-case>GPT</fixed-case>: Few-Shot Learners Go Multilingual</title>
      <author><first>Oleh</first><last>Shliazhko</last></author>
      <author><first>Alena</first><last>Fenogenova</last></author>
      <author><first>Maria</first><last>Tikhonova</last></author>
      <author><first>Anastasia</first><last>Kozlova</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Tatiana</first><last>Shavrina</last></author>
      <doi>10.1162/tacl_a_00633</doi>
      <abstract>This paper introduces mGPT, a multilingual variant of GPT-3, pretrained on 61 languages from 25 linguistically diverse language families using Wikipedia and the C4 Corpus. We detail the design and pretraining procedure. The models undergo an intrinsic and extrinsic evaluation: language modeling in all languages, downstream evaluation on cross-lingual NLU datasets and benchmarks in 33 languages, and world knowledge probing in 23 languages. The in-context learning abilities are on par with the contemporaneous language models while covering a larger number of languages, including underrepresented and low-resource languages of the Commonwealth of Independent States and the indigenous peoples in Russia. The source code and the language models are publicly available under the MIT license.</abstract>
      <pages>58–79</pages>
      <url hash="ad281290">2024.tacl-1.4</url>
      <bibkey>shliazhko-etal-2024-mgpt</bibkey>
    </paper>
    <paper id="5">
      <title>Cultural Adaptation of Recipes</title>
      <author><first>Yong</first><last>Cao</last></author>
      <author><first>Yova</first><last>Kementchedjhieva</last></author>
      <author><first>Ruixiang</first><last>Cui</last></author>
      <author><first>Antonia</first><last>Karamolegkou</last></author>
      <author><first>Li</first><last>Zhou</last></author>
      <author><first>Megan</first><last>Dare</last></author>
      <author><first>Lucia</first><last>Donatelli</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <doi>10.1162/tacl_a_00634</doi>
      <abstract>Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese- and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset composed of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally aware language models and their practical application in culturally diverse contexts.</abstract>
      <pages>80–99</pages>
      <url hash="a28fd6d2">2024.tacl-1.5</url>
      <bibkey>cao-etal-2024-cultural</bibkey>
    </paper>
    <paper id="6">
      <title>Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis</title>
      <author><first>Shiman</first><last>Zhao</last></author>
      <author><first>Yutao</first><last>Xie</last></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Tengjiao</first><last>Wang</last></author>
      <author><first>Jiahui</first><last>Yao</last></author>
      <author><first>Jiabin</first><last>Zheng</last></author>
      <doi>10.1162/tacl_a_00635</doi>
      <abstract>Few-shot Aspect Category Sentiment Analysis (ACSA) is a crucial task for aspect-based sentiment analysis, which aims to detect sentiment polarity for a given aspect category in a sentence with limited data. However, few-shot learning methods focus on distance metrics between the query and support sets to classify queries, heavily relying on aspect distributions in the embedding space. Thus, they suffer from overlapping distributions of aspect embeddings caused by irrelevant sentiment noise among sentences with multiple sentiment aspects, leading to misclassifications. To solve the above issues, we propose a metric-free method for few-shot ACSA, which models the associated relations among the aspects of support and query sentences by Dual Relations Propagation (DRP), addressing the passive effect of overlapping distributions. Specifically, DRP uses the dual relations (similarity and diversity) among the aspects of support and query sentences to explore intra-cluster commonality and inter-cluster uniqueness for alleviating sentiment noise and enhancing aspect features. Additionally, the dual relations are transformed from support-query to class-query to promote query inference by learning class knowledge. Experiments show that we achieve convincing performance on few-shot ACSA, especially an average improvement of 2.93% accuracy and 2.10% F1 score in the 3-way 1-shot setting.</abstract>
      <pages>100–119</pages>
      <url hash="886b8299">2024.tacl-1.6</url>
      <bibkey>zhao-etal-2024-metric</bibkey>
    </paper>
    <paper id="7">
      <title>Addressing the Binning Problem in Calibration Assessment through Scalar Annotations</title>
      <author><first>Zhengping</first><last>Jiang</last></author>
      <author><first>Anqi</first><last>Liu</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <doi>10.1162/tacl_a_00636</doi>
      <abstract>Computational linguistics models commonly target the prediction of discrete—categorical—labels. When assessing how well-calibrated these model predictions are, popular evaluation schemes require practitioners to manually determine a binning scheme: grouping labels into bins to approximate true label posterior. The problem is that these metrics are sensitive to binning decisions. We consider two solutions to the binning problem that apply at the stage of data annotation: collecting either distributed (redundant) labels or direct scalar value assignment. In this paper, we show that although both approaches address the binning problem by evaluating instance-level calibration, direct scalar assignment is significantly more cost-effective. We provide theoretical analysis and empirical evidence to support our proposal for dataset creators to adopt scalar annotation protocols to enable a higher-quality assessment of model calibration.</abstract>
      <pages>120–136</pages>
      <url hash="4f20ba0a">2024.tacl-1.7</url>
      <bibkey>jiang-etal-2024-addressing</bibkey>
    </paper>
    <paper id="8">
      <title>An Energy-based Model for Word-level <fixed-case>A</fixed-case>uto<fixed-case>C</fixed-case>ompletion in Computer-aided Translation</title>
      <author><first>Cheng</first><last>Yang</last></author>
      <author><first>Guoping</first><last>Huang</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Zhirui</first><last>Zhang</last></author>
      <author><first>Siheng</first><last>Li</last></author>
      <author><first>Mingming</first><last>Yang</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Yujiu</first><last>Yang</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <doi>10.1162/tacl_a_00637</doi>
      <abstract>Word-level AutoCompletion (WLAC) is a rewarding yet challenging task in Computer-aided Translation. Existing work addresses this task through a classification model based on a neural network that maps the hidden vector of the input context into its corresponding label (i.e., the candidate target word is treated as a label). Since the context hidden vector itself does not take the label into account and it is projected to the label through a linear classifier, the model cannot sufficiently leverage valuable information from the source sentence as verified in our experiments, which eventually hinders its overall performance. To alleviate this issue, this work proposes an energy-based model for WLAC, which enables the context hidden vector to capture crucial information from the source sentence. Unfortunately, training and inference suffer from efficiency and effectiveness challenges, therefore we employ three simple yet effective strategies to put our model into practice. Experiments on four standard benchmarks demonstrate that our reranking-based approach achieves substantial improvements (about 6.07%) over the previous state-of-the-art model. Further analyses show that each strategy of our approach contributes to the final performance.1</abstract>
      <pages>137–156</pages>
      <url hash="527ab2ef">2024.tacl-1.8</url>
      <bibkey>yang-etal-2024-energy</bibkey>
    </paper>
    <paper id="9">
      <title>Lost in the Middle: How Language Models Use Long Contexts</title>
      <author><first>Nelson F.</first><last>Liu</last></author>
      <author><first>Kevin</first><last>Lin</last></author>
      <author><first>John</first><last>Hewitt</last></author>
      <author><first>Ashwin</first><last>Paranjape</last></author>
      <author><first>Michele</first><last>Bevilacqua</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <doi>10.1162/tacl_a_00638</doi>
      <abstract>While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.</abstract>
      <pages>157–173</pages>
      <url hash="ad243033">2024.tacl-1.9</url>
      <bibkey>liu-etal-2024-lost</bibkey>
    </paper>
    <paper id="10">
      <title>Red Teaming Language Model Detectors with Language Models</title>
      <author><first>Zhouxing</first><last>Shi</last></author>
      <author><first>Yihan</first><last>Wang</last></author>
      <author><first>Fan</first><last>Yin</last></author>
      <author><first>Xiangning</first><last>Chen</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <doi>10.1162/tacl_a_00639</doi>
      <abstract>The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM’s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.</abstract>
      <pages>174–189</pages>
      <url hash="2a5886ed">2024.tacl-1.10</url>
      <bibkey>shi-etal-2024-red</bibkey>
    </paper>
    <paper id="11">
      <title>Text Attribute Control via Closed-Loop Disentanglement</title>
      <author><first>Lei</first><last>Sha</last></author>
      <author><first>Thomas</first><last>Lukasiewicz</last></author>
      <doi>10.1162/tacl_a_00640</doi>
      <abstract>Changing an attribute of a text without changing the content usually requires first disentangling the text into irrelevant attributes and content representations. After that, in the inference phase, the representation of one attribute is tuned to a different value, expecting that the corresponding attribute of the text can also be changed accordingly. The usual way of disentanglement is to add some constraints on the latent space of an encoder-decoder architecture, including adversarial-based constraints and mutual-information-based constraints. However, previous semi-supervised processes of attribute change are usually not enough to guarantee the success of attribute change and content preservation. In this paper, we propose a novel approach to achieve a robust control of attributes while enhancing content preservation. In this approach, we use a semi-supervised contrastive learning method to encourage the disentanglement of attributes in latent spaces. Differently from previous works, we re-disentangle the reconstructed sentence and compare the re-disentangled latent space with the original latent space, which makes a closed-loop disentanglement process. This also helps content preservation. In addition, the contrastive learning method is also able to replace the role of minimizing mutual information and adversarial training in the disentanglement process, which alleviates the computation cost. We conducted experiments on three text datasets, including the Yelp Service review dataset, the Amazon Product review dataset, and the GoEmotions dataset. The experimental results show the effectiveness of our model.</abstract>
      <pages>190–209</pages>
      <url hash="63eaa6d0">2024.tacl-1.11</url>
      <bibkey>sha-lukasiewicz-2024-text</bibkey>
    </paper>
    <paper id="12">
      <title>Unifying Structured Data as Graph for Data-to-Text Pre-Training</title>
      <author><first>Shujie</first><last>Li</last></author>
      <author><first>Liang</first><last>Li</last></author>
      <author><first>Ruiying</first><last>Geng</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Binhua</first><last>Li</last></author>
      <author><first>Guanghu</first><last>Yuan</last></author>
      <author><first>Wanwei</first><last>He</last></author>
      <author><first>Shao</first><last>Yuan</last></author>
      <author><first>Can</first><last>Ma</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <doi>10.1162/tacl_a_00641</doi>
      <abstract>Data-to-text (D2T) generation aims to transform structured data into natural language text. Data-to-text pre-training has proved to be powerful in enhancing D2T generation and yields impressive performance. However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph). In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different D2T generation tasks as graph-to-text generation. To effectively exploit the structural information of the input graph, we propose a structure-enhanced pre-training method for D2T generation by designing a structure-enhanced Transformer. Concretely, we devise a position matrix for the Transformer, encoding relative positional information of connected nodes in the input graph. In addition, we propose a new attention matrix to incorporate graph structures into the original Transformer by taking the available explicit connectivity structure into account. Extensive experiments on six benchmark datasets show the effectiveness of our model. Our source codes are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.</abstract>
      <pages>210–228</pages>
      <url hash="93e5bf66">2024.tacl-1.12</url>
      <bibkey>li-etal-2024-unifying</bibkey>
    </paper>
    <paper id="13">
      <title>Exploring Human-Like Translation Strategy with Large Language Models</title>
      <author><first>Zhiwei</first><last>He</last></author>
      <author><first>Tian</first><last>Liang</last></author>
      <author><first>Wenxiang</first><last>Jiao</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Yujiu</first><last>Yang</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Xing</first><last>Wang</last></author>
      <doi>10.1162/tacl_a_00642</doi>
      <abstract>Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. Compared to typical machine translation that focuses solely on source-to-target mapping, LLM-based translation can potentially mimic the human translation process, which might take preparatory steps to ensure high-quality translation. This work explores this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs first to analyze the given source sentence and induce three aspects of translation-related knowledge (keywords, topics, and relevant demonstrations) to guide the final translation process. Moreover, we employ a selection mechanism based on quality estimation to filter out noisy and unhelpful knowledge. Both automatic (3 LLMs × 11 directions × 2 automatic metrics) and human evaluation (preference study and MQM) demonstrate the effectiveness of MAPS. Further analysis shows that by mimicking the human translation process, MAPS reduces various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission. Source code is available at https://github.com/zwhe99/MAPS-mt.</abstract>
      <pages>229–246</pages>
      <url hash="3801d508">2024.tacl-1.13</url>
      <bibkey>he-etal-2024-exploring</bibkey>
    </paper>
    <paper id="14">
      <title>Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering</title>
      <author><first>Dingmin</first><last>Wang</last></author>
      <author><first>Qiuyuan</first><last>Huang</last></author>
      <author><first>Matthew</first><last>Jackson</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <doi>10.1162/tacl_a_00646</doi>
      <abstract>An open-domain question answering (QA) system usually follows a retrieve-then-read paradigm, in which a retriever is used to retrieve relevant passages from a large corpus, and then a reader generates answers based on the retrieved passages and the original question. In this paper, we propose a simple and novel mutual learning framework to improve the performance of retrieve-then-read-style models via an intermediate module named the knowledge selector, which we train with reinforcement learning. The key benefits of our proposed intermediate module are: 1) no requirement for additional annotated question-passage pairs; 2) improvements in both retrieval and QA performance, as well as computational efficiency, compared to prior competitive retrieve-then-read models; 3) with no finetuning, improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint.</abstract>
      <pages>247–263</pages>
      <url hash="3d6d9126">2024.tacl-1.14</url>
      <bibkey>wang-etal-2024-retrieve</bibkey>
    </paper>
    <paper id="15">
      <title>Explicitly Representing Syntax Improves Sentence-to-Layout Prediction of Unexpected Situations</title>
      <author><first>Wolf</first><last>Nuyts</last></author>
      <author><first>Ruben</first><last>Cartuyvels</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <doi>10.1162/tacl_a_00643</doi>
      <abstract>Recognizing visual entities in a natural language sentence and arranging them in a 2D spatial layout require a compositional understanding of language and space. This task of layout prediction is valuable in text-to-image synthesis as it allows localized and controlled in-painting of the image. In this comparative study it is shown that we can predict layouts from language representations that implicitly or explicitly encode sentence syntax, if the sentences mention similar entity-relationships to the ones seen during training. To test compositional understanding, we collect a test set of grammatically correct sentences and layouts describing compositions of entities and relations that unlikely have been seen during training. Performance on this test set substantially drops, showing that current models rely on correlations in the training data and have difficulties in understanding the structure of the input sentences. We propose a novel structural loss function that better enforces the syntactic structure of the input sentence and show large performance gains in the task of 2D spatial layout prediction conditioned on text. The loss has the potential to be used in other generation tasks where a tree-like structure underlies the conditioning modality. Code, trained models, and the USCOCO evaluation set are available via Github.1</abstract>
      <pages>264–282</pages>
      <url hash="131757be">2024.tacl-1.15</url>
      <bibkey>nuyts-etal-2024-explicitly</bibkey>
    </paper>
    <paper id="16">
      <title>Evaluating the Ripple Effects of Knowledge Editing in Language Models</title>
      <author><first>Roi</first><last>Cohen</last></author>
      <author><first>Eden</first><last>Biran</last></author>
      <author><first>Ori</first><last>Yoran</last></author>
      <author><first>Amir</first><last>Globerson</last></author>
      <author><first>Mor</first><last>Geva</last></author>
      <doi>10.1162/tacl_a_00644</doi>
      <abstract>Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g., “Jack Depp is the son of Johnny Depp”) introduces a “ripple effect” in the form of additional facts that the model needs to update (e.g., “Jack Depp is the sibling of Lily-Rose Depp”). To address this, we propose novel evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct RippleEdits, a diagnostic benchmark of 5K factual edits, capturing various types of ripple effects. We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model’s knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing.1</abstract>
      <pages>283–298</pages>
      <url hash="6e844169">2024.tacl-1.16</url>
      <bibkey>cohen-etal-2024-evaluating</bibkey>
    </paper>
    <paper id="17">
      <title>The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations</title>
      <author><first>Aina Garí</first><last>Soler</last></author>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <doi>10.1162/tacl_a_00647</doi>
      <abstract>When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. What is the best way to represent these words with a single vector, and are these representations of worse quality than those of in-vocabulary words? We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words. Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words. Their similarity values, however, must be interpreted with caution.</abstract>
      <pages>299–320</pages>
      <url hash="3b083dbd">2024.tacl-1.17</url>
      <bibkey>soler-etal-2024-impact</bibkey>
    </paper>
    <paper id="18">
      <title>Large Language Models Enable Few-Shot Clustering</title>
      <author><first>Vijay</first><last>Viswanathan</last></author>
      <author><first>Kiril</first><last>Gashteovski</last></author>
      <author><first>Kiril</first><last>Gashteovski</last></author>
      <author><first>Carolin</first><last>Lawrence</last></author>
      <author><first>Tongshuang</first><last>Wu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <doi>10.1162/tacl_a_00648</doi>
      <abstract>Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user’s intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model (LLM) can amplify an expert’s guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find that incorporating LLMs in the first two stages routinely provides significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.1</abstract>
      <pages>321–333</pages>
      <url hash="1e490298">2024.tacl-1.18</url>
      <bibkey>viswanathan-etal-2024-large</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>J</fixed-case>usti<fixed-case>LM</fixed-case>: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims</title>
      <author><first>Fengzhu</first><last>Zeng</last></author>
      <author><first>Wei</first><last>Gao</last></author>
      <doi>10.1162/tacl_a_00649</doi>
      <abstract>Justification is an explanation that supports the veracity assigned to a claim in fact-checking. However, the task of justification generation has been previously oversimplified as summarization of a fact-check article authored by fact-checkers. Therefore, we propose a realistic approach to generate justification based on retrieved evidence. We present a new benchmark dataset called ExClaim (for Explainable fact-checking of real-world Claims), and introduce JustiLM, a novel few-shot Justification generation based on retrieval-augmented Language Model by using fact-check articles as an auxiliary resource during training only. Experiments show that JustiLM achieves promising performance in justification generation compared to strong baselines, and can also enhance veracity classification with a straightforward extension.1 Code and dataset are released at https://github.com/znhy1024/JustiLM.</abstract>
      <pages>334–354</pages>
      <url hash="b637974e">2024.tacl-1.19</url>
      <bibkey>zeng-gao-2024-justilm</bibkey>
    </paper>
    <paper id="20">
      <title>To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation</title>
      <author><first>Jiaming</first><last>Luo</last></author>
      <author><first>Colin</first><last>Cherry</last></author>
      <author><first>George</first><last>Foster</last></author>
      <doi>10.1162/tacl_a_00645</doi>
      <abstract>We conduct a large-scale fine-grained comparative analysis of machine translations (MTs) against human translations (HTs) through the lens of morphosyntactic divergence. Across three language pairs and two types of divergence defined as the structural difference between the source and the target, MT is consistently more conservative than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments. Through analysis on different decoding algorithms, we attribute this discrepancy to the use of beam search that biases MT towards more convergent patterns. This bias is most amplified when the convergent pattern appears around 50% of the time in training data. Lastly, we show that for a majority of morphosyntactic divergences, their presence in HT is correlated with decreased MT performance, presenting a greater challenge for MT systems.</abstract>
      <pages>355–371</pages>
      <url hash="84f467ac">2024.tacl-1.20</url>
      <bibkey>luo-etal-2024-diverge</bibkey>
    </paper>
    <paper id="21">
      <title>What Do Self-Supervised Speech Models Know About Words?</title>
      <author><first>Ankita</first><last>Pasad</last></author>
      <author><first>Chung-Ming</first><last>Chien</last></author>
      <author><first>Shane</first><last>Settle</last></author>
      <author><first>Karen</first><last>Livescu</last></author>
      <doi>10.1162/tacl_a_00656</doi>
      <abstract>Many self-supervised speech models (S3Ms) have been introduced over the last few years, improving performance and data efficiency on various speech tasks. However, these empirical successes alone do not give a complete picture of what is learned during pre-training. Recent work has begun analyzing how S3Ms encode certain properties, such as phonetic and speaker information, but we still lack a proper understanding of knowledge encoded at the word level and beyond. In this work, we use lightweight analysis methods to study segment-level linguistic properties—word identity, boundaries, pronunciation, syntactic features, and semantic features—encoded in S3Ms. We present a comparative study of layer-wise representations from ten S3Ms and find that (i) the frame-level representations within each word segment are not all equally informative, and (ii) the pre-training objective and model size heavily influence the accessibility and distribution of linguistic information across layers. We also find that on several tasks—word discrimination, word segmentation, and semantic sentence similarity—S3Ms trained with visual grounding outperform their speech-only counterparts. Finally, our task-based analyses demonstrate improved performance on word segmentation and acoustic word discrimination while using simpler methods than prior work.1</abstract>
      <pages>372–391</pages>
      <url hash="3dcd2b85">2024.tacl-1.21</url>
      <bibkey>pasad-etal-2024-self</bibkey>
    </paper>
    <paper id="22">
      <title>Are Character-level Translations Worth the Wait? Comparing <fixed-case>B</fixed-case>y<fixed-case>T</fixed-case>5 and m<fixed-case>T</fixed-case>5 for Machine Translation</title>
      <author><first>Lukas</first><last>Edman</last></author>
      <author><first>Gabriele</first><last>Sarti</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Gertjan van</first><last>Noord</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <doi>10.1162/tacl_a_00651</doi>
      <abstract>Pretrained character-level and byte-level language models have been shown to be competitive with popular subword models across a range of Natural Language Processing tasks. However, there has been little research on their effectiveness for neural machine translation (NMT), particularly within the popular pretrain-then-finetune paradigm. This work performs an extensive comparison across multiple languages and experimental conditions of character- and subword-level pretrained models (ByT5 and mT5, respectively) on NMT. We show the effectiveness of character-level modeling in translation, particularly in cases where fine-tuning data is limited. In our analysis, we show how character models’ gains in translation quality are reflected in better translations of orthographically similar words and rare words. While evaluating the importance of source texts in driving model predictions, we highlight word-level patterns within ByT5, suggesting an ability to modulate word-level and character-level information during generation. We conclude by assessing the efficiency tradeoff of byte models, suggesting their usage in non-time-critical scenarios to boost translation quality.</abstract>
      <pages>392–410</pages>
      <url hash="1136a915">2024.tacl-1.22</url>
      <bibkey>edman-etal-2024-character</bibkey>
    </paper>
    <paper id="23">
      <title>Geographic Adaptation of Pretrained Language Models</title>
      <author><first>Valentin</first><last>Hofmann</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Janet B.</first><last>Pierrehumbert</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <doi>10.1162/tacl_a_00652</doi>
      <abstract>While pretrained language models (PLMs) have been shown to possess a plethora of linguistic knowledge, the existing body of research has largely neglected extralinguistic knowledge, which is generally difficult to obtain by pretraining on text alone. Here, we contribute to closing this gap by examining geolinguistic knowledge, i.e., knowledge about geographic variation in language. We introduce geoadaptation, an intermediate training step that couples language modeling with geolocation prediction in a multi-task learning setup. We geoadapt four PLMs, covering language groups from three geographic areas, and evaluate them on five different tasks: fine-tuned (i.e., supervised) geolocation prediction, zero-shot (i.e., unsupervised) geolocation prediction, fine-tuned language identification, zero-shot language identification, and zero-shot prediction of dialect features. Geoadaptation is very successful at injecting geolinguistic knowledge into the PLMs: The geoadapted PLMs consistently outperform PLMs adapted using only language modeling (by especially wide margins on zero-shot prediction tasks), and we obtain new state-of-the-art results on two benchmarks for geolocation prediction and language identification. Furthermore, we show that the effectiveness of geoadaptation stems from its ability to geographically retrofit the representation space of the PLMs.</abstract>
      <pages>411–431</pages>
      <url hash="e0cc9a7d">2024.tacl-1.23</url>
      <bibkey>hofmann-etal-2024-geographic</bibkey>
    </paper>
    <paper id="24">
      <title>Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension</title>
      <author><first>Sweta</first><last>Agrawal</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <doi>10.1162/tacl_a_00653</doi>
      <abstract>Automatic text simplification (TS) aims to automate the process of rewriting text to make it easier for people to read. A pre-requisite for TS to be useful is that it should convey information that is consistent with the meaning of the original text. However, current TS evaluation protocols assess system outputs for simplicity and meaning preservation without regard for the document context in which output sentences occur and for how people understand them. In this work, we introduce a human evaluation framework to assess whether simplified texts preserve meaning using reading comprehension questions. With this framework, we conduct a thorough human evaluation of texts by humans and by nine automatic systems. Supervised systems that leverage pre-training knowledge achieve the highest scores on the reading comprehension tasks among the automatic controllable TS systems. However, even the best-performing supervised system struggles with at least 14% of the questions, marking them as “unanswerable” based on simplified content. We further investigate how existing TS evaluation metrics and automatic question-answering systems approximate the human judgments we obtained.</abstract>
      <pages>432–448</pages>
      <url hash="8df122c2">2024.tacl-1.24</url>
      <bibkey>agrawal-carpuat-2024-text</bibkey>
    </paper>
    <paper id="25">
      <title>Simultaneous Selection and Adaptation of Source Data via Four-Level Optimization</title>
      <author><first>Pengtao</first><last>Xie</last></author>
      <author><first>Xingchen</first><last>Zhao</last></author>
      <author><first>Xuehai</first><last>He</last></author>
      <doi>10.1162/tacl_a_00658</doi>
      <abstract>In many NLP applications, to mitigate data deficiency in a target task, source data is collected to help with target model training. Existing transfer learning methods either select a subset of source examples that are close to the target domain or try to adapt all source examples into the target domain, then use selected or adapted source examples to train the target model. These methods either incur significant information loss or bear the risk that after adaptation, source examples which are originally already in the target domain may be outside the target domain. To address the limitations of these methods, we propose a four-level optimization based framework which simultaneously selects and adapts source data. Our method can automatically identify in-domain and out-of-domain source examples and apply example-specific processing methods: selection for in-domain examples and adaptation for out-of-domain examples. Experiments on various datasets demonstrate the effectiveness of our proposed method.</abstract>
      <pages>449–466</pages>
      <url hash="55003fba">2024.tacl-1.25</url>
      <bibkey>xie-etal-2024-simultaneous</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>C</fixed-case>onvo<fixed-case>S</fixed-case>ense: Overcoming Monotonous Commonsense Inferences for Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Sarah E.</first><last>Finch</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <doi>10.1162/tacl_a_00659</doi>
      <abstract>Mastering commonsense understanding and reasoning is a pivotal skill essential for conducting engaging conversations. While there have been several attempts to create datasets that facilitate commonsense inferences in dialogue contexts, existing datasets tend to lack in-depth details, restate information already present in the conversation, and often fail to capture the multifaceted nature of commonsense reasoning. In response to these limitations, we compile a new synthetic dataset for commonsense reasoning in dialogue contexts using GPT, ℂonvoSense, that boasts greater contextual novelty, offers a higher volume of inferences per example, and substantially enriches the detail conveyed by the inferences. Our dataset contains over 500,000 inferences across 12,000 dialogues with 10 popular inference types, which empowers the training of generative commonsense models for dialogue that are superior in producing plausible inferences with high novelty when compared to models trained on the previous datasets. To the best of our knowledge, ℂonvoSense is the first of its kind to provide such a multitude of novel inferences at such a large scale.</abstract>
      <pages>467–483</pages>
      <url hash="131f83b5">2024.tacl-1.26</url>
      <bibkey>finch-choi-2024-convosense</bibkey>
    </paper>
    <paper id="27">
      <title>Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies</title>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Michael</first><last>Saxon</last></author>
      <author><first>Wenda</first><last>Xu</last></author>
      <author><first>Deepak</first><last>Nathani</last></author>
      <author><first>Xinyi</first><last>Wang</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <doi>10.1162/tacl_a_00660</doi>
      <abstract>While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback—either produced by the LLM itself (self-correction) or some external system—are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.</abstract>
      <pages>484–506</pages>
      <url hash="2726c64d">2024.tacl-1.27</url>
      <bibkey>pan-etal-2024-automatically</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>K</fixed-case>o<fixed-case>BBQ</fixed-case>: <fixed-case>K</fixed-case>orean Bias Benchmark for Question Answering</title>
      <author><first>Jiho</first><last>Jin</last></author>
      <author><first>Jiseon</first><last>Kim</last></author>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Haneul</first><last>Yoo</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <author><first>Hwaran</first><last>Lee</last></author>
      <doi>10.1162/tacl_a_00661</doi>
      <abstract>Warning: This paper contains examples of stereotypes and biases. The Bias Benchmark for Question Answering (BBQ) is designed to evaluate social biases of language models (LMs), but it is not simple to adapt this benchmark to cultural contexts other than the US because social biases depend heavily on the cultural context. In this paper, we present KoBBQ, a Korean bias benchmark dataset, and we propose a general framework that addresses considerations for cultural adaptation of a dataset. Our framework includes partitioning the BBQ dataset into three classes—Simply-Transferred (can be used directly after cultural translation), Target-Modified (requires localization in target groups), and Sample-Removed (does not fit Korean culture)—and adding four new categories of bias specific to Korean culture. We conduct a large-scale survey to collect and validate the social biases and the targets of the biases that reflect the stereotypes in Korean culture. The resulting KoBBQ dataset comprises 268 templates and 76,048 samples across 12 categories of social bias. We use KoBBQ to measure the accuracy and bias scores of several state-of-the-art multilingual LMs. The results clearly show differences in the bias of LMs as measured by KoBBQ and a machine-translated version of BBQ, demonstrating the need for and utility of a well-constructed, culturally aware social bias benchmark.</abstract>
      <pages>507–524</pages>
      <url hash="860d3737">2024.tacl-1.28</url>
      <bibkey>jin-etal-2024-kobbq</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>A</fixed-case>uto<fixed-case>PEFT</fixed-case>: Automatic Configuration Search for Parameter-Efficient Fine-Tuning</title>
      <author><first>Han</first><last>Zhou</last></author>
      <author><first>Xingchen</first><last>Wan</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <doi>10.1162/tacl_a_00662</doi>
      <abstract>Large pretrained language models are widely used in downstream NLP tasks via task- specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating much fewer parameters than full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: We first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimization in a low-cost setup, we then discover a Pareto-optimal set of configurations with strong performance-cost trade-offs across different numbers of parameters that are also highly transferable across different tasks. Empirically, on GLUE and SuperGLUE tasks, we show that AutoPEFT-discovered configurations significantly outperform existing PEFT methods and are on par or better than FFT without incurring substantial training efficiency costs.</abstract>
      <pages>525–542</pages>
      <url hash="06b48cf9">2024.tacl-1.29</url>
      <bibkey>zhou-etal-2024-autopeft</bibkey>
    </paper>
    <paper id="30">
      <title>What Formal Languages Can Transformers Express? A Survey</title>
      <author><first>Lena</first><last>Strobl</last></author>
      <author><first>William</first><last>Merrill</last></author>
      <author><first>Gail</first><last>Weiss</last></author>
      <author><first>David</first><last>Chiang</last></author>
      <author><first>Dana</first><last>Angluin</last></author>
      <doi>10.1162/tacl_a_00663</doi>
      <abstract>As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring such questions can help clarify the power of transformers relative to other models of computation, their fundamental capabilities and limits, and the impact of architectural choices. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.</abstract>
      <pages>543–561</pages>
      <url hash="aac62bb1">2024.tacl-1.30</url>
      <bibkey>strobl-etal-2024-formal</bibkey>
    </paper>
    <paper id="31">
      <title>Text-to-<fixed-case>O</fixed-case>verpass<fixed-case>QL</fixed-case>: A Natural Language Interface for Complex Geodata Querying of <fixed-case>O</fixed-case>pen<fixed-case>S</fixed-case>treet<fixed-case>M</fixed-case>ap</title>
      <author><first>Michael</first><last>Staniek</last></author>
      <author><first>Raphael</first><last>Schumann</last></author>
      <author><first>Maike</first><last>Züfle</last></author>
      <author><first>Stefan</first><last>Riezler</last></author>
      <doi>10.1162/tacl_a_00654</doi>
      <abstract>We present Text-to-OverpassQL, a task designed to facilitate a natural language interface for querying geodata from OpenStreetMap (OSM). The Overpass Query Language (OverpassQL) allows users to formulate complex database queries and is widely adopted in the OSM ecosystem. Generating Overpass queries from natural language input serves multiple use-cases. It enables novice users to utilize OverpassQL without prior knowledge, assists experienced users with crafting advanced queries, and enables tool-augmented large language models to access information stored in the OSM database. In order to assess the performance of current sequence generation models on this task, we propose OverpassNL,1 a dataset of 8,352 queries with corresponding natural language inputs. We further introduce task specific evaluation metrics and ground the evaluation of the Text-to-OverpassQL task by executing the queries against the OSM database. We establish strong baselines by finetuning sequence-to-sequence models and adapting large language models with in-context examples. The detailed evaluation reveals strengths and weaknesses of the considered learning strategies, laying the foundations for further research into the Text-to-OverpassQL task.</abstract>
      <pages>562–575</pages>
      <url hash="a0c93cad">2024.tacl-1.31</url>
      <bibkey>staniek-etal-2024-text</bibkey>
    </paper>
    <paper id="32">
      <title>Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions</title>
      <author><first>Jiahuan</first><last>Li</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Shanbo</first><last>Cheng</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <doi>10.1162/tacl_a_00655</doi>
      <abstract>Large-scale pretrained language models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translation, without being explicitly trained on parallel corpora. It is intriguing how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7.5B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the translation performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs’ ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning with translation instructions, LLMs could learn to perform the translation task well even for those language pairs unseen during the instruction tuning phase.</abstract>
      <pages>576–592</pages>
      <url hash="4caf1b95">2024.tacl-1.32</url>
      <bibkey>li-etal-2024-eliciting</bibkey>
    </paper>
    <paper id="33">
      <title>Semantics of Multiword Expressions in Transformer-Based Models: A Survey</title>
      <author><first>Filip</first><last>Miletić</last></author>
      <author><first>Sabine Schulte im</first><last>Walde</last></author>
      <doi>10.1162/tacl_a_00657</doi>
      <abstract>Multiword expressions (MWEs) are composed of multiple words and exhibit variable degrees of compositionality. As such, their meanings are notoriously difficult to model, and it is unclear to what extent this issue affects transformer architectures. Addressing this gap, we provide the first in-depth survey of MWE processing with transformer models. We overall find that they capture MWE semantics inconsistently, as shown by reliance on surface patterns and memorized information. MWE meaning is also strongly localized, predominantly in early layers of the architecture. Representations benefit from specific linguistic properties, such as lower semantic idiosyncrasy and ambiguity of target expressions. Our findings overall question the ability of transformer models to robustly capture fine-grained semantics. Furthermore, we highlight the need for more directly comparable evaluation setups.</abstract>
      <pages>593–612</pages>
      <url hash="aefab9e8">2024.tacl-1.33</url>
      <bibkey>miletic-walde-2024-semantics</bibkey>
    </paper>
    <paper id="34">
      <title>The <fixed-case>T</fixed-case>hai Discourse Treebank: Annotating and Classifying <fixed-case>T</fixed-case>hai Discourse Connectives</title>
      <author><first>Ponrawee</first><last>Prasertsom</last></author>
      <author><first>Apiwat</first><last>Jaroonpol</last></author>
      <author><first>Attapol T.</first><last>Rutherford</last></author>
      <doi>10.1162/tacl_a_00650</doi>
      <abstract>Discourse analysis is a highly applicable area of natural language processing. In English and other languages, resources for discourse-based tasks are widely available. Thai, however, has hitherto lacked such resources. We present the Thai Discourse Treebank, the first, large Thai corpus annotated in the style of the Penn Discourse Treebank. The resulting corpus has over 10,000 sentences and 18,000 instances of connectives in 33 different relations. We release the corpus alongside our list of 148 potentially polysemous discourse connectives with a total of 340 form-sense pairs and their classification criteria to facilitate future research. We also develop models for connective identification and classification tasks. Our best models achieve an F1 of 0.96 in the identification task and 0.46 on the sense classification task. Our results serve as benchmarks for future models for Thai discourse tasks.</abstract>
      <pages>613–629</pages>
      <url hash="31c807bd">2024.tacl-1.34</url>
      <bibkey>prasertsom-etal-2024-thai</bibkey>
    </paper>
    <paper id="35">
      <title>Federated Learning for Exploiting Annotators’ Disagreements in Natural Language Processing</title>
      <author><first>Nuria</first><last>Rodríguez-Barroso</last></author>
      <author><first>Eugenio Martínez</first><last>Cámara</last></author>
      <author><first>Jose Camacho</first><last>Collados</last></author>
      <author><first>M. Victoria</first><last>Luzón</last></author>
      <author><first>Francisco</first><last>Herrera</last></author>
      <doi>10.1162/tacl_a_00664</doi>
      <abstract>The annotation of ambiguous or subjective NLP tasks is usually addressed by various annotators. In most datasets, these annotations are aggregated into a single ground truth. However, this omits divergent opinions of annotators, hence missing individual perspectives. We propose FLEAD (Federated Learning for Exploiting Annotators’ Disagreements), a methodology built upon federated learning to independently learn from the opinions of all the annotators, thereby leveraging all their underlying information without relying on a single ground truth. We conduct an extensive experimental study and analysis in diverse text classification tasks to show the contribution of our approach with respect to mainstream approaches based on majority voting and other recent methodologies that also learn from annotator disagreements.</abstract>
      <pages>630–648</pages>
      <url hash="97eb9928">2024.tacl-1.35</url>
      <bibkey>rodriguez-barroso-etal-2024-federated</bibkey>
    </paper>
    <paper id="36">
      <title>Computational Complexity of Natural Morphology Revisited</title>
      <author><first>Hajime</first><last>Senuma</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <doi>10.1162/tacl_a_00665</doi>
      <abstract>This paper revisits a classical, yet fundamental, discussion of theoretical computational linguistics: the computational complexity of natural languages. Past studies have revealed that syntax, as observed in Swiss-German, is not weakly context-free. Concerning morphology, Culy (1985) employed a construction in Bambara to show that morphology is not weakly context-free; however, Manaster-Ramer (1988) pointed out that the Bambara case can be problematic because the wordhood of the construction is reliant on special tonal behaviors, and it is ambiguous whether the behaviors belong to the morphological domain. This raises doubts about whether the case can be considered a genuine morphological phenomenon. In this paper, we argue that Classical Ainu, a language we examine, also defies weak context-freeness at the morphological level. The construction we introduce is unambiguously morphological because this language’s valency-sensitive structure and valency-changing operations, such as noun incorporation, preclude its grammatical interpretation as syntactic.</abstract>
      <pages>649–663</pages>
      <url hash="2722dceb">2024.tacl-1.36</url>
      <bibkey>senuma-aizawa-2024-computational</bibkey>
    </paper>
    <paper id="37">
      <title>Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis</title>
      <author><first>Sohee</first><last>Yang</last></author>
      <author><first>Jonghyeon</first><last>Kim</last></author>
      <author><first>Joel</first><last>Jang</last></author>
      <author><first>Seonghyeon</first><last>Ye</last></author>
      <author><first>Hyunji</first><last>Lee</last></author>
      <author><first>Minjoon</first><last>Seo</last></author>
      <doi>10.1162/tacl_a_00666</doi>
      <abstract>Previous work in prompt engineering for large language models has introduced different gradient-free probability-based prompt selection methods that aim to choose the optimal prompt among the candidates for a given task but have failed to provide a comprehensive and fair comparison between each other. In this paper, we propose a unified framework to interpret and evaluate the existing probability-based prompt selection methods by performing extensive experiments on 13 common and diverse NLP tasks. We find that each of the existing methods can be interpreted as some variant of the method that maximizes mutual information between the input and the predicted output (MI). Utilizing this finding, we develop several other combinatorial variants of MI and increase the effectiveness of the oracle prompt selection method from 87.79% to 94.98%, measured as the ratio of the performance of the selected prompt to that of the optimal oracle prompt. Furthermore, considering that all the methods rely on the output probability distribution of the model that might be biased, we propose a novel calibration method called Calibration by Marginalization (CBM) that is orthogonal to the existing methods and helps increase the prompt selection effectiveness of the best method to 96.85%, achieving 99.44% of the oracle prompt F1 without calibration.1</abstract>
      <pages>664–680</pages>
      <url hash="9cb84511">2024.tacl-1.37</url>
      <bibkey>yang-etal-2024-improving</bibkey>
    </paper>
    <paper id="38">
      <title>Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering</title>
      <author><first>Vaibhav</first><last>Adlakha</last></author>
      <author><first>Parishad</first><last>BehnamGhader</last></author>
      <author><first>Xing Han</first><last>Lu</last></author>
      <author><first>Nicholas</first><last>Meade</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <doi>10.1162/tacl_a_00667</doi>
      <abstract>Instruction-following models are attractive alternatives to fine-tuned approaches for question answering (QA). By simply prepending relevant documents and an instruction to their input, these models can be adapted to various information domains and tasks without additional training. However, these models tend to produce verbose responses with supplementary information, which makes traditional QA metrics like exact match (EM) and F1 unreliable for accurately quantifying model performance. In this work, we evaluate instruction-following models along two fronts: 1) how well they satisfy user’s information need (correctness), and 2) whether they disseminate information supported by the provided knowledge (faithfulness). Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and faithfulness and propose simple token-overlap metrics that correlate highly with human judgments. Our analysis reveals that for correctness, instruction-following models perform comparably to models specifically fine-tuned for that task. However, they struggle to accurately judge the relevance of the provided knowledge and often hallucinate in their responses. We hope our work encourages more holistic evaluation of instruction-following models for QA. Our code and human annotation data is available at https://github.com/McGill-NLP/instruct-qa.</abstract>
      <pages>681–699</pages>
      <url hash="56cf2300">2024.tacl-1.38</url>
      <bibkey>adlakha-etal-2024-evaluating</bibkey>
    </paper>
    <paper id="39">
      <title>The Ethics of Automating Legal Actors</title>
      <author><first>Josef</first><last>Valvoda</last></author>
      <author><first>Alec</first><last>Thompson</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <doi>10.1162/tacl_a_00668</doi>
      <abstract>The introduction of large public legal datasets has brought about a renaissance in legal NLP. Many of these datasets are composed of legal judgments—the product of judges deciding cases. Since ML algorithms learn to model the data they are trained on, several legal NLP models are models of judges. While some have argued for the automation of judges, in this position piece, we argue that automating the role of the judge raises difficult ethical challenges, in particular for common law legal systems. Our argument follows from the social role of the judge in actively shaping the law, rather than merely applying it. Since current NLP models are too far away from having the facilities necessary for this task, they should not be used to automate judges. Furthermore, even in the case that the models could achieve human-level capabilities, there would still be remaining ethical concerns inherent in the automation of the legal process.</abstract>
      <pages>700–720</pages>
      <url hash="cdeed0bf">2024.tacl-1.39</url>
      <bibkey>valvoda-etal-2024-ethics</bibkey>
    </paper>
    <paper id="40">
      <title>Source-Free Domain Adaptation for Question Answering with Masked Self-training</title>
      <author><first>Maxwell J.</first><last>Yin</last></author>
      <author><first>Boyu</first><last>Wang</last></author>
      <author><first>Yue</first><last>Dong</last></author>
      <author><first>Charles</first><last>Ling</last></author>
      <doi>10.1162/tacl_a_00669</doi>
      <abstract>Previous unsupervised domain adaptation (UDA) methods for question answering (QA) require access to source domain data while fine-tuning the model for the target domain. Source domain data may, however, contain sensitive information and should be protected. In this study, we investigate a more challenging setting, source-free UDA, in which we have only the pretrained source model and target domain data, without access to source domain data. We propose a novel self-training approach to QA models that integrates a specially designed mask module for domain adaptation. The mask is auto-adjusted to extract key domain knowledge when trained on the source domain. To maintain previously learned domain knowledge, certain mask weights are frozen during adaptation, while other weights are adjusted to mitigate domain shifts with pseudo-labeled samples generated in the target domain. Our empirical results on four benchmark datasets suggest that our approach significantly enhances the performance of pretrained QA models on the target domain, and even outperforms models that have access to the source data during adaptation.</abstract>
      <pages>721–737</pages>
      <url hash="2580a8ab">2024.tacl-1.40</url>
      <bibkey>yin-etal-2024-source-free</bibkey>
    </paper>
    <paper id="41">
      <title>Scope Ambiguities in Large Language Models</title>
      <author><first>Gaurav</first><last>Kamath</last></author>
      <author><first>Sebastian</first><last>Schuster</last></author>
      <author><first>Sowmya</first><last>Vajjala</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <doi>10.1162/tacl_a_00670</doi>
      <abstract>Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities. These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing. Despite this, there has been little research into how modern large language models treat them. In this paper, we investigate how different versions of certain autoregressive language models—GPT-2, GPT-3/3.5, Llama 2, and GPT-4—treat scope ambiguous sentences, and compare this with human judgments. We introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments. Using these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments, and (ii) can successfully identify human-preferred readings at a high level of accuracy (over 90% in some cases).1</abstract>
      <pages>738–754</pages>
      <url hash="cbd94e1a">2024.tacl-1.41</url>
      <bibkey>kamath-etal-2024-scope</bibkey>
    </paper>
    <paper id="42">
      <title>Visually Grounded Speech Models Have a Mutual Exclusivity Bias</title>
      <author><first>Leanne</first><last>Nortje</last></author>
      <author><first>Dan</first><last>Oneaţă</last></author>
      <author><first>Yevgen</first><last>Matusevych</last></author>
      <author><first>Herman</first><last>Kamper</last></author>
      <doi>10.1162/tacl_a_00672</doi>
      <abstract>When children learn new words, they employ constraints such as the mutual exclusivity (ME) bias: A novel word is mapped to a novel object rather than a familiar one. This bias has been studied computationally, but only in models that use discrete word representations as input, ignoring the high variability of spoken words. We investigate the ME bias in the context of visually grounded speech models that learn from natural images and continuous speech audio. Concretely, we train a model on familiar words and test its ME bias by asking it to select between a novel and a familiar object when queried with a novel word. To simulate prior acoustic and visual knowledge, we experiment with several initialization strategies using pretrained speech and vision networks. Our findings reveal the ME bias across the different initialization approaches, with a stronger bias in models with more prior (in particular, visual) knowledge. Additional tests confirm the robustness of our results, even when different loss functions are considered. Based on detailed analyses to piece out the model’s representation space, we attribute the ME bias to how familiar and novel classes are distinctly separated in the resulting space.</abstract>
      <pages>755–770</pages>
      <url hash="72f755b3">2024.tacl-1.42</url>
      <bibkey>nortje-etal-2024-visually</bibkey>
    </paper>
    <paper id="43">
      <title>Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias</title>
      <author><first>Itay</first><last>Itzhak</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <author><first>Nir</first><last>Rosenfeld</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <doi>10.1162/tacl_a_00673</doi>
      <abstract>Recent studies show that instruction tuning (IT) and reinforcement learning from human feedback (RLHF) improve the abilities of large language models (LMs) dramatically. While these tuning methods can help align models with human objectives and generate high-quality text, not much is known about their potential adverse effects. In this work, we investigate the effect of IT and RLHF on decision making and reasoning in LMs, focusing on three cognitive biases—the decoy effect, the certainty effect, and the belief bias—all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, Mistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.1</abstract>
      <pages>771–785</pages>
      <url hash="3e4e4f2f">2024.tacl-1.43</url>
      <bibkey>itzhak-etal-2024-instructed</bibkey>
    </paper>
    <paper id="44">
      <title>Beyond Boundaries: A Human-like Approach for Question Answering over Structured and Unstructured Information Sources</title>
      <author><first>Jens</first><last>Lehmann</last></author>
      <author><first>Dhananjay</first><last>Bhandiwad</last></author>
      <author><first>Preetam</first><last>Gattogi</last></author>
      <author><first>Sahar</first><last>Vahdati</last></author>
      <doi>10.1162/tacl_a_00671</doi>
      <abstract>Answering factual questions from heterogenous sources, such as graphs and text, is a key capacity of intelligent systems. Current approaches either (i) perform question answering over text and structured sources as separate pipelines followed by a merge step or (ii) provide an early integration, giving up the strengths of particular information sources. To solve this problem, we present “HumanIQ”, a method that teaches language models to dynamically combine retrieved information by imitating how humans use retrieval tools. Our approach couples a generic method for gathering human demonstrations of tool use with adaptive few-shot learning for tool augmented models. We show that HumanIQ confers significant benefits, including i) reducing the error rate of our strongest baseline (GPT-4) by over 50% across 3 benchmarks, (ii) improving human preference over responses from vanilla GPT-4 (45.3% wins, 46.7% ties, 8.0% loss), and (iii) outperforming numerous task-specific baselines.</abstract>
      <pages>786–802</pages>
      <url hash="a64ff5ef">2024.tacl-1.44</url>
      <bibkey>lehmann-etal-2024-beyond</bibkey>
    </paper>
  </volume>
</collection>
