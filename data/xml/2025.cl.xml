<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.cl">
  <volume id="1" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 51, Issue 1 - March 2025</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>March</month>
      <year>2025</year>
      <venue>cl</venue>
      <journal-volume>51</journal-volume>
      <journal-issue>1</journal-issue>
    </meta>
    <paper id="1">
      <title>Opening a New Chapter for Computational Linguistics</title>
      <author><first>Wei</first><last>Lu</last></author>
      <doi>10.1162/coli_e_00552</doi>
      <abstract>By the end of 2024, the journal Computational Linguistics has reached a significant milestone: It has published exactly 50 volumes over the past half-century. As we launch the first issue of Volume 51, this is an opportune moment to reflect on the journal’s legacy, ongoing evolution, and the exciting changes that lie ahead. Together, we embark on a journey to open a new chapter for this storied publication.</abstract>
      <pages>1–5</pages>
      <url hash="75bed9d3">2025.cl-1.1</url>
      <bibkey>lu-2025-opening</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>MUC</fixed-case>king In, or Fifty Years in Information Extraction</title>
      <author id="ralph-grishman"><first>Ralph</first><last>Grishman</last></author>
      <doi>10.1162/coli_a_00547</doi>
      <abstract>I want to thank the ACL for this Lifetime Achievement Award. I am deeply honored to be receiving it. I would also like to thank the students, faculty, and researchers who were members of the Proteus Project during most of my professional lifetime. It was an honor to serve that group.</abstract>
      <pages>7–22</pages>
      <url hash="eab7a061">2025.cl-1.2</url>
      <bibkey>grishman-2025-mucking</bibkey>
    </paper>
    <paper id="3">
      <title>e<fixed-case>RST</fixed-case>: A Signaled Graph Theory of Discourse Relations and Organization</title>
      <author><first>Amir</first><last>Zeldes</last></author>
      <author><first>Tatsuya</first><last>Aoyama</last></author>
      <author id="yang-janet-liu"><first>Yang Janet</first><last>Liu</last></author>
      <author><first>Siyao</first><last>Peng</last></author>
      <author><first>Debopam</first><last>Das</last></author>
      <author><first>Luke</first><last>Gessler</last></author>
      <doi>10.1162/coli_a_00538</doi>
      <abstract>In this article we present Enhanced Rhetorical Structure Theory (eRST), a new theoretical framework for computational discourse analysis, based on an expansion of Rhetorical Structure Theory (RST). The framework encompasses discourse relation graphs with tree-breaking, non-projective and concurrent relations, as well as implicit and explicit signals which give explainable rationales to our analyses. We survey shortcomings of RST and other existing frameworks, such as Segmented Discourse Representation Theory, the Penn Discourse Treebank, and Discourse Dependencies, and address these using constructs in the proposed theory. We provide annotation, search, and visualization tools for data, and present and evaluate a freely available corpus of English annotated according to our framework, encompassing 12 spoken and written genres with over 200K tokens. Finally, we discuss automatic parsing, evaluation metrics, and applications for data in our framework.</abstract>
      <pages>23–72</pages>
      <url hash="e1ebaaf8">2025.cl-1.3</url>
      <bibkey>zeldes-etal-2025-erst</bibkey>
    </paper>
    <paper id="4">
      <title>Machine Translation Meta Evaluation through Translation Accuracy Challenge Sets</title>
      <author><first>Nikita</first><last>Moghe</last></author>
      <author><first>Arnisa</first><last>Fazla</last></author>
      <author><first>Chantal</first><last>Amrhein</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author id="mark-steedman"><first>Mark</first><last>Steedman</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Liane</first><last>Guillou</last></author>
      <doi>10.1162/coli_a_00537</doi>
      <abstract>Recent machine translation (MT) metrics calibrate their effectiveness by correlating with human judgment. However, these results are often obtained by averaging predictions across large test sets without any insights into the strengths and weaknesses of these metrics across different error types. Challenge sets are used to probe specific dimensions of metric behavior but there are very few such datasets and they either focus on a limited number of phenomena or a limited number of language pairs. We introduce ACES, a contrastive challenge set spanning 146 language pairs, aimed at discovering whether metrics can identify 68 translation accuracy errors. These phenomena range from basic alterations at the word/character level to more intricate errors based on discourse and real-world knowledge. We conducted a large-scale study by benchmarking ACES on 47 metrics submitted to the WMT 2022 and WMT 2023 metrics shared tasks. We also measure their sensitivity to a range of linguistic phenomena. We further investigate claims that large language models (LLMs) are effective as MT evaluators, addressing the limitations of previous studies by using a dataset that covers a range of linguistic phenomena and language pairs and includes both low- and medium-resource languages. Our results demonstrate that different metric families struggle with different phenomena and that LLM-based methods are unreliable. We expose a number of major flaws with existing methods: Most metrics ignore the source sentence; metrics tend to prefer surface level overlap; and over-reliance on language-agnostic representations leads to confusion when the target language is similar to the source language. To further encourage detailed evaluation beyond singular scores, we expand ACES to include error span annotations, denoted as SPAN-ACES, and we use this dataset to evaluate span-based error metrics, showing that these metrics also need considerable improvement. Based on our observations, we provide a set of recommendations for building better MT metrics, including focusing on error labels instead of scores, ensembling, designing metrics to explicitly focus on the source sentence, focusing on semantic content rather than relying on the lexical overlap, and choosing the right pre-trained model for obtaining representations.</abstract>
      <pages>73–137</pages>
      <url hash="62052024">2025.cl-1.4</url>
      <bibkey>moghe-etal-2025-machine</bibkey>
    </paper>
    <paper id="5">
      <title>Compositionality and Sentence Meaning: Comparing Semantic Parsing and Transformers on a Challenging Sentence Similarity Dataset</title>
      <author><first>James</first><last>Fodor</last></author>
      <author><first>Simon De</first><last>Deyne</last></author>
      <author><first>Shinsuke</first><last>Suzuki</last></author>
      <doi>10.1162/coli_a_00536</doi>
      <abstract>One of the major outstanding questions in computational semantics is how humans integrate the meaning of individual words into a sentence in a way that enables understanding of complex and novel combinations of words, a phenomenon known as compositionality. Many approaches to modeling the process of compositionality can be classified as either “vector-based” models, in which the meaning of a sentence is represented as a vector of numbers, or “syntax-based” models, in which the meaning of a sentence is represented as a structured tree of labeled components. A major barrier in assessing and comparing these contrasting approaches is the lack of large, relevant datasets for model comparison. This article aims to address this gap by introducing a new dataset, STS3k, which consists of 2,800 pairs of sentences rated for semantic similarity by human participants. The sentence pairs have been selected to systematically vary different combinations of words, providing a rigorous test and enabling a clearer picture of the comparative strengths and weaknesses of vector-based and syntax-based methods. Our results show that when tested on the new STS3k dataset, state-of-the-art transformers poorly capture the pattern of human semantic similarity judgments, while even simple methods for combining syntax- and vector-based components into a novel hybrid model yield substantial improvements. We further show that this improvement is due to the ability of the hybrid model to replicate human sensitivity to specific changes in sentence structure. Our findings provide evidence for the value of integrating multiple methods to better reflect the way in which humans mentally represent compositional meaning.</abstract>
      <pages>139–190</pages>
      <url hash="83fd8b18">2025.cl-1.5</url>
      <bibkey>fodor-etal-2025-compositionality</bibkey>
    </paper>
    <paper id="6">
      <title>Evaluating Synthetic Data Generation from User Generated Text</title>
      <author><first>Jenny</first><last>Chim</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <doi>10.1162/coli_a_00540</doi>
      <abstract>User-generated content provides a rich resource to study social and behavioral phenomena. Although its application potential is currently limited by the paucity of expert labels and the privacy risks inherent in personal data, synthetic data can help mitigate this bottleneck. In this work, we introduce an evaluation framework to facilitate research on synthetic language data generation for user-generated text. We define a set of aspects for assessing data quality, namely, style preservation, meaning preservation, and divergence, as a proxy for privacy. We introduce metrics corresponding to each aspect. Moreover, through a set of generation strategies and representative tasks and baselines across domains, we demonstrate the relation between the quality aspects of synthetic user generated content, generation strategies, metrics, and downstream performance. To our knowledge, our work is the first unified evaluation framework for user-generated text in relation to the specified aspects, offering both intrinsic and extrinsic evaluation. We envisage it will facilitate developments towards shareable, high-quality synthetic language data.</abstract>
      <pages>191–233</pages>
      <url hash="76099fc1">2025.cl-1.6</url>
      <bibkey>chim-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="7">
      <title>Neural Semantic Parsing with Extremely Rich Symbolic Meaning Representations</title>
      <author><first>Xiao</first><last>Zhang</last></author>
      <author><first>Gosse</first><last>Bouma</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <doi>10.1162/coli_a_00542</doi>
      <abstract>Current open-domain neural semantics parsers show impressive performance. However, closer inspection of the symbolic meaning representations they produce reveals significant weaknesses: Sometimes they tend to merely copy character sequences from the source text to form symbolic concepts, defaulting to the most frequent word sense based in the training distribution. By leveraging the hierarchical structure of a lexical ontology, we introduce a novel compositional symbolic representation for concepts based on their position in the taxonomical hierarchy. This representation provides richer semantic information and enhances interpretability. We introduce a neural “taxonomical” semantic parser to utilize this new representation system of predicates, and compare it with a standard neural semantic parser trained on the traditional meaning representation format, employing a novel challenge set and evaluation metric for evaluation. Our experimental findings demonstrate that the taxonomical model, trained on much richer and complex meaning representations, is slightly subordinate in performance to the traditional model using the standard metrics for evaluation, but outperforms it when dealing with out-of-vocabulary concepts. We further show through neural model probing that training on a taxonomic representation enhances the model’s ability to learn the taxonomical hierarchy. This finding is encouraging for research in computational semantics that aims to combine data-driven distributional meanings with knowledge-based symbolic representations.</abstract>
      <pages>235–274</pages>
      <url hash="3fe0fcd7">2025.cl-1.7</url>
      <bibkey>zhang-etal-2025-neural</bibkey>
    </paper>
    <paper id="8">
      <title>A Survey on <fixed-case>LLM</fixed-case>-Generated Text Detection: Necessity, Methods, and Future Directions</title>
      <author><first>Junchao</first><last>Wu</last></author>
      <author><first>Shu</first><last>Yang</last></author>
      <author><first>Runzhe</first><last>Zhan</last></author>
      <author><first>Yulin</first><last>Yuan</last></author>
      <author><first>Lidia Sam</first><last>Chao</last></author>
      <author><first>Derek Fai</first><last>Wong</last></author>
      <doi>10.1162/coli_a_00549</doi>
      <abstract>The remarkable ability of large language models (LLMs) to comprehend, interpret, and generate complex language has rapidly integrated LLM-generated text into various aspects of daily life, where users increasingly accept it. However, the growing reliance on LLMs underscores the urgent need for effective detection mechanisms to identify LLM-generated text. Such mechanisms are critical to mitigating misuse and safeguarding domains like artistic expression and social networks from potential negative consequences. LLM-generated text detection, conceptualized as a binary classification task, seeks to determine whether an LLM produced a given text. Recent advances in this field stem from innovations in watermarking techniques, statistics-based detectors, and neural-based detectors. Human-assisted methods also play a crucial role. In this survey, we consolidate recent research breakthroughs in this field, emphasizing the urgent need to strengthen detector research. Additionally, we review existing datasets, highlighting their limitations and developmental requirements. Furthermore, we examine various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, real-world data issues, and ineffective evaluation frameworks. Finally, we outline intriguing directions for future research in LLM-generated text detection to advance responsible artificial intelligence. This survey aims to provide a clear and comprehensive introduction for newcomers while offering seasoned researchers valuable updates in the field.1</abstract>
      <pages>275–338</pages>
      <url hash="be1eb645">2025.cl-1.8</url>
      <bibkey>wu-etal-2025-survey</bibkey>
    </paper>
    <paper id="9">
      <title>Automatic Language Identification in Texts</title>
      <author><first>Tom</first><last>Lippincott</last></author>
      <doi>10.1162/coli_r_00521</doi>
      <pages>339–341</pages>
      <url hash="ece5801a">2025.cl-1.9</url>
      <bibkey>lippincott-2025-automatic</bibkey>
    </paper>
  </volume>
  <volume id="2" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 51, Issue 2 - June 2025</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>June</month>
      <year>2025</year>
      <venue>cl</venue>
      <journal-volume>51</journal-volume>
    </meta>
    <paper id="1">
      <title><fixed-case>D</fixed-case>i<fixed-case>B</fixed-case>i<fixed-case>MT</fixed-case>: A Gold Evaluation Benchmark for Studying Lexical Ambiguity in Machine Translation</title>
      <author><first>Federico</first><last>Martelli</last></author>
      <author><first>Stefano</first><last>Perrella</last></author>
      <author><first>Niccolò</first><last>Campolungo</last></author>
      <author><first>Tina</first><last>Munda</last></author>
      <author><first>Svetla</first><last>Koeva</last></author>
      <author><first>Carole</first><last>Tiberius</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <doi>10.1162/coli_a_00541</doi>
      <abstract>Despite the remarkable progress made in the field of Machine Translation (MT), current systems still struggle when translating ambiguous words, especially when these express infrequent meanings. In order to investigate and analyze the impact of lexical ambiguity on automatic translations, several tasks and evaluation benchmarks have been proposed over the course of the last few years. However, work in this research direction suffers from critical shortcomings. Indeed, existing evaluation datasets are not entirely manually curated, which significantly compromises their reliability. Furthermore, current literature fails to provide detailed insights into the nature of the errors produced by models translating ambiguous words, lacking a thorough manual analysis across languages. With a view to overcoming these limitations, we propose Disambiguation Biases in MT (DiBiMT), an entirely manually curated evaluation benchmark for investigating disambiguation biases in eight language combinations and assessing the ability of both commercial and non-commercial systems to handle ambiguous words. We also examine and detail the errors produced by models in this scenario by carrying out a manual error analysis in all language pairs. Additionally, we perform an extensive array of experiments aimed at studying the behavior of models when dealing with ambiguous words. Finally, we show the ineffectiveness of standard MT evaluation settings for assessing the disambiguation capabilities of systems and highlight the need for additional efforts in this research direction and ad-hoc testbeds such as DiBiMT. Our benchmark is available at: https://nlp.uniroma1.it/dibimt/.</abstract>
      <pages>343–413</pages>
      <url hash="e5d30ec7">2025.cl-2.1</url>
      <bibkey>martelli-etal-2025-dibimt</bibkey>
    </paper>
    <paper id="2">
      <title>Train and Constrain: Phonologically Informed Tongue Twister Generation from Topics and Paraphrases</title>
      <author><first>Tyler</first><last>Loakman</last></author>
      <author><first>Chen</first><last>Tang</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <doi>10.1162/coli_a_00544</doi>
      <abstract>Previous work in phonologically and phonetically grounded language generation has mainly focused on domains such as puns and poetry. In this article, we present new work on the generation of English tongue twisters—a form of language that is required to be conditioned on a phoneme level to maximize sound overlap, while maintaining semantic consistency with an input topic or phrase and still being grammatically correct. We present TwisterLister, a pipeline for generating phonologically informed tongue twisters from large language models (LLMs) that we use to generate TwistList 2.0, the largest annotated dataset of tongue twisters to date, consisting of 17k+ examples from a combination of human and LLM authors. Our generation pipeline involves the use of a phonologically constrained vocabulary alongside LLM prompting to generate novel, non-derivative tongue twister examples. We additionally present the results of automatic and human evaluation of smaller models trained on our generated dataset to demonstrate the extent to which phonologically motivated language types can be generated without explicit injection of phonological knowledge. Additionally, we introduce a phoneme-aware constrained decoding module (PACD) that can be integrated into an autoregressive language model and demonstrate that this method generates good quality tongue twisters both with and without fine-tuning the underlying language model. We also design and implement a range of automatic metrics for the task of tongue twister generation that is phonologically motivated and captures the unique essence of tongue twisters, primarily based on phonemic edit distance (PED).1</abstract>
      <pages>415–466</pages>
      <url hash="c4dc3cd1">2025.cl-2.2</url>
      <bibkey>loakman-etal-2025-train</bibkey>
    </paper>
    <paper id="3">
      <title>Eliciting and Improving the Causal Reasoning Abilities of Large Language Models with Conditional Statements</title>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <doi>10.1162/coli_a_00548</doi>
      <abstract>Causal reasoning, the ability to identify cause-and-effect relationships, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Complex causal structures are rarely expressed explicitly in the text, which could make learning them challenging for LLMs. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like if, we want to explore whether large language models of code (Code-LLMs) acquire better causal reasoning abilities, and whether code prompts better describe the causal structure than text prompts. Our experiments show that compared with general-purpose LLMs like Llama-2 and GPT-3, Code-LLMs like CodeLlama and Codex are significantly better in causal reasoning. Code prompts not only work well for Code-LLMs, but also help improve the performance of most general-purpose LLMs. To understand why code prompts are effective, we intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while models are more robust towards format perturbations. We further explore whether exposing models with more code with conditional statements aids in enhancing causal reasoning abilities. We finetune LLMs on such code corpus, and find their performance improves when prompted with either code prompts or text prompts.1</abstract>
      <pages>467–504</pages>
      <url hash="d2259610">2025.cl-2.3</url>
      <bibkey>liu-etal-2025-eliciting</bibkey>
    </paper>
    <paper id="4">
      <title>Investigating Idiomaticity in Word Representations</title>
      <author><first>Wei</first><last>He</last></author>
      <author><first>Tiago Kramer</first><last>Vieira</last></author>
      <author><first>Marcos</first><last>Garcia</last></author>
      <author id="carolina-scarton"><first>Carolina</first><last>Scarton</last></author>
      <author><first>Marco</first><last>Idiart</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <doi>10.1162/coli_a_00546</doi>
      <abstract>Idiomatic expressions are an integral part of human languages, often used to express complex ideas in compressed or conventional ways (e.g., eager beaver as a keen and enthusiastic person). However, their interpretations may not be straightforwardly linked to the meanings of their individual components in isolation and this may have an impact for compositional approaches. In this article, we investigate to what extent word representation models are able to go beyond compositional word combinations and capture multiword expression idiomaticity and some of the expected properties related to idiomatic meanings. We focus on noun compounds of varying levels of idiomaticity in two languages (English and Portuguese), presenting a dataset of minimal pairs containing human idiomaticity judgments for each noun compound at both type and token levels, their paraphrases and their occurrences in naturalistic and sense-neutral contexts, totalling 32,200 sentences. We propose this set of minimal pairs for evaluating how well a model captures idiomatic meanings, and define a set of fine-grained metrics of Affinity and Scaled Similarity, to determine how sensitive the models are to perturbations that may lead to changes in idiomaticity. Affinity is a comparative measure of the similarity between an experimental item, a target and a potential distractor, and Scaled Similarity incorporates a rescaling factor to magnify the meaningful similarities within the spaces defined by each specific model. The results obtained with a variety of representative and widely used models indicate that, despite superficial indications to the contrary in the form of high similarities, idiomaticity is not yet accurately represented in current models. Moreover, the performance of models with different levels of contextualization suggests that their ability to capture context is not yet able to go beyond more superficial lexical clues provided by the words and to actually incorporate the relevant semantic clues needed for idiomaticity. By proposing model-agnostic measures for assessing the ability of models to capture idiomaticity, this article contributes to determining limitations in the handling of non-compositional structures, which is one of the directions that needs to be considered for more natural, accurate, and robust language understanding. The source code and additional materials related to this paper are available at our GitHub repository.1</abstract>
      <pages>505–555</pages>
      <url hash="f42c98f5">2025.cl-2.4</url>
      <bibkey>he-etal-2025-investigating</bibkey>
    </paper>
    <paper id="5">
      <title>Dotless <fixed-case>A</fixed-case>rabic Text for Natural Language Processing</title>
      <author><first>Maged S.</first><last>Al-Shaibani</last></author>
      <author><first>Irfan</first><last>Ahmad</last></author>
      <doi>10.1162/coli_a_00535</doi>
      <abstract>This article introduces a novel representation of Arabic text as an alternative approach for Arabic NLP, inspired by the dotless script of ancient Arabic. We explored this representation through extensive analysis on various text corpora, differing in size and domain, and tokenized using multiple tokenization techniques. Furthermore, we examined the information density of this representation and compared it with the standard dotted Arabic text using text entropy analysis. Utilizing parallel corpora, we also drew comparisons between Arabic and English text analysis to gain additional insights. Our investigation extended to various upstream and downstream NLP tasks, including language modeling, text classification, sequence labeling, and machine translation, examining the implications of both the representations. Specifically, we performed seven different downstream tasks using various tokenization schemes comparing the standard dotted text with dotless Arabic text representations. Performance using both the representations was comparable across different tokenizations. However, dotless representation achieves these results with significant reduction in vocabulary sizes, and in some scenarios showing reduction of up to 50%. Additionally, we present a system that restores dots to the dotless Arabic text. This system is useful for tasks that require Arabic texts as output.</abstract>
      <pages>557–598</pages>
      <url hash="3fff5735">2025.cl-2.5</url>
      <bibkey>al-shaibani-ahmad-2025-dotless</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>LMLPA</fixed-case>: Language Model Linguistic Personality Assessment</title>
      <author><first>Jingyao</first><last>Zheng</last></author>
      <author><first>Xian</first><last>Wang</last></author>
      <author><first>Simo</first><last>Hosio</last></author>
      <author><first>Xiaoxian</first><last>Xu</last></author>
      <author><first>Lik-Hang</first><last>Lee</last></author>
      <doi>10.1162/coli_a_00550</doi>
      <abstract>Large language models (LLMs) are increasingly used in everyday life and research. One of the most common use cases is conversational interactions, enabled by the language generation capabilities of LLMs. Just as between two humans, a conversation between an LLM-powered entity and a human depends on the personality of the conversants. However, measuring the personality of a given LLM is currently a challenge. This article introduces the Language Model Linguistic Personality Assessment (LMLPA), a system designed to evaluate the linguistic personalities of LLMs. Our system helps to understand LLMs’ language generation capabilities by quantitatively assessing the distinct personality traits reflected in their linguistic outputs. Unlike traditional human-centric psychometrics, the LMLPA adapts a personality assessment questionnaire, specifically the Big Five Inventory, to align with the operational capabilities of LLMs, and also incorporates the findings from previous language-based personality measurement literature. To mitigate sensitivity to the order of options, our questionnaire is designed to be open-ended, resulting in textual answers. Thus, the Artificial Intelligence (AI) rater is needed to transform ambiguous personality information from text responses into clear numerical indicators of personality traits. Utilizing Principal Component Analysis and reliability validation methods, our findings demonstrate that LLMs possess distinct personality traits that can be effectively quantified by the LMLPA. This research contributes to Human-Centered AI and Computational Linguistics, providing a robust framework for future studies to refine AI personality assessments and expand their applications in multiple areas, including education and manufacturing.</abstract>
      <pages>599–640</pages>
      <url hash="f188c540">2025.cl-2.6</url>
      <bibkey>zheng-etal-2025-lmlpa</bibkey>
    </paper>
    <paper id="7">
      <title>Kallini et al. (2024) Do Not Compare Impossible Languages with Constituency-based Ones</title>
      <author><first>Tim</first><last>Hunter</last></author>
      <doi>10.1162/coli_a_00554</doi>
      <abstract>A central goal of linguistic theory is to find a precise characterization of the notion “possible human language”, in the form of a computational device that is capable of describing all and only the languages that can be acquired by a typically developing human child. The success of recent large language models (LLMs) in NLP applications arguably raises the possibility that LLMs might be computational devices that meet this goal. This would only be the case if, in addition to succeeding in learning human languages, LLMs struggle to learn “impossible” human languages. Kallini et al. (2024) conducted experiments aiming to test this by training GPT-2 on a variety of synthetic languages, and found that it learns some more successfully than others. They present these asymmetries as support for the idea that LLMs’ inductive biases align with what is regarded as “possible” for human languages, but the most significant comparison has a confound that makes this conclusion unwarranted.</abstract>
      <pages>641–650</pages>
      <url hash="4026238a">2025.cl-2.7</url>
      <bibkey>hunter-2025-kallini</bibkey>
    </paper>
    <paper id="8">
      <title>Language Models and Externalism: A Reply to Mandelkern and <fixed-case>L</fixed-case>inzen</title>
      <author><first>Gary</first><last>Ostertag</last></author>
      <doi>10.1162/coli_a_00551</doi>
      <abstract>Do texts generated by language models (LMs) refer? Mandelkern and Linzen (2024) argue that externalist principles point to an affirmative conclusion. What grounds reference, according to their externalism, is a term’s “natural history”. For example, ‘water’ refers to H2O among English speakers, and not to the phenomenally indistinguishable chemical XYZ, because H2O, and not XYZ, is implicated in the natural history of ‘water’. Appealing to the literature on contrastive explanation, I show that a term’s natural history does not generally ground its referential properties. Thus, Mandelkern and Linzen’s quick route to the referentiality of LM-generated texts fails.</abstract>
      <pages>651–659</pages>
      <url hash="d31f7d75">2025.cl-2.8</url>
      <bibkey>ostertag-2025-language</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>LLM</fixed-case>-based <fixed-case>NLG</fixed-case> Evaluation: Current Status and Challenges</title>
      <author><first>Mingqi</first><last>Gao</last></author>
      <author><first>Xinyu</first><last>Hu</last></author>
      <author><first>Xunjian</first><last>Yin</last></author>
      <author><first>Jie</first><last>Ruan</last></author>
      <author><first>Xiao</first><last>Pu</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <doi>10.1162/coli_a_00561</doi>
      <abstract>Evaluating natural language generation (NLG) is a vital but challenging problem in natural language processing. Traditional evaluation metrics mainly capturing content (e.g., n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human–LLM collaborative evaluation. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. Lastly, we discuss several open problems in this area and point out future research directions.</abstract>
      <pages>661–687</pages>
      <url hash="c9576b8c">2025.cl-2.9</url>
      <bibkey>gao-etal-2025-llm</bibkey>
    </paper>
    <paper id="10">
      <title>Socially Aware Language Technologies: Perspectives and Practices</title>
      <author><first>Diyi</first><last>Yang</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <author id="barbara-plank"><first>Barbara</first><last>Plank</last></author>
      <doi>10.1162/coli_a_00556</doi>
      <abstract>Language technologies have advanced substantially, particularly with the introduction of large language models. However, these advancements can exacerbate several issues that models have traditionally faced, including bias, evaluation, and risk. In this perspective piece, we argue that many of these issues share a common core: a lack of awareness of the social factors, interactions, and implications of the social environment in which NLP operates. We call this social awareness. While NLP is improving at addressing linguistic issues, there has been relatively limited progress in incorporating social awareness into models to work in all situations for all users. Integrating social awareness into NLP will improve the naturalness, usefulness, and safety of applications while also opening up new applications. Today, we are only at the start of a new, important era in the field.</abstract>
      <pages>689–703</pages>
      <url hash="10dcc07d">2025.cl-2.10</url>
      <bibkey>yang-etal-2025-socially</bibkey>
    </paper>
  </volume>
</collection>
