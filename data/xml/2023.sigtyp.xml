<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.sigtyp">
  <volume id="1" ingest-date="2023-04-29" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP</booktitle>
      <editor><first>Lisa</first><last>Beinborn</last></editor>
      <editor><first>Koustava</first><last>Goswami</last></editor>
      <editor><first>Saliha</first><last>Muradoğlu</last></editor>
      <editor><first>Alexey</first><last>Sorokin</last></editor>
      <editor><first>Ritesh</first><last>Kumar</last></editor>
      <editor><first>Andreas</first><last>Shcherbakov</last></editor>
      <editor><first>Edoardo M.</first><last>Ponti</last></editor>
      <editor><first>Ryan</first><last>Cotterell</last></editor>
      <editor><first>Ekaterina</first><last>Vylomova</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dubrovnik, Croatia</address>
      <month>May</month>
      <year>2023</year>
      <url hash="5ba79b0c">2023.sigtyp-1</url>
      <venue>sigtyp</venue>
    </meta>
    <frontmatter>
      <url hash="14d37a3b">2023.sigtyp-1.0</url>
      <bibkey>sigtyp-2023-research</bibkey>
    </frontmatter>
    <paper id="1">
      <title>You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models</title>
      <author><first>Tomasz</first><last>Limisiewicz</last><affiliation>Charles University in Prague</affiliation></author>
      <author><first>Dan</first><last>Malkin</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Gabriel</first><last>Stanovsky</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <pages>1-11</pages>
      <abstract>Multilingual models have been widely used for the cross-lingual transfer to low-resource languages. However, the performance on these languages is hindered by their under-representation in the pretraining data. To alleviate this problem, we propose a novel multilingual training technique based on teacher-student knowledge distillation. In this setting, we utilize monolingual teacher models optimized for their language. We use those teachers along with balanced (sub-sampled) data to distill the teachers’ knowledge into a single multilingual student. Our method outperforms standard training methods in low-resource languages and retains performance on high-resource languages while using the same amount of data. If applied widely, our approach can increase the representation of low-resource languages in NLP systems.</abstract>
      <url hash="a0c0886b">2023.sigtyp-1.1</url>
      <bibkey>limisiewicz-etal-2023-data</bibkey>
      <video href="2023.sigtyp-1.1.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Multilingual End-to-end Dependency Parsing with Linguistic Typology knowledge</title>
      <author><first>Chinmay</first><last>Choudhary</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Colm</first><last>O’riordan</last><affiliation>National University of Ireland</affiliation></author>
      <pages>12-21</pages>
      <abstract>We evaluate a Multilingual End-to-end BERT based Dependency Parser which parses an input sentence by directly predicting the relative head-position for each word within it. Our model is a Cross-lingual dependency parser which is trained on a diverse polyglot corpus of high-resource source languages, and is applied on a low-resource target language. To make model more robust to typological variations between source and target languages, and to facilitate the cross-lingual transferring, we utilized the Linguistic typology knowledge, available in typological databases WALS and URIEL. We induce such typology knowledge within our model through an auxiliary task within Multi-task Learning framework.</abstract>
      <url hash="7598154c">2023.sigtyp-1.2</url>
      <bibkey>choudhary-oriordan-2023-multilingual</bibkey>
      <doi>10.18653/v1/2023.sigtyp-1.2</doi>
    </paper>
    <paper id="3">
      <title>Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space</title>
      <author><first>Fred</first><last>Philippy</last><affiliation>University of Luxemburg</affiliation></author>
      <author><first>Siwen</first><last>Guo</last><affiliation>University of Luxemburg</affiliation></author>
      <author><first>Shohreh</first><last>Haddadan</last><affiliation>University of Luxemburg</affiliation></author>
      <pages>22-29</pages>
      <abstract>Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.</abstract>
      <url hash="b455fba5">2023.sigtyp-1.3</url>
      <bibkey>philippy-etal-2023-identifying</bibkey>
      <video href="2023.sigtyp-1.3.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Using Modern Languages to Parse Ancient Ones: a Test on <fixed-case>O</fixed-case>ld <fixed-case>E</fixed-case>nglish</title>
      <author><first>Luca</first><last>Brigada Villa</last><affiliation>University of Pavia</affiliation></author>
      <author><first>Martina</first><last>Giarda</last><affiliation>University of Bergamo</affiliation></author>
      <pages>30-41</pages>
      <abstract>In this paper we test the parsing performances of a multilingual parser on Old English data using different sets of languages, alone and combined with the target language, to train the models. We compare the results obtained by the models and we analyze more in deep the annotation of some peculiar syntactic constructions of the target language, providing plausible linguistic explanations of the errors made even by the best performing models.</abstract>
      <url hash="e1a5636d">2023.sigtyp-1.4</url>
      <bibkey>brigada-villa-giarda-2023-using</bibkey>
      <video href="2023.sigtyp-1.4.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.4</doi>
    </paper>
    <paper id="5">
      <title>The Denglisch Corpus of <fixed-case>G</fixed-case>erman-<fixed-case>E</fixed-case>nglish Code-Switching</title>
      <author><first>Doreen</first><last>Osmelak</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Shuly</first><last>Wintner</last><affiliation>University of Haifa</affiliation></author>
      <pages>42-51</pages>
      <abstract>When multilingual speakers involve in a conversation they inevitably introduce code-switching (CS), i.e., mixing of more than one language between and within utterances. CS is still an understudied phenomenon, especially in the written medium, and relatively few computational resources for studying it are available. We describe a corpus of German-English code-switching in social media interactions. We focus on some challenges in annotating CS, especially due to words whose language ID cannot be easily determined. We introduce a novel schema for such word-level annotation, with which we manually annotated a subset of the corpus. We then trained classifiers to predict and identify switches, and applied them to the remainder of the corpus. Thereby, we created a large scale corpus of German-English mixed utterances with precise indications of CS points.</abstract>
      <url hash="68e7f3df">2023.sigtyp-1.5</url>
      <bibkey>osmelak-wintner-2023-denglisch</bibkey>
      <video href="2023.sigtyp-1.5.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Trimming Phonetic Alignments Improves the Inference of Sound Correspondence Patterns from Multilingual Wordlists</title>
      <author><first>Frederic</first><last>Blum</last><affiliation>Max-Planck Institute for Evolutionary Anthropology</affiliation></author>
      <author><first>Johann-Mattis</first><last>List</last><affiliation>Universität Passau</affiliation></author>
      <pages>52-64</pages>
      <abstract>Sound correspondence patterns form the basis of cognate detection and phonological reconstruction in historical language comparison. Methods for the automatic inference of correspondence patterns from phonetically aligned cognate sets have been proposed, but their application to multilingual wordlists requires extremely well annotated datasets. Since annotation is tedious and time consuming, it would be desirable to find ways to improve aligned cognate data automatically. Taking inspiration from trimming techniques in evolutionary biology, which improve alignments by excluding problematic sites, we propose a workflow that trims phonetic alignments in comparative linguistics prior to the inference of correspondence patterns. Testing these techniques on a large standardized collection of ten datasets with expert annotations from different language families, we find that the best trimming technique substantially improves the overall consistency of the alignments, showing a clear increase in the proportion of frequent correspondence patterns and words exhibiting regular cognate relations.</abstract>
      <url hash="0eb2c053">2023.sigtyp-1.6</url>
      <bibkey>blum-list-2023-trimming</bibkey>
      <video href="2023.sigtyp-1.6.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.6</doi>
    </paper>
    <paper id="7">
      <title>A Crosslinguistic Database for Combinatorial and Semantic Properties of Attitude Predicates</title>
      <author><first>Deniz</first><last>Özyıldız</last><affiliation>Universität Konstanz</affiliation></author>
      <author><first>Ciyang</first><last>Qing</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Floris</first><last>Roelofsen</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Maribel</first><last>Romero</last><affiliation>Universität Konstanz</affiliation></author>
      <author><first>Wataru</first><last>Uegaki</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>65-75</pages>
      <abstract>We introduce a cross-linguistic database for attitude predicates, which references their combinatorial (syntactic) and semantic properties. Our data allows assessment of cross-linguistic generalizations about attitude predicates as well as discovery of new typological/cross-linguistic patterns. This paper motivates empirical and theoretical issues that our database will help to address, the sample predicates and the properties that it references, as well as our design and methodological choices. Two case studies illustrate how the database can be used to assess validity of cross-linguistic generalizations.</abstract>
      <url hash="eb55e098">2023.sigtyp-1.7</url>
      <bibkey>ozyildiz-etal-2023-crosslinguistic</bibkey>
      <video href="2023.sigtyp-1.7.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Corpus-based Syntactic Typological Methods for Dependency Parsing Improvement</title>
      <author><first>Diego</first><last>Alves</last><affiliation>University of Zagreb</affiliation></author>
      <author><first>Božo</first><last>Bekavac</last><affiliation>University of Zagreb</affiliation></author>
      <author><first>Daniel</first><last>Zeman</last><affiliation>Charles University in Prague</affiliation></author>
      <author><first>Marko</first><last>Tadić</last><affiliation>University of Zagreb</affiliation></author>
      <pages>76-88</pages>
      <abstract>This article presents a comparative analysis of four different syntactic typological approaches applied to 20 different languages to determine the most effective one to be used for the improvement of dependency parsing results via corpora combination. We evaluated these strategies by calculating the correlation between the language distances and the empirical LAS results obtained when languages were combined in pairs. From the results, it was possible to observe that the best method is based on the extraction of word order patterns which happen inside subtrees of the syntactic structure of the sentences.</abstract>
      <url hash="e15ee063">2023.sigtyp-1.8</url>
      <bibkey>alves-etal-2023-corpus</bibkey>
      <video href="2023.sigtyp-1.8.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.8</doi>
    </paper>
    <paper id="9">
      <title>Cross-lingual Transfer Learning with <fixed-case>P</fixed-case>ersian</title>
      <author><first>Sepideh</first><last>Mollanorozy</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Marc</first><last>Tanti</last><affiliation>University of Malta</affiliation></author>
      <author><first>Malvina</first><last>Nissim</last><affiliation>University of Groningen</affiliation></author>
      <pages>89-95</pages>
      <abstract>The success of cross-lingual transfer learning for POS tagging has been shown to be strongly dependent, among other factors, on the (typological and/or genetic) similarity of the low-resource language used for testing and the language(s) used in pre-training or to fine-tune the model. We further unpack this finding in two directions by zooming in on a single language, namely Persian. First, still focusing on POS tagging we run an in-depth analysis of the behaviour of Persian with respect to closely related languages and languages that appear to benefit from cross-lingual transfer with Persian. To do so, we also use the World Atlas of Language Structures to determine which properties are shared between Persian and other languages included in the experiments. Based on our results, Persian seems to be a reasonable potential language for Kurmanji and Tagalog low-resource languages for other tasks as well. Second, we test whether previous findings also hold on a task other than POS tagging to pull apart the benefit of language similarity and the specific task for which such benefit has been shown to hold. We gather sentiment analysis datasets for 31 target languages and through a series of cross-lingual experiments analyse which languages most benefit from Persian as the source. The set of languages that benefit from Persian had very little overlap across the two tasks, suggesting a strong task-dependent component in the usefulness of language similarity in cross-lingual transfer.</abstract>
      <url hash="3c88024f">2023.sigtyp-1.9</url>
      <bibkey>mollanorozy-etal-2023-cross</bibkey>
      <video href="2023.sigtyp-1.9.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.9</doi>
    </paper>
    <paper id="10">
      <title>Information-Theoretic Characterization of Vowel Harmony: A Cross-Linguistic Study on Word Lists</title>
      <author><first>Julius</first><last>Steuer</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Johann-Mattis</first><last>List</last><affiliation>Universität Passau</affiliation></author>
      <author><first>Badr M.</first><last>Abdullah</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>96-109</pages>
      <abstract>We present a cross-linguistic study of vowel harmony that aims to quantifies this phenomenon using data-driven computational modeling. Concretely, we define an information-theoretic measure of harmonicity based on the predictability of vowels in a natural language lexicon, which we estimate using phoneme-level language models (PLMs). Prior quantitative studies have heavily relied on inflected word-forms in the analysis on vowel harmony. On the contrary, we train our models using cross-linguistically comparable lemma forms with little or no inflection, which enables us to cover more under-studied languages. Training data for our PLMs consists of word lists offering a maximum of 1000 entries per language. Despite the fact that the data we employ are substantially smaller than previously used corpora, our experiments demonstrate the neural PLMs capture vowel harmony patterns in a set of languages that exhibit this phenomenon. Our work also demonstrates that word lists are a valuable resource for typological research, and offers new possibilities for future studies on low-resource, under-studied languages.</abstract>
      <url hash="2b066edc">2023.sigtyp-1.10</url>
      <bibkey>steuer-etal-2023-information</bibkey>
      <doi>10.18653/v1/2023.sigtyp-1.10</doi>
    </paper>
    <paper id="11">
      <title>Revisiting Dependency Length and Intervener Complexity Minimisation on a Parallel Corpus in 35 Languages</title>
      <author><first>Andrew Thomas</first><last>Dyer</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>110-119</pages>
      <abstract>In this replication study of previous research into dependency length minimisation (DLM), we pilot a new parallel multilingual parsed corpus to examine whether previous findings are upheld when controlling for variation in domain and sentence content between languages. We follow the approach of previous research in comparing the dependency lengths of observed sentences in a multilingual corpus to a variety of baselines: permutations of the sentences, either random or according to some fixed schema. We go on to compare DLM with intervener complexity measure (ICM), an alternative measure of syntactic complexity. Our findings uphold both dependency length and intervener complexity minimisation in all languages under investigation. We also find a markedly lesser extent of dependency length minimisation in verb-final languages, and the same for intervener complexity measure. We conclude that dependency length and intervener complexity minimisation as universals are upheld when controlling for domain and content variation, but that further research is needed into the asymmetry between verb-final and other languages in this regard.</abstract>
      <url hash="a3bbfe99">2023.sigtyp-1.11</url>
      <bibkey>dyer-2023-revisiting</bibkey>
      <video href="2023.sigtyp-1.11.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Does Topological Ordering of Morphological Segments Reduce Morphological Modeling Complexity? A Preliminary Study on 13 Languages</title>
      <author><first>Andreas</first><last>Shcherbakov</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Ekaterina</first><last>Vylomova</last><affiliation>University of Melbourne</affiliation></author>
      <pages>120-125</pages>
      <abstract>Generalization to novel forms and feature combinations is the key to efficient learning. Recently, Goldman et al. (2022) demonstrated that contemporary neural approaches to morphological inflection still struggle to generalize to unseen words and feature combinations, even in agglutinative languages. In this paper, we argue that the use of morphological segmentation in inflection modeling allows decomposing the problem into sub-problems of substantially smaller search space. We suggest that morphological segments may be globally topologically sorted according to their grammatical categories within a given language. Our experiments demonstrate that such segmentation provides all the necessary information for better generalization, especially in agglutinative languages.</abstract>
      <url hash="ac975c10">2023.sigtyp-1.12</url>
      <bibkey>shcherbakov-vylomova-2023-topological</bibkey>
      <video href="2023.sigtyp-1.12.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Findings of the <fixed-case>SIGTYP</fixed-case> 2023 Shared task on Cognate and Derivative Detection For Low-Resourced Languages</title>
      <author><first>Priya</first><last>Rani</last><affiliation>University of Galway</affiliation></author>
      <author><first>Koustava</first><last>Goswami</last><affiliation>Adobe Research Bangalore</affiliation></author>
      <author><first>Adrian</first><last>Doyle</last><affiliation>University of Galway</affiliation></author>
      <author><first>Theodorus</first><last>Fransen</last><affiliation>University of Galway</affiliation></author>
      <author><first>Bernardo</first><last>Stearns</last><affiliation>University of Galway</affiliation></author>
      <author><first>John P.</first><last>McCrae</last><affiliation>University of Galway</affiliation></author>
      <pages>126-131</pages>
      <abstract>This paper describes the structure and findings of the SIGTYP 2023 shared task on cognate and derivative detection for low-resourced languages, broken down into a supervised and unsupervised sub-task. The participants were asked to submit the test data’s final prediction. A total of nine teams registered for the shared task where seven teams registered for both sub-tasks. Only two participants ended up submitting system descriptions, with only one submitting systems for both sub-tasks. While all systems show a rather promising performance, all could be within the baseline score for the supervised sub-task. However, the system submitted for the unsupervised sub-task outperforms the baseline score.</abstract>
      <url hash="ca39677d">2023.sigtyp-1.13</url>
      <bibkey>rani-etal-2023-findings</bibkey>
      <video href="2023.sigtyp-1.13.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>ÚFAL</fixed-case> Submission for <fixed-case>SIGTYP</fixed-case> Supervised Cognate Detection Task</title>
      <author><first>Tomasz</first><last>Limisiewicz</last><affiliation>Charles University in Prague</affiliation></author>
      <pages>132-136</pages>
      <abstract>In this work, I present ÚFAL submission for the supervised task of detecting cognates and derivatives. Cognates are word pairs in different languages sharing the origin in earlier attested forms in ancestral language, while derivatives come directly from another language. For the task, I developed gradient boosted tree classifier trained on linguistic and statistical features. The solution came first from two delivered systems with an 87% F1 score on the test split. This write-up gives an insight into the system and shows the importance of using linguistic features and character-level statistics for the task.</abstract>
      <url hash="1d2257d7">2023.sigtyp-1.14</url>
      <bibkey>limisiewicz-2023-ufal</bibkey>
      <video href="2023.sigtyp-1.14.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>o<fixed-case>H</fixed-case>i<fixed-case>L</fixed-case>i at <fixed-case>SIGTYP</fixed-case> 2023: Ensemble Models for Cognate and Derivative Words Detection</title>
      <author><first>Liviu P.</first><last>Dinu</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Ioan-Bogdan</first><last>Iordache</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Ana Sabina</first><last>Uban</last><affiliation>University of Bucharest</affiliation></author>
      <pages>137-142</pages>
      <abstract>The identification of cognates and derivatives is a fundamental process in historical linguistics, on which any further research is based. In this paper we present our contribution to the SIGTYP 2023 Shared Task on cognate and derivative detection. We propose a multi-lingual solution based on features extracted from the alignment of the orthographic and phonetic representations of the words.</abstract>
      <url hash="507c9c6e">2023.sigtyp-1.15</url>
      <bibkey>dinu-etal-2023-cotohili</bibkey>
      <doi>10.18653/v1/2023.sigtyp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Multilingual <fixed-case>BERT</fixed-case> has an Accent: Evaluating <fixed-case>E</fixed-case>nglish Influences on Fluency in Multilingual Models</title>
      <author><first>Isabel</first><last>Papadimitriou</last><affiliation>Stanford University</affiliation></author>
      <author><first>Kezia</first><last>Lopez</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <pages>143-146</pages>
      <abstract>While multilingual language models can improve NLP performance on low-resource languages by leveraging higher-resource languages, they also reduce average performance on all languages (the ‘curse of multilinguality’). Here we show another problem with multilingual models: grammatical structures in higher-resource languages bleed into lower-resource languages, a phenomenon we call grammatical structure bias. We show this bias via a novel method for comparing the fluency of multilingual models to the fluency of monolingual Spanish and Greek models: testing their preference for two carefully-chosen variable grammatical structures (optional pronoun-drop in Spanish and optional Subject-Verb ordering in Greek). We find that multilingual BERT is biased toward the English-like setting (explicit pronouns and Subject-Verb-Object ordering) and against the default Spanish and Gerek settings, as compared to our monolingual control language model. With our case studies, we hope to bring to light the fine-grained ways in which multilingual models can be biased, and encourage more linguistically-aware fluency evaluation.</abstract>
      <url hash="bc920b07">2023.sigtyp-1.16</url>
      <bibkey>papadimitriou-etal-2023-multilingual-bert</bibkey>
      <doi>10.18653/v1/2023.sigtyp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Grambank’s Typological Advances Support Computational Research on Diverse Languages</title>
      <author><first>Hannah J.</first><last>Haynie</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Damián</first><last>Blasi</last><affiliation>Harvard University</affiliation></author>
      <author><first>Hedvig</first><last>Skirgård</last><affiliation>Max Planck Institute for Evolutionary Anthropology</affiliation></author>
      <author><first>Simon J.</first><last>Greenhill</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Quentin D.</first><last>Atkinson</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Russell D.</first><last>Gray</last><affiliation>Max Planck Institute for Evolutionary Anthropology</affiliation></author>
      <pages>147-149</pages>
      <abstract>Of approximately 7,000 languages around the world, only a handful have abundant computational resources. Extending the reach of language technologies to diverse, less-resourced languages is important for tackling the challenges of digital equity and inclusion. Here we introduce the Grambank typological database as a resource to support such efforts. To date, work that uses typological data to extend computational research to less-resourced languages has relied on cross-linguistic morphosyntax datasets that are sparsely populated, use categorical coding that can be difficult to interpret, and introduce redundant information across features. Grambank presents similar information (e.g. word order, grammatical relation marking, constructions like interrogatives and negation), but is designed to avoid several disadvantages of legacy typological resources. Grambank’s 195 features encode basic information about morphology and syntax for 2,467 languages. 83% of these languages are annotated for at least 100 features. By implementing binary coding for most features and curating the dataset to avoid logical dependencies, Grambank presents information in a user-friendly format for computational applications. The scale, completeness, reliability, format, and documentation of Grambank make it a useful resource for linguistically-informed models, cross-lingual NLP, and research targeting less-resourced languages.</abstract>
      <url hash="252512a1">2023.sigtyp-1.17</url>
      <bibkey>haynie-etal-2023-grambanks</bibkey>
      <video href="2023.sigtyp-1.17.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.17</doi>
    </paper>
    <paper id="18">
      <title>Language-Agnostic Measures Discriminate Inflection and Derivation</title>
      <author><first>Coleman</first><last>Haley</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Edoardo M.</first><last>Ponti</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Sharon</first><last>Goldwater</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>150-152</pages>
      <abstract>In morphology, a distinction is commonly drawn between inflection and derivation. However, a precise definition of this distinction which captures the way the terms are used across languages remains elusive within linguistic theory, typically being based on subjective tests. In this study, we present 4 quantitative measures which use the statistics of a raw text corpus in a language to estimate how much and how variably a morphological construction changes aspects of the lexical entry, specifically, the word’s form and the word’s semantic and syntactic properties (as operationalised by distributional word embeddings). Based on a sample of 26 languages, we find that we can reconstruct 90% of the classification of constructions into inflection and derivation in Unimorph using our 4 measures, providing large-scale cross-linguistic evidence that the concepts of inflection and derivation are associated with measurable signatures in terms of form and distribution signatures that behave consistently across a variety of languages. Critically, our measures and models are entirely language-agnostic, yet perform well across all languages studied. We find that while there is a high degree of consistency in the use of the terms inflection and derivation in terms of our measures, there are still many constructions near the model’s decision boundary between the two categories, indicating a gradient, rather than categorical, distinction.</abstract>
      <url hash="a24fa999">2023.sigtyp-1.18</url>
      <bibkey>haley-etal-2023-language</bibkey>
      <video href="2023.sigtyp-1.18.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Gradual Language Model Adaptation Using Fine-Grained Typology</title>
      <author><first>Marcell Richard</first><last>Fekete</last><affiliation>Aalborg University</affiliation></author>
      <author><first>Johannes</first><last>Bjerva</last><affiliation>Aalborg University</affiliation></author>
      <pages>153-158</pages>
      <abstract>Transformer-based language models (LMs) offer superior performance in a wide range of NLP tasks compared to previous paradigms. However, the vast majority of the world’s languages do not have adequate training data available for monolingual LMs (Joshi et al., 2020). While the use of multilingual LMs might address this data imbalance, there is evidence that multilingual LMs struggle when it comes to model adaptation to to resource-poor languages (Wu and Dredze, 2020), or to languages which have typological characteristics unseen by the LM (Üstün et al., 2022). Other approaches aim to adapt monolingual LMs to resource-poor languages that are related to the model language. However, there are conflicting findings regarding whether language relatedness correlates with successful adaptation (de Vries et al., 2021), or not (Ács et al., 2021). With gradual LM adaptation, our approach presented in this extended abstract, we add to the research direction of monolingual LM adaptation. Instead of direct adaptation to a target language, we propose adaptation in stages, first adapting to one or more intermediate languages before the final adaptation step. Inspired by principles of curriculum learning (Bengio et al., 2009), we search for an ideal ordering of languages that can result in improved LM performance on the target language. We follow evidence that typological similarity might correlate with the success of cross-lingual transfer (Pires et al., 2019; Üstün et al., 2022; de Vries et al., 2021) as we believe the success of this transfer is essential for successful model adaptation. Thus we order languages based on their relative typological similarity between them. In our approach, we quantify typological similarity using structural vectors as derived from counts of dependency links (Bjerva et al., 2019), as such fine-grained measures can give a more accurate picture of the typological characteristics of languages (Ponti et al., 2019). We believe that gradual LM adaptation may lead to improved LM performance on a range of resource-poor languages and typologically diverse languages. Additionally, it enables future research to evaluate the correlation between the success of cross-lingual transfer and various typological similarity measures.</abstract>
      <url hash="2f3a6a8d">2023.sigtyp-1.19</url>
      <bibkey>fekete-bjerva-2023-gradual</bibkey>
      <video href="2023.sigtyp-1.19.mp4"/>
      <doi>10.18653/v1/2023.sigtyp-1.19</doi>
    </paper>
    <paper id="20">
      <title>On the Nature of Discrete Speech Representations in Multilingual Self-supervised Models</title>
      <author><first>Badr M.</first><last>Abdullah</last><affiliation>Saarland University</affiliation></author>
      <author><first>Mohammed Maqsood</first><last>Shaik</last><affiliation>Saarland University</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last><affiliation>Saarland University</affiliation></author>
      <pages>159-161</pages>
      <abstract>Self-supervision has emerged as an effective paradigm for learning representations of spoken language from raw audio without explicit labels or transcriptions. Self-supervised speech models, such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021), have shown significant promise in improving the performance across different speech processing tasks. One of the main advantages of self-supervised speech models is that they can be pre-trained on a large sample of languages (Conneau et al., 2020; Babu et al.,2022), which facilitates cross-lingual transfer for low-resource languages (San et al., 2021). State-of-the-art self-supervised speech models include a quantization module that transforms the continuous acoustic input into a sequence of discrete units. One of the key questions in this area is whether the discrete representations learned via self-supervision are language-specific or language-universal. In other words, we ask: do the discrete units learned by a multilingual speech model represent the same speech sounds across languages or do they differ based on the specific language being spoken? From the practical perspective, this question has important implications for the development of speech models that can generalize across languages, particularly for low-resource languages. Furthermore, examining the level of linguistic abstraction in speech models that lack symbolic supervision is also relevant to the field of human language acquisition (Dupoux, 2018).</abstract>
      <url hash="549a20b2">2023.sigtyp-1.20</url>
      <bibkey>abdullah-etal-2023-nature</bibkey>
      <doi>10.18653/v1/2023.sigtyp-1.20</doi>
    </paper>
  </volume>
</collection>
