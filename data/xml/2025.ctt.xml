<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.ctt">
  <volume id="1" ingest-date="2025-08-07" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Creative-text Translation and Technology (CTT)</booktitle>
      <editor><first>Bram</first><last>Vanroy</last></editor>
      <editor><first>Marie-Aude</first><last>Lefer</last></editor>
      <editor><first>Lieve</first><last>Macken</last></editor>
      <editor><first>Paola</first><last>Ruffo</last></editor>
      <editor><first>Ana Guerberof</first><last>Arenas</last></editor>
      <editor><first>Damien</first><last>Hansen</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Geneva, Switzerland</address>
      <month>June</month>
      <year>2025</year>
      <url hash="e62d4cdf">2025.ctt-1</url>
      <venue>ctt</venue>
      <isbn>978-2-9701897-6-3</isbn>
    </meta>
    <frontmatter>
      <url hash="77b7cbe9">2025.ctt-1.0</url>
      <bibkey>ctt-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The Role of Translation Workflows in Overcoming Translation Difficulties: A Comparative Analysis of Human and Machine Translation (Post-Editing) Approaches</title>
      <author><first>Lieve</first><last>Macken</last></author>
      <author><first>Paola</first><last>Ruffo</last></author>
      <author><first>Joke</first><last>Daems</last></author>
      <pages>1–13</pages>
      <abstract>This study investigates the impact of different translation workflows and underlying machine translation technologies on the translation strategies used in literary translations. We compare human translation, translation within a computer-assisted translation (CAT) tool, and machine translation post-editing (MTPE), alongside neural machine translation (NMT) and large language models (LLMs). Using three short stories translated from English into Dutch, we annotated translation difficulties and strategies employed to overcome them. Our analysis reveals differences in translation solutions across modalities, highlighting the influence of technology on the final translation. The findings suggest that while MTPE tends to produce more literal translations, human translators and CAT tools exhibit greater creativity and employ more non-literal translation strategies. Additionally, LLMs reduced the number of literal translation solutions compared to traditional NMT systems. While our study provides valuable insights, it is limited by the use of only three texts and a single language pair. Further research is needed to explore these dynamics across a broader range of texts and languages, to better understand the full impact of translation workflows and technologies on literary translation.</abstract>
      <url hash="ccb910a1">2025.ctt-1.1</url>
      <bibkey>macken-etal-2025-role</bibkey>
    </paper>
    <paper id="2">
      <title>Does the perceived source of a translation (<fixed-case>NMT</fixed-case> vs. <fixed-case>HT</fixed-case>) impact student revision quality for news and literary texts?</title>
      <author><first>Xiaoye</first><last>Li</last></author>
      <author><first>Joke</first><last>Daems</last></author>
      <pages>14–26</pages>
      <abstract>With quality improvements in neural machine translation (NMT), scholars have argued that human translation revision and MT post-editing are becoming more alike, which would have implications for translator training. This study contributes to this growing body of work by exploring the ability of student translators (ZH-EN) to distinguish between NMT and human translation (HT) for news text and literary text and analyses how text type and student perceptions influence their subsequent revision process. We found that participants were reasonably adept at distinguishing between NMT and HT, particularly for literary texts. Participants’ revision quality was dependent on the text type as well as the perceived source of translation. The findings also highlight student translators’ limited competence in revision and post-editing, emphasizing the need to integrate NMT, revision, and post-editing into translation training programmes.</abstract>
      <url hash="b38bf2af">2025.ctt-1.2</url>
      <bibkey>li-daems-2025-perceived</bibkey>
    </paper>
    <paper id="3">
      <title>Effects of Domain-adapted Machine Translation on the Machine Translation User Experience of Video Game Translators</title>
      <author><first>Judith</first><last>Brenner</last></author>
      <author><first>Julia</first><last>Othlinghaus-Wulhorst</last></author>
      <pages>27–43</pages>
      <abstract>In this empirical study we examine three different translation modes with varying involvement of machine translation (MT) post-editing (PE) when translating video game texts. The three translation modes are translation from scratch without MT, full PE of MT output in a static way, and flexible PE as a combination of translation from scratch and post-editing of only those machine-translated sentences deemed useful by the translator. Data generation took place at the home offices of freelance game translators. In a mixed-methods approach, quantitative data was generated through keylogging, eye tracking, error annotation, and user experience questionnaires as well as qualitative data through interviews. Results show a negative perception of PE and suggest that translators’ user experience is positive when translating from scratch, neutral with a positive tendency when doing flexible PE of domain-adapted MT output and negative with static PE of generic MT output.</abstract>
      <url hash="9d60e590">2025.ctt-1.3</url>
      <bibkey>brenner-othlinghaus-wulhorst-2025-effects</bibkey>
    </paper>
    <paper id="4">
      <title>Fine-tuning and evaluation of <fixed-case>NMT</fixed-case> models for literary texts using <fixed-case>R</fixed-case>om<fixed-case>C</fixed-case>ro v.2.0</title>
      <author><first>Bojana</first><last>Mikelenić</last></author>
      <author><first>Antoni</first><last>Oliver</last></author>
      <author><first>Sergi Àlvarez</first><last>Vidal</last></author>
      <pages>44–51</pages>
      <abstract>This paper explores the fine-tuning and evaluation of neural machine translation (NMT) models for literary texts using RomCro v.2.0, an expanded multilingual and multidirectional parallel corpus. RomCro v.2.0 is based on RomCro v.1.0, but includes additional literary works, as well as texts in Catalan, making it a valuable resource for improving MT in underrepresented language pairs. Given the challenges of literary translation, where style, narrative voice, and cultural nuances must be preserved, fine-tuning on high-quality domain-specific data is essential for enhancing MT performance. We fine-tune existing NMT models with RomCro v.2.0 and evaluate their performance for six different language combinations using automatic metrics and for Spanish-Croatian and French-Catalan using manual evaluation. Results indicate that fine-tuned models outperform general-purpose systems, achieving greater fluency and stylistic coherence. These findings support the effectiveness of corpus-driven fine-tuning for literary translation and highlight the importance of curated high-quality corpus.</abstract>
      <url hash="823c5046">2025.ctt-1.4</url>
      <bibkey>mikelenic-etal-2025-fine</bibkey>
    </paper>
    <paper id="5">
      <title>Can Peter Pan Survive <fixed-case>MT</fixed-case>? A Stylometric Study of <fixed-case>LLM</fixed-case>s, <fixed-case>NMT</fixed-case>s, and <fixed-case>HT</fixed-case>s in Children’s Literature Translation</title>
      <author><first>Delu</first><last>Kong</last></author>
      <author><first>Lieve</first><last>Macken</last></author>
      <pages>52–70</pages>
      <abstract>This study focuses on evaluating the performance of machine translations (MTs) compared to human translations (HTs) in children’s literature translation (CLT) from a stylometric perspective. The research constructs a extitPeter Pan corpus, comprising 21 translations: 7 human translations (HTs), 7 large language model translations (LLMs), and 7 neural machine translation outputs (NMTs). The analysis employs a generic feature set (including lexical, syntactic, readability, and n-gram features) and a creative text translation (CTT-specific) feature set, which captures repetition, rhyme, translatability, and miscellaneous levels, yielding 447 linguistic features in total. Using classification and clustering techniques in machine learning, we conduct a stylometric analysis of these translations. Results reveal that in generic features, HTs and MTs exhibit significant differences in conjunction word distributions and the ratio of 1-word-gram-一样, while NMTs and LLMs show significant variation in descriptive words usage and adverb ratios. Regarding CTT-specific features, LLMs outperform NMTs in distribution, aligning more closely with HTs in stylistic characteristics, demonstrating the potential of LLMs in CLT.</abstract>
      <url hash="32c4e38d">2025.ctt-1.5</url>
      <bibkey>kong-macken-2025-peter</bibkey>
    </paper>
  </volume>
</collection>
