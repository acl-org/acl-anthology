<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.sigmorphon">
  <volume id="1" ingest-date="2024-06-20" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 21st SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
      <editor><first>Garrett</first><last>Nicolai</last></editor>
      <editor><first>Eleanor</first><last>Chodroff</last></editor>
      <editor><first>Frederic</first><last>Mailhot</last></editor>
      <editor><first>Çağrı</first><last>Çöltekin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2024</year>
      <url hash="64f38c25">2024.sigmorphon-1</url>
      <venue>sigmorphon</venue>
    </meta>
    <frontmatter>
      <url hash="8fa5ff6c">2024.sigmorphon-1.0</url>
      <bibkey>sigmorphon-2024-sigmorphon</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>V</fixed-case>e<fixed-case>L</fixed-case>e<fixed-case>P</fixed-case>a: a Verbal Lexicon of <fixed-case>P</fixed-case>ame</title>
      <author><first>Borja</first><last>Herce</last></author>
      <pages>1-6</pages>
      <abstract>This paper presents VeLePa, an inflected verbal lexicon of Central Pame (pbs, cent2154), an Otomanguean language from Mexico. This resource contains 12528 words in phonological form representing the complete inflectional paradigms of 216 verbs, supplemented with use frequencies. Computer-operable (CLDF) inflected lexicons of non-WEIRD underresourced languages are urgently needed to expand digital capacities in this languages (e.g. in NLP). VeLePa contributes to this, and does so with data from a language which is morphologically extraordinary, with unusually high levels of irregularity and multiple conjugations at various loci within the word: prefixes, stems, tone, and suffixes constitute different albeit interrelated subsystems of inflection.</abstract>
      <url hash="b9f9e0b1">2024.sigmorphon-1.1</url>
      <bibkey>herce-2024-velepa</bibkey>
      <doi>10.18653/v1/2024.sigmorphon-1.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>J</fixed-case>-<fixed-case>U</fixed-case>ni<fixed-case>M</fixed-case>orph: <fixed-case>J</fixed-case>apanese Morphological Annotation through the Universal Feature Schema</title>
      <author><first>Kosuke</first><last>Matsuzaki</last></author>
      <author><first>Masaya</first><last>Taniguchi</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <author><first>Keisuke</first><last>Sakaguchi</last></author>
      <pages>7-19</pages>
      <abstract>We introduce a Japanese Morphology dataset, J-UniMorph, developed based on the UniMorph feature schema. This dataset addresses the unique and rich verb forms characteristic of the language’s agglutinative nature. J-UniMorph distinguishes itself from the existing Japanese subset of UniMorph, which is automatically extracted from Wiktionary. On average, the Wiktionary Edition features around 12 inflected forms for each word and is primarily dominated by denominal verbs (i.e., [noun] + suru (do-PRS)). Morphologically, this inflection pattern is same as the verb suru (do). In contrast, J-UniMorph explores a much broader and more frequently used range of verb forms, offering 118 inflected forms for each word on average. It includes honorifics, a range of politeness levels, and other linguistic nuances, emphasizing the distinctive characteristics of the Japanese language. This paper presents detailed statistics and characteristics of J-UniMorph, comparing it with the Wiktionary Edition. We will release J-UniMorph and its interactive visualizer publicly available, aiming to support cross-linguistic research and various applications.</abstract>
      <url hash="1fe63362">2024.sigmorphon-1.2</url>
      <bibkey>matsuzaki-etal-2024-j</bibkey>
      <doi>10.18653/v1/2024.sigmorphon-1.2</doi>
    </paper>
    <paper id="3">
      <title>More than Just Statistical Recurrence: Human and Machine Unsupervised Learning of <fixed-case>M</fixed-case>āori Word Segmentation across Morphological Processes</title>
      <author><first>Ashvini</first><last>Varatharaj</last></author>
      <author><first>Simon</first><last>Todd</last></author>
      <pages>20-31</pages>
      <abstract>Non-Māori-speaking New Zealanders (NMS) are able to segment Māori words in a highly similar way to fluent speakers (Panther et al., 2024). This ability is assumed to derive through the identification and extraction of statistically recurrent forms. We examine this assumption by asking how NMS segmentations compare to those produced by Morfessor, an unsupervised machine learning model that operates based on statistical recurrence, across words formed by a variety of morphological processes. Both NMS and Morfessor succeed in segmenting words formed by concatenative processes (compounding and affixation without allomorphy), but NMS also succeed for words that invoke templates (reduplication and allomorphy) and other cues to morphological structure, implying that their learning process is sensitive to more than just statistical recurrence.</abstract>
      <url hash="eada1bed">2024.sigmorphon-1.3</url>
      <bibkey>varatharaj-todd-2024-just</bibkey>
      <doi>10.18653/v1/2024.sigmorphon-1.3</doi>
    </paper>
    <paper id="4">
      <title>Different Tokenization Schemes Lead to Comparable Performance in <fixed-case>S</fixed-case>panish Number Agreement</title>
      <author><first>Catherine</first><last>Arnett</last></author>
      <author><first>Tyler</first><last>Chang</last></author>
      <author><first>Sean</first><last>Trott</last></author>
      <pages>32-38</pages>
      <abstract>The relationship between language model tokenization and performance is an open area of research. Here, we investigate how different tokenization schemes impact number agreement in Spanish plurals. We find that morphologically-aligned tokenization performs similarly to other tokenization schemes, even when induced artificially for words that would not be tokenized that way during training. We then present exploratory analyses demonstrating that language model embeddings for different plural tokenizations have similar distributions along the embedding space axis that maximally distinguishes singular and plural nouns. Our results suggest that morphologically-aligned tokenization is a viable tokenization approach, and existing models already generalize some morphological patterns to new items. However, our results indicate that morphological tokenization is not strictly required for performance.</abstract>
      <url hash="8137f77b">2024.sigmorphon-1.4</url>
      <bibkey>arnett-etal-2024-different</bibkey>
      <doi>10.18653/v1/2024.sigmorphon-1.4</doi>
    </paper>
    <paper id="5">
      <title>Ye Olde <fixed-case>F</fixed-case>rench: Effect of Old and <fixed-case>M</fixed-case>iddle <fixed-case>F</fixed-case>rench on <fixed-case>SIGMORPHON</fixed-case>-<fixed-case>U</fixed-case>ni<fixed-case>M</fixed-case>orph Shared Task Data</title>
      <author><first>William</first><last>Kezerian</last></author>
      <author><first>Lam An</first><last>Wyner</last></author>
      <author><first>Sandro</first><last>Ansari</last></author>
      <author><first>Kristine M.</first><last>Yu</last></author>
      <pages>39-50</pages>
      <abstract>We offer one explanation for the historically low performance of French in the SIGMORPHON-UniMorph shared tasks. We conducted experiments replicating the 2023 task on French with the non-neural and neural baselines, first using the original task splits, and then using splits that excluded Old and Middle French lemmas. We applied a taxonomy on our errors using a framework based on Kyle Gorman’s “Weird Inflects but OK” 2019 annotation scheme, finding that a high portion of the French errors produced with the original splits were due to the inclusion of Old French forms, which was resolved with cleaned data.</abstract>
      <url hash="8dec2f80">2024.sigmorphon-1.5</url>
      <bibkey>kezerian-yu-2024-ye</bibkey>
      <doi>10.18653/v1/2024.sigmorphon-1.5</doi>
    </paper>
    <paper id="6">
      <title>The Effect of Model Capacity and Script Diversity on Subword Tokenization for <fixed-case>S</fixed-case>orani <fixed-case>K</fixed-case>urdish</title>
      <author><first>Ali</first><last>Salehi</last></author>
      <author><first>Cassandra L.</first><last>Jacobs</last></author>
      <pages>51-56</pages>
      <abstract>Tokenization and morphological segmentation continue to pose challenges for text processing and studies of human language. Here, we focus on written Soranî Kurdish, which uses a modified script based on Persian and Arabic, and its transliterations into the Kurdish Latin script. Importantly, Perso-Arabic and Latin-based writing systems demonstrate different statistical and structural properties, which may have significant effects on subword vocabulary learning. This has major consequences for frequency- or probability-based models of morphological induction. We explore the possibility that jointly training subword vocabularies using a source script along with its transliteration would improve morphological segmentation, subword tokenization, and whether gains are observed for one system over others. We find that joint training has a similar effect to increasing vocabulary size, while keeping subwords shorter in length, which produces higher-quality subwords that map onto morphemes.</abstract>
      <url hash="990f754a">2024.sigmorphon-1.6</url>
      <bibkey>salehi-jacobs-2024-effect</bibkey>
      <doi>10.18653/v1/2024.sigmorphon-1.6</doi>
    </paper>
    <paper id="7">
      <title>Decomposing Fusional Morphemes with Vector Embeddings</title>
      <author><first>Michael</first><last>Ginn</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <pages>57-66</pages>
      <abstract>Distributional approaches have proven effective in modeling semantics and phonology through vector embeddings. We explore whether distributional representations can also effectively model morphological information. We train static vector embeddings over morphological sequences. Then, we explore morpheme categories for fusional morphemes, which encode multiple linguistic dimensions, and often have close relationships to other morphemes. We study whether the learned vector embeddings align with these linguistic dimensions, finding strong evidence that this is the case. Our work uses two low-resource languages, Uspanteko and Tsez, demonstrating that distributional morphological representations are effective even with limited data.</abstract>
      <url hash="b27f9043">2024.sigmorphon-1.7</url>
      <bibkey>ginn-palmer-2024-decomposing</bibkey>
      <doi>10.18653/v1/2024.sigmorphon-1.7</doi>
    </paper>
    <paper id="8">
      <title>Acoustic barycenters as exemplar production targets</title>
      <author><first>Frederic</first><last>Mailhot</last></author>
      <author><first>Cassandra L.</first><last>Jacobs</last></author>
      <pages>67-76</pages>
      <abstract>We present a solution to the problem of exemplar-based language production from variable-duration tokens, leveraging algorithms from the domain of time-series clustering and classification. Our model stores and outputs tokens of phonetically rich and temporally variable representations of recorded speech. We show qualitatively and quantitatively that model outputs retain essential acoustic/phonetic characteristics despite the noise introduced by averaging, and also demonstrate the effects of similarity and indexical information as constraints on exemplar cloud selection.</abstract>
      <url hash="00f00ea2">2024.sigmorphon-1.8</url>
      <bibkey>mailhot-jacobs-2024-acoustic</bibkey>
      <doi>10.18653/v1/2024.sigmorphon-1.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>J</fixed-case>apanese Rule-based Grapheme-to-phoneme Conversion System and Multilingual Named Entity Dataset with International Phonetic Alphabet</title>
      <author><first>Yuhi</first><last>Matogawa</last></author>
      <author><first>Yusuke</first><last>Sakai</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Chihiro</first><last>Taguchi</last></author>
      <pages>77-86</pages>
      <abstract>In Japanese, loanwords are primarily written in Katakana, a syllabic writing system, based on their pronunciation. However, the transliterated loanwords often exhibit spelling variations, such as the word “Hepburn” being written as “ヘボン (hebon)”, “ヘプバーン (hepubaan)”, “ヘップバーン (heppubaan)”. These orthographical variants pose a bottleneck in multilingual Named Entity Recognition (NER), because named entities (NEs) do not have one-to-one matches. In this study, we introduce a rule-based grapheme-to-phoneme (G2P) system for Japanese based on literature in linguistics and a large-scale multilingual NE dataset with annotations of the International Phonetic Alphabet (IPA), focusing on IPA to address the Katakana spelling variations in loanwords. These rules and dataset are expected to be beneficial for tasks such as NE aggregation, G2P system, construction of cross-lingual language models, and entity linking. We hope our work advances research on Japanese NER with multilingual loanwords by solving the spelling ambiguities.</abstract>
      <url hash="84fc50fe">2024.sigmorphon-1.9</url>
      <bibkey>matogawa-etal-2024-japanese</bibkey>
      <doi>10.18653/v1/2024.sigmorphon-1.9</doi>
    </paper>
  </volume>
</collection>
