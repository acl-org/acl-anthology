<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.vardial">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects</booktitle>
      <editor><first>Marcos</first><last>Zampieri</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Nikola</first><last>Ljubešić</last></editor>
      <editor><first>Jörg</first><last>Tiedemann</last></editor>
      <editor><first>Yves</first><last>Scherrer</last></editor>
      <editor><first>Tommi</first><last>Jauhiainen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Kiyv, Ukraine</address>
      <month>April</month>
      <year>2021</year>
      <venue>vardial</venue>
    </meta>
    <frontmatter>
      <url hash="5dcc52a6">2021.vardial-1.0</url>
      <bibkey>vardial-2021-nlp</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Findings of the <fixed-case>V</fixed-case>ar<fixed-case>D</fixed-case>ial Evaluation Campaign 2021</title>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Gaman</first><last>Mihaela</last></author>
      <author><first>Radu Tudor</first><last>Ionescu</last></author>
      <author><first>Heidi</first><last>Jauhiainen</last></author>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Niko</first><last>Partanen</last></author>
      <author><first>Ruba</first><last>Priyadharshini</last></author>
      <author><first>Christoph</first><last>Purschke</last></author>
      <author><first>Eswari</first><last>Rajagopal</last></author>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>1–11</pages>
      <abstract>This paper describes the results of the shared tasks organized as part of the VarDial Evaluation Campaign 2021. The campaign was part of the eighth workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with EACL 2021. Four separate shared tasks were included this year: Dravidian Language Identification (DLI), Romanian Dialect Identification (RDI), Social Media Variety Geolocation (SMG), and Uralic Language Identification (ULI). DLI was organized for the first time and the other three continued a series of tasks from previous evaluation campaigns.</abstract>
      <url hash="2ce7b5d4">2021.vardial-1.1</url>
      <bibkey>chakravarthi-etal-2021-findings-vardial</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/moroco">MOROCO</pwcdataset>
    </paper>
    <paper id="2">
      <title>Hierarchical Transformer for Multilingual Machine Translation</title>
      <author><first>Albina</first><last>Khusainova</last></author>
      <author><first>Adil</first><last>Khan</last></author>
      <author><first>Adín Ramírez</first><last>Rivera</last></author>
      <author><first>Vitaly</first><last>Romanov</last></author>
      <pages>12–20</pages>
      <abstract>The choice of parameter sharing strategy in multilingual machine translation models determines how optimally parameter space is used and hence, directly influences ultimate translation quality. Inspired by linguistic trees that show the degree of relatedness between different languages, the new general approach to parameter sharing in multilingual machine translation was suggested recently. The main idea is to use these expert language hierarchies as a basis for multilingual architecture: the closer two languages are, the more parameters they share. In this work, we test this idea using the Transformer architecture and show that despite the success in previous work there are problems inherent to training such hierarchical models. We demonstrate that in case of carefully chosen training strategy the hierarchical architecture can outperform bilingual models and multilingual models with full parameter sharing.</abstract>
      <url hash="9548e505">2021.vardial-1.2</url>
      <bibkey>khusainova-etal-2021-hierarchical</bibkey>
    </paper>
    <paper id="3">
      <title>Regression Analysis of Lexical and Morpho-Syntactic Properties of Kiezdeutsch</title>
      <author><first>Diego</first><last>Frassinelli</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <author><first>Reem</first><last>Alatrash</last></author>
      <author><first>Dominik</first><last>Schlechtweg</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>21–27</pages>
      <abstract>Kiezdeutsch is a variety of German predominantly spoken by teenagers from multi-ethnic urban neighborhoods in casual conversations with their peers. In recent years, the popularity of Kiezdeutsch has increased among young people, independently of their socio-economic origin, and has spread in social media, too. While previous studies have extensively investigated this language variety from a linguistic and qualitative perspective, not much has been done from a quantitative point of view. We perform the first large-scale data-driven analysis of the lexical and morpho-syntactic properties of Kiezdeutsch in comparison with standard German. At the level of results, we confirm predictions of previous qualitative analyses and integrate them with further observations on specific linguistic phenomena such as slang and self-centered speaker attitude. At the methodological level, we provide logistic regression as a framework to perform bottom-up feature selection in order to quantify differences across language varieties.</abstract>
      <url hash="d305633d">2021.vardial-1.3</url>
      <bibkey>frassinelli-etal-2021-regression</bibkey>
    </paper>
    <paper id="4">
      <title>Representations of Language Varieties Are Reliable Given Corpus Similarity Measures</title>
      <author><first>Jonathan</first><last>Dunn</last></author>
      <pages>28–38</pages>
      <abstract>This paper measures similarity both within and between 84 language varieties across nine languages. These corpora are drawn from digital sources (the web and tweets), allowing us to evaluate whether such geo-referenced corpora are reliable for modelling linguistic variation. The basic idea is that, if each source adequately represents a single underlying language variety, then the similarity between these sources should be stable across all languages and countries. The paper shows that there is a consistent agreement between these sources using frequency-based corpus similarity measures. This provides further evidence that digital geo-referenced corpora consistently represent local language varieties.</abstract>
      <url hash="b64e4e4d">2021.vardial-1.4</url>
      <bibkey>dunn-2021-representations</bibkey>
    </paper>
    <paper id="5">
      <title>Whit’s the Richt Pairt o Speech: <fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> tagging for <fixed-case>S</fixed-case>cots</title>
      <author><first>Harm</first><last>Lameris</last></author>
      <author><first>Sara</first><last>Stymne</last></author>
      <pages>39–48</pages>
      <abstract>In this paper we explore PoS tagging for the Scots language. Scots is spoken in Scotland and Northern Ireland, and is closely related to English. As no linguistically annotated Scots data were available, we manually PoS tagged a small set that is used for evaluation and training. We use English as a transfer language to examine zero-shot transfer and transfer learning methods. We find that training on a very small amount of Scots data was superior to zero-shot transfer from English. Combining the Scots and English data led to further improvements, with a concatenation method giving the best results. We also compared the use of two different English treebanks and found that a treebank containing web data was superior in the zero-shot setting, while it was outperformed by a treebank containing a mix of genres when combined with Scots data.</abstract>
      <url hash="ce78f7c0">2021.vardial-1.5</url>
      <bibkey>lameris-stymne-2021-whits</bibkey>
    </paper>
    <paper id="6">
      <title>Efficient Unsupervised <fixed-case>NMT</fixed-case> for Related Languages with Cross-Lingual Language Models and Fidelity Objectives</title>
      <author><first>Rami</first><last>Aly</last></author>
      <author><first>Andrew</first><last>Caines</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>49–59</pages>
      <abstract>The most successful approach to Neural Machine Translation (NMT) when only monolingual training data is available, called unsupervised machine translation, is based on back-translation where noisy translations are generated to turn the task into a supervised one. However, back-translation is computationally very expensive and inefficient. This work explores a novel, efficient approach to unsupervised NMT. A transformer, initialized with cross-lingual language model weights, is fine-tuned exclusively on monolingual data of the target language by jointly learning on a paraphrasing and denoising autoencoder objective. Experiments are conducted on WMT datasets for German-English, French-English, and Romanian-English. Results are competitive to strong baseline unsupervised NMT models, especially for closely related source languages (German) compared to more distant ones (Romanian, French), while requiring about a magnitude less training time.</abstract>
      <url hash="bcbdfbf8">2021.vardial-1.6</url>
      <bibkey>aly-etal-2021-efficient</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2015">WMT 2015</pwcdataset>
    </paper>
    <paper id="7">
      <title>Fine-tuning Distributional Semantic Models for Closely-Related Languages</title>
      <author><first>Kushagra</first><last>Bhatia</last></author>
      <author><first>Divyanshu</first><last>Aggarwal</last></author>
      <author><first>Ashwini</first><last>Vaidya</last></author>
      <pages>60–66</pages>
      <abstract>In this paper we compare the performance of three models: SGNS (skip-gram negative sampling) and augmented versions of SVD (singular value decomposition) and PPMI (Positive Pointwise Mutual Information) on a word similarity task. We particularly focus on the role of hyperparameter tuning for Hindi based on recommendations made in previous work (on English). Our results show that there are language specific preferences for these hyperparameters. We extend the best settings for Hindi to a set of related languages: Punjabi, Gujarati and Marathi with favourable results. We also find that a suitably tuned SVD model outperforms SGNS for most of our languages and is also more robust in a low-resource setting.</abstract>
      <url hash="e4d222b8">2021.vardial-1.7</url>
      <attachment type="OptionalSupplementaryMaterial" hash="82ff6561">2021.vardial-1.7.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>bhatia-etal-2021-fine</bibkey>
    </paper>
    <paper id="8">
      <title>Discriminating Between Similar <fixed-case>N</fixed-case>ordic Languages</title>
      <author><first>René</first><last>Haas</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <pages>67–75</pages>
      <abstract>Automatic language identification is a challenging problem. Discriminating between closely related languages is especially difficult. This paper presents a machine learning approach for automatic language identification for the Nordic languages, which often suffer miscategorisation by existing state-of-the-art tools. Concretely we will focus on discrimination between six Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokmål), Faroese and Icelandic.</abstract>
      <url hash="19384313">2021.vardial-1.8</url>
      <bibkey>haas-derczynski-2021-discriminating</bibkey>
      <pwccode url="https://github.com/StrombergNLP/NordicDSL" additional="true">StrombergNLP/NordicDSL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nordic-langid">nordic_langid</pwcdataset>
    </paper>
    <paper id="9">
      <title>Naive <fixed-case>B</fixed-case>ayes-based Experiments in <fixed-case>R</fixed-case>omanian Dialect Identification</title>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Heidi</first><last>Jauhiainen</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <pages>76–83</pages>
      <abstract>This article describes the experiments and systems developed by the SUKI team for the second edition of the Romanian Dialect Identification (RDI) shared task which was organized as part of the 2021 VarDial Evaluation Campaign. We submitted two runs to the shared task and our second submission was the overall best submission by a noticeable margin. Our best submission used a character n-gram based naive Bayes classifier with adaptive language models. We describe our experiments on the development set leading to both submissions.</abstract>
      <url hash="584f3059">2021.vardial-1.9</url>
      <bibkey>jauhiainen-etal-2021-naive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/moroco">MOROCO</pwcdataset>
    </paper>
    <paper id="10">
      <title><fixed-case>U</fixed-case>nibuc<fixed-case>K</fixed-case>ernel: Geolocating <fixed-case>S</fixed-case>wiss <fixed-case>G</fixed-case>erman Jodels Using Ensemble Learning</title>
      <author><first>Gaman</first><last>Mihaela</last></author>
      <author><first>Sebastian</first><last>Cojocariu</last></author>
      <author><first>Radu Tudor</first><last>Ionescu</last></author>
      <pages>84–95</pages>
      <abstract>In this work, we describe our approach addressing the Social Media Variety Geolocation task featured in the 2021 VarDial Evaluation Campaign. We focus on the second subtask, which is based on a data set formed of approximately 30 thousand Swiss German Jodels. The dialect identification task is about accurately predicting the latitude and longitude of test samples. We frame the task as a double regression problem, employing an XGBoost meta-learner with the combined power of a variety of machine learning approaches to predict both latitude and longitude. The models included in our ensemble range from simple regression techniques, such as Support Vector Regression, to deep neural models, such as a hybrid neural network and a neural transformer. To minimize the prediction error, we approach the problem from a few different perspectives and consider various types of features, from low-level character n-grams to high-level BERT embeddings. The XGBoost ensemble resulted from combining the power of the aforementioned methods achieves a median distance of 23.6 km on the test data, which places us on the third place in the ranking, at a difference of 6.05 km and 2.9 km from the submissions on the first and second places, respectively.</abstract>
      <url hash="0b01f7c0">2021.vardial-1.10</url>
      <bibkey>mihaela-etal-2021-unibuckernel</bibkey>
    </paper>
    <paper id="11">
      <title>Optimizing a Supervised Classifier for a Difficult Language Identification Problem</title>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>96–101</pages>
      <abstract>This paper describes the system developed by the Laboratoire d’analyse statistique des textes for the Dravidian Language Identification (DLI) shared task of VarDial 2021. This task is particularly difficult because the materials consists of short YouTube comments, written in Roman script, from three closely related Dravidian languages, and a fourth category consisting of several other languages in varying proportions, all mixed with English. The proposed system is made up of a logistic regression model which uses as only features n-grams of characters with a maximum length of 5. After its optimization both in terms of the feature weighting and the classifier parameters, it ranked first in the challenge. The additional analyses carried out underline the importance of optimization, especially when the measure of effectiveness is the Macro-F1.</abstract>
      <url hash="2f5da910">2021.vardial-1.11</url>
      <bibkey>bestgen-2021-optimizing</bibkey>
    </paper>
    <paper id="12">
      <title>Comparing the Performance of <fixed-case>CNN</fixed-case>s and Shallow Models for Language Identification</title>
      <author><first>Andrea</first><last>Ceolin</last></author>
      <pages>102–112</pages>
      <abstract>In this work we compare the performance of convolutional neural networks and shallow models on three out of the four language identification shared tasks proposed in the VarDial Evaluation Campaign 2021. In our experiments, convolutional neural networks and shallow models yielded comparable performance in the Romanian Dialect Identification (RDI) and the Dravidian Language Identification (DLI) shared tasks, after the training data was augmented, while an ensemble of support vector machines and Naïve Bayes models was the best performing model in the Uralic Language Identification (ULI) task. While the deep learning models did not achieve state-of-the-art performance at the tasks and tended to overfit the data, the ensemble method was one of two methods that beat the existing baseline for the first track of the ULI shared task.</abstract>
      <url hash="404ffba3">2021.vardial-1.12</url>
      <bibkey>ceolin-2021-comparing</bibkey>
      <pwccode url="https://github.com/andreaceolin/vardial2021" additional="false">andreaceolin/vardial2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/moroco">MOROCO</pwcdataset>
    </paper>
    <paper id="13">
      <title>Dialect Identification through Adversarial Learning and Knowledge Distillation on <fixed-case>R</fixed-case>omanian <fixed-case>BERT</fixed-case></title>
      <author><first>George-Eduard</first><last>Zaharia</last></author>
      <author><first>Andrei-Marius</first><last>Avram</last></author>
      <author><first>Dumitru-Clementin</first><last>Cercel</last></author>
      <author><first>Traian</first><last>Rebedea</last></author>
      <pages>113–119</pages>
      <abstract>Dialect identification is a task with applicability in a vast array of domains, ranging from automatic speech recognition to opinion mining. This work presents our architectures used for the VarDial 2021 Romanian Dialect Identification subtask. We introduced a series of solutions based on Romanian or multilingual Transformers, as well as adversarial training techniques. At the same time, we experimented with a knowledge distillation tool in order to check whether a smaller model can maintain the performance of our best approach. Our best solution managed to obtain a weighted F1-score of 0.7324, allowing us to obtain the 2nd place on the leaderboard.</abstract>
      <url hash="1b362255">2021.vardial-1.13</url>
      <bibkey>zaharia-etal-2021-dialect</bibkey>
    </paper>
    <paper id="14">
      <title>Comparing Approaches to <fixed-case>D</fixed-case>ravidian Language Identification</title>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>120–127</pages>
      <abstract>This paper describes the submissions by team HWR to the Dravidian Language Identification (DLI) shared task organized at VarDial 2021 workshop. The DLI training set includes 16,674 YouTube comments written in Roman script containing code-mixed text with English and one of the three South Dravidian languages: Kannada, Malayalam, and Tamil. We submitted results generated using two models, a Naive Bayes classifier with adaptive language models, which has shown to obtain competitive performance in many language and dialect identification tasks, and a transformer-based model which is widely regarded as the state-of-the-art in a number of NLP tasks. Our first submission was sent in the closed submission track using only the training set provided by the shared task organisers, whereas the second submission is considered to be open as it used a pretrained model trained with external data. Our team attained shared second position in the shared task with the submission based on Naive Bayes. Our results reinforce the idea that deep learning methods are not as competitive in language identification related tasks as they are in many other text classification tasks.</abstract>
      <url hash="19975f07">2021.vardial-1.14</url>
      <bibkey>jauhiainen-etal-2021-comparing</bibkey>
    </paper>
    <paper id="15">
      <title>N-gram and Neural Models for <fixed-case>U</fixed-case>ralic Language Identification: <fixed-case>NRC</fixed-case> at <fixed-case>V</fixed-case>ar<fixed-case>D</fixed-case>ial 2021</title>
      <author><first>Gabriel</first><last>Bernier-Colborne</last></author>
      <author><first>Serge</first><last>Leger</last></author>
      <author><first>Cyril</first><last>Goutte</last></author>
      <pages>128–134</pages>
      <abstract>We describe the systems developed by the National Research Council Canada for the Uralic language identification shared task at the 2021 VarDial evaluation campaign. We evaluated two different approaches to this task: a probabilistic classifier exploiting only character 5-grams as features, and a character-based neural network pre-trained through self-supervision, then fine-tuned on the language identification task. The former method turned out to perform better, which casts doubt on the usefulness of deep learning methods for language identification, where they have yet to convincingly and consistently outperform simpler and less costly classification algorithms exploiting n-gram features.</abstract>
      <url hash="65413398">2021.vardial-1.15</url>
      <bibkey>bernier-colborne-etal-2021-n</bibkey>
    </paper>
    <paper id="16">
      <title>Social Media Variety Geolocation with geo<fixed-case>BERT</fixed-case></title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <pages>135–140</pages>
      <abstract>This paper describes the Helsinki–Ljubljana contribution to the VarDial 2021 shared task on social media variety geolocation. Following our successful participation at VarDial 2020, we again propose constrained and unconstrained systems based on the BERT architecture. In this paper, we report experiments with different tokenization settings and different pre-trained models, and we contrast our parameter-free regression approach with various classification schemes proposed by other participants at VarDial 2020. Both the code and the best-performing pre-trained models are made freely available.</abstract>
      <url hash="67e5355e">2021.vardial-1.16</url>
      <bibkey>scherrer-ljubesic-2021-social</bibkey>
    </paper>
  </volume>
</collection>
