<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.bea">
  <volume id="1" ingest-date="2022-06-30">
    <meta>
      <booktitle>Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022)</booktitle>
      <editor><first>Ekaterina</first><last>Kochmar</last></editor>
      <editor><first>Jill</first><last>Burstein</last></editor>
      <editor><first>Andrea</first><last>Horbach</last></editor>
      <editor><first>Ronja</first><last>Laarmann-Quante</last></editor>
      <editor><first>Nitin</first><last>Madnani</last></editor>
      <editor><first>Anaïs</first><last>Tack</last></editor>
      <editor><first>Victoria</first><last>Yaneva</last></editor>
      <editor><first>Zheng</first><last>Yuan</last></editor>
      <editor><first>Torsten</first><last>Zesch</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, Washington</address>
      <month>July</month>
      <year>2022</year>
      <url hash="ce45f200">2022.bea-1</url>
      <venue>bea</venue>
    </meta>
    <frontmatter>
      <url hash="6ecf8b43">2022.bea-1.0</url>
      <bibkey>bea-2022-innovative</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Using Item Response Theory to Measure Gender and Racial Bias of a <fixed-case>BERT</fixed-case>-based Automated <fixed-case>E</fixed-case>nglish Speech Assessment System</title>
      <author><first>Alexander</first><last>Kwako</last></author>
      <author><first>Yixin</first><last>Wan</last></author>
      <author><first>Jieyu</first><last>Zhao</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Li</first><last>Cai</last></author>
      <author><first>Mark</first><last>Hansen</last></author>
      <pages>1-7</pages>
      <abstract>Recent advances in natural language processing and transformer-based models have made it easier to implement accurate, automated English speech assessments. Yet, without careful examination, applications of these models may exacerbate social prejudices based on gender and race. This study addresses the need to examine potential biases of transformer-based models in the context of automated English speech assessment. For this purpose, we developed a BERT-based automated speech assessment system and investigated gender and racial bias of examinees’ automated scores. Gender and racial bias was measured by examining differential item functioning (DIF) using an item response theory framework. Preliminary results, which focused on a single verbal-response item, showed no statistically significant DIF based on gender or race for automated scores.</abstract>
      <url hash="8e1f7e9b">2022.bea-1.1</url>
      <bibkey>kwako-etal-2022-using</bibkey>
      <doi>10.18653/v1/2022.bea-1.1</doi>
      <video href="2022.bea-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Automatic scoring of short answers using justification cues estimated by <fixed-case>BERT</fixed-case></title>
      <author><first>Shunya</first><last>Takano</last></author>
      <author><first>Osamu</first><last>Ichikawa</last></author>
      <pages>8-13</pages>
      <abstract>Automated scoring technology for short-answer questions has been attracting attention to improve the fairness of scoring and reduce the burden on the scorer. In general, a large amount of data is required to train an automated scoring model. The training data consists of the answer texts and the scoring data assigned to them. It may also include annotations indicating key word sequences. These data must be prepared manually, which is costly. Many previous studies have created models with large amounts of training data specific to each question. This paper aims to achieve equivalent performance with less training data by utilizing a BERT model that has been pre-trained on a large amount of general text data not necessarily related to short answer questions. On the RIKEN dataset, the proposed method reduces the training data from the 800 data required in the past to about 400 data, and still achieves scoring accuracy comparable to that of humans.</abstract>
      <url hash="09330a86">2022.bea-1.2</url>
      <bibkey>takano-ichikawa-2022-automatic</bibkey>
      <doi>10.18653/v1/2022.bea-1.2</doi>
      <video href="2022.bea-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Mitigating Learnerese Effects for <fixed-case>CEFR</fixed-case> Classification</title>
      <author><first>Rricha</first><last>Jalota</last></author>
      <author><first>Peter</first><last>Bourgonje</last></author>
      <author><first>Jan</first><last>Van Sas</last></author>
      <author><first>Huiyan</first><last>Huang</last></author>
      <pages>14-21</pages>
      <abstract>The role of an author’s L1 in SLA can be challenging for automated CEFR classification, in that texts from different L1 groups may be too heterogeneous to combine them as training data. We experiment with recent debiasing approaches by attempting to devoid textual representations of L1 features. This results in a more homogeneous group when aggregating CEFR-annotated texts from different L1 groups, leading to better classification performance. Using iterative null-space projection, we marginally improve classification performance for a linear classifier by 1 point. An MLP (e.g. non-linear) classifier remains unaffected by this procedure. We discuss possible directions of future work to attempt to increase this performance gain.</abstract>
      <url hash="b983557c">2022.bea-1.3</url>
      <bibkey>jalota-etal-2022-mitigating</bibkey>
      <doi>10.18653/v1/2022.bea-1.3</doi>
      <video href="2022.bea-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Automatically Detecting Reduced-formed <fixed-case>E</fixed-case>nglish Pronunciations by Using Deep Learning</title>
      <author><first>Lei</first><last>Chen</last></author>
      <author><first>Chenglin</first><last>Jiang</last></author>
      <author><first>Yiwei</first><last>Gu</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Jiahong</first><last>Yuan</last></author>
      <pages>22-26</pages>
      <abstract>Reduced form pronunciations are widely used by native English speakers, especially in casual conversations. Second language (L2) learners have difficulty in processing reduced form pronunciations in listening comprehension and face challenges in production too. Meanwhile, training applications dedicated to reduced forms are still few. To solve this issue, we report on our first effort of using deep learning to evaluate L2 learners’ reduced form pronunciations. Compared with a baseline solution that uses an ASR to determine regular or reduced-formed pronunciations, a classifier that learns representative features via a convolution neural network (CNN) on low-level acoustic features, yields higher detection performance. F-1 metric has been increased from $0.690$ to $0.757$ on the reduction task. Furthermore, adding word entities to compute attention weights to better adjust the features learned by the CNN model helps increasing F-1 to $0.763$.</abstract>
      <url hash="c462e340">2022.bea-1.4</url>
      <bibkey>chen-etal-2022-automatically</bibkey>
      <doi>10.18653/v1/2022.bea-1.4</doi>
    </paper>
    <paper id="5">
      <title>A Baseline Readability Model for <fixed-case>C</fixed-case>ebuano</title>
      <author><first>Joseph Marvin</first><last>Imperial</last></author>
      <author><first>Lloyd Lois Antonie</first><last>Reyes</last></author>
      <author><first>Michael Antonio</first><last>Ibanez</last></author>
      <author><first>Ranz</first><last>Sapinit</last></author>
      <author><first>Mohammed</first><last>Hussien</last></author>
      <pages>27-32</pages>
      <abstract>In this study, we developed the first baseline readability model for the Cebuano language. Cebuano is the second most-used native language in the Philippines with about 27.5 million speakers. As the baseline, we extracted traditional or surface-based features, syllable patterns based from Cebuano’s documented orthography, and neural embeddings from the multilingual BERT model. Results show that the use of the first two handcrafted linguistic features obtained the best performance trained on an optimized Random Forest model with approximately 87% across all metrics. The feature sets and algorithm used also is similar to previous results in readability assessment for the Filipino language—showing potential of crosslingual application. To encourage more work for readability assessment in Philippine languages such as Cebuano, we open-sourced both code and data.</abstract>
      <url hash="ca6d7896">2022.bea-1.5</url>
      <bibkey>imperial-etal-2022-baseline</bibkey>
      <doi>10.18653/v1/2022.bea-1.5</doi>
      <video href="2022.bea-1.5.mp4"/>
      <revision id="1" href="2022.bea-1.5v1" hash="22c7f9d0"/>
      <revision id="2" href="2022.bea-1.5v2" hash="ca6d7896" date="2022-10-27">Fixed Acknowledgments.</revision>
      <pwccode url="https://github.com/imperialite/cebuano-readability" additional="false">imperialite/cebuano-readability</pwccode>
    </paper>
    <paper id="6">
      <title>Generation of Synthetic Error Data of Verb Order Errors for <fixed-case>S</fixed-case>wedish</title>
      <author><first>Judit</first><last>Casademont Moner</last></author>
      <author><first>Elena</first><last>Volodina</last></author>
      <pages>33-38</pages>
      <abstract>We report on our work-in-progress to generate a synthetic error dataset for Swedish by replicating errors observed in the authentic error annotated dataset. We analyze a small subset of authentic errors, capture regular patterns based on parts of speech, and design a set of rules to corrupt new data. We explore the approach and identify its capabilities, advantages and limitations as a way to enrich the existing collection of error-annotated data. This work focuses on word order errors, specifically those involving the placement of finite verbs in a sentence.</abstract>
      <url hash="3440537a">2022.bea-1.6</url>
      <bibkey>casademont-moner-volodina-2022-generation</bibkey>
      <doi>10.18653/v1/2022.bea-1.6</doi>
      <video href="2022.bea-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>A Dependency Treebank of Spoken Second Language <fixed-case>E</fixed-case>nglish</title>
      <author><first>Kristopher</first><last>Kyle</last></author>
      <author><first>Masaki</first><last>Eguchi</last></author>
      <author><first>Aaron</first><last>Miller</last></author>
      <author><first>Theodore</first><last>Sither</last></author>
      <pages>39-45</pages>
      <abstract>In this paper, we introduce a dependency treebank of spoken second language (L2) English that is annotated with part of speech (Penn POS) tags and syntactic dependencies (Universal Dependencies). We then evaluate the degree to which the use of this treebank as training data affects POS and UD annotation accuracy for L1 web texts, L2 written texts, and L2 spoken texts as compared to models trained on L1 texts only.</abstract>
      <url hash="d5b05af9">2022.bea-1.7</url>
      <bibkey>kyle-etal-2022-dependency</bibkey>
      <doi>10.18653/v1/2022.bea-1.7</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="8">
      <title>Starting from “Zero”: An Incremental Zero-shot Learning Approach for Assessing Peer Feedback Comments</title>
      <author><first>Qinjin</first><last>Jia</last></author>
      <author><first>Yupeng</first><last>Cao</last></author>
      <author><first>Edward</first><last>Gehringer</last></author>
      <pages>46-50</pages>
      <abstract>Peer assessment is an effective and efficient pedagogical strategy for delivering feedback to learners. Asking students to provide quality feedback, which contains suggestions and mentions problems, can promote metacognition by reviewers and better assist reviewees in revising their work. Thus, various supervised machine learning algorithms have been proposed to detect quality feedback. However, all these powerful algorithms have the same Achilles’ heel: the reliance on sufficient historical data. In other words, collecting adequate peer feedback for training a supervised algorithm can take several semesters before the model can be deployed to a new class. In this paper, we present a new paradigm, called incremental zero-shot learning (IZSL), to tackle the problem of lacking sufficient historical data. Our results show that the method can achieve acceptable “cold-start” performance without needing any domain data, and it outperforms BERT when trained on the same data collected incrementally.</abstract>
      <url hash="e9b244a5">2022.bea-1.8</url>
      <bibkey>jia-etal-2022-starting</bibkey>
      <doi>10.18653/v1/2022.bea-1.8</doi>
      <video href="2022.bea-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>On Assessing and Developing Spoken ’Grammatical Error Correction’ Systems</title>
      <author><first>Yiting</first><last>Lu</last></author>
      <author><first>Stefano</first><last>Bannò</last></author>
      <author><first>Mark</first><last>Gales</last></author>
      <pages>51-60</pages>
      <abstract>Spoken ‘grammatical error correction’ (SGEC) is an important process to provide feedback for second language learning. Due to a lack of end-to-end training data, SGEC is often implemented as a cascaded, modular system, consisting of speech recognition, disfluency removal, and grammatical error correction (GEC). This cascaded structure enables efficient use of training data for each module. It is, however, difficult to compare and evaluate the performance of individual modules as preceeding modules may introduce errors. For example the GEC module input depends on the output of non-native speech recognition and disfluency detection, both challenging tasks for learner data.This paper focuses on the assessment and development of SGEC systems. We first discuss metrics for evaluating SGEC, both individual modules and the overall system. The system-level metrics enable tuning for optimal system performance. A known issue in cascaded systems is error propagation between modules.To mitigate this problem semi-supervised approaches and self-distillation are investigated. Lastly, when SGEC system gets deployed it is important to give accurate feedback to users. Thus, we apply filtering to remove edits with low-confidence, aiming to improve overall feedback precision. The performance metrics are examined on a Linguaskill multi-level data set, which includes the original non-native speech, manual transcriptions and reference grammatical error corrections, to enable system analysis and development.</abstract>
      <url hash="019a3abe">2022.bea-1.9</url>
      <bibkey>lu-etal-2022-assessing</bibkey>
      <doi>10.18653/v1/2022.bea-1.9</doi>
      <video href="2022.bea-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Automatic True/False Question Generation for Educational Purpose</title>
      <author><first>Bowei</first><last>Zou</last></author>
      <author><first>Pengfei</first><last>Li</last></author>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Ai Ti</first><last>Aw</last></author>
      <pages>61-70</pages>
      <abstract>In field of teaching, true/false questioning is an important educational method for assessing students’ general understanding of learning materials. Manually creating such questions requires extensive human effort and expert knowledge. Question Generation (QG) technique offers the possibility to automatically generate a large number of questions. However, there is limited work on automatic true/false question generation due to the lack of training data and difficulty finding question-worthy content. In this paper, we propose an unsupervised True/False Question Generation approach (TF-QG) that automatically generates true/false questions from a given passage for reading comprehension test. TF-QG consists of a template-based framework that aims to test the specific knowledge in the passage by leveraging various NLP techniques, and a generative framework to generate more flexible and complicated questions by using a novel masking-and-infilling strategy. Human evaluation shows that our approach can generate high-quality and valuable true/false questions. In addition, simulated testing on the generated questions challenges the state-of-the-art inference models from NLI, QA, and fact verification tasks.</abstract>
      <url hash="d61a76b9">2022.bea-1.10</url>
      <bibkey>zou-etal-2022-automatic</bibkey>
      <doi>10.18653/v1/2022.bea-1.10</doi>
      <video href="2022.bea-1.10.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
    </paper>
    <paper id="11">
      <title>Fine-tuning Transformers with Additional Context to Classify Discursive Moves in Mathematics Classrooms</title>
      <author><first>Abhijit</first><last>Suresh</last></author>
      <author><first>Jennifer</first><last>Jacobs</last></author>
      <author><first>Margaret</first><last>Perkoff</last></author>
      <author><first>James H.</first><last>Martin</last></author>
      <author><first>Tamara</first><last>Sumner</last></author>
      <pages>71-81</pages>
      <abstract>“Talk moves” are specific discursive strategies used by teachers and students to facilitate conversations in which students share their thinking, and actively consider the ideas of others, and engage in rich discussions. Experts in instructional practices often rely on cues to identify and document these strategies, for example by annotating classroom transcripts. Prior efforts to develop automated systems to classify teacher talk moves using transformers achieved a performance of 76.32% F1. In this paper, we investigate the feasibility of using enriched contextual cues to improve model performance. We applied state-of-the-art deep learning approaches for Natural Language Processing (NLP), including Robustly optimized bidirectional encoder representations from transformers (Roberta) with a special input representation that supports previous and subsequent utterances as context for talk moves classification. We worked with the publically available TalkMoves dataset, which contains utterances sourced from real-world classroom sessions (human- transcribed and annotated). Through a series of experimentations, we found that a combination of previous and subsequent utterances improved the transformers’ ability to differentiate talk moves (by 2.6% F1). These results constitute a new state of the art over previously published results and provide actionable insights to those in the broader NLP community who are working to develop similar transformer-based classification models.</abstract>
      <url hash="aa50196a">2022.bea-1.11</url>
      <bibkey>suresh-etal-2022-fine</bibkey>
      <doi>10.18653/v1/2022.bea-1.11</doi>
    </paper>
    <paper id="12">
      <title>Cross-corpora experiments of automatic proficiency assessment and error detection for spoken <fixed-case>E</fixed-case>nglish</title>
      <author><first>Stefano</first><last>Bannò</last></author>
      <author><first>Marco</first><last>Matassoni</last></author>
      <pages>82-91</pages>
      <abstract>The growing demand for learning English as a second language has led to an increasing interest in automatic approaches for assessing spoken language proficiency. One of the most significant challenges in this field is the lack of publicly available annotated spoken data. Another common issue is the lack of consistency and coherence in human assessment.To tackle both problems, in this paper we address the task of automatically predicting the scores of spoken test responses of English-as-a-second-language learners by training neural models on written data and using the presence of grammatical errors as a feature, as they can be considered consistent indicators of proficiency through their distribution and frequency.Specifically, we train a feature extractor on EFCAMDAT, a large written corpus containing error annotations and proficiency levels assigned by human experts, in order to extract information related to grammatical errors and, in turn, we use the resulting model for inference on the CLC-FCE corpus, on the ICNALE corpus, and on the spoken section of the TLT-school corpus, a collection of proficiency tests taken by Italian students.The work investigates the impact of the feature extractor on spoken proficiency assessment as well as the written-to-spoken approach. We find that our error-based approach can be beneficial for assessing spoken proficiency. The results obtained on the considered datasets are discussed and evaluated with appropriate metrics.</abstract>
      <url hash="1ac3c172">2022.bea-1.12</url>
      <bibkey>banno-matassoni-2022-cross</bibkey>
      <doi>10.18653/v1/2022.bea-1.12</doi>
      <video href="2022.bea-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Activity focused Speech Recognition of Preschool Children in Early Childhood Classrooms</title>
      <author><first>Satwik</first><last>Dutta</last></author>
      <author><first>Dwight</first><last>Irvin</last></author>
      <author><first>Jay</first><last>Buzhardt</last></author>
      <author><first>John H.L.</first><last>Hansen</last></author>
      <pages>92-100</pages>
      <abstract>A supportive environment is vital for overall cognitive development in children. Challenges with direct observation and limitations of access to data driven approaches often hinder teachers or practitioners in early childhood research to modify or enhance classroom structures. Deploying sensor based tools in naturalistic preschool classrooms will thereby help teachers/practitioners to make informed decisions and better support student learning needs. In this study, two elements of eco-behavioral assessment: conversational speech and real-time location are fused together. While various challenges remain in developing Automatic Speech Recognition systems for spontaneous preschool children speech, efforts are made to develop a hybrid ASR engine reporting an effective Word-Error-Rate of 40%. The ASR engine further supports recognition of spoken words, WH-words, and verbs in various activity learning zones in a naturalistic preschool classroom scenario. Activity areas represent various locations within the physical ecology of an early childhood setting, each of which is suited for knowledge and skill enhancement in young children. Capturing children’s communication engagement in such areas could help teachers/practitioners fine-tune their daily activities, without the need for direct observation. This investigation provides evidence of the use of speech technology in educational settings to better support such early childhood intervention.</abstract>
      <url hash="f5608c46">2022.bea-1.13</url>
      <bibkey>dutta-etal-2022-activity</bibkey>
      <doi>10.18653/v1/2022.bea-1.13</doi>
    </paper>
    <paper id="14">
      <title>Structural information in mathematical formulas for exercise difficulty prediction: a comparison of <fixed-case>NLP</fixed-case> representations</title>
      <author><first>Ekaterina</first><last>Loginova</last></author>
      <author><first>Dries</first><last>Benoit</last></author>
      <pages>101-106</pages>
      <abstract>To tailor a learning system to the student’s level and needs, we must consider the characteristics of the learning content, such as its difficulty. While natural language processing allows us to represent text efficiently, the meaningful representation of mathematical formulas in an educational context is still understudied. This paper adopts structural embeddings as a possible way to bridge this gap. Our experiments validate the approach using publicly available datasets to show that incorporating syntactic information can improve performance in predicting the exercise difficulty.</abstract>
      <url hash="e3416641">2022.bea-1.14</url>
      <bibkey>loginova-benoit-2022-structural</bibkey>
      <doi>10.18653/v1/2022.bea-1.14</doi>
      <video href="2022.bea-1.14.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/math">MATH</pwcdataset>
    </paper>
    <paper id="15">
      <title>The Specificity and Helpfulness of Peer-to-Peer Feedback in Higher Education</title>
      <author><first>Roman</first><last>Rietsche</last></author>
      <author><first>Andrew</first><last>Caines</last></author>
      <author><first>Cornelius</first><last>Schramm</last></author>
      <author><first>Dominik</first><last>Pfütze</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>107-117</pages>
      <abstract>With the growth of online learning through MOOCs and other educational applications, it has become increasingly difficult for course providers to offer personalized feedback to students. Therefore asking students to provide feedback to each other has become one way to support learning. This peer-to-peer feedback has become increasingly important whether in MOOCs to provide feedback to thousands of students or in large-scale classes at universities. One of the challenges when allowing peer-to-peer feedback is that the feedback should be perceived as helpful, and an import factor determining helpfulness is how specific the feedback is. However, in classes including thousands of students, instructors do not have the resources to check the specificity of every piece of feedback between students. Therefore, we present an automatic classification model to measure sentence specificity in written feedback. The model was trained and tested on student feedback texts written in German where sentences have been labelled as general or specific. We find that we can automatically classify the sentences with an accuracy of 76.7% using a conventional feature-based approach, whereas transfer learning with BERT for German gives a classification accuracy of 81.1%. However, the feature-based approach comes with lower computational costs and preserves human interpretability of the coefficients. In addition we show that specificity of sentences in feedback texts has a weak positive correlation with perceptions of helpfulness. This indicates that specificity is one of the ingredients of good feedback, and invites further investigation.</abstract>
      <url hash="64f0d21a">2022.bea-1.15</url>
      <bibkey>rietsche-etal-2022-specificity</bibkey>
      <doi>10.18653/v1/2022.bea-1.15</doi>
      <video href="2022.bea-1.15.mp4"/>
    </paper>
    <paper id="16">
      <title>Similarity-Based Content Scoring - How to Make <fixed-case>S</fixed-case>-<fixed-case>BERT</fixed-case> Keep Up With <fixed-case>BERT</fixed-case></title>
      <author><first>Marie</first><last>Bexte</last></author>
      <author><first>Andrea</first><last>Horbach</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>118-123</pages>
      <abstract>The dominating paradigm for content scoring is to learn an instance-based model, i.e. to use lexical features derived from the learner answers themselves. An alternative approach that receives much less attention is however to learn a similarity-based model. We introduce an architecture that efficiently learns a similarity model and find that results on the standard ASAP dataset are on par with a BERT-based classification approach.</abstract>
      <url hash="c6fbe23e">2022.bea-1.16</url>
      <bibkey>bexte-etal-2022-similarity</bibkey>
      <doi>10.18653/v1/2022.bea-1.16</doi>
      <video href="2022.bea-1.16.mp4"/>
      <pwccode url="https://github.com/mariebexte/s-bert-similarity-based-content-scoring" additional="false">mariebexte/s-bert-similarity-based-content-scoring</pwccode>
    </paper>
    <paper id="17">
      <title>Don’t Drop the Topic - The Role of the Prompt in Argument Identification in Student Writing</title>
      <author><first>Yuning</first><last>Ding</last></author>
      <author><first>Marie</first><last>Bexte</last></author>
      <author><first>Andrea</first><last>Horbach</last></author>
      <pages>124-133</pages>
      <abstract>In this paper, we explore the role of topic information in student essays from an argument mining perspective. We cluster a recently released corpus through topic modeling into prompts and train argument identification models on different data settings. Results show that, given the same amount of training data, prompt-specific training performs better than cross-prompt training. However, the advantage can be overcome by introducing large amounts of cross-prompt training data.</abstract>
      <url hash="7843af83">2022.bea-1.17</url>
      <bibkey>ding-etal-2022-dont</bibkey>
      <doi>10.18653/v1/2022.bea-1.17</doi>
      <video href="2022.bea-1.17.mp4"/>
      <pwccode url="https://github.com/yuningding/bea-naacl-2022-38" additional="false">yuningding/bea-naacl-2022-38</pwccode>
    </paper>
    <paper id="18">
      <title><fixed-case>ALEN</fixed-case> App: Argumentative Writing Support To Foster <fixed-case>E</fixed-case>nglish Language Learning</title>
      <author><first>Thiemo</first><last>Wambsganss</last></author>
      <author><first>Andrew</first><last>Caines</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>134-140</pages>
      <abstract>This paper introduces a novel tool to support and engage English language learners with feedback on the quality of their argument structures. We present an approach which automatically detects claim-premise structures and provides visual feedback to the learner to prompt them to repair any broken argumentation structures.To investigate, if our persuasive feedback on language learners’ essay writing tasks engages and supports them in learning better English language, we designed the ALEN app (Argumentation for Learning English). We leverage an argumentation mining model trained on texts written by students and embed it in a writing support tool which provides students with feedback in their essay writing process. We evaluated our tool in two field-studies with a total of 28 students from a German high school to investigate the effects of adaptive argumentation feedback on their learning of English. The quantitative results suggest that using the ALEN app leads to a high self-efficacy, ease-of-use, intention to use and perceived usefulness for students in their English language learning process. Moreover, the qualitative answers indicate the potential benefits of combining grammar feedback with discourse level argumentation mining.</abstract>
      <url hash="b8bcce4d">2022.bea-1.18</url>
      <bibkey>wambsganss-etal-2022-alen</bibkey>
      <doi>10.18653/v1/2022.bea-1.18</doi>
    </paper>
    <paper id="19">
      <title>Assessing sentence readability for <fixed-case>G</fixed-case>erman language learners with broad linguistic modeling or readability formulas: When do linguistic insights make a difference?</title>
      <author><first>Zarah</first><last>Weiss</last></author>
      <author><first>Detmar</first><last>Meurers</last></author>
      <pages>141-153</pages>
      <abstract>We present a new state-of-the-art sentence-wise readability assessment model for German L2 readers. We build a linguistically broadly informed machine learning model and compare its performance against four commonly used readability formulas. To understand when the linguistic insights used to inform our model make a difference for readability assessment and when simple readability formulas suffice, we compare their performance based on two common automatic readability assessment tasks: predictive regression and sentence pair ranking. We find that leveraging linguistic insights yields top performances across tasks, but that for the identification of simplified sentences also readability formulas – which are easier to compute and more accessible – can be sufficiently precise. Linguistically informed modeling, however, is the only viable option for high quality outcomes in fine-grained prediction tasks. We then explore the sentence-wise readability profile of leveled texts written for language learners at a beginning, intermediate, and advanced level of German to showcase the valuable insights that sentence-wise readability assessment can have for the adaptation of learning materials and better understand how sentences’ individual readability contributes to larger texts’ overall readability.</abstract>
      <url hash="01ada88b">2022.bea-1.19</url>
      <bibkey>weiss-meurers-2022-assessing</bibkey>
      <doi>10.18653/v1/2022.bea-1.19</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/textcomplexityde">TextComplexityDE</pwcdataset>
    </paper>
    <paper id="20">
      <title>Parametrizable exercise generation from authentic texts: Effectively targeting the language means on the curriculum</title>
      <author><first>Tanja</first><last>Heck</last></author>
      <author><first>Detmar</first><last>Meurers</last></author>
      <pages>154-166</pages>
      <abstract>We present a parametrizable approach to exercise generation from authentic texts that addresses the need for digital materials designed to practice the language means on the curriculum in a real-life school setting. The tool builds on a language-aware searchengine that helps identify attractive texts rich in the language means to be practiced. Making use of state-of-the-art NLP, the relevant learning targets are identified and transformed intoexercise items embedded in the original context.While the language-aware search engine ensures that these contexts match the learner‘s interests based on the search term used, and the linguistic parametrization of the system then reranks the results to prioritize texts that richly represent the learning targets, for theexercise generation to proceed on this basis, an interactive configuration panel allows users to adjust exercise complexity through a range of parameters specifying both properties of thesource sentences and of the exercises.An evaluation of exercises generated from web documents for a representative sample of language means selected from the English curriculum of 7th grade in German secondary school showed that the ombination of language-aware search and exercise generationsuccessfully facilitates the process of generating exercises from authentic texts that support practice of the pedagogical targets.</abstract>
      <url hash="a6119c75">2022.bea-1.20</url>
      <bibkey>heck-meurers-2022-parametrizable</bibkey>
      <doi>10.18653/v1/2022.bea-1.20</doi>
    </paper>
    <paper id="21">
      <title>Selecting Context Clozes for Lightweight Reading Compliance</title>
      <author><first>Greg</first><last>Keim</last></author>
      <author><first>Michael</first><last>Littman</last></author>
      <pages>167-172</pages>
      <abstract>We explore a novel approach to reading compliance, leveraging large language models to select inline challenges that discourage skipping during reading. This lightweight ‘testing’ is accomplished through automatically identified context clozes where the reader must supply a missing word that would be hard to guess if earlier material was skipped. Clozes are selected by scoring each word by the contrast between its likelihood with and without prior sentences as context, preferring to leave gaps where this contrast is high. We report results of an initial human-participant test that indicates this method can find clozes that have this property.</abstract>
      <url hash="97ead864">2022.bea-1.21</url>
      <bibkey>keim-littman-2022-selecting</bibkey>
      <doi>10.18653/v1/2022.bea-1.21</doi>
    </paper>
    <paper id="22">
      <title>‘Meet me at the ribary’ – Acceptability of spelling variants in free-text answers to listening comprehension prompts</title>
      <author><first>Ronja</first><last>Laarmann-Quante</last></author>
      <author><first>Leska</first><last>Schwarz</last></author>
      <author><first>Andrea</first><last>Horbach</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>173-182</pages>
      <abstract>When listening comprehension is tested as a free-text production task, a challenge for scoring the answers is the resulting wide range of spelling variants. When judging whether a variant is acceptable or not, human raters perform a complex holistic decision. In this paper, we present a corpus study in which we analyze human acceptability decisions in a high stakes test for German. We show that for human experts, spelling variants are harder to score consistently than other answer variants.Furthermore, we examine how the decision can be operationalized using features that could be applied by an automatic scoring system. We show that simple measures like edit distance and phonetic similarity between a given answer and the target answer can model the human acceptability decisions with the same inter-annotator agreement as humans, and discuss implications of the remaining inconsistencies.</abstract>
      <url hash="494337fd">2022.bea-1.22</url>
      <bibkey>laarmann-quante-etal-2022-meet</bibkey>
      <doi>10.18653/v1/2022.bea-1.22</doi>
      <video href="2022.bea-1.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Educational Tools for Mapuzugun</title>
      <author><first>Cristian</first><last>Ahumada</last></author>
      <author><first>Claudio</first><last>Gutierrez</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>183-196</pages>
      <abstract>Mapuzugun is the language of the Mapuche people. Due to political and historical reasons, its number of speakers has decreased and the language has been excluded from the educational system in Chile and Argentina. For this reason, it is very important to support the revitalization of the Mapuzugun in all spaces and media of society. In this work we present a tool towards supporting educational activities of Mapuzugun, tailored to the characteristics of the language. The tool consists of three parts: design and development of an orthography detector and converter; a morphological analyzer; and an informal translator. We also present a case study with Mapuzugun students showing promising results.Short abstract in Mapuzugun: Tüfachi küzaw pegelfi kiñe zugun küzawpeyüm kelluaetew pu mapuzugun chillkatufe kimal kizu tañi zugun.</abstract>
      <url hash="d0ceebb5">2022.bea-1.23</url>
      <bibkey>ahumada-etal-2022-educational</bibkey>
      <doi>10.18653/v1/2022.bea-1.23</doi>
    </paper>
    <paper id="24">
      <title>An Evaluation of Binary Comparative Lexical Complexity Models</title>
      <author><first>Kai</first><last>North</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Matthew</first><last>Shardlow</last></author>
      <pages>197-203</pages>
      <abstract>Identifying complex words in texts is an important first step in text simplification (TS) systems. In this paper, we investigate the performance of binary comparative Lexical Complexity Prediction (LCP) models applied to a popular benchmark dataset — the CompLex 2.0 dataset used in SemEval-2021 Task 1. With the data from CompLex 2.0, we create a new dataset contain 1,940 sentences referred to as CompLex-BC. Using CompLex-BC, we train multiple models to differentiate which of two target words is more or less complex in the same sentence. A linear SVM model achieved the best performance in our experiments with an F1-score of 0.86.</abstract>
      <url hash="c80acdb8">2022.bea-1.24</url>
      <bibkey>north-etal-2022-evaluation</bibkey>
      <doi>10.18653/v1/2022.bea-1.24</doi>
    </paper>
    <paper id="25">
      <title>Toward Automatic Discourse Parsing of Student Writing Motivated by Neural Interpretation</title>
      <author><first>James</first><last>Fiacco</last></author>
      <author><first>Shiyan</first><last>Jiang</last></author>
      <author><first>David</first><last>Adamson</last></author>
      <author><first>Carolyn</first><last>Rosé</last></author>
      <pages>204-215</pages>
      <abstract>Providing effective automatic essay feedback is necessary for offering writing instruction at a massive scale. In particular, feedback for promoting coherent flow of ideas in essays is critical. In this paper we propose a state-of-the-art method for automated analysis of structure and flow of writing, referred to as Rhetorical Structure Theory (RST) parsing. In so doing, we lay a foundation for a generalizable approach to automated writing feedback related to structure and flow. We address challenges in automated rhetorical analysis when applied to student writing and evaluate our novel RST parser model on both a recent student writing dataset and a standard benchmark RST parsing dataset.</abstract>
      <url hash="bda2316f">2022.bea-1.25</url>
      <bibkey>fiacco-etal-2022-toward</bibkey>
      <doi>10.18653/v1/2022.bea-1.25</doi>
    </paper>
    <paper id="26">
      <title>Educational Multi-Question Generation for Reading Comprehension</title>
      <author><first>Manav</first><last>Rathod</last></author>
      <author><first>Tony</first><last>Tu</last></author>
      <author><first>Katherine</first><last>Stasaski</last></author>
      <pages>216-223</pages>
      <abstract>Automated question generation has made great advances with the help of large NLP generation models. However, typically only one question is generated for each intended answer. We propose a new task, Multi-Question Generation, aimed at generating multiple semantically similar but lexically diverse questions assessing the same concept. We develop an evaluation framework based on desirable qualities of the resulting questions. Results comparing multiple question generation approaches in the two-question generation condition show a trade-off between question answerability and lexical diversity between the two questions. We also report preliminary results from sampling multiple questions from our model, to explore generating more than two questions. Our task can be used to further explore the educational impact of showing multiple distinct question wordings to students.</abstract>
      <url hash="04926f63">2022.bea-1.26</url>
      <bibkey>rathod-etal-2022-educational</bibkey>
      <doi>10.18653/v1/2022.bea-1.26</doi>
      <video href="2022.bea-1.26.mp4"/>
      <pwccode url="https://github.com/kstats/multiquestiongeneration" additional="false">kstats/multiquestiongeneration</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="27">
      <title>Computationally Identifying Funneling and Focusing Questions in Classroom Discourse</title>
      <author><first>Sterling</first><last>Alic</last></author>
      <author><first>Dorottya</first><last>Demszky</last></author>
      <author><first>Zid</first><last>Mancenido</last></author>
      <author><first>Jing</first><last>Liu</last></author>
      <author><first>Heather</first><last>Hill</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>224-233</pages>
      <abstract>Responsive teaching is a highly effective strategy that promotes student learning. In math classrooms, teachers might {emph{funnel} students towards a normative answer or {emph{focus} students to reflect on their own thinking depending their understanding of math concepts. When teachers focus, they treat students’ contributions as resources for collective sensemaking, and thereby significantly improve students’ achievement and confidence in mathematics. We propose the task of computationally detecting funneling and focusing questions in classroom discourse. We do so by creating and releasing an annotated dataset of 2,348 teacher utterances labeled for funneling and focusing questions, or neither. We introduce supervised and unsupervised approaches to differentiating these questions. Our best model, a supervised RoBERTa model fine-tuned on our dataset, has a strong linear correlation of .76 with human expert labels and with positive educational outcomes, including math instruction quality and student achievement, showing the model’s potential for use in automated teacher feedback tools. Our unsupervised measures show significant but weaker correlations with human labels and outcomes, and they highlight interesting linguistic patterns of funneling and focusing questions. The high performance of the supervised measure indicates its promise for supporting teachers in their instruction.</abstract>
      <url hash="74be3f9a">2022.bea-1.27</url>
      <attachment type="attachment" hash="f8d9c7a0">2022.bea-1.27.attachment.zip</attachment>
      <bibkey>alic-etal-2022-computationally</bibkey>
      <doi>10.18653/v1/2022.bea-1.27</doi>
      <pwccode url="https://github.com/sterlingalic/funneling-focusing" additional="false">sterlingalic/funneling-focusing</pwccode>
    </paper>
    <paper id="28">
      <title>Towards an open-domain chatbot for language practice</title>
      <author><first>Gladys</first><last>Tyen</last></author>
      <author><first>Mark</first><last>Brenchley</last></author>
      <author><first>Andrew</first><last>Caines</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>234-249</pages>
      <abstract>State-of-the-art chatbots for English are now able to hold conversations on virtually any topic (e.g. Adiwardana et al., 2020; Roller et al., 2021). However, existing dialogue systems in the language learning domain still use hand-crafted rules and pattern matching, and are much more limited in scope. In this paper, we make an initial foray into adapting open-domain dialogue generation for second language learning. We propose and implement decoding strategies that can adjust the difficulty level of the chatbot according to the learner’s needs, without requiring further training of the chatbot. These strategies are then evaluated using judgements from human examiners trained in language education. Our results show that re-ranking candidate outputs is a particularly effective strategy, and performance can be further improved by adding sub-token penalties and filtering.</abstract>
      <url hash="6d075c39">2022.bea-1.28</url>
      <bibkey>tyen-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.bea-1.28</doi>
      <pwccode url="https://github.com/whgtyen/controllablecomplexitychatbot" additional="false">whgtyen/controllablecomplexitychatbot</pwccode>
    </paper>
    <paper id="29">
      <title>Response Construct Tagging: <fixed-case>NLP</fixed-case>-Aided Assessment for Engineering Education</title>
      <author><first>Ananya</first><last>Ganesh</last></author>
      <author><first>Hugh</first><last>Scribner</last></author>
      <author><first>Jasdeep</first><last>Singh</last></author>
      <author><first>Katherine</first><last>Goodman</last></author>
      <author><first>Jean</first><last>Hertzberg</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>250-261</pages>
      <abstract>Recent advances in natural language processing (NLP) have greatly helped educational applications, for both teachers and students. In higher education, there is great potential to use NLP tools for advancing pedagogical research. In this paper, we focus on how NLP can help understand student experiences in engineering, thus facilitating engineering educators to carry out large scale analysis that is helpful for re-designing the curriculum. Here, we introduce a new task we call response construct tagging (RCT), in which student responses to tailored survey questions are automatically tagged for six constructs measuring transformative experiences and engineering identity of students.We experiment with state-of-the-art classification models for this task and investigate the effects of different sources of additional information. Our best model achieves an F1 score of 48. We further investigate multi-task training on the related task of sentiment classification, which improves our model’s performance to 55 F1. Finally, we provide a detailed qualitative analysis of model performance.</abstract>
      <url hash="367bd76b">2022.bea-1.29</url>
      <bibkey>ganesh-etal-2022-response</bibkey>
      <doi>10.18653/v1/2022.bea-1.29</doi>
    </paper>
    <paper id="30">
      <title>Towards Automatic Short Answer Assessment for <fixed-case>F</fixed-case>innish as a Paraphrase Retrieval Task</title>
      <author><first>Li-Hsin</first><last>Chang</last></author>
      <author><first>Jenna</first><last>Kanerva</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <pages>262-271</pages>
      <abstract>Automatic grouping of textual answers has the potential of allowing batch grading, but is challenging because the answers, especially longer essays, have many claims. To explore the feasibility of grouping together answers based on their semantic meaning, this paper investigates the grouping of short textual answers, proxies of single claims. This is approached as a paraphrase identification task, where neural and non-neural sentence embeddings and a paraphrase identification model are tested. These methods are evaluated on a dataset consisting of over 4000 short textual answers from various disciplines. The results map out the suitable question types for the paraphrase identification model and those for the neural and non-neural methods.</abstract>
      <url hash="949129d1">2022.bea-1.30</url>
      <bibkey>chang-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.bea-1.30</doi>
      <video href="2022.bea-1.30.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/finnish-paraphrase-corpus">Finnish Paraphrase Corpus</pwcdataset>
    </paper>
    <paper id="31">
      <title>Incremental Disfluency Detection for Spoken Learner <fixed-case>E</fixed-case>nglish</title>
      <author><first>Lucy</first><last>Skidmore</last></author>
      <author><first>Roger</first><last>Moore</last></author>
      <pages>272-278</pages>
      <abstract>Incremental disfluency detection provides a framework for computing communicative meaning from hesitations, repetitions and false starts commonly found in speech. One application of this area of research is in dialogue-based computer-assisted language learning (CALL), where detecting learners’ production issues word-by-word can facilitate timely and pedagogically driven responses from an automated system. Existing research on disfluency detection in learner speech focuses on disfluency removal for subsequent downstream tasks, processing whole utterances non-incrementally. This paper instead explores the application of laughter as a feature for incremental disfluency detection and shows that when combined with silence, these features reduce the impact of learner errors on model precision as well as lead to an overall improvement of model performance. This work adds to the growing body of research incorporating laughter as a feature for dialogue processing tasks and provides further support for the application of multimodality in dialogue-based CALL systems.</abstract>
      <url hash="26e1193b">2022.bea-1.31</url>
      <bibkey>skidmore-moore-2022-incremental</bibkey>
      <doi>10.18653/v1/2022.bea-1.31</doi>
    </paper>
  </volume>
</collection>
