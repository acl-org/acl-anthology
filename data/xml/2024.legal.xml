<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.legal">
  <volume id="1" ingest-date="2024-05-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop on Legal and Ethical Issues in Human Language Technologies @ LREC-COLING 2024</booktitle>
      <editor><first>Ingo</first><last>Siegert</last></editor>
      <editor><first>Khalid</first><last>Choukri</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="402027cc">2024.legal-1</url>
      <venue>legal</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="878d6933">2024.legal-1.0</url>
      <bibkey>legal-2024-legal</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Compliance by Design Methodologies in the Legal Governance Schemes of <fixed-case>E</fixed-case>uropean Data Spaces</title>
      <author><first>Kossay</first><last>Talmoudi</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Isabelle</first><last>Gavanon</last></author>
      <pages>1–5</pages>
      <abstract>Creating novel ways of sharing data to boost the digital economy has been one of the growing priorities of the European Union. In order to realise a set of data-sharing modalities, the European Union funds several projects that aim to put in place Common Data Spaces. These infrastructures are set to be a catalyser for the data economy. However, many hurdles face their implementation. Legal compliance is still one of the major ambiguities of European Common Data Spaces and many initiatives intend to proactively integrate legal compliance schemes in the architecture of sectoral Data Spaces. The various initiatives must navigate a complex web of cross-cutting legal frameworks, including contract law, data protection, intellectual property, protection of trade secrets, competition law, European sovereignty, and cybersecurity obligations. As the conceptualisation of Data Spaces evolves and shows signs of differentiation from one sector to another, it is important to showcase the legal repercussions of the options of centralisation and decentralisation that can be observed in different Data Spaces. This paper will thus delve into their legal requirements and attempt to sketch out a stepping stone for understanding legal governance in data spaces.</abstract>
      <url hash="eabf2214">2024.legal-1.1</url>
      <bibkey>talmoudi-etal-2024-compliance</bibkey>
    </paper>
    <paper id="2">
      <title>A Legal Framework for Natural Language Model Training in <fixed-case>P</fixed-case>ortugal</title>
      <author><first>Ruben</first><last>Almeida</last></author>
      <author><first>Evelin</first><last>Amorim</last></author>
      <pages>6–12</pages>
      <abstract>Recent advances in deep learning have promoted the advent of many computational systems capable of performing intelligent actions that, until then, were restricted to the human intellect. In the particular case of human languages, these advances allowed the introduction of applications like ChatGPT that are capable of generating coherent text without being explicitly programmed to do so. Instead, these models use large volumes of textual data to learn meaningful representations of human languages. Associated with these advances, concerns about copyright and data privacy infringements caused by these applications have emerged. Despite these concerns, the pace at which new natural language processing applications continued to be developed largely outperformed the introduction of new regulations. Today, communication barriers between legal experts and computer scientists motivate many unintentional legal infringements during the development of such applications. In this paper, a multidisciplinary team intends to bridge this communication gap and promote more compliant Portuguese NLP research by presenting a series of everyday NLP use cases, while highlighting the Portuguese legislation that may arise during its development.</abstract>
      <url hash="90be3c2b">2024.legal-1.2</url>
      <bibkey>almeida-amorim-2024-legal</bibkey>
    </paper>
    <paper id="3">
      <title>Intellectual property rights at the training, development and generation stages of Large Language Models</title>
      <author><first>Christin</first><last>Kirchhübel</last></author>
      <author><first>Georgina</first><last>Brown</last></author>
      <pages>13–18</pages>
      <abstract>Large Language Models (LLMs) prompt new questions around Intellectual Property (IP): what is the IP status of the datasets used to train LLMs, the resulting LLMs themselves, and their outputs? The training needs of LLMs may be at odds with current copyright law, and there are active conversations around the ownership of their outputs. A report published by the House of Lords Committee following its inquiry into LLMs and generative AI criticises, among other things, the lack of government guidance, and stresses the need for clarity (through legislation, where appropriate) in this sphere. This paper considers the little guidance and caselaw there is involving AI more broadly to allow us to anticipate legal cases and arguments involving LLMs. Given the pre-emptive nature of this paper, it is not possible to provide comprehensive answers to these questions, but we hope to equip language technology communities with a more informed understanding of the current position with respect to UK copyright and patent law.</abstract>
      <url hash="ca1c348e">2024.legal-1.3</url>
      <bibkey>kirchhubel-brown-2024-intellectual</bibkey>
    </paper>
    <paper id="4">
      <title>Ethical Issues in Language Resources and Language Technology – New Challenges, New Perspectives</title>
      <author><first>Pawel</first><last>Kamocki</last></author>
      <author><first>Andreas</first><last>Witt</last></author>
      <pages>19–23</pages>
      <abstract>This article elaborates on the author’s contribution to the previous edition of the LREC conference, in which they proposed a tentative taxonomy of ethical issues that affect Language Resources (LRs) and Language Technology (LT) at the various stages of their lifecycle (conception, creation, use and evaluation). The proposed taxonomy was built around the following ethical principles: Privacy, Property, Equality, Transparency and Freedom. In this article, the authors would like to: 1) examine whether and how this taxonomy stood the test of time, in light of the recent developments in the legal framework and popularisation of Large Language Models (LLMs); 2) provide some details and a tentative checklist on how the taxonomy can be applied in practice; and 3) develop the taxonomy by adding new principles (Accountability; Risk Anticipation and Limitation; Reliability and Limited Confidence), to address the technological developments in LLMs and the upcoming Artificial Intelligence Act.</abstract>
      <url hash="ae181518">2024.legal-1.4</url>
      <bibkey>kamocki-witt-2024-ethical</bibkey>
    </paper>
    <paper id="5">
      <title>Legal and Ethical Considerations that Hinder the Use of <fixed-case>LLM</fixed-case>s in a <fixed-case>F</fixed-case>innish Institution of Higher Education</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <pages>24–27</pages>
      <abstract>Large language models (LLMs) make it possible to solve many business problems easier than ever before. However, embracing LLMs in an organization may be slowed down due to ethical and legal considerations. In this paper, we will describe some of these issues we have faced at our university while developing university-level NLP tools to empower teaching and study planning. The identified issues touch upon topics such as GDPR, copyright, user account management and fear towards the new technology.</abstract>
      <url hash="d5659e0f">2024.legal-1.5</url>
      <bibkey>hamalainen-2024-legal</bibkey>
    </paper>
    <paper id="6">
      <title>Implications of Regulations on Large Generative <fixed-case>AI</fixed-case> Models in the Super-Election Year and the Impact on Disinformation</title>
      <author><first>Vera</first><last>Schmitt</last></author>
      <author><first>Jakob</first><last>Tesch</last></author>
      <author><first>Eva</first><last>Lopez</last></author>
      <author><first>Tim</first><last>Polzehl</last></author>
      <author><first>Aljoscha</first><last>Burchardt</last></author>
      <author><first>Konstanze</first><last>Neumann</last></author>
      <author><first>Salar</first><last>Mohtaj</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>28–38</pages>
      <abstract>With the rise of Large Generative AI Models (LGAIMs), disinformation online has become more concerning than ever before. Within the super-election year 2024, the influence of mis- and disinformation can severely influence public opinion. To combat the increasing amount of disinformation online, humans need to be supported by AI-based tools to increase the effectiveness of detecting false content. This paper examines the critical intersection of the AI Act with the deployment of LGAIMs for disinformation detection and the implications from research, deployer, and the user’s perspective. The utilization of LGAIMs for disinformation detection falls under the high-risk category defined in the AI Act, leading to several obligations that need to be followed after the enforcement of the AI Act. Among others, the obligations include risk management, transparency, and human oversight which pose the challenge of finding adequate technical interpretations. Furthermore, the paper articulates the necessity for clear guidelines and standards that enable the effective, ethical, and legally compliant use of AI. The paper contributes to the discourse on balancing technological advancement with ethical and legal imperatives, advocating for a collaborative approach to utilizing LGAIMs in safeguarding information integrity and fostering trust in digital ecosystems.</abstract>
      <url hash="19ed3df7">2024.legal-1.6</url>
      <bibkey>schmitt-etal-2024-implications</bibkey>
    </paper>
    <paper id="7">
      <title>Selling Personal Information: Data Brokers and the Limits of <fixed-case>US</fixed-case> Regulation</title>
      <author><first>Denise</first><last>DiPersio</last></author>
      <pages>39–46</pages>
      <abstract>A principal pillar of the US Blueprint for an AI Bill of Rights is data privacy, specifically, that individuals should be protected from abusive practices by data collectors and data aggregators, and that users should have control over how their personal information is collected and used. An area that spotlights the need for such protections is found in the common practices of data brokers who scrape, purchase, process and reassemble personal information in bulk and sell it for a variety of downstream uses. Such activities almost always occur in the absence of users’ knowledge or meaningful consent, yet they are legal under US law. This paper examines how data brokers operate, provides some examples of recent US regulatory actions taken against them, summarizes federal efforts to redress data broker practices and concludes that as long as there continues to be no comprehensive federal data protection and privacy scheme, efforts to control such behavior will have only a limited effect. This paper also addresses the limits of informed consent on the use of personal information in language resources and suggests a solution in an holistic approach to data protection and privacy across the data/development life cycle.</abstract>
      <url hash="497860df">2024.legal-1.7</url>
      <bibkey>dipersio-2024-selling</bibkey>
    </paper>
    <paper id="8">
      <title>What Can <fixed-case>I</fixed-case> Do with this Data Point? Towards Modeling Legal and Ethical Aspects of Linguistic Data Collection and (Re-)use</title>
      <author><first>Annett</first><last>Jorschick</last></author>
      <author><first>Paul T.</first><last>Schrader</last></author>
      <author><first>Hendrik</first><last>Buschmeier</last></author>
      <pages>47–51</pages>
      <abstract>Linguistic data often inherits characteristics that limit open science practices such as data publication, sharing, and reuse. Part of the problem is researchers’ uncertainty about the legal requirements, which need to be considered at the beginning of study planning, when consent forms for participants, ethics applications, and data management plans need to be written. This paper presents a newly funded project that will develop a research data management infrastructure that will provide automated support to researchers in the planning, collection, storage, use, reuse, and sharing of data, taking into account ethical and legal aspects to encourage open science practices.</abstract>
      <url hash="17e33536">2024.legal-1.8</url>
      <bibkey>jorschick-etal-2024-data</bibkey>
    </paper>
    <paper id="9">
      <title>Data-Envelopes for Cultural Heritage: Going beyond Datasheets</title>
      <author><first>Maria</first><last>Eskevich</last></author>
      <author><first>Mrinalini</first><last>Luthra</last></author>
      <pages>52–65</pages>
      <abstract>Cultural heritage data is a rich source of information about the history and culture development in the past. When used with due understanding of its intrinsic complexity it can both support research in social sciences and humanities, and become input for machine learning and artificial intelligence algorithms. In all cases ethical and contextual considerations can be encouraged when the relevant information is provided in a clear and well structured form to potential users before they begin to interact with the data. Proposed data-envelopes, basing on the existing documentation frameworks, address the particular needs and challenges of the cultural heritage field while combining machine-readability and user-friendliness. We develop and test data-envelopes usability on the data from the Huygens Institute for History and Culture of the Netherlands. This paper presents the following contributions: i) we highlight the complexity of CH data, featuring the unique ethical and contextual considerations they entail; ii) we evaluate and compare existing dataset documentation frameworks, examining their suitability for CH datasets; iii) we introduce the “data-envelope”–a machine readable adaptation of existing dataset documentation frameworks, to tackle the specificities of CH datasets. Its modular form is designed to serve not only the needs of machine learning (ML), but also and especially broader user groups varying from humanities scholars, governmental monitoring authorities to citizen scientists and the general public. Importantly, the data-envelope framework emphasises the legal and ethical dimensions of dataset documentation, facilitating compliance with evolving data protection regulations and enhancing the accountability of data stewardship in the cultural heritage sector. We discuss and invite the readers for further conversation on the topic of ethical considerations, and how the different audiences should be informed about the importance of datasets documentation management and their context.</abstract>
      <url hash="ec3fd042">2024.legal-1.9</url>
      <bibkey>eskevich-luthra-2024-data</bibkey>
    </paper>
    <paper id="10">
      <title>Emotional Toll and Coping Strategies: Navigating the Effects of Annotating Hate Speech Data</title>
      <author><first>Maryam M.</first><last>AlEmadi</last></author>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <pages>66–72</pages>
      <abstract>Freedom of speech on online social media platforms, often comes with the cost of hate speech production. Hate speech can be very harmful to the peace and development of societies as they bring about conflict and encourage crime. To regulate the hate speech content, moderators and annotators are employed. In our research, we look at the effects of prolonged exposure to hate speech on the mental and physical health of these annotators, as well as researchers with work revolving around the topic of hate speech. Through the methodology of analyzing literature, we found that prolonged exposure to hate speech does mentally and physically impact annotators and researchers in this field. We also propose solutions to reduce these negative impacts such as providing mental health services, fair labor practices, psychological assessments and interventions, as well as developing AI to assist in the process of hate speech detection.</abstract>
      <url hash="9b0dd575">2024.legal-1.10</url>
      <bibkey>alemadi-zaghouani-2024-emotional</bibkey>
    </paper>
    <paper id="11">
      <title>User Perspective on Anonymity in Voice Assistants – A comparison between <fixed-case>G</fixed-case>ermany and <fixed-case>F</fixed-case>inland</title>
      <author><first>Ingo</first><last>Siegert</last></author>
      <author><first>Silas</first><last>Rech</last></author>
      <author><first>Tom</first><last>Bäckström</last></author>
      <author><first>Matthias</first><last>Haase</last></author>
      <pages>73–78</pages>
      <abstract>This study investigates the growing importance of voice assistants, particularly focusing on their usage patterns and associated user characteristics, trust perceptions, and concerns about data security. While previous research has identified correlations between the use of voice assistants and trust in these technologies, as well as data security concerns, little evidence exists regarding the relationship between individual user traits and perceived trust and security concerns. The study design involves surveying various user attributes, including technical proficiency, personality traits, and experience with digital technologies, alongside attitudes toward and usage of voice assistants. A comparison between Germany and Finland is conducted to explore potential cultural differences. The findings aim to inform strategies for enhancing voice assistant acceptance, including the implementation of anonymization methods.</abstract>
      <url hash="f8c764bf">2024.legal-1.11</url>
      <bibkey>siegert-etal-2024-user</bibkey>
    </paper>
  </volume>
</collection>
