<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.cl">
  <volume id="1">
    <meta>
      <booktitle>Computational Linguistics, Volume 48, Issue 1 - March 2022</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>March</month>
      <year>2022</year>
      <venue>cl</venue>
    </meta>
    <paper id="1">
      <title>Obituary: <fixed-case>M</fixed-case>artin Kay</title>
      <author><first>Ronald M.</first><last>Kaplan</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <doi>10.1162/coli_a_00424</doi>
      <pages>1–3</pages>
      <url hash="e434ff70">2022.cl-1.1</url>
      <bibkey>kaplan-uszkoreit-2022-obituary</bibkey>
    </paper>
    <paper id="2">
      <title>To Augment or Not to Augment? A Comparative Study on Text Augmentation Techniques for Low-Resource <fixed-case>NLP</fixed-case></title>
      <author><first>Gözde Gül</first><last>Şahin</last></author>
      <doi>10.1162/coli_a_00425</doi>
      <abstract>Abstract Data-hungry deep neural networks have established themselves as the de facto standard for many NLP tasks, including the traditional sequence tagging ones. Despite their state-of-the-art performance on high-resource languages, they still fall behind their statistical counterparts in low-resource scenarios. One methodology to counterattack this problem is text augmentation, that is, generating new synthetic training data points from existing data. Although NLP has recently witnessed several new textual augmentation techniques, the field still lacks a systematic performance analysis on a diverse set of languages and sequence tagging tasks. To fill this gap, we investigate three categories of text augmentation methodologies that perform changes on the syntax (e.g., cropping sub-sentences), token (e.g., random word insertion), and character (e.g., character swapping) levels. We systematically compare the methods on part-of-speech tagging, dependency parsing, and semantic role labeling for a diverse set of language families using various models, including the architectures that rely on pretrained multilingual contextualized language models such as mBERT. Augmentation most significantly improves dependency parsing, followed by part-of-speech tagging and semantic role labeling. We find the experimented techniques to be effective on morphologically rich languages in general rather than analytic languages such as Vietnamese. Our results suggest that the augmentation techniques can further improve over strong baselines based on mBERT, especially for dependency parsing. We identify the character-level methods as the most consistent performers, while synonym replacement and syntactic augmenters provide inconsistent improvements. Finally, we discuss that the results most heavily depend on the task, language pair (e.g., syntactic-level techniques mostly benefit higher-level tasks and morphologically richer languages), and model type (e.g., token-level augmentation provides significant improvements for BPE, while character-level ones give generally higher scores for char and mBERT based models).</abstract>
      <pages>5–42</pages>
      <url hash="a51f95a4">2022.cl-1.2</url>
      <bibkey>sahin-2022-augment</bibkey>
    </paper>
    <paper id="3">
      <title>Novelty Detection: A Perspective from Natural Language Processing</title>
      <author><first>Tirthankar</first><last>Ghosal</last></author>
      <author><first>Tanik</first><last>Saikh</last></author>
      <author><first>Tameesh</first><last>Biswas</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <doi>10.1162/coli_a_00429</doi>
      <abstract>The quest for new information is an inborn human trait and has always been quintessential for human survival and progress. Novelty drives curiosity, which in turn drives innovation. In Natural Language Processing (NLP), Novelty Detection refers to finding text that has some new information to offer with respect to whatever is earlier seen or known. With the exponential growth of information all across the Web, there is an accompanying menace of redundancy. A considerable portion of the Web contents are duplicates, and we need efficient mechanisms to retain new information and filter out redundant information. However, detecting redundancy at the semantic level and identifying novel text is not straightforward because the text may have less lexical overlap yet convey the same information. On top of that, non-novel/redundant information in a document may have assimilated from multiple source documents, not just one. The problem surmounts when the subject of the discourse is documents, and numerous prior documents need to be processed to ascertain the novelty/non-novelty of the current one in concern. In this work, we build upon our earlier investigations for document-level novelty detection and present a comprehensive account of our efforts toward the problem. We explore the role of pre-trained Textual Entailment (TE) models to deal with multiple source contexts and present the outcome of our current investigations. We argue that a multipremise entailment task is one close approximation toward identifying semantic-level non-novelty. Our recent approach either performs comparably or achieves significant improvement over the latest reported results on several datasets and across several related tasks (paraphrasing, plagiarism, rewrite). We critically analyze our performance with respect to the existing state of the art and show the superiority and promise of our approach for future investigations. We also present our enhanced dataset TAP-DLND 2.0 and several baselines to the community for further research on document-level novelty detection.</abstract>
      <pages>77–117</pages>
      <url hash="3a678836">2022.cl-1.3</url>
      <bibkey>ghosal-etal-2022-novelty</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="4">
      <title>Improved N-Best Extraction with an Evaluation on Language Data</title>
      <author><first>Johanna</first><last>Björklund</last></author>
      <author><first>Frank</first><last>Drewes</last></author>
      <author><first>Anna</first><last>Jonsson</last></author>
      <doi>10.1162/coli_a_00427</doi>
      <abstract>We show that a previously proposed algorithm for the N-best trees problem can be made more efficient by changing how it arranges and explores the search space. Given an integer N and a weighted tree automaton (wta) M over the tropical semiring, the algorithm computes N trees of minimal weight with respect to M. Compared with the original algorithm, the modifications increase the laziness of the evaluation strategy, which makes the new algorithm asymptotically more efficient than its predecessor. The algorithm is implemented in the software Betty, and compared to the state-of-the-art algorithm for extracting the N best runs, implemented in the software toolkit Tiburon. The data sets used in the experiments are wtas resulting from real-world natural language processing tasks, as well as artificially created wtas with varying degrees of nondeterminism. We find that Betty outperforms Tiburon on all tested data sets with respect to running time, while Tiburon seems to be the more memory-efficient choice.</abstract>
      <pages>119–153</pages>
      <url hash="91a1df11">2022.cl-1.4</url>
      <bibkey>bjorklund-etal-2022-improved</bibkey>
      <pwccode url="https://github.com/tm11ajn/betty" additional="false">tm11ajn/betty</pwccode>
    </paper>
    <paper id="5">
      <title>Linguistic Parameters of Spontaneous Speech for Identifying Mild Cognitive Impairment and <fixed-case>A</fixed-case>lzheimer Disease</title>
      <author><first>Veronika</first><last>Vincze</last></author>
      <author><first>Martina Katalin</first><last>Szabó</last></author>
      <author><first>Ildikó</first><last>Hoffmann</last></author>
      <author><first>László</first><last>Tóth</last></author>
      <author><first>Magdolna</first><last>Pákáski</last></author>
      <author><first>János</first><last>Kálmán</last></author>
      <author><first>Gábor</first><last>Gosztolya</last></author>
      <doi>10.1162/coli_a_00428</doi>
      <abstract>In this article, we seek to automatically identify Hungarian patients suffering from mild cognitive impairment (MCI) or mild Alzheimer disease (mAD) based on their speech transcripts, focusing only on linguistic features. In addition to the features examined in our earlier study, we introduce syntactic, semantic, and pragmatic features of spontaneous speech that might affect the detection of dementia. In order to ascertain the most useful features for distinguishing healthy controls, MCI patients, and mAD patients, we carry out a statistical analysis of the data and investigate the significance level of the extracted features among various speaker group pairs and for various speaking tasks. In the second part of the article, we use this rich feature set as a basis for an effective discrimination among the three speaker groups. In our machine learning experiments, we analyze the efficacy of each feature group separately. Our model that uses all the features achieves competitive scores, either with or without demographic information (3-class accuracy values: 68%–70%, 2-class accuracy values: 77.3%–80%). We also analyze how different data recording scenarios affect linguistic features and how they can be productively used when distinguishing MCI patients from healthy controls.</abstract>
      <pages>119–153</pages>
      <url hash="2038bfad">2022.cl-1.5</url>
      <bibkey>vincze-etal-2022-linguistic</bibkey>
    </paper>
    <paper id="6">
      <title>Deep Learning for Text Style Transfer: A Survey</title>
      <author><first>Di</first><last>Jin</last></author>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <author><first>Olga</first><last>Vechtomova</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <doi>10.1162/coli_a_00426</doi>
      <abstract>Text style transfer is an important task in natural language generation, which aims to control certain attributes in the generated text, such as politeness, emotion, humor, and many others. It has a long history in the field of natural language processing, and recently has re-gained significant attention thanks to the promising performance brought by deep neural models. In this article, we present a systematic survey of the research on neural text style transfer, spanning over 100 representative articles since the first neural text style transfer work in 2017. We discuss the task formulation, existing datasets and subtasks, evaluation, as well as the rich methodologies in the presence of parallel and non-parallel data. We also provide discussions on a variety of important topics regarding the future development of this task.1</abstract>
      <pages>155–205</pages>
      <url hash="ad5beda4">2022.cl-1.6</url>
      <bibkey>jin-etal-2022-deep</bibkey>
      <pwccode url="https://github.com/fuzhenxin/Style-Transfer-in-Text" additional="true">fuzhenxin/Style-Transfer-in-Text</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="7">
      <title>Probing Classifiers: Promises, Shortcomings, and Advances</title>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <doi>10.1162/coli_a_00422</doi>
      <abstract>Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple—a classifier is trained to predict some linguistic property from a model’s representations—and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.</abstract>
      <pages>207–219</pages>
      <url hash="4dd30d51">2022.cl-1.7</url>
      <bibkey>belinkov-2022-probing</bibkey>
    </paper>
    <paper id="8">
      <title>Revisiting the Boundary between <fixed-case>ASR</fixed-case> and <fixed-case>NLU</fixed-case> in the Age of Conversational Dialog Systems</title>
      <author><first>Manaal</first><last>Faruqui</last></author>
      <author><first>Dilek</first><last>Hakkani-Tür</last></author>
      <doi>10.1162/coli_a_00430</doi>
      <abstract>As more users across the world are interacting with dialog agents in their daily life, there is a need for better speech understanding that calls for renewed attention to the dynamics between research in automatic speech recognition (ASR) and natural language understanding (NLU). We briefly review these research areas and lay out the current relationship between them. In light of the observations we make in this article, we argue that (1) NLU should be cognizant of the presence of ASR models being used upstream in a dialog system’s pipeline, (2) ASR should be able to learn from errors found in NLU, (3) there is a need for end-to-end data sets that provide semantic annotations on spoken input, (4) there should be stronger collaboration between ASR and NLU research communities.</abstract>
      <pages>221–232</pages>
      <url hash="d99b7b9c">2022.cl-1.8</url>
      <bibkey>faruqui-hakkani-tur-2022-revisiting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/spoken-squad">Spoken-SQuAD</pwcdataset>
    </paper>
    <paper id="9">
      <title>Natural Language Processing: A Machine Learning Perspective by <fixed-case>Y</fixed-case>ue <fixed-case>Z</fixed-case>hang and Zhiyang Teng</title>
      <author><first>Julia</first><last>Ive</last></author>
      <doi>10.1162/coli_r_00423</doi>
      <pages>233–235</pages>
      <url hash="9ace3399">2022.cl-1.9</url>
      <bibkey>ive-2022-natural</bibkey>
    </paper>
    <paper id="10">
      <title>Erratum for “Formal Basis of a Language Universal”</title>
      <author><first>Miloš</first><last>Stanojević</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <doi>10.1162/coli_x_00432</doi>
      <pages>237–237</pages>
      <url hash="6fd084dd">2022.cl-1.10</url>
      <bibkey>stanojevic-steedman-2022-erratum</bibkey>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Computational Linguistics, Volume 48, Issue 2 - June 2022</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>June</month>
      <year>2022</year>
      <venue>cl</venue>
    </meta>
    <paper id="1">
      <title>Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis</title>
      <author><first>Saif M.</first><last>Mohammad</last></author>
      <doi>10.1162/coli_a_00433</doi>
      <abstract>The importance and pervasiveness of emotions in our lives makes affective computing a tremendously important and vibrant line of work. Systems for automatic emotion recognition (AER) and sentiment analysis can be facilitators of enormous progress (e.g., in improving public health and commerce) but also enablers of great harm (e.g., for suppressing dissidents and manipulating voters). Thus, it is imperative that the affective computing community actively engage with the ethical ramifications of their creations. In this article, I have synthesized and organized information from AI Ethics and Emotion Recognition literature to present fifty ethical considerations relevant to AER. Notably, this ethics sheet fleshes out assumptions hidden in how AER is commonly framed, and in the choices often made regarding the data, method, and evaluation. Special attention is paid to the implications of AER on privacy and social groups. Along the way, key recommendations are made for responsible AER. The objective of the ethics sheet is to facilitate and encourage more thoughtfulness on why to automate, how to automate, and how to judge success well before the building of AER systems. Additionally, the ethics sheet acts as a useful introductory document on emotion recognition (complementing survey articles).</abstract>
      <pages>239–278</pages>
      <url hash="36db3140">2022.cl-2.1</url>
      <bibkey>mohammad-2022-ethics-sheet</bibkey>
    </paper>
    <paper id="2">
      <title>Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarization</title>
      <author><first>Md Tahmid Rahman</first><last>Laskar</last></author>
      <author><first>Enamul</first><last>Hoque</last></author>
      <author><first>Jimmy Xiangji</first><last>Huang</last></author>
      <doi>10.1162/coli_a_00434</doi>
      <abstract>The Query-Focused Text Summarization (QFTS) task aims at building systems that generate the summary of the text document(s) based on the given query. A key challenge in addressing this task is the lack of large labeled data for training the summarization model. In this article, we address this challenge by exploring a series of domain adaptation techniques. Given the recent success of pre-trained transformer models in a wide range of natural language processing tasks, we utilize such models to generate abstractive summaries for the QFTS task for both single-document and multi-document scenarios. For domain adaptation, we apply a variety of techniques using pre-trained transformer-based summarization models including transfer learning, weakly supervised learning, and distant supervision. Extensive experiments on six datasets show that our proposed approach is very effective in generating abstractive summaries for the QFTS task while setting a new state-of-the-art result in several datasets across a set of automatic and human evaluation metrics.</abstract>
      <pages>279–320</pages>
      <url hash="e41774df">2022.cl-2.2</url>
      <bibkey>laskar-etal-2022-domain</bibkey>
      <pwccode url="https://github.com/tahmedge/preqfas" additional="false">tahmedge/preqfas</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mediqa-ans">MEDIQA-AnS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trecqa">TrecQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="3">
      <title>Challenges of Neural Machine Translation for Short Texts</title>
      <author><first>Yu</first><last>Wan</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Derek Fai</first><last>Wong</last></author>
      <author><first>Lidia Sam</first><last>Chao</last></author>
      <author><first>Liang</first><last>Yao</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <doi>10.1162/coli_a_00435</doi>
      <abstract>Short texts (STs) present in a variety of scenarios, including query, dialog, and entity names. Most of the exciting studies in neural machine translation (NMT) are focused on tackling open problems concerning long sentences rather than short ones. The intuition behind is that, with respect to human learning and processing, short sequences are generally regarded as easy examples. In this article, we first dispel this speculation via conducting preliminary experiments, showing that the conventional state-of-the-art NMT approach, namely, Transformer (Vaswani et al. 2017), still suffers from over-translation and mistranslation errors over STs. After empirically investigating the rationale behind this, we summarize two challenges in NMT for STs associated with translation error types above, respectively: (1) the imbalanced length distribution in training set intensifies model inference calibration over STs, leading to more over-translation cases on STs; and (2) the lack of contextual information forces NMT to have higher data uncertainty on short sentences, and thus NMT model is troubled by considerable mistranslation errors. Some existing approaches, like balancing data distribution for training (e.g., data upsampling) and complementing contextual information (e.g., introducing translation memory) can alleviate the translation issues in NMT for STs. We encourage researchers to investigate other challenges in NMT for STs, thus reducing ST translation errors and enhancing translation quality.</abstract>
      <pages>321–342</pages>
      <url hash="d9abf790">2022.cl-2.3</url>
      <bibkey>wan-etal-2022-challenges</bibkey>
    </paper>
    <paper id="4">
      <title>Annotation Curricula to Implicitly Train Non-Expert Annotators</title>
      <author><first>Ji-Ung</first><last>Lee</last></author>
      <author><first>Jan-Christoph</first><last>Klie</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/coli_a_00436</doi>
      <abstract>Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowdsourcing scenarios where domain expertise is not required. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances to be annotated according to a learning curriculum. To do so, this work formalizes annotation curricula for sentence- and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, we provide a proof of concept for annotation curricula in a carefully designed user study with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results indicate that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can be a promising research direction to improve data collection. To facilitate future research—for instance, to adapt annotation curricula to specific tasks and expert annotation scenarios—all code and data from the user study consisting of 2,400 annotations is made available.1</abstract>
      <pages>343–373</pages>
      <url hash="a1ccdcf0">2022.cl-2.4</url>
      <bibkey>lee-etal-2022-annotation</bibkey>
      <pwccode url="https://github.com/ukplab/annotation-curriculum" additional="false">ukplab/annotation-curriculum</pwccode>
    </paper>
    <paper id="5">
      <title>Assessing Corpus Evidence for Formal and Psycholinguistic Constraints on Nonprojectivity</title>
      <author><first>Himanshu</first><last>Yadav</last></author>
      <author><first>Samar</first><last>Husain</last></author>
      <author><first>Richard</first><last>Futrell</last></author>
      <doi>10.1162/coli_a_00437</doi>
      <abstract>Formal constraints on crossing dependencies have played a large role in research on the formal complexity of natural language grammars and parsing. Here we ask whether the apparent evidence for constraints on crossing dependencies in treebanks might arise because of independent constraints on trees, such as low arity and dependency length minimization. We address this question using two sets of experiments. In Experiment 1, we compare the distribution of formal properties of crossing dependencies, such as gap degree, between real trees and baseline trees matched for rate of crossing dependencies and various other properties. In Experiment 2, we model whether two dependencies cross, given certain psycholinguistic properties of the dependencies. We find surprisingly weak evidence for constraints originating from the mild context-sensitivity literature (gap degree and well-nestedness) beyond what can be explained by constraints on rate of crossing dependencies, topological properties of the trees, and dependency length. However, measures that have emerged from the parsing literature (e.g., edge degree, end-point crossings, and heads’ depth difference) differ strongly between real and random trees. Modeling results show that cognitive metrics relating to information locality and working-memory limitations affect whether two dependencies cross or not, but they do not fully explain the distribution of crossing dependencies in natural languages. Together these results suggest that crossing constraints are better characterized by processing pressures than by mildly context-sensitive constraints.</abstract>
      <pages>375–401</pages>
      <url hash="9d6ae8f1">2022.cl-2.5</url>
      <bibkey>yadav-etal-2022-assessing</bibkey>
    </paper>
    <paper id="6">
      <title>Dual Attention Model for Citation Recommendation with Analyses on Explainability of Attention Mechanisms and Qualitative Experiments</title>
      <author><first>Yang</first><last>Zhang</last></author>
      <author><first>Qiang</first><last>Ma</last></author>
      <doi>10.1162/coli_a_00438</doi>
      <abstract>Based on an exponentially increasing number of academic articles, discovering and citing comprehensive and appropriate resources have become non-trivial tasks. Conventional citation recommendation methods suffer from severe information losses. For example, they do not consider the section header of the paper that the author is writing and for which they need to find a citation, the relatedness between the words in the local context (the text span that describes a citation), or the importance of each word from the local context. These shortcomings make such methods insufficient for recommending adequate citations to academic manuscripts. In this study, we propose a novel embedding-based neural network called dual attention model for citation recommendation (DACR) to recommend citations during manuscript preparation. Our method adapts the embedding of three semantic pieces of information: words in the local context, structural contexts,1 and the section on which the author is working. A neural network model is designed to maximize the similarity between the embedding of the three inputs (local context words, section headers, and structural contexts) and the target citation appearing in the context. The core of the neural network model comprises self-attention and additive attention; the former aims to capture the relatedness between the contextual words and structural context, and the latter aims to learn their importance. Recommendation experiments on real-world datasets demonstrate the effectiveness of the proposed approach. To seek explainability on DACR, particularly the two attention mechanisms, the learned weights from them are investigated to determine how the attention mechanisms interpret “relatedness” and “importance” through the learned weights. In addition, qualitative analyses were conducted to testify that DACR could find necessary citations that were not noticed by the authors in the past due to the limitations of the keyword-based searching.</abstract>
      <pages>403–470</pages>
      <url hash="e40d8ac3">2022.cl-2.6</url>
      <bibkey>zhang-ma-2022-dual</bibkey>
    </paper>
    <paper id="7">
      <title>On Learning Interpreted Languages with Recurrent Models</title>
      <author><first>Denis</first><last>Paperno</last></author>
      <doi>10.1162/coli_a_00431</doi>
      <abstract>Can recurrent neural nets, inspired by human sequential data processing, learn to understand language? We construct simplified data sets reflecting core properties of natural language as modeled in formal syntax and semantics: recursive syntactic structure and compositionality. We find LSTM and GRU networks to generalize to compositional interpretation well, but only in the most favorable learning settings, with a well-paced curriculum, extensive training data, and left-to-right (but not right-to-left) composition.</abstract>
      <pages>471–482</pages>
      <url hash="1b28885c">2022.cl-2.7</url>
      <bibkey>paperno-2022-learning</bibkey>
    </paper>
    <paper id="8">
      <title>Boring Problems Are Sometimes the Most Interesting</title>
      <author><first>Richard</first><last>Sproat</last></author>
      <doi>10.1162/coli_a_00439</doi>
      <abstract>In a recent position paper, Turing Award Winners Yoshua Bengio, Geoffrey Hinton, and Yann LeCun make the case that symbolic methods are not needed in AI and that, while there are still many issues to be resolved, AI will be solved using purely neural methods. In this piece I issue a challenge: Demonstrate that a purely neural approach to the problem of text normalization is possible. Various groups have tried, but so far nobody has eliminated the problem of unrecoverable errors, errors where, due to insufficient training data or faulty generalization, the system substitutes some other reading for the correct one. Solutions have been proposed that involve a marriage of traditional finite-state methods with neural models, but thus far nobody has shown that the problem can be solved using neural methods alone. Though text normalization is hardly an “exciting” problem, I argue that until one can solve “boring” problems like that using purely AI methods, one cannot claim that AI is a success.</abstract>
      <pages>483–490</pages>
      <url hash="c7da70a3">2022.cl-2.8</url>
      <bibkey>sproat-2022-boring</bibkey>
    </paper>
  </volume>
  <volume id="3">
    <meta>
      <booktitle>Computational Linguistics, Volume 48, Issue 3 - September 2022</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>September</month>
      <year>2022</year>
      <venue>cl</venue>
    </meta>
    <paper id="1">
      <title>Linear-Time Calculation of the Expected Sum of Edge Lengths in Random Projective Linearizations of Trees</title>
      <author><first>Lluís</first><last>Alemany-Puig</last></author>
      <author><first>Ramon</first><last>Ferrer-i-Cancho</last></author>
      <doi>10.1162/coli_a_00442</doi>
      <abstract>The syntactic structure of a sentence is often represented using syntactic dependency trees. The sum of the distances between syntactically related words has been in the limelight for the past decades. Research on dependency distances led to the formulation of the principle of dependency distance minimization whereby words in sentences are ordered so as to minimize that sum. Numerous random baselines have been defined to carry out related quantitative studies on lan- guages. The simplest random baseline is the expected value of the sum in unconstrained random permutations of the words in the sentence, namely, when all the shufflings of the words of a sentence are allowed and equally likely. Here we focus on a popular baseline: random projective per- mutations of the words of the sentence, that is, permutations where the syntactic dependency structure is projective, a formal constraint that sentences satisfy often in languages. Thus far, the expectation of the sum of dependency distances in random projective shufflings of a sentence has been estimated approximately with a Monte Carlo procedure whose cost is of the order of Rn, where n is the number of words of the sentence and R is the number of samples; it is well known that the larger R is, the lower the error of the estimation but the larger the time cost. Here we pre- sent formulae to compute that expectation without error in time of the order of n. Furthermore, we show that star trees maximize it, and provide an algorithm to retrieve the trees that minimize it.</abstract>
      <pages>491–516</pages>
      <url hash="88a62714">2022.cl-3.1</url>
      <bibkey>alemany-puig-ferrer-i-cancho-2022-linear</bibkey>
    </paper>
    <paper id="2">
      <title>The Impact of Edge Displacement <fixed-case>V</fixed-case>aserstein Distance on <fixed-case>UD</fixed-case> Parsing Performance</title>
      <author><first>Mark</first><last>Anderson</last></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last></author>
      <doi>10.1162/coli_a_00440</doi>
      <abstract>We contribute to the discussion on parsing performance in NLP by introducing a measurement that evaluates the differences between the distributions of edge displacement (the directed distance of edges) seen in training and test data. We hypothesize that this measurement will be related to differences observed in parsing performance across treebanks. We motivate this by building upon previous work and then attempt to falsify this hypothesis by using a number of statistical methods. We establish that there is a statistical correlation between this measurement and parsing performance even when controlling for potential covariants. We then use this to establish a sampling technique that gives us an adversarial and complementary split. This gives an idea of the lower and upper bounds of parsing systems for a given treebank in lieu of freshly sampled data. In a broader sense, the methodology presented here can act as a reference for future correlation-based exploratory work in NLP.</abstract>
      <pages>517–554</pages>
      <url hash="54be90e9">2022.cl-3.2</url>
      <bibkey>anderson-gomez-rodriguez-2022-impact</bibkey>
      <video href="2022.cl-3.2.mp4"/>
      <pwccode url="https://github.com/markda/morphological-complexity" additional="false">markda/morphological-complexity</pwccode>
    </paper>
    <paper id="3">
      <title><fixed-case>UD</fixed-case>apter: Typology-based Language Adapters for Multilingual Dependency Parsing and Sequence Labeling</title>
      <author><first>Ahmet</first><last>Üstün</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Gosse</first><last>Bouma</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <doi>10.1162/coli_a_00443</doi>
      <abstract>Recent advances in multilingual language modeling have brought the idea of a truly universal parser closer to reality. However, such models are still not immune to the “curse of multilinguality”: Cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel language adaptation approach by introducing contextual language adapters to a multilingual parser. Contextual language adapters make it possible to learn adapters via language embeddings while sharing model parameters across languages based on contextual parameter generation. Moreover, our method allows for an easy but effective integration of existing linguistic typology features into the parsing model. Because not all typological features are available for every language, we further combine typological feature prediction with parsing in a multi-task model that achieves very competitive parsing performance without the need for an external prediction system for missing features. The resulting parser, UDapter, can be used for dependency parsing as well as sequence labeling tasks such as POS tagging, morphological tagging, and NER. In dependency parsing, it outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. In sequence labeling tasks, our parser surpasses the baseline on high resource languages, and performs very competitively in a zero-shot setting. Our in-depth analyses show that adapter generation via typological features of languages is key to this success.1</abstract>
      <pages>555–592</pages>
      <url hash="e4b74fbe">2022.cl-3.3</url>
      <bibkey>ustun-etal-2022-udapter</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="4">
      <title>Tractable Parsing for <fixed-case>CCG</fixed-case>s of Bounded Degree</title>
      <author><first>Lena Katharina</first><last>Schiffer</last></author>
      <author><first>Marco</first><last>Kuhlmann</last></author>
      <author><first>Giorgio</first><last>Satta</last></author>
      <doi>10.1162/coli_a_00441</doi>
      <abstract>Unlike other mildly context-sensitive formalisms, Combinatory Categorial Grammar (CCG) cannot be parsed in polynomial time when the size of the grammar is taken into account. Refining this result, we show that the parsing complexity of CCG is exponential only in the maximum degree of composition. When that degree is fixed, parsing can be carried out in polynomial time. Our finding is interesting from a linguistic perspective because a bounded degree of composition has been suggested as a universal constraint on natural language grammar. Moreover, ours is the first complexity result for a version of CCG that includes substitution rules, which are used in practical grammars but have been ignored in theoretical work.</abstract>
      <pages>593–633</pages>
      <url hash="987c3293">2022.cl-3.4</url>
      <bibkey>schiffer-etal-2022-tractable</bibkey>
      <video href="2022.cl-3.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Investigating Language Relationships in Multilingual Sentence Encoders Through the Lens of Linguistic Typology</title>
      <author><first>Rochelle</first><last>Choenni</last></author>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <doi>10.1162/coli_a_00444</doi>
      <abstract>Multilingual sentence encoders have seen much success in cross-lingual model transfer for downstream NLP tasks. The success of this transfer is, however, dependent on the model’s ability to encode the patterns of cross-lingual similarity and variation. Yet, we know relatively little about the properties of individual languages or the general patterns of linguistic variation that the models encode. In this article, we investigate these questions by leveraging knowledge from the field of linguistic typology, which studies and documents structural and semantic variation across languages. We propose methods for separating language-specific subspaces within state-of-the-art multilingual sentence encoders (LASER, M-BERT, XLM, and XLM-R) with respect to a range of typological properties pertaining to lexical, morphological, and syntactic structure. Moreover, we investigate how typological information about languages is distributed across all layers of the models. Our results show interesting differences in encoding linguistic variation associated with different pretraining strategies. In addition, we propose a simple method to study how shared typological properties of languages are encoded in two state-of-the-art multilingual models—M-BERT and XLM-R. The results provide insight into their information-sharing mechanisms and suggest that these linguistic properties are encoded jointly across typologically similar languages in these models.</abstract>
      <pages>635–672</pages>
      <url hash="b19e1a1e">2022.cl-3.5</url>
      <bibkey>choenni-shutova-2022-investigating</bibkey>
      <video href="2022.cl-3.5.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/tatoeba">Tatoeba</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="6">
      <title>Survey of Low-Resource Machine Translation</title>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Antonio Valerio</first><last>Miceli Barone</last></author>
      <author><first>Jindřich</first><last>Helcl</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <doi>10.1162/coli_a_00446</doi>
      <abstract>We present a survey covering the state of the art in low-resource machine translation (MT) research. There are currently around 7,000 languages spoken in the world and almost all language pairs lack significant resources for training machine translation models. There has been increasing interest in research addressing the challenge of producing useful translation models when very little translated training data is available. We present a summary of this topical research field and provide a description of the techniques evaluated by researchers in several recent shared tasks in low-resource MT.</abstract>
      <pages>673–732</pages>
      <url hash="d8060a0e">2022.cl-3.6</url>
      <bibkey>haddow-etal-2022-survey</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samanantar">Samanantar</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tatoeba">Tatoeba</pwcdataset>
    </paper>
    <paper id="7">
      <title>Position Information in Transformers: An Overview</title>
      <author><first>Philipp</first><last>Dufter</last></author>
      <author><first>Martin</first><last>Schmitt</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <doi>10.1162/coli_a_00445</doi>
      <abstract>Transformers are arguably the main workhorse in recent natural language processing research. By definition, a Transformer is invariant with respect to reordering of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this article, we provide an overview and theoretical comparison of existing methods to incorporate position information into Transformer models. The objectives of this survey are to (1) showcase that position information in Transformer is a vibrant and extensive research area; (2) enable the reader to compare existing methods by providing a unified notation and systematization of different approaches along important model dimensions; (3) indicate what characteristics of an application should be taken into account when selecting a position encoding; and (4) provide stimuli for future research.</abstract>
      <pages>733–763</pages>
      <url hash="0acd7075">2022.cl-3.7</url>
      <bibkey>dufter-etal-2022-position</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
  </volume>
</collection>
