<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.cl">
  <volume id="1">
    <meta>
      <booktitle>Computational Linguistics, Volume 48, Issue 1 - March 2022</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>March</month>
      <year>2022</year>
    </meta>
    <paper id="1">
      <title>Obituary: <fixed-case>M</fixed-case>artin Kay</title>
      <author><first>Ronald M.</first><last>Kaplan</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <doi>10.1162/coli_a_00424</doi>
      <pages>1–3</pages>
      <url hash="e434ff70">2022.cl-1.1</url>
      <bibkey>kaplan-uszkoreit-2022-obituary</bibkey>
    </paper>
    <paper id="2">
      <title>To Augment or Not to Augment? A Comparative Study on Text Augmentation Techniques for Low-Resource <fixed-case>NLP</fixed-case></title>
      <author><first>Gözde Gül</first><last>Şahin</last></author>
      <doi>10.1162/coli_a_00425</doi>
      <abstract>Abstract Data-hungry deep neural networks have established themselves as the de facto standard for many NLP tasks, including the traditional sequence tagging ones. Despite their state-of-the-art performance on high-resource languages, they still fall behind their statistical counterparts in low-resource scenarios. One methodology to counterattack this problem is text augmentation, that is, generating new synthetic training data points from existing data. Although NLP has recently witnessed several new textual augmentation techniques, the field still lacks a systematic performance analysis on a diverse set of languages and sequence tagging tasks. To fill this gap, we investigate three categories of text augmentation methodologies that perform changes on the syntax (e.g., cropping sub-sentences), token (e.g., random word insertion), and character (e.g., character swapping) levels. We systematically compare the methods on part-of-speech tagging, dependency parsing, and semantic role labeling for a diverse set of language families using various models, including the architectures that rely on pretrained multilingual contextualized language models such as mBERT. Augmentation most significantly improves dependency parsing, followed by part-of-speech tagging and semantic role labeling. We find the experimented techniques to be effective on morphologically rich languages in general rather than analytic languages such as Vietnamese. Our results suggest that the augmentation techniques can further improve over strong baselines based on mBERT, especially for dependency parsing. We identify the character-level methods as the most consistent performers, while synonym replacement and syntactic augmenters provide inconsistent improvements. Finally, we discuss that the results most heavily depend on the task, language pair (e.g., syntactic-level techniques mostly benefit higher-level tasks and morphologically richer languages), and model type (e.g., token-level augmentation provides significant improvements for BPE, while character-level ones give generally higher scores for char and mBERT based models).</abstract>
      <pages>5–42</pages>
      <url hash="a51f95a4">2022.cl-1.2</url>
      <bibkey>sahin-2022-augment</bibkey>
    </paper>
    <paper id="3">
      <title>Novelty Detection: A Perspective from Natural Language Processing</title>
      <author><first>Tirthankar</first><last>Ghosal</last></author>
      <author><first>Tanik</first><last>Saikh</last></author>
      <author><first>Tameesh</first><last>Biswas</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <doi>10.1162/coli_a_00429</doi>
      <abstract>The quest for new information is an inborn human trait and has always been quintessential for human survival and progress. Novelty drives curiosity, which in turn drives innovation. In Natural Language Processing (NLP), Novelty Detection refers to finding text that has some new information to offer with respect to whatever is earlier seen or known. With the exponential growth of information all across the Web, there is an accompanying menace of redundancy. A considerable portion of the Web contents are duplicates, and we need efficient mechanisms to retain new information and filter out redundant information. However, detecting redundancy at the semantic level and identifying novel text is not straightforward because the text may have less lexical overlap yet convey the same information. On top of that, non-novel/redundant information in a document may have assimilated from multiple source documents, not just one. The problem surmounts when the subject of the discourse is documents, and numerous prior documents need to be processed to ascertain the novelty/non-novelty of the current one in concern. In this work, we build upon our earlier investigations for document-level novelty detection and present a comprehensive account of our efforts toward the problem. We explore the role of pre-trained Textual Entailment (TE) models to deal with multiple source contexts and present the outcome of our current investigations. We argue that a multipremise entailment task is one close approximation toward identifying semantic-level non-novelty. Our recent approach either performs comparably or achieves significant improvement over the latest reported results on several datasets and across several related tasks (paraphrasing, plagiarism, rewrite). We critically analyze our performance with respect to the existing state of the art and show the superiority and promise of our approach for future investigations. We also present our enhanced dataset TAP-DLND 2.0 and several baselines to the community for further research on document-level novelty detection.</abstract>
      <pages>77–117</pages>
      <url hash="3a678836">2022.cl-1.3</url>
      <bibkey>ghosal-etal-2022-novelty</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="4">
      <title>Improved N-Best Extraction with an Evaluation on Language Data</title>
      <author><first>Johanna</first><last>Björklund</last></author>
      <author><first>Frank</first><last>Drewes</last></author>
      <author><first>Anna</first><last>Jonsson</last></author>
      <doi>10.1162/coli_a_00427</doi>
      <abstract>We show that a previously proposed algorithm for the N-best trees problem can be made more efficient by changing how it arranges and explores the search space. Given an integer N and a weighted tree automaton (wta) M over the tropical semiring, the algorithm computes N trees of minimal weight with respect to M. Compared with the original algorithm, the modifications increase the laziness of the evaluation strategy, which makes the new algorithm asymptotically more efficient than its predecessor. The algorithm is implemented in the software Betty, and compared to the state-of-the-art algorithm for extracting the N best runs, implemented in the software toolkit Tiburon. The data sets used in the experiments are wtas resulting from real-world natural language processing tasks, as well as artificially created wtas with varying degrees of nondeterminism. We find that Betty outperforms Tiburon on all tested data sets with respect to running time, while Tiburon seems to be the more memory-efficient choice.</abstract>
      <pages>119–153</pages>
      <url hash="91a1df11">2022.cl-1.4</url>
      <bibkey>bjorklund-etal-2022-improved</bibkey>
      <pwccode url="https://github.com/tm11ajn/betty" additional="false">tm11ajn/betty</pwccode>
    </paper>
    <paper id="5">
      <title>Linguistic Parameters of Spontaneous Speech for Identifying Mild Cognitive Impairment and <fixed-case>A</fixed-case>lzheimer Disease</title>
      <author><first>Veronika</first><last>Vincze</last></author>
      <author><first>Martina Katalin</first><last>Szabó</last></author>
      <author><first>Ildikó</first><last>Hoffmann</last></author>
      <author><first>László</first><last>Tóth</last></author>
      <author><first>Magdolna</first><last>Pákáski</last></author>
      <author><first>János</first><last>Kálmán</last></author>
      <author><first>Gábor</first><last>Gosztolya</last></author>
      <doi>10.1162/coli_a_00428</doi>
      <abstract>In this article, we seek to automatically identify Hungarian patients suffering from mild cognitive impairment (MCI) or mild Alzheimer disease (mAD) based on their speech transcripts, focusing only on linguistic features. In addition to the features examined in our earlier study, we introduce syntactic, semantic, and pragmatic features of spontaneous speech that might affect the detection of dementia. In order to ascertain the most useful features for distinguishing healthy controls, MCI patients, and mAD patients, we carry out a statistical analysis of the data and investigate the significance level of the extracted features among various speaker group pairs and for various speaking tasks. In the second part of the article, we use this rich feature set as a basis for an effective discrimination among the three speaker groups. In our machine learning experiments, we analyze the efficacy of each feature group separately. Our model that uses all the features achieves competitive scores, either with or without demographic information (3-class accuracy values: 68%–70%, 2-class accuracy values: 77.3%–80%). We also analyze how different data recording scenarios affect linguistic features and how they can be productively used when distinguishing MCI patients from healthy controls.</abstract>
      <pages>119–153</pages>
      <url hash="2038bfad">2022.cl-1.5</url>
      <bibkey>vincze-etal-2022-linguistic</bibkey>
    </paper>
    <paper id="6">
      <title>Deep Learning for Text Style Transfer: A Survey</title>
      <author><first>Di</first><last>Jin</last></author>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <author><first>Olga</first><last>Vechtomova</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <doi>10.1162/coli_a_00426</doi>
      <abstract>Text style transfer is an important task in natural language generation, which aims to control certain attributes in the generated text, such as politeness, emotion, humor, and many others. It has a long history in the field of natural language processing, and recently has re-gained significant attention thanks to the promising performance brought by deep neural models. In this article, we present a systematic survey of the research on neural text style transfer, spanning over 100 representative articles since the first neural text style transfer work in 2017. We discuss the task formulation, existing datasets and subtasks, evaluation, as well as the rich methodologies in the presence of parallel and non-parallel data. We also provide discussions on a variety of important topics regarding the future development of this task.1</abstract>
      <pages>155–205</pages>
      <url hash="ad5beda4">2022.cl-1.6</url>
      <bibkey>jin-etal-2022-deep</bibkey>
      <pwccode url="https://github.com/fuzhenxin/Style-Transfer-in-Text" additional="true">fuzhenxin/Style-Transfer-in-Text</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="7">
      <title>Probing Classifiers: Promises, Shortcomings, and Advances</title>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <doi>10.1162/coli_a_00422</doi>
      <abstract>Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple—a classifier is trained to predict some linguistic property from a model’s representations—and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.</abstract>
      <pages>207–219</pages>
      <url hash="4dd30d51">2022.cl-1.7</url>
      <bibkey>belinkov-2022-probing</bibkey>
    </paper>
    <paper id="8">
      <title>Revisiting the Boundary between <fixed-case>ASR</fixed-case> and <fixed-case>NLU</fixed-case> in the Age of Conversational Dialog Systems</title>
      <author><first>Manaal</first><last>Faruqui</last></author>
      <author><first>Dilek</first><last>Hakkani-Tür</last></author>
      <doi>10.1162/coli_a_00430</doi>
      <abstract>As more users across the world are interacting with dialog agents in their daily life, there is a need for better speech understanding that calls for renewed attention to the dynamics between research in automatic speech recognition (ASR) and natural language understanding (NLU). We briefly review these research areas and lay out the current relationship between them. In light of the observations we make in this article, we argue that (1) NLU should be cognizant of the presence of ASR models being used upstream in a dialog system’s pipeline, (2) ASR should be able to learn from errors found in NLU, (3) there is a need for end-to-end data sets that provide semantic annotations on spoken input, (4) there should be stronger collaboration between ASR and NLU research communities.</abstract>
      <pages>221–232</pages>
      <url hash="d99b7b9c">2022.cl-1.8</url>
      <bibkey>faruqui-hakkani-tur-2022-revisiting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/spoken-squad">Spoken-SQuAD</pwcdataset>
    </paper>
    <paper id="9">
      <title>Natural Language Processing: A Machine Learning Perspective by <fixed-case>Y</fixed-case>ue <fixed-case>Z</fixed-case>hang and Zhiyang Teng</title>
      <author><first>Julia</first><last>Ive</last></author>
      <doi>10.1162/coli_r_00423</doi>
      <pages>233–235</pages>
      <url hash="9ace3399">2022.cl-1.9</url>
      <bibkey>ive-2022-natural</bibkey>
    </paper>
    <paper id="10">
      <title>Erratum for “Formal Basis of a Language Universal”</title>
      <author><first>Miloš</first><last>Stanojević</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <doi>10.1162/coli_x_00432</doi>
      <pages>237–237</pages>
      <url hash="6fd084dd">2022.cl-1.10</url>
      <bibkey>stanojevic-steedman-2022-erratum</bibkey>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Computational Linguistics, Volume 48, Issue 2 - June 2022</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>June</month>
      <year>2022</year>
    </meta>
    <paper id="1">
      <title>Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis</title>
      <author><first>Saif M.</first><last>Mohammad</last></author>
      <doi>10.1162/coli_a_00433</doi>
      <abstract>The importance and pervasiveness of emotions in our lives makes affective computing a tremendously important and vibrant line of work. Systems for automatic emotion recognition (AER) and sentiment analysis can be facilitators of enormous progress (e.g., in improving public health and commerce) but also enablers of great harm (e.g., for suppressing dissidents and manipulating voters). Thus, it is imperative that the affective computing community actively engage with the ethical ramifications of their creations. In this article, I have synthesized and organized information from AI Ethics and Emotion Recognition literature to present fifty ethical considerations relevant to AER. Notably, this ethics sheet fleshes out assumptions hidden in how AER is commonly framed, and in the choices often made regarding the data, method, and evaluation. Special attention is paid to the implications of AER on privacy and social groups. Along the way, key recommendations are made for responsible AER. The objective of the ethics sheet is to facilitate and encourage more thoughtfulness on why to automate, how to automate, and how to judge success well before the building of AER systems. Additionally, the ethics sheet acts as a useful introductory document on emotion recognition (complementing survey articles).</abstract>
      <pages>239–278</pages>
      <url hash="36db3140">2022.cl-2.1</url>
      <bibkey>mohammad-2022-ethics-sheet</bibkey>
    </paper>
    <paper id="2">
      <title>Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarization</title>
      <author><first>Md Tahmid Rahman</first><last>Laskar</last></author>
      <author><first>Enamul</first><last>Hoque</last></author>
      <author><first>Jimmy Xiangji</first><last>Huang</last></author>
      <doi>10.1162/coli_a_00434</doi>
      <abstract>The Query-Focused Text Summarization (QFTS) task aims at building systems that generate the summary of the text document(s) based on the given query. A key challenge in addressing this task is the lack of large labeled data for training the summarization model. In this article, we address this challenge by exploring a series of domain adaptation techniques. Given the recent success of pre-trained transformer models in a wide range of natural language processing tasks, we utilize such models to generate abstractive summaries for the QFTS task for both single-document and multi-document scenarios. For domain adaptation, we apply a variety of techniques using pre-trained transformer-based summarization models including transfer learning, weakly supervised learning, and distant supervision. Extensive experiments on six datasets show that our proposed approach is very effective in generating abstractive summaries for the QFTS task while setting a new state-of-the-art result in several datasets across a set of automatic and human evaluation metrics.</abstract>
      <pages>279–320</pages>
      <url hash="e41774df">2022.cl-2.2</url>
      <bibkey>laskar-etal-2022-domain</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mediqa-ans">MEDIQA-AnS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trecqa">TrecQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="3">
      <title>Challenges of Neural Machine Translation for Short Texts</title>
      <author><first>Yu</first><last>Wan</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Derek Fai</first><last>Wong</last></author>
      <author><first>Lidia Sam</first><last>Chao</last></author>
      <author><first>Liang</first><last>Yao</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <doi>10.1162/coli_a_00435</doi>
      <abstract>Short texts (STs) present in a variety of scenarios, including query, dialog, and entity names. Most of the exciting studies in neural machine translation (NMT) are focused on tackling open problems concerning long sentences rather than short ones. The intuition behind is that, with respect to human learning and processing, short sequences are generally regarded as easy examples. In this article, we first dispel this speculation via conducting preliminary experiments, showing that the conventional state-of-the-art NMT approach, namely, Transformer (Vaswani et al. 2017), still suffers from over-translation and mistranslation errors over STs. After empirically investigating the rationale behind this, we summarize two challenges in NMT for STs associated with translation error types above, respectively: (1) the imbalanced length distribution in training set intensifies model inference calibration over STs, leading to more over-translation cases on STs; and (2) the lack of contextual information forces NMT to have higher data uncertainty on short sentences, and thus NMT model is troubled by considerable mistranslation errors. Some existing approaches, like balancing data distribution for training (e.g., data upsampling) and complementing contextual information (e.g., introducing translation memory) can alleviate the translation issues in NMT for STs. We encourage researchers to investigate other challenges in NMT for STs, thus reducing ST translation errors and enhancing translation quality.</abstract>
      <pages>321–342</pages>
      <url hash="d9abf790">2022.cl-2.3</url>
      <bibkey>wan-etal-2022-challenges</bibkey>
    </paper>
    <paper id="4">
      <title>Annotation Curricula to Implicitly Train Non-Expert Annotators</title>
      <author><first>Ji-Ung</first><last>Lee</last></author>
      <author><first>Jan-Christoph</first><last>Klie</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/coli_a_00436</doi>
      <abstract>Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowdsourcing scenarios where domain expertise is not required. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances to be annotated according to a learning curriculum. To do so, this work formalizes annotation curricula for sentence- and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, we provide a proof of concept for annotation curricula in a carefully designed user study with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results indicate that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can be a promising research direction to improve data collection. To facilitate future research—for instance, to adapt annotation curricula to specific tasks and expert annotation scenarios—all code and data from the user study consisting of 2,400 annotations is made available.1</abstract>
      <pages>343–373</pages>
      <url hash="a1ccdcf0">2022.cl-2.4</url>
      <bibkey>lee-etal-2022-annotation</bibkey>
      <pwccode url="https://github.com/ukplab/annotation-curriculum" additional="false">ukplab/annotation-curriculum</pwccode>
    </paper>
    <paper id="5">
      <title>Assessing Corpus Evidence for Formal and Psycholinguistic Constraints on Nonprojectivity</title>
      <author><first>Himanshu</first><last>Yadav</last></author>
      <author><first>Samar</first><last>Husain</last></author>
      <author><first>Richard</first><last>Futrell</last></author>
      <doi>10.1162/coli_a_00437</doi>
      <abstract>Formal constraints on crossing dependencies have played a large role in research on the formal complexity of natural language grammars and parsing. Here we ask whether the apparent evidence for constraints on crossing dependencies in treebanks might arise because of independent constraints on trees, such as low arity and dependency length minimization. We address this question using two sets of experiments. In Experiment 1, we compare the distribution of formal properties of crossing dependencies, such as gap degree, between real trees and baseline trees matched for rate of crossing dependencies and various other properties. In Experiment 2, we model whether two dependencies cross, given certain psycholinguistic properties of the dependencies. We find surprisingly weak evidence for constraints originating from the mild context-sensitivity literature (gap degree and well-nestedness) beyond what can be explained by constraints on rate of crossing dependencies, topological properties of the trees, and dependency length. However, measures that have emerged from the parsing literature (e.g., edge degree, end-point crossings, and heads’ depth difference) differ strongly between real and random trees. Modeling results show that cognitive metrics relating to information locality and working-memory limitations affect whether two dependencies cross or not, but they do not fully explain the distribution of crossing dependencies in natural languages. Together these results suggest that crossing constraints are better characterized by processing pressures than by mildly context-sensitive constraints.</abstract>
      <pages>375–401</pages>
      <url hash="9d6ae8f1">2022.cl-2.5</url>
      <bibkey>yadav-etal-2022-assessing</bibkey>
    </paper>
    <paper id="6">
      <title>Dual Attention Model for Citation Recommendation with Analyses on Explainability of Attention Mechanisms and Qualitative Experiments</title>
      <author><first>Yang</first><last>Zhang</last></author>
      <author><first>Qiang</first><last>Ma</last></author>
      <doi>10.1162/coli_a_00438</doi>
      <abstract>Based on an exponentially increasing number of academic articles, discovering and citing comprehensive and appropriate resources have become non-trivial tasks. Conventional citation recommendation methods suffer from severe information losses. For example, they do not consider the section header of the paper that the author is writing and for which they need to find a citation, the relatedness between the words in the local context (the text span that describes a citation), or the importance of each word from the local context. These shortcomings make such methods insufficient for recommending adequate citations to academic manuscripts. In this study, we propose a novel embedding-based neural network called dual attention model for citation recommendation (DACR) to recommend citations during manuscript preparation. Our method adapts the embedding of three semantic pieces of information: words in the local context, structural contexts,1 and the section on which the author is working. A neural network model is designed to maximize the similarity between the embedding of the three inputs (local context words, section headers, and structural contexts) and the target citation appearing in the context. The core of the neural network model comprises self-attention and additive attention; the former aims to capture the relatedness between the contextual words and structural context, and the latter aims to learn their importance. Recommendation experiments on real-world datasets demonstrate the effectiveness of the proposed approach. To seek explainability on DACR, particularly the two attention mechanisms, the learned weights from them are investigated to determine how the attention mechanisms interpret “relatedness” and “importance” through the learned weights. In addition, qualitative analyses were conducted to testify that DACR could find necessary citations that were not noticed by the authors in the past due to the limitations of the keyword-based searching.</abstract>
      <pages>403–470</pages>
      <url hash="e40d8ac3">2022.cl-2.6</url>
      <bibkey>zhang-ma-2022-dual</bibkey>
    </paper>
    <paper id="7">
      <title>On Learning Interpreted Languages with Recurrent Models</title>
      <author><first>Denis</first><last>Paperno</last></author>
      <doi>10.1162/coli_a_00431</doi>
      <abstract>Can recurrent neural nets, inspired by human sequential data processing, learn to understand language? We construct simplified data sets reflecting core properties of natural language as modeled in formal syntax and semantics: recursive syntactic structure and compositionality. We find LSTM and GRU networks to generalize to compositional interpretation well, but only in the most favorable learning settings, with a well-paced curriculum, extensive training data, and left-to-right (but not right-to-left) composition.</abstract>
      <pages>471–482</pages>
      <url hash="1b28885c">2022.cl-2.7</url>
      <bibkey>paperno-2022-learning</bibkey>
    </paper>
    <paper id="8">
      <title>Boring Problems Are Sometimes the Most Interesting</title>
      <author><first>Richard</first><last>Sproat</last></author>
      <doi>10.1162/coli_a_00439</doi>
      <abstract>In a recent position paper, Turing Award Winners Yoshua Bengio, Geoffrey Hinton, and Yann LeCun make the case that symbolic methods are not needed in AI and that, while there are still many issues to be resolved, AI will be solved using purely neural methods. In this piece I issue a challenge: Demonstrate that a purely neural approach to the problem of text normalization is possible. Various groups have tried, but so far nobody has eliminated the problem of unrecoverable errors, errors where, due to insufficient training data or faulty generalization, the system substitutes some other reading for the correct one. Solutions have been proposed that involve a marriage of traditional finite-state methods with neural models, but thus far nobody has shown that the problem can be solved using neural methods alone. Though text normalization is hardly an “exciting” problem, I argue that until one can solve “boring” problems like that using purely AI methods, one cannot claim that AI is a success.</abstract>
      <pages>483–490</pages>
      <url hash="c7da70a3">2022.cl-2.8</url>
      <bibkey>sproat-2022-boring</bibkey>
    </paper>
  </volume>
</collection>
