<?xml version='1.0' encoding='UTF-8'?>
<collection id="J18">
  <volume id="1">
    <meta>
      <booktitle>Computational Linguistics, Volume 44, Issue 1 - <fixed-case>A</fixed-case>pril 2018</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>April</month>
      <year>2018</year>
      <venue>cl</venue>
    </meta>
    <frontmatter>
      <bibkey>cl-2018-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Smart Enough to Talk With Us? Foundations and Challenges for Dialogue Capable <fixed-case>AI</fixed-case> Systems</title>
      <author><first>Barbara J.</first><last>Grosz</last></author>
      <pages>1–15</pages>
      <doi>10.1162/COLI_a_00313</doi>
      <url hash="40506484">J18-1001</url>
      <bibkey>grosz-2018-smart</bibkey>
    </paper>
    <paper id="2">
      <title>On the Derivational Entropy of Left-to-Right Probabilistic Finite-State Automata and Hidden <fixed-case>M</fixed-case>arkov Models</title>
      <author><first>Joan Andreu</first><last>Sánchez</last></author>
      <author><first>Martha Alicia</first><last>Rocha</last></author>
      <author><first>Verónica</first><last>Romero</last></author>
      <author><first>Mauricio</first><last>Villegas</last></author>
      <abstract>Probabilistic finite-state automata are a formalism that is widely used in many problems of automatic speech recognition and natural language processing. Probabilistic finite-state automata are closely related to other finite-state models as weighted finite-state automata, word lattices, and hidden Markov models. Therefore, they share many similar properties and problems. Entropy measures of finite-state models have been investigated in the past in order to study the information capacity of these models. The derivational entropy quantifies the uncertainty that the model has about the probability distribution it represents. The derivational entropy in a finite-state automaton is computed from the probability that is accumulated in all of its individual state sequences. The computation of the entropy from a weighted finite-state automaton requires a normalized model. This article studies an efficient computation of the derivational entropy of left-to-right probabilistic finite-state automata, and it introduces an efficient algorithm for normalizing weighted finite-state automata. The efficient computation of the derivational entropy is also extended to continuous hidden Markov models.</abstract>
      <pages>17–37</pages>
      <doi>10.1162/COLI_a_00306</doi>
      <url hash="97ec372b">J18-1002</url>
      <bibkey>sanchez-etal-2018-derivational</bibkey>
    </paper>
    <paper id="3">
      <title>A Notion of Semantic Coherence for Underspecified Semantic Representation</title>
      <author><first>Mehdi</first><last>Manshadi</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <author><first>James F.</first><last>Allen</last></author>
      <abstract>The general problem of finding satisfying solutions to constraint-based underspecified representations of quantifier scope is NP-complete. Existing frameworks, including Dominance Graphs, Minimal Recursion Semantics, and Hole Semantics, have struggled to balance expressivity and tractability in order to cover real natural language sentences with efficient algorithms. We address this trade-off with a general principle of coherence, which requires that every variable introduced in the domain of discourse must contribute to the overall semantics of the sentence. We show that every underspecified representation meeting this criterion can be efficiently processed, and that our set of representations subsumes all previously identified tractable sets.</abstract>
      <pages>39–83</pages>
      <doi>10.1162/COLI_a_00307</doi>
      <url hash="2b8bd05e">J18-1003</url>
      <bibkey>manshadi-etal-2018-notion</bibkey>
    </paper>
    <paper id="4">
      <title>Cache Transition Systems for Graph Parsing</title>
      <author><first>Daniel</first><last>Gildea</last></author>
      <author><first>Giorgio</first><last>Satta</last></author>
      <author><first>Xiaochang</first><last>Peng</last></author>
      <abstract>Motivated by the task of semantic parsing, we describe a transition system that generalizes standard transition-based dependency parsing techniques to generate a graph rather than a tree. Our system includes a cache with fixed size m, and we characterize the relationship between the parameter m and the class of graphs that can be produced through the graph-theoretic concept of tree decomposition. We find empirically that small cache sizes cover a high percentage of sentences in existing semantic corpora.</abstract>
      <pages>85-118</pages>
      <doi>10.1162/COLI_a_00308</doi>
      <url hash="be0917d7">J18-1004</url>
      <bibkey>gildea-etal-2018-cache</bibkey>
    </paper>
    <paper id="5">
      <title>Weighted <fixed-case>DAG</fixed-case> Automata for Semantic Graphs</title>
      <author><first>David</first><last>Chiang</last></author>
      <author><first>Frank</first><last>Drewes</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <author><first>Adam</first><last>Lopez</last></author>
      <author><first>Giorgio</first><last>Satta</last></author>
      <abstract>Graphs have a variety of uses in natural language processing, particularly as representations of linguistic meaning. A deficit in this area of research is a formal framework for creating, combining, and using models involving graphs that parallels the frameworks of finite automata for strings and finite tree automata for trees. A possible starting point for such a framework is the formalism of directed acyclic graph (DAG) automata, defined by Kamimura and Slutzki and extended by Quernheim and Knight. In this article, we study the latter in depth, demonstrating several new results, including a practical recognition algorithm that can be used for inference and learning with models defined on DAG automata. We also propose an extension to graphs with unbounded node degree and show that our results carry over to the extended formalism.</abstract>
      <pages>119-186</pages>
      <doi>10.1162/COLI_a_00309</doi>
      <url hash="25f7186d">J18-1005</url>
      <bibkey>chiang-etal-2018-weighted</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/amr-bank">AMR Bank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="6">
      <title>Book Review: <fixed-case>B</fixed-case>ayesian Analysis in Natural Language Processing by Shay <fixed-case>C</fixed-case>ohen</title>
      <author><first>Kevin</first><last>Duh</last></author>
      <pages>187-189</pages>
      <doi>10.1162/COLI_r_00310</doi>
      <url hash="6d06898c">J18-1006</url>
      <bibkey>duh-2018-book</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>M</fixed-case>etaphor: A Computational Perspective by Tony Veale, Ekaterina <fixed-case>S</fixed-case>hutova and Beata Beigman Klebanov</title>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <pages>191–192</pages>
      <doi>10.1162/COLI_r_00311</doi>
      <url hash="3e580fe5">J18-1007</url>
      <bibkey>strapparava-2018-metaphor</bibkey>
    </paper>
    <paper id="8">
      <title>Neural Network Methods for Natural Language Processing by Yoav <fixed-case>G</fixed-case>oldberg</title>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <author><first>Meng</first><last>Zhang</last></author>
      <pages>193–195</pages>
      <doi>10.1162/</doi>
      <url hash="ac8d6679">J18-1008</url>
      <bibkey>liu-zhang-2018-neural</bibkey>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Computational Linguistics, Volume 44, Issue 2 - June 2018</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>June</month>
      <year>2018</year>
      <venue>cl</venue>
    </meta>
    <frontmatter>
      <bibkey>cl-2018-linguistics-44</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Dependency Perspective on <fixed-case>RST</fixed-case> Discourse Parsing and Evaluation</title>
      <author><first>Mathieu</first><last>Morey</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Nicholas</first><last>Asher</last></author>
      <abstract>Computational text-level discourse analysis mostly happens within Rhetorical Structure Theory (RST), whose structures have classically been presented as constituency trees, and relies on data from the RST Discourse Treebank (RST-DT); as a result, the RST discourse parsing community has largely borrowed from the syntactic constituency parsing community. The standard evaluation procedure for RST discourse parsers is thus a simplified variant of PARSEVAL, and most RST discourse parsers use techniques that originated in syntactic constituency parsing. In this article, we isolate a number of conceptual and computational problems with the constituency hypothesis. We then examine the consequences, for the implementation and evaluation of RST discourse parsers, of adopting a dependency perspective on RST structures, a view advocated so far only by a few approaches to discourse parsing. While doing that, we show the importance of the notion of headedness of RST structures. We analyze RST discourse parsing as dependency parsing by adapting to RST a recent proposal in syntactic parsing that relies on head-ordered dependency trees, a representation isomorphic to headed constituency trees. We show how to convert the original trees from the RST corpus, RST-DT, and their binarized versions used by all existing RST parsers to head-ordered dependency trees. We also propose a way to convert existing simple dependency parser output to constituent trees. This allows us to evaluate and to compare approaches from both constituent-based and dependency-based perspectives in a unified framework, using constituency and dependency metrics. We thus propose an evaluation framework to compare extant approaches easily and uniformly, something the RST parsing community has lacked up to now. We can also compare parsers’ predictions to each other across frameworks. This allows us to characterize families of parsing strategies across the different frameworks, in particular with respect to the notion of headedness. Our experiments provide evidence for the conceptual similarities between dependency parsers and shift-reduce constituency parsers, and confirm that dependency parsing constitutes a viable approach to RST discourse parsing.</abstract>
      <pages>197–235</pages>
      <doi>10.1162/COLI_a_00314</doi>
      <url hash="23e320b3">J18-2001</url>
      <bibkey>morey-etal-2018-dependency</bibkey>
    </paper>
    <paper id="2">
      <title>Unrestricted Bridging Resolution</title>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Katja</first><last>Markert</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <abstract>In contrast to identity anaphors, which indicate coreference between a noun phrase and its antecedent, bridging anaphors link to their antecedent(s) via lexico-semantic, frame, or encyclopedic relations. Bridging resolution involves recognizing bridging anaphors and finding links to antecedents. In contrast to most prior work, we tackle both problems. Our work also follows a more wide-ranging definition of bridging than most previous work and does not impose any restrictions on the type of bridging anaphora or relations between anaphor and antecedent. We create a corpus (ISNotes) annotated for information status (IS), bridging being one of the IS subcategories. The annotations reach high reliability for all categories and marginal reliability for the bridging subcategory. We use a two-stage statistical global inference method for bridging resolution. Given all mentions in a document, the first stage, bridging anaphora recognition, recognizes bridging anaphors as a subtask of learning fine-grained IS. We use a cascading collective classification method where (i) collective classification allows us to investigate relations among several mentions and autocorrelation among IS classes and (ii) cascaded classification allows us to tackle class imbalance, important for minority classes such as bridging. We show that our method outperforms current methods both for IS recognition overall as well as for bridging, specifically. The second stage, bridging antecedent selection, finds the antecedents for all predicted bridging anaphors. We investigate the phenomenon of semantically or syntactically related bridging anaphors that share the same antecedent, a phenomenon we call sibling anaphors. We show that taking sibling anaphors into account in a joint inference model improves antecedent selection performance. In addition, we develop semantic and salience features for antecedent selection and suggest a novel method to build the candidate antecedent list for an anaphor, using the discourse scope of the anaphor. Our model outperforms previous work significantly.</abstract>
      <pages>237–284</pages>
      <doi>10.1162/COLI_a_00315</doi>
      <url hash="7a778b3c">J18-2002</url>
      <bibkey>hou-etal-2018-unrestricted</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="3">
      <title>Spurious Ambiguity and Focalization</title>
      <author><first>Glyn</first><last>Morrill</last></author>
      <author><first>Oriol</first><last>Valentín</last></author>
      <abstract>Spurious ambiguity is the phenomenon whereby distinct derivations in grammar may assign the same structural reading, resulting in redundancy in the parse search space and inefficiency in parsing. Understanding the problem depends on identifying the essential mathematical structure of derivations. This is trivial in the case of context free grammar, where the parse structures are ordered trees; in the case of type logical categorial grammar, the parse structures are proof nets. However, with respect to multiplicatives, intrinsic proof nets have not yet been given for displacement calculus, and proof nets for additives, which have applications to polymorphism, are not easy to characterize. In this context we approach here multiplicative-additive spurious ambiguity by means of the proof-theoretic technique of focalization.</abstract>
      <pages>285–327</pages>
      <doi>10.1162/COLI_a_00316</doi>
      <url hash="02f0cd37">J18-2003</url>
      <bibkey>morrill-valentin-2018-spurious</bibkey>
    </paper>
    <paper id="4">
      <title>The Influence of Context on the Learning of Metrical Stress Systems Using Finite-State Machines</title>
      <author><first>Cesko</first><last>Voeten</last></author>
      <author><first>Menno</first><last>van Zaanen</last></author>
      <abstract>Languages vary in the way stress is assigned to syllables within words. This article investigates the learnability of stress systems in a wide range of languages. The stress systems can be described using finite-state automata with symbols indicating levels of stress (primary, secondary, or no stress). Finite-state automata have been the focus of research in the area of grammatical inference for some time now. It has been shown that finite-state machines are learnable from examples using state-merging. One such approach, which aims to learn k-testable languages, has been applied to stress systems with some success. The family of k-testable languages has been shown to be efficiently learnable (in polynomial time). Here, we extend this approach to k, l-local languages by taking not only left context, but also right context, into account. We consider empirical results testing the performance of our learner using various amounts of context (corresponding to varying definitions of phonological locality). Our results show that our approach of learning stress patterns using state-merging is more reliant on left context than on right context. Additionally, some stress systems fail to be learned by our learner using either the left-context k-testable or the left-and-right-context k, l-local learning system. A more complex merging strategy, and hence grammar representation, is required for these stress systems.</abstract>
      <pages>329–348</pages>
      <doi>10.1162/COLI_a_00317</doi>
      <url hash="630b0499">J18-2004</url>
      <bibkey>voeten-van-zaanen-2018-influence</bibkey>
    </paper>
    <paper id="5">
      <title>Tree Structured <fixed-case>D</fixed-case>irichlet Processes for Hierarchical Morphological Segmentation</title>
      <author><first>Burcu</first><last>Can</last></author>
      <author><first>Suresh</first><last>Manandhar</last></author>
      <abstract>This article presents a probabilistic hierarchical clustering model for morphological segmentation. In contrast to existing approaches to morphology learning, our method allows learning hierarchical organization of word morphology as a collection of tree structured paradigms. The model is fully unsupervised and based on the hierarchical Dirichlet process. Tree hierarchies are learned along with the corresponding morphological paradigms simultaneously. Our model is evaluated on Morpho Challenge and shows competitive performance when compared to state-of-the-art unsupervised morphological segmentation systems. Although we apply this model for morphological segmentation, the model itself can also be used for hierarchical clustering of other types of data.</abstract>
      <pages>349–374</pages>
      <doi>10.1162/COLI_a_00318</doi>
      <url hash="f08ad2dd">J18-2005</url>
      <bibkey>can-manandhar-2018-tree</bibkey>
      <pwccode url="https://github.com/burcu-can/TreeStructuredDP" additional="false">burcu-can/TreeStructuredDP</pwccode>
    </paper>
    <paper id="6">
      <title>Domain-Sensitive Temporal Tagging By Jannik Strötgen, <fixed-case>M</fixed-case>ichael Gertz</title>
      <author><first>Ruihong</first><last>Huang</last></author>
      <pages>375–377</pages>
      <doi>10.1162/COLI_r_00319</doi>
      <url hash="2d883b84">J18-2006</url>
      <bibkey>huang-2018-domain</bibkey>
    </paper>
    <paper id="7">
      <title>Festina Lente: A Farewell from the Editor</title>
      <author><first>Paola</first><last>Merlo</last></author>
      <pages>379-385</pages>
      <doi>10.1162/COLI_e_00320</doi>
      <url hash="e81f0416">J18-2007</url>
      <bibkey>merlo-2018-festina</bibkey>
    </paper>
  </volume>
  <volume id="3">
    <meta>
      <booktitle>Computational Linguistics, Volume 44, Issue 3 - September 2018</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>September</month>
      <year>2018</year>
      <venue>cl</venue>
    </meta>
    <frontmatter>
      <bibkey>cl-2018-linguistics-44-issue</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>O</fixed-case>bituary: Aravind <fixed-case>K</fixed-case>. Joshi</title>
      <author><first>Bonnie</first><last>Webber</last></author>
      <pages>387–392</pages>
      <doi>10.1162/coli_a_00321</doi>
      <url hash="d1f17060">J18-3001</url>
      <bibkey>webber-2018-obituary</bibkey>
    </paper>
    <paper id="2">
      <title>A Structured Review of the Validity of <fixed-case>BLEU</fixed-case></title>
      <author><first>Ehud</first><last>Reiter</last></author>
      <abstract>The BLEU metric has been widely used in NLP for over 15 years to evaluate NLP systems, especially in machine translation and natural language generation. I present a structured review of the evidence on whether BLEU is a valid evaluation technique—in other words, whether BLEU scores correlate with real-world utility and user-satisfaction of NLP systems; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing.</abstract>
      <pages>393–401</pages>
      <doi>10.1162/coli_a_00322</doi>
      <url hash="91d6edf1">J18-3002</url>
      <bibkey>reiter-2018-structured</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="3">
      <title>Native Language Identification With Classifier Stacking and Ensembles</title>
      <author><first>Shervin</first><last>Malmasi</last></author>
      <author><first>Mark</first><last>Dras</last></author>
      <abstract>Ensemble methods using multiple classifiers have proven to be among the most successful approaches for the task of Native Language Identification (NLI), achieving the current state of the art. However, a systematic examination of ensemble methods for NLI has yet to be conducted. Additionally, deeper ensemble architectures such as classifier stacking have not been closely evaluated. We present a set of experiments using three ensemble-based models, testing each with multiple configurations and algorithms. This includes a rigorous application of meta-classification models for NLI, achieving state-of-the-art results on several large data sets, evaluated in both intra-corpus and cross-corpus modes.</abstract>
      <pages>403–446</pages>
      <doi>10.1162/coli_a_00323</doi>
      <url hash="86ee80bd">J18-3003</url>
      <bibkey>malmasi-dras-2018-native</bibkey>
    </paper>
    <paper id="4">
      <title>On the Complexity of <fixed-case>CCG</fixed-case> Parsing</title>
      <author><first>Marco</first><last>Kuhlmann</last></author>
      <author><first>Giorgio</first><last>Satta</last></author>
      <author><first>Peter</first><last>Jonsson</last></author>
      <abstract>We study the parsing complexity of Combinatory Categorial Grammar (CCG) in the formalism of Vijay-Shanker and Weir (1994). As our main result, we prove that any parsing algorithm for this formalism will take in the worst case exponential time when the size of the grammar, and not only the length of the input sentence, is included in the analysis. This sets the formalism of Vijay-Shanker and Weir (1994) apart from weakly equivalent formalisms such as Tree Adjoining Grammar, for which parsing can be performed in time polynomial in the combined size of grammar and input sentence. Our results contribute to a refined understanding of the class of mildly context-sensitive grammars, and inform the search for new, mildly context-sensitive versions of CCG.</abstract>
      <pages>447–482</pages>
      <doi>10.1162/coli_a_00324</doi>
      <url hash="86416279">J18-3004</url>
      <bibkey>kuhlmann-etal-2018-complexity</bibkey>
    </paper>
    <paper id="5">
      <title>Using Semantics for Granularities of Tokenization</title>
      <author><first>Martin</first><last>Riedl</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <abstract>Depending on downstream applications, it is advisable to extend the notion of tokenization from low-level character-based token boundary detection to identification of meaningful and useful language units. This entails both identifying units composed of several single words that form a several single words that form a, as well as splitting single-word compounds into their meaningful parts. In this article, we introduce unsupervised and knowledge-free methods for these two tasks. The main novelty of our research is based on the fact that methods are primarily based on distributional similarity, of which we use two flavors: a sparse count-based and a dense neural-based distributional semantic model. First, we introduce DRUID, which is a method for detecting MWEs. The evaluation on MWE-annotated data sets in two languages and newly extracted evaluation data sets for 32 languages shows that DRUID compares favorably over previous methods not utilizing distributional information. Second, we present SECOS, an algorithm for decompounding close compounds. In an evaluation of four dedicated decompounding data sets across four languages and on data sets extracted from Wiktionary for 14 languages, we demonstrate the superiority of our approach over unsupervised baselines, sometimes even matching the performance of previous language-specific and supervised methods. In a final experiment, we show how both decompounding and MWE information can be used in information retrieval. Here, we obtain the best results when combining word information with MWEs and the compound parts in a bag-of-words retrieval set-up. Overall, our methodology paves the way to automatic detection of lexical units beyond standard tokenization techniques without language-specific preprocessing steps such as POS tagging.</abstract>
      <pages>483–524</pages>
      <doi>10.1162/coli_a_00325</doi>
      <url hash="cde4859a">J18-3005</url>
      <bibkey>riedl-biemann-2018-using</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
    </paper>
    <paper id="6">
      <title>Feature-Based Decipherment for Machine Translation</title>
      <author><first>Iftekhar</first><last>Naim</last></author>
      <author><first>Parker</first><last>Riley</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <abstract>Orthographic similarities across languages provide a strong signal for unsupervised probabilistic transduction (decipherment) for closely related language pairs. The existing decipherment models, however, are not well suited for exploiting these orthographic similarities. We propose a log-linear model with latent variables that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive for the proposed log-linear model. To address this challenge, we perform approximate inference via Markov chain Monte Carlo sampling and contrastive divergence. Our results show that the proposed log-linear model with contrastive divergence outperforms the existing generative decipherment models by exploiting the orthographic features. The model both scales to large vocabularies and preserves accuracy in low- and no-resource contexts.</abstract>
      <pages>525–546</pages>
      <doi>10.1162/coli_a_00326</doi>
      <url hash="4060e909">J18-3006</url>
      <bibkey>naim-etal-2018-feature</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>S</fixed-case>urvey: Anaphora With Non-nominal Antecedents in Computational Linguistics: a <fixed-case>S</fixed-case>urvey</title>
      <author><first>Varada</first><last>Kolhatkar</last></author>
      <author><first>Adam</first><last>Roussel</last></author>
      <author><first>Stefanie</first><last>Dipper</last></author>
      <author><first>Heike</first><last>Zinsmeister</last></author>
      <abstract>This article provides an extensive overview of the literature related to the phenomenon of non-nominal-antecedent anaphora (also known as abstract anaphora or discourse deixis), a type of anaphora in which an anaphor like “that” refers to an antecedent (marked in boldface) that is syntactically non-nominal, such as the first sentence in “It’s way too hot here. That’s why I’m moving to Alaska.” Annotating and automatically resolving these cases of anaphora is interesting in its own right because of the complexities involved in identifying non-nominal antecedents, which typically represent abstract objects such as events, facts, and propositions. There is also practical value in the resolution of non-nominal-antecedent anaphora, as this would help computational systems in machine translation, summarization, and question answering, as well as, conceivably, any other task dependent on some measure of text understanding. Most of the existing approaches to anaphora annotation and resolution focus on nominal-antecedent anaphora, classifying many of the cases where the antecedents are syntactically non-nominal as non-anaphoric. There has been some work done on this topic, but it remains scattered and difficult to collect and assess. With this article, we hope to bring together and synthesize work done in disparate contexts up to now in order to identify fundamental problems and draw conclusions from an overarching perspective. Having a good picture of the current state of the art in this field can help researchers direct their efforts to where they are most necessary. Because of the great variety of theoretical approaches that have been brought to bear on the problem, there is an equally diverse array of terminologies that are used to describe it, so we will provide an overview and discussion of these terminologies. We also describe the linguistic properties of non-nominal-antecedent anaphora, examine previous annotation efforts that have addressed this topic, and present the computational approaches that aim at resolving non-nominal-antecedent anaphora automatically. We close with a review of the remaining open questions in this area and some of our recommendations for future research.</abstract>
      <pages>547–612</pages>
      <doi>10.1162/coli_a_00327</doi>
      <url hash="a9bff1ee">J18-3007</url>
      <bibkey>kolhatkar-etal-2018-survey</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/new-york-times-annotated-corpus">New York Times Annotated Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/parcorfull">ParCorFull</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
  </volume>
  <volume id="4">
    <meta>
      <booktitle>Computational Linguistics, Volume 44, Issue 4 - <fixed-case>D</fixed-case>ecember 2018</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>December</month>
      <year>2018</year>
      <venue>cl</venue>
    </meta>
    <frontmatter>
      <bibkey>cl-2018-linguistics-44-issue-4</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The Lost Combinator</title>
      <author><first>Mark</first><last>Steedman</last></author>
      <abstract/>
      <pages>613-629</pages>
      <doi>10.1162/coli_a_00328</doi>
      <url hash="1530d085">J18-4001</url>
      <bibkey>steedman-2018-lost</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>S</fixed-case>quib: The Language Resource Switchboard</title>
      <author><first>Claus</first><last>Zinn</last></author>
      <abstract>The CLARIN research infrastructure gives users access to an increasingly rich and diverse set of language-related resources and tools. Whereas there is ample support for searching resources using metadata-based search, or full-text search, or for aggregating resources into virtual collections, there is little support for users to help them process resources in one way or another. In spite of the large number of tools that process texts in many different languages, there is no single point of access where users can find tools to fit their needs and the resources they have. In this squib, we present the Language Resource Switchboard (LRS), which helps users to discover tools that can process their resources. For this, the LRS identifies all applicable tools for a given resource, lists the tasks the tools can achieve, and invokes the selected tool in such a way so that processing can start immediately with little or no prior tool parameterization.</abstract>
      <pages>631–639</pages>
      <doi>10.1162/coli_a_00329</doi>
      <url hash="43002422">J18-4002</url>
      <bibkey>zinn-2018-squib</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>S</fixed-case>quib: Reproducibility in Computational Linguistics: Are We Willing to Share?</title>
      <author><first>Martijn</first><last>Wieling</last></author>
      <author><first>Josine</first><last>Rawee</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <abstract>This study focuses on an essential precondition for reproducibility in computational linguistics: the willingness of authors to share relevant source code and data. Ten years after Ted Pedersen’s influential “Last Words” contribution in Computational Linguistics, we investigate to what extent researchers in computational linguistics are willing and able to share their data and code. We surveyed all 395 full papers presented at the 2011 and 2016 ACL Annual Meetings, and identified whether links to data and code were provided. If working links were not provided, authors were requested to provide this information. Although data were often available, code was shared less often. When working links to code or data were not provided in the paper, authors provided the code in about one third of cases. For a selection of ten papers, we attempted to reproduce the results using the provided data and code. We were able to reproduce the results approximately for six papers. For only a single paper did we obtain the exact same results. Our findings show that even though the situation appears to have improved comparing 2016 to 2011, empiricism in computational linguistics still largely remains a matter of faith. Nevertheless, we are somewhat optimistic about the future. Ensuring reproducibility is not only important for the field as a whole, but also seems worthwhile for individual researchers: The median citation count for studies with working links to the source code is higher.</abstract>
      <pages>641–649</pages>
      <doi>10.1162/coli_a_00330</doi>
      <url hash="ccff6a48">J18-4003</url>
      <bibkey>wieling-etal-2018-squib</bibkey>
    </paper>
    <paper id="4">
      <title>Last Words: What Can Be Accomplished with the State of the Art in Information Extraction? A Personal View</title>
      <author><first>Ralph</first><last>Weischedel</last></author>
      <author><first>Elizabeth</first><last>Boschee</last></author>
      <abstract>Though information extraction (IE) research has more than a 25-year history, F1 scores remain low. Thus, one could question continued investment in IE research. In this article, we present three applications where information extraction of entities, relations, and/or events has been used, and note the common features that seem to have led to success. We also identify key research challenges whose solution seems essential for broader successes. Because a few practical deployments already exist and because breakthroughs on particular challenges would greatly broaden the technology’s deployment, further R and D investments are justified.</abstract>
      <pages>651–658</pages>
      <doi>10.1162/coli_a_00331</doi>
      <url hash="037d1f20">J18-4004</url>
      <bibkey>weischedel-boschee-2018-last</bibkey>
    </paper>
    <paper id="5">
      <title>Book Review: Automatic Text Simplification by Horacio Saggion</title>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>659–661</pages>
      <doi>10.1162/coli_r_00332</doi>
      <url hash="5e06a826">J18-4005</url>
      <bibkey>wan-2018-book</bibkey>
    </paper>
    <paper id="6">
      <title>Introduction to the Special Issue on Language in Social Media: Exploiting Discourse and Other Contextual Information</title>
      <author><first>Farah</first><last>Benamara</last></author>
      <author><first>Diana</first><last>Inkpen</last></author>
      <author><first>Maite</first><last>Taboada</last></author>
      <abstract>Social media content is changing the way people interact with each other and share information, personal messages, and opinions about situations, objects, and past experiences. Most social media texts are short online conversational posts or comments that do not contain enough information for natural language processing (NLP) tools, as they are often accompanied by non-linguistic contextual information, including meta-data (e.g., the user’s profile, the social network of the user, and their interactions with other users). Exploiting such different types of context and their interactions makes the automatic processing of social media texts a challenging research task. Indeed, simply applying traditional text mining tools is clearly sub-optimal, as, typically, these tools take into account neither the interactive dimension nor the particular nature of this data, which shares properties with both spoken and written language. This special issue contributes to a deeper understanding of the role of these interactions to process social media data from a new perspective in discourse interpretation. This introduction first provides the necessary background to understand what context is from both the linguistic and computational linguistic perspectives, then presents the most recent context-based approaches to NLP for social media. We conclude with an overview of the papers accepted in this special issue, highlighting what we believe are the future directions in processing social media texts.</abstract>
      <pages>663–681</pages>
      <doi>10.1162/coli_a_00333</doi>
      <url hash="a690ebed">J18-4006</url>
      <bibkey>benamara-etal-2018-introduction</bibkey>
    </paper>
    <paper id="7">
      <title>Interactional Stancetaking in Online Forums</title>
      <author><first>Scott F.</first><last>Kiesling</last></author>
      <author><first>Umashanthi</first><last>Pavalanathan</last></author>
      <author><first>Jim</first><last>Fitzpatrick</last></author>
      <author><first>Xiaochuang</first><last>Han</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <abstract>Language is shaped by the relationships between the speaker/writer and the audience, the object of discussion, and the talk itself. In turn, language is used to reshape these relationships over the course of an interaction. Computational researchers have succeeded in operationalizing sentiment, formality, and politeness, but each of these constructs captures only some aspects of social and relational meaning. Theories of interactional stancetaking have been put forward as holistic accounts, but until now, these theories have been applied only through detailed qualitative analysis of (portions of) a few individual conversations. In this article, we propose a new computational operationalization of interpersonal stancetaking. We begin with annotations of three linked stance dimensions—affect, investment, and alignment—on 68 conversation threads from the online platform Reddit. Using these annotations, we investigate thread structure and linguistic properties of stancetaking in online conversations. We identify lexical features that characterize the extremes along each stancetaking dimension, and show that these stancetaking properties can be predicted with moderate accuracy from bag-of-words features, even with a relatively small labeled training set. These quantitative analyses are supplemented by extensive qualitative analysis, highlighting the compatibility of computational and qualitative methods in synthesizing evidence about the creation of interactional meaning.</abstract>
      <pages>683–718</pages>
      <doi>10.1162/coli_a_00334</doi>
      <url hash="c38d05fd">J18-4007</url>
      <bibkey>kiesling-etal-2018-interactional</bibkey>
    </paper>
    <paper id="8">
      <title>A Joint Model of Conversational Discourse Latent Topics on Microblogs</title>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <abstract>Conventional topic models are ineffective for topic extraction from microblog messages, because the data sparseness exhibited in short messages lacking structure and contexts results in poor message-level word co-occurrence patterns. To address this issue, we organize microblog messages as conversation trees based on their reposting and replying relations, and propose an unsupervised model that jointly learns word distributions to represent: (1) different roles of conversational discourse, and (2) various latent topics in reflecting content information. By explicitly distinguishing the probabilities of messages with varying discourse roles in containing topical words, our model is able to discover clusters of discourse words that are indicative of topical content. In an automatic evaluation on large-scale microblog corpora, our joint model yields topics with better coherence scores than competitive topic models from previous studies. Qualitative analysis on model outputs indicates that our model induces meaningful representations for both discourse and topics. We further present an empirical study on microblog summarization based on the outputs of our joint model. The results show that the jointly modeled discourse and topic representations can effectively indicate summary-worthy content in microblog conversations.</abstract>
      <pages>719–754</pages>
      <doi>10.1162/coli_a_00335</doi>
      <url hash="4afbe32b">J18-4008</url>
      <bibkey>li-etal-2018-joint-model</bibkey>
    </paper>
    <paper id="9">
      <title>Sarcasm Analysis Using Conversation Context</title>
      <author><first>Debanjan</first><last>Ghosh</last></author>
      <author><first>Alexander R.</first><last>Fabbri</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <abstract>Computational models for sarcasm detection have often relied on the content of utterances in isolation. However, the speaker’s sarcastic intent is not always apparent without additional context. Focusing on social media discussions, we investigate three issues: (1) does modeling conversation context help in sarcasm detection? (2) can we identify what part of conversation context triggered the sarcastic reply? and (3) given a sarcastic post that contains multiple sentences, can we identify the specific sentence that is sarcastic? To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the current turn. We show that LSTM networks with sentence-level attention on context and current turn, as well as the conditional LSTM network, outperform the LSTM model that reads only the current turn. As conversation context, we consider the prior turn, the succeeding turn, or both. Our computational models are tested on two types of social media platforms: Twitter and discussion forums. We discuss several differences between these data sets, ranging from their size to the nature of the gold-label annotations. To address the latter two issues, we present a qualitative analysis of the attention weights produced by the LSTM models (with attention) and discuss the results compared with human performance on the two tasks.</abstract>
      <pages>755–792</pages>
      <doi>10.1162/coli_a_00336</doi>
      <url hash="ddccfa9f">J18-4009</url>
      <bibkey>ghosh-etal-2018-sarcasm</bibkey>
    </paper>
    <paper id="10">
      <title>We Usually Don’t Like Going to the Dentist: Using Common Sense to Detect Irony on <fixed-case>T</fixed-case>witter</title>
      <author><first>Cynthia</first><last>Van Hee</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <author><first>Véronique</first><last>Hoste</last></author>
      <abstract>Although common sense and connotative knowledge come naturally to most people, computers still struggle to perform well on tasks for which such extratextual information is required. Automatic approaches to sentiment analysis and irony detection have revealed that the lack of such world knowledge undermines classification performance. In this article, we therefore address the challenge of modeling implicit or prototypical sentiment in the framework of automatic irony detection. Starting from manually annotated connoted situation phrases (e.g., “flight delays,” “sitting the whole day at the doctor’s office”), we defined the implicit sentiment held towards such situations automatically by using both a lexico-semantic knowledge base and a data-driven method. We further investigate how such implicit sentiment information affects irony detection by assessing a state-of-the-art irony classifier before and after it is informed with implicit sentiment information.</abstract>
      <pages>793–832</pages>
      <doi>10.1162/coli_a_00337</doi>
      <url hash="d45e20a3">J18-4010</url>
      <bibkey>van-hee-etal-2018-usually</bibkey>
    </paper>
    <paper id="11">
      <title>Combining Deep Learning and Argumentative Reasoning for the Analysis of Social Media Textual Content Using Small Data Sets</title>
      <author><first>Oana</first><last>Cocarascu</last></author>
      <author><first>Francesca</first><last>Toni</last></author>
      <abstract>The use of social media has become a regular habit for many and has changed the way people interact with each other. In this article, we focus on analyzing whether news headlines support tweets and whether reviews are deceptive by analyzing the interaction or the influence that these texts have on the others, thus exploiting contextual information. Concretely, we define a deep learning method for relation–based argument mining to extract argumentative relations of attack and support. We then use this method for determining whether news articles support tweets, a useful task in fact-checking settings, where determining agreement toward a statement is a useful step toward determining its truthfulness. Furthermore, we use our method for extracting bipolar argumentation frameworks from reviews to help detect whether they are deceptive. We show experimentally that our method performs well in both settings. In particular, in the case of deception detection, our method contributes a novel argumentative feature that, when used in combination with other features in standard supervised classifiers, outperforms the latter even on small data sets.</abstract>
      <pages>833–858</pages>
      <doi>10.1162/coli_a_00338</doi>
      <url hash="bf75fd2d">J18-4011</url>
      <bibkey>cocarascu-toni-2018-combining</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="12">
      <title>Modeling Speech Acts in Asynchronous Conversations: A Neural-<fixed-case>CRF</fixed-case> Approach</title>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Tasnim</first><last>Mohiuddin</last></author>
      <abstract>Participants in an asynchronous conversation (e.g., forum, e-mail) interact with each other at different times, performing certain communicative acts, called speech acts (e.g., question, request). In this article, we propose a hybrid approach to speech act recognition in asynchronous conversations. Our approach works in two main steps: a long short-term memory recurrent neural network (LSTM-RNN) first encodes each sentence separately into a task-specific distributed representation, and this is then used in a conditional random field (CRF) model to capture the conversational dependencies between sentences. The LSTM-RNN model uses pretrained word embeddings learned from a large conversational corpus and is trained to classify sentences into speech act types. The CRF model can consider arbitrary graph structures to model conversational dependencies in an asynchronous conversation. In addition, to mitigate the problem of limited annotated data in the asynchronous domains, we adapt the LSTM-RNN model to learn from synchronous conversations (e.g., meetings), using domain adversarial training of neural networks. Empirical evaluation shows the effectiveness of our approach over existing ones: (i) LSTM-RNNs provide better task-specific representations, (ii) conversational word embeddings benefit the LSTM-RNNs more than the off-the-shelf ones, (iii) adversarial training gives better domain-invariant representations, and (iv) the global CRF model improves over local models.</abstract>
      <pages>859–894</pages>
      <doi>10.1162/coli_a_00339</doi>
      <url hash="c65ca710">J18-4012</url>
      <bibkey>joty-mohiuddin-2018-modeling</bibkey>
    </paper>
    <paper id="13">
      <title>Reviewers for Volume 44</title>
      <pages>895–896</pages>
      <doi>10.1162/coli_x_00340</doi>
      <url hash="dd7fc0d5">J18-4013</url>
      <bibkey>nn-2018-reviewers</bibkey>
    </paper>
  </volume>
</collection>
