<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.wassa">
  <volume id="1" ingest-date="2023-07-21" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, &amp; Social Media Analysis</booktitle>
      <editor><first>Jeremy</first><last>Barnes</last></editor>
      <editor><first>Orphée</first><last>De Clercq</last></editor>
      <editor><first>Roman</first><last>Klinger</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="58136b60">2023.wassa-1</url>
      <venue>wassa</venue>
    </meta>
    <frontmatter>
      <url hash="37d3b7bd">2023.wassa-1.0</url>
      <bibkey>wassa-2023-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>PESTO</fixed-case>: A Post-User Fusion Network for Rumour Detection on Social Media</title>
      <author><first>Erxue</first><last>Min</last></author>
      <author><first>Sophia</first><last>Ananiadou</last><affiliation>University of Manchester</affiliation></author>
      <pages>1-10</pages>
      <abstract>Rumour detection on social media is an important topic due to the challenges of misinformation propagation and slow verification of misleading information. Most previous work focus on the response posts on social media, ignoring the useful characteristics of involved users and their relations. In this paper, we propose a novel framework, Post-User Fusion Network (PESTO), which models the patterns of rumours from both post diffusion and user social networks. Specifically, we propose a novel Chronologically-masked Transformer architecture to model both temporal sequence and diffusion structure of rumours, and apply a Relational Graph Convolutional Network to model the social relations of involved users, with a fusion network based on self-attention mechanism to incorporate the two aspects. Additionally, two data augmentation techniques are leveraged to improve the robustness and accuracy of our models. Empirical results on four datasets of English tweets show the superiority of the proposed method.</abstract>
      <url hash="37a4a156">2023.wassa-1.1</url>
      <bibkey>min-ananiadou-2023-pesto</bibkey>
      <doi>10.18653/v1/2023.wassa-1.1</doi>
      <video href="2023.wassa-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Sentimental Matters - Predicting Literary Quality by Sentiment Analysis and Stylometric Features</title>
      <author><first>Yuri</first><last>Bizzoni</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Pascale</first><last>Moreira</last></author>
      <author><first>Mads Rosendahl</first><last>Thomsen</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Kristoffer</first><last>Nielbo</last><affiliation>Aarhus University</affiliation></author>
      <pages>11-18</pages>
      <abstract>Over the years, the task of predicting reader appreciation or literary quality has been the object of several studies, but it remains a challenging problem in quantitative literary studies and computational linguistics alike, as its definition can vary a lot depending on the genre, the adopted features and the annotation system. This paper attempts to evaluate the impact of sentiment arc modelling versus more classical stylometric features for user-ratings of novels. We run our experiments on a corpus of English language narrative literary fiction from the 19th and 20th century, showing that syntactic and surface-level features can be powerful for the study of literary quality, but can be outperformed by sentiment-characteristics of a text.</abstract>
      <url hash="1d54b56f">2023.wassa-1.2</url>
      <bibkey>bizzoni-etal-2023-sentimental</bibkey>
      <doi>10.18653/v1/2023.wassa-1.2</doi>
      <video href="2023.wassa-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis</title>
      <author><first>Siddharth</first><last>Varia</last></author>
      <author><first>Shuai</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Kishaloy</first><last>Halder</last><affiliation>Amazon</affiliation></author>
      <author><first>Robert</first><last>Vacareanu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Miguel</first><last>Ballesteros</last><affiliation>Amazon</affiliation></author>
      <author><first>Yassine</first><last>Benajiba</last></author>
      <author><first>Neha</first><last>Anna John</last></author>
      <author><first>Rishita</first><last>Anubhai</last></author>
      <author><first>Smaranda</first><last>Muresan</last><affiliation>Amazon and Columbia University</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>Amazon and University of Pennsylvania</affiliation></author>
      <pages>19-27</pages>
      <abstract>Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis task which involves four elements from user-generated texts:aspect term, aspect category, opinion term, and sentiment polarity. Most computational approaches focus on some of the ABSA sub-taskssuch as tuple (aspect term, sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity) extraction using either pipeline or joint modeling approaches. Recently, generative approaches have been proposed to extract all four elements as (one or more) quadrupletsfrom text as a single task. In this work, we take a step further and propose a unified framework for solving ABSA, and the associated sub-tasksto improve the performance in few-shot scenarios. To this end, we fine-tune a T5 model with instructional prompts in a multi-task learning fashion covering all the sub-tasks, as well as the entire quadruple prediction task. In experiments with multiple benchmark datasets, we show that the proposed multi-task prompting approach brings performance boost (by absolute 8.29 F1) in the few-shot learning setting.</abstract>
      <url hash="fd1ec3a6">2023.wassa-1.3</url>
      <bibkey>varia-etal-2023-instruction</bibkey>
      <doi>10.18653/v1/2023.wassa-1.3</doi>
      <video href="2023.wassa-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>You Are What You Read: Inferring Personality From Consumed Textual Content</title>
      <author><first>Adam</first><last>Sutton</last></author>
      <author><first>Almog</first><last>Simchon</last></author>
      <author><first>Matthew</first><last>Edwards</last><affiliation>University of Bristol</affiliation></author>
      <author><first>Stephan</first><last>Lewandowsky</last></author>
      <pages>28-38</pages>
      <abstract>In this work we use consumed text to infer Big-5 personality inventories using data we have collected from the social media platform Reddit. We test our model on two datasets, sampled from participants who consumed either fiction content (<tex-math>N = 913</tex-math>) or news content (<tex-math>N = 213</tex-math>). We show that state-of-the-art models from a similar task using authored text do not translate well to this task, with average correlations of <tex-math>r=.06</tex-math> between the model’s predictions and ground-truth personality inventory dimensions. We propose an alternate method of generating average personality labels for each piece of text consumed, under which our model achieves correlations as high as <tex-math>r=.34</tex-math> when predicting personality from the text being read.</abstract>
      <url hash="71254190">2023.wassa-1.4</url>
      <bibkey>sutton-etal-2023-read</bibkey>
      <doi>10.18653/v1/2023.wassa-1.4</doi>
      <video href="2023.wassa-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title><fixed-case>UNIDECOR</fixed-case>: A Unified Deception Corpus for Cross-Corpus Deception Detection</title>
      <author><first>Aswathy</first><last>Velutharambath</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>39-51</pages>
      <abstract>Verbal deception has been studied in psychology, forensics, and computational linguistics for a variety of reasons, like understanding behaviour patterns, identifying false testimonies, and detecting deception in online communication. Varying motivations across research fields lead to differences in the domain choices to study and in the conceptualization of deception, making it hard to compare models and build robust deception detection systems for a given language. With this paper, we improve this situation by surveying available English deception datasets which include domains like social media reviews, court testimonials, opinion statements on specific topics, and deceptive dialogues from online strategy games. We consolidate these datasets into a single unified corpus. Based on this resource, we conduct a correlation analysis of linguistic cues of deception across datasets to understand the differences and perform cross-corpus modeling experiments which show that a cross-domain generalization is challenging to achieve. The unified deception corpus (UNIDECOR) can be obtained from <url>https://www.ims.uni-stuttgart.de/data/unidecor</url>.</abstract>
      <url hash="45fc2420">2023.wassa-1.5</url>
      <bibkey>velutharambath-klinger-2023-unidecor</bibkey>
      <doi>10.18653/v1/2023.wassa-1.5</doi>
      <video href="2023.wassa-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Discourse Mode Categorization of <fixed-case>B</fixed-case>engali Social Media Health Text</title>
      <author><first>Salim</first><last>Sazzed</last></author>
      <pages>52-57</pages>
      <abstract>The scarcity of annotated data is a major impediment to natural language processing (NLP) research in Bengali, a language that is considered low-resource. In particular, the health and medical domains suffer from a severe paucity of annotated data. Thus, this study aims to introduce BanglaSocialHealth, an annotated social media health corpus that provides sentence-level annotations of four distinct types of expression modes, namely narrative (NAR), informative (INF), suggestive (SUG), and inquiring (INQ) modes in Bengali. We provide details regarding the annotation procedures and report various statistics, such as the median and mean length of words in different sentence modes. Additionally, we apply classical machine learning (CML) classifiers and transformer-based language models to classify sentence modes. We find that most of the statistical properties are similar in different types of sentence modes. To determine the sentence mode, the transformer-based M-BERT model provides slightly better efficacy than the CML classifiers. Our developed corpus and analysis represent a much-needed contribution to Bengali NLP research in medical and health domains and have the potential to facilitate a range of downstream tasks, including question-answering, misinformation detection, and information retrieval.</abstract>
      <url hash="ae3f0112">2023.wassa-1.6</url>
      <bibkey>sazzed-2023-discourse</bibkey>
      <doi>10.18653/v1/2023.wassa-1.6</doi>
      <video href="2023.wassa-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Emotion and Sentiment Guided Paraphrasing</title>
      <author><first>Justin</first><last>Xie</last></author>
      <author><first>Ameeta</first><last>Agrawal</last><affiliation>Portland State University</affiliation></author>
      <pages>58-70</pages>
      <abstract>Paraphrase generation, a.k.a. paraphrasing, is a common and important task in natural language processing. Emotional paraphrasing, which changes the emotion embodied in a piece of text while preserving its meaning, has many potential applications, including moderating online dialogues and preventing cyberbullying. We introduce a new task of fine-grained emotional paraphrasing along emotion gradients, that is, altering the emotional intensities of the paraphrases in fine-grained settings following smooth variations in affective dimensions while preserving the meaning of the original text. We reconstruct several widely used paraphrasing datasets by augmenting the input and target texts with their fine-grained emotion labels. Then, we propose a framework for emotion and sentiment guided paraphrasing by leveraging pre-trained language models for conditioned text generation. Extensive evaluation of the fine-tuned models suggests that including fine-grained emotion labels in the paraphrase task significantly improves the likelihood of obtaining high-quality paraphrases that reflect the desired emotions while achieving consistently better scores in paraphrase metrics such as BLEU, ROUGE, and METEOR.</abstract>
      <url hash="12bcd4ef">2023.wassa-1.7</url>
      <bibkey>xie-agrawal-2023-emotion</bibkey>
      <doi>10.18653/v1/2023.wassa-1.7</doi>
      <video href="2023.wassa-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Emotions in Spoken Language - Do we need acoustics?</title>
      <author><first>Nadine</first><last>Probol</last></author>
      <author><first>Margot</first><last>Mieskes</last><affiliation>University of Applied Sciences Darmstadt</affiliation></author>
      <pages>71-84</pages>
      <abstract>Work on emotion detection is often focused on textual data from i.e. Social Media. If multimodal data (i.e. speech) is analysed, the focus again is often placed on the transcription. This paper takes a closer look at how crucial acoustic information actually is for the recognition of emotions from multimodal data. To this end we use the IEMOCAP data, which is one of the larger data sets that provides transcriptions, audio recordings and manual emotion categorization. We build models for emotion classification using text-only, acoustics-only and combining both modalities in order to examine the influence of the various modalities on the final categorization. Our results indicate that using text-only models outperform acoustics-only models. But combining text-only and acoustic-only models improves the results. Additionally, we perform a qualitative analysis and find that a range of misclassifications are due to factors not related to the model, but to the data such as, recording quality, a challenging classification task and misclassifications that are unsurprising for humans.</abstract>
      <url hash="30e62a8d">2023.wassa-1.8</url>
      <bibkey>probol-mieskes-2023-emotions</bibkey>
      <doi>10.18653/v1/2023.wassa-1.8</doi>
      <video href="2023.wassa-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Understanding Emotion Valence is a Joint Deep Learning Task</title>
      <author><first>Gabriel</first><last>Roccabruna</last><affiliation>University of Trento</affiliation></author>
      <author><first>Seyed Mahed</first><last>Mousavi</last></author>
      <author><first>Giuseppe</first><last>Riccardi</last><affiliation>University of Trento</affiliation></author>
      <pages>85-95</pages>
      <abstract>The valence analysis of speakers’ utterances or written posts helps to understand the activation and variations of the emotional state throughout the conversation. More recently, the concept of Emotion Carriers (EC) has been introduced to explain the emotion felt by the speaker and its manifestations. In this work, we investigate the natural inter-dependency of valence and ECs via a multi-task learning approach. We experiment with Pre-trained Language Models (PLM) for single-task, two-step, and joint settings for the valence and EC prediction tasks. We compare and evaluate the performance of generative (GPT-2) and discriminative (BERT) architectures in each setting. We observed that providing the ground truth label of one task improves the prediction performance of the models in the other task. We further observed that the discriminative model achieves the best trade-off of valence and EC prediction tasks in the joint prediction setting. As a result, we attain a single model that performs both tasks, thus, saving computation resources at training and inference times.</abstract>
      <url hash="01f0eadf">2023.wassa-1.9</url>
      <bibkey>roccabruna-etal-2023-understanding</bibkey>
      <doi>10.18653/v1/2023.wassa-1.9</doi>
      <video href="2023.wassa-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title><fixed-case>C</fixed-case>zech-ing the News: Article Trustworthiness Dataset for <fixed-case>C</fixed-case>zech</title>
      <author><first>Matyas</first><last>Bohacek</last></author>
      <author><first>Michal</first><last>Bravansky</last><affiliation>University College London, University of London and Charles University Prague</affiliation></author>
      <author><first>Filip</first><last>Trhlík</last></author>
      <author><first>Vaclav</first><last>Moravec</last></author>
      <pages>96-109</pages>
      <abstract>We present the Verifee dataset: a multimodal dataset of news articles with fine-grained trustworthiness annotations. We bring a diverse set of researchers from social, media, and computer sciences aboard to study this interdisciplinary problem holistically and develop a detailed methodology that assesses the texts through the lens of editorial transparency, journalist conventions, and objective reporting while penalizing manipulative techniques. We collect over 10,000 annotated articles from 60 Czech online news sources. Each item is categorized into one of the 4 proposed classes on the credibility spectrum – ranging from entirely trustworthy articles to deceptive ones – and annotated of its manipulative attributes. We fine-tune prominent sequence-to-sequence language models for the trustworthiness classification task on our dataset and report the best F-1 score of 0.53. We open-source the dataset, annotation methodology, and annotators’ instructions in full length at <url>https://www.verifee.ai/research/</url> to enable easy build-up work.</abstract>
      <url hash="5c1c0672">2023.wassa-1.10</url>
      <bibkey>bohacek-etal-2023-czech</bibkey>
      <doi>10.18653/v1/2023.wassa-1.10</doi>
      <video href="2023.wassa-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Towards Detecting Harmful Agendas in News Articles</title>
      <author><first>Melanie</first><last>Subbiah</last><affiliation>Columbia University</affiliation></author>
      <author><first>Amrita</first><last>Bhattacharjee</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Yilun</first><last>Hua</last></author>
      <author><first>Tharindu</first><last>Kumarage</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Huan</first><last>Liu</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>110-128</pages>
      <abstract>Manipulated news online is a growing problem which necessitates the use of automated systems to curtail its spread. We argue that while misinformation and disinformation detection have been studied, there has been a lack of investment in the important open challenge of detecting harmful agendas in news articles; identifying harmful agendas is critical to flag news campaigns with the greatest potential for real world harm. Moreover, due to real concerns around censorship, harmful agenda detectors must be interpretable to be effective. In this work, we propose this new task and release a dataset, NewsAgendas, of annotated news articles for agenda identification. We show how interpretable systems can be effective on this task and demonstrate that they can perform comparably to black-box models.</abstract>
      <url hash="1dd4ca98">2023.wassa-1.11</url>
      <bibkey>subbiah-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.wassa-1.11</doi>
      <video href="2023.wassa-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title><fixed-case>GSAC</fixed-case>: A <fixed-case>G</fixed-case>ujarati Sentiment Analysis Corpus from <fixed-case>T</fixed-case>witter</title>
      <author><first>Monil</first><last>Gokani</last><affiliation>International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad and International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Radhika</first><last>Mamidi</last><affiliation>International Institute of Information Technology Hyderabad, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>129-137</pages>
      <abstract>Sentiment Analysis is an important task for analysing online content across languages for tasks such as content moderation and opinion mining. Though a significant amount of resources are available for Sentiment Analysis in several Indian languages, there do not exist any large-scale, open-access corpora for Gujarati. Our paper presents and describes the Gujarati Sentiment Analysis Corpus (GSAC), which has been sourced from Twitter and manually annotated by native speakers of the language. We describe in detail our collection and annotation processes and conduct extensive experiments on our corpus to provide reliable baselines for future work using our dataset.</abstract>
      <url hash="0a2d6c5a">2023.wassa-1.12</url>
      <bibkey>gokani-mamidi-2023-gsac</bibkey>
      <doi>10.18653/v1/2023.wassa-1.12</doi>
      <video href="2023.wassa-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title>A Dataset for Explainable Sentiment Analysis in the <fixed-case>G</fixed-case>erman Automotive Industry</title>
      <author><first>Andrea</first><last>Zielinski</last></author>
      <author><first>Calvin</first><last>Spolwind</last></author>
      <author><first>Henning</first><last>Kroll</last></author>
      <author><first>Anna</first><last>Grimm</last></author>
      <pages>138-148</pages>
      <abstract>While deep learning models have greatly improved the performance of many tasks related to sentiment analysis and classification, they are often criticized for being untrustworthy due to their black-box nature. As a result, numerous explainability techniques have been proposed to better understand the model predictions and to improve the deep learning models. In this work, we introduce InfoBarometer, the first benchmark for examining interpretable methods related to sentiment analysis in the German automotive sector based on online news. Each news article in our dataset is annotated w.r.t. overall sentiment (i.e., positive, negative and neutral), the target of the sentiment (focusing on innovation-related topics such as e.g. electromobility) and the rationales, i.e., textual explanations for the sentiment label that can be leveraged during both training and evaluation. For this research, we compare different state-of-the-art approaches to perform sentiment analysis and observe that even models that perform very well in classification do not score high on explainability metrics like model plausibility and faithfulness. We calculated the polarity scores for the best method BERT and got an F-score of 73.6. Moreover, we evaluated different interpretability algorithms (LIME, SHAP, Integrated Gradients, Saliency) based on explicitly marked rationales by human annotators quantitatively and qualitatively. Our experiments demonstrate that the textual explanations often do not agree with human interpretations, and rarely help to justify the models decision. However, local and global features provide useful insights to help uncover spurious features in the model and biases within the dataset. We intend to make our dataset public for other researchers</abstract>
      <url hash="ce9d715d">2023.wassa-1.13</url>
      <bibkey>zielinski-etal-2023-dataset</bibkey>
      <doi>10.18653/v1/2023.wassa-1.13</doi>
      <video href="2023.wassa-1.13.mp4"/>
    </paper>
    <paper id="14">
      <title>Examining Bias in Opinion Summarisation through the Perspective of Opinion Diversity</title>
      <author><first>Nannan</first><last>Huang</last></author>
      <author><first>Lin</first><last>Tian</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Haytham</first><last>Fayek</last><affiliation>Royal Melbourne Institute of Technology and Facebook</affiliation></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <pages>149-161</pages>
      <abstract>Opinion summarisation is a task that aims to condense the information presented in the source documents while retaining the core message and opinions. A summary that only represents the majority opinions will leave the minority opinions unrepresented in the summary. In this paper, we use the stance towards a certain target as an opinion. We study bias in opinion summarisation from the perspective of opinion diversity, which measures whether the model generated summary can cover a diverse set of opinions. In addition, we examine opinion similarity, a measure of how closely related two opinions are in terms of their stance on a given topic, and its relationship with opinion diversity. Through the lense of stances towards a topic, we examine opinion diversity and similarity using three debatable topics under COVID-19. Experimental results on these topics revealed that a higher degree of similarity of opinions did not indicate good diversity or fairly cover the various opinions originally presented in the source documents. We found that BART and ChatGPT can better capture diverse opinions presented in the source documents.</abstract>
      <url hash="78963078">2023.wassa-1.14</url>
      <bibkey>huang-etal-2023-examining</bibkey>
      <doi>10.18653/v1/2023.wassa-1.14</doi>
      <video href="2023.wassa-1.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Fluency Matters! Controllable Style Transfer with Syntax Guidance</title>
      <author><first>Ji-Eun</first><last>Han</last></author>
      <author><first>Kyung-Ah</first><last>Sohn</last><affiliation>Ajou University</affiliation></author>
      <pages>162-171</pages>
      <abstract>Unsupervised text style transfer is a challenging task that aims to alter the stylistic attributes of a given text without affecting its original content. One of the methods to achieve this is controllable style transfer, which allows for the control of the degree of style transfer. However, an issue encountered with controllable style transfer is the instability of transferred text fluency when the degree of the style transfer changes. To address this problem, we propose a novel approach that incorporates additional syntax parsing information during style transfer. By leveraging the syntactic information, our model is guided to generate natural sentences that effectively reflect the desired style while maintaining fluency. Experimental results show that our method achieves robust performance and improved fluency compared to previous controllable style transfer methods.</abstract>
      <url hash="7fc62910">2023.wassa-1.15</url>
      <bibkey>han-sohn-2023-fluency</bibkey>
      <doi>10.18653/v1/2023.wassa-1.15</doi>
      <video href="2023.wassa-1.15.mp4"/>
    </paper>
    <paper id="16">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations</title>
      <author><first>Hamideh</first><last>Ghanadian</last><affiliation>University of Ottawa</affiliation></author>
      <author><first>Isar</first><last>Nejadgholi</last></author>
      <author><first>Hussein</first><last>Al Osman</last><affiliation>University of Ottawa</affiliation></author>
      <pages>172-183</pages>
      <abstract>This paper presents a novel framework for quantitatively evaluating the interactive ChatGPT model in the context of suicidality assessment from social media posts, utilizing the University of Maryland Reddit suicidality dataset. We conduct a technical evaluation of ChatGPT’s performance on this task using Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned transformer-based models. Additionally, we investigate the impact of different temperature parameters on ChatGPT’s response generation and discuss the optimal temperature based on the inconclusiveness rate of ChatGPT. Our results indicate that while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance. Moreover, our analysis sheds light on how adjusting the ChatGPT’s hyperparameters can improve its ability to assist mental health professionals in this critical task.</abstract>
      <url hash="7bc71af4">2023.wassa-1.16</url>
      <bibkey>ghanadian-etal-2023-chatgpt</bibkey>
      <doi>10.18653/v1/2023.wassa-1.16</doi>
      <video href="2023.wassa-1.16.mp4"/>
    </paper>
    <paper id="17">
      <title>Unsupervised Domain Adaptation using Lexical Transformations and Label Injection for <fixed-case>T</fixed-case>witter Data</title>
      <author><first>Akshat</first><last>Gupta</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Xiaomo</first><last>Liu</last><affiliation>JP Morgan AI Research</affiliation></author>
      <author><first>Sameena</first><last>Shah</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <pages>184-193</pages>
      <abstract>Domain adaptation is an important and widely studied problem in natural language processing. A large body of literature tries to solve this problem by adapting models trained on the source domain to the target domain. In this paper, we instead solve this problem from a dataset perspective. We modify the source domain dataset with simple lexical transformations to reduce the domain shift between the source dataset distribution and the target dataset distribution. We find that models trained on the transformed source domain dataset performs significantly better than zero-shot models. Using our proposed transformations to convert standard English to tweets, we reach an unsupervised part-of-speech (POS) tagging accuracy of 92.14% (from 81.54% zero shot accuracy), which is only slightly below the supervised performance of 94.45%. We also use our proposed transformations to synthetically generate tweets and augment the Twitter dataset to achieve state-of-the-art performance for POS tagging.</abstract>
      <url hash="1f02e808">2023.wassa-1.17</url>
      <bibkey>gupta-etal-2023-unsupervised</bibkey>
      <doi>10.18653/v1/2023.wassa-1.17</doi>
    </paper>
    <paper id="18">
      <title>Transformer-based cynical expression detection in a corpus of <fixed-case>S</fixed-case>panish <fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ube reviews</title>
      <author><first>Samuel</first><last>Gonzalez-Lopez</last><affiliation>Universidad Tecnológica de Nogales</affiliation></author>
      <author><first>Steven</first><last>Bethard</last><affiliation>University of Arizona</affiliation></author>
      <pages>194-201</pages>
      <abstract>Consumers of services and products exhibit a wide range of behaviors on social networks when they are dissatisfied. In this paper, we consider three types of cynical expressions negative feelings, specific reasons, and attitude of being right and annotate a corpus of 3189 comments in Spanish on car analysis channels from YouTube. We evaluate both token classification and text classification settings for this problem, and compare performance of different pre-trained models including BETO, SpanBERTa, Multilingual Bert, and RoBERTuito. The results show that models achieve performance above 0.8 F1 for all types of cynical expressions in the text classification setting, but achieve lower performance (around 0.6-0.7 F1) for the harder token classification setting.</abstract>
      <url hash="7fda7067">2023.wassa-1.18</url>
      <bibkey>gonzalez-lopez-bethard-2023-transformer</bibkey>
      <doi>10.18653/v1/2023.wassa-1.18</doi>
      <video href="2023.wassa-1.18.mp4"/>
    </paper>
    <paper id="19">
      <title>Multilingual Language Models are not Multicultural: A Case Study in Emotion</title>
      <author><first>Shreya</first><last>Havaldar</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Bhumika</first><last>Singhal</last></author>
      <author><first>Sunny</first><last>Rai</last><affiliation>University of Pennsylvania, University of Pennsylvania and Mahindra University</affiliation></author>
      <author><first>Langchen</first><last>Liu</last></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <pages>202-214</pages>
      <abstract>Emotions are experienced and expressed differently across the world. In order to use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, LMs must reflect this cultural variation in emotion. In this study, we investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages. We find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding to prompts in other languages. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.</abstract>
      <url hash="8bd43059">2023.wassa-1.19</url>
      <bibkey>havaldar-etal-2023-multilingual</bibkey>
      <doi>10.18653/v1/2023.wassa-1.19</doi>
      <video href="2023.wassa-1.19.mp4"/>
    </paper>
    <paper id="20">
      <title>Painsight: An Extendable Opinion Mining Framework for Detecting Pain Points Based on Online Customer Reviews</title>
      <author><first>Yukyung</first><last>Lee</last></author>
      <author><first>Jaehee</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Doyoon</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Yookyung</first><last>Kho</last></author>
      <author><first>Younsun</first><last>Kim</last></author>
      <author><first>Pilsung</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>215-227</pages>
      <abstract>As the e-commerce market continues to expand and online transactions proliferate, customer reviews have emerged as a critical element in shaping the purchasing decisions of prospective buyers. Previous studies have endeavored to identify key aspects of customer reviews through the development of sentiment analysis models and topic models. However, extracting specific dissatisfaction factors remains a challenging task. In this study, we delineate the pain point detection problem and propose Painsight, an unsupervised framework for automatically extracting distinct dissatisfaction factors from customer reviews without relying on ground truth labels. Painsight employs pre-trained language models to construct sentiment analysis and topic models, leveraging attribution scores derived from model gradients to extract dissatisfaction factors. Upon application of the proposed methodology to customer review data spanning five product categories, we successfully identified and categorized dissatisfaction factors within each group, as well as isolated factors for each type. Notably, Painsight outperformed benchmark methods, achieving substantial performance enhancements and exceptional results in human evaluations.</abstract>
      <url hash="d648b7c5">2023.wassa-1.20</url>
      <bibkey>lee-etal-2023-painsight</bibkey>
      <doi>10.18653/v1/2023.wassa-1.20</doi>
      <video href="2023.wassa-1.20.mp4"/>
    </paper>
    <paper id="21">
      <title>Context-Dependent Embedding Utterance Representations for Emotion Recognition in Conversations</title>
      <author><first>Patrícia</first><last>Pereira</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Helena</first><last>Moniz</last><affiliation>Universidade de Lisboa</affiliation></author>
      <author><first>Isabel</first><last>Dias</last></author>
      <author><first>Joao Paulo</first><last>Carvalho</last><affiliation>Instituto Superior Técnico and INESC-ID</affiliation></author>
      <pages>228-236</pages>
      <abstract>Emotion Recognition in Conversations (ERC) has been gaining increasing importance as conversational agents become more and more common. Recognizing emotions is key for effective communication, being a crucial component in the development of effective and empathetic conversational agents. Knowledge and understanding of the conversational context are extremely valuable for identifying the emotions of the interlocutor. We thus approach Emotion Recognition in Conversations leveraging the conversational context, i.e., taking into attention previous conversational turns. The usual approach to model the conversational context has been to produce context-independent representations of each utterance and subsequently perform contextual modeling of these. Here we propose context-dependent embedding representations of each utterance by leveraging the contextual representational power of pre-trained transformer language models. In our approach, we feed the conversational context appended to the utterance to be classified as input to the RoBERTa encoder, to which we append a simple classification module, thus discarding the need to deal with context after obtaining the embeddings since these constitute already an efficient representation of such context. We also investigate how the number of introduced conversational turns influences our model performance. The effectiveness of our approach is validated on the open-domain DailyDialog dataset and on the task-oriented EmoWOZ dataset.</abstract>
      <url hash="c5aa379d">2023.wassa-1.21</url>
      <bibkey>pereira-etal-2023-context</bibkey>
      <doi>10.18653/v1/2023.wassa-1.21</doi>
      <video href="2023.wassa-1.21.mp4"/>
    </paper>
    <paper id="22">
      <title>Combining Active Learning and Task Adaptation with <fixed-case>BERT</fixed-case> for Cost-Effective Annotation of Social Media Datasets</title>
      <author><first>Jens</first><last>Lemmens</last></author>
      <author><first>Walter</first><last>Daelemans</last><affiliation>University of Antwerp</affiliation></author>
      <pages>237-250</pages>
      <abstract>Social media provide a rich source of data that can be mined and used for a wide variety of research purposes. However, annotating this data can be expensive, yet necessary for state-of-the-art pre-trained language models to achieve high prediction performance. Therefore, we combine pool-based active learning based on prediction uncertainty (an established method for reducing annotation costs) with unsupervised task adaptation through Masked Language Modeling (MLM). The results on three different datasets (two social media corpora, one benchmark dataset) show that task adaptation significantly improves results and that with only a fraction of the available training data, this approach reaches similar F1-scores as those achieved by an upper-bound baseline model fine-tuned on all training data. We hereby contribute to the scarce corpus of research on active learning with pre-trained language models and propose a cost-efficient annotation sampling and fine-tuning approach that can be applied to a wide variety of tasks and datasets.</abstract>
      <url hash="afea650a">2023.wassa-1.22</url>
      <bibkey>lemmens-daelemans-2023-combining</bibkey>
      <doi>10.18653/v1/2023.wassa-1.22</doi>
      <video href="2023.wassa-1.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Improving <fixed-case>D</fixed-case>utch Vaccine Hesitancy Monitoring via Multi-Label Data Augmentation with <fixed-case>GPT</fixed-case>-3.5</title>
      <author><first>Jens</first><last>Van Nooten</last></author>
      <author><first>Walter</first><last>Daelemans</last><affiliation>University of Antwerp</affiliation></author>
      <pages>251-270</pages>
      <abstract>In this paper, we leverage the GPT-3.5 language model both using the Chat-GPT API interface and the GPT-3.5 API interface to generate realistic examples of anti-vaccination tweets in Dutch with the aim of augmenting an imbalanced multi-label vaccine hesitancy argumentation classification dataset. In line with previous research, we devise a prompt that, on the one hand, instructs the model to generate realistic examples based on the gold standard dataset and, on the other hand, to assign multiple pseudo-labels (or a single pseudo-label) to the generated instances. We then augment our gold standard data with the generated examples and evaluate the impact thereof in a cross-validation setting with several state-of-the-art Dutch large language models. This augmentation technique predominantly shows improvements in F1 for classifying underrepresented classes while increasing the overall recall, paired with a slight decrease in precision for more common classes. Furthermore, we examine how well the synthetic data generalises to human data in the classification task. To our knowledge, we are the first to utilise Chat-GPT and GPT-3.5 for augmenting a Dutch multi-label dataset classification task.</abstract>
      <url hash="8580c08b">2023.wassa-1.23</url>
      <bibkey>van-nooten-daelemans-2023-improving</bibkey>
      <doi>10.18653/v1/2023.wassa-1.23</doi>
      <video href="2023.wassa-1.23.mp4"/>
    </paper>
    <paper id="24">
      <title>Emotion Analysis of Tweets Banning Education in <fixed-case>A</fixed-case>fghanistan</title>
      <author><first>Mohammad Ali</first><last>Hussiny</last></author>
      <author><first>Lilja</first><last>Øvrelid</last><affiliation>Dept. of Informatics, University of Oslo</affiliation></author>
      <pages>271-277</pages>
      <abstract>This paper introduces the first emotion-annotated dataset for the Dari variant of Persian spoken in Afghanistan. The LetHerLearn dataset contains 7,600 tweets posted in reaction to the Taliban’s ban of women’s rights to education in 2022 and has been manually annotated according to Ekman’s emotion categories. We here detail the data collection and annotation process, present relevant dataset statistics as well as initial experiments on the resulting dataset, benchmarking a number of different neural architectures for the task of Dari emotion classification.</abstract>
      <url hash="c39359cc">2023.wassa-1.24</url>
      <bibkey>hussiny-ovrelid-2023-emotion</bibkey>
      <doi>10.18653/v1/2023.wassa-1.24</doi>
      <video href="2023.wassa-1.24.mp4"/>
    </paper>
    <paper id="25">
      <title>Identifying Slurs and Lexical Hate Speech via Light-Weight Dimension Projection in Embedding Space</title>
      <author><first>Sanne</first><last>Hoeken</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Sina</first><last>Zarrieß</last><affiliation>Bielefeld University</affiliation></author>
      <author><first>Ozge</first><last>Alacam</last><affiliation>Bielefeld University</affiliation></author>
      <pages>278-289</pages>
      <abstract>The prevalence of hate speech on online platforms has become a pressing concern for society, leading to increased attention towards detecting hate speech. Prior work in this area has primarily focused on identifying hate speech at the utterance level that reflects the complex nature of hate speech. In this paper, we propose a targeted and efficient approach to identifying hate speech by detecting slurs at the lexical level using contextualized word embeddings. We hypothesize that slurs have a systematically different representation than their neutral counterparts, making them identifiable through existing methods for discovering semantic dimensions in word embeddings. The results demonstrate the effectiveness of our approach in predicting slurs, confirming linguistic theory that the meaning of slurs is stable across contexts. Our robust hate dimension approach for slur identification offers a promising solution to tackle a smaller yet crucial piece of the complex puzzle of hate speech detection.</abstract>
      <url hash="b54b0c47">2023.wassa-1.25</url>
      <bibkey>hoeken-etal-2023-identifying</bibkey>
      <doi>10.18653/v1/2023.wassa-1.25</doi>
      <video href="2023.wassa-1.25.mp4"/>
    </paper>
    <paper id="26">
      <title>Sentiment and Emotion Classification in Low-resource Settings</title>
      <author><first>Jeremy</first><last>Barnes</last><affiliation>University of the Basque Country</affiliation></author>
      <pages>290-304</pages>
      <abstract>The popularity of sentiment and emotion analysis has lead to an explosion of datasets, approaches, and papers. However, these are often tested in optimal settings, where plentiful training and development data are available, and compared mainly with recent state-of-the-art models that have been similarly evaluated. In this paper, we instead present a systematic comparison of sentiment and emotion classification methods, ranging from rule- and dictionary-based methods to recently proposed few-shot and prompting methods with large language models. We test these methods in-domain, out-of-domain, and in cross-lingual settings and find that in low-resource settings, rule- and dictionary-based methods perform as well or better than few-shot and prompting methods, especially for emotion classification. Zero-shot cross-lingual approaches, however, still outperform in-language dictionary induction.</abstract>
      <url hash="9e931fd1">2023.wassa-1.26</url>
      <bibkey>barnes-2023-sentiment</bibkey>
      <doi>10.18653/v1/2023.wassa-1.26</doi>
    </paper>
    <paper id="27">
      <title>Analyzing Subjectivity Using a Transformer-Based Regressor Trained on Naïve Speakers’ Judgements</title>
      <author><first>Elena</first><last>Savinova</last><affiliation>Radboud University</affiliation></author>
      <author><first>Fermin</first><last>Moscoso Del Prado</last><affiliation>University of Cambridge</affiliation></author>
      <pages>305-314</pages>
      <abstract>The problem of subjectivity detection is often approached as a preparatory binary task for sentiment analysis, despite the fact that theoretically subjectivity is often defined as a matter of degree. In this work, we approach subjectivity analysis as a regression task and test the efficiency of a transformer RoBERTa model in annotating subjectivity of online news, including news from social media, based on a small subset of human-labeled training data. The results of experiments comparing our model to an existing rule-based subjectivity regressor and a state-of-the-art binary classifier reveal that: 1) our model highly correlates with the human subjectivity ratings and outperforms the widely used rule-based “pattern” subjectivity regressor (De Smedt and Daelemans, 2012); 2) our model performs well as a binary classifier and generalizes to the benchmark subjectivity dataset (Pang and Lee, 2004); 3) in contrast, state-of-the-art classifiers trained on the benchmark dataset show catastrophic performance on our human-labeled data. The results bring to light the issues of the gold standard subjectivity dataset, and the models trained on it, which seem to distinguish between the origin/style of the texts rather than subjectivity as perceived by human English speakers.</abstract>
      <url hash="e2858326">2023.wassa-1.27</url>
      <bibkey>savinova-moscoso-del-prado-2023-analyzing</bibkey>
      <doi>10.18653/v1/2023.wassa-1.27</doi>
      <video href="2023.wassa-1.27.mp4"/>
    </paper>
    <paper id="28">
      <title>A Fine Line Between Irony and Sincerity: Identifying Bias in Transformer Models for Irony Detection</title>
      <author><first>Aaron</first><last>Maladry</last></author>
      <author><first>Els</first><last>Lefever</last><affiliation>Ghent University</affiliation></author>
      <author><first>Cynthia</first><last>Van Hee</last><affiliation>Universiteit Gent and Universiteit Gent</affiliation></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>315-324</pages>
      <abstract>In this paper we investigate potential bias in fine-tuned transformer models for irony detection. Bias is defined in this research as spurious associations between word n-grams and class labels, that can cause the system to rely too much on superficial cues and miss the essence of the irony. For this purpose, we looked for correlations between class labels and words that are prone to trigger irony, such as positive adjectives, intensifiers and topical nouns. Additionally, we investigate our irony model’s predictions before and after manipulating the data set through irony trigger replacements. We further support these insights with state-of-the-art explainability techniques (Layer Integrated Gradients, Discretized Integrated Gradients and Layer-wise Relevance Propagation). Both approaches confirm the hypothesis that transformer models generally encode correlations between positive sentiments and ironic texts, with even higher correlations between vividly expressed sentiment and irony. Based on these insights, we implemented a number of modification strategies to enhance the robustness of our irony classifier.</abstract>
      <url hash="b550201a">2023.wassa-1.28</url>
      <bibkey>maladry-etal-2023-fine</bibkey>
      <doi>10.18653/v1/2023.wassa-1.28</doi>
      <video href="2023.wassa-1.28.mp4"/>
    </paper>
    <paper id="29">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> is fun, but it is not funny! Humor is still challenging Large Language Models</title>
      <author><first>Sophie</first><last>Jentzsch</last><affiliation>German Aerospace Center (DLR)</affiliation></author>
      <author><first>Kristian</first><last>Kersting</last><affiliation>TU Darmstadt</affiliation></author>
      <pages>325-340</pages>
      <abstract>Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI’s ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny?We put ChatGPT’s sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT’s capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional explanations for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the classification of jokes. ChatGPT has not solved computational humor yet but it can be a big leap toward “funny” machines.</abstract>
      <url hash="3e5099b7">2023.wassa-1.29</url>
      <bibkey>jentzsch-kersting-2023-chatgpt</bibkey>
      <doi>10.18653/v1/2023.wassa-1.29</doi>
      <video href="2023.wassa-1.29.mp4"/>
    </paper>
    <paper id="30">
      <title>How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques</title>
      <author><first>Michela</first><last>Lorandi</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University and University of Aberdeen</affiliation></author>
      <pages>341-353</pages>
      <abstract>Recent advances in the development of large Pretrained Language Models, such as GPT, BERT and Bloom, have achieved remarkable performance on a wide range of different NLP tasks. However, when used for text generation tasks, these models still have limitations when it comes to controlling the content and style of the generated text, often producing content that is incorrect, irrelevant, or inappropriate in the context of a given task. In this survey paper, we explore methods for controllable text generation with a focus on sentiment control. We systematically collect papers from the ACL Anthology, create a categorisation scheme based on different control techniques and controlled attributes, and use the scheme to categorise and compare methods. The result is a detailed and comprehensive overview of state-of-the-art techniques for sentiment-controlled text generation categorised on the basis of how the control is implemented and what attributes are controlled and providing a clear idea of their relative strengths and weaknesses.</abstract>
      <url hash="18223038">2023.wassa-1.30</url>
      <bibkey>lorandi-belz-2023-control</bibkey>
      <doi>10.18653/v1/2023.wassa-1.30</doi>
      <video href="2023.wassa-1.30.mp4"/>
    </paper>
    <paper id="31">
      <title>Transformer-based Prediction of Emotional Reactions to Online Social Network Posts</title>
      <author><first>Irene</first><last>Benedetto</last><affiliation>Polytechnic Institute of Turin</affiliation></author>
      <author><first>Moreno</first><last>La Quatra</last></author>
      <author><first>Luca</first><last>Cagliero</last><affiliation>Polytechnic Institute of Turin</affiliation></author>
      <author><first>Luca</first><last>Vassio</last><affiliation>Polytechnic Institute of Turin</affiliation></author>
      <author><first>Martino</first><last>Trevisan</last><affiliation>University of Trieste</affiliation></author>
      <pages>354-364</pages>
      <abstract>Emotional reactions to Online Social Network posts have recently gained importance in the study of the online ecosystem. Prior to post publication, the number of received reactions can be predicted based on either the textual content of the post or the related metadata. However, existing approaches suffer from both the lack of semantic-aware language understanding models and the limited explainability of the prediction models. To overcome these issues, we present a new transformer-based method to predict the number of emotional reactions of different types to social posts. It leverages the attention mechanism to capture arbitrary semantic textual relations neglected by prior works. Furthermore, it also provides end-users with textual explanations of the predictions. The results achieved on a large collection of Facebook posts confirm the applicability of the presented methodology.</abstract>
      <url hash="a42ae185">2023.wassa-1.31</url>
      <bibkey>benedetto-etal-2023-transformer</bibkey>
      <doi>10.18653/v1/2023.wassa-1.31</doi>
      <video href="2023.wassa-1.31.mp4"/>
    </paper>
    <paper id="32">
      <title>Transfer Learning for Code-Mixed Data: Do Pretraining Languages Matter?</title>
      <author><first>Kushal</first><last>Tatariya</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Heather</first><last>Lent</last><affiliation>Aalborg University</affiliation></author>
      <author><first>Miryam</first><last>de Lhoneux</last><affiliation>KU Leuven</affiliation></author>
      <pages>365-378</pages>
      <abstract>Monolinguals make up a minority of the world’s speakers, and yet most language technologies lag behind in handling linguistic behaviours produced by bilingual and multilingual speakers. A commonly observed phenomenon in such communities is code-mixing, which is prevalent on social media, and thus requires attention in NLP research. In this work, we look into the ability of pretrained language models to handle code-mixed data, with a focus on the impact of languages present in pretraining on the downstream performance of the model as measured on the task of sentiment analysis. Ultimately, we find that the pretraining language has little effect on performance when the model sees code-mixed data during downstream finetuning. We also evaluate the models on code-mixed data in a zero-shot setting, after task-specific finetuning on a monolingual dataset. We find that this brings out differences in model performance that can be attributed to the pretraining languages. We present a thorough analysis of these findings that also looks at model performance based on the composition of participating languages in the code-mixed datasets.</abstract>
      <url hash="19626bd8">2023.wassa-1.32</url>
      <bibkey>tatariya-etal-2023-transfer</bibkey>
      <doi>10.18653/v1/2023.wassa-1.32</doi>
      <video href="2023.wassa-1.32.mp4"/>
    </paper>
    <paper id="33">
      <title>Can <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> Understand Causal Language in Science Claims?</title>
      <author><first>Yuheun</first><last>Kim</last></author>
      <author><first>Lu</first><last>Guo</last><affiliation>Syracuse University</affiliation></author>
      <author><first>Bei</first><last>Yu</last><affiliation>School of Information Studies, Syracuse University</affiliation></author>
      <author><first>Yingya</first><last>Li</last><affiliation>Harvard Medical School and Boston Children’s Hospital</affiliation></author>
      <pages>379-389</pages>
      <abstract>This study evaluated ChatGPT’s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.</abstract>
      <url hash="1b6a6f26">2023.wassa-1.33</url>
      <bibkey>kim-etal-2023-chatgpt</bibkey>
      <doi>10.18653/v1/2023.wassa-1.33</doi>
      <video href="2023.wassa-1.33.mp4"/>
    </paper>
    <paper id="34">
      <title>Systematic Evaluation of <fixed-case>GPT</fixed-case>-3 for Zero-Shot Personality Estimation</title>
      <author><first>Adithya</first><last>V Ganesan</last><affiliation>, State University of New York, Stony Brook</affiliation></author>
      <author><first>Yash Kumar</first><last>Lal</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>August</first><last>Nilsson</last></author>
      <author><first>H.</first><last>Schwartz</last><affiliation>Stony Brook University (SUNY)</affiliation></author>
      <pages>390-400</pages>
      <abstract>Very large language models (LLMs) perform extremely well on a spectrum of NLP tasks in a zero-shot setting. However, little is known about their performance on human-level NLP problems which rely on understanding psychological concepts, such as assessing personality traits. In this work, we investigate the zero-shot ability of GPT-3 to estimate the Big 5 personality traits from users’ social media posts. Through a set of systematic experiments, we find that zero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA for broad classification upon injecting knowledge about the trait in the prompts. However, when prompted to provide fine-grained classification, its performance drops to close to a simple most frequent class (MFC) baseline. We further analyze where GPT-3 performs better, as well as worse, than a pretrained lexical model, illustrating systematic errors that suggest ways to improve LLMs on human-level NLP tasks. The code for this project is available on Github.</abstract>
      <url hash="85dbc3d7">2023.wassa-1.34</url>
      <bibkey>v-ganesan-etal-2023-systematic</bibkey>
      <doi>10.18653/v1/2023.wassa-1.34</doi>
      <video href="2023.wassa-1.34.mp4"/>
    </paper>
    <paper id="35">
      <title>Utterance Emotion Dynamics in Children’s Poems: Emotional Changes Across Age</title>
      <author><first>Daniela</first><last>Teodorescu</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Alona</first><last>Fyshe</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Saif</first><last>Mohammad</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>401-415</pages>
      <abstract>Emerging psychopathology studies are showing that patterns of changes in emotional state — emotion dynamics — are associated with overall well-being and mental health. More recently, there has been some work in tracking emotion dynamics through one’s utterances, allowing for data to be collected on a larger scale across time and people. However, several questions about how emotion dynamics change with age, especially in children, and when determined through children’s writing, remain unanswered. In this work, we use both a lexicon and a machine learning based approach to quantify characteristics of emotion dynamics determined from poems written by children of various ages. We show that both approaches point to similar trends: consistent increasing intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and dominance) with age and a consistent decreasing valence with age. We also find increasing emotional variability, rise rates (i.e., emotional reactivity), and recovery rates (i.e., emotional regulation) with age. These results act as a useful baselines for further research in how patterns of emotions expressed by children change with age, and their association with mental health.</abstract>
      <url hash="3989b0e3">2023.wassa-1.35</url>
      <bibkey>teodorescu-etal-2023-utterance</bibkey>
      <doi>10.18653/v1/2023.wassa-1.35</doi>
      <video href="2023.wassa-1.35.mp4"/>
    </paper>
    <paper id="36">
      <title>Annotating and Training for Population Subjective Views</title>
      <author><first>Maria</first><last>Alexeeva</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Caroline</first><last>Hyland</last></author>
      <author><first>Keith</first><last>Alcock</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Allegra A. Beal</first><last>Cohen</last><affiliation>University of Florida</affiliation></author>
      <author><first>Hubert</first><last>Kanyamahanga</last></author>
      <author><first>Isaac Kobby</first><last>Anni</last></author>
      <author><first>Mihai</first><last>Surdeanu</last><affiliation>University of Arizona</affiliation></author>
      <pages>416-430</pages>
      <abstract>In this paper, we present a dataset of subjective views (beliefs and attitudes) held by individuals or groups. We analyze the usefulness of the dataset by training a neural classifier that identifies belief-containing sentences that are relevant for our broader project of interest—scientific modeling of complex systems. We also explore and discuss difficulties related to annotation of subjective views and propose ways of addressing them.</abstract>
      <url hash="0debf68e">2023.wassa-1.36</url>
      <bibkey>alexeeva-etal-2023-annotating</bibkey>
      <doi>10.18653/v1/2023.wassa-1.36</doi>
      <video href="2023.wassa-1.36.mp4"/>
    </paper>
    <paper id="37">
      <title>Exploration of Contrastive Learning Strategies toward more Robust Stance Detection</title>
      <author><first>Udhaya Kumar</first><last>Rajendran</last></author>
      <author><first>Amine</first><last>Trabelsi</last><affiliation>Université de Sherbrooke</affiliation></author>
      <pages>431-440</pages>
      <abstract>Stance Detection is the task of identifying the position of an author of a text towards an issue or a target. Previous studies on Stance Detection indicate that the existing systems are non-robust to the variations and errors in input sentences. Our proposed methodology uses Contrastive Learning to learn sentence representations by bringing semantically similar sentences and sentences implying the same stance closer to each other in the embedding space. We compare our approach to a pretrained transformer model directly finetuned with the stance datasets. We use char-level and word-level adversarial perturbation attacks to measure the resilience of the models and we show that our approach achieves better performances and is more robust to the different adversarial perturbations introduced to the test data. The results indicate that our approach performs better on small-sized and class-imbalanced stance datasets.</abstract>
      <url hash="7bcb65cd">2023.wassa-1.37</url>
      <bibkey>rajendran-trabelsi-2023-exploration</bibkey>
      <doi>10.18653/v1/2023.wassa-1.37</doi>
      <video href="2023.wassa-1.37.mp4"/>
    </paper>
    <paper id="38">
      <title>Adapting Emotion Detection to Analyze Influence Campaigns on Social Media</title>
      <author><first>Ankita</first><last>Bhaumik</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Andy</first><last>Bernhardt</last></author>
      <author><first>Gregorios</first><last>Katsios</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Ning</first><last>Sa</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Tomek</first><last>Strzalkowski</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <pages>441-451</pages>
      <abstract>Social media is an extremely potent tool for influencing public opinion, particularly during important events such as elections, pandemics, and national conflicts. Emotions are a crucial aspect of this influence, but detecting them accurately in the political domain is a significant challenge due to the lack of suitable emotion labels and training datasets. In this paper, we present a generalized approach to emotion detection that can be adapted to the political domain with minimal performance sacrifice. Our approach is designed to be easily integrated into existing models without the need for additional training or fine-tuning. We demonstrate the zero-shot and few-shot performance of our model on the 2017 French presidential elections and propose efficient emotion groupings that would aid in effectively analyzing influence campaigns and agendas on social media.</abstract>
      <url hash="c4b01d21">2023.wassa-1.38</url>
      <bibkey>bhaumik-etal-2023-adapting</bibkey>
      <doi>10.18653/v1/2023.wassa-1.38</doi>
      <video href="2023.wassa-1.38.mp4"/>
    </paper>
    <paper id="39">
      <title>Not Just Iconic: Emoji Interpretation is Shaped by Use</title>
      <author><first>Brianna</first><last>O’Boyle</last></author>
      <author><first>Gabriel</first><last>Doyle</last><affiliation>San Diego State University</affiliation></author>
      <pages>452-457</pages>
      <abstract>Where do the meaning of emoji come from? Though it is generally assumed that emoji are fully iconic, with meanings derived from their visual forms, we argue that this is only one component of their meaning. We surveyed users and non-users of the Chinese social media platform WeChat for their interpretations of emoji specific to WeChat. We find that some emoji show significant differences in their interpretations between users and non-users, and based on how familiar a person is with the specific emoji. We argue that this reflects a more complex process for building the meaning of emoji on a platform than pure iconicity.</abstract>
      <url hash="19a1e8f9">2023.wassa-1.39</url>
      <bibkey>oboyle-doyle-2023-just</bibkey>
      <doi>10.18653/v1/2023.wassa-1.39</doi>
      <video href="2023.wassa-1.39.mp4"/>
    </paper>
    <paper id="40">
      <title>The Paradox of Multilingual Emotion Detection</title>
      <author><first>Luna</first><last>De Bruyne</last><affiliation>Universiteit Gent</affiliation></author>
      <pages>458-466</pages>
      <abstract>The dominance of English is a well-known issue in NLP research. In this position paper, I turn to state-of-the-art psychological insights to explain why this problem is especially persistent in research on automatic emotion detection, and why the seemingly promising approach of using multilingual models to include lower-resourced languages might not be the desired solution. Instead, I campaign for the use of models that acknowledge linguistic and cultural differences in emotion conceptualization and verbalization. Moreover, I see much potential in NLP to better understand emotions and emotional language use across different languages.</abstract>
      <url hash="1c2c5b21">2023.wassa-1.40</url>
      <bibkey>de-bruyne-2023-paradox</bibkey>
      <doi>10.18653/v1/2023.wassa-1.40</doi>
      <video href="2023.wassa-1.40.mp4"/>
    </paper>
    <paper id="41">
      <title>Sadness and Anxiety Language in <fixed-case>R</fixed-case>eddit Messages Before and After Quitting a Job</title>
      <author><first>Molly</first><last>Ireland</last><affiliation>Receptiviti</affiliation></author>
      <author><first>Micah</first><last>Iserman</last></author>
      <author><first>Kiki</first><last>Adams</last></author>
      <pages>467-478</pages>
      <abstract>People globally quit their jobs at high rates during the COVID-19 pandemic, yet there is scant research about emotional trajectories surrounding voluntary resignations before or during that era. To explore long-term emotional language patterns before and after quitting a job, we amassed a Reddit sample of people who indicated resigning on a specific day (n = 7,436), each of whom was paired with a comparison user matched on posting history. After excluding people on the basis of low posting frequency and word count, we analyzed 150.3 million words (53.1% from 5,134 target users who indicated quitting) using SALLEE, a dictionary-based syntax-aware tool, and Linguistic Inquiry and Word Count (LIWC) dictionaries. Based on posts in the year before and after quitting, people who had quit their jobs used more sadness and anxiety language than matched comparison users. Lower rates of “I” pronouns and cognitive processing language were associated with less sadness and anxiety surrounding quitting. Emotional trajectories during and before the pandemic were parallel, though pandemic messages were more negative. The results have relevance for strategic self-distancing as a means of regulating negative emotions around major life changes.</abstract>
      <url hash="7d4fa411">2023.wassa-1.41</url>
      <bibkey>ireland-etal-2023-sadness</bibkey>
      <doi>10.18653/v1/2023.wassa-1.41</doi>
      <video href="2023.wassa-1.41.mp4"/>
    </paper>
    <paper id="42">
      <title>Communicating Climate Change: A Comparison Between Tweets and Speeches by <fixed-case>G</fixed-case>erman Members of Parliament</title>
      <author><first>Robin</first><last>Schaefer</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>Christoph</first><last>Abels</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>Stephan</first><last>Lewandowsky</last></author>
      <author><first>Manfred</first><last>Stede</last><affiliation>Universität Potsdam</affiliation></author>
      <pages>479-496</pages>
      <abstract>Twitter and parliamentary speeches are very different communication channels, but many members of parliament (MPs) make use of both. Focusing on the topic of climate change, we undertake a comparative analysis of speeches and tweets uttered by MPs in Germany in a recent six-year period. By keyword/hashtag analyses and topic modeling, we find substantial differences along party lines, with left-leaning parties discussing climate change through a crisis frame, while liberal and conservative parties try to address climate change through the lens of climate-friendly technology and practices. Only the AfD denies the need to adopt climate change mitigating measures, demeaning those concerned about a deteriorating climate as climate cult or fanatics. Our analysis reveals that climate change communication does not differ substantially between Twitter and parliamentary speeches, but across the political spectrum.</abstract>
      <url hash="4777c7a8">2023.wassa-1.42</url>
      <bibkey>schaefer-etal-2023-communicating</bibkey>
      <doi>10.18653/v1/2023.wassa-1.42</doi>
      <video href="2023.wassa-1.42.mp4"/>
    </paper>
    <paper id="43">
      <title>Modelling Political Aggression on Social Media Platforms</title>
      <author><first>Akash</first><last>Rawat</last></author>
      <author><first>Nazia</first><last>Nafis</last></author>
      <author><first>Dnyaneshwar</first><last>Bhadane</last></author>
      <author><first>Diptesh</first><last>Kanojia</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Rudra</first><last>Murthy</last><affiliation>IBM India Ltd</affiliation></author>
      <pages>497-510</pages>
      <abstract>Recent years have seen a proliferation of aggressive social media posts, often wreaking even real-world consequences for victims. Aggressive behaviour on social media is especially evident during important sociopolitical events such as elections, communal incidents, and public protests. In this paper, we introduce a dataset in English to model political aggression. The dataset comprises public tweets collated across the time-frames of two of the most recent Indian general elections. We manually annotate this data for the task of aggression detection and analyze this data for aggressive behaviour. To benchmark the efficacy of our dataset, we perform experiments by fine-tuning pre-trained language models and comparing the results with models trained on an existing but general domain dataset. Our models consistently outperform the models trained on existing data. Our best model achieves a macro F1-score of 66.66 on our dataset. We also train models on a combined version of both datasets, achieving best macro F1-score of 92.77, on our dataset. Additionally, we create subsets of code-mixed and non-code-mixed data from the combined dataset to observe variations in results due to the Hindi-English code-mixing phenomenon. We publicly release the anonymized data, code, and models for further research.</abstract>
      <url hash="64344030">2023.wassa-1.43</url>
      <bibkey>rawat-etal-2023-modelling</bibkey>
      <doi>10.18653/v1/2023.wassa-1.43</doi>
      <video href="2023.wassa-1.43.mp4"/>
    </paper>
    <paper id="44">
      <title>Findings of <fixed-case>WASSA</fixed-case> 2023 Shared Task on Empathy, Emotion and Personality Detection in Conversation and Reactions to News Articles</title>
      <author><first>Valentin</first><last>Barriere</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Shabnam</first><last>Tafreshi</last></author>
      <author><first>Salvatore</first><last>Giorgi</last></author>
      <pages>511-525</pages>
      <abstract>This paper presents the results of the WASSA 2023 shared task on predicting empathy, emotion, and personality in conversations and reactions to news articles. Participating teams were given access to a new dataset from Omitaomu et al. (2022) comprising empathic and emotional reactions to news articles. The dataset included formal and informal text, self-report data, and third-party annotations. Specifically, the dataset contained news articles (where harm is done to a person, group, or other) and crowd-sourced essays written in reaction to the article. After reacting via essays, crowd workers engaged in conversations about the news articles. Finally, the crowd workers self-reported their empathic concern and distress, personality (using the Big Five), and multi-dimensional empathy (via the Interpersonal Reactivity Index). A third-party annotated both the conversational turns (for empathy, emotion polarity, and emotion intensity) and essays (for multi-label emotions). Thus, the dataset contained outcomes (self-reported or third-party annotated) at the turn level (within conversations) and the essay level. Participation was encouraged in five tracks: (i) predicting turn-level empathy, emotion polarity, and emotion intensity in conversations, (ii) predicting state empathy and distress scores, (iii) predicting emotion categories, (iv) predicting personality, and (v) predicting multi-dimensional trait empathy. In total, 21 teams participated in the shared task. We summarize the methods and resources used by the participating teams.</abstract>
      <url hash="58474bbf">2023.wassa-1.44</url>
      <bibkey>barriere-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.wassa-1.44</doi>
    </paper>
    <paper id="45">
      <title><fixed-case>YNU</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>WASSA</fixed-case>-2023 Shared Task 1: Large-scale Language Model with <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Fine-Tuning for Empathy Detection and Emotion Classification</title>
      <author><first>Yukun</first><last>Wang</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>526-530</pages>
      <abstract>This paper describes the system for the YNU-HPCC team in WASSA-2023 Shared Task 1: Empathy Detection and Emotion Classification. This task needs to predict the empathy, emotion, and personality of the empathic reactions. This system is mainly based on the Decoding-enhanced BERT with disentangled attention (DeBERTa) model with parameter-efficient fine-tuning (PEFT) and the Robustly Optimized BERT Pretraining Approach (RoBERTa). Low-Rank Adaptation (LoRA) fine-tuning in PEFT is used to reduce the training parameters of large language models. Moreover, back translation is introduced to augment the training dataset. This system achieved relatively good results on the competition’s official leaderboard. The code of this system is available here.</abstract>
      <url hash="4cf32268">2023.wassa-1.45</url>
      <bibkey>wang-etal-2023-ynu</bibkey>
      <doi>10.18653/v1/2023.wassa-1.45</doi>
      <video href="2023.wassa-1.45.mp4"/>
    </paper>
    <paper id="46">
      <title><fixed-case>A</fixed-case>ditya<fixed-case>P</fixed-case>atkar at <fixed-case>WASSA</fixed-case> 2023 Empathy, Emotion, and Personality Shared Task: <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a-Based Emotion Classification of Essays, Improving Performance on Imbalanced Data</title>
      <author><first>Aditya</first><last>Patkar</last></author>
      <author><first>Suraj</first><last>Chandrashekhar</last></author>
      <author><first>Ram Mohan Rao</first><last>Kadiyala</last></author>
      <pages>531-535</pages>
      <abstract>This paper presents a study on using the RoBERTa language model for emotion classification of essays as part of the ‘Shared Task on Empathy Detection, Emotion Classification and Personality Detection in Interactions’ organized as part of ‘WASSA 2023’ at ‘ACL 2023’. Emotion classification is a challenging task in natural language processing, and imbalanced datasets further exacerbate this challenge. In this study, we explore the use of various data balancing techniques in combination with RoBERTa to improve the classification performance. We evaluate the performance of our approach (denoted by adityapatkar on Codalab) on a benchmark multi-label dataset of essays annotated with eight emotion categories, provided by the Shared Task organizers. Our results show that the proposed approach achieves the best macro F1 score in the competition’s training and evaluation phase. Our study provides insights into the potential of RoBERTa for handling imbalanced data in emotion classification. The results can have implications for the natural language processing tasks related to emotion classification.</abstract>
      <url hash="436051f0">2023.wassa-1.46</url>
      <bibkey>patkar-etal-2023-adityapatkar</bibkey>
      <doi>10.18653/v1/2023.wassa-1.46</doi>
      <video href="2023.wassa-1.46.mp4"/>
    </paper>
    <paper id="47">
      <title>Curtin <fixed-case>OCAI</fixed-case> at <fixed-case>WASSA</fixed-case> 2023 Empathy, Emotion and Personality Shared Task: Demographic-Aware Prediction Using Multiple Transformers</title>
      <author><first>Md Rakibul</first><last>Hasan</last></author>
      <author><first>Md Zakir</first><last>Hossain</last></author>
      <author><first>Tom</first><last>Gedeon</last></author>
      <author><first>Susannah</first><last>Soon</last></author>
      <author><first>Shafin</first><last>Rahman</last></author>
      <pages>536-541</pages>
      <abstract>The WASSA 2023 shared task on predicting empathy, emotion and other personality traits consists of essays, conversations and articles in textual form and participants’ demographic information in numerical form. To address the tasks, our contributions include (1) converting numerical information into meaningful text information using appropriate templates, (2) summarising lengthy articles, and (3) augmenting training data by paraphrasing. To achieve these contributions, we leveraged two separate T5-based pre-trained transformers. We then fine-tuned pre-trained BERT, DistilBERT and ALBERT for predicting empathy and personality traits. We used the Optuna hyperparameter optimisation framework to fine-tune learning rates, batch sizes and weight initialisation. Our proposed system achieved its highest performance – a Pearson correlation coefficient of 0.750 – on the onversation-level empathy prediction task1 . The system implementation is publicly available at https: //github.com/hasan-rakibul/WASSA23-empathy-emotion.</abstract>
      <url hash="57da3c11">2023.wassa-1.47</url>
      <bibkey>hasan-etal-2023-curtin</bibkey>
      <doi>10.18653/v1/2023.wassa-1.47</doi>
      <video href="2023.wassa-1.47.mp4"/>
    </paper>
    <paper id="48">
      <title><fixed-case>T</fixed-case>eam_<fixed-case>H</fixed-case>awk at <fixed-case>WASSA</fixed-case> 2023 Empathy, Emotion, and Personality Shared Task: Multi-tasking Multi-encoder based transformers for Empathy and Emotion Prediction in Conversations</title>
      <author><first>Addepalli Sai</first><last>Srinivas</last></author>
      <author><first>Nabarun</first><last>Barua</last></author>
      <author><first>Santanu</first><last>Pal</last></author>
      <pages>542-547</pages>
      <abstract>In this paper, we present Team Hawk’s participation in Track 1 of the WASSA 2023 shared task. The objective of the task is to understand the empathy that emerges between individuals during their conversations. In our study, we developed a multi-tasking framework that is capable of automatically assessing empathy, intensity of emotion, and polarity of emotion within participants’ conversations. Our proposed core model extends the transformer architecture, utilizing two separate RoBERTa-based encoders to encode both the articles and conversations. Subsequently, a sequence of self-attention, position-wise feed-forward, and dense layers are employed to predict the regression scores for the three sub-tasks: empathy, intensity of emotion, and polarity of emotion. Our best model achieved average Pearson’s correlation of 0.7710 (Empathy: 0.7843, Emotion Polarity: 0.7917, Emotion Intensity: 0.7381) on the released development set and 0.7250 (Empathy: 0.8090, Emotion Polarity: 0.7010, Emotion Intensity: 0.6650) on the released test set. These results earned us the 3rd position in the test set evaluation phase of Track 1.</abstract>
      <url hash="65775c0a">2023.wassa-1.48</url>
      <bibkey>srinivas-etal-2023-team</bibkey>
      <doi>10.18653/v1/2023.wassa-1.48</doi>
      <video href="2023.wassa-1.48.mp4"/>
    </paper>
    <paper id="49">
      <title><fixed-case>NCUEE</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>WASSA</fixed-case> 2023 Shared Task 1: Empathy and Emotion Prediction Using Sentiment-Enhanced <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a Transformers</title>
      <author><first>Tzu-Mi</first><last>Lin</last></author>
      <author><first>Jung-Ying</first><last>Chang</last></author>
      <author><first>Lung-Hao</first><last>Lee</last></author>
      <pages>548-552</pages>
      <abstract>This paper describes our proposed system design for the WASSA 2023 shared task 1. We propose a unified architecture of ensemble neural networks to integrate the original RoBERTa transformer with two sentiment-enhanced RoBERTa-Twitter and EmoBERTa models. For Track 1 at the speech-turn level, our best submission achieved an average Pearson correlation score of 0.7236, ranking fourth for empathy, emotion polarity and emotion intensity prediction. For Track 2 at the essay-level, our best submission obtained an average Pearson correlation score of 0.4178 for predicting empathy and distress scores, ranked first among all nine submissions.</abstract>
      <url hash="e7752bfc">2023.wassa-1.49</url>
      <bibkey>lin-etal-2023-ncuee</bibkey>
      <doi>10.18653/v1/2023.wassa-1.49</doi>
      <video href="2023.wassa-1.49.mp4"/>
    </paper>
    <paper id="50">
      <title>Domain Transfer for Empathy, Distress, and Personality Prediction</title>
      <author><first>Fabio</first><last>Gruschka</last></author>
      <author><first>Allison</first><last>Lahnala</last></author>
      <author><first>Charles</first><last>Welch</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>553-557</pages>
      <abstract>This research contributes to the task of predicting empathy and personality traits within dialogue, an important aspect of natural language processing, as part of our experimental work for the WASSA 2023 Empathy and Emotion Shared Task. For predicting empathy, emotion polarity, and emotion intensity on turns within a dialogue, we employ adapters trained on social media interactions labeled with empathy ratings in a stacked composition with the target task adapters. Furthermore, we embed demographic information to predict Interpersonal Reactivity Index (IRI) subscales and Big Five Personality Traits utilizing BERT-based models. The results from our study provide valuable insights, contributing to advancements in understanding human behavior and interaction through text. Our team ranked 2nd on the personality and empathy prediction tasks, 4th on the interpersonal reactivity index, and 6th on the conversational task.</abstract>
      <url hash="ebd21fcc">2023.wassa-1.50</url>
      <bibkey>gruschka-etal-2023-domain</bibkey>
      <doi>10.18653/v1/2023.wassa-1.50</doi>
    </paper>
    <paper id="51">
      <title>Converge at <fixed-case>WASSA</fixed-case> 2023 Empathy, Emotion and Personality Shared Task: A Transformer-based Approach for Multi-Label Emotion Classification</title>
      <author><first>Aditya</first><last>Paranjape</last></author>
      <author><first>Gaurav</first><last>Kolhatkar</last></author>
      <author><first>Yash</first><last>Patwardhan</last></author>
      <author><first>Omkar</first><last>Gokhale</last></author>
      <author><first>Shweta</first><last>Dharmadhikari</last></author>
      <pages>558-563</pages>
      <abstract>In this paper, we highlight our approach for the “WASSA 2023 Shared-Task 1: Empathy Detection and Emotion Classification”. By accurately identifying emotions from textual sources of data, deep learning models can be trained to understand and interpret human emotions more effectively. The classification of emotions facilitates the creation of more emotionally intelligent systems that can better understand and respond to human emotions. We compared multiple transformer-based models for multi-label classification. Ensembling and oversampling were used to improve the performance of the system. A threshold-based voting mechanism performed on three models (Longformer, BERT, BigBird) yields the highest overall macro F1-score of 0.6605.</abstract>
      <url hash="2b36f744">2023.wassa-1.51</url>
      <bibkey>paranjape-etal-2023-converge</bibkey>
      <doi>10.18653/v1/2023.wassa-1.51</doi>
      <video href="2023.wassa-1.51.mp4"/>
    </paper>
    <paper id="52">
      <title><fixed-case>PICT</fixed-case>-<fixed-case>CLRL</fixed-case> at <fixed-case>WASSA</fixed-case> 2023 Empathy, Emotion and Personality Shared Task: Empathy and Distress Detection using Ensembles of Transformer Models</title>
      <author><first>Tanmay</first><last>Chavan</last></author>
      <author><first>Kshitij</first><last>Deshpande</last></author>
      <author><first>Sheetal</first><last>Sonawane</last></author>
      <pages>564-568</pages>
      <abstract>This paper presents our approach for the WASSA 2023 Empathy, Emotion and Personality Shared Task. Empathy and distress are human feelings that are implicitly expressed in natural discourses. Empathy and distress detection are crucial challenges in Natural Language Processing that can aid our understanding of conversations. The provided dataset consists of several long-text examples in the English language, with each example associated with a numeric score for empathy and distress. We experiment with several BERT-based models as a part of our approach. We also try various ensemble methods. Our final submission has a Pearson’s r score of 0.346, placing us third in the empathy and distress detection subtask.</abstract>
      <url hash="8d971764">2023.wassa-1.52</url>
      <bibkey>chavan-etal-2023-pict</bibkey>
      <doi>10.18653/v1/2023.wassa-1.52</doi>
      <video href="2023.wassa-1.52.mp4"/>
    </paper>
    <paper id="53">
      <title>Team Bias Busters at <fixed-case>WASSA</fixed-case> 2023 Empathy, Emotion and Personality Shared Task: Emotion Detection with Generative Pretrained Transformers</title>
      <author><first>Andrew</first><last>Nedilko</last></author>
      <author><first>Yi</first><last>Chu</last></author>
      <pages>569-573</pages>
      <abstract>This paper describes the approach that we used to take part in the multi-label multi-class emotion classification as Track 3 of the WASSA 2023 Empathy, Emotion and Personality Shared Task at ACL 2023. The overall goal of this track is to build models that can predict 8 classes (7 emotions + neutral) based on short English essays written in response to news article that talked about events perceived as harmful to people. We used OpenAI generative pretrained transformers with full-scale APIs for the emotion prediction task by fine-tuning a GPT-3 model and doing prompt engineering for zero-shot / few-shot learning with ChatGPT and GPT-4 models based on multiple experiments on the dev set. The most efficient method was fine-tuning a GPT-3 model which allowed us to beat our baseline character-based XGBoost Classifier and rank 2nd among all other participants by achieving a macro F1 score of 0.65 and a micro F1 score of 0.7 on the final blind test set.</abstract>
      <url hash="8dd59c0e">2023.wassa-1.53</url>
      <bibkey>nedilko-chu-2023-team</bibkey>
      <doi>10.18653/v1/2023.wassa-1.53</doi>
      <video href="2023.wassa-1.53.mp4"/>
    </paper>
    <paper id="54">
      <title><fixed-case>HIT</fixed-case>-<fixed-case>SCIR</fixed-case> at <fixed-case>WASSA</fixed-case> 2023: Empathy and Emotion Analysis at the Utterance-Level and the Essay-Level</title>
      <author><first>Xin</first><last>Lu</last></author>
      <author><first>Zhuojun</first><last>Li</last></author>
      <author><first>Yanpeng</first><last>Tong</last></author>
      <author><first>Yanyan</first><last>Zhao</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>574-580</pages>
      <abstract>This paper introduces the participation of team HIT-SCIR to the WASSA 2023 Shared Task on Empathy Detection and Emotion Classification and Personality Detection in Interactions. We focus on three tracks: Track 1 (Empathy and Emotion Prediction in Conversations, CONV), Track 2 (Empathy Prediction, EMP) and Track 3 (Emotion Classification, EMO), and designed three different models to address them separately. For Track 1, we designed a direct fine-tuning DeBERTa model for three regression tasks at the utterance-level. For Track 2, we designed a multi-task learning RoBERTa model for two regression tasks at the essay-level. For Track 3, we designed a RoBERTa model with data augmentation for the classification task at the essay-level. Finally, our team ranked 1st in the Track 1 (CONV), 5th in the Track 2 (EMP) and 3rd in the Track 3 (EMO) in the evaluation phase.</abstract>
      <url hash="3117a482">2023.wassa-1.54</url>
      <bibkey>lu-etal-2023-hit</bibkey>
      <doi>10.18653/v1/2023.wassa-1.54</doi>
      <video href="2023.wassa-1.54.mp4"/>
    </paper>
    <paper id="55">
      <title><fixed-case>VISU</fixed-case> at <fixed-case>WASSA</fixed-case> 2023 Shared Task: Detecting Emotions in Reaction to News Stories Using Transformers and Stacked Embeddings</title>
      <author><first>Vivek</first><last>Kumar</last></author>
      <author><first>Prayag</first><last>Tiwari</last></author>
      <author><first>Sushmita</first><last>Singh</last></author>
      <pages>581-586</pages>
      <abstract>Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion Classification from essays written in reaction to news articles. Emotion detection from complex dialogues is challenging and often requires context/domain understanding. Therefore in this research, we have focused on developing deep learning (DL) models using the combination of word embedding representations with tailored prepossessing strategies to capture the nuances of emotions expressed. Our experiments used static and contextual embeddings (individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and Transformer based models. We occupied rank tenth in the emotion detection task by scoring a Macro F1-Score of 0.2717, validating the efficacy of our implemented approaches for small and imbalanced datasets with mixed categories of target emotions.</abstract>
      <url hash="5e4926a9">2023.wassa-1.55</url>
      <bibkey>kumar-etal-2023-visu</bibkey>
      <doi>10.18653/v1/2023.wassa-1.55</doi>
      <video href="2023.wassa-1.55.mp4"/>
    </paper>
    <paper id="56">
      <title>Findings of <fixed-case>WASSA</fixed-case> 2023 Shared Task: Multi-Label and Multi-Class Emotion Classification on Code-Mixed Text Messages</title>
      <author><first>Iqra</first><last>Ameer</last></author>
      <author><first>Necva</first><last>Bölücü</last></author>
      <author><first>Hua</first><last>Xu</last></author>
      <author><first>Ali</first><last>Al Bataineh</last></author>
      <pages>587-595</pages>
      <abstract>We present the results of the WASSA 2023 Shared-Task 2: Emotion Classification on code-mixed text messages (Roman Urdu + English), which included two tracks for emotion classification: multi-label and multi-class. The participants were provided with a dataset of code-mixed SMS messages in English and Roman Urdu labeled with 12 emotions for both tracks. A total of 5 teams (19 team members) participated in the shared task. We summarized the methods, resources, and tools used by the participating teams. We also made the data freely available for further improvements to the task.</abstract>
      <url hash="603a214c">2023.wassa-1.56</url>
      <bibkey>ameer-etal-2023-findings</bibkey>
      <revision id="1" href="2023.wassa-1.56v1" hash="55c51558"/>
      <revision id="2" href="2023.wassa-1.56v2" hash="603a214c" date="2023-07-31">Author order update.</revision>
      <doi>10.18653/v1/2023.wassa-1.56</doi>
    </paper>
    <paper id="57">
      <title>Emotion classification on code-mixed text messages via soft prompt tuning</title>
      <author><first>Jinghui</first><last>Zhang</last></author>
      <author><first>Dongming</first><last>Yang</last></author>
      <author><first>Siyu</first><last>Bao</last></author>
      <author><first>Lina</first><last>Cao</last></author>
      <author><first>Shunguo</first><last>Fan</last></author>
      <pages>596-600</pages>
      <abstract>Emotion classification on code-mixed text messages is challenging due to the multilingual languages and non-literal cues (i.e., emoticons). To solve these problems, we propose an innovative soft prompt tuning method, which is lightweight and effective to release potential abilities of the pre-trained language models and improve the classification results. Firstly, we transform emoticons into textual information to utilize their rich emotional information. Then, variety of innovative templates and verbalizers are applied to promote emotion classification. Extensive experiments show that transforming emoticons and employing prompt tuning both benefit the performance. Finally, as a part of WASSA 2023, we obtain the accuracy of 0.972 in track MLEC and 0.892 in track MCEC, yielding the second place in both two tracks.</abstract>
      <url hash="1ed39c42">2023.wassa-1.57</url>
      <bibkey>zhang-etal-2023-emotion</bibkey>
      <doi>10.18653/v1/2023.wassa-1.57</doi>
    </paper>
    <paper id="58">
      <title><fixed-case>P</fixed-case>recog<fixed-case>IIITH</fixed-case>@<fixed-case>WASSA</fixed-case>2023: Emotion Detection for <fixed-case>U</fixed-case>rdu-<fixed-case>E</fixed-case>nglish Code-mixed Text</title>
      <author><first>Bhaskara Hanuma</first><last>Vedula</last></author>
      <author><first>Prashant</first><last>Kodali</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <pages>601-605</pages>
      <abstract>Code-mixing refers to the phenomenon of using two or more languages interchangeably within a speech or discourse context. This practice is particularly prevalent on social media platforms, and determining the embedded affects in a code-mixed sentence remains as a challenging problem. In this submission we describe our system for WASSA 2023 Shared Task on Emotion Detection in English-Urdu code-mixed text. In our system we implement a multiclass emotion detection model with label space of 11 emotions. Samples are code-mixed English-Urdu text, where Urdu is written in romanised form. Our submission is limited to one of the subtasks - Multi Class classification and we leverage transformer-based Multilingual Large Language Models (MLLMs), XLM-RoBERTa and Indic-BERT. We fine-tune MLLMs on the released data splits, with and without pre-processing steps (translation to english), for classifying texts into the appropriate emotion category. Our methods did not surpass the baseline, and our submission is ranked sixth overall.</abstract>
      <url hash="be03486a">2023.wassa-1.58</url>
      <bibkey>vedula-etal-2023-precogiiith</bibkey>
      <doi>10.18653/v1/2023.wassa-1.58</doi>
      <video href="2023.wassa-1.58.mp4"/>
    </paper>
    <paper id="59">
      <title><fixed-case>B</fixed-case>p<fixed-case>H</fixed-case>igh at <fixed-case>WASSA</fixed-case> 2023: Using Contrastive Learning to build Sentence Transformer models for Multi-Class Emotion Classification in Code-mixed <fixed-case>U</fixed-case>rdu</title>
      <author><first>Bhavish</first><last>Pahwa</last></author>
      <pages>606-610</pages>
      <abstract>In this era of digital communication and social media, texting and chatting among individuals occur mainly through code-mixed or Romanized versions of the native language prevalent in the region. The presence of Romanized and code-mixed language develops the need to build NLP systems in these domains to leverage the digital content for various use cases. This paper describes our contribution to the subtask MCEC of the shared task WASSA 2023:Shared Task on Multi-Label and Multi-Class Emotion Classification on Code-Mixed Text Messages. We explore how one can build sentence transformers models for low-resource languages using unsupervised data by leveraging contrastive learning techniques described in the SIMCSE paper and using the sentence transformer developed to build classification models using the SetFit approach. Additionally, we’ll publish our code and models on GitHub and HuggingFace, two open-source hosting services.</abstract>
      <url hash="7b04ad3f">2023.wassa-1.59</url>
      <bibkey>pahwa-2023-bphigh</bibkey>
      <doi>10.18653/v1/2023.wassa-1.59</doi>
      <video href="2023.wassa-1.59.mp4"/>
    </paper>
    <paper id="60">
      <title><fixed-case>YNU</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>WASSA</fixed-case> 2023: Using Text-Mixed Data Augmentation for Emotion Classification on Code-Mixed Text Message</title>
      <author><first>Xuqiao</first><last>Ran</last></author>
      <author><first>You</first><last>Zhang</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Dan</first><last>Xu</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>611-615</pages>
      <abstract>Emotion classification on code-mixed texts has been widely used in real-world applications. In this paper, we build a system that participates in the WASSA 2023 Shared Task 2 for emotion classification on code-mixed text messages from Roman Urdu and English. The main goal of the proposed method is to adopt a text-mixed data augmentation for robust code-mixed text representation. We mix texts with both multi-label (track 1) and multi-class (track 2) annotations in a unified multilingual pre-trained model, i.e., XLM-RoBERTa, for both subtasks. Our results show that the proposed text-mixed method performs competitively, ranking first in both tracks, achieving an average Macro F1 score of 0.9782 on the multi-label track and of 0.9329 on the multi-class track.</abstract>
      <url hash="78ed3e8d">2023.wassa-1.60</url>
      <bibkey>ran-etal-2023-ynu</bibkey>
      <doi>10.18653/v1/2023.wassa-1.60</doi>
    </paper>
    <paper id="61">
      <title>Generative Pretrained Transformers for Emotion Detection in a Code-Switching Setting</title>
      <author><first>Andrew</first><last>Nedilko</last></author>
      <pages>616-620</pages>
      <abstract>This paper describes the approach that we utilized to participate in the shared task for multi-label and multi-class emotion classification organized as part of WASSA 2023 at ACL 2023. The objective was to build mod- els that can predict 11 classes of emotions, or the lack thereof (neutral class) based on code- mixed Roman Urdu and English SMS text messages. We participated in Track 2 of this task - multi-class emotion classification (MCEC). We used generative pretrained transformers, namely ChatGPT because it has a commercially available full-scale API, for the emotion detec- tion task by leveraging the prompt engineer- ing and zero-shot / few-shot learning method- ologies based on multiple experiments on the dev set. Although this was the first time we used a GPT model for the purpose, this ap- proach allowed us to beat our own baseline character-based XGBClassifier, as well as the baseline model trained by the organizers (bert- base-multilingual-cased). We ranked 4th and achieved the macro F1 score of 0.7038 and the accuracy of 0.7313 on the blind test set.</abstract>
      <url hash="4099b452">2023.wassa-1.61</url>
      <bibkey>nedilko-2023-generative</bibkey>
      <doi>10.18653/v1/2023.wassa-1.61</doi>
      <video href="2023.wassa-1.61.mp4"/>
    </paper>
  </volume>
</collection>
