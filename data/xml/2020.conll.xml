<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.conll">
  <volume id="1" ingest-date="2020-11-05">
    <meta>
      <booktitle>Proceedings of the 24th Conference on Computational Natural Language Learning</booktitle>
      <editor><first>Raquel</first><last>Fernández</last></editor>
      <editor><first>Tal</first><last>Linzen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="92d9bf6e">2020.conll-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Enriching Word Embeddings with Temporal and Spatial Information</title>
      <author><first>Hongyu</first><last>Gong</last></author>
      <author><first>Suma</first><last>Bhat</last></author>
      <author><first>Pramod</first><last>Viswanath</last></author>
      <pages>1–11</pages>
      <abstract>The meaning of a word is closely linked to sociocultural factors that can change over time and location, resulting in corresponding meaning changes. Taking a global view of words and their meanings in a widely used language, such as English, may require us to capture more refined semantics for use in time-specific or location-aware situations, such as the study of cultural trends or language use. However, popular vector representations for words do not adequately include temporal or spatial information. In this work, we present a model for learning word representation conditioned on time and location. In addition to capturing meaning changes over time and location, we require that the resulting word embeddings retain salient semantic and geometric properties. We train our model on time- and location-stamped corpora, and show using both quantitative and qualitative evaluations that it can capture semantics across time and locations. We note that our model compares favorably with the state-of-the-art for time-specific embedding, and serves as a new benchmark for location-specific embeddings.</abstract>
      <url hash="db212df9">2020.conll-1.1</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c3fc5900">2020.conll-1.1.OptionalSupplementaryMaterial.zip</attachment>
    </paper>
    <paper id="2">
      <title>Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension</title>
      <author><first>Ekta</first><last>Sood</last></author>
      <author><first>Simon</first><last>Tannert</last></author>
      <author><first>Diego</first><last>Frassinelli</last></author>
      <author><first>Andreas</first><last>Bulling</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>12–25</pages>
      <abstract>While neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks, it remains unclear to which extent learned attention resembles human visual attention. In this paper, we propose a new method that leverages eye-tracking data to investigate the relationship between human visual attention and neural attention in machine reading comprehension. To this end, we introduce a novel 23 participant eye tracking dataset - MQA-RC, in which participants read movie plots and answered pre-defined questions. We compare state of the art networks based on long short-term memory (LSTM), convolutional neural models (CNN) and XLNet Transformer architectures. We find that higher similarity to human attention and performance significantly correlates to the LSTM and CNN models. However, we show this relationship does not hold true for the XLNet models – despite the fact that the XLNet performs best on this challenging task. Our results suggest that different architectures seem to learn rather different neural attention strategies and similarity of neural to human attention does not guarantee best performance.</abstract>
      <url hash="5eb79643">2020.conll-1.2</url>
    </paper>
    <paper id="3">
      <title>Neural Proof Nets</title>
      <author><first>Konstantinos</first><last>Kogkalidis</last></author>
      <author><first>Michael</first><last>Moortgat</last></author>
      <author><first>Richard</first><last>Moot</last></author>
      <pages>26–40</pages>
      <abstract>Linear logic and the linear λ-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on ÆThel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear λ-calculus with an accuracy of as high as 70%.</abstract>
      <url hash="dfb23f26">2020.conll-1.3</url>
    </paper>
    <paper id="4">
      <title><fixed-case>T</fixed-case>axi<fixed-case>NLI</fixed-case>: Taking a Ride up the <fixed-case>NLU</fixed-case> Hill</title>
      <author><first>Pratik</first><last>Joshi</last></author>
      <author><first>Somak</first><last>Aditya</last></author>
      <author><first>Aalok</first><last>Sathe</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>41–55</pages>
      <abstract>Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remains unclear as to which specific concepts are learnt by the trained systems and where they can achieve strong generalization. To investigate this question, we propose a taxonomic hierarchy of categories that are relevant for the NLI task. We introduce TaxiNLI, a new dataset, that has 10k examples from the MNLI dataset with these taxonomic labels. Through various experiments on TaxiNLI, we observe that whereas for certain taxonomic categories SOTA neural models have achieved near perfect accuracies—a large jump over the previous models—some categories still remain difficult. Our work adds to the growing body of literature that shows the gaps in the current NLI systems and datasets through a systematic presentation and analysis of reasoning categories.</abstract>
      <url hash="feb958b6">2020.conll-1.4</url>
    </paper>
    <paper id="5">
      <title>Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives</title>
      <author><first>Elisa</first><last>Kreiss</last></author>
      <author><first>Zijian</first><last>Wang</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <pages>56–68</pages>
      <abstract>Crime reporting is a prevalent form of journalism with the power to shape public perceptions and social policies. How does the language of these reports act on readers? We seek to address this question with the SuspectGuilt Corpus of annotated crime stories from English-language newspapers in the U.S. For SuspectGuilt, annotators read short crime articles and provided text-level ratings concerning the guilt of the main suspect as well as span-level annotations indicating which parts of the story they felt most influenced their ratings. SuspectGuilt thus provides a rich picture of how linguistic choices affect subjective guilt judgments. We use SuspectGuilt to train and assess predictive models which validate the usefulness of the corpus, and show that these models benefit from genre pretraining and joint supervision from the text-level ratings and span-level annotations. Such models might be used as tools for understanding the societal effects of crime reporting.</abstract>
      <url hash="b3312e60">2020.conll-1.5</url>
      <attachment type="OptionalSupplementaryMaterial" hash="79b11380">2020.conll-1.5.OptionalSupplementaryMaterial.zip</attachment>
    </paper>
    <paper id="6">
      <title>On the Frailty of Universal <fixed-case>POS</fixed-case> Tags for Neural <fixed-case>UD</fixed-case> Parsers</title>
      <author><first>Mark</first><last>Anderson</last></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last></author>
      <pages>69–96</pages>
      <abstract>We present an analysis on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as fea-tures for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionality. We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem.</abstract>
      <url hash="27be3738">2020.conll-1.6</url>
    </paper>
    <paper id="7">
      <title>Classifying Syntactic Errors in Learner Language</title>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Dmitry</first><last>Nikolaev</last></author>
      <author><first>Yevgeni</first><last>Berzak</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>97–107</pages>
      <abstract>We present a method for classifying syntactic errors in learner language, namely errors whose correction alters the morphosyntactic structure of a sentence. The methodology builds on the established Universal Dependencies syntactic representation scheme, and provides complementary information to other error-classification systems. Unlike existing error classification methods, our method is applicable across languages, which we showcase by producing a detailed picture of syntactic errors in learner English and learner Russian. We further demonstrate the utility of the methodology for analyzing the outputs of leading Grammatical Error Correction (GEC) systems.</abstract>
      <url hash="4599a9a8">2020.conll-1.7</url>
      <attachment type="OptionalSupplementaryMaterial" hash="9016a375">2020.conll-1.7.OptionalSupplementaryMaterial.pdf</attachment>
    </paper>
    <paper id="8">
      <title>How to Probe Sentence Embeddings in Low-Resource Languages: On Structural Design Choices for Probing Task Evaluation</title>
      <author><first>Steffen</first><last>Eger</last></author>
      <author><first>Johannes</first><last>Daxenberger</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>108–118</pages>
      <abstract>Sentence encoders map sentences to real valued vectors for use in downstream applications. To peek into these representations—e.g., to increase interpretability of their results—probing tasks have been designed which query them for linguistic knowledge. However, designing probing tasks for lesser-resourced languages is tricky, because these often lack largescale annotated data or (high-quality) dependency parsers as a prerequisite of probing task design in English. To investigate how to probe sentence embeddings in such cases, we investigate sensitivity of probing task results to structural design choices, conducting the first such large scale study. We show that design choices like size of the annotated probing dataset and type of classifier used for evaluation do (sometimes substantially) influence probing outcomes. We then probe embeddings in a multilingual setup with design choices that lie in a ‘stable region’, as we identify for English, and find that results on English do not transfer to other languages. Fairer and more comprehensive sentence-level probing evaluation should thus be carried out on multiple languages in the future.</abstract>
      <url hash="6692a27b">2020.conll-1.8</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8d3596c0">2020.conll-1.8.OptionalSupplementaryMaterial.pdf</attachment>
    </paper>
    <paper id="9">
      <title>Understanding the Source of Semantic Regularities in Word Embeddings</title>
      <author><first>Hsiao-Yu</first><last>Chiang</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <author><first>Zachary</first><last>Pardos</last></author>
      <pages>119–131</pages>
      <abstract>Semantic relations are core to how humans understand and express concepts in the real world using language. Recently, there has been a thread of research aimed at modeling these relations by learning vector representations from text corpora. Most of these approaches focus strictly on leveraging the co-occurrences of relationship word pairs within sentences. In this paper, we investigate the hypothesis that examples of a lexical relation in a corpus are fundamental to a neural word embedding’s ability to complete analogies involving the relation. Our experiments, in which we remove all known examples of a relation from training corpora, show only marginal degradation in analogy completion performance involving the removed relation. This finding enhances our understanding of neural word embeddings, showing that co-occurrence information of a particular semantic relation is the not the main source of their structural regularity.</abstract>
      <url hash="a9eea6e9">2020.conll-1.9</url>
    </paper>
    <paper id="10">
      <title>Finding The Right One and Resolving it</title>
      <author><first>Payal</first><last>Khullar</last></author>
      <author><first>Arghya</first><last>Bhattacharya</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>132–141</pages>
      <abstract>One-anaphora has figured prominently in theoretical linguistic literature, but computational linguistics research on the phenomenon is sparse. Not only that, the long standing linguistic controversy between the determinative and the nominal anaphoric element one has propagated in the limited body of computational work on one-anaphora resolution, making this task harder than it is. In the present paper, we resolve this by drawing from an adequate linguistic analysis of the word one in different syntactic environments - once again highlighting the significance of linguistic theory in Natural Language Processing (NLP) tasks. We prepare an annotated corpus marking actual instances of one-anaphora with their textual antecedents, and use the annotations to experiment with state-of-the art neural models for one-anaphora resolution. Apart from presenting a strong neural baseline for this task, we contribute a gold-standard corpus, which is, to the best of our knowledge, the biggest resource on one-anaphora till date.</abstract>
      <url hash="51985920">2020.conll-1.10</url>
    </paper>
    <paper id="11">
      <title>Bridging Information-Seeking Human Gaze and Machine Reading Comprehension</title>
      <author><first>Jonathan</first><last>Malmaud</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <author><first>Yevgeni</first><last>Berzak</last></author>
      <pages>142–152</pages>
      <abstract>In this work, we analyze how human gaze during reading comprehension is conditioned on the given reading comprehension question, and whether this signal can be beneficial for machine reading comprehension. To this end, we collect a new eye-tracking dataset with a large number of participants engaging in a multiple choice reading comprehension task. Our analysis of this data reveals increased fixation times over parts of the text that are most relevant for answering the question. Motivated by this finding, we propose making automated reading comprehension more human-like by mimicking human information-seeking reading behavior during reading comprehension. We demonstrate that this approach leads to performance gains on multiple choice question answering in English for a state-of-the-art reading comprehension model.</abstract>
      <url hash="42856397">2020.conll-1.11</url>
    </paper>
    <paper id="12">
      <title>A Corpus of Very Short Scientific Summaries</title>
      <author><first>Yifan</first><last>Chen</last></author>
      <author><first>Tamara</first><last>Polajnar</last></author>
      <author><first>Colin</first><last>Batchelor</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <pages>153–164</pages>
      <abstract>We present a new summarisation task, taking scientific articles and producing journal table-of-contents entries in the chemistry domain. These are one- or two-sentence author-written summaries that present the key findings of a paper. This is a first look at this summarisation task with an open access publication corpus consisting of titles and abstracts, as input texts, and short author-written advertising blurbs, as the ground truth. We introduce the dataset and evaluate it with state-of-the-art summarisation methods.</abstract>
      <url hash="aee8def9">2020.conll-1.12</url>
    </paper>
    <paper id="13">
      <title>Recurrent babbling: evaluating the acquisition of grammar from limited input data</title>
      <author><first>Ludovica</first><last>Pannitto</last></author>
      <author><first>Aurélie</first><last>Herbelot</last></author>
      <pages>165–176</pages>
      <abstract>Recurrent Neural Networks (RNNs) have been shown to capture various aspects of syntax from raw linguistic input. In most previous experiments, however, learning happens over unrealistic corpora, which do not reflect the type and amount of data a child would be exposed to. This paper remedies this state of affairs by training an LSTM over a realistically sized subset of child-directed input. The behaviour of the network is analysed over time using a novel methodology which consists in quantifying the level of grammatical abstraction in the model’s generated output (its ‘babbling’), compared to the language it has been exposed to. We show that the LSTM indeed abstracts new structures as learning proceeds.</abstract>
      <url hash="c12d1427">2020.conll-1.13</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c8d1247d">2020.conll-1.13.OptionalSupplementaryMaterial.zip</attachment>
    </paper>
    <paper id="14">
      <title>A simple repair mechanism can alleviate computational demands of pragmatic reasoning: simulations and complexity analysis</title>
      <author><first>Jacqueline</first><last>van Arkel</last></author>
      <author><first>Marieke</first><last>Woensdregt</last></author>
      <author><first>Mark</first><last>Dingemanse</last></author>
      <author><first>Mark</first><last>Blokpoel</last></author>
      <pages>177–194</pages>
      <abstract>How can people communicate successfully while keeping resource costs low in the face of ambiguity? We present a principled theoretical analysis comparing two strategies for disambiguation in communication: (i) pragmatic reasoning, where communicators reason about each other, and (ii) other-initiated repair, where communicators signal and resolve trouble interactively. Using agent-based simulations and computational complexity analyses, we compare the efficiency of these strategies in terms of communicative success, computation cost and interaction cost. We show that agents with a simple repair mechanism can increase efficiency, compared to pragmatic agents, by reducing their computational burden at the cost of longer interactions. We also find that efficiency is highly contingent on the mechanism, highlighting the importance of explicit formalisation and computational rigour.</abstract>
      <url hash="9750a37f">2020.conll-1.14</url>
    </paper>
    <paper id="15">
      <title>Acquiring language from speech by learning to remember and predict</title>
      <author><first>Cory</first><last>Shain</last></author>
      <author><first>Micha</first><last>Elsner</last></author>
      <pages>195–214</pages>
      <abstract>Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech, while more recent proposals stress the importance of prediction for language learning. In this study, we propose a broad-coverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream. Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals. We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.</abstract>
      <url hash="74e5fb17">2020.conll-1.15</url>
    </paper>
    <paper id="16">
      <title>Identifying Incorrect Labels in the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>-2003 Corpus</title>
      <author><first>Frederick</first><last>Reiss</last></author>
      <author><first>Hong</first><last>Xu</last></author>
      <author><first>Bryan</first><last>Cutler</last></author>
      <author><first>Karthik</first><last>Muthuraman</last></author>
      <author><first>Zachary</first><last>Eichenberger</last></author>
      <pages>215–226</pages>
      <abstract>The CoNLL-2003 corpus for English-language named entity recognition (NER) is one of the most influential corpora for NER model research. A large number of publications, including many landmark works, have used this corpus as a source of ground truth for NER tasks. In this paper, we examine this corpus and identify over 1300 incorrect labels (out of 35089 in the corpus). In particular, the number of incorrect labels in the test fold is comparable to the number of errors that state-of-the-art models make when running inference over this corpus. We describe the process by which we identified these incorrect labels, using novel variants of techniques from semi-supervised learning. We also summarize the types of errors that we found, and we revisit several recent results in NER in light of the corrected data. Finally, we show experimentally that our corrections to the corpus have a positive impact on three state-of-the-art models.</abstract>
      <url hash="b111e911">2020.conll-1.16</url>
    </paper>
    <paper id="17">
      <title>When is a bishop not like a rook? When it’s like a rabbi! Multi-prototype <fixed-case>BERT</fixed-case> embeddings for estimating semantic relationships</title>
      <author><first>Gabriella</first><last>Chronis</last></author>
      <author><first>Katrin</first><last>Erk</last></author>
      <pages>227–244</pages>
      <abstract>This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level. We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018). These embeddings retain contextual knowledge that is critical for some type-level tasks, while being less cumbersome and less subject to outlier effects than exemplar models. Similarity and relatedness estimation, both type-level tasks, benefit from this contextual knowledge, indicating the context-sensitivity of these processes. BERT’s token level knowledge also allows the testing of a type-level hypothesis about lexical abstractness, demonstrating the relationship between token-level phenomena and type-level concreteness ratings. Our findings provide important insight into the interpretability of BERT: layer 7 approximates semantic similarity, while the final layer (11) approximates relatedness.</abstract>
      <url hash="5787a8a1">2020.conll-1.17</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e80847ef">2020.conll-1.17.OptionalSupplementaryMaterial.zip</attachment>
    </paper>
    <paper id="18">
      <title>Processing effort is a poor predictor of cross-linguistic word order frequency</title>
      <author><first>Brennan</first><last>Gonering</last></author>
      <author><first>Emily</first><last>Morgan</last></author>
      <pages>245–255</pages>
      <abstract>Some have argued that word orders which are more difficult to process should be rarer cross-linguistically. Our current study fails to replicate the results of Maurits, Navarro, and Perfors (2010), who used an entropy-based Uniform Information Density (UID) measure to moderately predict the Greenbergian typology of transitive word orders. We additionally report an inability of three measures of processing difficulty — entropy-based UID, surprisal-based UID, and pointwise mutual information — to correctly predict the correct typological distribution, using transitive constructions from 20 languages in the Universal Dependencies project (version 2.5). However, our conclusions are limited by data sparsity.</abstract>
      <url hash="b0ddf5bd">2020.conll-1.18</url>
    </paper>
    <paper id="19">
      <title>Relations between comprehensibility and adequacy errors in machine translation output</title>
      <author><first>Maja</first><last>Popović</last></author>
      <pages>256–264</pages>
      <abstract>This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues. The main finding is that good comprehensibility, similarly to good fluency, can mask a number of adequacy errors. Of all major adequacy errors, 30% were fully comprehensible, thus fully misleading the reader to accept the incorrect information. Another 25% of major adequacy errors were perceived as almost comprehensible, thus being potentially misleading. Also, a vast majority of omissions (about 70%) is hidden by comprehensibility. Further analysis of misleading translations revealed that the most frequent error types are ambiguity, mistranslation, noun phrase error, word-by-word translation, untranslated word, subject-verb agreement, and spelling error in the source text. However, none of these error types appears exclusively in misleading translations, but are also frequent in fully incorrect (incomprehensible inadequate) and discarded correct (incomprehensible adequate) translations. Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations.</abstract>
      <url hash="8e87c2e8">2020.conll-1.19</url>
    </paper>
    <paper id="20">
      <title>Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment</title>
      <author><first>Hartger</first><last>Veeman</last></author>
      <author><first>Marc</first><last>Allassonnière-Tang</last></author>
      <author><first>Aleksandrs</first><last>Berdicevskis</last></author>
      <author><first>Ali</first><last>Basirat</last></author>
      <pages>265–275</pages>
      <abstract>Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two experiments to address these questions about language typology and human cognition. In both experiments, we predict the gender of nouns in language X using a classifier trained on the nouns of language Y, and take the classifier’s accuracy as a measure of transferability of gender systems. First, we show that for 22 Indo-European languages the transferability decreases as the phylogenetic distance increases. This correlation supports the claim that some gender assignment factors are idiosyncratic, and as the languages diverge, the proportion of shared inherited idiosyncrasies diminishes. Second, we show that when the classifier is trained on two Afro-Asiatic languages and tested on the same 22 Indo-European languages (or vice versa), its performance is still significantly above the chance baseline, thus showing that universal factors exist and, moreover, can be captured by word embeddings. When the classifier is tested across families and on inanimate nouns only, the performance is still above baseline, indicating that the universal factors are not limited to biological sex.</abstract>
      <url hash="65b7a793">2020.conll-1.20</url>
    </paper>
    <paper id="21">
      <title>Modelling Lexical Ambiguity with Density Matrices</title>
      <author><first>Francois</first><last>Meyer</last></author>
      <author><first>Martha</first><last>Lewis</last></author>
      <pages>276–290</pages>
      <abstract>Words can have multiple senses. Compositional distributional models of meaning have been argued to deal well with finer shades of meaning variation known as polysemy, but are not so well equipped to handle word senses that are etymologically unrelated, or homonymy. Moving from vectors to density matrices allows us to encode a probability distribution over different senses of a word, and can also be accommodated within a compositional distributional model of meaning. In this paper we present three new neural models for learning density matrices from a corpus, and test their ability to discriminate between word senses on a range of compositional datasets. When paired with a particular composition method, our best model outperforms existing vector-based compositional models as well as strong sentence encoders.</abstract>
      <url hash="a6b0e982">2020.conll-1.21</url>
      <attachment type="OptionalSupplementaryMaterial" hash="1cb732f8">2020.conll-1.21.OptionalSupplementaryMaterial.zip</attachment>
    </paper>
    <paper id="22">
      <title>Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech</title>
      <author><first>William</first><last>Havard</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Jean-Pierre</first><last>Chevrot</last></author>
      <pages>291–301</pages>
      <abstract>The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units. This suggests that the ideal way of learning a language is by starting from full semantic units. In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task. We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information. We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient. We also explore at which level of the network’s architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using low-level or high-level segments in isolation.</abstract>
      <url hash="8ccf84d3">2020.conll-1.22</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5743c3a6">2020.conll-1.22.OptionalSupplementaryMaterial.zip</attachment>
    </paper>
    <paper id="23">
      <title>Learning to ground medical text in a 3<fixed-case>D</fixed-case> human atlas</title>
      <author><first>Dusan</first><last>Grujicic</last></author>
      <author><first>Gorjan</first><last>Radevski</last></author>
      <author><first>Tinne</first><last>Tuytelaars</last></author>
      <author><first>Matthew</first><last>Blaschko</last></author>
      <pages>302–312</pages>
      <abstract>In this paper, we develop a method for grounding medical text into a physically meaningful and interpretable space corresponding to a human atlas. We build on text embedding architectures such as Bert and introduce a loss function that allows us to reason about the semantic and spatial relatedness of medical texts by learning a projection of the embedding into a 3D space representing the human body. We quantitatively and qualitatively demonstrate that our proposed method learns a context sensitive and spatially aware mapping, in both the inter-organ and intra-organ sense, using a large scale medical text dataset from the “Large-scale online biomedical semantic indexing” track of the 2020 BioASQ challenge. We extend our approach to a self-supervised setting, and find it to be competitive with a classification based method, and a fully supervised variant of approach.</abstract>
      <url hash="f37a0315">2020.conll-1.23</url>
      <attachment type="OptionalSupplementaryMaterial" hash="f9140a89">2020.conll-1.23.OptionalSupplementaryMaterial.zip</attachment>
    </paper>
    <paper id="24">
      <title>Representation Learning for Type-Driven Composition</title>
      <author><first>Gijs</first><last>Wijnholds</last></author>
      <author><first>Mehrnoosh</first><last>Sadrzadeh</last></author>
      <author><first>Stephen</first><last>Clark</last></author>
      <pages>313–324</pages>
      <abstract>This paper is about learning word representations using grammatical type information. We use the syntactic types of Combinatory Categorial Grammar to develop multilinear representations, i.e. maps with n arguments, for words with different functional types. The multilinear maps of words compose with each other to form sentence representations. We extend the skipgram algorithm from vectors to multi- linear maps to learn these representations and instantiate it on unary and binary maps for transitive verbs. These are evaluated on verb and sentence similarity and disambiguation tasks and a subset of the SICK relatedness dataset. Our model performs better than previous type- driven models and is competitive with state of the art representation learning methods such as BERT and neural sentence encoders.</abstract>
      <url hash="8dfa1349">2020.conll-1.24</url>
    </paper>
    <paper id="25">
      <title>Word Representations Concentrate and This is Good News!</title>
      <author><first>Romain</first><last>Couillet</last></author>
      <author><first>Yagmur Gizem</first><last>Cinar</last></author>
      <author><first>Eric</first><last>Gaussier</last></author>
      <author><first>Muhammad</first><last>Imran</last></author>
      <pages>325–334</pages>
      <abstract>This article establishes that, unlike the legacy tf*idf representation, recent natural language representations (word embedding vectors) tend to exhibit a so-called <i>concentration of measure phenomenon</i>, in the sense that, as the representation size <tex-math>p</tex-math> and database size <tex-math>n</tex-math> are both large, their behavior is similar to that of large dimensional Gaussian random vectors. This phenomenon may have important consequences as machine learning algorithms for natural language data could be amenable to improvement, thereby providing new theoretical insights into the field of natural language processing.</abstract>
      <url hash="559aa2de">2020.conll-1.25</url>
    </paper>
    <paper id="26">
      <title>“<fixed-case>L</fixed-case>az<fixed-case>I</fixed-case>mpa”: Lazy and Impatient neural agents learn to communicate efficiently</title>
      <author><first>Mathieu</first><last>Rita</last></author>
      <author><first>Rahma</first><last>Chaabouni</last></author>
      <author><first>Emmanuel</first><last>Dupoux</last></author>
      <pages>335–343</pages>
      <abstract>Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, “LazImpa”, where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.</abstract>
      <url hash="38192e0a">2020.conll-1.26</url>
      <attachment type="OptionalSupplementaryMaterial" hash="df2dd4d9">2020.conll-1.26.OptionalSupplementaryMaterial.zip</attachment>
    </paper>
    <paper id="27">
      <title>Re-solve it: simulating the acquisition of core semantic competences from small data</title>
      <author><first>Aurélie</first><last>Herbelot</last></author>
      <pages>344–354</pages>
      <abstract>Many tasks are considered to be ‘solved’ in the computational linguistics literature, but the corresponding algorithms operate in ways which are radically different from human cognition. I illustrate this by coming back to the notion of semantic competence, which includes basic linguistic skills encompassing both referential phenomena and generic knowledge, in particular a) the ability to denote, b) the mastery of the lexicon, or c) the ability to model one’s language use on others. Even though each of those faculties has been extensively tested individually, there is still no computational model that would account for their joint acquisition under the conditions experienced by a human. In this paper, I focus on one particular aspect of this problem: the amount of linguistic data available to the child or machine. I show that given the first competence mentioned above (a denotation function), the other two can in fact be learned from very limited data (2.8M token), reaching state-of-the-art performance. I argue that both the nature of the data and the way it is presented to the system matter to acquisition.</abstract>
      <url hash="0ad4f894">2020.conll-1.27</url>
    </paper>
    <paper id="28">
      <title>In Media Res: A Corpus for Evaluating Named Entity Linking with Creative Works</title>
      <author><first>Adrian M.P.</first><last>Brasoveanu</last></author>
      <author><first>Albert</first><last>Weichselbraun</last></author>
      <author><first>Lyndon</first><last>Nixon</last></author>
      <pages>355–364</pages>
      <abstract>Annotation styles express guidelines that direct human annotators in what rules to follow when creating gold standard annotations of text corpora. These guidelines not only shape the gold standards they help create, but also influence the training and evaluation of Named Entity Linking (NEL) tools, since different annotation styles correspond to divergent views on the entities present in the same texts. Such divergence is particularly present in texts from the media domain that contain references to creative works. In this work we present a corpus of 1000 annotated documents selected from the media domain. Each document is presented with multiple gold standard annotations representing various annotation styles. This corpus is used to evaluate a series of Named Entity Linking tools in order to understand the impact of the differences in annotation styles on the reported accuracy when processing highly ambiguous entities such as names of creative works. Relaxed annotation guidelines that include overlap styles lead to better results across all tools.</abstract>
      <url hash="8cfc2c8c">2020.conll-1.28</url>
    </paper>
    <paper id="29">
      <title>Analogies minus analogy test: measuring regularities in word embeddings</title>
      <author><first>Louis</first><last>Fournier</last></author>
      <author><first>Emmanuel</first><last>Dupoux</last></author>
      <author><first>Ewan</first><last>Dunbar</last></author>
      <pages>365–375</pages>
      <abstract>Vector space models of words have long been claimed to capture linguistic regularities as simple vector translations, but problems have been raised with this claim. We decompose and empirically analyze the classic arithmetic word analogy test, to motivate two new metrics that address the issues with the standard test, and which distinguish between class-wise offset concentration (similar directions between pairs of words drawn from different broad classes, such as France-London, China-Ottawa,...) and pairing consistency (the existence of a regular transformation between correctly-matched pairs such as France:Paris::China:Beijing). We show that, while the standard analogy test is flawed, several popular word embeddings do nevertheless encode linguistic regularities.</abstract>
      <url hash="3086904a">2020.conll-1.29</url>
    </paper>
    <paper id="30">
      <title>Word associations and the distance properties of context-aware word embeddings</title>
      <author><first>Maria</first><last>A. Rodriguez</last></author>
      <author><first>Paola</first><last>Merlo</last></author>
      <pages>376–385</pages>
      <abstract>What do people know when they know the meaning of words? Word associations have been widely used to tap into lexical repre- sentations and their structure, as a way of probing semantic knowledge in humans. We investigate whether current word embedding spaces (contextualized and uncontextualized) can be considered good models of human lexi- cal knowledge by studying whether they have comparable characteristics to human associa- tion spaces. We study the three properties of association rank, asymmetry of similarity and triangle inequality. We find that word embeddings are good mod- els of some word associations properties. They replicate well human associations between words, and, like humans, their context-aware variants show violations of the triangle in- equality. While they do show asymmetry of similarities, their asymmetries do not map those of human association norms.</abstract>
      <url hash="d951870e">2020.conll-1.30</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b7589150">2020.conll-1.30.OptionalSupplementaryMaterial.zip</attachment>
    </paper>
    <paper id="31">
      <title><fixed-case>T</fixed-case>r<fixed-case>C</fixed-case>laim-19: The First Collection for <fixed-case>T</fixed-case>urkish Check-Worthy Claim Detection with Annotator Rationales</title>
      <author><first>Yavuz Selim</first><last>Kartal</last></author>
      <author><first>Mucahid</first><last>Kutlu</last></author>
      <pages>386–395</pages>
      <abstract>Massive misinformation spread over Internet has many negative impacts on our lives. While spreading a claim is easy, investigating its veracity is hard and time consuming, Therefore, we urgently need systems to help human fact-checkers. However, available data resources to develop effective systems are limited and the vast majority of them is for English. In this work, we introduce TrClaim-19, which is the very first labeled dataset for Turkish check-worthy claims. TrClaim-19 consists of labeled 2287 Turkish tweets with annotator rationales, enabling us to better understand the characteristics of check-worthy claims. The rationales we collected suggest that claims’ topics and their possible negative impacts are the main factors affecting their check-worthiness.</abstract>
      <url hash="bc4498ae">2020.conll-1.31</url>
    </paper>
    <paper id="32">
      <title>Discourse structure interacts with reference but not syntax in neural language models</title>
      <author><first>Forrest</first><last>Davis</last></author>
      <author><first>Marten</first><last>van Schijndel</last></author>
      <pages>396–407</pages>
      <abstract>Language models (LMs) trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different linguistic representations. In particular, we utilized stimuli from psycholinguistic studies showing that humans can condition reference (i.e. coreference resolution) and syntactic processing on the same discourse structure (implicit causality). We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information. Our results further suggest that LM behavior can contradict not only learned representations of discourse but also syntactic agreement, pointing to shortcomings of standard language modeling.</abstract>
      <url hash="c9e34400">2020.conll-1.32</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b767daa4">2020.conll-1.32.OptionalSupplementaryMaterial.pdf</attachment>
    </paper>
    <paper id="33">
      <title>Continual Adaptation for Efficient Machine Communication</title>
      <author><first>Robert</first><last>Hawkins</last></author>
      <author><first>Minae</first><last>Kwon</last></author>
      <author><first>Dorsa</first><last>Sadigh</last></author>
      <author><first>Noah</first><last>Goodman</last></author>
      <pages>408–419</pages>
      <abstract>To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. Recent neural language models are able to comprehend and produce the existing conventions present in their training data, but are not able to flexibly and interactively adapt those conventions on the fly as humans do. We introduce an interactive repeated reference task as a benchmark for models of adaptation in communication and propose a regularized continual learning framework that allows an artificial agent initialized with a generic language model to more accurately and efficiently communicate with a partner over time. We evaluate this framework through simulations on COCO and in real-time reference game experiments with human partners.</abstract>
      <url hash="949e6710">2020.conll-1.33</url>
      <attachment type="OptionalSupplementaryMaterial" hash="3fedb713">2020.conll-1.33.OptionalSupplementaryMaterial.pdf</attachment>
    </paper>
    <paper id="34">
      <title>Diverse and Relevant Visual Storytelling with Scene Graph Embeddings</title>
      <author><first>Xudong</first><last>Hong</last></author>
      <author><first>Rakshith</first><last>Shetty</last></author>
      <author><first>Asad</first><last>Sayeed</last></author>
      <author><first>Khushboo</first><last>Mehra</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <author><first>Bernt</first><last>Schiele</last></author>
      <pages>420–430</pages>
      <abstract>A problem in automatically generated stories for image sequences is that they use overly generic vocabulary and phrase structure and fail to match the distributional characteristics of human-generated text. We address this problem by introducing explicit representations for objects and their relations by extracting scene graphs from the images. Utilizing an embedding of this scene graph enables our model to more explicitly reason over objects and their relations during story generation, compared to the global features from an object classifier used in previous work. We apply metrics that account for the diversity of words and phrases of generated stories as well as for reference to narratively-salient image features and show that our approach outperforms previous systems. Our experiments also indicate that our models obtain competitive results on reference-based metrics.</abstract>
      <url hash="605ba217">2020.conll-1.34</url>
    </paper>
    <paper id="35">
      <title>Alleviating Digitization Errors in Named Entity Recognition for Historical Documents</title>
      <author><first>Emanuela</first><last>Boros</last></author>
      <author><first>Ahmed</first><last>Hamdi</last></author>
      <author><first>Elvys</first><last>Linhares Pontes</last></author>
      <author><first>Luis Adrián</first><last>Cabrera-Diego</last></author>
      <author><first>Jose G.</first><last>Moreno</last></author>
      <author><first>Nicolas</first><last>Sidere</last></author>
      <author><first>Antoine</first><last>Doucet</last></author>
      <pages>431–441</pages>
      <abstract>This paper tackles the task of named entity recognition (NER) applied to digitized historical texts obtained from processing digital images of newspapers using optical character recognition (OCR) techniques. We argue that the main challenge for this task is that the OCR process leads to misspellings and linguistic errors in the output text. Moreover, historical variations can be present in aged documents, which can impact the performance of the NER process. We conduct a comparative evaluation on two historical datasets in German and French against previous state-of-the-art models, and we propose a model based on a hierarchical stack of Transformers to approach the NER task for historical data. Our findings show that the proposed model clearly improves the results on both historical datasets, and does not degrade the results for modern datasets.</abstract>
      <url hash="d4f90c0d">2020.conll-1.35</url>
    </paper>
    <paper id="36">
      <title>Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models</title>
      <author><first>Steven</first><last>Derby</last></author>
      <author><first>Paul</first><last>Miller</last></author>
      <author><first>Barry</first><last>Devereux</last></author>
      <pages>442–454</pages>
      <abstract>Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks. In this paper, we make meaningful comparisons between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent. We also construct a new set of word embeddings using the output embeddings to create locally-optimal approximations for the intermediate representations from the language model. These locally-optimal embeddings demonstrate excellent performance across all our evaluations.</abstract>
      <url hash="ecbb9454">2020.conll-1.36</url>
      <attachment type="OptionalSupplementaryMaterial" hash="3590f41e">2020.conll-1.36.OptionalSupplementaryMaterial.pdf</attachment>
    </paper>
    <paper id="37">
      <title>On the Computational Power of Transformers and Its Implications in Sequence Modeling</title>
      <author><first>Satwik</first><last>Bhattamishra</last></author>
      <author><first>Arkil</first><last>Patel</last></author>
      <author><first>Navin</first><last>Goyal</last></author>
      <pages>455–475</pages>
      <abstract>Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their power and inherent limitations is still nascent. In particular, the roles of various components in Transformers such as positional encodings, attention heads, residual connections, and feedforward networks, are not clear. In this paper, we take a step towards answering these questions. We analyze the computational power as captured by Turing-completeness. We first provide an alternate and simpler proof to show that vanilla Transformers are Turing-complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing-complete. We further analyze the necessity of each component for the Turing-completeness of the network; interestingly, we find that a particular type of residual connection is necessary. We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks.</abstract>
      <url hash="8e7c189f">2020.conll-1.37</url>
      <attachment type="OptionalSupplementaryMaterial" hash="37522be7">2020.conll-1.37.OptionalSupplementaryMaterial.zip</attachment>
    </paper>
    <paper id="38">
      <title>An Expectation Maximisation Algorithm for Automated Cognate Detection</title>
      <author><first>Roddy</first><last>MacSween</last></author>
      <author><first>Andrew</first><last>Caines</last></author>
      <pages>476–485</pages>
      <abstract>In historical linguistics, cognate detection is the task of determining whether sets of words have common etymological roots. Inspired by the comparative method used by human linguists, we develop a system for automated cognate detection that frames the task as an inference problem for a general statistical model consisting of observed data (potentially cognate pairs of words), latent variables (the cognacy status of pairs) and unknown global parameters (which sounds correspond between languages). We then give a specific instance of such a model along with an expectation-maximisation algorithm to infer its parameters. We evaluate our system on a corpus of 8140 cognate sets, finding the performance of our method to be comparable to the state of the art. We additionally carry out qualitative analysis demonstrating advantages it has over existing systems. We also suggest several ways our work could be extended within the general theoretical framework we propose.</abstract>
      <url hash="653449dd">2020.conll-1.38</url>
    </paper>
    <paper id="39">
      <title>Filler-gaps that neural networks fail to generalize</title>
      <author><first>Debasmita</first><last>Bhattacharya</last></author>
      <author><first>Marten</first><last>van Schijndel</last></author>
      <pages>486–495</pages>
      <abstract>It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. In this work, we probe for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap dependencies across different surface constructions. For models to generalize abstract patterns in expected ways to unseen data, they must share representational features in predictable ways. We use cumulative priming to test for representational overlap between disparate filler-gap constructions in English and find evidence that the models learn a general representation for the existence of filler-gap dependencies. However, we find no evidence that the models learn any of the shared underlying grammatical constraints we tested. Our work raises questions about the degree to which RNN language models learn abstract linguistic representations.</abstract>
      <url hash="b734a741">2020.conll-1.39</url>
      <attachment type="OptionalSupplementaryMaterial" hash="0ae01d96">2020.conll-1.39.OptionalSupplementaryMaterial.zip</attachment>
    </paper>
    <paper id="40">
      <title>Don’t Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding</title>
      <author><first>Qile</first><last>Zhu</last></author>
      <author><first>Haidar</first><last>Khan</last></author>
      <author><first>Saleh</first><last>Soltan</last></author>
      <author><first>Stephen</first><last>Rawls</last></author>
      <author><first>Wael</first><last>Hamza</last></author>
      <pages>496–506</pages>
      <abstract>Semantic parsing is one of the key components of natural language understanding systems. A successful parse transforms an input utterance to an action that is easily understood by the system. Many algorithms have been proposed to solve this problem, from conventional rule-based or statistical slot-filling systems to shift-reduce based neural parsers. For complex parsing tasks, the state-of-the-art method is based on an autoregressive sequence to sequence model that generates the parse directly. This model is slow at inference time, generating parses in O(n) decoding steps (n is the length of the target sequence). In addition, we demonstrate that this method performs poorly in zero-shot cross-lingual transfer learning settings. In this paper, we propose a non-autoregressive parser which is based on the insertion transformer to overcome these two issues. Our approach 1) speeds up decoding by 3x while outperforming the autoregressive model and 2) significantly improves cross-lingual transfer in the low-resource setting by 37% compared to autoregressive baseline. We test our approach on three wellknown monolingual datasets: ATIS, SNIPS and TOP. For cross-lingual semantic parsing, we use the MultiATIS++ and the multilingual TOP datasets.</abstract>
      <url hash="aba91e7a">2020.conll-1.40</url>
    </paper>
    <paper id="41">
      <title>Learning Context-free Languages with Nondeterministic Stack <fixed-case>RNN</fixed-case>s</title>
      <author><first>Brian</first><last>DuSell</last></author>
      <author><first>David</first><last>Chiang</last></author>
      <pages>507–519</pages>
      <abstract>We present a differentiable stack data structure that simultaneously and tractably encodes an exponential number of stack configurations, based on Lang’s algorithm for simulating nondeterministic pushdown automata. We call the combination of this data structure with a recurrent neural network (RNN) controller a Nondeterministic Stack RNN. We compare our model against existing stack RNNs on various formal languages, demonstrating that our model converges more reliably to algorithmic behavior on deterministic tasks, and achieves lower cross-entropy on inherently nondeterministic tasks.</abstract>
      <url hash="21cf90f6">2020.conll-1.41</url>
    </paper>
    <paper id="42">
      <title>Generating Narrative Text in a Switching Dynamical System</title>
      <author><first>Noah</first><last>Weber</last></author>
      <author><first>Leena</first><last>Shekhar</last></author>
      <author><first>Heeyoung</first><last>Kwon</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <author><first>Nathanael</first><last>Chambers</last></author>
      <pages>520–530</pages>
      <abstract>Early work on narrative modeling used explicit plans and goals to generate stories, but the language generation itself was restricted and inflexible. Modern methods use language models for more robust generation, but often lack an explicit representation of the scaffolding and dynamics that guide a coherent narrative. This paper introduces a new model that integrates explicit narrative structure with neural language models, formalizing narrative modeling as a Switching Linear Dynamical System (SLDS). A SLDS is a dynamical system in which the latent dynamics of the system (i.e. how the state vector transforms over time) is controlled by top-level discrete switching variables. The switching variables represent narrative structure (e.g., sentiment or discourse states), while the latent state vector encodes information on the current state of the narrative. This probabilistic formulation allows us to control generation, and can be learned in a semi-supervised fashion using both labeled and unlabeled data. Additionally, we derive a Gibbs sampler for our model that can “fill in” arbitrary parts of the narrative, guided by the switching variables. Our filled-in (English language) narratives outperform several baselines on both automatic and human evaluations</abstract>
      <url hash="96ab0e53">2020.conll-1.42</url>
    </paper>
    <paper id="43">
      <title>What Are You Trying to Do? Semantic Typing of Event Processes</title>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Haoyu</first><last>Wang</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>531–542</pages>
      <abstract>This paper studies a new cognitively motivated semantic typing task,multi-axis event process typing, that, given anevent process, attempts to infer free-form typelabels describing (i) the type of action made bythe process and (ii) the type of object the pro-cess seeks to affect. This task is inspired bycomputational and cognitive studies of eventunderstanding, which suggest that understand-ing processes of events is often directed by rec-ognizing the goals, plans or intentions of theprotagonist(s). We develop a large dataset con-taining over 60k event processes, featuring ul-tra fine-grained typing on both the action andobject type axes with very large (10ˆ3∼10ˆ4)label vocabularies. We then propose a hybridlearning framework,P2GT, which addressesthe challenging typing problem with indirectsupervision from glosses1and a joint learning-to-rank framework. As our experiments indi-cate,P2GTsupports identifying the intent ofprocesses, as well as the fine semantic type ofthe affected object. It also demonstrates the ca-pability of handling few-shot cases, and stronggeneralizability on out-of-domain processes.</abstract>
      <url hash="a3844df0">2020.conll-1.43</url>
    </paper>
    <paper id="44">
      <title>A Corpus for Outbreak Detection of Diseases Prevalent in <fixed-case>L</fixed-case>atin <fixed-case>A</fixed-case>merica</title>
      <author><first>Antonella</first><last>Dellanzo</last></author>
      <author><first>Viviana</first><last>Cotik</last></author>
      <author><first>Jose</first><last>Ochoa-Luna</last></author>
      <pages>543–551</pages>
      <abstract>In this paper we present an annotated corpus which can be used for training and testing algorithms to automatically extract information about diseases outbreaks from news and health reports. We also propose initial approaches to extract information from it. The corpus has been constructed with two main tasks in mind. The first one, to extract entities about outbreaks such as disease, host, location among others. The second one, to retrieve relations among entities, for instance, in such geographic location fifteen cases of a given disease were reported. Overall, our goal is to offer resources and tools to perform an automated analysis so as to support early detection of disease outbreaks and therefore diminish their spreading.</abstract>
      <url hash="b05001d8">2020.conll-1.44</url>
    </paper>
    <paper id="45">
      <title>Are Pretrained Language Models Symbolic Reasoners over Knowledge?</title>
      <author><first>Nora</first><last>Kassner</last></author>
      <author><first>Benno</first><last>Krojer</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>552–564</pages>
      <abstract>How can pretrained language models (PLMs) learn factual knowledge from the training set? We investigate the two most important mechanisms: reasoning and memorization. Prior work has attempted to quantify the number of facts PLMs learn, but we present, using synthetic data, the first study that investigates the causal relation between facts present in training and facts learned by the PLM. For reasoning, we show that PLMs seem to learn to apply some symbolic reasoning rules correctly but struggle with others, including two-hop reasoning. Further analysis suggests that even the application of learned reasoning rules is flawed. For memorization, we identify schema conformity (facts systematically supported by other facts) and frequency as key factors for its success.</abstract>
      <url hash="af9c22c2">2020.conll-1.45</url>
    </paper>
    <paper id="46">
      <title>Understanding Linguistic Accommodation in Code-Switched Human-Machine Dialogues</title>
      <author><first>Tanmay</first><last>Parekh</last></author>
      <author><first>Emily</first><last>Ahn</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>565–577</pages>
      <abstract>Code-switching is a ubiquitous phenomenon in multilingual communities. Natural language technologies that wish to communicate like humans must therefore adaptively incorporate code-switching techniques when they are deployed in multilingual settings. To this end, we propose a Hindi-English human-machine dialogue system that elicits code-switching conversations in a controlled setting. It uses different code-switching agent strategies to understand how users respond and accommodate to the agent’s language choice. Through this system, we collect and release a new dataset CommonDost, comprising of 439 human-machine multilingual conversations. We adapt pre-defined metrics to discover linguistic accommodation from users to agents. Finally, we compare these dialogues with Spanish-English dialogues collected in a similar setting, and analyze the impact of linguistic and socio-cultural factors on code-switching patterns across the two language pairs.</abstract>
      <url hash="2731f8b2">2020.conll-1.46</url>
    </paper>
    <paper id="47">
      <title>Identifying robust markers of <fixed-case>P</fixed-case>arkinson’s disease in typing behaviour using a <fixed-case>CNN</fixed-case>-<fixed-case>LSTM</fixed-case> network</title>
      <author><first>Neil</first><last>Dhir</last></author>
      <author><first>Mathias</first><last>Edman</last></author>
      <author><first>Álvaro</first><last>Sanchez Ferro</last></author>
      <author><first>Tom</first><last>Stafford</last></author>
      <author><first>Colin</first><last>Bannard</last></author>
      <pages>578–595</pages>
      <abstract>There is urgent need for non-intrusive tests that can detect early signs of Parkinson’s disease (PD), a debilitating neurodegenerative disorder that affects motor control. Recent promising research has focused on disease markers evident in the fine-motor behaviour of typing. Most work to date has focused solely on the timing of keypresses without reference to the linguistic content. In this paper we argue that the identity of the key combinations being produced should impact how they are handled by people with PD, and provide evidence that natural language processing methods can thus be of help in identifying signs of disease. We test the performance of a bi-directional LSTM with convolutional features in distinguishing people with PD from age-matched controls typing in English and Spanish, both in clinics and online.</abstract>
      <url hash="3d20d767">2020.conll-1.47</url>
    </paper>
    <paper id="48">
      <title>An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference</title>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Zheng</first><last>Xin</last></author>
      <author><first>Xiaoan</first><last>Ding</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <pages>596–608</pages>
      <abstract>The prior work on natural language inference (NLI) debiasing mainly targets at one or few known biases while not necessarily making the models more robust. In this paper, we focus on the model-agnostic debiasing strategies and explore how to (or is it possible to) make the NLI models robust to multiple distinct adversarial attacks while keeping or even strengthening the models’ generalization power. We firstly benchmark prevailing neural NLI models including pretrained ones on various adversarial datasets. We then try to combat distinct known biases by modifying a mixture of experts (MoE) ensemble method and show that it’s nontrivial to mitigate multiple NLI biases at the same time, and that model-level ensemble method outperforms MoE ensemble method. We also perform data augmentation including text swap, word substitution and paraphrase and prove its efficiency in combating various (though not all) adversarial attacks at the same time. Finally, we investigate several methods to merge heterogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen NLI models.</abstract>
      <url hash="068fd0ce">2020.conll-1.48</url>
    </paper>
    <paper id="49">
      <title>Cloze Distillation: Improving Neural Language Models with Human Next-Word Prediction</title>
      <author><first>Tiwalayo</first><last>Eisape</last></author>
      <author><first>Noga</first><last>Zaslavsky</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>609–619</pages>
      <abstract>Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human next-word predictions. Here we evaluate several state-of-the-art language models for their match to human next-word predictions and to reading time behavior from eye movements. We then propose a novel method for distilling the linguistic information implicit in human linguistic predictions into pre-trained LMs: Cloze Distillation. We apply this method to a baseline neural LM and show potential improvement in reading time prediction and generalization to held-out human cloze data.</abstract>
      <url hash="3828b793">2020.conll-1.49</url>
    </paper>
    <paper id="50">
      <title>Disentangling dialects: a neural approach to <fixed-case>I</fixed-case>ndo-<fixed-case>A</fixed-case>ryan historical phonology and subgrouping</title>
      <author><first>Chundra</first><last>Cathcart</last></author>
      <author><first>Taraka</first><last>Rama</last></author>
      <pages>620–630</pages>
      <abstract>This paper seeks to uncover patterns of sound change across Indo-Aryan languages using an LSTM encoder-decoder architecture. We augment our models with embeddings represent-ing language ID, part of speech, and other features such as word embeddings. We find that a highly augmented model shows highest accuracy in predicting held-out forms, and investigate other properties of interest learned by our models’ representations. We outline extensions to this architecture that can better capture variation in Indo-Aryan sound change.</abstract>
      <url hash="b3c08983">2020.conll-1.50</url>
    </paper>
    <paper id="51">
      <title>A Dataset for Linguistic Understanding, Visual Evaluation, and Recognition of Sign Languages: The K-<fixed-case>RSL</fixed-case></title>
      <author><first>Alfarabi</first><last>Imashev</last></author>
      <author><first>Medet</first><last>Mukushev</last></author>
      <author><first>Vadim</first><last>Kimmelman</last></author>
      <author><first>Anara</first><last>Sandygulova</last></author>
      <pages>631–640</pages>
      <abstract>The paper presents the first dataset that aims to serve interdisciplinary purposes for the utility of computer vision community and sign language linguistics. To date, a majority of Sign Language Recognition (SLR) approaches focus on recognising sign language as a manual gesture recognition problem. However, signers use other articulators: facial expressions, head and body position and movement to convey linguistic information. Given the important role of non-manual markers, this paper proposes a dataset and presents a use case to stress the importance of including non-manual features to improve the recognition accuracy of signs. To the best of our knowledge no prior publicly available dataset exists that explicitly focuses on non-manual components responsible for the grammar of sign languages. To this end, the proposed dataset contains 28250 videos of signs of high resolution and quality, with annotation of manual and non-manual components. We conducted a series of evaluations in order to investigate whether non-manual components would improve signs’ recognition accuracy. We release the dataset to encourage SLR researchers and help advance current progress in this area toward real-time sign language interpretation. Our dataset will be made publicly available at https://krslproject.github.io/krsl-corpus</abstract>
      <url hash="0e44ede5">2020.conll-1.51</url>
    </paper>
    <paper id="52">
      <title>From Dataset Recycling to Multi-Property Extraction and Beyond</title>
      <author><first>Tomasz</first><last>Dwojak</last></author>
      <author><first>Michał</first><last>Pietruszka</last></author>
      <author><first>Łukasz</first><last>Borchmann</last></author>
      <author><first>Jakub</first><last>Chłędowski</last></author>
      <author><first>Filip</first><last>Graliński</last></author>
      <pages>641–651</pages>
      <abstract>This paper investigates various Transformer architectures on the WikiReading Information Extraction and Machine Reading Comprehension dataset. The proposed dual-source model outperforms the current state-of-the-art by a large margin. Next, we introduce WikiReading Recycled - a newly developed public dataset, and the task of multiple-property extraction. It uses the same data as WikiReading but does not inherit its predecessor’s identified disadvantages. In addition, we provide a human-annotated test set with diagnostic subsets for a detailed analysis of model performance.</abstract>
      <url hash="ed5e0dcc">2020.conll-1.52</url>
    </paper>
    <paper id="53">
      <title>How well does surprisal explain N400 amplitude under different experimental conditions?</title>
      <author><first>James</first><last>Michaelov</last></author>
      <author><first>Benjamin</first><last>Bergen</last></author>
      <pages>652–663</pages>
      <abstract>We investigate the extent to which word surprisal can be used to predict a neural measure of human language processing difficulty—the N400. To do this, we use recurrent neural networks to calculate the surprisal of stimuli from previously published neurolinguistic studies of the N400. We find that surprisal can predict N400 amplitude in a wide range of cases, and the cases where it cannot do so provide valuable insight into the neurocognitive processes underlying the response.</abstract>
      <url hash="1149f756">2020.conll-1.53</url>
    </paper>
  </volume>
  <volume id="shared" ingest-date="2020-11-05">
    <meta>
      <booktitle>Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing</booktitle>
      <editor><first>Stephan</first><last>Oepen</last></editor>
      <editor><first>Omri</first><last>Abend</last></editor>
      <editor><first>Lasha</first><last>Abzianidze</last></editor>
      <editor><first>Johan</first><last>Bos</last></editor>
      <editor><first>Jan</first><last>Hajič</last></editor>
      <editor><first>Daniel</first><last>Hershcovich</last></editor>
      <editor><first>Bin</first><last>Li</last></editor>
      <editor><first>Tim</first><last>O'Gorman</last></editor>
      <editor><first>Nianwen</first><last>Xue</last></editor>
      <editor><first>Daniel</first><last>Zeman</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="3e727ac8">2020.conll-shared.0</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>MRP</fixed-case> 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing</title>
      <author><first>Stephan</first><last>Oepen</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <author><first>Lasha</first><last>Abzianidze</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <author><first>Jan</first><last>Hajic</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Bin</first><last>Li</last></author>
      <author><first>Tim</first><last>O’Gorman</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <pages>1–22</pages>
      <abstract>The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four of these representation frameworks, additional training and evaluation data was provided for one additional language per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu</abstract>
      <url hash="f8cd7578">2020.conll-shared.1</url>
      <attachment type="Attachment" hash="c1532ed4">2020.conll-shared.1.Attachment.pdf</attachment>
    </paper>
    <paper id="2">
      <title><fixed-case>DRS</fixed-case> at <fixed-case>MRP</fixed-case> 2020: Dressing up Discourse Representation Structures as Graphs</title>
      <author><first>Lasha</first><last>Abzianidze</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <pages>23–32</pages>
      <abstract>Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpretation, which is usually depicted as nested boxes. In contrast, a directed labeled graph is a common data structure used to encode semantics of natural language texts. The paper describes the procedure of dressing up DRSs as directed labeled graphs to include DRT as a new framework in the 2020 shared task on Cross-Framework and Cross-Lingual Meaning Representation Parsing. Since one of the goals of the shared task is to encourage unified models for several semantic graph frameworks, the conversion procedure was biased towards making the DRT graph framework somewhat similar to other graph-based meaning representation frameworks.</abstract>
      <url hash="d6b01e77">2020.conll-shared.2</url>
    </paper>
    <paper id="3">
      <title><fixed-case>FGD</fixed-case> at <fixed-case>MRP</fixed-case> 2020: <fixed-case>P</fixed-case>rague Tectogrammatical Graphs</title>
      <author><first>Daniel</first><last>Zeman</last></author>
      <author><first>Jan</first><last>Hajic</last></author>
      <pages>33–39</pages>
      <abstract>Prague Tectogrammatical Graphs (PTG) is a meaning representation framework that originates in the tectogrammatical layer of the Prague Dependency Treebank (PDT) and is theoretically founded in Functional Generative Description of language (FGD). PTG in its present form has been prepared for the CoNLL 2020 shared task on Cross-Framework Meaning Representation Parsing (MRP). It is generated automatically from the Prague treebanks and stored in the JSON-based MRP graph interchange format. The conversion is partially lossy; in this paper we describe what part of annotation was included and how it is represented in PTG.</abstract>
      <url hash="9df5ca9e">2020.conll-shared.3</url>
    </paper>
    <paper id="4">
      <title>Hitachi at <fixed-case>MRP</fixed-case> 2020: Text-to-Graph-Notation Transducer</title>
      <author><first>Hiroaki</first><last>Ozaki</last></author>
      <author><first>Gaku</first><last>Morio</last></author>
      <author><first>Yuta</first><last>Koreeda</last></author>
      <author><first>Terufumi</first><last>Morishita</last></author>
      <author><first>Toshinori</first><last>Miyoshi</last></author>
      <pages>40–52</pages>
      <abstract>This paper presents our proposed parser for the shared task on Meaning Representation Parsing (MRP 2020) at CoNLL, where participant systems were required to parse five types of graphs in different languages. We propose to unify these tasks as a text-to-graph-notation transduction in which we convert an input text into a graph notation. To this end, we designed a novel Plain Graph Notation (PGN) that handles various graphs universally. Then, our parser predicts a PGN-based sequence by leveraging Transformers and biaffine attentions. Notably, our parser can handle any PGN-formatted graphs with fewer framework-specific modifications. As a result, ensemble versions of the parser tied for 1st place in both cross-framework and cross-lingual tracks.</abstract>
      <url hash="2a4dc896">2020.conll-shared.4</url>
    </paper>
    <paper id="5">
      <title><fixed-case>ÚFAL</fixed-case> at <fixed-case>MRP</fixed-case> 2020: Permutation-invariant Semantic Parsing in <fixed-case>PERIN</fixed-case></title>
      <author><first>David</first><last>Samuel</last></author>
      <author><first>Milan</first><last>Straka</last></author>
      <pages>53–64</pages>
      <abstract>We present PERIN, a novel permutation-invariant approach to sentence-to-graph semantic parsing. PERIN is a versatile, cross-framework and language independent architecture for universal modeling of semantic structures. Our system participated in the CoNLL 2020 shared task, Cross-Framework Meaning Representation Parsing (MRP 2020), where it was evaluated on five different frameworks (AMR, DRG, EDS, PTG and UCCA) across four languages. PERIN was one of the winners of the shared task. The source code and pretrained models are available at http://www.github.com/ufal/perin.</abstract>
      <url hash="1462f746">2020.conll-shared.5</url>
    </paper>
    <paper id="6">
      <title><fixed-case>HIT</fixed-case>-<fixed-case>SCIR</fixed-case> at <fixed-case>MRP</fixed-case> 2020: Transition-based Parser and Iterative Inference Parser</title>
      <author><first>Longxu</first><last>Dou</last></author>
      <author><first>Yunlong</first><last>Feng</last></author>
      <author><first>Yuqiu</first><last>Ji</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>65–72</pages>
      <abstract>This paper describes our submission system (HIT-SCIR) for the CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning Representation Parsing. The task includes five frameworks for graph-based meaning representations, i.e., UCCA, EDS, PTG, AMR, and DRG. Our solution consists of two sub-systems: transition-based parser for Flavor (1) frameworks (UCCA, EDS, PTG) and iterative inference parser for Flavor (2) frameworks (DRG, AMR). In the final evaluation, our system is ranked 3rd among the seven team both in Cross-Framework Track and Cross-Lingual Track, with the macro-averaged MRP F1 score of 0.81/0.69.</abstract>
      <url hash="2da692ee">2020.conll-shared.6</url>
    </paper>
    <paper id="7">
      <title><fixed-case>HUJI</fixed-case>-<fixed-case>KU</fixed-case> at <fixed-case>MRP</fixed-case> 2020: Two Transition-based Neural Parsers</title>
      <author><first>Ofir</first><last>Arviv</last></author>
      <author><first>Ruixiang</first><last>Cui</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <pages>73–82</pages>
      <abstract>This paper describes the HUJI-KU system submission to the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2020 Conference for Computational Language Learning (CoNLL), employing TUPA and the HIT-SCIR parser, which were, respectively, the baseline system and winning system in the 2019 MRP shared task. Both are transition-based parsers using BERT contextualized embeddings. We generalized TUPA to support the newly-added MRP frameworks and languages, and experimented with multitask learning with the HIT-SCIR parser. We reached 4th place in both the crossframework and cross-lingual tracks.</abstract>
      <url hash="6a26281e">2020.conll-shared.7</url>
    </paper>
    <paper id="8">
      <title><fixed-case>JBNU</fixed-case> at <fixed-case>MRP</fixed-case> 2020: <fixed-case>AMR</fixed-case> Parsing Using a Joint State Model for Graph-Sequence Iterative Inference</title>
      <author><first>Seung-Hoon</first><last>Na</last></author>
      <author><first>Jinwoo</first><last>Min</last></author>
      <pages>83–87</pages>
      <abstract>This paper describes the Jeonbuk National University (JBNU) system for the 2020 shared task on Cross-Framework Meaning Representation Parsing at the Conference on Computational Natural Language Learning. Among the five frameworks, we address only the abstract meaning representation framework and propose a joint state model for the graph-sequence iterative inference of (Cai and Lam, 2020) for a simplified graph-sequence inference. In our joint state model, we update only a single joint state vector during the graph-sequence inference process instead of keeping the dual state vectors, and all other components are exactly the same as in (Cai and Lam, 2020).</abstract>
      <url hash="fda4850e">2020.conll-shared.8</url>
    </paper>
  </volume>
</collection>
