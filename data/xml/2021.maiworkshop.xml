<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.maiworkshop">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Multimodal Artificial Intelligence</booktitle>
      <editor><first>Amir</first><last>Zadeh</last></editor>
      <editor><first>Louis-Philippe</first><last>Morency</last></editor>
      <editor><first>Paul Pu</first><last>Liang</last></editor>
      <editor><first>Candace</first><last>Ross</last></editor>
      <editor><first>Ruslan</first><last>Salakhutdinov</last></editor>
      <editor><first>Soujanya</first><last>Poria</last></editor>
      <editor><first>Erik</first><last>Cambria</last></editor>
      <editor><first>Kelly</first><last>Shi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2021</year>
      <url hash="f2f0aab4">2021.maiworkshop-1</url>
      <venue>maiworkshop</venue>
    </meta>
    <frontmatter>
      <url hash="46cec279">2021.maiworkshop-1.0</url>
      <bibkey>maiworkshop-2021-multimodal</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Multimodal Weighted Fusion of Transformers for Movie Genre Classification</title>
      <author><first>Isaac</first><last>Rodríguez Bribiesca</last></author>
      <author><first>Adrián Pastor</first><last>López Monroy</last></author>
      <author><first>Manuel</first><last>Montes-y-Gómez</last></author>
      <pages>1–5</pages>
      <abstract>The Multimodal Transformer showed to be a competitive model for multimodal tasks involving textual, visual and audio signals. However, as more modalities are involved, its late fusion by concatenation starts to have a negative impact on the model’s performance. Besides, interpreting model’s predictions becomes difficult, as one would have to look at the different attention activation matrices. In order to overcome these shortcomings, we propose to perform late fusion by adding a GMU module, which effectively allows the model to weight modalities at instance level, improving its performance while providing a better interpretabilty mechanism. In the experiments, we compare our proposed model (MulT-GMU) against the original implementation (MulT-Concat) and a SOTA model tested in a movie genre classification dataset. Our approach, MulT-GMU, outperforms both, MulT-Concat and previous SOTA model.</abstract>
      <url hash="a8109f04">2021.maiworkshop-1.1</url>
      <doi>10.18653/v1/2021.maiworkshop-1.1</doi>
      <bibkey>rodriguez-bribiesca-etal-2021-multimodal</bibkey>
    </paper>
    <paper id="2">
      <title>On Randomized Classification Layers and Their Implications in Natural Language Generation</title>
      <author><first>Gal-Lev</first><last>Shalev</last></author>
      <author><first>Gabi</first><last>Shalev</last></author>
      <author><first>Joseph</first><last>Keshet</last></author>
      <pages>6–11</pages>
      <abstract>In natural language generation tasks, a neural language model is used for generating a sequence of words forming a sentence. The topmost weight matrix of the language model, known as the classification layer, can be viewed as a set of vectors, each representing a target word from the target dictionary. The target word vectors, along with the rest of the model parameters, are learned and updated during training. In this paper, we analyze the properties encoded in the target vectors and question the necessity of learning these vectors. We suggest to randomly draw the target vectors and set them as fixed so that no weights updates are being made during training. We show that by excluding the vectors from the optimization, the number of parameters drastically decreases with a marginal effect on the performance. We demonstrate the effectiveness of our method in image-captioning and machine-translation.</abstract>
      <url hash="a72f37ea">2021.maiworkshop-1.2</url>
      <doi>10.18653/v1/2021.maiworkshop-1.2</doi>
      <bibkey>shalev-etal-2021-randomized</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>COIN</fixed-case>: Conversational Interactive Networks for Emotion Recognition in Conversation</title>
      <author><first>Haidong</first><last>Zhang</last></author>
      <author><first>Yekun</first><last>Chai</last></author>
      <pages>12–18</pages>
      <abstract>Emotion recognition in conversation has received considerable attention recently because of its practical industrial applications. Existing methods tend to overlook the immediate mutual interaction between different speakers in the speaker-utterance level, or apply single speaker-agnostic RNN for utterances from different speakers. We propose COIN, a conversational interactive model to mitigate this problem by applying state mutual interaction within history contexts. In addition, we introduce a stacked global interaction module to capture the contextual and inter-dependency representation in a hierarchical manner. To improve the robustness and generalization during training, we generate adversarial examples by applying the minor perturbations on multimodal feature inputs, unveiling the benefits of adversarial examples for emotion detection. The proposed model empirically achieves the current state-of-the-art results on the IEMOCAP benchmark dataset.</abstract>
      <url hash="9780facb">2021.maiworkshop-1.3</url>
      <doi>10.18653/v1/2021.maiworkshop-1.3</doi>
      <bibkey>zhang-chai-2021-coin</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="4">
      <title>A First Look: Towards Explainable <fixed-case>T</fixed-case>ext<fixed-case>VQA</fixed-case> Models via Visual and Textual Explanations</title>
      <author><first>Varun</first><last>Nagaraj Rao</last></author>
      <author><first>Xingjian</first><last>Zhen</last></author>
      <author><first>Karen</first><last>Hovsepian</last></author>
      <author><first>Mingwei</first><last>Shen</last></author>
      <pages>19–29</pages>
      <abstract>Explainable deep learning models are advantageous in many situations. Prior work mostly provide unimodal explanations through post-hoc approaches not part of the original system design. Explanation mechanisms also ignore useful textual information present in images. In this paper, we propose MTXNet, an end-to-end trainable multimodal architecture to generate multimodal explanations, which focuses on the text in the image. We curate a novel dataset TextVQA-X, containing ground truth visual and multi-reference textual explanations that can be leveraged during both training and evaluation. We then quantitatively show that training with multimodal explanations complements model performance and surpasses unimodal baselines by up to 7% in CIDEr scores and 2% in IoU. More importantly, we demonstrate that the multimodal explanations are consistent with human interpretations, help justify the models’ decision, and provide useful insights to help diagnose an incorrect prediction. Finally, we describe a real-world e-commerce application for using the generated multimodal explanations.</abstract>
      <url hash="349c5b08">2021.maiworkshop-1.4</url>
      <doi>10.18653/v1/2021.maiworkshop-1.4</doi>
      <bibkey>nagaraj-rao-etal-2021-first</bibkey>
      <pwccode url="https://github.com/amzn/explainable-text-vqa" additional="false">amzn/explainable-text-vqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/textvqa">TextVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vqa-hat">VQA-HAT</pwcdataset>
    </paper>
    <paper id="5">
      <title>Multi Task Learning based Framework for Multimodal Classification</title>
      <author><first>Danting</first><last>Zeng</last></author>
      <pages>30–35</pages>
      <abstract>Large-scale multi-modal classification aim to distinguish between different multi-modal data, and it has drawn dramatically attentions since last decade. In this paper, we propose a multi-task learning-based framework for the multimodal classification task, which consists of two branches: multi-modal autoencoder branch and attention-based multi-modal modeling branch. Multi-modal autoencoder can receive multi-modal features and obtain the interactive information which called multi-modal encoder feature, and use this feature to reconstitute all the input data. Besides, multi-modal encoder feature can be used to enrich the raw dataset, and improve the performance of downstream tasks (such as classification task). As for attention-based multimodal modeling branch, we first employ attention mechanism to make the model focused on important features, then we use the multi-modal encoder feature to enrich the input information, achieve a better performance. We conduct extensive experiments on different dataset, the results demonstrate the effectiveness of proposed framework.</abstract>
      <url hash="e12103f7">2021.maiworkshop-1.5</url>
      <doi>10.18653/v1/2021.maiworkshop-1.5</doi>
      <bibkey>zeng-2021-multi</bibkey>
    </paper>
    <paper id="6">
      <title>Validity-Based Sampling and Smoothing Methods for Multiple Reference Image Captioning</title>
      <author><first>Shunta</first><last>Nagasawa</last></author>
      <author><first>Yotaro</first><last>Watanabe</last></author>
      <author><first>Hitoshi</first><last>Iyatomi</last></author>
      <pages>36–41</pages>
      <abstract>In image captioning, multiple captions are often provided as ground truths, since a valid caption is not always uniquely determined. Conventional methods randomly select a single caption and treat it as correct, but there have been few effective training methods that utilize multiple given captions. In this paper, we proposed two training technique for making effective use of multiple reference captions: 1) validity-based caption sampling (VBCS), which prioritizes the use of captions that are estimated to be highly valid during training, and 2) weighted caption smoothing (WCS), which applies smoothing only to the relevant words the reference caption to reflect multiple reference captions simultaneously. Experiments show that our proposed methods improve CIDEr by 2.6 points and BLEU4 by 0.9 points from baseline on the MSCOCO dataset.</abstract>
      <url hash="e727cd9f">2021.maiworkshop-1.6</url>
      <doi>10.18653/v1/2021.maiworkshop-1.6</doi>
      <bibkey>nagasawa-etal-2021-validity</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="7">
      <title>Modality-specific Distillation</title>
      <author><first>Woojeong</first><last>Jin</last></author>
      <author><first>Maziar</first><last>Sanjabi</last></author>
      <author><first>Shaoliang</first><last>Nie</last></author>
      <author><first>Liang</first><last>Tan</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Hamed</first><last>Firooz</last></author>
      <pages>42–53</pages>
      <abstract>Large neural networks are impractical to deploy on mobile devices due to their heavy computational cost and slow inference. Knowledge distillation (KD) is a technique to reduce the model size while retaining performance by transferring knowledge from a large “teacher” model to a smaller “student” model. However, KD on multimodal datasets such as vision-language datasets is relatively unexplored and digesting such multimodal information is challenging since different modalities present different types of information. In this paper, we propose modality-specific distillation (MSD) to effectively transfer knowledge from a teacher on multimodal datasets. Existing KD approaches can be applied to multimodal setup, but a student doesn’t have access to modality-specific predictions. Our idea aims at mimicking a teacher’s modality-specific predictions by introducing an auxiliary loss term for each modality. Because each modality has different importance for predictions, we also propose weighting approaches for the auxiliary losses; a meta-learning approach to learn the optimal weights on these loss terms. In our experiments, we demonstrate the effectiveness of our MSD and the weighting scheme and show that it achieves better performance than KD.</abstract>
      <url hash="89505c22">2021.maiworkshop-1.7</url>
      <doi>10.18653/v1/2021.maiworkshop-1.7</doi>
      <bibkey>jin-etal-2021-modality</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hateful-memes">Hateful Memes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli-ve">SNLI-VE</pwcdataset>
    </paper>
    <paper id="8">
      <title>Cold Start Problem For Automated Live Video Comments</title>
      <author><first>Hao</first><last>Wu</last></author>
      <author><first>François</first><last>Pitie</last></author>
      <author><first>Gareth</first><last>Jones</last></author>
      <pages>54–62</pages>
      <abstract>Live video comments, or ”danmu”, are an emerging feature on Asian online video platforms. Danmu are time-synchronous comments that are overlaid on a video playback. These comments uniquely enrich the experience and engagement of their users. These comments have become a determining factor in the popularity of the videos. Similar to the ”cold start problem” in recommender systems, a video will only start to attract attention when sufficient danmu comments have been posted on it. We study this video cold start problem and examine how new comments can be generated automatically on less-commented videos. We propose to predict the danmu comments by exploiting a multi-modal combination of the video visual content, subtitles, audio signals, and any surrounding comments (when they exist). Our method fuses these multi-modalities in a transformer network which is then trained for different comment density scenarios. We evaluate our proposed system through both a retrieval based evaluation method, as well as human judgement. Results show that our proposed system improves significantly over state-of-the-art methods.</abstract>
      <url hash="a924bbb6">2021.maiworkshop-1.8</url>
      <doi>10.18653/v1/2021.maiworkshop-1.8</doi>
      <bibkey>wu-etal-2021-cold</bibkey>
    </paper>
    <paper id="9">
      <title>¡<fixed-case>Q</fixed-case>ué maravilla! Multimodal Sarcasm Detection in <fixed-case>S</fixed-case>panish: a Dataset and a Baseline</title>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <pages>63–68</pages>
      <abstract>We construct the first ever multimodal sarcasm dataset for Spanish. The audiovisual dataset consists of sarcasm annotated text that is aligned with video and audio. The dataset represents two varieties of Spanish, a Latin American variety and a Peninsular Spanish variety, which ensures a wider dialectal coverage for this global language. We present several models for sarcasm detection that will serve as baselines in the future research. Our results show that results with text only (89%) are worse than when combining text with audio (91.9%). Finally, the best results are obtained when combining all the modalities: text, audio and video (93.1%). Our dataset will be published on Zenodo with access granted by request.</abstract>
      <url hash="645e8ed2">2021.maiworkshop-1.9</url>
      <doi>10.18653/v1/2021.maiworkshop-1.9</doi>
      <bibkey>alnajjar-hamalainen-2021-que</bibkey>
    </paper>
    <paper id="10">
      <title>A Package for Learning on Tabular and Text Data with Transformers</title>
      <author><first>Ken</first><last>Gu</last></author>
      <author><first>Akshay</first><last>Budhkar</last></author>
      <pages>69–73</pages>
      <abstract>Recent progress in natural language processing has led to Transformer architectures becoming the predominant model used for natural language tasks. However, in many real- world datasets, additional modalities are included which the Transformer does not directly leverage. We present Multimodal- Toolkit, an open-source Python package to incorporate text and tabular (categorical and numerical) data with Transformers for downstream applications. Our toolkit integrates well with Hugging Face’s existing API such as tokenization and the model hub which allows easy download of different pre-trained models.</abstract>
      <url hash="2665785e">2021.maiworkshop-1.10</url>
      <doi>10.18653/v1/2021.maiworkshop-1.10</doi>
      <bibkey>gu-budhkar-2021-package</bibkey>
    </paper>
    <paper id="11">
      <title>Semantic Aligned Multi-modal Transformer for Vision-<fixed-case>L</fixed-case>anguage<fixed-case>U</fixed-case>nderstanding: A Preliminary Study on Visual <fixed-case>QA</fixed-case></title>
      <author><first>Han</first><last>Ding</last></author>
      <author><first>Li Erran</first><last>Li</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <author><first>Yi</first><last>Xu</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <author><first>Zheng</first><last>Du</last></author>
      <author><first>Belinda</first><last>Zeng</last></author>
      <pages>74–78</pages>
      <abstract>Recent vision-language understanding approaches adopt a multi-modal transformer pre-training and finetuning paradigm. Prior work learns representations of text tokens and visual features with cross-attention mechanisms and captures the alignment solely based on indirect signals. In this work, we propose to enhance the alignment mechanism by incorporating image scene graph structures as the bridge between the two modalities, and learning with new contrastive objectives. In our preliminary study on the challenging compositional visual question answering task, we show the proposed approach achieves improved results, demonstrating potentials to enhance vision-language understanding.</abstract>
      <url hash="592411fd">2021.maiworkshop-1.11</url>
      <doi>10.18653/v1/2021.maiworkshop-1.11</doi>
      <bibkey>ding-etal-2021-semantic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="12">
      <title><fixed-case>G</fixed-case>ragh<fixed-case>VQA</fixed-case>: Language-Guided Graph Neural Networks for Graph-based Visual Question Answering</title>
      <author><first>Weixin</first><last>Liang</last></author>
      <author><first>Yanhao</first><last>Jiang</last></author>
      <author><first>Zixuan</first><last>Liu</last></author>
      <pages>79–86</pages>
      <abstract>Images are more than a collection of objects or attributes — they represent a web of relationships among interconnected objects. Scene Graph has emerged as a new modality as a structured graphical representation of images. Scene Graph encodes objects as nodes connected via pairwise relations as edges. To support question answering on scene graphs, we propose GraphVQA, a language-guided graph neural network framework that translates and executes a natural language question as multiple iterations of message passing among graph nodes. We explore the design space of GraphVQA framework, and discuss the trade-off of different design choices. Our experiments on GQA dataset show that GraphVQA outperforms the state-of-the-art accuracy by a large margin (88.43% vs. 94.78%).</abstract>
      <url hash="79fb1c7f">2021.maiworkshop-1.12</url>
      <attachment type="OptionalSupplementaryMaterial" hash="255e2868">2021.maiworkshop-1.12.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.maiworkshop-1.12</doi>
      <bibkey>liang-etal-2021-graghvqa</bibkey>
      <pwccode url="https://github.com/codexxxl/GraphVQA" additional="false">codexxxl/GraphVQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
    </paper>
    <paper id="13">
      <title>Learning to Select Question-Relevant Relations for Visual Question Answering</title>
      <author><first>Jaewoong</first><last>Lee</last></author>
      <author><first>Heejoon</first><last>Lee</last></author>
      <author><first>Hwanhee</first><last>Lee</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>87–96</pages>
      <abstract>Previous existing visual question answering (VQA) systems commonly use graph neural networks(GNNs) to extract visual relationships such as semantic relations or spatial relations. However, studies that use GNNs typically ignore the importance of each relation and simply concatenate outputs from multiple relation encoders. In this paper, we propose a novel layer architecture that fuses multiple visual relations through an attention mechanism to address this issue. Specifically, we develop a model that uses question embedding and joint embedding of the encoders to obtain dynamic attention weights with regard to the type of questions. Using the learnable attention weights, the proposed model can efficiently use the necessary visual relation features for a given question. Experimental results on the VQA 2.0 dataset demonstrate that the proposed model outperforms existing graph attention network-based architectures. Additionally, we visualize the attention weight and show that the proposed model assigns a higher weight to relations that are more relevant to the question.</abstract>
      <url hash="e2e33677">2021.maiworkshop-1.13</url>
      <doi>10.18653/v1/2021.maiworkshop-1.13</doi>
      <bibkey>lee-etal-2021-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
  </volume>
</collection>
