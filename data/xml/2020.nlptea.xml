<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.nlptea">
  <volume id="1" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
      <editor><first>Erhong</first><last>YANG</last></editor>
      <editor><first>Endong</first><last>XUN</last></editor>
      <editor><first>Baolin</first><last>ZHANG</last></editor>
      <editor><first>Gaoqi</first><last>RAO</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
      <venue>nlptea</venue>
    </meta>
    <frontmatter>
      <url hash="b5ddfc36">2020.nlptea-1.0</url>
      <bibkey>nlp-tea-2020-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Non-Autoregressive Grammatical Error Correction Toward a Writing Support System</title>
      <author><first>Hiroki</first><last>Homma</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>1–10</pages>
      <abstract>There are several problems in applying grammatical error correction (GEC) to a writing support system. One of them is the handling of sentences in the middle of the input. Till date, the performance of GEC for incomplete sentences is not well-known. Hence, we analyze the performance of each model for incomplete sentences. Another problem is the correction speed. When the speed is slow, the usability of the system is limited, and the user experience is degraded. Therefore, in this study, we also focus on the non-autoregressive (NAR) model, which is a widely studied fast decoding method. We perform GEC in Japanese with traditional autoregressive and recent NAR models and analyze their accuracy and speed.</abstract>
      <url hash="430af987">2020.nlptea-1.1</url>
      <bibkey>homma-komachi-2020-non</bibkey>
    </paper>
    <paper id="2">
      <title>Arabisc: Context-Sensitive Neural Spelling Checker</title>
      <author><first>Yasmin</first><last>Moslem</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>11–19</pages>
      <abstract>Traditional statistical approaches to spelling correction usually consist of two consecutive processes — error detection and correction — and they are generally computationally intensive. Current state-of-the-art neural spelling correction models usually attempt to correct spelling errors directly over an entire sentence, which, as a consequence, lacks control of the process, e.g. they are prone to overcorrection. In recent years, recurrent neural networks (RNNs), in particular long short-term memory (LSTM) hidden units, have proven increasingly popular and powerful models for many natural language processing (NLP) problems. Accordingly, we made use of a bidirectional LSTM language model (LM) for our context-sensitive spelling detection and correction model which is shown to have much control over the correction process. While the use of LMs for spelling checking and correction is not new to this line of NLP research, our proposed approach makes better use of the rich neighbouring context, not only from before the word to be corrected, but also after it, via a dual-input deep LSTM network. Although in theory our proposed approach can be applied to any language, we carried out our experiments on Arabic, which we believe adds additional value given the fact that there are limited linguistic resources readily available in Arabic in comparison to many languages. Our experimental results demonstrate that the proposed methods are effective in both improving the quality of correction suggestions and minimising overcorrection.</abstract>
      <url hash="4c42ebde">2020.nlptea-1.2</url>
      <bibkey>moslem-etal-2020-arabisc</bibkey>
      <pwccode url="https://github.com/ymoslem/Arabisc" additional="false">ymoslem/Arabisc</pwccode>
    </paper>
    <paper id="3">
      <title><fixed-case>LXPER</fixed-case> Index 2.0: Improving Text Readability Assessment Model for <fixed-case>L</fixed-case>2 <fixed-case>E</fixed-case>nglish Students in <fixed-case>K</fixed-case>orea</title>
      <author><first>Bruce W.</first><last>Lee</last></author>
      <author><first>Jason</first><last>Lee</last></author>
      <pages>20–24</pages>
      <abstract>Developing a text readability assessment model specifically for texts in a foreign English Language Training (ELT) curriculum has never had much attention in the field of Natural Language Processing. Hence, most developed models show extremely low accuracy for L2 English texts, up to the point where not many even serve as a fair comparison. In this paper, we investigate a text readability assessment model for L2 English learners in Korea. In accordance, we improve and expand the Text Corpus of the Korean ELT curriculum (CoKEC-text). Each text is labeled with its target grade level. We train our model with CoKEC-text and significantly improve the accuracy of readability assessment for texts in the Korean ELT curriculum.</abstract>
      <url hash="d04bb779">2020.nlptea-1.3</url>
      <bibkey>lee-lee-2020-lxper</bibkey>
    </paper>
    <paper id="4">
      <title>Overview of <fixed-case>NLPTEA</fixed-case>-2020 Shared Task for <fixed-case>C</fixed-case>hinese Grammatical Error Diagnosis</title>
      <author><first>Gaoqi</first><last>Rao</last></author>
      <author><first>Erhong</first><last>Yang</last></author>
      <author><first>Baolin</first><last>Zhang</last></author>
      <pages>25–35</pages>
      <abstract>This paper presents the NLPTEA 2020 shared task for Chinese Grammatical Error Diagnosis (CGED) which seeks to identify grammatical error types, their range of occurrence and recommended corrections within sentences written by learners of Chinese as a foreign language. We describe the task definition, data preparation, performance metrics, and evaluation results. Of the 30 teams registered for this shared task, 17 teams developed the system and submitted a total of 43 runs. System performances achieved a significant progress, reaching F1 of 91% in detection level, 40% in position level and 28% in correction level. All data sets with gold standards and scoring scripts are made publicly available to researchers.</abstract>
      <url hash="f767fb1d">2020.nlptea-1.4</url>
      <bibkey>rao-etal-2020-overview</bibkey>
    </paper>
    <paper id="5">
      <title>Combining <fixed-case>R</fixed-case>es<fixed-case>N</fixed-case>et and Transformer for <fixed-case>C</fixed-case>hinese Grammatical Error Diagnosis</title>
      <author><first>Shaolei</first><last>Wang</last></author>
      <author><first>Baoxin</first><last>Wang</last></author>
      <author><first>Jiefu</first><last>Gong</last></author>
      <author><first>Zhongyuan</first><last>Wang</last></author>
      <author><first>Xiao</first><last>Hu</last></author>
      <author><first>Xingyi</first><last>Duan</last></author>
      <author><first>Zizhuo</first><last>Shen</last></author>
      <author><first>Gang</first><last>Yue</last></author>
      <author><first>Ruiji</first><last>Fu</last></author>
      <author><first>Dayong</first><last>Wu</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Shijin</first><last>Wang</last></author>
      <author><first>Guoping</first><last>Hu</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>36–43</pages>
      <abstract>Grammatical error diagnosis is an important task in natural language processing. This paper introduces our system at NLPTEA-2020 Task: Chinese Grammatical Error Diagnosis (CGED). CGED aims to diagnose four types of grammatical errors which are missing words (M), redundant words (R), bad word selection (S) and disordered words (W). Our system is built on the model of multi-layer bidirectional transformer encoder and ResNet is integrated into the encoder to improve the performance. We also explore two ensemble strategies including weighted averaging and stepwise ensemble selection from libraries of models to improve the performance of single model. In official evaluation, our system obtains the highest F1 scores at identification level and position level. We also recommend error corrections for specific error types and achieve the second highest F1 score at correction level.</abstract>
      <url hash="7407b589">2020.nlptea-1.5</url>
      <bibkey>wang-etal-2020-combining-resnet</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>C</fixed-case>hinese Grammatical Error Diagnosis with Graph Convolution Network and Multi-task Learning</title>
      <author><first>Yikang</first><last>Luo</last></author>
      <author><first>Zuyi</first><last>Bao</last></author>
      <author><first>Chen</first><last>Li</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <pages>44–48</pages>
      <abstract>This paper describes our participating system on the Chinese Grammatical Error Diagnosis (CGED) 2020 shared task. For the detection subtask, we propose two BERT-based approaches 1) with syntactic dependency trees enhancing the model performance and 2) under the multi-task learning framework to combine the sequence labeling and the sequence-to-sequence (seq2seq) models. For the correction subtask, we utilize the masked language model, the seq2seq model and the spelling check model to generate corrections based on the detection results. Finally, our system achieves the highest recall rate on the top-3 correction and the second best F1 score on identification level and position level.</abstract>
      <url hash="5ae77165">2020.nlptea-1.6</url>
      <bibkey>luo-etal-2020-chinese</bibkey>
    </paper>
    <paper id="7">
      <title>Integrating <fixed-case>BERT</fixed-case> and Score-based Feature Gates for <fixed-case>C</fixed-case>hinese Grammatical Error Diagnosis</title>
      <author><first>Yongchang</first><last>Cao</last></author>
      <author><first>Liang</first><last>He</last></author>
      <author><first>Robert</first><last>Ridley</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <pages>49–56</pages>
      <abstract>This paper describes our proposed model for the Chinese Grammatical Error Diagnosis (CGED) task in NLPTEA2020. The goal of CGED is to use natural language processing techniques to automatically diagnose Chinese grammatical errors in sentences. To this end, we design and implement a CGED model named BERT with Score-feature Gates Error Diagnoser (BSGED), which is based on the BERT model, Bidirectional Long Short-Term Memory (BiLSTM) and conditional random field (CRF). In order to address the problem of losing partial-order relationships when embedding continuous feature items as with previous works, we propose a gating mechanism for integrating continuous feature items, which effectively retains the partial-order relationships between feature items. We perform LSTM processing on the encoding result of the BERT model, and further extract the sequence features. In the final test-set evaluation, we obtained the highest F1 score at the detection level and are among the top 3 F1 scores at the identification level.</abstract>
      <url hash="7055b8ac">2020.nlptea-1.7</url>
      <bibkey>cao-etal-2020-integrating</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>BERT</fixed-case> Enhanced Neural Machine Translation and Sequence Tagging Model for <fixed-case>C</fixed-case>hinese Grammatical Error Diagnosis</title>
      <author><first>Deng</first><last>Liang</last></author>
      <author><first>Chen</first><last>Zheng</last></author>
      <author><first>Lei</first><last>Guo</last></author>
      <author><first>Xin</first><last>Cui</last></author>
      <author><first>Xiuzhang</first><last>Xiong</last></author>
      <author><first>Hengqiao</first><last>Rong</last></author>
      <author><first>Jinpeng</first><last>Dong</last></author>
      <pages>57–66</pages>
      <abstract>This paper presents the UNIPUS-Flaubert team’s hybrid system for the NLPTEA 2020 shared task of Chinese Grammatical Error Diagnosis (CGED). As a challenging NLP task, CGED has attracted increasing attention recently and has not yet fully benefited from the powerful pre-trained BERT-based models. We explore this by experimenting with three types of models. The position-tagging models and correction-tagging models are sequence tagging models fine-tuned on pre-trained BERT-based models, where the former focuses on detecting, positioning and classifying errors, and the latter aims at correcting errors. We also utilize rich representations from BERT-based models by transferring the BERT-fused models to the correction task, and further improve the performance by pre-training on a vast size of unsupervised synthetic data. To the best of our knowledge, we are the first to introduce and transfer the BERT-fused NMT model and sequence tagging model into the Chinese Grammatical Error Correction field. Our work achieved the second highest F1 score at the detecting errors, the best F1 score at correction top1 subtask and the second highest F1 score at correction top3 subtask.</abstract>
      <url hash="8c8495fd">2020.nlptea-1.8</url>
      <bibkey>liang-etal-2020-bert</bibkey>
    </paper>
    <paper id="9">
      <title>A Hybrid System for <fixed-case>NLPTEA</fixed-case>-2020 <fixed-case>CGED</fixed-case> Shared Task</title>
      <author><first>Meiyuan</first><last>Fang</last></author>
      <author><first>Kai</first><last>Fu</last></author>
      <author><first>Jiping</first><last>Wang</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Jin</first><last>Huang</last></author>
      <author><first>Yitao</first><last>Duan</last></author>
      <pages>67–77</pages>
      <abstract>This paper introduces our system at NLPTEA2020 shared task for CGED, which is able to detect, locate, identify and correct grammatical errors in Chinese writings. The system consists of three components: GED, GEC, and post processing. GED is an ensemble of multiple BERT-based sequence labeling models for handling GED tasks. GEC performs error correction. We exploit a collection of heterogenous models, including Seq2Seq, GECToR and a candidate generation module to obtain correction candidates. Finally in the post processing stage, results from GED and GEC are fused to form the final outputs. We tune our models to lean towards optimizing precision, which we believe is more crucial in practice. As a result, among the six tracks in the shared task, our system performs well in the correction tracks: measured in F1 score, we rank first, with the highest precision, in the TOP3 correction track and third in the TOP1 correction track, also with the highest precision. Ours are among the top 4 to 6 in other tracks, except for FPR where we rank 12. And our system achieves the highest precisions among the top 10 submissions at IDENTIFICATION and POSITION tracks.</abstract>
      <url hash="178743cf">2020.nlptea-1.9</url>
      <bibkey>fang-etal-2020-hybrid</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>C</fixed-case>hinese Grammatical Error Correction Based on Hybrid Models with Data Augmentation</title>
      <author><first>Yi</first><last>Wang</last></author>
      <author><first>Ruibin</first><last>Yuan</last></author>
      <author><first>Yan‘gen</first><last>Luo</last></author>
      <author><first>Yufang</first><last>Qin</last></author>
      <author><first>NianYong</first><last>Zhu</last></author>
      <author><first>Peng</first><last>Cheng</last></author>
      <author><first>Lihuan</first><last>Wang</last></author>
      <pages>78–86</pages>
      <abstract>A better Chinese Grammatical Error Diagnosis (CGED) system for automatic Grammatical Error Correction (GEC) can benefit foreign Chinese learners and lower Chinese learning barriers. In this paper, we introduce our solution to the CGED2020 Shared Task Grammatical Error Correction in detail. The task aims to detect and correct grammatical errors that occur in essays written by foreign Chinese learners. Our solution combined data augmentation methods, spelling check methods, and generative grammatical correction methods, and achieved the best recall score in the Top 1 Correction track. Our final result ranked fourth among the participants.</abstract>
      <url hash="4dad2ddc">2020.nlptea-1.10</url>
      <bibkey>wang-etal-2020-chinese-grammatical</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>TMU</fixed-case>-<fixed-case>NLP</fixed-case> System Using <fixed-case>BERT</fixed-case>-based Pre-trained Model to the <fixed-case>NLP</fixed-case>-<fixed-case>TEA</fixed-case> <fixed-case>CGED</fixed-case> Shared Task 2020</title>
      <author><first>Hongfei</first><last>Wang</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>87–90</pages>
      <abstract>In this paper, we introduce our system for NLPTEA 2020 shared task of Chinese Grammatical Error Diagnosis (CGED). In recent years, pre-trained models have been extensively studied, and several downstream tasks have benefited from their utilization. In this study, we treat the grammar error diagnosis (GED) task as a grammatical error correction (GEC) problem and propose a method that incorporates a pre-trained model into an encoder-decoder model to solve this problem.</abstract>
      <url hash="78a3d30d">2020.nlptea-1.11</url>
      <bibkey>wang-komachi-2020-tmu</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>CYUT</fixed-case> Team <fixed-case>C</fixed-case>hinese Grammatical Error Diagnosis System Report in <fixed-case>NLPTEA</fixed-case>-2020 <fixed-case>CGED</fixed-case> Shared Task</title>
      <author><first>Shih-Hung</first><last>Wu</last></author>
      <author><first>Junwei</first><last>Wang</last></author>
      <pages>91–96</pages>
      <abstract>This paper reports our Chinese Grammatical Error Diagnosis system in the NLPTEA-2020 CGED shared task. In 2020, we sent two runs with two approaches. The first one is a combination of conditional random fields (CRF) and a BERT model deep-learning approach. The second one is a BERT model deep-learning approach. The official results shows that our run1 achieved the highest precision rate 0.9875 with the lowest false positive rate 0.0163 on detection, while run2 gives a more balanced performance.</abstract>
      <url hash="6664e87c">2020.nlptea-1.12</url>
      <bibkey>wu-wang-2020-cyut</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>C</fixed-case>hinese Grammatical Error Diagnosis Based on <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a-<fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case>-<fixed-case>CRF</fixed-case> Model</title>
      <author><first>Yingjie</first><last>Han</last></author>
      <author><first>Yingjie</first><last>Yan</last></author>
      <author><first>Yangchao</first><last>Han</last></author>
      <author><first>Rui</first><last>Chao</last></author>
      <author><first>Hongying</first><last>Zan</last></author>
      <pages>97–101</pages>
      <abstract>Chinese Grammatical Error Diagnosis (CGED) is a natural language processing task for the NLPTEA6 workshop. The goal of this task is to automatically diagnose grammatical errors in Chinese sentences written by L2 learners. This paper proposes a RoBERTa-BiLSTM-CRF model to detect grammatical errors in sentences. Firstly, RoBERTa model is used to obtain word vectors. Secondly, word vectors are input into BiLSTM layer to learn context features. Last, CRF layer without hand-craft features work for processing the output by BiLSTM. The optimal global sequences are obtained according to state transition matrix of CRF and adjacent labels of training data. In experiments, the result of RoBERTa-CRF model and ERNIE-BiLSTM-CRF model are compared, and the impacts of parameters of the models and the testing datasets are analyzed. In terms of evaluation results, our recall score of RoBERTa-BiLSTM-CRF ranks fourth at the detection level.</abstract>
      <url hash="53f4a543">2020.nlptea-1.13</url>
      <attachment type="Dataset" hash="25241c14">2020.nlptea-1.13.Dataset.rar</attachment>
      <attachment type="Software" hash="fc7964d4">2020.nlptea-1.13.Software.rar</attachment>
      <bibkey>han-etal-2020-chinese</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>C</fixed-case>hinese Grammatical Errors Diagnosis System Based on <fixed-case>BERT</fixed-case> at <fixed-case>NLPTEA</fixed-case>-2020 <fixed-case>CGED</fixed-case> Shared Task</title>
      <author><first>Hongying</first><last>Zan</last></author>
      <author><first>Yangchao</first><last>Han</last></author>
      <author><first>Haotian</first><last>Huang</last></author>
      <author><first>Yingjie</first><last>Yan</last></author>
      <author><first>Yuke</first><last>Wang</last></author>
      <author><first>Yingjie</first><last>Han</last></author>
      <pages>102–107</pages>
      <abstract>In the process of learning Chinese, second language learners may have various grammatical errors due to the negative transfer of native language. This paper describes our submission to the NLPTEA 2020 shared task on CGED. We present a hybrid system that utilizes both detection and correction stages. The detection stage is a sequential labelling model based on BiLSTM-CRF and BERT contextual word representation. The correction stage is a hybrid model based on the n-gram and Seq2Seq. Without adding additional features and external data, the BERT contextual word representation can effectively improve the performance metrics of Chinese grammatical error detection and correction.</abstract>
      <url hash="c2ce541f">2020.nlptea-1.14</url>
      <bibkey>zan-etal-2020-chinese</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>C</fixed-case>hinese Grammatical Error Detection Based on <fixed-case>BERT</fixed-case> Model</title>
      <author><first>Yong</first><last>Cheng</last></author>
      <author><first>Mofan</first><last>Duan</last></author>
      <pages>108–113</pages>
      <abstract>Automatic grammatical error correction is of great value in assisting second language writing. In 2020, the shared task for Chinese grammatical error diagnosis(CGED) was held in NLP-TEA. As the LDU team, we participated the competition and submitted the final results. Our work mainly focused on grammatical error detection, that is, to judge whether a sentence contains grammatical errors. We used the BERT pre-trained model for binary classification, and we achieve 0.0391 in FPR track, ranking the second in all teams. In error detection track, the accuracy, recall and F-1 of our submitted result are 0.9851, 0.7496 and 0.8514 respectively.</abstract>
      <url hash="6cbd986f">2020.nlptea-1.15</url>
      <bibkey>cheng-duan-2020-chinese</bibkey>
    </paper>
    <paper id="16">
      <title>Named-Entity Based Sentiment Analysis of <fixed-case>N</fixed-case>epali News Media Texts</title>
      <author><first>Birat</first><last>Bade Shrestha</last></author>
      <author><first>Bal Krishna</first><last>Bal</last></author>
      <pages>114–120</pages>
      <abstract>Due to the general availability, relative abundance and wide diversity of opinions, news Media texts are very good sources for sentiment analysis. However, the major challenge with such texts is the difficulty in aligning the expressed opinions to the concerned political leaders as this entails a non-trivial task of named-entity recognition and anaphora resolution. In this work, our primary focus is on developing a Natural Language Processing (NLP) pipeline involving a robust Named-Entity Recognition followed by Anaphora Resolution and then after alignment of the recognized and resolved named-entities, in this case, political leaders to the correct class of opinions as expressed in the texts. We visualize the popularity of the politicians via the time series graph of positive and negative sentiments as an outcome of the pipeline. We have achieved the performance metrics of the individual components of the pipeline as follows: Part of speech tagging – 93.06% (F1-score), Named-Entity Recognition – 86% (F1-score), Anaphora Resolution – 87.45% (Accuracy), Sentiment Analysis – 80.2% (F1-score).</abstract>
      <url hash="eb0ae892">2020.nlptea-1.16</url>
      <bibkey>bade-shrestha-bal-2020-named</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>SEMA</fixed-case>: Text Simplification Evaluation through Semantic Alignment</title>
      <author><first>Xuan</first><last>Zhang</last></author>
      <author><first>Huizhou</first><last>Zhao</last></author>
      <author><first>KeXin</first><last>Zhang</last></author>
      <author><first>Yiyang</first><last>Zhang</last></author>
      <pages>121–128</pages>
      <abstract>Text simplification is an important branch of natural language processing. At present, methods used to evaluate the semantic retention of text simplification are mostly based on string matching. We propose the SEMA (text Simplification Evaluation Measure through Semantic Alignment), which is based on semantic alignment. Semantic alignments include complete alignment, partial alignment and hyponymy alignment. Our experiments show that the evaluation results of SEMA have a high consistency with human evaluation for the simplified corpus of Chinese and English news texts.</abstract>
      <url hash="8ab0302a">2020.nlptea-1.17</url>
      <bibkey>zhang-etal-2020-sema</bibkey>
    </paper>
    <paper id="18">
      <title>A Corpus Linguistic Perspective on the Appropriateness of Pop Songs for Teaching <fixed-case>C</fixed-case>hinese as a Second Language</title>
      <author><first>Xiangyu</first><last>Chi</last></author>
      <author><first>Gaoqi</first><last>Rao</last></author>
      <pages>129–137</pages>
      <abstract>Language and music are closely related. Regarding the linguistic feature richness, pop songs are probably suitable to be used as extracurricular materials in language teaching. In order to prove this point, this paper presents the Contemporary Chinese Pop Lyrics (CCPL) corpus. Based on that, we investigated and evaluated the appropriateness of pop songs for Teaching Chinese as a Second Language (TCSL) with the assistance of Natural Language Processing methods from the perspective of Chinese character coverage, lexical coverage and the addressed topic similarity. Some suggestions in Chinese teaching with the aid of pop lyrics are provided.</abstract>
      <url hash="be7426d4">2020.nlptea-1.18</url>
      <bibkey>chi-rao-2020-corpus</bibkey>
    </paper>
  </volume>
</collection>
