<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.slpat">
  <volume id="1" ingest-date="2022-05-15">
    <meta>
      <booktitle>Ninth Workshop on Speech and Language Processing for Assistive Technologies (SLPAT-2022)</booktitle>
      <editor><first>Sarah</first><last>Ebling</last></editor>
      <editor><first>Emily</first><last>Prud’hommeaux</last></editor>
      <editor><first>Preethi</first><last>Vaidyanathan</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="7a7fc642">2022.slpat-1</url>
      <venue>slpat</venue>
    </meta>
    <frontmatter>
      <url hash="5595916b">2022.slpat-1.0</url>
      <bibkey>slpat-2022-speech</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Design principles of an open-source language modeling microservice package for <fixed-case>AAC</fixed-case> text-entry applications</title>
      <author><first>Brian</first><last>Roark</last></author>
      <author><first>Alexander</first><last>Gutkin</last></author>
      <pages>1-16</pages>
      <abstract>We present MozoLM, an open-source language model microservice package intended for use in AAC text-entry applications, with a particular focus on the design principles of the library. The intent of the library is to allow the ensembling of multiple diverse language models without requiring the clients (user interface designers, system users or speech-language pathologists) to attend to the formats of the models. Issues around privacy, security, dynamic versus static models, and methods of model combination are explored and specific design choices motivated. Some simulation experiments demonstrating the benefits of personalized language model ensembling via the library are presented.</abstract>
      <url hash="bb606aa8">2022.slpat-1.1</url>
      <bibkey>roark-gutkin-2022-design</bibkey>
      <doi>10.18653/v1/2022.slpat-1.1</doi>
      <video href="2022.slpat-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title><fixed-case>C</fixed-case>olor<fixed-case>C</fixed-case>ode: A <fixed-case>B</fixed-case>ayesian Approach to Augmentative and Alternative Communication with Two Buttons</title>
      <author><first>Matthew</first><last>Daly</last></author>
      <pages>17-23</pages>
      <abstract>Many people with severely limited muscle control can only communicate through augmentative and alternative communication (AAC) systems with a small number of buttons. In this paper, we present the design for ColorCode, which is an AAC system with two buttons that uses Bayesian inference to determine what the user wishes to communicate. Our information-theoretic analysis of ColorCode simulations shows that it is efficient in extracting information from the user, even in the presence of errors, achieving nearly optimal error correction. ColorCode is provided as open source software.</abstract>
      <url hash="c94bc999">2022.slpat-1.2</url>
      <bibkey>daly-2022-colorcode</bibkey>
      <doi>10.18653/v1/2022.slpat-1.2</doi>
      <video href="2022.slpat-1.2.mp4"/>
      <pwccode url="https://github.com/mrdaly/colorcode" additional="false">mrdaly/colorcode</pwccode>
    </paper>
    <paper id="3">
      <title>A glimpse of assistive technology in daily life</title>
      <author><first>Preethi</first><last>Vaidyanathan</last></author>
      <author><first>Angela</first><last>Wislon</last></author>
      <author><first>Doug</first><last>Sawyer</last></author>
      <author><first>Amy</first><last>Diego</last></author>
      <author><first>Augustine</first><last>Webster</last></author>
      <author><first>Katerina</first><last>Fassov</last></author>
      <author><first>James</first><last>Brinton</last></author>
      <author><first>Jenn</first><last>Rubenstein</last></author>
      <pages>24-29</pages>
      <abstract>Robitaille (2010) wrote ‘if all technology companies have accessibility in their mind then people with disabilities won’t be left behind.’ Current technology has come a long way from where it stood decades ago; however, researchers and manufacturers often do not include people with disabilities in the design process and tend to accommodate them after the fact. In this paper we share feedback from four assistive technology users who rely on one or more assistive technology devices in their everyday lives. We believe end users should be part of the design process and that by bringing together experts and users, we can bridge the research/practice gap.</abstract>
      <url hash="21d8bf34">2022.slpat-1.3</url>
      <bibkey>vaidyanathan-etal-2022-glimpse</bibkey>
      <doi>10.18653/v1/2022.slpat-1.3</doi>
    </paper>
    <paper id="4">
      <title>A comparison study on patient-psychologist voice diarization</title>
      <author><first>Rachid</first><last>Riad</last></author>
      <author><first>Hadrien</first><last>Titeux</last></author>
      <author><first>Laurie</first><last>Lemoine</last></author>
      <author><first>Justine</first><last>Montillot</last></author>
      <author><first>Agnes</first><last>Sliwinski</last></author>
      <author><first>Jennifer</first><last>Bagnou</last></author>
      <author><first>Xuan</first><last>Cao</last></author>
      <author><first>Anne-Catherine</first><last>Bachoud-Levi</last></author>
      <author><first>Emmanuel</first><last>Dupoux</last></author>
      <pages>30-36</pages>
      <abstract>Conversations between a clinician and a patient, in natural conditions, are valuable sources of information for medical follow-up. The automatic analysis of these dialogues could help extract new language markers and speed up the clinicians’ reports. Yet, it is not clear which model is the most efficient to detect and identify the speaker turns, especially for individuals with speech disorders. Here, we proposed a split of the data that allows conducting a comparative evaluation of different diarization methods. We designed and trained end-to-end neural network architectures to directly tackle this task from the raw signal and evaluate each approach under the same metric. We also studied the effect of fine-tuning models to find the best performance. Experimental results are reported on naturalistic clinical conversations between Psychologists and Interviewees, at different stages of Huntington’s disease, displaying a large panel of speech disorders. We found out that our best end-to-end model achieved 19.5 % IER on the test set, compared to 23.6% achieved by the finetuning of the X-vector architecture. Finally, we observed that we could extract clinical markers directly from the automatic systems, highlighting the clinical relevance of our methods.</abstract>
      <url hash="2349f946">2022.slpat-1.4</url>
      <bibkey>riad-etal-2022-comparison</bibkey>
      <doi>10.18653/v1/2022.slpat-1.4</doi>
    </paper>
    <paper id="5">
      <title>Producing <fixed-case>S</fixed-case>tandard <fixed-case>G</fixed-case>erman Subtitles for <fixed-case>S</fixed-case>wiss <fixed-case>G</fixed-case>erman <fixed-case>TV</fixed-case> Content</title>
      <author><first>Johanna</first><last>Gerlach</last></author>
      <author><first>Jonathan</first><last>Mutal</last></author>
      <author><first>Bouillon</first><last>Pierrette</last></author>
      <pages>37-43</pages>
      <abstract>In this study we compare two approaches (neural machine translation and edit-based) and the use of synthetic data for the task of translating normalised Swiss German ASR output into correct written Standard German for subtitles, with a special focus on syntactic differences. Results suggest that NMT is better suited to this task and that relatively simple rule-based generation of training data could be a valuable approach for cases where little training data is available and transformations are simple.</abstract>
      <url hash="b49f1258">2022.slpat-1.5</url>
      <bibkey>gerlach-etal-2022-producing</bibkey>
      <doi>10.18653/v1/2022.slpat-1.5</doi>
      <video href="2022.slpat-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Investigating the Medical Coverage of a Translation System into Pictographs for Patients with an Intellectual Disability</title>
      <author><first>Magali</first><last>Norré</last></author>
      <author><first>Vincent</first><last>Vandeghinste</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <author><first>Bouillon</first><last>Pierrette</last></author>
      <pages>44-49</pages>
      <abstract>Communication between physician and patients can lead to misunderstandings, especially for disabled people. An automatic system that translates natural language into a pictographic language is one of the solutions that could help to overcome this issue. In this preliminary study, we present the French version of a translation system using the Arasaac pictographs and we investigate the strategies used by speech therapists to translate into pictographs. We also evaluate the medical coverage of this tool for translating physician questions and patient instructions.</abstract>
      <url hash="e8cd017e">2022.slpat-1.6</url>
      <bibkey>norre-etal-2022-investigating</bibkey>
      <doi>10.18653/v1/2022.slpat-1.6</doi>
      <video href="2022.slpat-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>On the Ethical Considerations of Text Simplification</title>
      <author><first>Sian</first><last>Gooding</last></author>
      <pages>50-57</pages>
      <abstract>This paper outlines the ethical implications of text simplification within the framework of assistive systems. We argue that a distinction should be made between the technologies that perform text simplification and the realisation of these in assistive technologies. When using the latter as a motivation for research, it is important that the subsequent ethical implications be carefully considered. We provide guidelines for the framing of text simplification independently of assistive systems, as well as suggesting directions for future research and discussion based on the concerns raised.</abstract>
      <url hash="a55ffbae">2022.slpat-1.7</url>
      <bibkey>gooding-2022-ethical</bibkey>
      <doi>10.18653/v1/2022.slpat-1.7</doi>
      <video href="2022.slpat-1.7.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="8">
      <title>Applying the Stereotype Content Model to assess disability bias in popular pre-trained <fixed-case>NLP</fixed-case> models underlying <fixed-case>AI</fixed-case>-based assistive technologies</title>
      <author><first>Brienna</first><last>Herold</last></author>
      <author><first>James</first><last>Waller</last></author>
      <author><first>Raja</first><last>Kushalnagar</last></author>
      <pages>58-65</pages>
      <abstract>Stereotypes are a positive or negative, generalized, and often widely shared belief about the attributes of certain groups of people, such as people with sensory disabilities. If stereotypes manifest in assistive technologies used by deaf or blind people, they can harm the user in a number of ways, especially considering the vulnerable nature of the target population. AI models underlying assistive technologies have been shown to contain biased stereotypes, including racial, gender, and disability biases. We build on this work to present a psychology-based stereotype assessment of the representation of disability, deafness, and blindness in BERT using the Stereotype Content Model. We show that BERT contains disability bias, and that this bias differs along established stereotype dimensions.</abstract>
      <url hash="26ff2437">2022.slpat-1.8</url>
      <bibkey>herold-etal-2022-applying</bibkey>
      <doi>10.18653/v1/2022.slpat-1.8</doi>
      <video href="2022.slpat-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title><fixed-case>C</fixed-case>ue<fixed-case>B</fixed-case>ot: Cue-Controlled Response Generation for Assistive Interaction Usages</title>
      <author><first>Shachi</first><last>H. Kumar</last></author>
      <author><first>Hsuan</first><last>Su</last></author>
      <author><first>Ramesh</first><last>Manuvinakurike</last></author>
      <author><first>Max</first><last>Pinaroc</last></author>
      <author><first>Sai</first><last>Prasad</last></author>
      <author><first>Saurav</first><last>Sahay</last></author>
      <author><first>Lama</first><last>Nachman</last></author>
      <pages>66-79</pages>
      <abstract>Conversational assistants are ubiquitous among the general population, however, these systems have not had an impact on people with disabilities, or speech and language disorders, for whom basic day-to-day communication and social interaction is a huge struggle. Language model technology can play a huge role in empowering these users and help them interact with others with less effort via interaction support. To enable this population, we build a system that can represent them in a social conversation and generate responses that can be controlled by the users using cues/keywords. We build models that can speed up this communication by suggesting relevant cues in the dialog response context. We also introduce a keyword-loss to lexically constrain the model response output. We present automatic and human evaluation of our cue/keyword predictor and the controllable dialog system to show that our models perform significantly better than models without control. Our evaluation and user study shows that keyword-control on end-to-end response generation models is powerful and can enable and empower users with degenerative disorders to carry out their day-to-day communication.</abstract>
      <url hash="f1e20d11">2022.slpat-1.9</url>
      <bibkey>h-kumar-etal-2022-cuebot</bibkey>
      <doi>10.18653/v1/2022.slpat-1.9</doi>
      <video href="2022.slpat-1.9.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="10">
      <title>Challenges in assistive technology development for an endangered language: an <fixed-case>I</fixed-case>rish (<fixed-case>G</fixed-case>aelic) perspective</title>
      <author><first>Ailbhe</first><last>Ni Chasaide</last></author>
      <author><first>Emily</first><last>Barnes</last></author>
      <author><first>Neasa</first><last>Ní Chiaráin</last></author>
      <author><first>Ronan</first><last>McGuirk</last></author>
      <author><first>Oisín</first><last>Morrin</last></author>
      <author><first>Muireann</first><last>Nic Corcráin</last></author>
      <author><first>Julia</first><last>Cummins</last></author>
      <pages>80-87</pages>
      <abstract>This paper describes three areas of assistive technology development which deploy the resources and speech technology for Irish (Gaelic), newly emerging from the ABAIR initiative. These include (i) a screenreading facility for visually impaired people, (ii) an application to help develop phonological awareness and early literacy for dyslexic people (iii) a speech-enabled AAC system for non-speaking people. Each of these is at a different stage of development and poses unique challenges: these are dis-cussed along with the approaches adopted to address them. Three guiding principles underlie development. Firstly, the sociolinguistic context and the needs of the community are essential considerations in setting priorities. Secondly, development needs to be language sensitive. The need for skilled researchers with a deep knowledge of Irish structure is illustrated in the case of (ii) and (iii), where aspects of Irish linguistic structure (phonological, morphological and grammatical) and the striking differences from English pose challenges for systems aimed at bilingual Irish-English users. Thirdly, and most importantly, the users and their support networks are central – not as passive recipients of ready-made technologies, but as active partners at every stage of development, from design to implementation, evaluation and dissemination.</abstract>
      <url hash="52bff89f">2022.slpat-1.10</url>
      <bibkey>ni-chasaide-etal-2022-challenges</bibkey>
      <doi>10.18653/v1/2022.slpat-1.10</doi>
      <video href="2022.slpat-1.10.mp4"/>
    </paper>
  </volume>
</collection>
