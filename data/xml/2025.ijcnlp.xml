<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.ijcnlp">
  <volume id="long" ingest-date="2026-01-12" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics</booktitle>
      <editor><first>Kentaro</first><last>Inui</last></editor>
      <editor><first>Sakriani</first><last>Sakti</last></editor>
      <editor><first>Haofen</first><last>Wang</last></editor>
      <editor><first>Derek F.</first><last>Wong</last></editor>
      <editor id="pushpak-bhattacharyya"><first>Pushpak</first><last>Bhattacharyya</last></editor>
      <editor><first>Biplab</first><last>Banerjee</last></editor>
      <editor><first>Asif</first><last>Ekbal</last></editor>
      <editor><first>Tanmoy</first><last>Chakraborty</last></editor>
      <editor><first>Dhirendra Pratap</first><last>Singh</last></editor>
      <publisher>The Asian Federation of Natural Language Processing and The Association for Computational Linguistics</publisher>
      <address>Mumbai, India</address>
      <month>December</month>
      <year>2025</year>
      <url hash="ac7f3d70">2025.ijcnlp-long</url>
      <venue>ijcnlp</venue>
      <venue>aacl</venue>
      <isbn>979-8-89176-298-5</isbn>
    </meta>
    <frontmatter>
      <url hash="4a0ebf23">2025.ijcnlp-long.0</url>
      <bibkey>ijcnlp-2025-long</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>HEARTS</fixed-case>: A Holistic Framework for Explainable, Sustainable and Robust Text Stereotype Detection</title>
      <author><first>Theo</first><last>King</last><affiliation>Holistic AI</affiliation></author>
      <author id="zekun-wu" orcid="0009-0009-0729-4885"><first>Zekun</first><last>Wu</last><affiliation>Department of Computer Science, University College London, University of London and Holistic AI</affiliation></author>
      <author><first>Adriano</first><last>Koshiyama</last><affiliation>Department of Computer Science, University College London, University of London</affiliation></author>
      <author><first>Emre</first><last>Kazim</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Philip Colin</first><last>Treleaven</last><affiliation>University College London, University of London</affiliation></author>
      <pages>1-18</pages>
      <abstract>A stereotype is a generalised claim about a social group. Such claims change with culture and context and are often phrased in everyday language, which makes them hard to detect: the State of the Art Large Language Models (LLMs) reach only 68% macro-F1 on the yes/no task “does this sentence contain a stereotype?”. We present HEARTS, a Holistic framework for Explainable, sustAinable and Robust Text Stereotype detection that brings together NLP and social-science. The framework is built on the Expanded Multi-Grain Stereotype Dataset (EMGSD), 57201 English sentences that cover gender, profession, nationality, race, religion and LGBTQ+ topics, adding 10% more data for under-represented groups while keeping high annotator agreement (<tex-math>\kappa = 0.82</tex-math>). Fine-tuning the lightweight ALBERT-v2 model on EMGSD raises binary detection scores to 81.5% macro-F1, matching full BERT while producing 200<tex-math>\times</tex-math> less CO<tex-math>_2</tex-math>. For Explainability, we blend SHAP and LIME token level scores and introduce a confidence measure that increases when the model is correct (<tex-math>\rho = 0.18</tex-math>). We then use HEARTS to assess 16 SOTA LLMs on 1050 neutral prompts each for stereotype propagation: stereotype rates fall by 23% between model generations, yet clear differences remain across model families (LLaMA <tex-math>&gt;</tex-math> Gemini <tex-math>&gt;</tex-math> GPT <tex-math>&gt;</tex-math> Claude). HEARTS thus supplies a practical, low-carbon and interpretable toolkit for measuring stereotype bias in language.</abstract>
      <url hash="12d577bf">2025.ijcnlp-long.1</url>
      <bibkey>king-etal-2025-hearts</bibkey>
    </paper>
    <paper id="2">
      <title>Roles of <fixed-case>MLLM</fixed-case>s in Visually Rich Document Retrieval for <fixed-case>RAG</fixed-case>: A Survey</title>
      <author id="xiantao-zhang" orcid="0009-0008-8169-4927"><first>Xiantao</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>19-36</pages>
      <abstract>Visually rich documents (VRDs) challenge retrieval-augmented generation (RAG) with layout-dependent semantics, brittle OCR, and evidence spread across complex figures and structured tables. This survey examines how Multimodal Large Language Models (MLLMs) are being used to make VRD retrieval practical for RAG. We organize the literature into three roles: *Modality-Unifying Captioners*, *Multimodal Embedders*, and *End-to-End Representers*. We compare these roles along retrieval granularity, information fidelity, latency and index size, and compatibility with reranking and grounding. We also outline key trade-offs and offer some practical guidance on when to favor each role.Finally, we identify promising directions for future research, including adaptive retrieval units, model size reduction, and the development of evaluation methods.</abstract>
      <url hash="a3fea6a6">2025.ijcnlp-long.2</url>
      <bibkey>zhang-2025-roles</bibkey>
    </paper>
    <paper id="3">
      <title>With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting</title>
      <author id="stephen-meisenbacher" orcid="0000-0001-9230-5001"><first>Stephen</first><last>Meisenbacher</last></author>
      <author id="florian-matthes" orcid="0000-0002-6667-5452"><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>37-47</pages>
      <abstract>Recent work in Differential Privacy with Natural Language Processing (DP NLP) has proposed numerous promising techniques in the form of *text rewriting* mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is that of *dataset size*, or rather, the effect of dataset size on a mechanism’s efficacy for utility and privacy preservation. In this work, we are the first to introduce this factor in the evaluation of DP text privatization, where we design utility and privacy tests on large-scale datasets with dynamic split sizes. We run these tests on datasets of varying size with up to one million texts, and we focus on quantifying the effect of increasing dataset size on the privacy-utility trade-off. Our findings reveal that dataset size plays an integral part in evaluating DP text rewriting mechanisms; additionally, these findings call for more rigorous evaluation procedures in DP NLP, as well as shed light on the future of DP NLP in practice and at scale.</abstract>
      <url hash="34dd8adb">2025.ijcnlp-long.3</url>
      <bibkey>meisenbacher-matthes-2025-privacy</bibkey>
    </paper>
    <paper id="4">
      <title>From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models</title>
      <author><first>Mahammed</first><last>Kamruzzaman</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Abdullah</first><last>Al Monsur</last><affiliation>University of South Florida</affiliation></author>
      <author id="gene-louis-kim" orcid="0000-0002-3375-1957"><first>Gene Louis</first><last>Kim</last><affiliation>University of South Florida</affiliation></author>
      <author id="anshuman-chhabra" orcid="0000-0001-9376-2896"><first>Anshuman</first><last>Chhabra</last><affiliation>University of South Florida</affiliation></author>
      <pages>48-68</pages>
      <abstract>Emotions are a fundamental facet of human experience, varying across individuals, cultural contexts, and nationalities. Given the recent success of Large Language Models (LLMs) as role-playing agents, we examine whether LLMs exhibit emotional stereotypes when assigned nationality-specific personas. Specifically, we investigate how different countries are represented in pre-trained LLMs through emotion attributions and whether these attributions align with cultural norms. To provide a deeper interpretive lens, we incorporate four key cultural dimensions, namely Power Distance, Uncertainty Avoidance, Long-Term Orientation, and Individualism, derived from Hofstede’s cross-cultural framework. Our analysis reveals significant nationality-based differences, with emotions such as shame, fear, and joy being disproportionately assigned across regions. Furthermore, we observe notable misalignment between LLM-generated and human emotional responses, particularly for negative emotions, highlighting the presence of reductive and potentially biased stereotypes in LLM outputs.</abstract>
      <url hash="cbdbe15b">2025.ijcnlp-long.4</url>
      <bibkey>kamruzzaman-etal-2025-anger</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>REGULAR</fixed-case>: A Framework for Relation-Guided Multi-Span Question Generation</title>
      <author><first>Jiayi</first><last>Lin</last></author>
      <author id="chenyang-zhang" orcid="0009-0003-6390-930X"><first>Chenyang</first><last>Zhang</last></author>
      <author id="bingxuan-hou" orcid="0009-0004-8068-9211"><first>Bingxuan</first><last>Hou</last><affiliation>Tongji University</affiliation></author>
      <author id="dongyu-zhang-0054" orcid="0009-0005-4104-0054"><first>Dongyu</first><last>Zhang</last></author>
      <author><first>Qingqing</first><last>Hong</last></author>
      <author id="junli-wang" orcid="0000-0002-7185-9731"><first>Junli</first><last>Wang</last><affiliation>Tongji University</affiliation></author>
      <pages>69-85</pages>
      <abstract>To alleviate the high cost of manually annotating Question Answering (QA) datasets, Question Generation (QG) requires the model to generate a question related to the given answer and passage. This work primarily focuses on Multi-Span Question Generation (MSQG), where the generated question corresponds to multiple candidate answers. Existing QG methods may not suit MSQG as they typically overlook the correlation between the candidate answers and generate trivial questions, which limits the quality of the synthetic datasets. Based on the observation that relevant entities typically share the same relationship with the same entity, we propose REGULAR, a framework of RElation-GUided MuLti-SpAn Question GeneRation. REGULAR first converts passages into relation graphs and extracts candidate answers from the relation graphs. Then, REGULAR utilizes a QG model to generate a set of candidate questions and a QA model to obtain the best question. We construct over 100,000 questions using Wikipedia corpora, named REGULAR-WIKI, and conduct experiments to compare our synthetic datasets with other synthetic QA datasets. The experiment results show that models trained with REGULAR-WIKI achieve the best performance. We also conduct ablation studies and statistical analysis to verify the quality of our synthetic dataset. Our code and data are available at https://github.com/PluseLin/REGULAR.</abstract>
      <url hash="8ebd8ddb">2025.ijcnlp-long.5</url>
      <bibkey>lin-etal-2025-regular</bibkey>
    </paper>
    <paper id="6">
      <title>Feature Decomposition-Augmentation Network for Multimodal Sentiment Analysis</title>
      <author><first>Dapeng</first><last>Yin</last><affiliation>Tongji University</affiliation></author>
      <author id="bingxuan-hou" orcid="0009-0004-8068-9211"><first>Bingxuan</first><last>Hou</last><affiliation>Tongji University</affiliation></author>
      <author><first>Mengna</first><last>Gao</last></author>
      <author><first>Shuyue</first><last>Zhu</last><affiliation>Tongji University</affiliation></author>
      <author id="junli-wang" orcid="0000-0002-7185-9731"><first>Junli</first><last>Wang</last><affiliation>Tongji University</affiliation></author>
      <pages>86-98</pages>
      <abstract>Multimodal sentiment analysis identifies human emotional tendencies by analyzing text, visual, and auditory modalities. In most studies, the textual modality is usually considered to contain the most emotional information and is regarded as the dominant modality. Existing methods mostly map auxiliary modalities into a semantic space close to the dominant modality, which overly relies on the dominant modality. In this work, we propose a Feature Decomposition-Augmentation (FeaDA) framework, which aims to elevate the role of auxiliary modalities in multimodal data fusion. We first design a projector to decompose auxiliary modalities into partial features, which contain features for emotion judgment, and then utilize these decomposed features to guide the fusion process with KL loss, thereby enhancing the status of auxiliary modality fusion. To verify the effectiveness of our method, we conducted experiments on the CMU-MOSI, CMU-MOSEI, and CH-SIMS datasets. The experimental results show that our FeaDA framework outperforms mutilmodal sentiment analysis methods of the same type in main metrics. Our code is available at https://github.com/PowerLittleYin/FeaDA-main.</abstract>
      <url hash="1f0716aa">2025.ijcnlp-long.6</url>
      <bibkey>yin-etal-2025-feature</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>CSPLADE</fixed-case>: Learned Sparse Retrieval with Causal Language Models</title>
      <author id="zhichao-xu" orcid="0000-0002-2370-4487"><first>Zhichao</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Aosong</first><last>Feng</last><affiliation>Yale University</affiliation></author>
      <author id="yijun-tian" orcid="0000-0003-2795-6080"><first>Yijun</first><last>Tian</last></author>
      <author><first>Haibo</first><last>Ding</last><affiliation>Amazon</affiliation></author>
      <author><first>Lin Lee</first><last>Cheong</last><affiliation>Amazon</affiliation></author>
      <pages>99-114</pages>
      <abstract>In recent years, dense retrieval has been the focus of information retrieval (IR) research. While effective, dense retrieval produces uninterpretable dense vectors, and suffers from the drawback of large index size. Learned sparse retrieval (LSR) has emerged as promising alternative, achieving competitive retrieval performance while also being able to leverage the classical inverted index data structure for efficient retrieval. However, limited works have explored scaling LSR beyond BERT scale. In this work, we identify two challenges in training large language models (LLM) for LSR: (1) training instability during the early stage of contrastive training; (2) suboptimal performance due to pre-trained LLM’s unidirectional attention. To address these challenges, we propose two corresponding techniques: (1) a lightweight adaptation training phase to eliminate training instability; (2) two model variants to enable bidirectional information. With these techniques, we are able to train LSR models with 8B scale LLM, and achieve competitive retrieval performance with reduced index size. Furthermore, we are among the first to analyze the performance-efficiency tradeoff of LLM-based LSR model through the lens of model quantization. Our findings provide insights into adapting LLMs for efficient retrieval modeling.</abstract>
      <url hash="d34acf29">2025.ijcnlp-long.7</url>
      <bibkey>xu-etal-2025-csplade</bibkey>
    </paper>
    <paper id="8">
      <title>Bias Amplification: Large Language Models as Increasingly Biased Media</title>
      <author><first>Ze</first><last>Wang</last></author>
      <author id="zekun-wu" orcid="0009-0009-0729-4885"><first>Zekun</first><last>Wu</last><affiliation>Department of Computer Science, University College London, University of London and Holistic AI</affiliation></author>
      <author><first>Yichi</first><last>Zhang</last><affiliation>Holistic AI</affiliation></author>
      <author id="xin-guan" orcid="0000-0003-1670-4937"><first>Xin</first><last>Guan</last><affiliation>Holistic AI</affiliation></author>
      <author id="navya-jain" orcid="0000-0002-0248-3726"><first>Navya</first><last>Jain</last></author>
      <author><first>Qinyang</first><last>Lu</last></author>
      <author><first>Saloni</first><last>Gupta</last></author>
      <author><first>Adriano</first><last>Koshiyama</last><affiliation>Department of Computer Science, University College London, University of London</affiliation></author>
      <pages>115-132</pages>
      <abstract>Model collapse—a phenomenon where models degrade in performance due to indiscriminate use of synthetic data—is well studied. However, its role in bias amplification—the progressive reinforcement of pre-existing social biases in Large Language Models (LLMs)—remains underexplored. In this paper, we formally define the conditions for bias amplification and demonstrate through statistical simulations that bias can intensify even in the absence of sampling errors, the primary driver of model collapse. Empirically, we investigate political bias amplification in GPT-2 using a custom-built benchmark for sentence continuation tasks. Our findings reveal a progressively increasing right-leaning bias. Furthermore, we evaluate three mitigation strategies—Overfitting, Preservation, and Accumulation—and show that bias amplification persists even when model collapse is mitigated. Finally, a mechanistic interpretation identifies distinct sets of neurons responsible for model collapse and bias amplification, suggesting they arise from different underlying mechanisms.</abstract>
      <url hash="e956ed65">2025.ijcnlp-long.8</url>
      <bibkey>wang-etal-2025-bias</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>STAR</fixed-case>: Self-Automated Back-Querying for Production Data Generation</title>
      <author><first>Kellen Tan</first><last>Cheng</last><affiliation>Princeton University</affiliation></author>
      <author id="anna-lisa-gentile" orcid="0000-0002-6401-4175"><first>Anna Lisa</first><last>Gentile</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Chad</first><last>DeLuca</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Guang-Jie</first><last>Ren</last><affiliation>International Business Machines</affiliation></author>
      <pages>133-148</pages>
      <abstract>The pervasiveness of large language models (LLMs) in enterprise settings has also brought forth a significant amount of risks associated with their usage. Guardrails technologies aim to mitigate this risk by filtering LLMs’ input/output text through various detectors. However, developing and maintaining robust detectors has many challenges, one of which is the difficulty in acquiring production-quality labeled data on real LLM outputs before deployment. In this work, we propose STAR, a simple yet intuitive solution to generate production-like labeled data for LLMs’ guardrails development. STAR is based on two key ideas: (i) using self-automated back-querying to synthetically generate data, paired with (ii) a sparse human-in-the-loop clustering technique to label the data. The aim of self-automated back-querying is to construct a parallel corpus roughly representative of the original dataset and resembling real LLM output. We then infuse existing datasets with our synthetically generated examples to produce robust training data for our detectors. We test our technique on one of the most difficult and nuanced detectors: the identification of health advice in LLM output, and demonstrate improvement versus other solutions. Our detector is able to outperform GPT-4o by up to 3.48%, despite having 400x less parameters.</abstract>
      <url hash="84c0c1f8">2025.ijcnlp-long.9</url>
      <bibkey>cheng-etal-2025-star</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>GQSA</fixed-case>: Group Quantization and Sparsity for Accelerating Large Language Model Inference</title>
      <author><first>Chao</first><last>Zeng</last></author>
      <author id="songwei-liu" orcid="0000-0003-4047-1483"><first>Songwei</first><last>Liu</last><affiliation>bytedance</affiliation></author>
      <author><first>Shu</first><last>Yang</last></author>
      <author><first>Fangmin</first><last>Chen</last></author>
      <author><first>Xing</first><last>Mei</last></author>
      <author><first>Lean</first><last>Fu</last><affiliation>Bytedance</affiliation></author>
      <pages>149-165</pages>
      <abstract>Model compression has emerged as a mainstream solution to reduce memory usage and computational overhead. This paper proposes GQSA, a novel model compression framework specifically designed for LLMs. Traditional methods typically focus exclusively on either quantization or sparsification, but relying on a single strategy often results in significant performance loss at high compression rates. In contrast, GQSA integrates quantization and sparsification in a tightly coupled manner, leveraging GPU-friendly structured group sparsity and quantization for efficient acceleration. Building upon system-algorithm co-design principles, we propose a two-stage sparse optimization strategy that ensures the performance superiority of the compressed model. On the engine side, we introduce a “task-centric” parallel strategy, which, to the best of our knowledge, is the first application in the domain of sparse computing. Compared to the traditional 2:4 sparse method, the GQSA offers a more flexible and adjustable sparsity rate, as well as a higher weight compression rate, and is efficiently compatible with weight-only quantization methods. Experimental results demonstrate that, under the GQSA W4S50% compression setting, the model’s accuracy surpasses that of both 2:4 pruning and W2 quantization. Furthermore, at the inference level, GQSA outperforms W2 by 1.26 <tex-math>\times</tex-math> and 2:4 pruning by 2.35 <tex-math>\times</tex-math> in terms of speed.</abstract>
      <url hash="72bb9d76">2025.ijcnlp-long.10</url>
      <bibkey>zeng-etal-2025-gqsa</bibkey>
    </paper>
    <paper id="11">
      <title>Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms</title>
      <author id="yuxi-sun-952x" orcid="0009-0009-1180-952X"><first>Yuxi</first><last>Sun</last></author>
      <author><first>Wei</first><last>Gao</last><affiliation>Singapore Management University</affiliation></author>
      <author id="hongzhan-lin" orcid="0000-0002-4111-8334"><first>Hongzhan</first><last>Lin</last></author>
      <author id="jing-ma" orcid="0000-0002-7464-8331"><first>Jing</first><last>Ma</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>166-184</pages>
      <abstract>Human behaviors are often guided or constrained by social norms, which are defined as shared, commonsense rules. For example, underlying an action report a witnessed crime are social norms that inform our conduct, such as It is expected to be brave to report crimes. Current AI systems that assess valence (i.e., support or oppose) of human actions by leveraging large-scale data training not grounded on explicit norms may be difficult to explain, and thus untrustworthy. Emulating human assessors by considering social norms can help AI models better understand and predict valence. While multiple norms come into play, conflicting norms can create tension and directly influence human behavior. For example, when deciding whether to report a witnessed crime, one may balance bravery against self-protection. In this paper, we introduce ClarityEthic, a novel ethical assessment approach, to enhance valence prediction and explanation by generating conflicting social norms behind human actions, which strengthens the moral reasoning capabilities of language models by using a contrastive learning strategy. Extensive experiments demonstrate that our method outperforms strong baseline approaches, and human evaluations confirm that the generated social norms provide plausible explanations for the assessment of human behaviors.</abstract>
      <url hash="6c39872d">2025.ijcnlp-long.11</url>
      <bibkey>sun-etal-2025-explainable</bibkey>
    </paper>
    <paper id="12">
      <title>Enhancing <fixed-case>ID</fixed-case> and Text Fusion via Alternative Training in Session-based Recommendation</title>
      <author id="juanhui-li" orcid="0000-0003-4909-1778"><first>Juanhui</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author id="haoyu-han" orcid="0000-0002-2529-6042"><first>Haoyu</first><last>Han</last></author>
      <author id="zhikai-chen" orcid="0009-0009-7305-8629"><first>Zhikai</first><last>Chen</last></author>
      <author id="harry-shomer" orcid="0000-0001-5081-1870"><first>Harry</first><last>Shomer</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Wei</first><last>Jin</last><affiliation>Emory University</affiliation></author>
      <author><first>Amin</first><last>Javari</last><affiliation>The Home Depot</affiliation></author>
      <author id="hui-liu-3495" orcid="0000-0002-3555-3495"><first>Hui</first><last>Liu</last></author>
      <pages>185-199</pages>
      <abstract>Session-based recommendation systems have attracted growing interest for their ability to provide personalized recommendations based on users’ in-session behaviors. While ID-based methods have shown strong performance, they often struggle with long-tail items and overlook valuable textual information. To incorporate text information, various approaches have been proposed, generally employing a naive fusion framework. Interestingly, this approach often fails to outperform the best single-modality baseline. Further exploration indicates a potential imbalance issue in the naive fusion method, where the ID tends to dominate the training and the text is undertrained. This issue indicates that the naive fusion method might not be as effective in combining ID and text as once believed. To address this, we propose AlterRec, an alternative training framework that separates the optimization of ID and text to avoid the imbalance issue. AlterRec also designs an effective strategy to enhance the interaction between the two modalities, facilitating mutual interaction and more effective text integration. Extensive experiments demonstrate the effectiveness of AlterRec in session-based recommendation.</abstract>
      <url hash="9b48eb27">2025.ijcnlp-long.12</url>
      <bibkey>li-etal-2025-enhancing-id</bibkey>
    </paper>
    <paper id="13">
      <title>Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart Reasoning Data Generation</title>
      <author><first>Zijian</first><last>Li</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jingjing</first><last>Fu</last></author>
      <author><first>Lei</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <author id="jiang-bian" orcid="0000-0002-9472-600X"><first>Jiang</first><last>Bian</last><affiliation>Microsoft</affiliation></author>
      <author id="jun-zhang-1898" orcid="0000-0002-5222-1898"><first>Jun</first><last>Zhang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <pages>200-234</pages>
      <abstract>Visual reasoning is crucial for multimodal large language models (MLLMs) to address complex chart queries, yet high-quality rationale data remains scarce. Existing methods leveraged (M)LLMs for data generation, but direct prompting often yields limited precision and diversity. In this paper, we propose <i>Chain of Functions (CoF)</i>, a novel programmatic reasoning data generation pipeline that utilizes freely-explored reasoning paths as supervision to ensure data precision and diversity. Specifically, it starts with human-free exploration among the atomic functions (e.g., maximum data and arithmetic operations) to generate diverse function chains, which are then translated into linguistic rationales and questions with only a moderate open-sourced LLM. <i>CoF</i> provides multiple benefits: 1) Precision: function-governed generation reduces hallucinations compared to freeform generation; 2) Diversity: enumerating function chains enables varied question taxonomies; 3) Explainability: function chains serve as built-in rationales, allowing fine-grained evaluation beyond overall accuracy; 4) Practicality: it eliminates reliance on extremely large models. Employing <i>CoF</i>, we construct the <i>ChartCoF</i> dataset, with 1.4k complex reasoning Q&amp;A for fine-grained analysis and 50k Q&amp;A for reasoning enhancement.Experiments show that <i>ChartCoF</i> improves performance for MLLMs on widely used benchmarks, and the fine-grained evaluation on <i>ChartCoF</i> reveals varying performance across question taxonomies and step numbers for each MLLM. Furthermore, the novel paradigm of function-governed rationale generation in <i>CoF</i> could inspire broader applications beyond charts. The code and data have been publicly available at <url>https://github.com/microsoft/Chain-of-Functions</url>.</abstract>
      <url hash="644a405b">2025.ijcnlp-long.13</url>
      <bibkey>li-etal-2025-chain-functions</bibkey>
    </paper>
    <paper id="14">
      <title>Topology-Aware Gated Graph Neural Network for Social Bot Detection</title>
      <author><first>Pi</first><last>Jiebin</last></author>
      <author><first>Yantuan</first><last>Xian</last><affiliation>Kunming University of Science and Technology</affiliation></author>
      <author id="yuxin-huang" orcid="0000-0003-1277-6212"><first>Yuxin</first><last>Huang</last></author>
      <author><first>Yan</first><last>Xiang</last><affiliation>Kunmimg University of Science and Technology</affiliation></author>
      <author><first>Ran</first><last>Song</last><affiliation>Kunmimg University of Science and Technology</affiliation></author>
      <author id="zhengtao-yu" orcid="0000-0001-8952-8984"><first>Zhengtao</first><last>Yu</last><affiliation>Kunming University of Science and Technology</affiliation></author>
      <pages>235-245</pages>
      <abstract>The rapid growth of social networks has led to a surge in social bots, which often disseminate low-quality content and may manipulate public opinion, posing threats to online security. Although recent GNN-based bot detection methods perform strongly, they still face two major challenges. First, deep GNNs are prone to over-smoothing: neighbor aggregation blends bot and human node representations, obscuring bot-specific features. Second, social graphs are dominated by human–human and human–bot connections, while direct bot–bot links are scarce, making it difficult for effective bot representations to propagate within GNNs. To address these issues, we propose a Topology-Aware Gated Graph Neural Network () to detect social bots. employs topology-aware data augmentation to synthesize realistic bot nodes that preserve the original graph structure, mitigating class imbalance; it also introduces a hierarchical gating mechanism that restructures node embeddings into a tree format, selectively filtering noise and enhancing discriminative features. Experiments on three standard benchmark datasets show that consistently surpasses leading baselines in highly imbalanced settings, delivering superior accuracy and robustness.</abstract>
      <url hash="c69a674a">2025.ijcnlp-long.14</url>
      <bibkey>jiebin-etal-2025-topology</bibkey>
    </paper>
    <paper id="15">
      <title>Minimizing Queries, Maximizing Impact: Adaptive Score-Based Attack and Defense for Sentiment Analysis</title>
      <author><first>Yigit Efe</first><last>Enhos</last></author>
      <author id="shira-wein" orcid="0000-0002-1062-0866"><first>Shira</first><last>Wein</last><affiliation>Amherst College</affiliation></author>
      <author><first>Scott</first><last>Alfeld</last><affiliation>Amherst College</affiliation></author>
      <pages>246-258</pages>
      <abstract>While state-of-the-art large language models find high rates of success on text classification tasks such as sentiment analysis, they still exhibit vulnerabilities to adversarial examples: meticulously crafted perturbations of input data that guide models into making false predictions. These adversarial attacks are of particular concern when the systems in question are user-facing. While many attacks are able to reduce the accuracy of Transformer-based classifiers by a substantial margin, they often require a large amount of computational time and a large number of queries made to the attacked model, which makes the attacks susceptible to detection. In this work, we resolve the limitations of high query counts and necessary computational time by proposing a query-efficient word-level attack that is fast during deployment and does not compromise the attack success rate of state-of-the-art methods. Our attack constructs a dictionary of adversarial word substitutions based on prior data and leverages these substitutions to flip the sentiment classification of the text. Our attack method achieves an average of 27.49 queries—over 30% fewer than the closest competitor—while maintaining a 99.70% attack success rate. We also develop an effective defense strategy inspired by our attack approach.</abstract>
      <url hash="a5af24ea">2025.ijcnlp-long.15</url>
      <bibkey>enhos-etal-2025-minimizing</bibkey>
    </paper>
    <paper id="16">
      <title>Generating Text from Uniform Meaning Representation</title>
      <author><first>Emma</first><last>Markle</last></author>
      <author><first>Reihaneh</first><last>Iranmanesh</last></author>
      <author id="shira-wein" orcid="0000-0002-1062-0866"><first>Shira</first><last>Wein</last><affiliation>Amherst College</affiliation></author>
      <pages>259-271</pages>
      <abstract>Uniform Meaning Representation (UMR) is a recently developed graph-based semantic representation, which expands on Abstract Meaning Representation (AMR) in a number of ways, in particular through the inclusion of document-level information and multilingual flexibility. In order to effectively adopt and leverage UMR for downstream tasks, efforts must be placed toward developing a UMR technological ecosystem. Though only a small amount of UMR annotations have been produced to date, in this work, we investigate the first approaches to producing text from multilingual UMR graphs. Exploiting the structural similarity between UMR and AMR graphs and the wide availability of AMR technologies, we introduce (1) a baseline approach which passes UMR graphs to AMR-to-text generation models, (2) a pipeline conversion of UMR to AMR, then using AMR-to-text generation models, and (3) a fine-tuning approach for both foundation models and AMR-to-text generation models with UMR data. Our best performing models achieve multilingual BERTscores of 0.825 for English and 0.882 for Chinese, a promising indication of the effectiveness of fine-tuning approaches for UMR-to-text generation even with limited UMR data.</abstract>
      <url hash="f82b2448">2025.ijcnlp-long.16</url>
      <bibkey>markle-etal-2025-generating</bibkey>
    </paper>
    <paper id="17">
      <title>Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?</title>
      <author><first>Kai</first><last>Yan</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author id="yufei-xu" orcid="0000-0002-9931-5138"><first>Yufei</first><last>Xu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zhengyin</first><last>Du</last><affiliation>ByteDance</affiliation></author>
      <author><first>Xuesong</first><last>Yao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zheyu</first><last>Wang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Xiaowen</first><last>Guo</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jiecao</first><last>Chen</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>272-291</pages>
      <abstract>The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years seems to bring us close enough to the “last exam” for LLMs to surpass humanity. However, is the LLMs’ remarkable reasoning ability indeed coming from true intelligence by human standards, or are they actually reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM’s recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60 percent performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to reevaluate the true intelligence level of cutting-edge LLMs.</abstract>
      <url hash="984d673e">2025.ijcnlp-long.17</url>
      <bibkey>yan-etal-2025-recitation</bibkey>
    </paper>
    <paper id="18">
      <title>Judging the Judges: A Systematic Study of Position Bias in <fixed-case>LLM</fixed-case>-as-a-Judge</title>
      <author><first>Lin</first><last>Shi</last></author>
      <author><first>Chiyu</first><last>Ma</last></author>
      <author><first>Wenhua</first><last>Liang</last></author>
      <author id="xingjian-diao" orcid="0000-0001-9605-4991"><first>Xingjian</first><last>Diao</last></author>
      <author id="weicheng-ma" orcid="0000-0001-7494-9874"><first>Weicheng</first><last>Ma</last><affiliation>Oakland University</affiliation></author>
      <author id="soroush-vosoughi" orcid="0000-0002-2564-8909"><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>292-314</pages>
      <abstract>LLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases—particularly position bias, the tendency to favor solutions based on their position within the prompt—compromise its reliability. This exploratory study evaluates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness. Our experiments, involving 15 LLM judges across MTBench and DevBench with 22 tasks and approximately 40 solution-generating models, result in over 150,000 evaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level factors contributing to bias. The findings confirm that position bias is not due to random chance and varies significantly across judges and tasks. While position bias is weakly influenced by the length of prompt components, it is strongly affected by the quality gap between solutions. Our agreement and disagreement analysis among judges further provides insights into the distribution of judging difficulty across the dataset, and highlights the potential for dataset modifications.</abstract>
      <url hash="aef8b709">2025.ijcnlp-long.18</url>
      <bibkey>shi-etal-2025-judging</bibkey>
    </paper>
    <paper id="19">
      <title>Item-Language Model: Improving Large Language Model for Recommendation via Item-Language Representation Learning</title>
      <author><first>Li</first><last>Yang</last></author>
      <author id="anushya-subbiah" orcid="0009-0009-6005-6050"><first>Anushya</first><last>Subbiah</last><affiliation>Google</affiliation></author>
      <author><first>Hardik</first><last>Patel</last><affiliation>Google</affiliation></author>
      <author><first>Judith Yue</first><last>Li</last></author>
      <author><first>Yanwei</first><last>Song</last></author>
      <author><first>Reza</first><last>Mirghaderi</last></author>
      <author id="vikram-aggarwal" orcid="0009-0004-1750-6806"><first>Vikram</first><last>Aggarwal</last></author>
      <author id="fuli-feng" orcid="0000-0002-5828-9842"><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author id="zenglin-xu" orcid="0000-0001-5550-6461"><first>Zenglin</first><last>Xu</last><affiliation>Shanghai Academy of AI for Science and Fudan University</affiliation></author>
      <author><first>Dongfang</first><last>Liu</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <author id="qifan-wang" orcid="0000-0002-7570-5756"><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <pages>315-330</pages>
      <abstract>Large Language Models (LLMs) have recently made significant advancements in tackling complex tasks, such as retrieving hard-to-find information and solving intricate problems. Consequently, various approaches have been proposed to integrate LLMs into recommender systems, primarily by embedding them within existing architectures or training them on the recommendation data. However, most existing methods fail to effectively incorporate user-item interaction signals into pretrained LLMs due to the modality gap between interaction data and the LLM’s internal knowledge. To address this challenge, we propose the Item-Language Model (ILM) to enhance LLMs for recommendation. ILM consists of two main components: An item-language representation learning module, where an ILM encoder is pretrained to generate text-aligned item representations. And an item-language co-training module, where the ILM encoder is integrated into a pretrained LLM for the recommendation tasks. Extensive experiments demonstrate the superior performance of our approach over several state-of-the-art methods, validating the importance of text-aligned item representations in bridging this modality gap. Our ablation studies further reveal the effectiveness of our model design for integrating the interaction knowledge into LLMs for recommendation tasks. Our code is available at: https://anonymous.4open.science/r/ILM-7AD4/.</abstract>
      <url hash="236bb649">2025.ijcnlp-long.19</url>
      <bibkey>yang-etal-2025-item</bibkey>
    </paper>
    <paper id="20">
      <title>Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models</title>
      <author id="zahraa-al-sahili" orcid="0000-0002-4902-5685"><first>Zahraa</first><last>Al Sahili</last></author>
      <author id="ioannis-patras" orcid="0000-0003-3913-4738"><first>Ioannis</first><last>Patras</last><affiliation>Queen Mary, University of London</affiliation></author>
      <author id="matthew-purver" orcid="0000-0003-2297-1273"><first>Matthew</first><last>Purver</last><affiliation>Queen Mary University of London and Institut Jožef Stefan</affiliation></author>
      <pages>331-352</pages>
      <abstract>Multilingual vision–language models (VLMs) promise universal image–text retrieval, yet their social biases remain under‐explored.We perform the first systematic audit of four public multilingual CLIP variants—M‐CLIP, NLLB‐CLIP, CAPIVARA‐CLIP, and the debiased SigLIP‐2—covering ten languages that differ in resource availability and morphological gender marking.Using balanced subsets of FairFace and the PATA stereotype suite in a zero‐shot setting, we quantify race and gender bias and measure stereotype amplification.Contrary to the intuition that multilinguality mitigates bias, <i>every</i> model exhibits stronger gender skew than its English‐only baseline.CAPIVARA‐CLIP shows its largest biases precisely in the low‐resource languages it targets, while the shared encoder of NLLB‐CLIP and SigLIP‐2 transfers English gender stereotypes into gender‐neutral languages; loosely coupled encoders largely avoid this leakage.Although SigLIP‐2 reduces agency and communion skews, it inherits—and in caption‐sparse contexts (e.g., Xhosa) amplifies—the English anchor’s crime associations.Highly gendered languages consistently magnify all bias types, yet gender‐neutral languages remain vulnerable whenever cross‐lingual weight sharing imports foreign stereotypes.Aggregated metrics thus mask language‐specific “hot spots,” underscoring the need for fine‐grained, language‐aware bias evaluation in future multilingual VLM research.</abstract>
      <url hash="564136b9">2025.ijcnlp-long.20</url>
      <bibkey>al-sahili-etal-2025-breaking</bibkey>
    </paper>
    <paper id="21">
      <title>A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models</title>
      <author id="adam-m-morgan" orcid="0000-0002-6046-526X"><first>Adam M.</first><last>Morgan</last><affiliation>NYU Langone Health</affiliation></author>
      <author id="adeen-flinker" orcid="0000-0003-1247-1283"><first>Adeen</first><last>Flinker</last><affiliation>New York University</affiliation></author>
      <pages>353-371</pages>
      <abstract>We present an automated pipeline for estimating Verb Frame Frequencies (VFFs), the frequency with which a verb appears in particular syntactic frames. VFFs provide a powerful window into syntax in both human and machine language systems, but existing tools for calculating them are limited in scale, accuracy, or accessibility. We use large language models (LLMs) to generate a corpus of sentences containing 476 English verbs. Next, by instructing an LLM to behave like an expert linguist, we had it analyze the syntactic structure of the sentences in this corpus. This pipeline outperforms two widely used syntactic parsers across multiple evaluation datasets. Furthermore, it requires far fewer resources than manual parsing (the gold-standard), thereby enabling rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF database with broader verb coverage, finer-grained syntactic distinctions, and explicit estimates of the relative frequencies of structural alternates commonly studied in psycholinguistics. The pipeline is easily customizable and extensible to new verbs, syntactic frames, and even other languages. We present this work as a proof of concept for automated frame frequency estimation, and release all code and data to support future research.</abstract>
      <url hash="f065d793">2025.ijcnlp-long.21</url>
      <bibkey>morgan-flinker-2025-scalable</bibkey>
    </paper>
    <paper id="22">
      <title>Ibom <fixed-case>NLP</fixed-case>: A Step Toward Inclusive Natural Language Processing for <fixed-case>N</fixed-case>igeria’s Minority Languages</title>
      <author><first>Oluwadara</first><last>Kalejaiye</last><affiliation>Howard University</affiliation></author>
      <author><first>Luel Hagos</first><last>Beyene</last><affiliation>AIMS Research and Innovation Centre</affiliation></author>
      <author id="david-ifeoluwa-adelani" orcid="0000-0002-0193-2083"><first>David Ifeoluwa</first><last>Adelani</last><affiliation>McGill University</affiliation></author>
      <author><first>Mmekut-mfon Gabriel</first><last>Edet</last><affiliation>KoraHQ</affiliation></author>
      <author id="aniefon-daniel-akpan" orcid="0000-0003-1153-4473"><first>Aniefon Daniel</first><last>Akpan</last><affiliation>National Institute for Nigerian Languages</affiliation></author>
      <author id="eno-abasi-urua" orcid="0000-0001-7711-4901"><first>Eno-Abasi</first><last>Urua</last><affiliation>University of Uyo</affiliation></author>
      <author id="anietie-andy" orcid="0000-0002-7043-3042"><first>Anietie</first><last>Andy</last><affiliation>Howard University</affiliation></author>
      <pages>372-382</pages>
      <abstract>Nigeria is the most populous country in Africa with a population of more than 200 million people. More than 500 languages are spoken in Nigeria and it is one of the most linguistically diverse countries in the world. Despite this, natural language processing (NLP) research has mostly focused on the following four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e &lt;1% of the languages spoken in Nigeria). This is in part due to the unavailability of textual data in these languages to train and apply NLP algorithms. In this work, we introduce Ibom—a dataset for machine translation and topic classification in four Coastal Nigerian languages from the Akwa Ibom State region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus on extending Flores-200 benchmark to these languages, and further align the translated texts with topic labels based on SIB-200 classification dataset. Our evaluation shows that current LLMs perform poorly on machine translation for these languages in both zero-and-few shot settings. However, we find the few-shot samples to steadily improve topic classification with more shots.</abstract>
      <url hash="8697ee50">2025.ijcnlp-long.22</url>
      <bibkey>kalejaiye-etal-2025-ibom</bibkey>
    </paper>
    <paper id="23">
      <title>An Analysis of the Impact of Problem Paraphrasing on <fixed-case>LLM</fixed-case>-Based Mathematical Problem Solving</title>
      <author><first>Yerim</first><last>Han</last><affiliation>Chungnam National University</affiliation></author>
      <author id="hyein-seo" orcid="0000-0003-3107-0880"><first>Hyein</first><last>Seo</last></author>
      <author id="hyuk-namgoong" orcid="0000-0001-5166-3064"><first>Hyuk</first><last>Namgoong</last><affiliation>Chungnam National University</affiliation></author>
      <author id="sangkeun-jung" orcid="0000-0003-3531-0618"><first>Sangkeun</first><last>Jung</last><affiliation>Chungnam National University</affiliation></author>
      <pages>383-395</pages>
      <abstract>Recent advances in large language models (LLMs) have significantly improved mathematical problem-solving. Among various techniques, paraphrasing problem statements has emerged as a promising strategy to enhance model understanding and accuracy.We define twelve paraphrasing types grounded in mathematics education theory and analyze their impact on LLM performance across different configurations. To automate selection, we propose a Paraphrase Type Selector that predicts effective paraphrases for each problem.Experiments on MATH-500, SVAMP, and AIME shows consistent performance gain from paraphrased problems. On MATH-500 with LLaMA 3.1-8B, combining the original with the best five paraphrased problems improves accuracy by +8.4%, with the selector achieving an additional +1.33% gain.</abstract>
      <url hash="abb08490">2025.ijcnlp-long.23</url>
      <bibkey>han-etal-2025-analysis</bibkey>
    </paper>
    <paper id="24">
      <title>Synthetic Singers: A Review of Deep-Learning-based Singing Voice Synthesis Approaches</title>
      <author id="changhao-pan" orcid="0009-0004-6023-1764"><first>Changhao</first><last>Pan</last></author>
      <author><first>Dongyu</first><last>Yao</last></author>
      <author id="yu-zhang-0281" orcid="0009-0007-4594-0281"><first>Yu</first><last>Zhang</last></author>
      <author id="wenxiang-guo" orcid="0009-0006-7997-4140"><first>Wenxiang</first><last>Guo</last></author>
      <author><first>Jingyu</first><last>Lu</last></author>
      <author id="zhiyuan-zhu-0420" orcid="0009-0006-1276-0420"><first>Zhiyuan</first><last>Zhu</last></author>
      <author id="zhou-zhao" orcid="0000-0001-6121-0384"><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>396-416</pages>
      <abstract>Recent advances in singing voice synthesis (SVS) have attracted substantial attention from both academia and industry. With the advent of large language models and novel generative paradigms, producing controllable, high‐fidelity singing voices has become an attainable goal. Yet the field still lacks a comprehensive survey that systematically analyzes deep‐learning‐based singing voice systems and their enabling technologies.To address the aforementioned issue, this survey first categorizes existing systems by task type and then organizes current architectures into two major paradigms: cascaded and end-to-end approaches. Moreover, we provide an in-depth analysis of core technologies, covering singing modeling and control techniques. Finally, we review relevant datasets, annotation tools, and evaluation benchmarks that support training and assessment. In appendix, we introduce training strategies and further discussion of SVS. This survey provides an up-to-date review of the literature on SVS models, which would be a useful reference for both researchers and engineers. Related materials are available at https://github.com/David-Pigeon/SyntheticSingers.</abstract>
      <url hash="8ee5e334">2025.ijcnlp-long.24</url>
      <bibkey>pan-etal-2025-synthetic</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>ASA</fixed-case>udio: A Survey of Advanced Spatial Audio Research</title>
      <author id="zhiyuan-zhu-0420" orcid="0009-0006-1276-0420"><first>Zhiyuan</first><last>Zhu</last></author>
      <author id="yu-zhang-0281" orcid="0009-0007-4594-0281"><first>Yu</first><last>Zhang</last></author>
      <author id="wenxiang-guo" orcid="0009-0006-7997-4140"><first>Wenxiang</first><last>Guo</last></author>
      <author id="changhao-pan" orcid="0009-0004-6023-1764"><first>Changhao</first><last>Pan</last></author>
      <author id="zhou-zhao" orcid="0000-0001-6121-0384"><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>417-442</pages>
      <abstract>With the rapid development of spatial audio technologies today, applications in AR, VR and other scenarios have garnered extensive attention. Unlike traditional mono sound, spatial audio offers a more realistic and immersive auditory experience. Despite notable progress in the field, there remains a lack of comprehensive surveys that systematically organize and analyze these methods and their underlying technologies. In this paper, we provide a comprehensive overview of spatial audio and systematically review recent literature in the area. To address this, we chronologically outline existing work related to spatial audio and categorize these studies based on input-output representations, as well as generation and understanding tasks, thereby summarizing various research aspects of spatial audio. In addition, we review related datasets, evaluation metrics, and benchmarks, offering insights from both training and evaluation perspectives. Related materials are available at https://github.com/dieKarotte/ASAudio.</abstract>
      <url hash="45d7ce5a">2025.ijcnlp-long.25</url>
      <bibkey>zhu-etal-2025-asaudio</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>M</fixed-case>oss<fixed-case>N</fixed-case>et: Mixture of State-Space Experts is a Multi-Head Attention</title>
      <author><first>Shikhar</first><last>Tuli</last><affiliation>Samsung Research</affiliation></author>
      <author id="james-seale-smith" orcid="0000-0001-9210-0161"><first>James Seale</first><last>Smith</last><affiliation>Xero</affiliation></author>
      <author><first>Haris</first><last>Jeelani</last></author>
      <author><first>Chi-Heng</first><last>Lin</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Abhishek</first><last>Patel</last></author>
      <author><first>Vasili</first><last>Ramanishka</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Yen-Chang</first><last>Hsu</last><affiliation>TSMC</affiliation></author>
      <author><first>Hongxia</first><last>Jin</last><affiliation>Head of AI center</affiliation></author>
      <pages>443-458</pages>
      <abstract>Large language models (LLMs) have significantly advanced generative applications in natural language processing (NLP). Recent trends in model architectures revolve around efficient variants of transformers or state-space/gated-recurrent models (SSMs, GRMs). However, prevailing SSM/GRM-based methods often emulate only a single attention head, potentially limiting their expressiveness. In this work, we propose MossNet, a novel mixture-of-state-space-experts architecture that emulates a linear multi-head attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation not only in channel-mixing multi-layered perceptron (MLP) blocks but also in the time-mixing SSM kernels to realize multiple “attention heads.” Extensive experiments on language modeling and downstream evaluations show that MossNet outperforms both transformer- and SSM-based architectures of similar model size and data budgets. Larger variants of MossNet, trained on trillions of tokens, further confirm its scalability and superior performance. In addition, real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU demonstrate favorable runtime speed and resource usage compared to similarly sized baselines. Our results suggest that MossNet is a compelling new direction for efficient, high-performing recurrent LLM architectures.</abstract>
      <url hash="461613df">2025.ijcnlp-long.26</url>
      <bibkey>tuli-etal-2025-mossnet</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>LLM</fixed-case>-Based Behavior Prediction for Social Media Users with Continuous Memory</title>
      <author><first>Kun</first><last>Li</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengwei</first><last>Dai</last></author>
      <author><first>Wei</first><last>Zhou</last><affiliation>Institute of Information Engeering</affiliation></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>459-474</pages>
      <abstract>Large language models (LLMs) have demonstrated strong capabilities in simulating social roles and generating human-like behaviors. However, their effectiveness in predicting real-world user behavior under continuous memory accumulation remains largely unexplored. Most existing studies focus on short-term interactions or static personas, neglecting the dynamic nature of users’ historical experiences in social media environments. To address this gap, we introduce FineRob, a novel dataset for fine-grained behavior prediction of social media users, which includes long-term memory traces from 1,866 users across three platforms. Each behavior is decomposed into three elements: object, type, and content, resulting in 78.6k QA records.We identify that as memory accumulates, prediction accuracy drops significantly due to the model’s difficulty in accessing detailed historical information. We further propose the OM-CoT fine-tuning framework to enhance the model’s ability to process and utilize long-term memory. Experimental results show that our method effectively reduces the performance degradation caused by memory growth, improving fine-grained behavior prediction. .</abstract>
      <url hash="2a58a6ca">2025.ijcnlp-long.27</url>
      <bibkey>li-etal-2025-llm-based</bibkey>
    </paper>
    <paper id="28">
      <title>Hassles and Uplifts Detection on Social Media Narratives</title>
      <author id="jiyu-chen" orcid="0000-0002-8057-3022"><first>Jiyu</first><last>Chen</last><affiliation>CSIRO</affiliation></author>
      <author id="sarvnaz-karimi" orcid="0000-0002-4927-3937"><first>Sarvnaz</first><last>Karimi</last><affiliation>CSIRO</affiliation></author>
      <author id="diego-molla" orcid="0000-0003-4973-0963"><first>Diego</first><last>Molla</last><affiliation>Macquarie University</affiliation></author>
      <author id="andreas-duenser" orcid="0000-0002-7423-0736"><first>Andreas</first><last>Duenser</last><affiliation>Commonwealth Scientific and Industrial Research Organisation, CSIRO</affiliation></author>
      <author id="maria-kangas" orcid="0000-0001-8693-2949"><first>Maria</first><last>Kangas</last><affiliation>Macquarie University</affiliation></author>
      <author id="cecile-paris" orcid="0000-0003-3816-0176"><first>Cecile</first><last>Paris</last><affiliation>CSIRO</affiliation></author>
      <pages>475-489</pages>
      <abstract>Hassles and uplifts are psychological constructs of individuals’ positive or negative responses to daily minor incidents, with cumulative impacts on mental health. These concepts are largely overlooked in NLP, where existing tasks and models focus on identifying general sentiment expressed in text. These, however, cannot satisfy targeted information needs in psychological inquiry. To address this, we introduce Hassles and Uplifts Detection (HUD), a novel NLP application to identify these constructs in social media language.We evaluate various language models and task adaptation approaches on a probing dataset collected from a private, real-time emotional venting platform. Some of our models achieve F scores close to 80%. We also identify open opportunities to improve affective language understanding in support of studies in psychology.</abstract>
      <url hash="8b82580c">2025.ijcnlp-long.28</url>
      <bibkey>chen-etal-2025-hassles</bibkey>
    </paper>
    <paper id="29">
      <title>Role-Aware Language Models for Secure and Contextualized Access Control in Organizations</title>
      <author id="saeed-almheiri" orcid="0009-0006-1330-7302"><first>Saeed</first><last>Almheiri</last></author>
      <author><first>Yerulan</first><last>Kongrat</last></author>
      <author><first>Adrian</first><last>Santosh</last></author>
      <author><first>Ruslan</first><last>Tasmukhanov</last></author>
      <author><first>Josemaria Loza</first><last>Vera</last></author>
      <author><first>Muhammad Dehan</first><last>Al Kautsar</last></author>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>490-511</pages>
      <abstract>As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts.</abstract>
      <url hash="e2a72820">2025.ijcnlp-long.29</url>
      <bibkey>almheiri-etal-2025-role</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>SEA</fixed-case>-<fixed-case>LION</fixed-case>: <fixed-case>S</fixed-case>outheast <fixed-case>A</fixed-case>sian Languages in One Network</title>
      <author id="raymond-ng"><first>Raymond</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Thanh Ngan</first><last>Nguyen</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Huang</first><last>Yuli</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Tai Ngee</first><last>Chia</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Leong Wai</first><last>Yi</last></author>
      <author><first>Wei Qi</first><last>Leong</last><affiliation>AI Singapore</affiliation></author>
      <author id="xianbin-yong" orcid="0000-0002-0062-0942"><first>Xianbin</first><last>Yong</last><affiliation>National University of Singapore</affiliation></author>
      <author id="jian-gang-ngui" orcid="0009-0003-2326-552X"><first>Jian Gang</first><last>Ngui</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yosephine</first><last>Susanto</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Nicholas</first><last>Cheng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Hamsawardhini</first><last>Rengarajan</last></author>
      <author><first>Peerat</first><last>Limkonchotiwat</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Adithya Venkatadri</first><last>Hulagadri</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Kok Wai</first><last>Teng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yeo Yeow</first><last>Tong</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Bryan</first><last>Siow</last><affiliation>Singapore Management University and National University of Singapore</affiliation></author>
      <author><first>Wei Yi</first><last>Teo</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Tan Choon</first><last>Meng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Brandon</first><last>Ong</last></author>
      <author><first>Zhi Hao</first><last>Ong</last><affiliation>National University of Singapore</affiliation></author>
      <author id="jann-railey-montalan" orcid="0009-0005-3964-6807"><first>Jann Railey</first><last>Montalan</last><affiliation>AI Singapore and Ateneo de Manila University</affiliation></author>
      <author><first>Adwin</first><last>Chan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Sajeban</first><last>Antonyrex</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Ren</first><last>Lee</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Esther</first><last>Choa</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>David Ong</first><last>Tat-Wee</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Bing Jie Darius</first><last>Liu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>William Chandra</first><last>Tjhi</last><affiliation>National University of Singapore</affiliation></author>
      <author id="erik-cambria" orcid="0000-0002-3030-1280"><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Leslie</first><last>Teo</last><affiliation>National University of Singapore</affiliation></author>
      <pages>512-526</pages>
      <abstract>Recently, Large Language Models (LLMs) have dominated much of the artificial intelligence scene with their ability to process and generate natural languages. However, the majority of LLM research and development remains English-centric, leaving low-resource languages such as those in the Southeast Asian (SEA) region under-represented. To address this representation gap, we introduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge multilingual LLMs designed for SEA languages. The SEA-LION family of LLMs supports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese, Malay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages large-scale multilingual continued pre-training with a comprehensive post-training regime involving multiple stages of instruction fine-tuning, alignment, and model merging. Evaluation results on multilingual benchmarks indicate that our models achieve state-of-the-art performance across LLMs supporting SEA languages. We open-source the models to benefit the wider SEA community.</abstract>
      <url hash="040a81b7">2025.ijcnlp-long.30</url>
      <bibkey>ng-etal-2025-sea</bibkey>
    </paper>
    <paper id="31">
      <title>Do <fixed-case>LLM</fixed-case>s Need Inherent Reasoning Before Reinforcement Learning? A Study in <fixed-case>K</fixed-case>orean Self-Correction</title>
      <author id="hongjin-kim" orcid="0000-0002-3492-2543"><first>Hongjin</first><last>Kim</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author><first>Jaewook</first><last>Lee</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author><first>Kiyoung</first><last>Lee</last></author>
      <author id="jong-hun-shin" orcid="0000-0002-4764-9371"><first>Jong-hun</first><last>Shin</last></author>
      <author><first>Soojong</first><last>Lim</last></author>
      <author><first>Oh-Woog</first><last>Kwon</last></author>
      <pages>527-542</pages>
      <abstract>Large Language Models (LLMs) demonstrate strong reasoning and self-correction abilities in high-resource languages like English, but their performance remains limited in low-resource languages such as Korean. In this study, we investigate whether reinforcement learning (RL) can enhance Korean reasoning abilities to a degree comparable to English. Our findings reveal that RL alone yields limited improvements when applied to models lacking inherent Korean reasoning capabilities. To address this, we explore several fine-tuning strategies and show that aligning the model’s internal reasoning processes with Korean inputs—particularly by tuning Korean-specific neurons in early layers—is key to unlocking RL’s effectiveness. We introduce a self-correction code-switching dataset to facilitate this alignment and observe significant performance gains in both mathematical reasoning and self-correction tasks. Ultimately, we conclude that the crucial factor in multilingual reasoning enhancement is not injecting new linguistic knowledge, but effectively eliciting and aligning existing reasoning capabilities. Our study provides a new perspective on how internal translation and neuron-level tuning contribute to multilingual reasoning alignment in LLMs.</abstract>
      <url hash="d8b2bcdb">2025.ijcnlp-long.31</url>
      <bibkey>kim-etal-2025-llms</bibkey>
    </paper>
    <paper id="32">
      <title>Multilingual Iterative Model Pruning: What Matters?</title>
      <author><first>Haryo Akbarianto</first><last>Wibowo</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author id="haiyue-song" orcid="0000-0003-1159-0918"><first>Haiyue</first><last>Song</last><affiliation>National Institute of Information and Communications Technology (NICT)</affiliation></author>
      <author id="hideki-tanaka" orcid="0009-0001-4498-7017"><first>Hideki</first><last>Tanaka</last><affiliation>National Institute of Information and Communications Technology (NICT)</affiliation></author>
      <author><first>Masao</first><last>Utiyama</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>543-571</pages>
      <abstract>Pruning techniques have been studied to construct small models for efficiency, yet the effect of cross-lingual, which shows language performance transferability, is understudied in this field. In this work, we investigate cross-lingual effects in multilingual large language model compression using iterative pruning and recovery. We employ structured layer pruning with LoRA-based recovery and knowledge distillation, testing whether calibration languages different from target evaluation languages can preserve multilingual performance. Experiments on Qwen2.5-7B and Llama3.1-8B demonstrate that any recovery language consistently outperforms no-recovery baselines, with even low-resource languages like Swahili providing ~5% improvements. In contrast to expectations, dominant pretraining languages do not always yield the best results, where Indonesian achieves the highest performance in Llama3.1-8B, while Japanese performs the best in Qwen2.5-7B. Our findings reveal that cross-lingual calibration effectively maintains multilingual capabilities in the iterative pruning.</abstract>
      <url hash="28fa53b8">2025.ijcnlp-long.32</url>
      <bibkey>wibowo-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="33">
      <title>Counterfactual Evaluation for Blind Attack Detection in <fixed-case>LLM</fixed-case>-based Evaluation Systems</title>
      <author><first>Lijia</first><last>Liu</last></author>
      <author><first>Takumi</first><last>Kondo</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Kyohei</first><last>Atarashi</last><affiliation>Kyoto University and RIKEN AIP</affiliation></author>
      <author id="koh-takeuchi" orcid="0000-0002-3245-888X"><first>Koh</first><last>Takeuchi</last><affiliation>Kyoto University</affiliation></author>
      <author id="jiyi-li" orcid="0000-0003-4997-3850"><first>Jiyi</first><last>Li</last><affiliation>Hokkaido University and University of Yamanashi</affiliation></author>
      <author><first>Shigeru</first><last>Saito</last></author>
      <author id="hisashi-kashima" orcid="0000-0002-2770-0184"><first>Hisashi</first><last>Kashima</last><affiliation>Kyoto University</affiliation></author>
      <pages>572-584</pages>
      <abstract>This paper investigates defenses in LLM-based evaluation, where prompt injection attacks can manipulate scores by deceiving the evaluation system. We formalize blind attacks as a class in which candidate answers are crafted independently of the true answer. To counter such attacks, we propose an evaluation framework that combines standard and counterfactual evaluation. Experiments show it significantly improves attack detection with minimal performance trade-offs for recent models.</abstract>
      <url hash="214991cc">2025.ijcnlp-long.33</url>
      <bibkey>liu-etal-2025-counterfactual</bibkey>
    </paper>
    <paper id="34">
      <title>Hidden in Plain Text: Emergence &amp; Mitigation of Steganographic Collusion in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yohan</first><last>Mathew</last><affiliation>Yohan Mathew (Independent)</affiliation></author>
      <author><first>Ollie</first><last>Matthews</last><affiliation>AI Security Institute</affiliation></author>
      <author id="robert-mccarthy" orcid="0000-0002-2140-6988"><first>Robert</first><last>McCarthy</last></author>
      <author id="joan-velja" orcid="0009-0002-4032-0140"><first>Joan</first><last>Velja</last><affiliation>University of Oxford and University of Amsterdam</affiliation></author>
      <author><first>Christian</first><last>Schroeder de Witt</last></author>
      <author id="dylan-cope" orcid="0000-0003-1147-8010"><first>Dylan</first><last>Cope</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Nandi</first><last>Schoots</last></author>
      <pages>585-624</pages>
      <abstract>The rapid proliferation of frontier model agents promises significant societal advances but also raises concerns about systemic risks arising from unsafe interactions.Collusion to the disadvantage of others has been identified as a central form of undesirable agent cooperation.The use of information hiding (steganography) in agent communications could render such collusion practically undetectable.This underscores the need for investigations into the possibility of such behaviours emerging and the robustness corresponding countermeasures.To investigate this problem we design two approaches – a gradient-based reinforcement learning (GBRL) method and an in-context reinforcement learning (ICRL) method – for reliably eliciting sophisticated LLM-generated linguistic text steganography.We demonstrate, for the first time, that unintended steganographic collusion in LLMs can arise due to mispecified reward incentives during training.Additionally, we find that standard mitigations — both passive oversight of model outputs and active mitigation through communication paraphrasing — are not fully effective at preventing this steganographic communication.Our findings imply that (i) emergence of steganographic collusion is a plausible concern that should be monitored and researched, and (ii) preventing emergence may require innovation in mitigation techniques.</abstract>
      <url hash="d34ea769">2025.ijcnlp-long.34</url>
      <bibkey>mathew-etal-2025-hidden</bibkey>
    </paper>
    <paper id="35">
      <title>A Survey on <fixed-case>LLM</fixed-case>-Assisted Clinical Trial Recruitment</title>
      <author id="shrestha-ghosh" orcid="0000-0002-1711-0500"><first>Shrestha</first><last>Ghosh</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Moritz</first><last>Schneider</last><affiliation>Boehringer Ingelheim</affiliation></author>
      <author><first>Carina</first><last>Reinicke</last><affiliation>Boehringer Ingelheim</affiliation></author>
      <author id="carsten-eickhoff" orcid="0000-0001-9895-4061"><first>Carsten</first><last>Eickhoff</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <pages>625-646</pages>
      <abstract>Clinical trials are designed in natural language and the task of matching them to patients, represented via both structured and unstructured textual data, benefits from knowledge aggregation and reasoning abilities of LLMs. LLMs with their ability to consolidate distributed knowledge hold the potential to build a more general solution than classical approaches that employ trial-specific heuristics. Yet, adoption of LLMs in critical domains, such as clinical research, comes with many challenges, such as, the availability of public benchmarks, the dimensions of evaluation and data sensitivity. In this survey, we contextualize emerging LLM-based approaches in clinical trial recruitment. We examine the main components of the clinical trial recruitment process, discuss existing challenges in adopting LLM technologies in clinical research and exciting future directions.</abstract>
      <url hash="c6e75395">2025.ijcnlp-long.35</url>
      <bibkey>ghosh-etal-2025-survey-llm</bibkey>
    </paper>
    <paper id="36">
      <title>The Learning Dynamics of Subword Segmentation for Morphologically Diverse Languages</title>
      <author><first>Francois</first><last>Meyer</last><affiliation>University of Cape Town</affiliation></author>
      <author id="jan-buys" orcid="0000-0003-1994-5832"><first>Jan</first><last>Buys</last><affiliation>University of Cape Town</affiliation></author>
      <pages>647-661</pages>
      <abstract>Subword segmentation is typically applied in preprocessing and stays fixed during training. Alternatively, it can be learned during training to optimise the training objective. In this paper we study the learning dynamics of subword segmentation: if a language model can dynamically optimise tokenisation, how do its subwords evolve during pretraining and finetuning? To explore this, we extend the subword segmental language model (SSLM), a framework for learning subwords during training, to support pretraining and finetuning. We train models for three typologically diverse languages to study learning dynamics across the morphological spectrum: Isi-Xhosa is conjunctive (long word forms composed of many morphemes), Setswana is disjunctive (morphemes written as separate words), and English represents a typological middle ground. We analyse subword dynamics from a linguistic perspective, tracking morphology, productivity, and fertility. We identify four stages of subword learning, with the morphologically complex isi-Xhosa exhibiting greater instability. During finetuning, subword boundaries shift to become finer-grained. Lastly, we show that learnable subwords offers a promising approach to improve text generation and cross-lingual transfer for low-resource, morphologically complex languages.</abstract>
      <url hash="e7aaa639">2025.ijcnlp-long.36</url>
      <bibkey>meyer-buys-2025-learning</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>PRALEKHA</fixed-case>: Cross-Lingual Document Alignment for <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Sanjay</first><last>Suryanarayanan</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author id="haiyue-song" orcid="0000-0003-1159-0918"><first>Haiyue</first><last>Song</last><affiliation>National Institute of Information and Communications Technology (NICT)</affiliation></author>
      <author><first>Mohammed Safi Ur Rahman</first><last>Khan</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology and Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Anoop</first><last>Kunchukuttan</last><affiliation>Microsoft and Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>662-676</pages>
      <abstract>Mining parallel document pairs for document-level machine translation (MT) remains challenging due to the limitations of existing Cross-Lingual Document Alignment (CLDA) techniques. Existing methods often rely on metadata such as URLs, which are scarce, or on pooled document representations that fail to capture fine-grained alignment cues. Moreover, the limited context window of sentence embedding models hinders their ability to represent document-level context, while sentence-based alignment introduces a combinatorially large search space, leading to high computational cost. To address these challenges for Indic languages, we introduce Pralekha, a benchmark containing over 3 million aligned document pairs across 11 Indic languages and English, which includes 1.5 million English–Indic pairs. Furthermore, we propose Document Alignment Coefficient (DAC), a novel metric for fine-grained document alignment. Unlike pooling-based methods, DAC aligns documents by matching smaller chunks and computes similarity as the ratio of aligned chunks to the average number of chunks in a pair. Intrinsic evaluation shows that our chunk-based method is 2–3× faster while maintaining competitive performance, and that DAC achieves substantial gains over pooling-based baselines. Extrinsic evaluation further demonstrates that document-level MT models trained on DAC-aligned pairs consistently outperform those using baseline alignment methods. These results highlight DAC’s effectiveness for parallel document mining. The dataset and evaluation framework are publicly available to support further research.</abstract>
      <url hash="a33e3339">2025.ijcnlp-long.37</url>
      <bibkey>suryanarayanan-etal-2025-pralekha</bibkey>
    </paper>
    <paper id="38">
      <title>Structured Document Translation via Format Reinforcement Learning</title>
      <author id="haiyue-song" orcid="0000-0003-1159-0918"><first>Haiyue</first><last>Song</last><affiliation>National Institute of Information and Communications Technology (NICT)</affiliation></author>
      <author><first>Johannes</first><last>Eschbach-Dymanus</last><affiliation>SAP SE</affiliation></author>
      <author id="hour-kaing" orcid="0000-0003-4389-2144"><first>Hour</first><last>Kaing</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Sumire</first><last>Honda</last></author>
      <author id="hideki-tanaka" orcid="0009-0001-4498-7017"><first>Hideki</first><last>Tanaka</last><affiliation>National Institute of Information and Communications Technology (NICT)</affiliation></author>
      <author id="bianka-buschbeck"><first>Bianka</first><last>Buschbeck</last><affiliation>SAP SE</affiliation></author>
      <author><first>Masao</first><last>Utiyama</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>677-697</pages>
      <abstract>Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose Format Reinforcement Learning (FormatRL), which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we propose StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.</abstract>
      <url hash="3bc3ecbe">2025.ijcnlp-long.38</url>
      <bibkey>song-etal-2025-structured</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>S</fixed-case>tu<fixed-case>D</fixed-case>: A Multimodal Approach for Stuttering Detection with <fixed-case>RAG</fixed-case> and Fusion Strategies</title>
      <author><first>Pragya</first><last>Khanna</last></author>
      <author><first>Priyanka</first><last>Kommagouni</last></author>
      <author><first>Vamshi Raghu Simha</first><last>Narasinga</last></author>
      <author><first>Anil</first><last>Vuppala</last></author>
      <pages>698-707</pages>
      <abstract>Stuttering is a complex speech disorder that challenges both ASR systems and clinical assessment. We propose a multimodal stuttering detection and classification model that integrates acoustic and linguistic features through a two-stage fusion mechanism. Fine-tuned Wav2Vec 2.0 and HuBERT extract acoustic embeddings, which are early fused with MFCC features to capture fine-grained spectral and phonetic variations, while Llama-2 embeddings from Whisper ASR transcriptions provide linguistic context. To enhance robustness against out-of-distribution speech patterns, we incorporate Retrieval-Augmented Generation or adaptive classification. Our model achieves state-of-the-art performance on SEP-28k and FluencyBank, demonstrating significant improvements in detecting challenging stuttering events. Additionally, our analysis highlights the complementary nature of acoustic and linguistic modalities, reinforcing the need for multimodal approaches in speech disorder detection.</abstract>
      <url hash="7fe46a29">2025.ijcnlp-long.39</url>
      <bibkey>khanna-etal-2025-stud</bibkey>
    </paper>
    <paper id="40">
      <title>Deconstructing Attention: Investigating Design Principles for Effective Language Modeling</title>
      <author id="huiyin-xue" orcid="0000-0002-8705-6431"><first>Huiyin</first><last>Xue</last></author>
      <author id="nafise-sadat-moosavi" orcid="0000-0002-8332-307X"><first>Nafise Sadat</first><last>Moosavi</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Nikolaos</first><last>Aletras</last><affiliation>University of Sheffield, University of Sheffield and Amazon</affiliation></author>
      <pages>708-727</pages>
      <abstract>The success of Transformer language models is widely credited to their dot-product attention mechanism, which interweaves a set of key design principles: mixing information across positions (enabling multi-token interactions), sequence-dependent activations (where attention weights adapt to each input), a specific mathematical form (dot-product similarities plus softmax weighting), and coupling of queries and keys to evolving hidden states (grounding attention in the current layer). However, the necessity of each of these principles remains largely untested. In this work, we systematically deconstruct attention by designing controlled variants that selectively relax these principles, applied both uniformly across all layers and in hybrid architectures where only some layers retain standard attention. Our empirical analysis reveals that mechanisms for mixing tokens are indispensable, as their absence collapses models to near-random behavior, while the exact mathematical form and sequence dependency can be substantially relaxed, especially when preserved in just a subset of layers. Surprisingly, even variants that fail in isolation can achieve robust performance when interleaved with standard attention, highlighting a cooperative effect. These findings deepen our understanding of what truly underpins attention’s effectiveness and open new avenues for simplifying language models without sacrificing performance.</abstract>
      <url hash="0b72a864">2025.ijcnlp-long.40</url>
      <bibkey>xue-etal-2025-deconstructing</bibkey>
    </paper>
    <paper id="41">
      <title>Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance</title>
      <author><first>Ahmed</first><last>Alajrami</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Xingwei</first><last>Tan</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Nikolaos</first><last>Aletras</last><affiliation>University of Sheffield, University of Sheffield and Amazon</affiliation></author>
      <pages>728-742</pages>
      <abstract>Instruction-tuning plays a vital role in enhancing the task-solving abilities of large language models (LLMs), improving their usability in generating helpful responses on various tasks. However, previous work has demonstrated that they are sensitive to minor variations in instruction phrasing. In this paper, we explore whether introducing perturbations in instruction-tuning data can enhance LLMs’ resistance against noisy instructions. We focus on how instruction-tuning with perturbations, such as removing stop words or shuffling words, affects LLMs’ performance on the original and perturbed versions of widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics and potential shifts in model behavior. Surprisingly, our results suggest that instruction-tuning on perturbed instructions can, in some cases, improve downstream performance. These findings highlight the importance of including perturbed instructions in instruction-tuning, which can make LLMs more resilient to noisy user inputs.</abstract>
      <url hash="93e7db43">2025.ijcnlp-long.41</url>
      <bibkey>alajrami-etal-2025-fine</bibkey>
    </paper>
    <paper id="42">
      <title>Captions Speak Louder than Images: Generalizing Foundation Models for <fixed-case>E</fixed-case>-commerce from High-quality Multimodal Instruction Data</title>
      <author><first>Xinyi</first><last>Ling</last></author>
      <author><first>Hanwen</first><last>Du</last></author>
      <author id="bo-peng-1828" orcid="0009-0000-7569-1828"><first>Bo</first><last>Peng</last><affiliation>Google</affiliation></author>
      <author><first>Zhihui</first><last>Zhu</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author id="xia-ning" orcid="0000-0002-6842-1165"><first>Xia</first><last>Ning</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>743-768</pages>
      <abstract>Multimodal foundation models (MFMs) have demonstrated strong capabilities in e-commerce by effectively leveraging multimodal data to enhance product understanding and user experienceHowever, the development of e-commerce MFMs is hindered by two challenges: (1) the scarcity of large-scale, high-quality multimodal benchmark datasets; and (2) the lack of effective multimodal information integration methods in e-commerce. To address these challenges, we introduce MMECInstruct, the first large-scale, high-quality multimodal instruction dataset designed specifically for e-commerce MFMs. MMECInstruct comprises 75,000 samples covering 7 real-world e-commerce tasks, supporting both in-domain (IND) and out-of-domain (OOD) evaluations. Leveraging MMECInstruct, we develop CASLIE, a lightweight framework that enhances multimodal information understanding and integration for e-commerce. Our comprehensive evaluation demonstrates that MMECInstruct endows CASLIE with advanced capability and strong generalizability in e-commerce applications. MMECInstruct and CASLIE models are publicly accessible through https://github.com/ninglab/CASLIE.</abstract>
      <url hash="82fcbeba">2025.ijcnlp-long.42</url>
      <bibkey>ling-etal-2025-captions</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>E</fixed-case>com<fixed-case>MMMU</fixed-case>: Strategic Utilization of Visuals for Robust Multimodal <fixed-case>E</fixed-case>-commerce Models</title>
      <author><first>Xinyi</first><last>Ling</last></author>
      <author><first>Hanwen</first><last>Du</last></author>
      <author><first>Zhihui</first><last>Zhu</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author id="xia-ning" orcid="0000-0002-6842-1165"><first>Xia</first><last>Ning</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>769-790</pages>
      <abstract>E-commerce platforms are rich in multimodal data, featuring a variety of images that depict product details. However, this raises an important question: do these images always enhance product understanding, or can they sometimes introduce redundancy or degrade performance? Existing datasets are limited in both scale and design, making it difficult to systematically examine this question. To this end, we introduce EcomMMMU, an e-commerce multimodal multitask understanding dataset with 406,190 samples and 8,989,510 images. EcomMMMU is comprised of multi-image visual-language data designed with 8 essential tasks and a specialized VSS subset to benchmark the capability of multimodal large language models (MLLMs) to effectively utilize visual content. Analysis on EcomMMMU reveals that product images do not consistently improve performance and can, in some cases, degrade it. This indicates that MLLMs may struggle to effectively leverage rich visual content for e-commerce tasks. Building on these insights, we propose SUMEI, a data-driven method that strategically utilizes multiple images via predicting visual utilities before using them for downstream tasks. Comprehensive experiments demonstrate the effectiveness and robustness of SUMEI. The data and code are available through https://github.com/ninglab/EcomMMMU.</abstract>
      <url hash="a7232b01">2025.ijcnlp-long.43</url>
      <bibkey>ling-etal-2025-ecommmmu</bibkey>
    </paper>
    <paper id="44">
      <title>Multilingual, Not Multicultural: Uncovering the Cultural Empathy Gap in <fixed-case>LLM</fixed-case>s through a Comparative Empathetic Dialogue Benchmark</title>
      <author id="woojin-lee" orcid="0009-0008-8726-6990"><first>Woojin</first><last>Lee</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author><first>Yujin</first><last>Sim</last></author>
      <author id="hongjin-kim" orcid="0000-0002-3492-2543"><first>Hongjin</first><last>Kim</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author id="harksoo-kim" orcid="0000-0002-8286-7198"><first>Harksoo</first><last>Kim</last><affiliation>Konkuk University</affiliation></author>
      <pages>791-809</pages>
      <abstract>Large Language Models (LLMs) demonstrate remarkable multilingual capabilities, yet it remains unclear whether they are truly multicultural. Do they merely process different languages, or can they genuinely comprehend the unique cultural contexts embedded within them? This study investigates this critical question by examining whether LLM’s perception of emotion and empathy differs across linguistic and cultural boundaries. To facilitate this, we introduce the Korean Empathetic Dialogues (KoED), a benchmark extending the English-based EmpatheticDialogues (ED) dataset. Moving beyond direct translation, we meticulously reconstructed dialogues specifically selected for their potential for cultural adaptation, aligning them with Korean emotional nuances and incorporating key cultural concepts like ‘jeong’ and ‘han’ that lack direct English equivalents. Our cross-cultural evaluation of leading multilingual LLMs reveals a significant “cultural empathy gap”: models consistently underperform on KoED compared to ED, struggling especially with uniquely Korean emotional expressions. Notably, the Korean-centric model, EXAONE, exhibits significantly higher cultural appropriateness. This result provides compelling evidence that aligns with the “data provenance effect”, suggesting that the cultural alignment of pre-training data is a critical factor for genuine empathetic communication. These findings demonstrate that current LLMs have cultural blind spots and underscore the necessity of benchmarks like KoED to move beyond simple linguistic fluency towards truly culturally adaptive AI systems.</abstract>
      <url hash="e8e0222f">2025.ijcnlp-long.44</url>
      <bibkey>lee-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>H</fixed-case>i<fixed-case>PPO</fixed-case>: Exploring A Novel Hierarchical Pronunciation Assessment Approach for Spoken Languages</title>
      <author id="bi-cheng-yan" orcid="0009-0002-1987-2418"><first>Bi-Cheng</first><last>Yan</last></author>
      <author><first>Hsin Wei</first><last>Wang</last></author>
      <author><first>Fu-An</first><last>Chao</last><affiliation>National Taiwan Normal University</affiliation></author>
      <author id="tien-hong-lo" orcid="0009-0006-1825-1918"><first>Tien-Hong</first><last>Lo</last><affiliation>National Taiwan Normal University</affiliation></author>
      <author><first>Yung-Chang</first><last>Hsu</last></author>
      <author id="berlin-chen" orcid="0000-0003-0693-8932"><first>Berlin</first><last>Chen</last><affiliation>National Taiwan Normal University</affiliation></author>
      <pages>810-823</pages>
      <abstract>Automatic pronunciation assessment (APA) seeks to quantify a second language (L2) learner’s pronunciation proficiency in a target language by offering timely and fine-grained diagnostic feedback. Most existing efforts on APA have predominantly concentrated on highly constrained reading-aloud tasks (where learners are prompted to read a reference text aloud); however, assessing pronunciation quality in unscripted speech (or free-speaking scenarios) remains relatively underexplored. In light of this, we first propose HiPPO, a hierarchical pronunciation assessment model tailored for spoken languages, which evaluates an L2 learner’s oral proficiency at multiple linguistic levels based solely on the speech uttered by the learner. To improve the overall accuracy of assessment, a contrastive ordinal regularizer and a curriculum learning strategy are introduced for model training. The former aims to generate score-discriminative features by exploiting the ordinal nature of regression targets, while the latter gradually ramps up the training complexity to facilitate the assessment task that takes unscripted speech as input. Experiments conducted on the Speechocean762 benchmark dataset validates the feasibility and superiority of our method in relation to several cutting-edge baselines.</abstract>
      <url hash="4067eccd">2025.ijcnlp-long.45</url>
      <bibkey>yan-etal-2025-hippo</bibkey>
    </paper>
    <paper id="46">
      <title>Positional Bias in Long-Document Ranking: Impact, Assessment, and Mitigation</title>
      <author><first>Leonid</first><last>Boytsov</last></author>
      <author><first>David</first><last>Akinpelu</last></author>
      <author id="nipun-katyal" orcid="0000-0003-3482-975X"><first>Nipun</first><last>Katyal</last></author>
      <author><first>Tianyi</first><last>Lin</last><affiliation>Google</affiliation></author>
      <author><first>Fangwei</first><last>Gao</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Yutian</first><last>Zhao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Jeffrey</first><last>Huang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author id="eric-nyberg"><first>Eric</first><last>Nyberg</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>824-856</pages>
      <abstract>We tested over 20 Transformer models for ranking long documents (including recent LongP models trained with FlashAttention and RankGPT models “powered” by OpenAI and Anthropic cloud APIs).We compared them with the simple FirstP baseline, which applied the same model to truncated input (up to 512 tokens).On MS MARCO, TREC DL, and Robust04 no long-document model outperformed FirstP by more than 5% (on average).We hypothesized that this lack of improvement is not due to inherent model limitations,but due to benchmark positional bias (most relevant passages tend to occur early in documents),which is known to exist in MS MARCO.To confirm this, we analyzed positional relevance distributions across four long-document corpora (with six query sets) and observed the same early-position bias.Surprisingly, we also found bias in six BEIR collections, which are typically categorized asshort-document datasets.We then introduced a new diagnostic dataset, MS MARCO FarRelevant, where relevant spans were deliberately placed beyond the first 512 tokens.On this dataset, many long-context models—including RankGPT—performed at random-baseline level, suggesting overfitting to positional bias.We also experimented with debiasing training data, but with limited success.Our findings (1) highlight the need for careful benchmark design in evaluating long-context models for document ranking, (2) identify model types that are more robust to positional bias, and (3) motivate further work on approaches to debias training data.We release our code and data to support further research.</abstract>
      <url hash="785be33f">2025.ijcnlp-long.46</url>
      <bibkey>boytsov-etal-2025-positional</bibkey>
    </paper>
    <paper id="47">
      <title>How Reliable are Causal Probing Interventions?</title>
      <author><first>Marc E.</first><last>Canby</last></author>
      <author id="adam-davies" orcid="0000-0002-0610-2732"><first>Adam</first><last>Davies</last></author>
      <author><first>Chirag</first><last>Rastogi</last><affiliation>Qualcomm and University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Julia</first><last>Hockenmaier</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>857-878</pages>
      <abstract>Causal probing aims to analyze foundation models by examining how intervening on their representation of various latent properties impacts their outputs. Recent works have cast doubt on the theoretical basis of several leading causal probing methods, but it has been unclear how to systematically evaluate the effectiveness of these methods in practice. To address this, we define two key causal probing desiderata: *completeness* (how thoroughly the representation of the target property has been transformed) and *selectivity* (how little non-targeted properties have been impacted). We find that there is an inherent tradeoff between the two, which we define as *reliability*, their harmonic mean. We introduce an empirical analysis framework to measure and evaluate these quantities, allowing us to make the first direct comparisons between different families of leading causal probing methods (e.g., linear vs. nonlinear, or concept removal vs. counterfactual interventions). We find that: (1) all methods show a clear tradeoff between completeness and selectivity; (2) more complete and reliable methods have a greater impact on LLM behavior; and (3) nonlinear interventions are almost always more reliable than linear interventions.Our project webpage is available at: [https://ahdavies6.github.io/causal_probing_reliability/](https://ahdavies6.github.io/causal_probing_reliability/)</abstract>
      <url hash="66719ec9">2025.ijcnlp-long.47</url>
      <bibkey>canby-etal-2025-reliable</bibkey>
    </paper>
    <paper id="48">
      <title>wav<fixed-case>CSE</fixed-case>: Learning Fixed-size Unified Speech Embeddings via Feature-based Multi-Task Learning</title>
      <author id="braveenan-sritharan" orcid="0009-0006-5053-1450"><first>Braveenan</first><last>Sritharan</last><affiliation>University of Georgia</affiliation></author>
      <author id="uthayasanker-thayasivam" orcid="0000-0002-3936-8174"><first>Uthayasanker</first><last>Thayasivam</last><affiliation>University of Moratuwa</affiliation></author>
      <pages>879-887</pages>
      <abstract>Modern speech applications require compact embeddings that generalize across both linguistic and paralinguistic tasks. However, most existing embeddings are task-specific and fail to transfer effectively across domains. We propose wavCSE, a feature-based multi-task learning model that produces a fixed-size unified speech embedding suitable for both linguistic and paralinguistic tasks. wavCSE is jointly trained on keyword spotting, speaker identification, and emotion recognition, achieving state-of-the-art performance on all three tasks. The resulting unified embedding is then evaluated on twelve downstream tasks spanning both linguistic and paralinguistic domains. Experimental results show that it outperforms strong baselines on nine of the twelve tasks, indicating effective generalization across domains. To streamline embedding generation, we introduce a recursive layer selection strategy that identifies the most informative hidden layer outputs from the upstream model and refine how these selected outputs are aggregated in the downstream model. These enhancements reduce memory usage and computational cost while improving task performance, making them broadly applicable to self-supervised learning-based speech processing models.</abstract>
      <url hash="0b1860df">2025.ijcnlp-long.48</url>
      <bibkey>sritharan-thayasivam-2025-wavcse</bibkey>
    </paper>
    <paper id="49">
      <title>Unveiling Empathic Triggers in Online Interactions via Empathy Cause Identification</title>
      <author><first>Calliope Chloe</first><last>Bandera</last></author>
      <author><first>Gyeongeun</first><last>Lee</last></author>
      <author id="natalie-parde" orcid="0000-0003-0072-7499"><first>Natalie</first><last>Parde</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>888-899</pages>
      <abstract>Effectively learning language patterns that provoke empathetic expression is vital to creating emotionally intelligent technologies; however, this problem has historically been overlooked. We address this gap by proposing the new task of empathy cause identification: a challenging task aimed at pinpointing specific triggers prompting empathetic responses in communicative settings. We correspondingly introduce AcnEmpathize-Cause, a novel dataset consisting of 4K cause-identified sentences, and explore various models to evaluate and demonstrate the dataset’s efficacy. This research not only contributes to the understanding of empathy in textual communication but also paves the way for the development of AI systems capable of more nuanced and supportive interactions.</abstract>
      <url hash="798eaeec">2025.ijcnlp-long.49</url>
      <bibkey>bandera-etal-2025-unveiling</bibkey>
    </paper>
    <paper id="50">
      <title>Assessing the Limits of In-Context Learning beyond Functions using Partially Ordered Relation</title>
      <author><first>Debanjan</first><last>Dutta</last></author>
      <author id="faizanuddin-ansari" orcid="0009-0009-5517-8846"><first>Faizanuddin</first><last>Ansari</last></author>
      <author id="swagatam-das" orcid="0000-0001-6843-4508"><first>Swagatam</first><last>Das</last></author>
      <pages>900-918</pages>
      <abstract>Generating rational and generally accurate responses to tasks, often accompanied by example demonstrations, highlights Large Language Model’s (LLM’s) remarkable In-Context Learning (ICL) capabilities without requiring updates to the model’s parameter space. Despite having an ongoing exploration focused on the inference from a document-level concept, its behavior in learning well-defined functions or relations in context needs a careful investigation. In this article, we present the performance of ICL on partially ordered relation by introducing the notion of inductively increasing complexity in prompts. In most cases, the saturated performance of the chosen metric indicates that while ICL offers some benefits, its effectiveness remains constrained as we increase the complexity in the prompts even in presence of sufficient demonstrative examples. The behavior is evident from our empirical findings and has further been theoretically justified in term of its implicit optimization process.The code is available here.</abstract>
      <url hash="fc9337e6">2025.ijcnlp-long.50</url>
      <bibkey>dutta-etal-2025-assessing</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>P</fixed-case>ro<fixed-case>S</fixed-case>witch: Knowledge-Guided Instruction Tuning to Switch Between Professional and Non-Professional Responses</title>
      <author id="chang-zong" orcid="0000-0001-7757-0659"><first>Chang</first><last>Zong</last><affiliation>Zhejiang University of Science and Technology</affiliation></author>
      <author id="yuyan-chen" orcid="0000-0002-4381-486X"><first>Yuyan</first><last>Chen</last></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jian</first><last>Shao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <author id="heng-chang" orcid="0000-0002-4978-8041"><first>Heng</first><last>Chang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yueting</first><last>Zhuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>919-935</pages>
      <abstract>Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including question answering and controlled text generation. However, studies into their ability to switch between opposite styles of responses in professional domains remain underexplored. This study introduces a novel approach, named ProSwitch, which enables a language model to switch between professional and non-professional answers, by tuning and evaluating through the guidance of domain and style knowledge. ProSwitch unfolds in three phases: LLM-augmented preparation to collect domain knowledge and QA pairs, instruction tuning to optimize LLMs with multiple levels of knowledge, and comprehensive evaluation to assess both style discrimination and reference-based quality of the generated text. Comparative analysis of ProSwitch against general and specialized LLMs reveals that our approach outperforms baselines in switching between professional and non-professional responses.</abstract>
      <url hash="4bd21e63">2025.ijcnlp-long.51</url>
      <bibkey>zong-etal-2025-proswitch</bibkey>
    </paper>
    <paper id="52">
      <title>Reasoning Models Reason Well, Until They Don’t</title>
      <author><first>Revanth</first><last>Rameshkumar</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jimson</first><last>Huang</last><affiliation>Purdue University</affiliation></author>
      <author id="yunxin-sun" orcid="0000-0003-1674-0057"><first>Yunxin</first><last>Sun</last></author>
      <author><first>Fei</first><last>Xia</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Abulhair</first><last>Saparov</last><affiliation>Purdue University</affiliation></author>
      <pages>936-956</pages>
      <abstract>Large language models (LLMs) have shown significant progress in reasoning tasks. However, recent studies show that transformers and LLMs fail catastrophically once reasoning problems exceed modest complexity. We revisit these findings through the lens of large reasoning models (LRMs)LLMs fine-tuned with incentives for step‐by‐step argumentation and self‐verification. LRM performance on graph and reasoning benchmarks such as **NLGraph** seem extraordinary, with some even claiming they are capable of generalized reasoning and innovation in reasoning-intensive fields such as mathematics, physics, medicine, and law. However, by more carefully scaling the complexity of reasoning problems, we show existing benchmarks actually have limited complexity. We develop a new dataset, the **Deep Reasoning Dataset (DeepRD)**, along with a generative process for producing unlimited examples of scalable complexity. We use this dataset to evaluate model performance on graph connectivity and natural language proof planning. We find that the performance of LRMs drop abruptly at sufficient complexity and do not generalize. We also relate our LRM results to the distributions of the complexities of large, real-world knowledge graphs, interaction graphs, and proof datasets. We find the majority of real-world examples fall inside the LRMs’ success regime, yet the long tails expose substantial failure potential. Our analysis highlights the near-term utility of LRMs while underscoring the need for new methods that generalize beyond the complexity of examples in the training distribution.</abstract>
      <url hash="6e0d5447">2025.ijcnlp-long.52</url>
      <bibkey>rameshkumar-etal-2025-reasoning</bibkey>
    </paper>
    <paper id="53">
      <title>Chain-of-Query: Unleashing the Power of <fixed-case>LLM</fixed-case>s in <fixed-case>SQL</fixed-case>-Aided Table Understanding via Multi-Agent Collaboration</title>
      <author id="songyuan-sui" orcid="0009-0004-1989-8700"><first>Songyuan</first><last>Sui</last><affiliation>Rice University</affiliation></author>
      <author><first>Hongyi</first><last>Liu</last></author>
      <author><first>Serena</first><last>Liu</last></author>
      <author id="li-li-8904" orcid="0000-0002-3365-8904"><first>Li</first><last>Li</last><affiliation>Samsung</affiliation></author>
      <author id="soo-hyun-choi" orcid="0000-0001-5768-9978"><first>Soo-Hyun</first><last>Choi</last><affiliation>Samsung Electronics America</affiliation></author>
      <author><first>Rui</first><last>Chen</last><affiliation>Samsung Electronics America</affiliation></author>
      <author id="xia-hu" orcid="0000-0003-2234-3226"><first>Xia</first><last>Hu</last><affiliation>Rice University</affiliation></author>
      <pages>957-986</pages>
      <abstract>Table understanding requires structured, multi-step reasoning. Large Language Models (LLMs) struggle with it due to the structural complexity of tabular data. Recently, multi-agent frameworks for SQL generation have shown promise in tackling the challenges of understanding tabular data, but existing approaches often suffer from limitations such as the inability to comprehend table structure for reliable SQL generation, error propagation that results in invalid queries, and over-reliance on execution correctness. To address these issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for SQL-aided table understanding. CoQ adopts natural-language-style representations of table schemas to abstract away structural noise and enhance understanding. It employs a clause-by-clause SQL generation strategy to improve query quality and introduces a hybrid reasoning division that separates SQL-based mechanical reasoning from LLM-based logical inference, thereby reducing reliance on execution outcomes. Extensive experiments across four models and five widely used benchmarks demonstrate that CoQ achieves substantial accuracy improvements and significantly lowers invalid SQL rates compared to prior generic LLM-based, SQL-aided, and hybrid baselines, confirming its superior effectiveness in table understanding. The code is available at https://github.com/SongyuanSui/ChainofQuery.</abstract>
      <url hash="b8929cf4">2025.ijcnlp-long.53</url>
      <bibkey>sui-etal-2025-chain</bibkey>
    </paper>
    <paper id="54">
      <title>Multimodal Language Models for Financial Forecasting from Interleaved Sequences of Text and Time Series</title>
      <author><first>Ross</first><last>Koval</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author id="nicholas-andrews" orcid="0000-0002-6097-9164"><first>Nicholas</first><last>Andrews</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Xifeng</first><last>Yan</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>987-1001</pages>
      <abstract>Text and time series data offer complementary views of financial markets: news articles provide narrative context about company events, while stock prices reflect how markets react to those events. However, despite their complementary nature, effectively integrating these interleaved modalities for improved forecasting remains challenging. In this work, we propose a unified neural architecture that models these interleaved sequences using modality-specific experts, allowing the model to learn unique time series patterns, while still enabling joint reasoning across modalities and preserving pretrained language understanding capabilities. To further improve multimodal understanding, we introduce a cross-modal alignment framework with a salient token weighting mechanism that learns to align representations across modalities with a focus on the most informative tokens. We demonstrate the effectiveness of our approach on a large-scale financial forecasting task, achieving state-of-the-art performance across a wide variety of strong unimodal and multimodal baselines. We develop an interpretability method that reveals insights into the value of time series-context and reinforces the design of our cross-modal alignment objective. Finally, we demonstrate that these improvements translate to meaningful economic gains in investment simulations.</abstract>
      <url hash="adb33ba9">2025.ijcnlp-long.54</url>
      <bibkey>koval-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="55">
      <title>Comparing Language Models of Different Scales for Security-Focused Tabular Query Generation and Reasoning</title>
      <author><first>Varivashya</first><last>Poladi</last></author>
      <author id="sandipan-dandapat"><first>Sandipan</first><last>Dandapat</last><affiliation>Microsoft</affiliation></author>
      <pages>1002-1016</pages>
      <abstract>Security-related data often exists in complex, multi-table formats and is scarce due to privacy and compliance constraints—posing a major challenge for training and evaluating language models (LMs) on security reasoning tasks. In this work, we systematically investigate the performance of large language models (LLMs) across different parameter scales in generating and solving multi-step, semantically rich queries over realistic security scenarios represented through three interlinked tabular datasets. We assess models on three key axes (i) their ability to formulate insightful, high complexity security questions; (ii) the quality and coherence of their reasoning chains; and (iii) their accuracy in deriving actionable answers from the underlying data. To address data scarcity, we propose a diffusion-based synthetic data generation pipeline that amplifies the existing dataset while preserving domain semantics and statistical structure. Our findings reveal that while large models often outperform in reasoning depth and query formulation, smaller models show surprising efficiency and accuracy. The study provides actionable insights for deploying generative models in security analytics and opens avenues for synthetic data-driven evaluation of LLMs in low-resource, high-stakes domains.</abstract>
      <url hash="667e2cd5">2025.ijcnlp-long.55</url>
      <bibkey>poladi-dandapat-2025-comparing</bibkey>
    </paper>
    <paper id="56">
      <title>Generate but Verify: Answering with Faithfulness in <fixed-case>RAG</fixed-case>-based Question Answering</title>
      <author><first>Simone</first><last>Filice</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author><first>Elad</first><last>Haramaty</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author id="guy-horowitz" orcid="0009-0001-5093-7235"><first>Guy</first><last>Horowitz</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author><first>Zohar</first><last>Karnin</last><affiliation>tii</affiliation></author>
      <author><first>Liane</first><last>Lewin-Eytan</last><affiliation>TII</affiliation></author>
      <author><first>Alex</first><last>Shtoff</last><affiliation>Technology Innovation Institute</affiliation></author>
      <pages>1017-1037</pages>
      <abstract>Retrieval-Augmented Generation (RAG) enhances LLMs by grounding answers in retrieved passages, which is key in factual Question Answering. However, generated answers may still be unfaithful to the passages, either due to retrieval or generation errors. Many RAG downstream applications rely on assessing answer faithfulness for applying fallback strategies, yet address it implicitly, without a consistent evaluation methodology. We introduce the task of Answering with Faithfulness (AwF), which brings faithfulness prediction to the forefront, explicitly coupling it with answer generation. We define variants of the precision and recall metrics tailored to this task, facilitating direct evaluation and comparison of different AwF methods.We then demonstrate, both theoretically and empirically, that for RAG applications using AwF as a sub-procedure, an improvement to the AwF metrics translates to an improvement to the downstream performance. This results in improved performance for recently published results.</abstract>
      <url hash="08dd22f4">2025.ijcnlp-long.56</url>
      <bibkey>filice-etal-2025-generate</bibkey>
    </paper>
    <paper id="57">
      <title>Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning Length?</title>
      <author id="celine-lee" orcid="0000-0002-8436-5124"><first>Celine</first><last>Lee</last></author>
      <author id="alexander-m-rush" orcid="0000-0002-9900-1606"><first>Alexander M</first><last>Rush</last><affiliation>Cornell University and School of Engineering and Applied Sciences, Harvard University</affiliation></author>
      <author><first>Keyon</first><last>Vafa</last></author>
      <pages>1038-1060</pages>
      <abstract>Large language models (LLMs) often benefit from verbalized reasoning at inference time, but it remains unclear which aspects of task difficulty these extra reasoning tokens address. To investigate this question, we construct a controlled setting where task complexity can be precisely manipulated to study its effect on reasoning length. Deterministic finite automata (DFAs) offer a formalism through which we can characterize task complexity through measurable properties such as run length (number of reasoning steps required) and state-space size (decision complexity). We first show that across different tasks and models of different sizes and training paradigms, there exists an optimal amount of reasoning tokens such that the probability of producing a correct solution is maximized. We then investigate which properties of complexity govern this critical length: we find that task instances with longer corresponding underlying DFA runs (i.e. demand greater latent state-tracking requirements) correlate with longer reasoning lengths, but, surprisingly, that DFA size (i.e. state-space complexity) does not. We then demonstrate an implication of these findings: being able to predict the optimal number of reasoning tokens for new problems and filtering out non-optimal length answers results in consistent accuracy improvements.</abstract>
      <url hash="b160258a">2025.ijcnlp-long.57</url>
      <bibkey>lee-etal-2025-critical</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>MAJI</fixed-case>: A Multi-Agent Workflow for Augmenting Journalistic Interviews</title>
      <author><first>Kaiwen</first><last>Guo</last></author>
      <author><first>Yimeng</first><last>Wu</last><affiliation>Sohu</affiliation></author>
      <pages>1061-1083</pages>
      <abstract>Journalistic interviews are creative, dynamic processes where success hinges on insightful, real-time questioning. While Large Language Models (LLMs) can assist, their tendency to generate coherent but uninspired questions optimizes for probable, not insightful, continuations. This paper investigates whether a structured, multi-agent approach can overcome this limitation to act as a more effective creative partner for journalists. We introduce MAJI, a system designed for this purpose, which employs a divergent-convergent architecture: a committee of specialized agents generates a diverse set of questions, and a convergent agent selects the optimal one. We evaluated MAJI against a suite of strong LLM baselines. Our results demonstrate that our multi-agent framework produces questions that are more coherent, elaborate, and original (+36.9% for our best model vs. a standard LLM baseline), exceeded strong LLM baselines on key measures of creative question quality. Most critically, in a blind survey, professional journalists preferred MAJI’s selected questions over those from the baseline by a margin of more than two to one. We present the system’s evolution, highlighting the architectural trade-offs that enable MAJI to augment, rather than simply automate, journalistic inquiry. We will release the code upon publication.</abstract>
      <url hash="b8019900">2025.ijcnlp-long.58</url>
      <bibkey>guo-wu-2025-maji</bibkey>
    </paper>
    <paper id="59">
      <title>How Aligned Are Unimodal Language and Graph Encodings of Chemical Molecules?</title>
      <author id="congfeng-cao" orcid="0000-0001-9011-3807"><first>Congfeng</first><last>Cao</last></author>
      <author><first>Zhi</first><last>Zhang</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <author id="jelke-bloem" orcid="0000-0003-2221-0554"><first>Jelke</first><last>Bloem</last><affiliation>University of Amsterdam</affiliation></author>
      <author id="khalil-simaan"><first>Khalil</first><last>Sima’an</last></author>
      <pages>1084-1097</pages>
      <abstract>Chemical molecules can be represented as graphs or as language descriptions. Training unimodal models on graphs results in different encodings than training them on language. Therefore, the existing literature force-aligns the unimodal models during training to use them in downstream applications such as drug discovery. But to what extent are <i>graph</i> and <i>language</i> unimodal model representations inherently aligned, i.e., aligned prior to any force-alignment training? Knowing this is useful for a more expedient and effective forced-alignment. For the first time, we explore methods to gauge the alignment of graph and language unimodal models. We find compelling differences between models and their ability to represent slight structural differences without force-alignment. We also present an unified unimodal alignment (<b>U2A</b>) benchmark for gauging the inherent alignment between graph and language encoders which we make available with this paper.</abstract>
      <url hash="16e5b064">2025.ijcnlp-long.59</url>
      <bibkey>cao-etal-2025-aligned</bibkey>
    </paper>
    <paper id="60">
      <title>Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models</title>
      <author id="hirohane-takagi" orcid="0009-0001-7021-3744"><first>Hirohane</first><last>Takagi</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Gouki</first><last>Minegishi</last><affiliation>University of Tokyo</affiliation></author>
      <author><first>Shota</first><last>Kizawa</last></author>
      <author><first>Issey</first><last>Sukeda</last></author>
      <author id="hitomi-yanaka" orcid="0000-0003-0354-6116"><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <pages>1098-1115</pages>
      <abstract>Although behavioral studies have documented numerical reasoning errors in large language models (LLMs), the underlying representational mechanisms remain unclear. We hypothesize that numerical attributes occupy shared latent subspaces and investigate two questions: (1) How do LLMs internally integrate multiple numerical attributes of a single entity? (2) How does irrelevant numerical context perturb these representations and their downstream outputs? To address these questions, we combine linear probing with partial correlation analysis and prompt-based vulnerability tests across models of varying sizes. Our results show that LLMs encode real-world numerical correlations but tend to systematically amplify them. Moreover, irrelevant context induces consistent shifts in magnitude representations, with downstream effects that vary by model size. These findings reveal a vulnerability in LLM decision-making and lay the groundwork for fairer, representation-aware control under multi-attribute entanglement.</abstract>
      <url hash="9b937055">2025.ijcnlp-long.60</url>
      <bibkey>takagi-etal-2025-interpreting</bibkey>
    </paper>
    <paper id="61">
      <title><fixed-case>E</fixed-case>mplif<fixed-case>AI</fixed-case>: a Fine-grained Dataset for <fixed-case>J</fixed-case>apanese Empathetic Medical Dialogues in 28 Emotion Labels</title>
      <author id="wan-jou-she" orcid="0000-0002-1549-5777"><first>Wan Jou</first><last>She</last><affiliation>Kyoto Institute of Technology</affiliation></author>
      <author><first>Lis</first><last>Pereira</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Fei</first><last>Cheng</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Sakiko</first><last>Yahata</last></author>
      <author><first>Panote</first><last>Siriaraya</last><affiliation>Kyoto Institute of Technology</affiliation></author>
      <author id="eiji-aramaki" orcid="0000-0003-0201-3609"><first>Eiji</first><last>Aramaki</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>1116-1131</pages>
      <abstract>This paper introduces EmplifAI, a Japanese empathetic dialogue dataset designed to support patients coping with chronic medical conditions. They often experience a wide range of positive and negative emotions (e.g., hope and despair) that shift across different stages of disease management. EmplifAI addresses this complexity by providing situation-based dialogues grounded in 28 fine-grained emotion categories, adapted and validated from the GoEmotions taxonomy. The dataset includes 280 medically contextualized situations and 4,125 two-turn dialogues, collected through crowdsourcing and expert review.To evaluate emotional alignment in empathetic dialogues, we assessed model predictions on situation–dialogue pairs using BERTScore across multiple large language models (LLMs), achieving F1 scores of ≤ 0.83. Fine-tuning a baseline Japanese LLM (LLM-jp-3.1-13b-instruct4) with EmplifAI resulted in notable improvements in fluency, general empathy, and emotion-specific empathy. Furthermore, we compared the scores assigned by LLM-as-a-Judge and human raters on dialogues generated by multiple LLMs to validate our evaluation pipeline and discuss the insights and potential risks derived from the correlation analysis.</abstract>
      <url hash="c62e5e75">2025.ijcnlp-long.61</url>
      <bibkey>she-etal-2025-emplifai</bibkey>
    </paper>
    <paper id="62">
      <title>Beyond Classification: Towards Speech Emotion Reasoning with Multitask <fixed-case>A</fixed-case>udio<fixed-case>LLM</fixed-case>s</title>
      <author><first>Wenyu</first><last>Zhang</last><affiliation>I2R, A*STAR</affiliation></author>
      <author><first>Yingxu</first><last>He</last></author>
      <author><first>Geyu</first><last>Lin</last><affiliation>Institute of Infocomm Research, A*STAR</affiliation></author>
      <author><first>Zhuohan</first><last>Liu</last><affiliation>, A*STAR</affiliation></author>
      <author><first>Shuo</first><last>Sun</last><affiliation>, A*STAR</affiliation></author>
      <author id="bin-wang" orcid="0000-0001-9760-8343"><first>Bin</first><last>Wang</last></author>
      <author><first>Xunlong</first><last>Zou</last><affiliation>A*STAR</affiliation></author>
      <author><first>Jeremy H. M.</first><last>Wong</last></author>
      <author id="qiongqiong-wang" orcid="0000-0002-9903-0618"><first>Qiongqiong</first><last>Wang</last></author>
      <author><first>Hardik Bhupendra</first><last>Sailor</last><affiliation>A*STAR</affiliation></author>
      <author id="nancy-chen" orcid="0000-0003-0872-5877"><first>Nancy F.</first><last>Chen</last></author>
      <author id="aiti-aw"><first>AiTi</first><last>Aw</last><affiliation>I2R</affiliation></author>
      <pages>1132-1148</pages>
      <abstract>Audio Large Language Models (AudioLLMs) have achieved strong results in semantic tasks like speech recognition and translation, but remain limited in modeling paralinguistic cues such as emotion. Existing approaches often treat emotion understanding as a classification problem, offering little insight into the underlying rationale behind predictions. In this work, we explore emotion reasoning, a strategy that leverages the generative capabilities of AudioLLMs to enhance emotion recognition by producing semantically aligned, evidence-grounded explanations. To support this in multitask AudioLLMs, we introduce a unified framework combining reasoning-augmented data supervision, dual-encoder architecture, and task-alternating training. This approach enables AudioLLMs to effectively learn different tasks while incorporating emotional reasoning. Experiments on IEMOCAP and MELD show that our approach not only improves emotion prediction accuracy but also enhances the coherence and evidential grounding of the generated responses. Experiments on two out-of-domain datasets demonstrate the generalization capabilities of the resulting model.</abstract>
      <url hash="618b4d7e">2025.ijcnlp-long.62</url>
      <bibkey>zhang-etal-2025-beyond-classification</bibkey>
    </paper>
    <paper id="63">
      <title>On the Convergence of Moral Self-Correction in Large Language Models</title>
      <author><first>Guangliang</first><last>Liu</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Haitao</first><last>Mao</last></author>
      <author><first>Bochuan</first><last>Cao</last></author>
      <author><first>Xitong</first><last>Zhang</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Zhiyu</first><last>Xue</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Rongrong</first><last>Wang</last><affiliation>Michigan State University</affiliation></author>
      <author id="kristen-johnson"><first>Kristen</first><last>Johnson</last><affiliation>Michigan State University</affiliation></author>
      <pages>1149-1165</pages>
      <abstract>Large Language Models (LLMs) are able to improve their responses when instructed to do so, a capability known as self-correction. When instructions provide only a general and abstract goal without specific details about potential issues in the response, LLMs must rely on their internal knowledge to improve response quality, a process referred to as intrinsic self-correction. The empirical success of intrinsic self-correction is evident in various applications, but how and why it is effective remains unknown. Focusing on moral self-correction in LLMs, we reveal a key characteristic of intrinsic self-correction: performance convergence through multi-round interactions; and provide a mechanistic analysis of this convergence behavior. Based on our experimental results and analysis, we uncover the underlying mechanism of convergence: consistently injected self-correction instructions activate moral concepts that reduce model uncertainty, leading to converged performance as the activated moral concepts stabilize over successive rounds. This paper demonstrates the strong potential of moral self-correction by showing that it exhibits a desirable property of converged performance.</abstract>
      <url hash="2781dc56">2025.ijcnlp-long.63</url>
      <bibkey>liu-etal-2025-convergence</bibkey>
    </paper>
    <paper id="64">
      <title><fixed-case>F</fixed-case>ast<fixed-case>VLM</fixed-case>: Self-Speculative Decoding for Fast Vision-Language Model Inference</title>
      <author><first>Divya Jyoti</first><last>Bajpai</last></author>
      <author id="manjesh-kumar-hanawal" orcid="0000-0002-1807-5487"><first>Manjesh Kumar</first><last>Hanawal</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <pages>1166-1183</pages>
      <abstract>Vision-language Models (VLMs) have made significant strides in visual understanding and query response generation, but often face challenges of high computational cost and inference latency due to autoregressive decoding. In this work, we introduce an imitation-learning-based Self-Speculative Decoding (SSD) framework, named FastVLM, to address these limitations. Our approach employs a lightweight draft model for token generation in an autoregressive manner, while a full model verifies these tokens non-autoregressively. Accepted tokens proceed seamlessly, while rejected tokens are corrected by the full model and used to guide the draft model’s refinement. Through an imitation network, FastVLM enhances the draft model by integrating deeper-level insights from the full model’s architecture. Also, it maintains the performance integrity of the full model while training the draft model, achieving a balance between efficiency and accuracy. Our method speeds up the inference process by <tex-math>1.55-1.85\times</tex-math> as compared to the final layer with minimal loss in performance. The source code is available at https://github.com/Div290/SSD.</abstract>
      <url hash="98b4c0f0">2025.ijcnlp-long.64</url>
      <bibkey>bajpai-hanawal-2025-fastvlm</bibkey>
    </paper>
    <paper id="65">
      <title>Beyond Memorization: Assessing Semantic Generalization in Large Language Models Using Phrasal Constructions</title>
      <author id="wesley-scivetti" orcid="0009-0000-8378-2754"><first>Wesley</first><last>Scivetti</last></author>
      <author><first>Melissa</first><last>Torgbi</last></author>
      <author><first>Mollie</first><last>Shichman</last></author>
      <author><first>Taylor</first><last>Pellegrin</last></author>
      <author><first>Austin</first><last>Blodgett</last></author>
      <author id="claire-bonial"><first>Claire</first><last>Bonial</last><affiliation>Georgetown University and Army Research Lab</affiliation></author>
      <author id="harish-tayyar-madabushi" orcid="0000-0001-5260-3653"><first>Harish</first><last>Tayyar Madabushi</last><affiliation>University of Bath</affiliation></author>
      <pages>1184-1201</pages>
      <abstract>The web-scale of pretraining data has created an important evaluation challenge: to disentangle linguistic competence on cases well-represented in pretraining data from generalization to out-of-domain language, specifically the dynamic, real-world instances less common in pretraining data. To this end, we construct a diagnostic evaluation to systematically assess natural language understanding in LLMs by leveraging Construction Grammar (CxG). CxG provides a psycholinguistically grounded framework for testing generalization, as it explicitly links syntactic forms to abstract, non-lexical meanings. Our novel inference evaluation dataset consists of English phrasal constructions, for which speakers are known to be able to abstract over commonplace instantiations in order to understand and produce creative instantiations. Our evaluation dataset uses CxG to evaluate two central questions: first, if models can “understand” the semantics of sentences for instances that are likely to appear in pretraining data less often, but are intuitive and easy for people to understand. Second, if LLMs can deploy the appropriate constructional semantics given constructions that are syntactically identical but with divergent meanings. Our results demonstrate that state-of-the-art models, including GPT-o1, exhibit a performance drop of over 40% on our second task, revealing a failure to generalize over syntactically identical forms to arrive at distinct constructional meanings in the way humans do. We make our novel dataset and associated experimental data, including prompts and model responses, publicly available.</abstract>
      <url hash="81be4e4b">2025.ijcnlp-long.65</url>
      <bibkey>scivetti-etal-2025-beyond</bibkey>
    </paper>
    <paper id="66">
      <title>Improving Sign Language Understanding with a Multi-Stream Masked Autoencoder Trained on <fixed-case>ASL</fixed-case> Videos</title>
      <author id="junwen-mo" orcid="0000-0002-5189-269X"><first>Junwen</first><last>Mo</last><affiliation>The University of Tokyo</affiliation></author>
      <author id="minhduc-vo" orcid="0000-0003-4839-032X"><first>MinhDuc</first><last>Vo</last><affiliation>The University of Tokyo, Tokyo Institute of Technology</affiliation></author>
      <author><first>Hideki</first><last>Nakayama</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>1202-1218</pages>
      <abstract>Sign language understanding remains a significant challenge, particularly for low-resource sign languages with limited annotated data. Motivated by the success of large-scale pretraining in deep learning, we propose Multi-Stream Masked Autoencoder (MS-MAE) — a simple yet effective framework for learning sign language representations from skeleton-based video data. We pretrained a model with MS-MAE on the YouTube-ASL dataset, and then adapted it to multiple downstream tasks across different sign languages. Experimental results show that MS-MAE achieves competitive or superior performance on a range of isolated sign language recognition benchmarks and gloss-free sign language translation tasks across several sign languages. These findings highlight the potential of leveraging large-scale, high-resource sign language data to boost performance in low-resource sign language scenarios. Additionally, visualization of the model’s attention maps reveals its ability to cluster adjacent pose sequences within a sentence, some of which align with individual signs, offering insights into the mechanisms underlying successful transfer learning.</abstract>
      <url hash="ffe7d56d">2025.ijcnlp-long.66</url>
      <bibkey>mo-etal-2025-improving</bibkey>
    </paper>
    <paper id="67">
      <title>Quantifying Phonosemantic Iconicity Distributionally in 6 Languages</title>
      <author><first>George</first><last>Flint</last></author>
      <author><first>Kaustubh</first><last>Kislay</last></author>
      <pages>1219-1237</pages>
      <abstract>Language is, as commonly theorized, largely arbitrary. Yet, systematic relationships between phonetics and semantics have been observed in many specific cases. To what degree could those systematic relationships manifest themselves in large scale, quantitative investigations–both in previously identified and unidentified phenomena? This work undertakes a distributional approach to quantifying phonosemantic iconicity at scale across 6 diverse languages (English, Spanish, Hindi, Finnish, Turkish, and Tamil). In each language, we analyze the alignment of morphemes’ phonetic and semantic similarity spaces with a suite of statistical measures, and discover an array of interpretable phonosemantic alignments not previously identified in the literature, along with crosslinguistic patterns. We also analyze 5 previously hypothesized phonosemantic alignments, finding support for some such alignments and mixed results for others.</abstract>
      <url hash="acba0710">2025.ijcnlp-long.67</url>
      <bibkey>flint-kislay-2025-quantifying</bibkey>
    </paper>
    <paper id="68">
      <title>Fine-grained Confidence Estimation for Spurious Correctness Detection in Large Language Models</title>
      <author><first>Ai</first><last>Ishii</last><affiliation>Japan Advanced Institute of Science and Technology, Tokyo Institute of Technology and BIPROGY</affiliation></author>
      <author><first>Naoya</first><last>Inoue</last><affiliation>Ichikara, RIKEN and Japan Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Hisami</first><last>Suzuki</last><affiliation>NII, National Institute of Informatics</affiliation></author>
      <author id="satoshi-sekine" orcid="0000-0002-6837-4098"><first>Satoshi</first><last>Sekine</last><affiliation>NII</affiliation></author>
      <pages>1238-1257</pages>
      <abstract>In the deployment of Large Language Models (LLMs), “spurious correctness”—where answers are correct but reasoning contains errors—poses a critical risk by creating an illusion of reliability. While prior work on LLM confidence estimation focuses on answer-level or entire reasoning path confidence, these coarse-grained approaches fail to identify which specific parts of the reasoning contain errors. We propose a fine-grained confidence estimation framework that computes confidence scores for individual evidence triplets within reasoning chains, enabling precise localization of errors. Using carefully designed prompts, we generate answers, evidence in triplet format, and their respective confidence scores simultaneously, allowing automatic detection of spurious correctness patterns where partial evidence contains factual errors. Evaluated on both Japanese and English multi-hop QA benchmarks across multiple models from three model families representing different architectures and training approaches, we show that our approach exhibits superior calibration performance for evidence confidence and demonstrates effective ability to detect spurious correct answers (up to 0.84 on our primary discrimination metric). The consistent improvements across languages demonstrate the generalizability of our method. As a secondary benefit, joint generation of confidence scores improves answer confidence calibration by up to 43%. This prompt-based approach requires no model retraining and is immediately applicable to existing LLMs.</abstract>
      <url hash="cb7eb666">2025.ijcnlp-long.68</url>
      <bibkey>ishii-etal-2025-fine</bibkey>
    </paper>
    <paper id="69">
      <title>Observing Micromotives and Macrobehavior of Large Language Models</title>
      <author><first>Yuyang</first><last>Cheng</last></author>
      <author><first>Xingwei</first><last>Qu</last><affiliation>University of Manchester</affiliation></author>
      <author id="tomas-goldsack" orcid="0000-0003-2205-8193"><first>Tomas</first><last>Goldsack</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author id="chung-chi-chen" orcid="0000-0003-3680-9277"><first>Chung-Chi</first><last>Chen</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>1258-1276</pages>
      <abstract>Thomas C. Schelling, awarded the 2005 Nobel Memorial Prize in Economic Sciences, pointed out that ”individuals decisions (micromotives), while often personal and localized, can lead to societal outcomes (macrobehavior) that are far more complex and different from what the individuals intended.” The current research related to large language models’ (LLMs’) micromotives, such as preferences or biases, assumes that users will make more appropriate decisions once LLMs are devoid of preferences or biases. Consequently, a series of studies has focused on removing bias from LLMs. In the NLP community, while there are many discussions on LLMs’ micromotives, previous studies have seldom conducted a systematic examination of how LLMs may influence society’s macrobehavior. In this paper, we follow the design of Schelling’s model of segregation to observe the relationship between the micromotives and macrobehavior of LLMs. Our results indicate that, regardless of the level of bias in LLMs, a highly segregated society will emerge as more people follow LLMs’ suggestions. We hope our discussion will spark further consideration of the fundamental assumption regarding the mitigation of LLMs’ micromotives and encourage a reevaluation of how LLMs may influence users and society.</abstract>
      <url hash="93c4351f">2025.ijcnlp-long.69</url>
      <bibkey>cheng-etal-2025-observing</bibkey>
    </paper>
    <paper id="70">
      <title><fixed-case>S</fixed-case>ci<fixed-case>H</fixed-case>allu: A Multi-Granularity Hallucination Detection Dataset for Scientific Writing</title>
      <author><first>Adiba Ibnat</first><last>Hossain</last></author>
      <author><first>Sagnik Ray</first><last>Choudhury</last><affiliation>University of North Texas</affiliation></author>
      <author id="hamed-alhoori" orcid="0000-0002-4733-6586"><first>Hamed</first><last>Alhoori</last><affiliation>Northern Illinois University</affiliation></author>
      <pages>1277-1304</pages>
      <abstract>Large Language Models (LLMs) are increasingly used to support scientific writing, but their tendency to produce hallucinated content threatens academic reliability. Existing benchmarks have addressed hallucination detection in general-domain tasks, such as fact-checking or question answering, but they do not reflect the fine-grained, domain-specific needs of scientific communication. We introduce SciHallu, a dataset for identifying hallucinations in academic text at three levels of granularity: token, sentence, and paragraph. To establish a reliable ground truth, we select source passages from research papers published prior to the widespread adoption of LLMs. Our dataset includes both hallucinated and non-hallucinated paragraph instances, constructed through controlled perturbations at varying levels of noise and validated by human annotators. A rationale is paired with each instance, explaining the nature of the modification. SciHallu covers multiple academic fields, such as Computer Science, Health Sciences, and Humanities and Social Sciences. It is built using a model-guided annotation pipeline, followed by expert human validation. We evaluate state-of-the-art LLMs on both binary and fine-grained classification tasks, revealing challenges in detecting subtle hallucinations. SciHallu supports the development of context-aware systems for more trustworthy scientific content generation.</abstract>
      <url hash="6e527be4">2025.ijcnlp-long.70</url>
      <bibkey>hossain-etal-2025-scihallu</bibkey>
    </paper>
    <paper id="71">
      <title>Enhancing Investment Opinion Ranking through Argument-Based Sentiment Analysis</title>
      <author id="chung-chi-chen" orcid="0000-0003-3680-9277"><first>Chung-Chi</first><last>Chen</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author id="hen-hsen-huang" orcid="0000-0001-9169-3081"><first>Hen-Hsen</first><last>Huang</last><affiliation>Institute of Information Science, Academia Sinica</affiliation></author>
      <author id="hsin-hsi-chen" orcid="0000-0001-9757-9423"><first>Hsin-Hsi</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <author id="hiroya-takamura" orcid="0000-0002-3244-8294"><first>Hiroya</first><last>Takamura</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Ichiro</first><last>Kobayashi</last><affiliation>Ochanomizu University</affiliation></author>
      <author><first>Yusuke</first><last>Miyao</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>1305-1315</pages>
      <abstract>In the era of rapid Internet and social media development, individuals readily share their investment opinions online. The overwhelming volume of such opinions makes comprehensive evaluation impractical, highlighting the need for an effective recommendation system that can identify valuable insights. To address this challenge, we propose an argument-based sentiment analysis framework that incorporates a new perspective on opinion strength. Our approach introduces the concept of a Fuzzy Strength Degree (FSD), derived from the difference between analysts’ target and closing prices, to quantify the intensity of opinions. By integrating argument mining techniques, we further decompose each opinion into claims and premises, examine their relationships, and use these structures to evaluate the persuasive strength of the arguments. This dual strategy allows us to rank both professional and amateur investor opinions without relying on user history or social signals. Experiments show that our method works best for analyst reports, while on social media, simpler approaches based on wording and professionalism features perform better. Moreover, our analysis of professional analysts’ and traders’ behaviors reveals that top-ranked opinions are more likely to influence subsequent market actions. These findings demonstrate that argument structure and quantified opinion strength provide a novel and reliable foundation for investment opinion recommendation.</abstract>
      <url hash="525f3eed">2025.ijcnlp-long.71</url>
      <bibkey>chen-etal-2025-enhancing-investment</bibkey>
    </paper>
    <paper id="72">
      <title>A <fixed-case>J</fixed-case>apanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical <fixed-case>NLP</fixed-case></title>
      <author><first>Shinnosuke</first><last>Ono</last></author>
      <author><first>Issey</first><last>Sukeda</last></author>
      <author><first>Takuro</first><last>Fujii</last><affiliation>Nomura Research Institute, Inc.</affiliation></author>
      <author><first>Kosei</first><last>Buma</last></author>
      <author><first>Shunsuke</first><last>Sasaki</last></author>
      <pages>1316-1332</pages>
      <abstract>We present **JPharmatron**, a Japanese domain-specific large language model (LLM) for the pharmaceutical field, developed through continual pre-training on two billion Japanese pharmaceutical tokens and eight billion English biomedical tokens. For rigorous evaluation, we introduce **JPharmaBench**, a benchmark suite consisting of three new benchmarks: YakugakuQA, based on national pharmacist licensing exams; NayoseQA, which tests cross-lingual synonym and terminology normalization; and SogoCheck, a novel task involving cross-document consistency checking.We evaluate our model against open-source medical LLMs and commercial models, including GPT-4o. Experimental results show that **JPharmatron** outperforms existing open models and achieves competitive performance with commercial ones.Interestingly, even GPT-4o performs poorly on SogoCheck, suggesting that cross-sentence consistency reasoning remains an open challenge.**JPharmatron** enables secure and local model deployment for pharmaceutical tasks, where privacy and legal constraints limit the use of closed models. Besides, **JPharmaBench** offers a reproducible framework for evaluating Japanese pharmaceutical natural language processing. Together, they demonstrate the feasibility of practical and cost-efficient language models for Japanese healthcare and pharmaceutical sectors.Our model, codes, and datasets are available on HuggingFace: https://huggingface.co/collections/EQUES/jpharmatron and https://huggingface.co/collections/EQUES/jpharmabench.</abstract>
      <url hash="b843509d">2025.ijcnlp-long.72</url>
      <bibkey>ono-etal-2025-japanese</bibkey>
    </paper>
    <paper id="73">
      <title>Investigating Feasibility of Large Language Model Agent Collaboration in <fixed-case>M</fixed-case>inecraft and Comparison with Human-Human Collaboration</title>
      <author><first>Yuki</first><last>Hirota</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last><affiliation>Nagoya University and NTT</affiliation></author>
      <pages>1333-1347</pages>
      <abstract>In recent years, there has been growing interest in agents that collaborate with humans on creative tasks, and research has begun to explore such collaboration within Minecraft. However, most existing studies on agents in Minecraft focus on scenarios where an agent constructs objects independently on the basis of given instructions, making it difficult to achieve joint construction through dialogue-based cooperation with humans. Prior work, such as the Action-Utterance Model, used small-scale large language models (LLMs), which resulted in limited accuracy. In this study, we attempt to build an agent capable of collaborative construction using LLMs by integrating the framework of the Action-Utterance Model with that of Creative Agents, which leverages more recent and powerful LLMs for more accurate and flexible building. We had two agents conduct the Collaborative Garden Task through simulations and evaluate both the generated gardens and the dialogue content. Through this evaluation, we confirm that the agents are capable of producing gardens with a certain level of quality and can actively offer suggestions and assert their opinions. Furthermore, we conduct a comparative analysis with human-human collaboration to identify current challenges faced by agents and to discuss future directions for improvement toward achieving more human-like cooperative behavior.</abstract>
      <url hash="4d8d44ac">2025.ijcnlp-long.73</url>
      <bibkey>hirota-higashinaka-2025-investigating</bibkey>
    </paper>
    <paper id="74">
      <title>Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data</title>
      <author><first>Hao</first><last>Xiong</last></author>
      <author><first>Chuanyuan</first><last>Tan</last><affiliation>Soochow University</affiliation></author>
      <author><first>Wenliang</first><last>Chen</last><affiliation>Soochow University, China</affiliation></author>
      <pages>1348-1361</pages>
      <abstract>Unstructured Knowledge Editing (UKE) is crucial for updating the relevant knowledge of large language models (LLMs). It focuses on unstructured inputs, such as long or free-form texts, which are common forms of real-world knowledge. Although previous studies have proposed effective methods and tested them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2) Abnormal failure of fine-tuning (FT) based methods for UKE.To address these issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by extending two existing UKE datasets with locality test data from the unstructured and structured views. This enables a systematic evaluation of the Locality of post-edited models. Furthermore, we identify four factors that may affect the performance of FT-based methods. Based on these factors, we conduct experiments to determine how the well-performing FT-based methods should be trained for the UKE task, providing a training recipe for future research. Our experimental results indicate that the FT-based method with the optimal setting (FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art (SOTA).In batch editing scenarios, FT-UKE shows strong performance as well, with its advantage over SOTA methods increasing as the batch size grows, expanding the average metric lead from +6.78% to +10.80%. Our code and data will be released on Github.</abstract>
      <url hash="e8588cc0">2025.ijcnlp-long.74</url>
      <bibkey>xiong-etal-2025-fine</bibkey>
    </paper>
    <paper id="75">
      <title>Optimizing the Arrangement of Citations in Related Work Section</title>
      <author><first>Masashi</first><last>Oshika</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Ryohei</first><last>Sasano</last><affiliation>Nagoya University</affiliation></author>
      <pages>1362-1373</pages>
      <abstract>In related work section of a scientific paper, authors collect relevant citations and structure them into coherent paragraphs that follow a logical order. Previous studies have addressed citation recommendation and related work section generation in settings where both the citations and their order are provided in advance. However, they have not adequately addressed the optimal ordering of these citations, which is a critical step for achieving fully automated related work section generation. In this study, we propose a new task, citation arrangement, which focuses on determining the optimal order of cited papers to enable fully automated related work section generation. Our approach decomposes citation arrangement into three tasks: citation clustering, paragraph ordering, and citation ordering within a paragraph. For each task, we propose a method that uses a large language model (LLM) in combination with a graph-based technique to comprehensively consider the context of each paper and the relationships among all cited papers. The experimental results show that our method is more effective than methods that generate outputs for each task using only an LLM.</abstract>
      <url hash="b6f5338c">2025.ijcnlp-long.75</url>
      <bibkey>oshika-sasano-2025-optimizing</bibkey>
    </paper>
    <paper id="76">
      <title>Ability Transfer Through Language Mixing</title>
      <author id="petr-hyner" orcid="0009-0008-8678-7081"><first>Petr</first><last>Hyner</last><affiliation>University of Ostrava</affiliation></author>
      <author id="jan-mrogala" orcid="0009-0006-6259-4512"><first>Jan</first><last>Mrógala</last></author>
      <author><first>Jan</first><last>Hula</last><affiliation>CIIRC, Czech Technical University, Czech Technical University of Prague</affiliation></author>
      <pages>1374-1381</pages>
      <abstract>We systematically investigate cross-lingual ability transfer in language models through controlled experiments across three problem sets: algorithmic addition, graph navigation, and natural language modeling. Our experimental design creates high-resource and low-resource “language” pairs differing in vocabulary, grammar, and computational requirements. We show that training on mixed datasets consistently enables strong positive transfer, significantly improving low-resource language performance compared to training on low amount of data in isolation. We observe improvements from 0% to 100% accuracy in arithmetic tasks, from 24% to 98% accuracy in graph navigation tasks, and 69.6% perplexity reduction in natural language modeling. We demonstrate that transfer effectiveness depends on computational complexity and linguistic differences, where grammar modifications support stronger transfer than vocabulary modifications. These findings provide compelling evidence that cross-lingual ability transfer is a robust mechanism which contributes to the quality of large language models in low-resource languages.</abstract>
      <url hash="0cdebf14">2025.ijcnlp-long.76</url>
      <bibkey>hyner-etal-2025-ability</bibkey>
    </paper>
    <paper id="77">
      <title>Enhancing Training Data Quality through Influence Scores for Generalizable Classification: A Case Study on Sexism Detection</title>
      <author><first>Rabiraj</first><last>Bandyopadhyay</last><affiliation>GESIS – Leibniz Institute for the Social Sciences</affiliation></author>
      <author id="dennis-assenmacher" orcid="0000-0001-9219-1956"><first>Dennis</first><last>Assenmacher</last><affiliation>GESIS – Leibniz Institute for the Social Sciences</affiliation></author>
      <author id="jose-maria-alonso-moral" orcid="0000-0003-3673-421X"><first>Jose Maria</first><last>Alonso-Moral</last><affiliation>Universidade de Santiago de Compostela</affiliation></author>
      <author><first>Claudia</first><last>Wagner</last><affiliation>Rheinisch Westfälische Technische Hochschule Aachen</affiliation></author>
      <pages>1382-1403</pages>
      <abstract>The quality of training data is crucial for the performance of supervised machine learning models. In particular, poor annotation quality and spurious correlations between labels and features in text dataset can significantly degrade model generalization. This problem is especially pronounced in harmful language detection, where prior studies have revealed major deficiencies in existing datasets. In this work, we design and test data selection methods based on learnability measures to improve dataset quality. Using a sexism dataset with counterfactuals designed to avoid spurious correlations, we show that pruning with EL2N and PVI scores can lead to significant performance increases and outperforms submodular and random selection. Our analysis reveals that in presence of label imbalance models rely on dataset shortcuts; especially easy-to-classify sexist instances and hard-to-classify non-sexist instances contain shortcuts. Pruning these instances leads to performances increases. Pruning hard-to-classify instances is in general a promising strategy as well when shortcuts are not present.</abstract>
      <url hash="12acad79">2025.ijcnlp-long.77</url>
      <bibkey>bandyopadhyay-etal-2025-enhancing</bibkey>
      <revision id="1" href="2025.ijcnlp-long.77v1" hash="a5ec5b7d"/>
      <revision id="2" href="2025.ijcnlp-long.77v2" hash="12acad79" date="2026-01-29">Update affiliations.</revision>
    </paper>
    <paper id="78">
      <title>Mitigating Label Length Bias in Large Language Models</title>
      <author id="mario-sanz-guerrero" orcid="0009-0001-3581-8369"><first>Mario</first><last>Sanz-Guerrero</last><affiliation>Johannes-Gutenberg Universität Mainz</affiliation></author>
      <author id="katharina-von-der-wense"><first>Katharina</first><last>von der Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <pages>1404-1420</pages>
      <abstract>Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call *label length bias*, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose *normalized contextual calibration* (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.</abstract>
      <url hash="b62c4828">2025.ijcnlp-long.78</url>
      <bibkey>sanz-guerrero-von-der-wense-2025-mitigating</bibkey>
    </paper>
    <paper id="79">
      <title>Social Bias in Popular Question-Answering Benchmarks</title>
      <author id="angelie-kraft" orcid="0000-0002-2980-952X"><first>Angelie</first><last>Kraft</last><affiliation>Leuphana Universität Lüneburg and Universität Hamburg</affiliation></author>
      <author id="judith-simon" orcid="0009-0007-9958-2520"><first>Judith</first><last>Simon</last><affiliation>Universität Hamburg</affiliation></author>
      <author id="sonja-schimmler" orcid="0000-0002-8786-7250"><first>Sonja</first><last>Schimmler</last><affiliation>Technische Universität Berlin and Fraunhofer FOKUS</affiliation></author>
      <pages>1421-1438</pages>
      <abstract>Question-answering (QA) and reading comprehension (RC) benchmarks are commonly used for assessing the capabilities of large language models (LLMs) to retrieve and reproduce knowledge. However, we demonstrate that popular QA and RC benchmarks do not cover questions about different demographics or regions in a representative way. We perform a content analysis of 30 benchmark papers and a quantitative analysis of 20 respective benchmark datasets to learn (1) who is involved in the benchmark creation, (2) whether the benchmarks exhibit social bias, or whether this is addressed or prevented, and (3) whether the demographics of the creators and annotators correspond to particular biases in the content. Most benchmark papers analyzed provide insufficient information about those involved in benchmark creation, particularly the annotators. Notably, just one (WinoGrande) explicitly reports measures taken to address social representation issues. Moreover, the data analysis revealed gender, religion, and geographic biases across a wide range of encyclopedic, commonsense, and scholarly benchmarks. Our work adds to the mounting criticism of AI evaluation practices and shines a light on biased benchmarks being a potential source of LLM bias by incentivizing biased inference heuristics.</abstract>
      <url hash="4c8af987">2025.ijcnlp-long.79</url>
      <bibkey>kraft-etal-2025-social</bibkey>
    </paper>
    <paper id="80">
      <title><fixed-case>P</fixed-case>roof<fixed-case>T</fixed-case>eller: Exposing recency bias in <fixed-case>LLM</fixed-case> reasoning and its side effects on communication</title>
      <author><first>Mayank</first><last>Jobanputra</last></author>
      <author id="alisa-kovtunova" orcid="0000-0001-9936-0943"><first>Alisa</first><last>Kovtunova</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Brisca</first><last>Balthes</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Fedor Grigoryevich</first><last>Pogulskiy</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Yifan</first><last>Wang</last></author>
      <author id="stefan-borgwardt" orcid="0000-0003-0924-8478"><first>Stefan</first><last>Borgwardt</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author id="vera-demberg" orcid="0000-0002-8834-0020"><first>Vera</first><last>Demberg</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>1439-1462</pages>
      <abstract>Large language models (LLMs) are increasingly applied in domains that demand reliable and interpretable reasoning. While formal methods can generate provably correct proofs, these proofs are often inaccessible to non-expert users. This raises a natural question: can LLMs, when given a verified proof, faithfully interpret its reasoning and communicate it clearly? We introduce <tex-math>ProofTeller</tex-math>, a benchmark that evaluates this ability across three tasks: (1) identifying key proof steps, (2) summarizing the reasoning, and (3) explaining the result in concise natural language. The benchmark covers three domains: _Biology_, _Drones_, and _Recipes_, representing scientific, safety-critical, and everyday reasoning scenarios. We find a consistent near-conclusion bias: LLMs tend to focus on steps closest to the final proof conclusion rather than on the most informative ones. A targeted human study confirms that explanations based on such steps are rated less appropriate for end users. These findings indicate that even when reasoning is provided, current LLMs face challenges in communicating key information in a useful manner, highlighting the need for LLMs that can communicate important details reliably.</abstract>
      <url hash="7cf264dc">2025.ijcnlp-long.80</url>
      <bibkey>jobanputra-etal-2025-proofteller</bibkey>
    </paper>
    <paper id="81">
      <title>Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical <fixed-case>RE</fixed-case></title>
      <author><first>Varvara</first><last>Arzt</last></author>
      <author id="allan-hanbury" orcid="0000-0002-7149-5843"><first>Allan</first><last>Hanbury</last><affiliation>Complexity Science Hub and Technische Universität Wien</affiliation></author>
      <author id="michael-wiegand" orcid="0000-0002-5403-1078"><first>Michael</first><last>Wiegand</last><affiliation>Universität Vienna</affiliation></author>
      <author id="gabor-recski" orcid="0000-0001-5551-3100"><first>Gabor</first><last>Recski</last><affiliation>TU Wien Institute for Information Systems Engineering</affiliation></author>
      <author><first>Terra</first><last>Blevins</last><affiliation>Northeastern University</affiliation></author>
      <pages>1463-1484</pages>
      <abstract>Analysing the generalisation capabilities of relation extraction (RE) models is crucial for assessing whether they learn robust relational patterns or rely on spurious correlations. Our cross-dataset experiments find that RE models struggle with unseen data, even within similar domains. Notably, higher intra-dataset performance does not indicate better transferability, instead often signaling overfitting to dataset-specific artefacts. Our results also show that data quality, rather than lexical similarity, is key to robust transfer, and the choice of optimal adaptation strategy depends on the quality of data available: while fine-tuning yields the best cross-dataset performance with high-quality data, few-shot in-context learning (ICL) is more effective with noisier data. However, even in these cases, zero-shot baselines occasionally outperform all cross-dataset results. Structural issues in RE benchmarks, such as single-relation per sample constraints and non-standardised negative class definitions, further hinder model transferability. We release our dataset splits with sample IDs and code for reproducibility.</abstract>
      <url hash="22e02048">2025.ijcnlp-long.81</url>
      <bibkey>arzt-etal-2025-relation</bibkey>
    </paper>
    <paper id="82">
      <title>Whose story is it? Personalizing story generation by inferring author styles</title>
      <author id="nischal-ashok-kumar" orcid="0000-0002-7022-5948"><first>Nischal</first><last>Ashok Kumar</last></author>
      <author id="chau-minh-pham"><first>Chau Minh</first><last>Pham</last></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Andrew</first><last>Lan</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <pages>1485-1540</pages>
      <abstract>Personalization is critical for improving user experience in interactive writing and educational applications, yet remains understudied in story generation. We study the task of personalizing story generation, where our goal is to mimic an author’s writing style, given other stories written by them. We collect Mythos, a dataset of 3.6k stories from 112 authors, with an average of 16 stories per author, across five distinct sources reflecting diverse story-writing settings. We propose a two-stage pipeline for personalized story generation: first, we infer authors’ implicit writing characteristics and organize them into an Author Writing Sheet, which is validated by humans to be of high quality; second, we simulate the author’s persona using tailored persona descriptions and personalized story rules. We find that stories personalized using the Author Writing Sheet outperform a non-personalized baseline, achieving a 78% win-rate in capturing authors’ past style and 59% in similarity to ground-truth author stories. Human evaluation supports these findings and further highlights trends, such as Reddit stories being easier to personalize, and the Creativity and Language Use aspects of stories being easier to personalize than the Plot.</abstract>
      <url hash="55e445b8">2025.ijcnlp-long.82</url>
      <bibkey>ashok-kumar-etal-2025-whose</bibkey>
    </paper>
    <paper id="83">
      <title>The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure</title>
      <author><first>Niyati</first><last>Bafna</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Tianjian</first><last>Li</last><affiliation>Johns Hopkins University</affiliation></author>
      <author id="kenton-murray" orcid="0000-0002-5628-1003"><first>Kenton</first><last>Murray</last><affiliation>Johns Hopkins University</affiliation></author>
      <author id="david-r-mortensen" orcid="0000-0002-3927-6851"><first>David R.</first><last>Mortensen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>David</first><last>Yarowsky</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Hale</first><last>Sirin</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>1541-1568</pages>
      <abstract>Multilingual generation with large language models (LLMs) is often of poor quality for mid- to low-resource languages, but the causes for this are not well-understood. We first demonstrate the existence of an implicit task-solving→translation pipeline for generation, whereby the model first solves the required task in a largely target-language-agnostic manner, and subsequently translates answer concepts into the intended target language. We hypothesize that the failure of the translation stage, despite task-solving success, is an important culprit for the observed low quality of final outputs, and formalize this as the translation barrier hypothesis. We quantify the extent to which either stage in the pipeline is responsible for final failure for a word translation task across 108 language pairs, and find that the translation barrier explains a dominant portion of error for a majority of language pairs, and is especially severe for low-resource target languages. Our results highlight an important bottleneck for end-to-end multilingual generation, relevant for future work seeking to improve multilinguality in LLMs.</abstract>
      <url hash="f82c2a36">2025.ijcnlp-long.83</url>
      <bibkey>bafna-etal-2025-translation</bibkey>
    </paper>
    <paper id="84">
      <title><fixed-case>T</fixed-case>a<fixed-case>CL</fixed-case>-<fixed-case>C</fixed-case>o<fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>: Task-adaptive Contrastive Learning with Cooperative Mixture of Experts for Multi-task Social Media Analysis</title>
      <author><first>Wang</first><last>Xingren</last></author>
      <author id="hongde-liu" orcid="0009-0003-3267-5002"><first>Hongde</first><last>Liu</last></author>
      <author><first/><last>Liushanhong</last></author>
      <author><first>Feiyang</first><last>Meng</last><affiliation>Zhengzhou University</affiliation></author>
      <author><first>Chenyuan</first><last>He</last></author>
      <author><first>Senbin</first><last>Zhu</last><affiliation>Zhengzhou University</affiliation></author>
      <author><first>Li</first><last>Zechen</last><affiliation>Huazhong Agricultural University</affiliation></author>
      <author id="yuxiang-jia" orcid="0000-0003-0481-0740"><first>Yuxiang</first><last>Jia</last><affiliation>Zhengzhou University</affiliation></author>
      <pages>1569-1581</pages>
      <abstract>Social media has become a crucial platform for information dissemination and opinion expression. The massive and continuous generation of user content has given rise to various natural language processing tasks, such as sentiment analysis and topic classification. However, existing mainstream approaches typically focus on modeling individual tasks in isolation, lacking systematic exploration of collaborative modeling across multiple tasks. This neglects the inherent correlations among social media tasks, thereby limiting the model’s ability to fully comprehend and exploit the rich, multi-dimensional semantic information embedded in text. To address this challenge, we propose <tex-math>\textbf{Ta}</tex-math>sk-adaptive <tex-math>\textbf{C}</tex-math>ontrastive <tex-math>\textbf{L}</tex-math>earning with <tex-math>\textbf{Co}</tex-math>operative <tex-math>\textbf{M}</tex-math>ixture <tex-math>\textbf{o}</tex-math>f <tex-math>\textbf{E}</tex-math>xperts (<tex-math>\textbf{TaCL-CoMoE}</tex-math>), a unified framework for social media multi-task learning. Specifically, we improve the gating mechanism by replacing the traditional softmax routing with sigmoid activation, enabling cooperative selection among multiple experts and mitigating the “expert monopoly” phenomenon. In addition, we introduce a task-adaptive contrastive learning strategy to further enhance the model’s ability to capture and distinguish semantic structures across different tasks. Experimental results on multiple public social media datasets demonstrate that TaCL-CoMoE consistently achieves state-of-the-art (SOTA) performance. The code is available at https://github.com/wxr2847/TaCL-CoMoE.</abstract>
      <url hash="ea5336ba">2025.ijcnlp-long.84</url>
      <bibkey>xingren-etal-2025-tacl</bibkey>
    </paper>
    <paper id="85">
      <title>Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection</title>
      <author><first>Saad</first><last>Almohaimeed</last></author>
      <author><first>Saleh</first><last>Almohaimeed</last><affiliation>King Saud University</affiliation></author>
      <author id="damla-turgut" orcid="0000-0001-8635-7198"><first>Damla</first><last>Turgut</last><affiliation>University of Central Florida</affiliation></author>
      <author id="ladislau-boloni" orcid="0000-0001-5336-9651"><first>Ladislau</first><last>Bölöni</last><affiliation>University of Central Florida</affiliation></author>
      <pages>1582-1592</pages>
      <abstract>Implicit hate speech has increasingly been recognized as a significant issue for social media platforms. While much of the research has traditionally focused on harmful speech in general, the need for generalizable techniques to detect veiled and subtle forms of hate has become increasingly pressing. Based on lexicon analysis, we hypothesize that implicit hate speech is already present in publicly available harmful speech datasets but may not have been explicitly recognized or labeled by annotators. Additionally, crowdsourced datasets are prone to mislabeling due to the complexity of the task and often influenced by annotators’ subjective interpretations. In this paper, we propose an approach to address the detection of implicit hate speech and enhance generalizability across diverse datasets by leveraging existing harmful speech datasets. Our method comprises three key components: influential sample identification, reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental results demonstrate the effectiveness of our approach in improving implicit hate detection, achieving a +12.9-point F1 score improvement compared to the baseline.</abstract>
      <url hash="f794f16b">2025.ijcnlp-long.85</url>
      <bibkey>almohaimeed-etal-2025-towards</bibkey>
    </paper>
    <paper id="86">
      <title><fixed-case>SEAG</fixed-case>raph: Unveiling the Whole Story of Paper Review Comments</title>
      <author id="jianxiang-yu" orcid="0009-0006-9900-9815"><first>Jianxiang</first><last>Yu</last><affiliation>East China Normal University</affiliation></author>
      <author id="jiaqi-tan" orcid="0009-0005-1711-9205"><first>Jiaqi</first><last>Tan</last><affiliation>East China Normal University</affiliation></author>
      <author id="zichen-ding" orcid="0009-0000-1436-0291"><first>Zichen</first><last>Ding</last></author>
      <author id="jiapeng-zhu" orcid="0009-0009-5957-1661"><first>Jiapeng</first><last>Zhu</last><affiliation>East China Normal University</affiliation></author>
      <author id="jiahao-li" orcid="0009-0005-3093-9670"><first>Jiahao</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author id="yao-cheng" orcid="0000-0001-9179-7032"><first>Yao</first><last>Cheng</last></author>
      <author><first>Qier</first><last>Cui</last><affiliation>East China Normal University</affiliation></author>
      <author id="yunshi-lan" orcid="0000-0002-0192-8498"><first>Yunshi</first><last>Lan</last><affiliation>East China Normal University</affiliation></author>
      <author id="yao-liu" orcid="0000-0002-6393-4295"><first>Yao</first><last>Liu</last><affiliation>East China Normal University</affiliation></author>
      <author id="xiang-li-2483" orcid="0009-0003-0142-2483"><first>Xiang</first><last>Li</last><affiliation>East China Normal University</affiliation></author>
      <pages>1593-1614</pages>
      <abstract>Peer review, as a cornerstone of scientific research, ensures the integrity and quality of scholarly work by providing authors with objective feedback for refinement. However, in the traditional peer review process, authors often receive vague or insufficiently detailed feedback, which provides limited assistance and leads to a more time-consuming review cycle. If authors can identify some specific weaknesses in their paper, they can not only address the reviewer’s concerns but also improve their work. This raises the critical question of how to enhance authors’ comprehension of review comments. In this paper, we present SEAGraph a novel framework developed to clarify review comments by uncovering the underlying intentions behind them. We construct two types of graphs for each paper: the semantic mind graph, which captures the author’s thought process, and the hierarchical background graph, which delineates the research domains related to the paper. A retrieval method is then designed to extract relevant content from both graphs, facilitating coherent explanations for the review comments. Extensive experiments show that SEAGraph excels in review comment understanding tasks, offering significant benefits to authors. By bridging the gap between reviewers’ critiques and authors’ comprehension, SEAGraph contributes to a more efficient, transparent, and collaborative scientific publishing ecosystem. Our code is available at https://anonymous.4open.science/r/seagraph/.</abstract>
      <url hash="c9a51b72">2025.ijcnlp-long.86</url>
      <bibkey>yu-etal-2025-seagraph</bibkey>
    </paper>
    <paper id="87">
      <title>A Comparative Study of Human-operated and <fixed-case>AI</fixed-case>-driven Guidance with a Teleoperated Mobile Robot</title>
      <author><first>Ao</first><last>Guo</last></author>
      <author id="shota-mochizuki" orcid="0009-0007-4092-8734"><first>Shota</first><last>Mochizuki</last></author>
      <author><first>Sanae</first><last>Yamashita</last></author>
      <author><first>Hoshimure</first><last>Kenya</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Jun</first><last>Baba</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last><affiliation>Nagoya University and NTT</affiliation></author>
      <pages>1615-1627</pages>
      <abstract>Recent advances in large language models (LLMs) such as GPT-4o offer the potential for enhancing AI-driven robotic interactions, but their effectiveness in mobile tour guidance remains unexplored. This study investigates the differences between human-operated and AI-driven guidance at an aquarium using Teleco, a teleoperated mobile robot, in a real-world field experiment. A total of 277 guidance sessions were collected under two modes: human-operated, where the operator controlled all dialogue, actions, and movement, and AI-driven, where GPT-4o generated responses while the operator only controlled the robot’s actions and movement. Our results indicate that human-operated guidance places greater emphasis on visitor movement, spatial positioning during observation guidance, and empathetic expressions, whereas AI-driven guidance promotes conversational engagement by frequently prompting visitors to ask questions. In addition, we found that user behaviors, including users’ gaze patterns and vocabulary richness, also serve as valuable indicators reflecting their overall experience during guidance interactions. Furthermore, empathetic expression is recognized as the key differentiating factor between the two guidance modes, significantly influencing users’ overall experience.</abstract>
      <url hash="925385db">2025.ijcnlp-long.87</url>
      <bibkey>guo-etal-2025-comparative</bibkey>
    </paper>
    <paper id="88">
      <title>R²-<fixed-case>C</fixed-case>o<fixed-case>D</fixed-case>: Understanding Text-Graph Complementarity in Relational Reasoning via Knowledge Co-Distillation</title>
      <author><first>Zhen</first><last>Wu</last></author>
      <author><first>Ritam</first><last>Dutt</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author id="luke-m-breitfeller" orcid="0009-0007-7209-0526"><first>Luke M.</first><last>Breitfeller</last><affiliation>National Institutes of Health and CMU, Carnegie Mellon University</affiliation></author>
      <author id="armineh-nourbakhsh" orcid="0009-0004-1908-8679"><first>Armineh</first><last>Nourbakhsh</last><affiliation>Apple</affiliation></author>
      <author><first>Siddharth</first><last>Parekh</last></author>
      <author id="carolyn-rose" orcid="0000-0003-1128-5155"><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>1628-1652</pages>
      <abstract>Relational reasoning lies at the core of many NLP tasks, drawing on complementary signals from text and graphs. While prior research has investigated how to leverage this dual complementarity, a detailed and systematic understanding of text-graph interplay and its effect on hybrid models remains underexplored. We take an analysis-driven approach to investigate text–graph representation complementarity via a unified architecture that supports knowledge co-distillation (CoD). We explore five tasks involving relational reasoning that differ in how text and graph structures encode the information needed to solve that task. By tracking how these dual representations evolve during training, we uncover interpretable patterns of alignment and divergence, and provide insights into when and why their integration is beneficial.</abstract>
      <url hash="ae23bb9f">2025.ijcnlp-long.88</url>
      <bibkey>wu-etal-2025-r2</bibkey>
    </paper>
    <paper id="89">
      <title><fixed-case>T</fixed-case>hai<fixed-case>OCRB</fixed-case>ench: A Task-Diverse Benchmark for Vision-Language Understanding in <fixed-case>T</fixed-case>hai</title>
      <author><first>Surapon</first><last>Nonesung</last><affiliation>SCB 10X</affiliation></author>
      <author><first>Teetouch</first><last>Jaknamon</last></author>
      <author><first>Sirinya</first><last>Chaiophat</last><affiliation>SCB 10X</affiliation></author>
      <author><first>Natapong</first><last>Nitarach</last><affiliation>SCB 10X</affiliation></author>
      <author><first>Chanakan</first><last>Wittayasakpan</last><affiliation>SCB 10X</affiliation></author>
      <author id="warit-sirichotedumrong" orcid="0000-0002-8850-3010"><first>Warit</first><last>Sirichotedumrong</last><affiliation>SCB 10X</affiliation></author>
      <author><first>Adisai</first><last>Na-Thalang</last><affiliation>SCB 10X</affiliation></author>
      <author><first>Kunat</first><last>Pipatanakul</last><affiliation>SCB 10X</affiliation></author>
      <pages>1653-1675</pages>
      <abstract>We present ThaiOCRBench, the first comprehensive benchmark for evaluating vision-language models (VLMs) on Thai text-rich visual understanding tasks. Despite recent progress in multimodal modeling, existing benchmarks predominantly focus on high-resource languages, leaving Thai underrepresented, especially in tasks requiring document structure understanding. ThaiOCRBench addresses this gap by offering a diverse, human-annotated dataset comprising 2,808 samples across 13 task categories. We evaluate a wide range of state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and open-source systems. Results show a significant performance gap, with proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source counterparts. Notably, fine-grained text recognition and handwritten content extraction exhibit the steepest performance drops among open-source models. Through detailed error analysis, we identify key challenges such as language bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a standardized framework for assessing VLMs in low-resource, script-complex settings, and provides actionable insights for improving Thai-language document understanding.</abstract>
      <url hash="b30faa95">2025.ijcnlp-long.89</url>
      <bibkey>nonesung-etal-2025-thaiocrbench</bibkey>
    </paper>
    <paper id="90">
      <title>An Adversary-Resistant Multi-Agent <fixed-case>LLM</fixed-case> System via Credibility Scoring</title>
      <author><first>Sana</first><last>Ebrahimi</last></author>
      <author id="mohsen-dehghankar" orcid="0009-0006-1687-8012"><first>Mohsen</first><last>Dehghankar</last><affiliation>University of Illinois Chicago</affiliation></author>
      <author id="abolfazl-asudeh" orcid="0000-0002-5251-6186"><first>Abolfazl</first><last>Asudeh</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>1676-1693</pages>
      <abstract>While multi-agent LLM systems show strong capabilities in various domains, they are highly vulnerable to adversarial and low-performing agents. To resolve this issue, in this paper, we introduce a general and adversary-resistant multi-agent LLM framework based on credibility scoring. We model the collaborative query-answering process as an iterative game, where the agents communicate and contribute to a final system output. Our system associates a credibility score that is used when aggregating the team outputs. The credibility scores are learned gradually based on the past contributions of each agent in query answering. Our experiments across multiple tasks and settings demonstrate our system’s effectiveness in mitigating adversarial influence and enhancing the resilience of multi-agent cooperation, even in the adversary-majority settings.</abstract>
      <url hash="1b6969b3">2025.ijcnlp-long.90</url>
      <bibkey>ebrahimi-etal-2025-adversary</bibkey>
    </paper>
    <paper id="91">
      <title>Are <fixed-case>LLM</fixed-case>s Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning</title>
      <author><first>Ying</first><last>Su</last></author>
      <author><first>Mingwen</first><last>Liu</last></author>
      <author><first>Zhijiang</first><last>Guo</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <pages>1694-1708</pages>
      <abstract>Logical reasoning is a pivotal component in the field of artificial intelligence. Proof planning, particularly in contexts requiring the validation of explanation accuracy, continues to present challenges. The recent advancement of large language models (LLMs) has led to significant progress in natural language proof planning, evolving from one-stage generators to more complex three-stage systems that include additional searchers or verifiers. While these assisted methods improve the quality of generated results, they also introduce increased search efforts and computational costs. Furthermore, the generative process itself remains underexplored. In this study, we propose a stepwise decoding approach augmented by contrastive learning to address two common errors encountered during the LLM generator’s decoding process. We fine-tune the language model using both vanilla and enhanced hard negatives to mitigate these decoding errors. Empirical results demonstrate the effectiveness of our strategy. Additionally, our further analysis reveals that even larger LLMs still struggle to generate rigorous logical chains.</abstract>
      <url hash="72707697">2025.ijcnlp-long.91</url>
      <bibkey>su-etal-2025-llms</bibkey>
    </paper>
    <paper id="92">
      <title><fixed-case>N</fixed-case>yaya<fixed-case>RAG</fixed-case>: Realistic Legal Judgment Prediction with <fixed-case>RAG</fixed-case> under the <fixed-case>I</fixed-case>ndian Common Law System</title>
      <author id="shubham-kumar-nigam" orcid="0000-0003-2564-7866"><first>Shubham Kumar</first><last>Nigam</last><affiliation>University of Birmingham</affiliation></author>
      <author><first>Balaramamahanthi Deepak</first><last>Patnaik</last></author>
      <author><first>Shivam</first><last>Mishra</last></author>
      <author><first>Ajay Varghese</first><last>Thomas</last></author>
      <author><first>Noel</first><last>Shallum</last><affiliation>Symbiosis Law School Pune</affiliation></author>
      <author><first>Kripabandhu</first><last>Ghosh</last><affiliation>Indian Institute of Science Education and Research Kolkata</affiliation></author>
      <author id="arnab-bhattacharya" orcid="0000-0001-7331-0788"><first>Arnab</first><last>Bhattacharya</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>1709-1726</pages>
      <abstract>Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.</abstract>
      <url hash="622f7686">2025.ijcnlp-long.92</url>
      <bibkey>nigam-etal-2025-nyayarag</bibkey>
    </paper>
    <paper id="93">
      <title>Exploring Working Memory Capacity in <fixed-case>LLM</fixed-case>s: From Stressors to Human-Inspired Strategies</title>
      <author><first>Eunjin</first><last>Hong</last></author>
      <author><first>Sumin</first><last>Cho</last></author>
      <author><first>Juae</first><last>Kim</last><affiliation>Hankuk University of Foreign Studies</affiliation></author>
      <pages>1727-1744</pages>
      <abstract>Large language models (LLMs) exhibit inherent limitations in working memory, which often affect their overall capabilities. However, prior studies have largely focused on describing such constraints without identifying their causes or providing practical strategies to cope with them. In this paper, we investigate the limited working memory capacity of LLMs through a series of empirical studies. Specifically, we examine the factors involved in the limited capacity and explore strategies to make more effective use of it. Our analysis shows that the number and difficulty of tasks in a single input largely strain the working memory of LLMs. In response, we design a cognitive marker consisting of simple token sequences theoretically grounded in cognitive science. Further analyses show that the cognitive marker reduces the overall prediction difficulty and uncertainty for the models to process the input, and its effectiveness is confirmed across various evaluation settings. Overall, our study incorporates cognitively motivated perspectives into the analysis of model behavior and highlights the need for deeper exploration of working memory in LLMs.</abstract>
      <url hash="d6d64c4c">2025.ijcnlp-long.93</url>
      <bibkey>hong-etal-2025-exploring</bibkey>
    </paper>
    <paper id="94">
      <title><fixed-case>CLASSER</fixed-case>: Cross-lingual Annotation Projection enhancement through Script Similarity for Fine-grained Named Entity Recognition</title>
      <author id="prachuryya-kaushik" orcid="0009-0007-9299-4426"><first>Prachuryya</first><last>Kaushik</last></author>
      <author id="ashish-anand" orcid="0000-0002-0024-3358"><first>Ashish</first><last>Anand</last><affiliation>Indian Institute of Technology, Guwahati</affiliation></author>
      <pages>1745-1760</pages>
      <abstract>We introduce CLASSER, a cross-lingual annotation projection framework enhanced through script similarity, to create fine-grained named entity recognition (FgNER) datasets for low-resource languages. Manual annotation for named entity recognition (NER) is expensive, and distant supervision often produces noisy data that are often limited to high-resource languages. CLASSER employs a two-stage process: first projection of annotations from high-resource NER datasets to target language by using source-to-target parallel corpora and a projection tool built on a multilingual encoder, then refining them by leveraging datasets in script-similar languages. We apply this to five low-resource Indian languages: *Assamese*, *Marathi*, *Nepali*, *Sanskrit*, and *Bodo*, a vulnerable language. The resulting dataset comprises 1.8M sentences, 2.6M entity mentions and 24.7M tokens. Through rigorous analyses, the effectiveness of our method and the high quality of the resulting dataset are ascertained with F1 score improvements of 26% in Marathi and 46% in Sanskrit over the current state-of-the-art. We further extend our analyses to zero-shot and cross-lingual settings, systematically investigating the impact of script similarity and multilingualism on cross-lingual FgNER performance. The dataset is publicly available at [huggingface.co/datasets/prachuryyaIITG/CLASSER](https://huggingface.co/datasets/prachuryyaIITG/CLASSER).</abstract>
      <url hash="a3d25e6a">2025.ijcnlp-long.94</url>
      <bibkey>kaushik-anand-2025-classer</bibkey>
    </paper>
    <paper id="95">
      <title>On the Interplay between Positional Encodings, Morphological Complexity, and Word Order Flexibility</title>
      <author id="kushal-tatariya" orcid="0009-0005-4714-5244"><first>Kushal</first><last>Tatariya</last></author>
      <author><first>Wessel</first><last>Poelman</last><affiliation>KU Leuven</affiliation></author>
      <author id="miryam-de-lhoneux" orcid="0000-0001-8844-2126"><first>Miryam</first><last>de Lhoneux</last><affiliation>KU Leuven</affiliation></author>
      <pages>1761-1778</pages>
      <abstract>Language model architectures are predominately first created for English and afterwards applied to other languages. This can lead to problems for languages that are structurally different from English. We study one specific architectural choice: positional encodings. We do this through the lens of the trade-off hypothesis: the supposed interplay between morphological complexity and word order flexibility. This hypothesis states there exists a trade-off between the two: a more morphologically complex language can have a more flexible word order, and vice-versa. Positional encodings are a direct target to investigate the implications of this hypothesis in relation to language modelling. We pre-train and evaluate three monolingual model variants with absolute, relative and no position encodings for seven typologically diverse languages and evaluate on four downstream tasks. We fail to find a consistent trend with various proxies for morphological complexity and word order flexibility. Our work shows choice of tasks, languages, and metrics are essential for drawing stable conclusions.</abstract>
      <url hash="7ff70bad">2025.ijcnlp-long.95</url>
      <bibkey>tatariya-etal-2025-interplay</bibkey>
    </paper>
    <paper id="96">
      <title>Doppelganger-<fixed-case>JC</fixed-case>: Benchmarking the <fixed-case>LLM</fixed-case>s’ Understanding of Cross-Lingual Homographs between <fixed-case>J</fixed-case>apanese and <fixed-case>C</fixed-case>hinese</title>
      <author><first>Yuka</first><last>Kitamura</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Jiahao</first><last>Huang</last></author>
      <author id="akiko-aizawa"><first>Akiko</first><last>Aizawa</last><affiliation>National Institute of Informatics</affiliation></author>
      <pages>1779-1794</pages>
      <abstract>The recent development of LLMs is remarkable, but they still struggle to handle cross-lingual homographs effectively. This research focuses on the cross-lingual homographs between Japanese and Chinese—the spellings of the words are the same, but their meanings differ entirely between the two languages. We introduce a new benchmark dataset named Doppelganger-JC to evaluate the ability of LLMs to handle them correctly. We provide three kinds of tasks for evaluation: word meaning tasks, word meaning in context tasks, and translation tasks. Through the evaluation, we found that LLMs’ performance in understanding and using homographs is significantly inferior to that of humans. We pointed out the significant issue of homograph shortcut, which means that the model tends to preferentially interpret the cross-lingual homographs in its easy-to-understand language. We investigate the potential cause of this homograph shortcut from a linguistic perspective and pose that it is difficult for LLMs to recognize a word as a cross-lingual homograph, especially when it shares the same part-of-speech (POS) in both languages. The data and code is publicly available here: <url>https://github.com/0017-alt/Doppelganger-JC.git</url>.</abstract>
      <url hash="ca8a29eb">2025.ijcnlp-long.96</url>
      <bibkey>kitamura-etal-2025-doppelganger</bibkey>
    </paper>
    <paper id="97">
      <title>Don’t Take it Literally! Idiom-aware <fixed-case>V</fixed-case>ietnamese Translation via In-context Learning</title>
      <author id="luan-thanh-nguyen" orcid="0000-0003-4882-8336"><first>Luan</first><last>Thanh Nguyen</last><affiliation>University of Information Technology, Vietnam National University Ho Chi Minh City</affiliation></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>1795-1814</pages>
      <abstract>The translation of idiomatic expressions often results in misunderstandings and inaccuracies, affecting everyday communication as well as machine translation systems. This paper introduces Idiom-aware Vietnamese Translation (IDiAT), a new framework for the evaluation of idiomatic translation for Vietnamese, along with state-of-the-art results for this task. We collect and curate a high-quality Vietnamese-English idiom set that serves as a resource for in-context learning (ICL). IDiAT’s evaluation benchmark includes both idiomatic and non-idiomatic text pairs to assess general translation quality and idiomatic translation performance. We leverage ICL in large language models to augment few-shot demonstrations with idiom and topic descriptions and consequently improve the translation accuracy. Empirical results demonstrate that our IDiAT-based ICL outperforms traditional supervised methods using only a few data samples. Multiple evaluations confirm the effectiveness of our proposed approach. Though focusing on the Vietnamese language, our approach advances idiomatic translation and contributes to the development of culturally aware translation systems, paving the way for future research in low-resource languages. The experimental materials are publicly available.</abstract>
      <url hash="fb94e0f7">2025.ijcnlp-long.97</url>
      <bibkey>thanh-nguyen-kordjamshidi-2025-dont</bibkey>
    </paper>
    <paper id="98">
      <title><fixed-case>LLM</fixed-case>s Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis</title>
      <author id="favour-y-aghaebe" orcid="0009-0004-7255-3787"><first>Favour Y.</first><last>Aghaebe</last></author>
      <author id="elizabeth-a-williams" orcid="0000-0002-1431-7549"><first>Elizabeth A</first><last>Williams</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Tanefa</first><last>Apekey</last><affiliation>University of Sheffield</affiliation></author>
      <author id="nafise-sadat-moosavi" orcid="0000-0002-8332-307X"><first>Nafise Sadat</first><last>Moosavi</last><affiliation>University of Sheffield</affiliation></author>
      <pages>1815-1833</pages>
      <abstract>Clinical interventions often hinge on age: medications and procedures safe for adults may be harmful to children or ineffective for older adults. However, as language models are increasingly integrated into biomedical evidence synthesis workflows, it remains uncertain whether these systems preserve such crucial demographic distinctions. To address this gap, we evaluate how well state-of-the-art language models retain age-related information when generating abstractive summaries of biomedical studies.We construct DemogSummary, a novel age-stratified dataset of systematic review primary studies, covering child, adult, and older adult populations. We evaluate three prominent summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed Demographic Salience Score (DSS), which quantifies age-related entity retention and hallucination.Our results reveal systematic disparities across models and age groups: demographic fidelity is lowest for adult-focused summaries, and underrepresented populations are more prone to hallucinations. These findings highlight the limitations of current LLMs in faithful and bias-free summarisation and point to the need for fairness-aware evaluation frameworks and summarisation pipelines in biomedical NLP.</abstract>
      <url hash="1cc6f6b6">2025.ijcnlp-long.98</url>
      <bibkey>aghaebe-etal-2025-llms</bibkey>
    </paper>
    <paper id="99">
      <title><fixed-case>KERLQA</fixed-case>: Knowledge-Enhanced Reinforcement Learning for Question Answering in Low-resource Languages</title>
      <author><first>Sello</first><last>Ralethe</last><affiliation>University of Cape Town</affiliation></author>
      <author id="jan-buys" orcid="0000-0003-1994-5832"><first>Jan</first><last>Buys</last><affiliation>University of Cape Town</affiliation></author>
      <pages>1834-1846</pages>
      <abstract>Question answering in low-resource languages faces critical challenges when models encounter questions beyond their knowledge boundaries, often producing confident but incorrect answers. We propose Knowledge-Enhanced Reinforcement Learning for Question Answering (KERLQA), a novel approach that combines knowledge graph integration with reinforcement learning to enable principled abstention decisions. Unlike existing refusal-tuned methods that make binary decisions based solely on internal confidence, KERLQA implements a three-way decision process: answer with internal knowledge, answer with external knowledge assistance, or abstain. Using a composite reward function that jointly optimizes for correctness, appropriate abstention, and efficient knowledge utilization, we train policies via PPO and DPO with dynamic calibration for low-resource settings. Experiments on CommonsenseQA and OpenBookQA across English and four South African languages show KERLQA achieves improved F1 scores, with up to 6.2 point improvements in low-resource languages. Our analysis reveals that KERLQA reduces false positive abstention rates by 30% while expanding the boundary of answerable questions through external knowledge integration.</abstract>
      <url hash="e223716d">2025.ijcnlp-long.99</url>
      <bibkey>ralethe-buys-2025-kerlqa</bibkey>
    </paper>
    <paper id="100">
      <title>The Visual Counter <fixed-case>T</fixed-case>uring Test (<fixed-case>VCT</fixed-case>²): A Benchmark for Evaluating <fixed-case>AI</fixed-case>-Generated Image Detection and the Visual <fixed-case>AI</fixed-case> Index (<fixed-case>V</fixed-case>_<fixed-case>AI</fixed-case>)</title>
      <author><first>Nasrin</first><last>Imanpour</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Abhilekh</first><last>Borah</last></author>
      <author><first>Shashwat</first><last>Bajpai</last></author>
      <author><first>Subhankar</first><last>Ghosh</last></author>
      <author><first>Sainath Reddy</first><last>Sankepally</last></author>
      <author id="hasnat-md-abdullah" orcid="0000-0003-1928-7358"><first>Hasnat Md</first><last>Abdullah</last></author>
      <author><first>Nishoak</first><last>Kosaraju</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Shreyas</first><last>Dixit</last></author>
      <author><first>Ashhar</first><last>Aziz</last></author>
      <author><first>Shwetangshu</first><last>Biswas</last><affiliation>University of South Carolina, National Institute of Technology Silchar, National Institute of Technology Silchar and National Institute of Technology Silchar</affiliation></author>
      <author><first>Vinija</first><last>Jain</last><affiliation>Facebook</affiliation></author>
      <author id="aman-chadha" orcid="0000-0001-6621-9003"><first>Aman</first><last>Chadha</last><affiliation>Amazon Web Services</affiliation></author>
      <author id="song-wang-5295" orcid="0000-0003-4152-5295"><first>Song</first><last>Wang</last><affiliation>University of South Carolina</affiliation></author>
      <author id="amit-p-sheth" orcid="0000-0002-0021-5293"><first>Amit</first><last>Sheth</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Amitava</first><last>Das</last><affiliation>University of South Carolina</affiliation></author>
      <pages>1847-1862</pages>
      <abstract>The rapid progress and widespread availability of text-to-image (T2I) generation models have heightened concerns about the misuse of AI-generated visuals, particularly in the context of misinformation campaigns. Existing AI-generated image detection (AGID) methods often overfit to known generators and falter on outputs from newer or unseen models. To systematically address this generalization gap, we introduce the Visual Counter Turing Test (VCT^2), a comprehensive benchmark of 166,000 images, comprising both real and synthetic prompt-image pairs produced by six state-of-the-art (SoTA) T2I systems: Stable Diffusion 2.1, SDXL, SD3 Medium, SD3.5 Large, DALL·E 3, and Midjourney 6. We curate two distinct subsets: COCO_AI, featuring structured captions from MS COCO, and Twitter_AI, containing narrative-style tweets from The New York Times. Under a unified zero-shot evaluation, we benchmark 17 leading AGID models and observe alarmingly low detection accuracy, 58% on COCO_AI and 58.34% on Twitter_AI. To transcend binary classification, we propose the Visual AI Index (V_AI), an interpretable, prompt-agnostic realism metric based on twelve low-level visual features, enabling us to quantify and rank the perceptual quality of generated outputs with greater nuance. Correlation analysis reveals a moderate inverse relationship between V_AI and detection accuracy: Pearson rho of -0.532 on COCO_AI and rho of -0.503 on Twitter_AI; suggesting that more visually realistic images tend to be harder to detect, a trend observed consistently across generators. We release COCO_AI and Twitter_AI to catalyze future advances in robust AGID and perceptual realism assessment.</abstract>
      <url hash="4cfc3e37">2025.ijcnlp-long.100</url>
      <bibkey>imanpour-etal-2025-visual</bibkey>
    </paper>
    <paper id="101">
      <title>Hildoc: Leveraging <fixed-case>H</fixed-case>ilbert Curve Representation for Accurate and Efficient Document Retrieval</title>
      <author id="muhammad-al-qurishi" orcid="0000-0002-7594-7325"><first>Muhammad</first><last>AL-Qurishi</last><affiliation>Elm Company Research and Innovation</affiliation></author>
      <author id="zhaozhi-qian" orcid="0000-0002-4561-0342"><first>Zhaozhi</first><last>Qian</last><affiliation>Elm Europe</affiliation></author>
      <author><first>Faroq</first><last>AL-Tam</last><affiliation>ELM</affiliation></author>
      <author><first>Riad</first><last>Souissi</last></author>
      <pages>1863-1876</pages>
      <abstract>Document retrieval is a critical challenge in information retrieval systems, where the goal is to efficiently retrieve relevant documents in response to a given query. Dense retrieval methods, which utilize vector embeddings to represent semantic information, require effective indexing to ensure fast and accurate retrieval. Existing methods, such as MEVI, have attempted to address this by using hierarchical K-Means for clustering, but they often face limitations in computational efficiency and retrieval accuracy. In this paper, we introduce the Hildoc Index, a novel document indexing approach that leverages the Hilbert Curve to map document embeddings onto a one-dimensional space. This innovative representation facilitates efficient clustering using a 1D quantile-based algorithm, ensuring uniform partition sizes and preserving the inherent structure of the data. As a result, Hildoc Index not only reduces training complexity but also enhances retrieval accuracy and speed during inference. Our method can be seamlessly integrated into both dense retrieval systems and hybrid ensemble systems. Through comprehensive experiments on standard benchmarks like MSMARCO Passage and Natural Questions, we demonstrate that the Hildoc Index significantly outperforms the current state-of-the-art MEVI in terms of both retrieval speed and recall. These results underscore the Hildoc Index as a solution for fast and accurate dense document retrieval.</abstract>
      <url hash="673d33dd">2025.ijcnlp-long.101</url>
      <bibkey>al-qurishi-etal-2025-hildoc</bibkey>
    </paper>
    <paper id="102">
      <title>Rethinking what matters: Effective and Robust Multilingual Realignment for Low-Resource Languages</title>
      <author><first>Quang Phuoc</first><last>Nguyen</last></author>
      <author><first>David</first><last>Anugraha</last><affiliation>Stanford University</affiliation></author>
      <author><first>Félix</first><last>Gaschi</last><affiliation>Posos</affiliation></author>
      <author><first>Jun Bin</first><last>Cheng</last><affiliation>Ontario Tech University</affiliation></author>
      <author id="en-shiun-annie-lee" orcid="0000-0003-4592-3522"><first>En-Shiun Annie</first><last>Lee</last></author>
      <pages>1877-1905</pages>
      <abstract>Realignment is a promising strategy to improve cross-lingual transfer in multilingual language models. However, empirical results are mixed and often unreliable, particularly for typologically distant or low-resource languages (LRLs) compared to English. Moreover, word realignment tools often rely on high-quality parallel data, which can be scarce or noisy for many LRLs. In this work, we conduct an extensive empirical study to investigate whether realignment truly benefits from using all available languages, or if strategically selected subsets can offer comparable or even improved cross-lingual transfer, and study the impact on LRLs. Our controlled experiments show that realignment can be particularly effective for LRLs and that using carefully selected, linguistically diverse subsets can match full multilingual alignment, and even outperform it for unseen LRLs. This indicates that effective realignment does not require exhaustive language coverage and can reduce data collection overhead, while remaining both efficient and robust when guided by informed language selection.</abstract>
      <url hash="84b11a47">2025.ijcnlp-long.102</url>
      <bibkey>nguyen-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="103">
      <title>Decode Like a Clinician: Enhancing <fixed-case>LLM</fixed-case> Fine-Tuning with Temporal Structured Data Representation</title>
      <author><first>Daniel</first><last>Fadlon</last></author>
      <author><first>David</first><last>Dov</last><affiliation>Causal Med LTD and Duke University</affiliation></author>
      <author><first>Aviya</first><last>Bennett</last></author>
      <author><first>Daphna</first><last>Heller-Miron</last></author>
      <author><first>Gad</first><last>Levy</last><affiliation>Tel Aviv University</affiliation></author>
      <author id="kfir-bar" orcid="0000-0002-1354-2955"><first>Kfir</first><last>Bar</last><affiliation>Reichman University</affiliation></author>
      <author><first>Ahuva</first><last>Weiss-Meilik</last></author>
      <pages>1906-1922</pages>
      <abstract>Predictive modeling of hospital patient data is challenging due to its structured format, irregular timing of measurements, and variation in data representation across institutions. While traditional models often struggle with such inconsistencies, Large Language Models (LLMs) offer a flexible alternative. In this work, we propose a method for verbalizing structured Electronic Health Records (EHRs) into a format suitable for LLMs and systematically examine how to include time-stamped clinical observations—such as lab tests and vital signs—from previous time points in the prompt. We study how different ways of structuring this temporal information affect predictive performance, and whether fine-tuning alone enables LLMs to effectively reason over such data. Evaluated on two real-world hospital datasets and MIMIC-IV, our approach achieves strong in-hospital and cross-hospital performance, laying the groundwork for more generalizable clinical modeling.</abstract>
      <url hash="9826565e">2025.ijcnlp-long.103</url>
      <bibkey>fadlon-etal-2025-decode</bibkey>
    </paper>
    <paper id="104">
      <title>Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces</title>
      <author id="farhan-sheth" orcid="0009-0009-9371-6983"><first>Farhan</first><last>Sheth</last></author>
      <author><first/><last>Girish</last></author>
      <author id="mohd-mujtaba-akhtar" orcid="0009-0000-1982-7110"><first>Mohd Mujtaba</first><last>Akhtar</last></author>
      <author id="muskaan-singh" orcid="0009-0008-2638-7000"><first>Muskaan</first><last>Singh</last><affiliation>University of Ulster</affiliation></author>
      <pages>1923-1932</pages>
      <abstract>In this work, we address the challenge of generalizable audio deepfake detection (ADD) across diverse speech synthesis paradigms—including conventional text-to-speech (TTS) systems and modern diffusion or flow-matching (FM) based generators. Prior work has mostly targeted individual synthesis families and often fails to generalize across paradigms due to overfitting to generation-specific artifacts. We hypothesize that synthetic speech, irrespective of its generative origin, leaves behind shared structural distortions in the embedding space that can be aligned through geometry-aware modeling. To this end, we propose RHYME, a unified detection framework that fuses utterance-level embeddings from diverse pretrained speech encoders using non-Euclidean projections. RHYME maps representations into hyperbolic and spherical manifolds—where hyperbolic geometry excels at modeling hierarchical generator families, and spherical projections capture angular, energy-invariant cues such as periodic vocoder artifacts. The fused representation is obtained via Riemannian barycentric averaging, enabling synthesis invariant alignment. RHYME outperforms individual PTMs and homogeneous fusion baselines, achieving top performance and setting new state-of-the-art in cross-paradigm ADD.</abstract>
      <url hash="c297f485">2025.ijcnlp-long.104</url>
      <bibkey>sheth-etal-2025-curved</bibkey>
    </paper>
    <paper id="105">
      <title><fixed-case>MELAC</fixed-case>: Massive Evaluation of Large Language Models with Alignment of Culture in <fixed-case>P</fixed-case>ersian Language</title>
      <author><first>Farhan</first><last>Farsi</last><affiliation>Amirkabir University of Technology</affiliation></author>
      <author><first>Farnaz</first><last>Aghababaloo</last></author>
      <author><first>Shahriar Shariati</first><last>Motlagh</last><affiliation>Mazandaran University of Iran</affiliation></author>
      <author><first>Parsa</first><last>Ghofrani</last></author>
      <author id="mohammadali-sadraeijavaheri" orcid="0000-0002-7267-4894"><first>MohammadAli</first><last>SadraeiJavaheri</last><affiliation>Part AI Research Center</affiliation></author>
      <author><first>Shayan</first><last>Bali</last></author>
      <author><first>Amir Hossein</first><last>Shabani</last></author>
      <author><first>Farbod</first><last>Bijary</last><affiliation>Amirkabir University of Technology</affiliation></author>
      <author><first>Ghazal</first><last>Zamaninejad</last></author>
      <author><first>AmirMohammad</first><last>Salehoof</last></author>
      <author><first>Saeedeh</first><last>Momtazi</last><affiliation>Amirkabir University of Technology</affiliation></author>
      <pages>1933-1950</pages>
      <abstract>As large language models (LLMs) become increasingly embedded in our daily lives, evaluating their quality and reliability across diverse contexts has become essential. While comprehensive benchmarks exist for assessing LLM performance in English, there remains a significant gap in evaluation resources for other languages. Moreover, because most LLMs are trained primarily on data rooted in European and American cultures, they often lack familiarity with non-Western cultural contexts. To address this limitation, our study focuses on the Persian language and Iranian culture. We introduce 19 new evaluation datasets specifically designed to assess LLMs on topics such as Iranian law, Persian grammar, Persian idioms, and university entrance exams. Using these datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing cultural and linguistic evaluation gap in the field. The evaluation results are publicly available on our live leaderboard: https://huggingface.co/spaces/opll-org/Open-Persian-LLM-Leaderboard</abstract>
      <url hash="5917f326">2025.ijcnlp-long.105</url>
      <bibkey>farsi-etal-2025-melac</bibkey>
    </paper>
    <paper id="106">
      <title>Atomic Consistency Preference Optimization for Long-Form Question Answering</title>
      <author id="jingfeng-chen" orcid="0009-0000-3458-5463"><first>Jingfeng</first><last>Chen</last></author>
      <author><first>Raghuveer</first><last>Thirukovalluru</last></author>
      <author><first>Junlin</first><last>Wang</last></author>
      <author><first>Kaiwei</first><last>Luo</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Apple and Duke University</affiliation></author>
      <pages>1951-1963</pages>
      <abstract>Large Language Models (LLMs) often produce factoid hallucinations - plausible yet incorrect answers. A common mitigation strategy is model alignment, which improves factual accuracy by training on curated (factual, non-factual) pairs. However, this approach often relies on a stronger model (e.g., GPT-4) or an external knowledge base to assess factual correctness that may not always be accessible. Addressing this, we propose Atomic Consistency Preference Optimization (ACPO), a self-supervised preference-tuning method that enhances factual accuracy without external supervision. ACPO leverages atomic consistency signals (i.e., the agreement of individual facts across multiple stochastic responses) to identify high- and low-quality data pairs for model alignment. Despite being fully self-supervised, ACPO outperforms the strong supervised alignment baseline by 1.95 points averaged across Phi-3 and Llama3 on the LongFact and BioGen datasets, demonstrating its effectiveness in improving factual reliability without relying on external models or knowledge bases.</abstract>
      <url hash="b43e1e74">2025.ijcnlp-long.106</url>
      <bibkey>chen-etal-2025-atomic</bibkey>
    </paper>
    <paper id="107">
      <title>Breaking Bad: Norms for Valence, Arousal, and Dominance for over 10k <fixed-case>E</fixed-case>nglish Multiword Expressions</title>
      <author id="saif-mohammad" orcid="0000-0003-2716-7516"><first>Saif M.</first><last>Mohammad</last></author>
      <pages>1964-1988</pages>
      <abstract>Factor analysis studies have shown that the primary dimensions of word meaning are Valence (V), Arousal (A), and Dominance (D). Existing lexicons such as the NRC VAD Lexicon, published in 2018, include VAD association ratings for words. Here, we present a complement to it, which has **human ratings of valence, arousal, and dominance for ∼10k English Multiword Expressions (MWEs) and their constituent words**. We also increase the coverage of unigrams, especially words that have become more common since 2018. In all, the new **NRC VAD Lexicon v2 now has entries for ∼10k MWEs and ∼25k words, in addition to the entries in v1**. We show that the associations are highly reliable. We use the lexicon to examine emotional characteristics of MWEs, including: 1. The degree to which MWEs (idioms, noun compounds, and verb particle constructions) exhibit strong emotionality; 2. The degree of emotional compositionality in MWEs. The lexicon enables a wide variety of research in NLP, Psychology, Public Health, Digital Humanities, and Social Sciences. The NRC VAD Lexicon v2 is freely available through the project web- page: http://saifmohammad.com/ WebPages/nrc-vad.html</abstract>
      <url hash="f6563a30">2025.ijcnlp-long.107</url>
      <bibkey>mohammad-2025-breaking</bibkey>
    </paper>
    <paper id="108">
      <title>A Multimodal Recaptioning Framework to Account for Perceptual Diversity Across Languages in Vision-Language Modeling</title>
      <author id="kyle-buettner" orcid="0000-0003-2635-9866"><first>Kyle</first><last>Buettner</last><affiliation>University of Pittsburgh and University of Pittsburgh</affiliation></author>
      <author><first>Jacob T.</first><last>Emmerson</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Adriana</first><last>Kovashka</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>1989-2006</pages>
      <abstract>When captioning an image, people describe objects in diverse ways, such as by using different terms and/or including details that are perceptually noteworthy to them. Descriptions can be especially unique across languages and cultures. Modern vision-language models (VLMs) gain understanding of images with text in different languages often through training on machine translations of English captions. However, this process relies on input content written from the perception of English speakers, leading to a perceptual bias. In this work, we outline a framework to address this bias. We specifically use a small amount of native speaker data, nearest-neighbor example guidance, and multimodal LLM reasoning to augment captions to better reflect descriptions in a target language. When adding the resulting rewrites to multilingual CLIP finetuning, we improve on German and Japanese text-image retrieval case studies (up to +3.5 mean recall, +4.4 on native vs. translation errors). We also propose a mechanism to build understanding of object description variation across languages, and offer insights into cross-dataset and cross-language generalization.</abstract>
      <url hash="53cb306a">2025.ijcnlp-long.108</url>
      <bibkey>buettner-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="109">
      <title>Characterizing Mamba’s Selective Memory using Auto-Encoders</title>
      <author><first>Tamanna</first><last>Hossain</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Robert L. Logan</first><last>Iv</last><affiliation>Dataminr</affiliation></author>
      <author><first>Chandrasekhara Ganesh</first><last>Jagadeesan</last><affiliation>Dataminr, Inc</affiliation></author>
      <author id="sameer-singh" orcid="0000-0003-0621-6323"><first>Sameer</first><last>Singh</last><affiliation>University of California, Irvine</affiliation></author>
      <author id="joel-tetreault"><first>Joel R.</first><last>Tetreault</last></author>
      <author><first>Alejandro</first><last>Jaimes</last></author>
      <pages>2007-2022</pages>
      <abstract>State space models (SSMs) are a promising alternative to transformers for language modeling because they use fixed memory during inference. However, this fixed memory usage requires some information loss in the hidden state when processing long sequences. While prior work has studied the sequence length at which this information loss occurs, it does not characterize the types of information SSM language models (LMs) tend to forget. In this paper, we address this knowledge gap by identifying the types of tokens (e.g., parts of speech, named entities) and sequences (e.g., code, math problems) that are more frequently forgotten by SSM LMs. We achieve this by training an auto-encoder to reconstruct sequences from the SSM’s hidden state, and measure information loss by comparing inputs with their reconstructions. We perform experiments using the Mamba family of SSM LMs (130M–1.4B) on sequences ranging from 4–256 tokens. Our results show significantly higher rates of information loss on math-related tokens (e.g., numbers, variables), mentions of organization entities, and alternative dialects to Standard American English. We then examine the frequency that these tokens appear in Mamba’s pretraining data and find that less prevalent tokens tend to be the ones Mamba is most likely to forget. By identifying these patterns, our work provides clear direction for future research to develop methods that better control Mamba’s ability to retain important information.</abstract>
      <url hash="03b203da">2025.ijcnlp-long.109</url>
      <bibkey>hossain-etal-2025-characterizing</bibkey>
    </paper>
    <paper id="110">
      <title>Task-Aligned Tool Recommendation for Large Language Models</title>
      <author id="hang-gao-7497" orcid="0009-0003-9458-7497"><first>Hang</first><last>Gao</last><affiliation>Rutgers University</affiliation></author>
      <author id="yongfeng-zhang" orcid="0000-0003-2633-8555"><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <pages>2023-2045</pages>
      <abstract>By augmenting Large Language Models (LLMs) with external tools, their capacity to solve complex problems has been significantly enhanced. However, despite ongoing advancements in the parsing capabilities of LLMs, incorporating all available tools simultaneously in the prompt remains impractical due to the vast number of external tools. Consequently, it is essential to provide LLMs with a precise set of tools tailored to the specific task, considering both quantity and quality. Current tool retrieval methods primarily focus on refining the ranking list of tools and directly packaging a fixed number of top-ranked tools as the tool set. However, these approaches often fail to equip LLMs with the optimal set of tools prior to execution, since the optimal number of tools for different tasks could be different, resulting in inefficiencies such as redundant or unsuitable tools, which impede immediate access to the most relevant tools. This paper addresses the challenge of recommending precise toolsets for LLMs. We introduce the problem of tool recommendation, define its scope, and propose a novel Precision-driven Tool Recommendation (PTR) approach. PTR captures an initial, concise set of tools by leveraging historical tool bundle usage and dynamically adjusts the tool set by performing tool matching, culminating in a multi-view-based tool addition. Additionally, we present a new dataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness of tool recommendation for LLMs. We further validate our design choices through comprehensive experiments, demonstrating promising accuracy across two open benchmarks and our RecTools dataset.</abstract>
      <url hash="4e8aba7d">2025.ijcnlp-long.110</url>
      <bibkey>gao-zhang-2025-task</bibkey>
    </paper>
    <paper id="111">
      <title><fixed-case>INTERCHART</fixed-case>: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information</title>
      <author id="anirudh-iyengar-kaniyar-narayana-iyengar" orcid="0009-0001-2368-6303"><first>Anirudh Iyengar Kaniyar Narayana</first><last>Iyengar</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Srija</first><last>Mukhopadhyay</last></author>
      <author><first>Adnan</first><last>Qidwai</last><affiliation>IBM Research</affiliation></author>
      <author><first>Shubhankar</first><last>Singh</last><affiliation>Mercer Mettl</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>2046-2067</pages>
      <abstract>We introduce InterChart, a diagnostic benchmark that evaluates how well vision-language models (VLMs) reason across multiple related charts, a task central to real-world applications such as scientific reporting, financial analysis, and public policy dashboards. Unlike prior benchmarks focusing on isolated, visually uniform charts, InterChart challenges models with diverse question types ranging from entity inference and trend correlation to numerical estimation and abstract multi-step reasoning grounded in 2–3 thematically or structurally related charts. We organize the benchmark into three tiers of increasing difficulty: (1) factual reasoning over individual charts, (2) integrative analysis across synthetically aligned chart sets, and (3) semantic inference over visually complex, real-world chart pairs. Our evaluation of state-of-the-art open- and closed-source VLMs reveals consistent and steep accuracy declines as chart complexity increases. We find that models perform better when we decompose multi-entity charts into simpler visual units, underscoring their struggles with cross-chart integration. By exposing these systematic limitations, InterChart provides a rigorous framework for advancing multimodal reasoning in complex, multi-visual environments.</abstract>
      <url hash="14cfeeb1">2025.ijcnlp-long.111</url>
      <bibkey>iyengar-etal-2025-interchart</bibkey>
    </paper>
    <paper id="112">
      <title><fixed-case>L</fixed-case>ang<fixed-case>C</fixed-case>ompress: Language-Aware Compression of Large Language Models</title>
      <author id="dieu-hien-nguyen" orcid="0000-0001-5238-733X"><first>Dieu-Hien</first><last>Nguyen</last></author>
      <author id="nguyen-khang-le" orcid="0000-0001-6585-5470"><first>Nguyen-Khang</first><last>Le</last><affiliation>Japan Advanced Institute of Science and Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Truong Dinh</first><last>Do</last></author>
      <author id="minh-le-nguyen"><first>Le-Minh</first><last>Nguyen</last><affiliation>Japan Advanced Institute of Science and Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>2068-2077</pages>
      <abstract>Large Language Models (LLMs) demonstrate strong multilingual capabilities but are costly to deploy due to their size and computational demands. To mitigate this, compression techniques such as pruning and quantization are widely used. However, these methods face two key limitations: (1) they assume access to high-quality instruction or calibration data, which is often unavailable for low-resource languages; and (2) they aim to preserve multilingual generality, making them inefficient for language-specific applications. We introduce LangCompress, a language-aware compression framework that enhances existing compression methods for targeted deployment. LangCompress is method-agnostic and improves state-of-the-art pruning and quantization approaches. It features two core components: an iterative self-supervised pipeline for generating instruction data in the target language, and a vocabulary simplification strategy that reduces the LM head to focus on key tokens. Experiments on perplexity, translation, and summarization tasks show that LangCompress improves performance in the target language. The code and data are publicly available.</abstract>
      <url hash="adf590c7">2025.ijcnlp-long.112</url>
      <bibkey>nguyen-etal-2025-langcompress</bibkey>
    </paper>
    <paper id="113">
      <title>The Confidence Paradox: Can <fixed-case>LLM</fixed-case> Know When It’s Wrong?</title>
      <author id="sahil-tripathi" orcid="0009-0009-4739-7117"><first>Sahil</first><last>Tripathi</last></author>
      <author><first>MD Tabrez</first><last>Nafis</last></author>
      <author id="imran-hussain" orcid="0000-0002-2585-8316"><first>Imran</first><last>Hussain</last><affiliation>Jamia Hamdard University</affiliation></author>
      <author id="jiechao-gao" orcid="0000-0003-0628-1416"><first>Jiechao</first><last>Gao</last></author>
      <pages>2078-2087</pages>
      <abstract>Document Visual Question Answering (DocVQA) models often produce overconfident or ethically misaligned responses, especially under uncertainty. Existing models like LayoutLMv3, UDOP, and DONUT focus on accuracy but lack ethical calibration. We propose HonestVQA, a model-agnostic, self-supervised framework that aligns model confidence with correctness using weighted loss and contrastive learning. We introduce two new metrics—Honesty Score (H-Score) and Ethical Confidence Index (ECI)—to evaluate ethical alignment. HonestVQA improves accuracy and F1 by up to 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets, while reducing overconfidence. It also generalizes well across domains, achieving 78.9% accuracy and 76.1% F1-score.</abstract>
      <url hash="8f8fd6d1">2025.ijcnlp-long.113</url>
      <bibkey>tripathi-etal-2025-confidence</bibkey>
    </paper>
    <paper id="114">
      <title><fixed-case>D</fixed-case>harma<fixed-case>B</fixed-case>ench: Evaluating Language Models on Buddhist Texts in <fixed-case>S</fixed-case>anskrit and <fixed-case>T</fixed-case>ibetan</title>
      <author><first>Kai Golan</first><last>Hashiloni</last></author>
      <author id="shay-b-cohen"><first>Shay</first><last>Cohen</last><affiliation>Reichman University</affiliation></author>
      <author><first>Asaf</first><last>Shina</last></author>
      <author><first>Jingyi</first><last>Yang</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Orr Meir</first><last>Zwebner</last><affiliation>Reichman University</affiliation></author>
      <author><first>Nicola</first><last>Bajetta</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Guy</first><last>Bilitski</last></author>
      <author><first>Rebecca</first><last>Sundén</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Guy</first><last>Maduel</last></author>
      <author><first>Ryan</first><last>Conlon</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Ari</first><last>Barzilai</last></author>
      <author><first>Daniel</first><last>Mass</last></author>
      <author><first>Shanshan</first><last>Jia</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Aviv</first><last>Naaman</last><affiliation>reichman university</affiliation></author>
      <author><first>Sonam</first><last>Choden</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Sonam</first><last>Jamtsho</last></author>
      <author><first>Yadi</first><last>Qu</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Harunaga</first><last>Isaacson</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Dorji</first><last>Wangchuk</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Shai</first><last>Fine</last><affiliation>Reichman University</affiliation></author>
      <author><first>Orna</first><last>Almogi</last><affiliation>Universität Hamburg</affiliation></author>
      <author id="kfir-bar" orcid="0000-0002-1354-2955"><first>Kfir</first><last>Bar</last><affiliation>Reichman University</affiliation></author>
      <pages>2088-2110</pages>
      <abstract>We assess the capabilities of large language models on tasks involving Buddhist texts written in Sanskrit and Classical Tibetan—two typologically distinct, low-resource historical languages. To this end, we introduce DharmaBench, a benchmark suite comprising 13 classification and detection tasks grounded in Buddhist textual traditions: six in Sanskrit and seven in Tibetan, with four shared across both. The tasks are curated from scratch, tailored to the linguistic and cultural characteristics of each language. We evaluate a range of models, from proprietary systems like GPT-4o to smaller, domain-specific open-weight models, analyzing their performance across tasks and languages. All datasets and code are publicly released, under the CC-BY-4 License and the Apache-2.0 License respectively, to support research on historical language processing and the development of culturally inclusive NLP systems.</abstract>
      <url hash="f506cfc4">2025.ijcnlp-long.114</url>
      <bibkey>hashiloni-etal-2025-dharmabench</bibkey>
    </paper>
    <paper id="115">
      <title><fixed-case>B</fixed-case>hasha<fixed-case>S</fixed-case>etu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages</title>
      <author id="subhadip-maji" orcid="0000-0002-8802-1572"><first>Subhadip</first><last>Maji</last></author>
      <author id="arnab-bhattacharya" orcid="0000-0001-7331-0788"><first>Arnab</first><last>Bhattacharya</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>2111-2129</pages>
      <abstract>Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.</abstract>
      <url hash="a7108e75">2025.ijcnlp-long.115</url>
      <bibkey>maji-bhattacharya-2025-bhashasetu</bibkey>
    </paper>
    <paper id="116">
      <title>Are Humans as Brittle as Large Language Models?</title>
      <author><first>Jiahui</first><last>Li</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Sean</first><last>Papay</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author id="roman-klinger" orcid="0000-0002-2014-6619"><first>Roman</first><last>Klinger</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <pages>2130-2155</pages>
      <abstract>The output of large language models (LLMs) is unstable, due both to non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to prompt changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs.</abstract>
      <url hash="11094a46">2025.ijcnlp-long.116</url>
      <bibkey>li-etal-2025-humans</bibkey>
    </paper>
    <paper id="117">
      <title>Large Temporal Models: Unlocking Temporal Understanding in <fixed-case>LLM</fixed-case>s for Temporal Relation Classification</title>
      <author><first>Omri</first><last>Homburger</last></author>
      <author id="kfir-bar" orcid="0000-0002-1354-2955"><first>Kfir</first><last>Bar</last><affiliation>Reichman University</affiliation></author>
      <pages>2156-2171</pages>
      <abstract>We present Large Temporal Model, a Large Language Model (LLM) that excels in Temporal Relation Classification (TRC). We show how a carefully designed fine-tuning strategy, using a novel two-step fine-tuning approach, can adapt LLMs for TRC. Our approach is focused on global TRC, enabling simultaneous classification of all temporal relations within a document. Unlike traditional pairwise methods, our approach performs global inference in a single step, improving both efficiency and consistency. Evaluations on the MATRES and OmniTemp benchmarks demonstrate that, for the first time, an LLM achieves state-of-the-art performance, outperforming previous pairwise and global TRC methods. Results show that our global approach produces more consistent and accurate temporal graphs. Ablation studies further validate the effectiveness of our two-step fine-tuning strategy, while analyses reveal why our approach succeeds in increasing performance and reducing inconsistencies.</abstract>
      <url hash="0cbb0e29">2025.ijcnlp-long.117</url>
      <bibkey>homburger-bar-2025-large</bibkey>
    </paper>
    <paper id="118">
      <title>What Are They Talking About? A Benchmark of Knowledge-Grounded Discussion Summarization</title>
      <author id="weixiao-zhou" orcid="0009-0006-8929-0834"><first>Weixiao</first><last>Zhou</last></author>
      <author id="junnan-zhu" orcid="0000-0002-9856-2946"><first>Junnan</first><last>Zhu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Gengyao</first><last>Li</last></author>
      <author id="xianfu-cheng" orcid="0000-0003-1130-8302"><first>Xianfu</first><last>Cheng</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author id="xinnian-liang" orcid="0000-0002-4744-6179"><first>Xinnian</first><last>Liang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Feifei</first><last>Zhai</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>2172-2191</pages>
      <abstract>Traditional dialogue summarization primarily focuses on dialogue content, assuming it comprises adequate information for a clear summary. However, this assumption often fails for discussions grounded in shared background, where participants frequently omit context and use implicit references. This results in summaries that are confusing to readers unfamiliar with the background. To address this, we introduce Knowledge-Grounded Discussion Summarization (KGDS), a novel task that produces a supplementary background summary for context and a clear opinion summary with clarified references. To facilitate research, we construct the first KGDS benchmark, featuring news-discussion pairs and expert-created multi-granularity gold annotations for evaluating sub-summaries. We also propose a novel hierarchical evaluation framework with fine-grained and interpretable metrics. Our extensive evaluation of 12 advanced large language models (LLMs) reveals that KGDS remains a significant challenge. The models frequently miss key facts and retain irrelevant ones in background summarization, and often fail to resolve implicit references in opinion summary integration.</abstract>
      <url hash="77575c12">2025.ijcnlp-long.118</url>
      <bibkey>zhou-etal-2025-talking</bibkey>
    </paper>
    <paper id="119">
      <title><fixed-case>C</fixed-case>trl<fixed-case>S</fixed-case>hift: Steering Language Models for Dense Quotation Retrieval with Dynamic Prompts</title>
      <author><first>Chuang</first><last>Liang</last></author>
      <author><first>Wei</first><last>Li</last><affiliation>Beijing Language and Culture University</affiliation></author>
      <author><first>Yanqiu</first><last>Shao</last><affiliation>Beijing Language and Culture University</affiliation></author>
      <pages>2192-2204</pages>
      <abstract>Quotation recommendation is an inherently asymmetric retrieval task, where the intended meaning of a quote often diverges from surface expressions, creating significant semantic shifts. Combined with minimal lexical overlap, this poses a core challenge for classic dense retrievers, which struggle to capture non-literal and rhetorical alignments. To bridge this semantic gap, we propose introducing controllable signals to guide the model’s attention toward abstract, context-relevant concepts. We propose CtrlShift, a framework that leverages a Variational Autoencoder (VAE) to capture latent associations between context and quotation, which is used to derive context-aware control signals to modulate semantic focus and support bidirectional alignment and rhetorical intent modeling. Experiments show that our method consistently outperforms baselines on the quotation recommendation task and can be effectively transfered to the general purposed benchmark. Further, CtrlShift integrates seamlessly with general-purpose generative models without additional fine-tuning, and provides satisfactory interpretability by generating textual explaination to uncover the model’s focus on abstract, citation-aligned semantics.</abstract>
      <url hash="ab2dcc91">2025.ijcnlp-long.119</url>
      <bibkey>liang-etal-2025-ctrlshift</bibkey>
    </paper>
    <paper id="120">
      <title><fixed-case>P</fixed-case>er<fixed-case>C</fixed-case>o<fixed-case>R</fixed-case>: Evaluating Commonsense Reasoning in <fixed-case>P</fixed-case>ersian via Multiple-Choice Sentence Completion</title>
      <author id="morteza-alikhani" orcid="0009-0006-1711-3425"><first>Morteza</first><last>Alikhani</last><affiliation>MCI Next</affiliation></author>
      <author><first>Mohammadtaha</first><last>Bagherifard</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Erfan</first><last>Zinvandi</last><affiliation>Hamrah e Aval</affiliation></author>
      <author id="mehran-sarmadi" orcid="0009-0000-8069-8370"><first>Mehran</first><last>Sarmadi</last></author>
      <pages>2205-2224</pages>
      <abstract>We introduced PerCoR—Persian Commonsense Reasoning—the first large-scale Persian benchmark for commonsense reasoning. PerCoR contains 106K multiple-choice sentence-completion problems drawn from more than forty news, cultural and other web sources. We adopt a linguistically grounded, conjunction-based segmentation strategy to generate coherent prefix–continuation pairs. To create challenging distractors, we propose DRESS-AF—Distractor Ranking via Embedding Similarity Scoring and Adversarial Filtering—a generation-free adversarial filtering method that selects distractors from the pool of gold continuations while maximising model confusion. Human annotators score 89% on PerCoR, while OpenAI-o3 achieves the highest performance at 92.18%, followed closely by Claude-Sonnet-3.7 (91.17%). The strongest open-source model, DeepSeek-R1, reaches 82.51%, underscoring both the dataset’s difficulty and the remaining performance gap in Persian commonsense reasoning. We further show that DRESS-AF transfers to the English HellaSwag benchmark, increasing its difficulty without hurting human solvability. The dataset is available at https://huggingface.co/datasets/MCINext/PerCoR .</abstract>
      <url hash="643bc08e">2025.ijcnlp-long.120</url>
      <bibkey>alikhani-etal-2025-percor</bibkey>
    </paper>
    <paper id="121">
      <title><fixed-case>Q</fixed-case>2<fixed-case>E</fixed-case>: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval</title>
      <author id="shubhashis-roy-dipta" orcid="0000-0002-9176-1782"><first>Shubhashis</first><last>Roy Dipta</last></author>
      <author id="francis-ferraro" orcid="0000-0003-2413-9368"><first>Francis</first><last>Ferraro</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <pages>2225-2245</pages>
      <abstract>Recent approaches have shown impressive proficiency in extracting and leveraging parametric knowledge from Large-Language Models (LLMs) and Vision-Language Models (VLMs). In this work, we consider how we can improve the retrieval of videos related to complex real-world events by automatically extracting latent parametric knowledge about those events. We present Q2E: a Query-to-Event decomposition method for zero-shot multilingual text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our approach demonstrates that we can enhance the understanding of otherwise overly simplified human queries by decomposing the query using the knowledge embedded in LLMs and VLMs. We additionally show how to apply our approach to both visual and speech-based inputs. To combine this varied multimodal knowledge, we adopt entropy-based fusion scoring for zero-shot fusion. Q2E outperforms the previous SOTA on the MultiVENT dataset by 8 NDCG points, while improving on MSR-VTT and MSVD by 4 and 3 points, respectively, outperforming several existing retrieval methods, including many fine-tuned and SOTA zero-shot approaches. We have released both code and data.</abstract>
      <url hash="e3a3d390">2025.ijcnlp-long.121</url>
      <bibkey>roy-dipta-ferraro-2025-q2e</bibkey>
    </paper>
    <paper id="122">
      <title><fixed-case>V</fixed-case>ideo<fixed-case>C</fixed-case>hain: A Transformer-Based Framework for Multi-hop Video Question Generation</title>
      <author id="arpan-phukan" orcid="0000-0002-9253-1022"><first>Arpan</first><last>Phukan</last><affiliation>Indian Institute of Technology, Patna</affiliation></author>
      <author id="anupam-pandey" orcid="0009-0003-2456-7319"><first>Anupam</first><last>Pandey</last><affiliation>Indian Institute of Technology, Patna,</affiliation></author>
      <author id="deepjyoti-bodo" orcid="0009-0009-0425-4896"><first>Deepjyoti</first><last>Bodo</last></author>
      <author id="asif-ekbal" orcid="0000-0003-3612-8834"><first>Asif</first><last>Ekbal</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <pages>2246-2266</pages>
      <abstract>Multi-hop Question Generation (QG) effectively evaluates reasoning but remains confined to text; Video Question Generation (VideoQG) is limited to zero-hop questions over single segments. To address this, we introduce VideoChain, a novel Multi-hop Video Question Generation (MVQG) framework designed to generate questions that require reasoning across multiple, temporally separated video segments. VideoChain features a modular architecture built on a modified BART backbone enhanced with video embeddings, capturing textual and visual dependencies. Using the TVQA+ dataset, we automatically construct the large-scale MVQ-60 dataset by merging zero-hop QA pairs, ensuring scalability and diversity. Evaluations show VideoChain’s strong performance across standard generation metrics: ROUGE-L (0.6454), ROUGE-1 (0.6854), BLEU-1 (0.6711), BERTScore-F1 (0.7967), and semantic similarity (0.8110). These results highlight the model’s ability to generate coherent, contextually grounded, and reasoning-intensive questions. To facilitate future research, we publicly release our code and dataset.</abstract>
      <url hash="b9acd8d2">2025.ijcnlp-long.122</url>
      <bibkey>phukan-etal-2025-videochain</bibkey>
    </paper>
    <paper id="123">
      <title>Interpreting the Effects of Quantization on <fixed-case>LLM</fixed-case>s</title>
      <author><first>Manpreet</first><last>Singh</last><affiliation>Dalhousie University</affiliation></author>
      <author id="hassan-sajjad" orcid="0000-0002-8584-6595"><first>Hassan</first><last>Sajjad</last><affiliation>Dalhousie University</affiliation></author>
      <pages>2267-2281</pages>
      <abstract>Quantization offers a practical solution to deploy LLMs in resource-constraint environments. However, its impact on internal representations remains understudied, raising questions about the reliability of quantized models. In this study, we employ a range of interpretability techniques to investigate how quantization affects model and neuron behavior. We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings reveal that the impact of quantization on model calibration is generally minor. Analysis of neuron activations indicates that the number of dead neurons, i.e., those with activation values close to 0 across the dataset, remains consistent regardless of quantization. In terms of neuron contribution to predictions, we observe that smaller full precision models exhibit fewer salient neurons, whereas larger models tend to have more, with the exception of Llama-2-7B. The effect of quantization on neuron redundancy varies across models. Overall, our findings suggest that effect of quantization may vary by model and tasks, however, we did not observe any drastic change which may discourage the use of quantization as a reliable model compression technique.</abstract>
      <url hash="787a5785">2025.ijcnlp-long.123</url>
      <bibkey>singh-sajjad-2025-interpreting</bibkey>
    </paper>
    <paper id="124">
      <title>Large Language Models Encode Semantics and Alignment in Linearly Separable Representations</title>
      <author><first>Baturay</first><last>Saglam</last><affiliation>Yale University</affiliation></author>
      <author><first>Paul</first><last>Kassianik</last><affiliation>Cisco</affiliation></author>
      <author><first>Blaine</first><last>Nelson</last><affiliation>Cisco</affiliation></author>
      <author><first>Sajana</first><last>Weerawardhena</last><affiliation>Cisco</affiliation></author>
      <author><first>Yaron</first><last>Singer</last></author>
      <author><first>Amin</first><last>Karbasi</last></author>
      <pages>2282-2303</pages>
      <abstract>Understanding the latent space geometry of large language models (LLMs) is key to interpreting their behavior and improving alignment. Yet it remains unclear to what extent LLMs linearly organize representations related to semantic understanding. To explore this, we conduct a large-scale empirical study of hidden representations in 11 autoregressive models across six scientific topics. We find that high-level semantic information consistently resides in low-dimensional subspaces that form linearly separable representations across domains. This separability becomes more pronounced in deeper layers and under prompts that elicit structured reasoning or alignment behavior—even when surface content remains unchanged. These findings motivate geometry-aware tools that operate directly in latent space to detect and mitigate harmful and adversarial content. As a proof of concept, we train an MLP probe on final-layer hidden states as a lightweight latent-space guardrail. This approach substantially improves refusal rates on malicious queries and prompt injections that bypass both the model’s built-in safety alignment and external token-level filters.</abstract>
      <url hash="4b3decb5">2025.ijcnlp-long.124</url>
      <bibkey>saglam-etal-2025-large</bibkey>
    </paper>
    <paper id="125">
      <title>Beyond statistical significance: Quantifying uncertainty and statistical variability in multilingual and multitask <fixed-case>NLP</fixed-case> evaluation</title>
      <author><first>Jonne</first><last>Sälevä</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Duygu</first><last>Ataman</last></author>
      <author id="constantine-lignos" orcid="0000-0001-6410-2848"><first>Constantine</first><last>Lignos</last><affiliation>Brandeis University</affiliation></author>
      <pages>2304-2321</pages>
      <abstract>We introduce a set of resampling-based methods for quantifying uncertainty and statistical precision of evaluation metrics in multilingual and/or multitask NLP benchmarks.We show how experimental variation in performance scores arises from both model and data-related sources, and that accounting for both of them is necessary to avoid substantially underestimating the overall variability over hypothetical replications.Using multilingual question answering, machine translation, and named entity recognition as example tasks, we also demonstrate how resampling methods are useful for quantifying the replication uncertainty of various quantities used in leaderboards such as model rankings and pairwise differences between models.</abstract>
      <url hash="b5768878">2025.ijcnlp-long.125</url>
      <bibkey>saleva-etal-2025-beyond</bibkey>
    </paper>
    <paper id="126">
      <title>What Would You Ask When You First Saw <tex-math>a^2+b^2=c^2</tex-math>? Evaluating <fixed-case>LLM</fixed-case> on Curiosity-Driven Question Generation</title>
      <author><first>Shashidhar Reddy</first><last>Javaji</last></author>
      <author><first>Zining</first><last>Zhu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <pages>2322-2354</pages>
      <abstract>Large language models (LLMs) are increasingly widely used as critical components of knowledge retrieval systems and agentic systems. These systems can benefit from knowledge-seeking capabilities of LLMs, in other words, curiosity. However, this capability has not been evaluated quantitatively. Towards bridging this gap, we propose an evaluation framework, CDQG (Curiosity-Driven Question Generation). The CDQG task prompts LLMs to generate questions about a statement introducing scientific knowledge, simulating a curious person when facing the statement for the first time. The CDQG dataset contains 1,988 statements including physics, chemistry, and mathematics with distinct levels of difficulty, general knowledge statements, and intentionally erroneous statements. We score the qualities of the questions generated by LLMs along multiple dimensions. These scores are validated by rigorous controlled ablation studies and human evaluations. While large models like GPT-4 and Mistral 8x7b can generate highly coherent and relevant questions, the smaller Phi-2 model is equally or more effective. This indicates that size does not solely determine a model’s knowledge acquisition potential. CDQG quantifies a critical model capability, and opens up research opportunities for developing future knowledge retrieval systems driven by LLMs.</abstract>
      <url hash="1e0ca0fb">2025.ijcnlp-long.126</url>
      <bibkey>javaji-zhu-2025-ask</bibkey>
    </paper>
    <paper id="127">
      <title>Can <fixed-case>AI</fixed-case> Validate Science? Benchmarking <fixed-case>LLM</fixed-case>s on Claim →<fixed-case>E</fixed-case>vidence Reasoning in <fixed-case>AI</fixed-case> Papers</title>
      <author><first>Shashidhar Reddy</first><last>Javaji</last></author>
      <author id="yupeng-cao" orcid="0000-0002-4024-2026"><first>Yupeng</first><last>Cao</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author id="haohang-li" orcid="0009-0002-3604-7284"><first>Haohang</first><last>Li</last></author>
      <author id="yangyang-yu" orcid="0009-0009-4595-1786"><first>Yangyang</first><last>Yu</last><affiliation>Accenture</affiliation></author>
      <author id="nikhil-muralidhar" orcid="0000-0001-7068-2981"><first>Nikhil</first><last>Muralidhar</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Zining</first><last>Zhu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <pages>2355-2379</pages>
      <abstract>Large language models (LLMs) are increasingly being used for complex research tasks such as literature review, idea generation, and scientific paper analysis, yet their ability to truly understand and process the intricate relationships within complex research papers, such as the logical links between claims and supporting evidence remains largely unexplored. In this study, we present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs’ capabilities in scientific claim-evidence extraction and validation, a task that reflects deeper comprehension of scientific argumentation. We systematically compare three approaches which are inspired by divide and conquer approaches, across six diverse LLMs, highlighting model-specific strengths and weaknesses in scientific comprehension. Through evaluation involving over 300 claim-evidence pairs across multiple research domains, we reveal significant limitations in LLMs’ ability to process complex scientific content. Our results demonstrate that closed-source models like GPT-4 and Claude consistently outperform open-source counterparts in precision and recall across claim-evidence identification tasks. Furthermore, strategically designed three-pass and one-by-one prompting approaches significantly improve LLMs’ abilities to accurately link dispersed evidence with claims, although this comes at increased computational cost. CLAIM-BENCH sets a new standard for evaluating scientific comprehension in LLMs, offering both a diagnostic tool and a path forward for building systems capable of deeper, more reliable reasoning across full-length papers.</abstract>
      <url hash="f04813a7">2025.ijcnlp-long.127</url>
      <bibkey>javaji-etal-2025-ai</bibkey>
    </paper>
    <paper id="128">
      <title>More Than a Score: Probing the Impact of Prompt Specificity on <fixed-case>LLM</fixed-case> Code Generation</title>
      <author id="yangtian-zi" orcid="0000-0003-2606-3280"><first>Yangtian</first><last>Zi</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Harshitha</first><last>Menon</last><affiliation>Lawrence Livermore National Labs</affiliation></author>
      <author id="arjun-guha" orcid="0000-0002-7493-3271"><first>Arjun</first><last>Guha</last><affiliation>Northeastern University</affiliation></author>
      <pages>2380-2402</pages>
      <abstract>State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general benchmarks like HumanEval (Chen et al., 2021) but underperform on specialized suites such as ParEval (Nichols et al., 2024). Is this due to LLMs missing domain knowledge or insufficient prompt detail is given? To answer this, we introduce PartialOrderEval, which augments any code generation benchmark with a partial order of prompts from minimal to maximally detailed. Applying it to HumanEval and both serial and OpenMP subsets of ParEval, we measure how pass@1 scales with prompt specificity. Our experiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of prompt sensitivity across different tasks, and a qualitative analysis highlights explicit I/O specifications, edge-case handling, and stepwise breakdowns as the key drivers of prompt detail improvement.</abstract>
      <url hash="711056b9">2025.ijcnlp-long.128</url>
      <bibkey>zi-etal-2025-score</bibkey>
    </paper>
    <paper id="129">
      <title>Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation</title>
      <author id="galann-pennec" orcid="0009-0003-0131-5805"><first>Galann</first><last>Pennec</last></author>
      <author><first>Zhengyuan</first><last>Liu</last><affiliation>Agency for Science, Technology and Research (A*STAR), Singapore</affiliation></author>
      <author id="nicholas-asher"><first>Nicholas</first><last>Asher</last><affiliation>CNRS</affiliation></author>
      <author id="philippe-muller" orcid="0000-0002-6765-4020"><first>Philippe</first><last>Muller</last><affiliation>IRIT, University of Toulouse</affiliation></author>
      <author id="nancy-chen" orcid="0000-0003-0872-5877"><first>Nancy F.</first><last>Chen</last></author>
      <pages>2403-2426</pages>
      <abstract>Vision-Language Models (VLMs) often struggle to balance visual and textual information when summarizing complex multimodal inputs, such as entire TV show episodes. In this paper, we propose a zero-shot video-to-text summarization approach that builds its own screenplay-like representation of an episode, effectively integrating key video moments, dialogue, and character information into a unified document. Unlike previous approaches, we simultaneously generate screenplays and name the characters in zero-shot, using only the audio, video, and transcripts as input. Additionally, we highlight that existing summarization metrics can fail to assess the multimodal content in summaries. To address this, we introduce MFactSum, a multimodal metric that evaluates summaries with respect to both vision and text modalities. Using MFactSum, we evaluate our screenplay summaries on the SummScreen3D dataset, demonstrating superiority against state-of-the-art VLMs such as Gemini 1.5 by generating summaries containing 20% more relevant visual information while requiring 75% less of the video as input.</abstract>
      <url hash="4543d1e2">2025.ijcnlp-long.129</url>
      <bibkey>pennec-etal-2025-integrating</bibkey>
    </paper>
    <paper id="130">
      <title><fixed-case>PMPO</fixed-case>: A Self-Optimizing Framework for Creating High-Fidelity Measurement Tools for Social Bias in Large Language Models</title>
      <author><first>Zeqiang</first><last>Wang</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Yuqi</first><last>Wang</last></author>
      <author><first>Xinyue</first><last>Wu</last></author>
      <author><first>Chenxi</first><last>Li</last></author>
      <author id="yiran-liu" orcid="0009-0007-2193-1452"><first>Yiran</first><last>Liu</last></author>
      <author><first>Linghan</first><last>Ge</last></author>
      <author id="zhan-yu" orcid="0000-0003-2792-8178"><first>Zhan</first><last>Yu</last><affiliation>East China Normal University</affiliation></author>
      <author id="jiaxin-shi" orcid="0000-0002-6060-926X"><first>Jiaxin</first><last>Shi</last></author>
      <author id="suparna-de" orcid="0000-0001-7439-6077"><first>Suparna</first><last>De</last><affiliation>University of Surrey</affiliation></author>
      <pages>2427-2440</pages>
      <abstract>The potential of Large Language Models (LLMs) as instruments for measuring social phenomena is constrained by the methodological limitations of current probing techniques. Prevailing methods rely on static, handcrafted probe sets whose quality is highly dependent on their authors’ subjective expertise. This results in measurement tools with inconsistent statistical reliability that defy systematic optimization. Such an “artisanal” approach, akin to using an “uneven ruler,” undermines the scientific rigor of its findings and severely limits the applicability of LLMs in the social sciences. To elevate bias measurement from a craft to a science, we introduce the Psychometric-driven Probe Optimization (PMPO) framework. This framework treats a probe set as an optimizable scientific instrument and, for the first time, utilizes a Neural Genetic Algorithm that leverages a powerful LLM as a “neural genetic operator.” Through a hybrid strategy of gradient-guided mutation and creative rephrasing, PMPO automatically enhances the probe set’s reliability, sensitivity, and diversity. We first establish the external validity of our foundational measurement method (PLC), demonstrating a high correlation between its measurement of occupational gender bias and real-world U.S. Bureau of Labor Statistics data (average Pearson’s r=0.83, p&lt;.001). Building on this, we show that the PMPO framework can elevate a standard probe set’s internal consistency (Cronbach’s Alpha) from 0.90 to an exceptional 0.96 within 10 generations. Critically, in a rigorous, double-blind “Turing Test,” probes evolved by PMPO from non-expert seeds were judged by sociology experts to have achieved a level of quality, sophistication, and nuance that is comparable to, and even indistinguishable from, those handcrafted by domain experts. This work provides a systematic pathway to upgrade LLM measurement tools from artisanal artifacts to automated scientific instruments, offering an unprecedented and trustworthy tool for AI safety auditing and computational social science.</abstract>
      <url hash="8e3a81b1">2025.ijcnlp-long.130</url>
      <bibkey>wang-etal-2025-pmpo</bibkey>
    </paper>
    <paper id="131">
      <title><fixed-case>ELR</fixed-case>-1000: A Community-Generated Dataset for Endangered <fixed-case>I</fixed-case>ndic Indigenous Languages</title>
      <author><first>Neha</first><last>Joshi</last><affiliation>Karya</affiliation></author>
      <author><first>Pamir</first><last>Gogoi</last></author>
      <author><first>AasimBaig</first><last>Mirza</last><affiliation>Karya Inc</affiliation></author>
      <author><first>Aayush</first><last>Jansari</last><affiliation>Karya Inc</affiliation></author>
      <author><first>Aditya</first><last>Yadavalli</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ayushi</first><last>Pandey</last><affiliation>University of Dublin, Trinity College</affiliation></author>
      <author><first>Arunima</first><last>Shukla</last><affiliation>Karya</affiliation></author>
      <author id="deepthi-sudharsan" orcid="0000-0002-1990-3010"><first>Deepthi</first><last>Sudharsan</last></author>
      <author id="kalika-bali" orcid="0000-0001-9275-742X"><first>Kalika</first><last>Bali</last><affiliation>Microsoft Research Labs</affiliation></author>
      <author><first>Vivek</first><last>Seshadri</last></author>
      <pages>2441-2457</pages>
      <abstract>We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000—captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models’ capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context—including background information about the languages, translation examples, and guidelines for cultural preservation—leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.</abstract>
      <url hash="399a9887">2025.ijcnlp-long.131</url>
      <bibkey>joshi-etal-2025-elr</bibkey>
    </paper>
    <paper id="132">
      <title>Pragmatic Theories Enhance Understanding of Implied Meanings in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Takuma</first><last>Sato</last></author>
      <author><first>Seiya</first><last>Kawano</last><affiliation>Nara Institute of Science and Technology, Japan and RIKEN</affiliation></author>
      <author><first>Koichiro</first><last>Yoshino</last><affiliation>Tokyo Institute of Technology/Institute of Science Tokyo and RIKEN</affiliation></author>
      <pages>2458-2477</pages>
      <abstract>The ability to accurately interpret implied meanings plays a crucial role in human communication and language use, and language models are also expected to possess this capability. This study demonstrates that providing language models with pragmatic theories as prompts is an effective in-context learning approach for tasks to understand implied meanings. Specifically, we propose an approach in which an overview of pragmatic theories, such as Gricean pragmatics and Relevance Theory, is presented as a prompt to the language model, guiding it through a step-by-step reasoning process to derive a final interpretation. Experimental results showed that, compared to the baseline, which prompts intermediate reasoning without presenting pragmatic theories (0-shot Chain-of-Thought), our methods enabled language models to achieve up to 9.6% higher scores on pragmatic reasoning tasks. Furthermore, we show that even without explaining the details of pragmatic theories, merely mentioning their names in the prompt leads to a certain performance improvement (around 1-3%) in larger models compared to the baseline.</abstract>
      <url hash="5dedc424">2025.ijcnlp-long.132</url>
      <bibkey>sato-etal-2025-pragmatic</bibkey>
    </paper>
    <paper id="133">
      <title><fixed-case>I</fixed-case>ndic<fixed-case>C</fixed-case>laim<fixed-case>B</fixed-case>uster: A Multilingual Claim Verification Dataset</title>
      <author><first>Pritam</first><last>Pal</last><affiliation>Jadavpur University</affiliation></author>
      <author><first>Shyamal Krishna</first><last>Jana</last><affiliation>Jadavpur University</affiliation></author>
      <author id="dipankar-das" orcid="0000-0002-8110-9344"><first>Dipankar</first><last>Das</last><affiliation>Jadavpur University</affiliation></author>
      <pages>2478-2489</pages>
      <abstract>The present article introduces **IndicClaimBuster**, a novel multilingual claim verification dataset comprising <tex-math>\approx</tex-math> 9K claims and their corresponding evidence in English, Hindi, Bengali, and Hindi-English CodeMixed texts. The data set covers three key domains: politics, law and order, and health, to address the challenges of verifiable facts. Each claim was sourced from reputable Indian news portals and is accompanied by three pieces of evidence, two LLM-generated and one manually curated. Additionally, a separate attempt was conducted to generate refuted claims by employing an LLM. We further develop two frameworks: an unsupervised baseline and a two-stage pipeline that comprises evidence retrieval and veracity prediction modules. For retrieval, we fine-tuned SBERT models, with e5-base demonstrating superior average performance across languages, whereas for veracity prediction, multilingual transformers (mBERT, XLM-R, MuRIL, IndicBERTv2) were fine-tuned. Results indicate MuRIL and IndicBERTv2 excel in Indian languages, while XLM-R performs the best for CodeMix. Our work contributes a high-quality multilingual dataset and strong baseline methodologies, offering valuable resources for advancing automated claim verification in linguistically diverse and low-resource settings for Indian languages. The IndicClaimBuster dataset is available at: https://github.com/pritampal98/indic-claim-buster</abstract>
      <url hash="1f36913b">2025.ijcnlp-long.133</url>
      <bibkey>pal-etal-2025-indicclaimbuster</bibkey>
    </paper>
    <paper id="134">
      <title><fixed-case>I</fixed-case>ncogni<fixed-case>T</fixed-case>ext: Privacy-enhancing Conditional Text Anonymization via <fixed-case>LLM</fixed-case>-based Private Attribute Randomization</title>
      <author><first>Ahmed</first><last>Frikha</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Nassim</first><last>Walha</last><affiliation>Siemens AG and Deutsches Krebsforschungszentrum</affiliation></author>
      <author><first>Krishna Kanth</first><last>Nakka</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ricardo</first><last>Mendes</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author id="xue-jiang" orcid="0000-0002-4959-9910"><first>Xue</first><last>Jiang</last></author>
      <author><first>Xuebing</first><last>Zhou</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>2490-2501</pages>
      <abstract>In this work, we address the problem of text anonymization where the goal is to prevent adversaries from correctly inferring private attributes of the author, while keeping the text utility, i.e., meaning and semantics. We propose IncogniText, a technique that anonymizes the text to mislead a potential adversary into predicting a wrong private attribute value. Our empirical evaluation shows a reduction of private attribute leakage by more than across 8 different private attributes. Finally, we demonstrate the maturity of IncogniText for real-world applications by distilling its anonymization capability into a set of LoRA parameters associated with an on-device model. Our results show the possibility of reducing privacy leakage by more than half with limited impact on utility.</abstract>
      <url hash="b9ab5fe7">2025.ijcnlp-long.134</url>
      <bibkey>frikha-etal-2025-incognitext</bibkey>
    </paper>
    <paper id="135">
      <title>Crypto-<fixed-case>LLM</fixed-case>: Two-Stage Language Model Pre-training with Ciphered and Natural Language Data</title>
      <author><first>Yohei</first><last>Kobashi</last><affiliation>The University of Tokyo, The University of Tokyo</affiliation></author>
      <author><first>Fumiya</first><last>Uchiyama</last></author>
      <author><first>Takeshi</first><last>Kojima</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Andrew</first><last>Gambardella</last><affiliation>The University of Tokyo, Tokyo University</affiliation></author>
      <author><first>Qi</first><last>Cao</last><affiliation>The University of Tokyo</affiliation></author>
      <author id="yusuke-iwasawa" orcid="0000-0002-1321-2622"><first>Yusuke</first><last>Iwasawa</last><affiliation>The University of Tokyo, The University of Tokyo</affiliation></author>
      <author><first>Yutaka</first><last>Matsuo</last><affiliation>The University of Tokyo and The University of Tokyo</affiliation></author>
      <pages>2502-2520</pages>
      <abstract>As the adoption of large language models (LLMs) continues to grow, the risk of sensitive data leakage from their training datasets has become a critical concern. This study proposes a novel method for encrypting training data using a polyalphabetic substitution cipher. This approach prevents the model from learning sensitive information while allowing it to capture abstract linguistic patterns. We pre-trained a Llama 3 model (551M parameters) using approximately 7.5 billion tokens of encrypted data and subsequently conducted continual pre-training with another 2.5 billion tokens of plaintext data. The effectiveness of the model was evaluated by comparing its downstream task performance with a model trained solely on plaintext data. In addition, we evaluated the risk of sensitive data leakage through name reconstruction, true-prefix and data extraction attacks. These results demonstrate the potential of our approach to balance data security with model performance.</abstract>
      <url hash="b1a7b9c7">2025.ijcnlp-long.135</url>
      <bibkey>kobashi-etal-2025-crypto</bibkey>
    </paper>
    <paper id="136">
      <title>Not Just a Piece of Cake: Cross-Lingual Fine-Tuning for Idiom Identification</title>
      <author><first>Ofri</first><last>Hefetz</last></author>
      <author><first>Kai Golan</first><last>Hashiloni</last></author>
      <author><first>Alon</first><last>Mannor</last></author>
      <author id="kfir-bar" orcid="0000-0002-1354-2955"><first>Kfir</first><last>Bar</last><affiliation>Reichman University</affiliation></author>
      <pages>2521-2537</pages>
      <abstract>We investigate cross-lingual fine-tuning for idiomatic expression identification, addressing the limited availability of annotated data in many languages. We evaluate encoder and generative decoder models to examine their ability to generalize idiom identification across languages. Additionally, we conduct an explainability study using linear probing and LogitLens to analyze how idiomatic meaning is represented across model layers. Results show consistent cross-lingual transfer, with English emerging as a strong source language. All code and models are released to support future research.</abstract>
      <url hash="f7c977b6">2025.ijcnlp-long.136</url>
      <bibkey>hefetz-etal-2025-just</bibkey>
    </paper>
    <paper id="137">
      <title>Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics</title>
      <author><first>Chunhua</first><last>Liu</last><affiliation>University of Melbourne</affiliation></author>
      <author id="hong-yi-lin" orcid="0009-0004-5368-8897"><first>Hong Yi</first><last>Lin</last></author>
      <author><first>Patanamon</first><last>Thongtanunam</last><affiliation>University of Melbourne</affiliation></author>
      <pages>2538-2560</pages>
      <abstract>Language models have shown strong capabilities across a wide range of tasks in software engineering, such as code generation, yet they suffer from hallucinations. While hallucinations have been studied independently in natural language and code generation, their occurrence in tasks involving code changes which have a structurally complex and context-dependent format of code remains largely unexplored. This paper presents the first comprehensive analysis of hallucinations in two critical tasks involving code change to natural language generation: commit message generation and code review comment generation. We quantify the prevalence of hallucinations in recent language models and explore a range of metric-based approaches to automatically detect them. Our findings reveal that approximately 50% of generated code reviews and 20% of generated commit messages contain hallucinations. Whilst commonly used metrics are weak detectors on their own, combining multiple metrics substantially improves performance. Notably, model confidence and feature attribution metrics effectively contribute to hallucination detection, showing promise for inference-time detection.</abstract>
      <url hash="17d9a674">2025.ijcnlp-long.137</url>
      <bibkey>liu-etal-2025-hallucinations</bibkey>
    </paper>
    <paper id="138">
      <title>Differential Mamba</title>
      <author id="nadav-schneider" orcid="0000-0002-3838-5482"><first>Nadav</first><last>Schneider</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <author id="itamar-zimerman" orcid="0000-0001-8321-0609"><first>Itamar</first><last>Zimerman</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Eliya</first><last>Nachmani</last><affiliation>Ben Gurion University of the Negev and Research, Google</affiliation></author>
      <pages>2561-2575</pages>
      <abstract>Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models.</abstract>
      <url hash="43d48cb7">2025.ijcnlp-long.138</url>
      <bibkey>schneider-etal-2025-differential</bibkey>
    </paper>
    <paper id="139">
      <title>From Templates to Natural Language: Generalization Challenges in Instruction-Tuned <fixed-case>LLM</fixed-case>s for Spatial Reasoning</title>
      <author id="kranti-chalamalasetti"><first>Chalamalasetti</first><last>Kranti</last><affiliation>Universität Potsdam</affiliation></author>
      <author id="sherzod-hakimov" orcid="0000-0002-7421-6213"><first>Sherzod</first><last>Hakimov</last><affiliation>Universität Potsdam</affiliation></author>
      <author id="david-schlangen" orcid="0000-0002-2686-6887"><first>David</first><last>Schlangen</last><affiliation>University of Potsdam</affiliation></author>
      <pages>2576-2591</pages>
      <abstract>Instruction-tuned large language models (LLMs) have shown strong performance on a variety of tasks; however, generalizing from synthetic to human-authored instructions in grounded environments remains a challenge for them. In this work, we study generalization challenges in spatial grounding tasks where models interpret and translate instructions for building object arrangements on a 2.5D grid. We fine-tune LLMs using only synthetic instructions and evaluate their performance on a benchmark dataset containing both synthetic and human-authored instructions. Our results reveal that while models generalize well on simple tasks, their performance degrades significantly on more complex tasks. We present a detailed error analysis of the gaps in instruction generalization.</abstract>
      <url hash="126a9705">2025.ijcnlp-long.139</url>
      <bibkey>kranti-etal-2025-templates</bibkey>
    </paper>
    <paper id="140">
      <title>Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization</title>
      <author id="masahiro-kaneko" orcid="0000-0002-5117-5447"><first>Masahiro</first><last>Kaneko</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <author id="zeerak-talat"><first>Zeerak</first><last>Talat</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author id="timothy-baldwin" orcid="0000-0003-4525-6950"><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>2592-2609</pages>
      <abstract>Iterative jailbreak methods that repeatedly rewrite and input prompts into large language models (LLMs) to induce harmful outputs—using the model’s previous responses to guide each new iteration—have been found to be a highly effective attack strategy. Despite being an effective attack strategy against LLMs and their safety mechanisms, existing defenses do not proactively disrupt this dynamic trial-and-error cycle. In this study, we propose a novel framework that dynamically updates its defense strategy through online learning in response to each new prompt from iterative jailbreak methods. Leveraging the distinctions between harmful jailbreak-generated prompts and typical harmless prompts, we introduce a reinforcement learning-based approach that optimizes prompts to ensure appropriate responses for harmless tasks while explicitly rejecting harmful prompts. Additionally, to curb overfitting to the narrow band of partial input rewrites explored during an attack, we introduce Past‐Direction Gradient Damping (PDGD). Experiments conducted on three LLMs show that our approach significantly outperforms five existing defense methods against five iterative jailbreak methods. Moreover, our results indicate that our prompt optimization strategy simultaneously enhances response quality for harmless tasks.</abstract>
      <url hash="bc72d89a">2025.ijcnlp-long.140</url>
      <bibkey>kaneko-etal-2025-online</bibkey>
    </paper>
    <paper id="141">
      <title><fixed-case>HARBOR</fixed-case>: Exploring Persona Dynamics in Multi-Agent Competition</title>
      <author id="kenan-jiang" orcid="0009-0007-9489-8630"><first>Kenan</first><last>Jiang</last><affiliation>Emory University</affiliation></author>
      <author id="li-xiong" orcid="0000-0001-7354-0428"><first>Li</first><last>Xiong</last><affiliation>Emory University</affiliation></author>
      <author><first>Fei</first><last>Liu</last><affiliation>Emory University</affiliation></author>
      <pages>2610-2632</pages>
      <abstract>We investigate factors contributing to LLM agents’ success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent’s behavior in a competitive setting? (b) Can an agent effectively profile its competitors’ behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening the understanding of multi-agent workflows in competitive environments.</abstract>
      <url hash="69c854e8">2025.ijcnlp-long.141</url>
      <bibkey>jiang-etal-2025-harbor</bibkey>
    </paper>
    <paper id="142">
      <title>A Diagnostic Framework for Auditing Reference-Free Vision-Language Metrics</title>
      <author id="angeline-charles" orcid="0009-0004-8460-0675"><first>Angeline</first><last>Charles</last><affiliation>Moodys Corporation</affiliation></author>
      <author id="srikant-panda" orcid="0009-0000-5969-1166"><first>Srikant</first><last>Panda</last></author>
      <author id="amit-agarwal" orcid="0009-0004-8867-6271"><first>Amit</first><last>Agarwal</last><affiliation>Oracle</affiliation></author>
      <author id="hitesh-laxmichand-patel" orcid="0009-0009-7492-5173"><first>Hitesh Laxmichand</first><last>Patel</last><affiliation>Oracle</affiliation></author>
      <author id="priyaranjan-pattnayak" orcid="0009-0005-8299-0208"><first>Priyaranjan</first><last>Pattnayak</last><affiliation>Oracle</affiliation></author>
      <author><first>Bhargava</first><last>Kumar</last><affiliation>TD Securities</affiliation></author>
      <author><first>Tejaswini</first><last>Kumar</last><affiliation>Columbia University</affiliation></author>
      <pages>2633-2644</pages>
      <abstract>Reference-free metrics such as CLIPScore and PAC-S are increasingly used in vision-language tasks due to their scalability and independence from human-written references. However, their reliability under linguistic, visual, and cultural variation remains underexplored. In this work, we present a systematic audit of CLIPScore and PAC-S using an eight-factor diagnostic framework applied to MS-COCO validation images. Our analysis reveals consistent failure modes across dimensions including object size, content category, syntax, named entities, spatial relations and cultural context. Both metrics penalize captions referencing African (−5.5%, −4.8%) and Arabian (−4.9%, −5.3%) cultures, favor large-object and animal-centric scenes (by 20-30%) and show limited sensitivity to spatial negation and word order. CLIPScore correlates more strongly with syntactic complexity, while PAC-S demonstrates greater robustness to verbosity and named–entity variation highlighting complementary strengths rather than superiority. These findings expose cultural and content bias, weak semantic robustness, and limited compositional understanding. We conclude with design recommendations to improve fairness, scale invariance, and semantic grounding in future reference-free evaluation metrics.</abstract>
      <url hash="01162dfe">2025.ijcnlp-long.142</url>
      <bibkey>charles-etal-2025-diagnostic</bibkey>
    </paper>
    <paper id="143">
      <title>Small Changes, Large Consequences: Analyzing the Allocational Fairness of <fixed-case>LLM</fixed-case>s in Hiring Contexts</title>
      <author><first>Preethi</first><last>Seshadri</last><affiliation>University of California, Irvine</affiliation></author>
      <author id="hongyu-chen" orcid="0009-0002-4064-0793"><first>Hongyu</first><last>Chen</last><affiliation>Cohere</affiliation></author>
      <author id="sameer-singh" orcid="0000-0003-0621-6323"><first>Sameer</first><last>Singh</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Seraphina</first><last>Goldfarb-Tarrant</last><affiliation>Cohere</affiliation></author>
      <pages>2645-2665</pages>
      <abstract>Large language models (LLMs) are increasingly being deployed in high-stakes applications like hiring, yet their potential for unfair decision-making remains understudied in generative and retrieval settings. In this work, we examine the allocational fairness of LLM-based hiring systems through two tasks that reflect actual HR usage: resume summarization and applicant ranking. By constructing a synthetic resume dataset with controlled perturbations and curating job postings, we investigate whether model behavior differs across demographic groups. Our findings reveal that generated summaries exhibit meaningful differences more frequently for race than for gender perturbations. Models also display non-uniform retrieval selection patterns across demographic groups and exhibit high ranking sensitivity to both gender and race perturbations. Surprisingly, retrieval models can show comparable sensitivity to both demographic and non-demographic changes, suggesting that fairness issues may stem from broader model brittleness. Overall, our results indicate that LLM-based hiring systems, especially in the retrieval stage, can exhibit notable biases that lead to discriminatory outcomes in real-world contexts.</abstract>
      <url hash="98087085">2025.ijcnlp-long.143</url>
      <bibkey>seshadri-etal-2025-small</bibkey>
    </paper>
    <paper id="144">
      <title><fixed-case>C</fixed-case>ulture<fixed-case>G</fixed-case>uard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</title>
      <author id="raviraj-bhuminand-joshi" orcid="0000-0003-1892-1812"><first>Raviraj Bhuminand</first><last>Joshi</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Rakesh</first><last>Paul</last></author>
      <author><first>Kanishk</first><last>Singla</last></author>
      <author><first>Anusha</first><last>Kamath</last></author>
      <author><first>Michael</first><last>Evans</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Katherine</first><last>Luna</last><affiliation>NVIDIA</affiliation></author>
      <author id="shaona-ghosh" orcid="0000-0003-4658-5174"><first>Shaona</first><last>Ghosh</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Utkarsh</first><last>Vaidya</last></author>
      <author><first>Eileen Margaret Peters</first><last>Long</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Sanjay Singh</first><last>Chauhan</last></author>
      <author><first>Niranjan</first><last>Wartikar</last><affiliation>NVIDIA</affiliation></author>
      <pages>2666-2685</pages>
      <abstract>The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets. We present CultureGuard, a novel solution for curating culturally aligned, high-quality safety datasets across multiple languages. Our approach introduces a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering. This pipeline enables the conversion and expansion of the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese. The resulting dataset, Nemotron-Safety-Guard-Dataset-v3, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-8B-v3 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. Furthermore, we show our moderately multilingual fine-tuning enables robust cross-lingual transfer and strong zero-shot generalization to unseen languages. We also benchmark the latest open LLMs on multilingual safety and observe that these LLMs are more prone to give unsafe responses when prompted in non-English languages. This work advances multilingual LLM safety by enabling the development of culturally aware safety guard models.</abstract>
      <url hash="08efad36">2025.ijcnlp-long.144</url>
      <bibkey>joshi-etal-2025-cultureguard</bibkey>
    </paper>
    <paper id="145">
      <title>Revisiting Word Embeddings in the <fixed-case>LLM</fixed-case> Era</title>
      <author id="yash-mahajan" orcid="0000-0003-4025-9896"><first>Yash</first><last>Mahajan</last></author>
      <author><first>Matthew</first><last>Freestone</last></author>
      <author><first>Naman</first><last>Bansal</last><affiliation>Google</affiliation></author>
      <author><first>Sathyanarayanan N.</first><last>Aakur</last><affiliation>Auburn University</affiliation></author>
      <author id="santu-karmaker" orcid="0000-0001-5744-6925"><first>Santu</first><last>Karmaker</last><affiliation>University of Central Florida</affiliation></author>
      <pages>2686-2717</pages>
      <abstract>Large Language Models (LLMs) have recently shown remarkable advancement in various NLP tasks. As such, a popular trend has emerged lately where NLP researchers extract word/sentence/document embeddings from these large decoder-only models and use them for various inference tasks with promising results. However, it is still unclear whether the performance improvement of LLM-induced embeddings is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This is the central question we investigate in the paper by systematically comparing classical decontextualized and contextualized word embeddings with the same for LLM-induced embeddings. Our results show that LLMs cluster semantically related words more tightly and perform better on analogy tasks in decontextualized settings. However, in contextualized settings, classical models like SimCSE often outperform LLMs in sentence-level similarity assessment tasks, highlighting their continued relevance for fine-grained semantics.</abstract>
      <url hash="ec913a0f">2025.ijcnlp-long.145</url>
      <bibkey>mahajan-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="146">
      <title>Who Remembers What? Tracing Information Fidelity in Human-<fixed-case>AI</fixed-case> Chains</title>
      <author id="suvojit-acharjee" orcid="0000-0002-7540-4777"><first>Suvojit</first><last>Acharjee</last><affiliation>Institute of Engineering and Management</affiliation></author>
      <author><first>Utathya</first><last>Aich</last><affiliation>Cnh Industrial</affiliation></author>
      <author><first>Diptarka</first><last>Mandal</last></author>
      <author><first>Asfak</first><last>Ali</last><affiliation>Jadavpur University, Kolkata</affiliation></author>
      <pages>2718-2726</pages>
      <abstract>In many real-world settings like journalism, law, medicine, and science communication, information is passed from one person or system to another through multiple rounds of summarization or rewriting. This process, known as multi-hop information transfer, also happens increasingly in workflows involving large language models (LLMs). But while summarization models and factuality metrics have improved, we still don’t fully understand how meaning and factual accuracy hold up across long chains of transformations, especially when both humans and LLMs are involved.In this paper, we take a fresh look at this problem by combining insights from cognitive science (Bartlett’s serial reproduction) and information theory (Shannon’s noisy-channel model). We build a new dataset of 700 five-step transmission chains that include human-only, LLM-only, mixed human-LLM, and cross-LLM settings across a wide range of source texts. To track how meaning degrades, we introduce three new metrics: Information Degradation Rate (IDR) for semantic drift, Meaning Preservation Entropy (MPE) for uncertainty in factual content, and Cascaded Hallucination Propagation Index (CHPI) for how hallucinations accumulate over time. Our findings reveal that hybrid chains behave asymmetrically. When a human summary is refined by a language model, the final output tends to preserve meaning well, suggesting that models can improve upon human-written summaries. The code and data will be available at : https://github.com/transtrace6/TransTrace.git.</abstract>
      <url hash="da973ee1">2025.ijcnlp-long.146</url>
      <bibkey>acharjee-etal-2025-remembers</bibkey>
    </paper>
    <paper id="147">
      <title><fixed-case>QA</fixed-case>-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs</title>
      <author><first>Maria</first><last>Tseytlin</last></author>
      <author><first>Paul</first><last>Roit</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Omri</first><last>Abend</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Ido</first><last>Dagan</last><affiliation>Bar-Ilan University</affiliation></author>
      <author id="ayal-klein" orcid="0000-0003-2665-7898"><first>Ayal</first><last>Klein</last><affiliation>Ariel University Center of Samaria</affiliation></author>
      <pages>2727-2741</pages>
      <abstract>Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR’s noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.</abstract>
      <url hash="dd7fc1f5">2025.ijcnlp-long.147</url>
      <bibkey>tseytlin-etal-2025-qa</bibkey>
    </paper>
    <paper id="148">
      <title>On Memorization of Large Language Models in Logical Reasoning</title>
      <author><first>Chulin</first><last>Xie</last><affiliation>Google</affiliation></author>
      <author><first>Yangsibo</first><last>Huang</last><affiliation>Google</affiliation></author>
      <author><first>Chiyuan</first><last>Zhang</last><affiliation>Google</affiliation></author>
      <author><first>Da</first><last>Yu</last><affiliation>Google</affiliation></author>
      <author><first>Xinyun</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author id="bill-yuchen-lin"><first>Bill Yuchen</first><last>Lin</last><affiliation>xAI and University of Washington</affiliation></author>
      <author><first>Bo</first><last>Li</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Badih</first><last>Ghazi</last><affiliation>Google</affiliation></author>
      <author id="ravi-kumar" orcid="0000-0002-2203-2586"><first>Ravi</first><last>Kumar</last><affiliation>Google</affiliation></author>
      <pages>2742-2785</pages>
      <abstract>Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs’ reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using two dynamically generated logical reasoning benchmarks based on Knights and Knaves (K&amp;K) puzzles and Zebra puzzles (DynamicZebra). We find that LLMs could interpolate and memorize the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet they struggle with slight variations of these puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. Through in-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers, we establish that LLMs develop reasoning skills on logical puzzles alongside memorization. Finally, our analysis based on a per-sample memorization score sheds light on how LLMs switch between reasoning and memorization when solving logical puzzles.</abstract>
      <url hash="e3d27a94">2025.ijcnlp-long.148</url>
      <bibkey>xie-etal-2025-memorization</bibkey>
    </paper>
    <paper id="149">
      <title><fixed-case>C</fixed-case>ontrol<fixed-case>M</fixed-case>ed: Adding Reasoning Control to Medical Language Model</title>
      <author><first>Sung-Min</first><last>Lee</last><affiliation>KT</affiliation></author>
      <author><first>Siyoon</first><last>Lee</last><affiliation>Korea Telecom Research</affiliation></author>
      <author><first>Juyeon</first><last>Kim</last><affiliation>Korea Telecom Research</affiliation></author>
      <author><first>Kyoungmin</first><last>Roh</last><affiliation>Korea Telecom Research</affiliation></author>
      <pages>2786-2799</pages>
      <abstract>Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce <b>ControlMed</b>, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both <i>direct</i> and <i>reasoning responses</i>; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis.</abstract>
      <url hash="c6a59440">2025.ijcnlp-long.149</url>
      <bibkey>lee-etal-2025-controlmed</bibkey>
    </paper>
    <paper id="150">
      <title>No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning.</title>
      <author><first>Abhishek</first><last>Rajgaria</last></author>
      <author><first>Kushagra</first><last>Dixit</last></author>
      <author><first>Mayank</first><last>Vyas</last></author>
      <author><first>Harshavardhan</first><last>Kalalbandi</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>2800-2821</pages>
      <abstract>Temporal Table Reasoning poses a significant challenge for Large Language Models (LLMs), requiring effective reasoning to extract relevant insights. Despite existence of multiple prompting methods, their impact on table reasoning remains largely unexplored. Furthermore, model performance varies drastically across different table and context structures, making it difficult to determine an optimal approach. This work investigates multiple prompting technique on diverse table types to determine that performance depends on factors such as entity type, table structure, requirement of additional context and question complexity, with “NO” single method consistently outperforming others. To address this, we introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts to context and integrates structured reasoning. SEAR_Unified, its cost-efficient variant. We also demonstrate that optional table refactoring (preprocessing) enhances both approaches when tables lack structural consistency. Our results demonstrate that SEAR prompts achieve superior performance across all table types compared to baseline prompting techniques</abstract>
      <url hash="f10a4b2a">2025.ijcnlp-long.150</url>
      <bibkey>rajgaria-etal-2025-universal</bibkey>
    </paper>
    <paper id="151">
      <title><fixed-case>C</fixed-case>lin<fixed-case>S</fixed-case>tructor: <fixed-case>AI</fixed-case>-Powered Structuring of Unstructured Clinical Texts</title>
      <author><first>Karthikeyan</first><last>K</last><affiliation>Duke University</affiliation></author>
      <author><first>Raghuveer</first><last>Thirukovalluru</last></author>
      <author id="david-carlson" orcid="0000-0003-1005-6385"><first>David</first><last>Carlson</last><affiliation>Duke University</affiliation></author>
      <pages>2822-2836</pages>
      <abstract>Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question–answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2–3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.</abstract>
      <url hash="fe29c431">2025.ijcnlp-long.151</url>
      <bibkey>k-etal-2025-clinstructor</bibkey>
    </paper>
    <paper id="152">
      <title>Adaptive Collaborative Labeling with <fixed-case>MLLM</fixed-case>s for Low-Resource Multimodal Emotion Recognition</title>
      <author><first>Wenwen</first><last>Zhuang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lu</first><last>Xiang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shubei</first><last>Tang</last></author>
      <author id="yaping-zhang" orcid="0000-0001-6892-905X"><first>Yaping</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yu</first><last>Zhou</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>2837-2853</pages>
      <abstract>Multimodal emotion recognition (MER) plays a crucial role in human-centric AI applications, yet existing models struggle in low-resource scenarios due to their heavy reliance on large amounts of high-quality labeled data. To address this challenge, we propose Adaptive Collaborative Labeling for Low-Resource MER (ACL-MER), a novel framework that leverages off-the-shelf multimodal large language models (MLLMs) to effectively exploit abundant unlabeled data. Specifically, ACL-MER incorporates a diverse teacher model zoo, wherein each MLLM specializes in a specific modality and is prompted to generate chain-of-thought predictions accompanied by scalar confidence scores. Rather than directly adopting these pseudo-labels, ACL-MER introduces an adaptive refinement strategy that selectively distills knowledge based on teacher confidence, iteratively guiding the lightweight student model toward robust learning under limited supervision. Extensive experiments on two benchmarks demonstrate that ACL-MER consistently outperforms strong baselines, especially in extremely low-resource settings.</abstract>
      <url hash="2afcba9a">2025.ijcnlp-long.152</url>
      <bibkey>zhuang-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="153">
      <title>Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning</title>
      <author id="nhi-hoai-doan" orcid="0009-0007-0354-4619"><first>Nhi Hoai</first><last>Doan</last></author>
      <author><first>Tatsuya</first><last>Hiraoka</last><affiliation>Nara Institute of Science and Technology, Mohamed bin Zayed University of Artificial Intelligence and RIKEN</affiliation></author>
      <author id="kentaro-inui" orcid="0000-0001-6510-604X"><first>Kentaro</first><last>Inui</last><affiliation>MBZUAI, RIKEN and Tohoku University</affiliation></author>
      <pages>2854-2876</pages>
      <abstract>This paper investigates the relationship between large language models’ (LLMs) ability to recognize repetitive input patterns and their performance on in-context learning (ICL). In contrast to prior work that has primarily focused on attention heads, we examine this relationship from the perspective of skill neurons, specifically repetition neurons. Our experiments reveal that the impact of these neurons on ICL performance varies depending on the depth of the layer in which they reside. By comparing the effects of repetition neurons and induction heads, we further identify strategies for reducing repetitive outputs while maintaining strong ICL capabilities.</abstract>
      <url hash="fb7a4789">2025.ijcnlp-long.153</url>
      <bibkey>doan-etal-2025-understanding</bibkey>
    </paper>
    <paper id="154">
      <title><fixed-case>R</fixed-case>e<fixed-case>V</fixed-case>ision: A Dataset and Baseline <fixed-case>VLM</fixed-case> for Privacy-Preserving Task-Oriented Visual Instruction Rewriting</title>
      <author><first>Abhijit</first><last>Mishra</last></author>
      <author id="mingda-li" orcid="0009-0000-6569-0290"><first>Mingda</first><last>Li</last></author>
      <author id="hsiang-fu" orcid="0009-0007-2334-1704"><first>Hsiang</first><last>Fu</last><affiliation>Arizona State University</affiliation></author>
      <author id="richard-noh" orcid="0009-0002-0539-9746"><first>Richard</first><last>Noh</last></author>
      <author id="minji-kim" orcid="0009-0007-4132-4393"><first>Minji</first><last>Kim</last></author>
      <pages>2877-2889</pages>
      <abstract>Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores **Visual Instruction Rewriting**, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs **(250M parameters)** with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (&lt;500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.</abstract>
      <url hash="01f7cb4f">2025.ijcnlp-long.154</url>
      <bibkey>mishra-etal-2025-revision</bibkey>
    </paper>
    <paper id="155">
      <title>Quantifying Cognitive Bias Induction in <fixed-case>LLM</fixed-case>-Generated Content</title>
      <author id="abeer-alessa" orcid="0009-0009-9929-4654"><first>Abeer</first><last>Alessa</last></author>
      <author><first>Param</first><last>Somane</last></author>
      <author><first>Akshaya Thenkarai</first><last>Lakshminarasimhan</last></author>
      <author><first>Julian</first><last>Skirzynski</last><affiliation>University of California, San Diego</affiliation></author>
      <author id="julian-mcauley" orcid="0000-0003-0955-7588"><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>Jessica Maria</first><last>Echterhoff</last><affiliation>Amazon</affiliation></author>
      <pages>2890-2910</pages>
      <abstract>Large language models (LLMs) are integrated into applications like shopping reviews, summarization, or medical diagnosis support, where their use affects human decisions. We investigate the extent to which LLMs expose users to biased content and demonstrate its effect on human decision-making. We assess five LLM families in summarization and news fact-checking tasks, evaluating the consistency of LLMs with their context and their tendency to hallucinate on a new self-updating dataset. Our findings show that LLMs expose users to content that changes the context’s sentiment in 26.42% of cases (framing bias), hallucinate on 60.33% of post-knowledge-cutoff questions, and highlight context from earlier parts of the prompt (primacy bias) in 10.12% of cases, averaged across all tested models. We further find that humans are 32% more likely to purchase the same product after reading a summary of the review generated by an LLM rather than the original review. To address these issues, we evaluate 18 mitigation methods across three LLM families and find the effectiveness of targeted interventions.</abstract>
      <url hash="056cd479">2025.ijcnlp-long.155</url>
      <bibkey>alessa-etal-2025-quantifying</bibkey>
    </paper>
    <paper id="156">
      <title>Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation</title>
      <author><first>Daniil</first><last>Gurgurov</last></author>
      <author><first>Katharina</first><last>Trinley</last></author>
      <author><first>Yusser</first><last>Al Ghussin</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Tanja</first><last>Baeumel</last></author>
      <author><first>Josef Van</first><last>Genabith</last></author>
      <author id="simon-ostermann" orcid="0000-0002-0899-0657"><first>Simon</first><last>Ostermann</last><affiliation>German Research Center for AI</affiliation></author>
      <pages>2911-2937</pages>
      <abstract>Large language models (LLMs) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B &amp; 32B across <tex-math>\textbf{21 typologically diverse languages}</tex-math>, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share overlapping neurons, reflecting internal representations of linguistic proximity.Through <tex-math>\textbf{language arithmetics}</tex-math>, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming established replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that neuron steering enhances downstream performance and reveal internal <tex-math>\textbf{"fallback"}</tex-math> mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at https://github.com/d-gurgurov/Language-Neurons-Manipulation.</abstract>
      <url hash="3905f837">2025.ijcnlp-long.156</url>
      <bibkey>gurgurov-etal-2025-language</bibkey>
    </paper>
    <paper id="157">
      <title><fixed-case>FOCUS</fixed-case>: A Benchmark for Targeted Socratic Question Generation via Source-Span Grounding</title>
      <author><first>Surawat</first><last>Pothong</last></author>
      <author id="machi-shimmei" orcid="0000-0002-4790-8863"><first>Machi</first><last>Shimmei</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Naoya</first><last>Inoue</last><affiliation>Ichikara, RIKEN and Japan Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Paul</first><last>Reisert</last><affiliation>Beyond Reason</affiliation></author>
      <author><first>Ana</first><last>Brassard</last><affiliation>RIKEN AIP</affiliation></author>
      <author><first>Wenzhi</first><last>Wang</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Shoichi</first><last>Naito</last><affiliation>Ricoh Company, Ltd.</affiliation></author>
      <author><first>Jungmin</first><last>Choi</last><affiliation>RIKEN</affiliation></author>
      <author id="kentaro-inui" orcid="0000-0001-6510-604X"><first>Kentaro</first><last>Inui</last><affiliation>MBZUAI, RIKEN and Tohoku University</affiliation></author>
      <pages>2938-2958</pages>
      <abstract>We present FOCUS, a benchmark and task setting for Socratic question generation that delivers more informative and targeted feedback to learners. Unlike prior datasets, which rely on broad typologies and lack grounding in the source text, FOCUS introduces a new formulation: each Socratic question is paired with a fine-grained, 11-type typology and an explicit source span from the argument it targets. This design supports clearer, more actionable feedback and facilitates interpretable model evaluation. FOCUS includes 440 annotated instances with moderate partial-match agreement, establishing it as a reliable benchmark. Baseline experiments with representative state-of-the-art models reveal, through detailed error analysis, that even strong models struggle with span selection and context-sensitive categories. An extension study on the LogicClimate dataset further confirms the generalizability of the task and annotation framework. FOCUS sets a new standard for pedagogically grounded and informative Socratic question generation.</abstract>
      <url hash="3b512125">2025.ijcnlp-long.157</url>
      <bibkey>pothong-etal-2025-focus</bibkey>
    </paper>
    <paper id="158">
      <title><fixed-case>M</fixed-case>ed<fixed-case>P</fixed-case>ath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking</title>
      <author id="nishant-mishra" orcid="0000-0003-3725-3987"><first>Nishant</first><last>Mishra</last></author>
      <author><first>Wilker</first><last>Aziz</last><affiliation>University of Amsterdam</affiliation></author>
      <author id="iacer-calixto" orcid="0000-0001-6244-7906"><first>Iacer</first><last>Calixto</last><affiliation>Amsterdam UMC, University of Amsterdam</affiliation></author>
      <pages>2959-2978</pages>
      <abstract>Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths—i.e., from general to specific—in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.</abstract>
      <url hash="1f7d914c">2025.ijcnlp-long.158</url>
      <bibkey>mishra-etal-2025-medpath</bibkey>
    </paper>
    <paper id="159">
      <title><fixed-case>AURA</fixed-case>-<fixed-case>QG</fixed-case>: Automated Unsupervised Replicable Assessment for Question Generation</title>
      <author><first>Rajshekar</first><last>K</last></author>
      <author id="harshad-khadilkar" orcid="0000-0003-3601-778X"><first>Harshad</first><last>Khadilkar</last><affiliation>Franklin Templeton and Indian Institute of Technology, Bombay</affiliation></author>
      <author id="pushpak-bhattacharyya"><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>2979-2992</pages>
      <abstract>Question Generation (QG) is central to information retrieval, education, and knowledge assessment, yet its progress is bottlenecked by unreliable and non-scalable evaluation practices. Traditional metrics fall short in structured settings like document-grounded QG, and human evaluation, while insightful, remains expensive, inconsistent, and difficult to replicate at scale. We introduce AURA-QG: an Automated, Unsupervised, Replicable Assessment pipeline that scores question sets using only the source document. It captures four orthogonal dimensions i.e., answerability, non-redundancy, coverage, and structural entropy, without needing reference questions or relative baselines. Our method is modular, efficient, and agnostic to the question generation strategy. Through extensive experiments across four domains i.e., car manuals, economic surveys, health brochures, and fiction, we demonstrate its robustness across input granularities and prompting paradigms. Chain-of-Thought prompting, which first extracts answer spans and then generates targeted questions, consistently yields higher answerability and coverage, validating the pipeline’s fidelity. The metrics also exhibit strong agreement with human judgments, reinforcing their reliability for practical adoption. The complete implementation of our evaluation pipeline is publicly available.</abstract>
      <url hash="491bcf3e">2025.ijcnlp-long.159</url>
      <bibkey>k-etal-2025-aura</bibkey>
    </paper>
    <paper id="160">
      <title><fixed-case>EFSA</fixed-case>-<fixed-case>CLC</fixed-case>: Enhancing Zero-shot Entity-level Financial Sentiment Analysis with Cross-lingual Collaboration</title>
      <author><first>Senbin</first><last>Zhu</last><affiliation>Zhengzhou University</affiliation></author>
      <author id="hongde-liu" orcid="0009-0003-3267-5002"><first>Hongde</first><last>Liu</last></author>
      <author><first>Chenyuan</first><last>He</last></author>
      <author id="yuxiang-jia" orcid="0000-0003-0481-0740"><first>Yuxiang</first><last>Jia</last><affiliation>Zhengzhou University</affiliation></author>
      <pages>2993-3003</pages>
      <abstract>Entity-level sentiment analysis is becoming increasingly important in the context of diverse financial texts, and large language models demonstrate significant potential under zero-shot settings. While it is well recognized that different languages embody distinct cognitive patterns, the use of multilingual capabilities in large language models to enable cross-lingual collaborative reasoning in the financial domain remains insufficiently studied. To address this, we propose a Cross-Lingual Collaboration (CLC) method: first, financial texts are aligned from one language to another based on semantic and syntactic structures, enabling the model to capture complementary linguistic features. Then, we integrate sentiment analysis results from both languages through redundancy removal and conflict resolution, enhancing the effectiveness of cross-lingual collaboration. Our experiments cover seven languages from three language families, including six UN official languages, and evaluate CLC on two English datasets and one Chinese dataset. Results show that multilingual collaboration improves sentiment analysis accuracy, especially among linguistically similar languages. Furthermore, stronger reasoning capabilities in LLMs amplify these benefits. Our code is available at https://anonymous.4open.science/r/Cross-lingual-Collaboration.</abstract>
      <url hash="b47175bc">2025.ijcnlp-long.160</url>
      <bibkey>zhu-etal-2025-efsa</bibkey>
    </paper>
    <paper id="161">
      <title>Enhancing Low-Resource Text Classification with <fixed-case>LLM</fixed-case>-Generated Corpora : A Case Study on Olfactory Reference Extraction</title>
      <author><first>Cédric</first><last>Boscher</last></author>
      <author><first>Shannon</first><last>Bruderer</last><affiliation>Université Lumiére (Lyon II)</affiliation></author>
      <author id="christine-largeron" orcid="0000-0003-1059-4095"><first>Christine</first><last>Largeron</last><affiliation>Université Jean Monnet</affiliation></author>
      <author id="veronique-eglin" orcid="0000-0001-8738-2088"><first>Véronique</first><last>Eglin</last><affiliation>Institut National des Sciences Appliquées de Lyon</affiliation></author>
      <author><first>Elöd</first><last>Egyed-Zsigmond</last></author>
      <pages>3004-3027</pages>
      <abstract>Extracting sensory information from text, particularly olfactory references, is challenging due to limited annotated datasets and the implicit, subjective nature of sensory experiences. This study investigates whether GPT-4o-generated data can complement or replace human annotations. We evaluate human- and LLM-labeled corpora on two tasks: coarse-grained detection of olfactory content and fine-grained sensory term extraction. Despite lexical variation, generated texts align well with real data in semantic and sensorimotor embedding spaces. Models trained on synthetic data perform strongly, especially in low-resource settings. Human annotations offer better recall by capturing implicit and diverse aspects of sensoriality, while GPT-4o annotations show higher precision through clearer pattern alignment. Data augmentation experiments confirm the utility of synthetic data, though trade-offs remain between label consistency and lexical diversity. These findings support using synthetic data to enhance sensory information mining when annotated data is limited.</abstract>
      <url hash="6cad8e2f">2025.ijcnlp-long.161</url>
      <bibkey>boscher-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="162">
      <title>Learning from *Sufficient* Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies</title>
      <author id="jonathan-kamp" orcid="0000-0003-0916-7459"><first>Jonathan</first><last>Kamp</last></author>
      <author><first>Lisa</first><last>Beinborn</last></author>
      <author id="antske-fokkens" orcid="0000-0002-6628-6916"><first>Antske</first><last>Fokkens</last><affiliation>VU University Amsterdam</affiliation></author>
      <pages>3028-3044</pages>
      <abstract>Human explanations of natural language, *rationales*, form a tool to assess whether models learn a label *for the right reasons* or rely on dataset-specific shortcuts. *Sufficiency* is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.</abstract>
      <url hash="43a7cbea">2025.ijcnlp-long.162</url>
      <bibkey>kamp-etal-2025-learning</bibkey>
    </paper>
    <paper id="163">
      <title><fixed-case>C</fixed-case>ontrast<fixed-case>S</fixed-case>core: Towards Higher Quality, Less Biased, More Efficient Evaluation Metrics with Contrastive Evaluation</title>
      <author id="xiao-wang-5933" orcid="0009-0004-5987-5933"><first>Xiao</first><last>Wang</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Daniil</first><last>Larionov</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Siwei</first><last>Wu</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author id="yiqi-liu" orcid="0000-0002-8070-5056"><first>Yiqi</first><last>Liu</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Steffen</first><last>Eger</last><affiliation>University of Technology Nuremberg</affiliation></author>
      <author id="nafise-sadat-moosavi" orcid="0000-0002-8332-307X"><first>Nafise Sadat</first><last>Moosavi</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <pages>3045-3060</pages>
      <abstract>Recent advances in automatic evaluation of natural language generation have increasingly relied on large language models as general-purpose metrics. While effective, these approaches often require high-capacity models, which introduce substantial computational costs, and remain susceptible to known evaluation pathologies, such as over-reliance on likelihood. We introduce ContrastScore, a contrastive evaluation paradigm that builds on the widely used BARTScore formulation by comparing token-level probabilities between a stronger and a weaker model. Instead of relying on single-model likelihoods or prompt-based judgments, ContrastScore captures disagreement between models to better reflect confidence and uncertainty in generation quality. Empirical results on summarization and machine translation benchmarks show that ContrastScore, instantiated with paired moderate-scale models across both Qwen and LLaMA families, consistently outperforms larger alternatives, such as Qwen 7B and LLaMA 8B, in correlation with human ratings. In addition to improving evaluation quality, ContrastScore significantly reduces susceptibility to likelihood bias, offering a more robust and cost-effective alternative to larger LLM-based evaluation methods.</abstract>
      <url hash="3f182673">2025.ijcnlp-long.163</url>
      <bibkey>wang-etal-2025-contrastscore</bibkey>
    </paper>
    <paper id="164">
      <title><fixed-case>M</fixed-case>odern<fixed-case>BERT</fixed-case> or <fixed-case>D</fixed-case>e<fixed-case>BERT</fixed-case>a<fixed-case>V</fixed-case>3? Examining Architecture and Data Influence on Transformer Encoder Models Performance</title>
      <author id="wissam-antoun" orcid="0000-0001-8021-5834"><first>Wissam</first><last>Antoun</last></author>
      <author id="benoit-sagot" orcid="0000-0002-0107-8526"><first>Benoît</first><last>Sagot</last><affiliation>Inria</affiliation></author>
      <author id="djame-seddah"><first>Djamé</first><last>Seddah</last><affiliation>Inria Paris</affiliation></author>
      <pages>3061-3074</pages>
      <abstract>Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT’s primary advantage being its support for long context, faster training, and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.</abstract>
      <url hash="37878e97">2025.ijcnlp-long.164</url>
      <bibkey>antoun-etal-2025-modernbert</bibkey>
    </paper>
    <paper id="165">
      <title>Simplified Rewriting Improves Expert Summarization</title>
      <author><first>Xingmeng</first><last>Zhao</last><affiliation>University of Colorado Anschutz Medical Campus</affiliation></author>
      <author id="tongnian-wang" orcid="0000-0003-3928-1754"><first>Tongnian</first><last>Wang</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <author><first>Anthony</first><last>Rios</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <pages>3075-3097</pages>
      <abstract>Radiology report summarization (RRS) is critical for clinical workflows, requiring concise Impressions “distilled from detailed Findings.” This paper proposes a novel prompting strategy that enhances RRS by introducing a layperson summary as an intermediate step. This summary helps normalize key observations and simplify complex terminology using communication techniques inspired by doctor–patient interactions. Combined with few-shot in-context learning, this approach improves the model’s ability to map generalized descriptions to specific clinical findings. We evaluate our method on three benchmark datasets, MIMIC-CXR, CheXpert, and MIMIC-III, and compare it against state-of-the-art open-source language models in the 7B/8B parameter range, such as Llama-3.1-8B-Instruct. Results show consistent improvements in summarization quality, with gains of up to 5% on some metrics for prompting, and more than 20% for some models when instruction tuning.</abstract>
      <url hash="2c7bb2fd">2025.ijcnlp-long.165</url>
      <bibkey>zhao-etal-2025-simplified</bibkey>
    </paper>
    <paper id="166">
      <title><fixed-case>RAST</fixed-case>e<fixed-case>R</fixed-case>: Robust, Agentic, and Structured Temporal Reasoning</title>
      <author><first>Dan</first><last>Schumacher</last><affiliation>University of Texas at San Antonio and University of Texas at San Antonio</affiliation></author>
      <author id="fatemeh-haji" orcid="0009-0002-5022-059X"><first>Fatemeh</first><last>Haji</last></author>
      <author><first>Tara</first><last>Grey</last></author>
      <author><first>Niharika</first><last>Bandlamudi</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <author><first>Nupoor</first><last>Karnik</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <author><first>Gagana Uday</first><last>Kumar</last></author>
      <author><first>Cho-Yu Jason</first><last>Chiang</last><affiliation>Peraton</affiliation></author>
      <author id="peyman-najafirad" orcid="0000-0001-9671-577X"><first>Peyman</first><last>Najafirad</last><affiliation>University of Texas, San Antonio</affiliation></author>
      <author id="nishant-vishwamitra" orcid="0000-0002-3728-1921"><first>Nishant</first><last>Vishwamitra</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <author><first>Anthony</first><last>Rios</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <pages>3098-3123</pages>
      <abstract>Temporal question answering (TQA) remains a persistent challenge for large language models (LLMs), particularly in retrieval-augmented generation (RAG) settings where retrieved content may be irrelevant, outdated, or temporally inconsistent. This is especially critical in applications like clinical event ordering, policy tracking, and real-time decision-making, which require reliable temporal reasoning even under noisy or misleading context. To address this challenge, we introduce RASTeR: Robust, Agentic, and Structured, Temporal Reasoning, an agentic prompting framework that separates context evaluation from answer generation. RASTeR first assesses the relevance and temporal coherence of retrieved context, then constructs a structured temporal knowledge graph (TKG) to better facilitate reasoning. When inconsistencies are detected, RASTeR selectively corrects or discards context before generating an answer. Across multiple datasets and LLMs, RASTeR consistently improves robustness: defined here as the model’s ability to generate correct predictions despite suboptimal context. We further validate our approach through a “needle-in-the-haystack” study, in which relevant context is buried among irrelevant distractors. Even with forty distractors, RASTeR achieves 75% accuracy, compared to the runner-up model, which reaches only 62%.</abstract>
      <url hash="7928ae81">2025.ijcnlp-long.166</url>
      <bibkey>schumacher-etal-2025-raster</bibkey>
    </paper>
    <paper id="167">
      <title><fixed-case>R</fixed-case>easoning<fixed-case>W</fixed-case>eekly: A General Knowledge and Verbal Reasoning Challenge for Large Language Models</title>
      <author><first>Zixuan</first><last>Wu</last><affiliation>Northeastern University</affiliation></author>
      <author id="francesca-lucchetti" orcid="0009-0002-5837-6097"><first>Francesca</first><last>Lucchetti</last></author>
      <author id="aleksander-boruch-gruszecki" orcid="0000-0001-5769-6684"><first>Aleksander</first><last>Boruch-Gruszecki</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jingmiao</first><last>Zhao</last><affiliation>Bloomberg</affiliation></author>
      <author id="carolyn-jane-anderson"><first>Carolyn Jane</first><last>Anderson</last><affiliation>Wellesley College</affiliation></author>
      <author id="joydeep-biswas" orcid="0000-0002-1211-1731"><first>Joydeep</first><last>Biswas</last><affiliation>NVIDIA and The University of Texas at Austin</affiliation></author>
      <author id="federico-cassano" orcid="0000-0002-9318-7454"><first>Federico</first><last>Cassano</last><affiliation>Cursor AI and Northeastern University</affiliation></author>
      <author id="arjun-guha" orcid="0000-0002-7493-3271"><first>Arjun</first><last>Guha</last><affiliation>Northeastern University</affiliation></author>
      <pages>3124-3140</pages>
      <abstract>Existing benchmarks for frontier models often test specialized, “PhD-level” knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark with 613 problems based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models; however correct solutions are easy to verify, and models’ mistakes are easy to spot. As LLMs are more widely deployed in society, we believe it is useful to develop benchmarks for frontier models that humans can understand without the need for deep domain expertise.Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models on our benchmark, despite being on par with other models when tested on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with “I give up” before providing an answer that it knows is wrong. R1 can also be remarkably “uncertain” in its output and in rare cases, it does not “finish thinking,” which suggests the need for techniques to “wrap up” before the context window limit is reached. We also quantify the effectiveness of reasoning longer to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.</abstract>
      <url hash="f5653161">2025.ijcnlp-long.167</url>
      <bibkey>wu-etal-2025-reasoningweekly</bibkey>
    </paper>
    <paper id="168">
      <title>Noise May Drown Out Words but Foster Compositionality: The Advantage of the Erasure and Deletion Noisy Channels on Emergent Communication</title>
      <author id="cezary-klamra" orcid="0000-0003-4321-8862"><first>Cezary</first><last>Klamra</last></author>
      <author><first>Francijn</first><last>Keur</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <author id="raquel-g-alhama" orcid="0000-0002-8573-6716"><first>Raquel G.</first><last>Alhama</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <pages>3141-3166</pages>
      <abstract>We investigate communication emerging in noisy environments with the goal of capturing the impact of message disruption on the emerged protocols. We implement two different noise mechanisms, inspired by the erasure and deletion channels studied in information theory, and simulate a referential game in a neural agent-based model with a variable message length channel. We leverage a stochastic evaluation setting to apply noise only after a message is sampled, which adds ecological validity and allows us to estimate information-theoretic measures of the emerged protocol directly from symbol probabilities. Contrary to our expectations, the emerged protocols do not become more redundant with the presence of noise; instead, we observe that certain levels of noise encourage the sender to produce more compositional messages, although the impact varies depending on the type of noise and input representation.</abstract>
      <url hash="5bae1868">2025.ijcnlp-long.168</url>
      <bibkey>klamra-etal-2025-noise</bibkey>
    </paper>
    <paper id="169">
      <title>Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels</title>
      <author><first>Sourya Dipta</first><last>Das</last><affiliation>SHL</affiliation></author>
      <author><first>Shubham</first><last>Kumar</last><affiliation>SHL Labs</affiliation></author>
      <author><first>Kuldeep</first><last>Yadav</last></author>
      <pages>3167-3179</pages>
      <abstract>Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.</abstract>
      <url hash="6e116778">2025.ijcnlp-long.169</url>
      <bibkey>das-etal-2025-zero</bibkey>
    </paper>
    <paper id="170">
      <title><fixed-case>LLM</fixed-case>-Guided Lifecycle-Aware Clustering of Multi-Turn Customer Support Conversations</title>
      <author id="priyaranjan-pattnayak" orcid="0009-0005-8299-0208"><first>Priyaranjan</first><last>Pattnayak</last><affiliation>Oracle</affiliation></author>
      <author><first>Sanchari</first><last>Chowdhuri</last><affiliation>Oracle</affiliation></author>
      <author id="amit-agarwal" orcid="0009-0004-8867-6271"><first>Amit</first><last>Agarwal</last><affiliation>Oracle</affiliation></author>
      <author id="hitesh-laxmichand-patel" orcid="0009-0009-7492-5173"><first>Hitesh Laxmichand</first><last>Patel</last><affiliation>Oracle</affiliation></author>
      <pages>3180-3206</pages>
      <abstract>Clustering customer chat data is vital for cloud providers handling multi-service queries. Traditional methods struggle with overlapping concerns and create broad, static clusters that degrade over time. Re-clustering disrupts continuity, making issue tracking difficult. We propose an adaptive system that segments multi-turn chats into service-specific concerns and incrementally refines clusters as new issues arise. Cluster quality is tracked via Davies–Bouldin Index (DBI) and Silhouette Scores, with LLM-based splitting applied only to degraded clusters. Our method improves Silhouette Scores by over 100% and reduces DBI by 65.6% compared to baselines, enabling scalable, real-time analytics without full re-clustering.</abstract>
      <url hash="bad0ebf5">2025.ijcnlp-long.170</url>
      <bibkey>pattnayak-etal-2025-llm</bibkey>
    </paper>
    <paper id="171">
      <title>Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities</title>
      <author><first>Yunxiang</first><last>Yan</last></author>
      <author><first>Tomohiro</first><last>Sawada</last></author>
      <author><first>Kartik</first><last>Goyal</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>3207-3234</pages>
      <abstract>While question-answering (QA) benchmark performance is an automatic and scalable method to compare LLMs, it is an indirect method of evaluating their underlying problem-solving capabilities. Therefore, we propose a holistic and generalizable framework based on **cascaded question disclosure** that provides a more accurate estimate of the models’ problem-solving capabilities while maintaining the scalability and automation. This approach collects model responses in a stagewise manner with each stage revealing partial information about the question designed to elicit generalized reasoning in LLMs. We find that our approach not only provides a better comparison between LLMs, but also induces better intermediate traces in models compared to the standard QA paradigm. We empirically verify this behavior on diverse reasoning and knowledge-heavy QA datasets by comparing LLMs of varying sizes and families. Our approach narrows the performance gap observed in the standard QA evaluation settings, indicating that the prevalent indirect QA paradigm of evaluation overestimates the differences in performance between models.We further validate our findings by extensive ablation studies.</abstract>
      <url hash="8243a315">2025.ijcnlp-long.171</url>
      <bibkey>yan-etal-2025-cascaded</bibkey>
    </paper>
    <paper id="172">
      <title>Minority-Aware Satisfaction Estimation in Dialogue Systems via Preference-Adaptive Reinforcement Learning</title>
      <author><first>Yahui</first><last>Fu</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Zi Haur</first><last>Pang</last><affiliation>Kyoto University</affiliation></author>
      <author id="tatsuya-kawahara" orcid="0000-0002-2686-2296"><first>Tatsuya</first><last>Kawahara</last><affiliation>Kyoto University</affiliation></author>
      <pages>3235-3249</pages>
      <abstract>User satisfaction in dialogue systems is inherently subjective. When the same response strategy is applied across users, minority users may assign different satisfaction ratings than majority users due to variations in individual intents and preferences. However, existing alignment methods typically train one-size-fits-all models that aim for broad consensus, often overlooking minority perspectives and user-specific adaptation. We propose a unified framework that models both individual- and group-level preferences for user satisfaction estimation. First, we introduce Chain-of-Personalized-Reasoning (CoPeR) to capture individual preferences through interpretable reasoning chains. Second, we propose an expectation-maximization-based Majority-Minority Preference-Aware Clustering (M²PC) algorithm that discovers distinct user groups in an unsupervised manner to learn group-level preferences. Finally, we integrate these components into a preference-adaptive reinforcement learning framework (PAda-PPO) that jointly optimizes alignment with both individual and group preferences. Experiments on the Emotional Support Conversation dataset demonstrate consistent improvements in user satisfaction estimation, particularly for underrepresented user groups.</abstract>
      <url hash="10e08bec">2025.ijcnlp-long.172</url>
      <bibkey>fu-etal-2025-minority</bibkey>
    </paper>
    <paper id="173">
      <title>The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1</title>
      <author><first>Kaiwen</first><last>Zhou</last></author>
      <author><first>Chengzhi</first><last>Liu</last></author>
      <author><first>Xuandong</first><last>Zhao</last><affiliation>UC Berkeley</affiliation></author>
      <author><first>Shreedhar</first><last>Jangam</last></author>
      <author><first>Jayanth</first><last>Srinivasa</last></author>
      <author id="gaowen-liu" orcid="0009-0000-9194-1233"><first>Gaowen</first><last>Liu</last></author>
      <author><first>Dawn</first><last>Song</last><affiliation>University of California Berkeley</affiliation></author>
      <author id="xin-eric-wang" orcid="0000-0003-2605-5504"><first>Xin Eric</first><last>Wang</last><affiliation>University of California, Santa Barbara and Simular</affiliation></author>
      <pages>3250-3265</pages>
      <abstract>The rapid development of large reasoning models (LRMs), such as OpenAI-o3 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models (LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise serious safety concerns, particularly regarding their potential for misuse. In this work, we present a comprehensive safety assessment of these reasoning models, leveraging established safety benchmarks to evaluate their compliance with safety regulations. Furthermore, we investigate their susceptibility to adversarial attacks, such as jailbreaking and prompt injection, to assess their robustness in real-world applications. Through our multi-faceted analysis, we uncover four key findings: (1) There is a significant safety gap between the open-source reasoning models and the o3-mini model, on both safety benchmark and attack, suggesting more safety effort on open LRMs is needed. (2) The distilled reasoning model shows poorer safety performance compared to its safety-aligned base models. (3) The stronger the model’s reasoning ability, the greater the potential harm it may cause when answering unsafe questions. (4) The thinking process in R1 models poses greater safety concerns than their final answers. Our study provides insights into the security implications of reasoning models and highlights the need for further advancements in R1 models’ safety to close the gap.</abstract>
      <url hash="631b49a1">2025.ijcnlp-long.173</url>
      <bibkey>zhou-etal-2025-hidden</bibkey>
    </paper>
    <paper id="174">
      <title>Agnus <fixed-case>LLM</fixed-case>: Robust and Flexible Entity Disambiguation with decoder-only Language Models</title>
      <author><first>Kristian</first><last>Noullet</last></author>
      <author id="ayoub-ourgani" orcid="0009-0006-4507-0848"><first>Ayoub</first><last>Ourgani</last></author>
      <author id="niklas-thomas-lakner" orcid="0009-0001-7855-3524"><first>Niklas Thomas</first><last>Lakner</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author id="lukas-kinder" orcid="0009-0008-8275-1284"><first>Lukas</first><last>Kinder</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author id="tobias-kafer" orcid="0000-0003-0576-7457"><first>Tobias</first><last>Käfer</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <pages>3266-3284</pages>
      <abstract>Entity disambiguation (ED) links ambiguous mentions in text to entries in a knowledge base and is a core task in entity linking systems. While pretrained decoder-only language models (DLMs) offer strong generalization capabilities, their effective use in ED has been restricted due to sensitivity to candidate order, susceptibility to hallucinated outputs, and potential dataset leakage. We introduce Agnus a zero-shot ED framework that addresses these challenges through three core innovations: (1) order-invariant candidate encoding via shared positional embeddings and modified autoregressive attention masking, which eliminates bias on input ordering; (2) constrained decoding that ensures outputs are restricted to valid candidates, effectively preventing hallucinations; and (3) synthetic dataset creation approach as a diagnostic tool for data contamination detection and mitigation. Agnus eliminates up to 15.2% of F1 variability caused by candidate permutations, delivering consistent and order-robust predictions previously unattainable with autoregressive architectures. In our experiments, Agnus achieves state-of-the-art performance on four standard ED benchmarks, surpassing prior zero-shot approaches by an average 3.7% using small language models. We release code, data including candidate sets, and a synthetic benchmark to support reproducibility and controlled evaluation.</abstract>
      <url hash="facf8d7e">2025.ijcnlp-long.174</url>
      <bibkey>noullet-etal-2025-agnus</bibkey>
    </paper>
    <paper id="175">
      <title><fixed-case>M</fixed-case>u<fixed-case>S</fixed-case>ci<fixed-case>C</fixed-case>laims: Multimodal Scientific Claim Verification</title>
      <author><first>Yash Kumar</first><last>Lal</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Manikanta</first><last>Bandham</last></author>
      <author><first>Mohammad Saqib</first><last>Hasan</last></author>
      <author><first>Apoorva</first><last>Kashi</last></author>
      <author><first>Mahnaz</first><last>Koupaee</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <pages>3285-3307</pages>
      <abstract>Assessing scientific claims requires identifying, extracting, and reasoning with multimodal data expressed in information-rich figures in scientific literature. Despite the large body of work in scientific QA, figure captioning, and other multimodal reasoning tasks over chart-based data, there are no readily usable multimodal benchmarks that directly test claim verification abilities. To remedy this gap, we introduce a new benchmark MuSciClaims accompanied by diagnostics tasks. We automatically extract supported claims from scientific articles, which we manually perturb to produce contradicted claims. The perturbations are designed to test for a specific set of claim verification capabilities. We also introduce a suite of diagnostic tasks that help understand model failures. Our results show most vision-language models are poor (~0.3-0.5 F1), with even the best model only achieving 0.72 F1. They are also biased towards judging claims as supported, likely misunderstanding nuanced perturbations within the claims. Our diagnostics show models are bad at localizing correct evidence within figures, struggle with aggregating information across modalities, and often fail to understand basic components of the figure.</abstract>
      <url hash="ad84b544">2025.ijcnlp-long.175</url>
      <bibkey>lal-etal-2025-musciclaims</bibkey>
    </paper>
    <paper id="176">
      <title>Program Synthesis Dialog Agents for Interactive Decision-Making</title>
      <author><first>Matthew</first><last>Toles</last></author>
      <author><first>Nikhil</first><last>Balwani</last></author>
      <author><first>Rattandeep</first><last>Singh</last></author>
      <author><first>Valentina Giulia Sartori</first><last>Rodriguez</last></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>3308-3323</pages>
      <abstract>Many real-world eligibility problems, ranging from medical diagnosis to tax planning, can be mapped to decision problems expressed in natural language, wherein a model must make a binary choice based on the features of the user. Large-scale domains such as legal codes or frequently updated funding opportunities render human annotation (e.g., web forms or decision trees) impractical, suggesting a need for agents that can automatically assist in decision-making. Since relevant information is often only known to the user, it is important that these agents can ask the right questions. To evaluate this task, we propose BeNYfits, a new benchmark for determining user eligibility for multiple overlapping social benefits opportunities through interactive decision-making. Our experiments show that current language models struggle with frequent hallucinations, with GPT-4o scoring only 35.7 F1 using a ReAct-style chain-of-thought. We therefore introduce ProADA, a novel approach that uses program synthesis to assist in decision-making by mapping dialog planning to a code generation problem and using gaps in structured data to determine the best next action. Our agent, ProADA, improves the F1 score to 56.2 while using nearly the same number of dialog turns.</abstract>
      <url hash="8febd9e4">2025.ijcnlp-long.176</url>
      <bibkey>toles-etal-2025-program</bibkey>
    </paper>
    <paper id="177">
      <title>Learning a Continue-Thinking Token for Enhanced Test-Time Scaling</title>
      <author id="liran-ringel" orcid="0009-0006-5822-6856"><first>Liran</first><last>Ringel</last><affiliation>Computer Science Departmen, Technion-Israel Institute of Technology</affiliation></author>
      <author><first>Elad</first><last>Tolochinsky</last><affiliation>Technion - Israel Institute of Technology, Technion</affiliation></author>
      <author><first>Yaniv</first><last>Romano</last><affiliation>Technion, Technion</affiliation></author>
      <pages>3324-3345</pages>
      <abstract>Test-time scaling has emerged as an effective approach for improving language model performance by utilizing additional compute at inference time. Recent studies have shown that overriding end-of-thinking tokens (e.g., replacing "/think&gt;” with “Wait”) can extend reasoning steps and improve accuracy. In this work, we explore whether a dedicated continue-thinking token can be learned to trigger extended reasoning. We augment distilled versions of DeepSeek-R1 with a single learned "&lt;|continue-thinking|&gt;” token, training only its embedding via reinforcement learning while keeping the model weights frozen. Our experiments show that this learned token achieves improved accuracy on standard math benchmarks compared to both the baseline model and a test-time scaling approach that uses a fixed token (e.g., “Wait”) for budget forcing. In particular, we observe that in cases where the fixed-token approach enhances the base model’s accuracy, our method achieves a markedly greater improvement. For example, on the GSM8K benchmark, the fixed-token approach yields a 1.3% absolute improvement in accuracy, whereas our learned-token method achieves a 4.2% improvement over the base model that does not use budget forcing.</abstract>
      <url hash="fbb53f1e">2025.ijcnlp-long.177</url>
      <bibkey>ringel-etal-2025-learning</bibkey>
    </paper>
    <paper id="178">
      <title>Video-guided Machine Translation: A Survey of Models, Datasets, and Challenges</title>
      <author><first>Pinaki</first><last>Das</last></author>
      <author><first>Virendra</first><last>Singh</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author id="pushpak-bhattacharyya"><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author id="gholamreza-haffari"><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University, Monash University and Monash University</affiliation></author>
      <pages>3346-3356</pages>
      <abstract>In recent years, machine translation has evolved with the integration of multimodal information. Infusion of multi-modality into translation tasks decreases ambiguation and enhances translation scores. Common modalities include images, speech, and videos, which provide additional context alongside the text to be translated. While multimodal translation with images has been extensively studied, video-guided machine translation (VMT) has gained increasing attention, particularly since Wang et al. 2019 first explored this task. In this paper, we provide a comprehensive overview of VMT, highlighting its unique challenges, methodologies, and recent advancements. Unlike previous surveys that primarily focus on image-guided multimodal machine translation, this work explores the distinct complexities and opportunities introduced by adding video as a modality to the translation task.</abstract>
      <url hash="f8e192da">2025.ijcnlp-long.178</url>
      <bibkey>das-etal-2025-video</bibkey>
    </paper>
    <paper id="179">
      <title><fixed-case>P</fixed-case>ro<fixed-case>ST</fixed-case>: Progressive Sub-task Training for <fixed-case>P</fixed-case>areto-Optimal Multi-agent Systems Using Small Language Models</title>
      <author id="biddut-sarker-bijoy" orcid="0000-0003-4698-4554"><first>Biddut Sarker</first><last>Bijoy</last><affiliation>, State University of New York at Stony Brook</affiliation></author>
      <author><first>Mohammad Saqib</first><last>Hasan</last></author>
      <author id="pegah-alipoormolabashi" orcid="0009-0009-0131-2579"><first>Pegah</first><last>Alipoormolabashi</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author id="avirup-sil"><first>Avirup</first><last>Sil</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Aruna</first><last>Balasubramanian</last><affiliation>, State University of New York, Stony Brook</affiliation></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <pages>3357-3375</pages>
      <abstract>Multi-agent systems with smaller language models (SLMs) present a viable alternative to single agent systems powered by large language models (LLMs) for addressing complex problems. In this work, we study how these alternatives compare in terms of both effectiveness and efficiency. To study this trade-off, we instantiate single and multi-agent systems for the complex problems in the AppWorld environment using different sized language models.We find that difficulties with long-trajectory learning in smaller language models (SLMs) limit their performance. Even when trained for specialized roles, SLMs fail to learn all subtasks effectively. To address this issue, we introduce a simple progressive sub-task training strategy, which introduces new sub-tasks progressively in each training epoch. We find that this novel strategy, analogous to instance level curriculum learning, consistently improves the effectiveness of multi-agents at all configurations. Our Pareto analysis shows that fine-tuned multi-agent systems yield better effectiveness-efficiency trade-offs. Additional ablations and analyses shows the importance of our progressive training strategy and its ability to reduce subtask error rates.</abstract>
      <url hash="22db75bc">2025.ijcnlp-long.179</url>
      <bibkey>bijoy-etal-2025-prost</bibkey>
    </paper>
    <paper id="180">
      <title>Rethinking Large Language Model Architectures for Sequential Recommendations</title>
      <author id="hanbing-wang" orcid="0000-0002-5198-8085"><first>Hanbing</first><last>Wang</last></author>
      <author id="xiaorui-liu" orcid="0000-0001-8217-5688"><first>Xiaorui</first><last>Liu</last><affiliation>North Carolina State University</affiliation></author>
      <author id="wenqi-fan" orcid="0000-0002-4049-1233"><first>Wenqi</first><last>Fan</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author id="xiangyu-zhao" orcid="0000-0003-2926-4416"><first>Xiangyu</first><last>Zhao</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Venkataramana</first><last>Kini</last><affiliation>Amazon</affiliation></author>
      <author><first>Devendra Pratap</first><last>Yadav</last><affiliation>Amazon</affiliation></author>
      <author id="fei-wang-0602" orcid="0000-0001-8265-0602"><first>Fei</first><last>Wang</last><affiliation>KronosAI</affiliation></author>
      <author><first>Zhen</first><last>Wen</last><affiliation>Amazon</affiliation></author>
      <author id="hui-liu-3495" orcid="0000-0002-3555-3495"><first>Hui</first><last>Liu</last></author>
      <pages>3376-3391</pages>
      <abstract>In recent times, there has been a shift towards adapting sequential recommendation to LLM paradigm to harness the capabilities of LLMs. These methods typically formulate recommendation data into natural language and train the model to forecast the subsequent item in an auto-regressive manner. Despite their notable success, the significant computational burden during inference poses a major challenge to their practical implementation. In this study, we aim to streamline current LLM-based recommendation models and introduce a straightforward yet highly effective model Lite-LLM4Rec. The primary objective of Lite-LLM4Rec is to ensure efficient inference for the sequential recommendation task. Lite-LLM4Rec circumvents the step-by-step beam search decoding by employing a direct item projection head to produce ranking scores in one step. This design arises from our empirical finding that beam search decoding is ultimately unnecessary for sequential recommendations. Additionally, Lite-LLM4Rec introduces a hierarchical LLM structure crafted to efficiently handle the extensive contextual information of items and redundant computation issue, thus diminishing computational overhead while enjoying the power of LLMs. Experiments on four publicly available datasets validate the efficacy of Lite-LLM4Rec in enhancing both performance and inference efficiency (notably 46.8% performance improvement and 99.48% efficiency improvement on ML-1m) compared to existing LLM-based methods. Our implementations are available at: <url>https://github.com/HanbingWang2001/Lite-LLM4Rec-PyTorch</url>.</abstract>
      <url hash="04a5c6bf">2025.ijcnlp-long.180</url>
      <bibkey>wang-etal-2025-rethinking-large</bibkey>
    </paper>
    <paper id="181">
      <title><fixed-case>DSBC</fixed-case> : Data Science task Benchmarking with Context engineering</title>
      <author id="ram-mohan-rao-kadiyala" orcid="0009-0008-3783-5550"><first>Ram Mohan Rao</first><last>Kadiyala</last></author>
      <author id="jebish-purbey" orcid="0009-0000-5620-3024"><first>Jebish</first><last>Purbey</last><affiliation>Tribhuvan University</affiliation></author>
      <author id="siddhant-gupta" orcid="0009-0009-2311-8312"><first>Siddhant</first><last>Gupta</last></author>
      <author><first>Giulio</first><last>Martini</last></author>
      <author id="suman-debnath" orcid="0009-0009-5667-2432"><first>Suman</first><last>Debnath</last></author>
      <author><first>Hamza</first><last>Farooq</last><affiliation>University of California, Los Angeles and Stanford University</affiliation></author>
      <pages>3392-3424</pages>
      <abstract>Recent advances in large language models (LLMs) have significantly impacted data science workflows, giving rise to specialized data science agents designed to automate analytical tasks. Despite rapid adoption, systematic benchmarks evaluating the efficacy and limitations of these agents remain scarce. In this paper, we introduce a comprehensive benchmark specifically crafted to reflect real-world user interactions with data science agents by observing usage of our commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet, Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with context engineering, multi-step with context engineering, and with SmolAgent. Our benchmark assesses performance across a diverse set of eight data science task categories, additionally exploring the sensitivity of models to common prompting issues, such as data leakage and slightly ambiguous instructions. We further investigate the influence of temperature parameters on overall and task-specific outcomes for each model and approach. Our findings reveal distinct performance disparities among the evaluated models and methodologies, highlighting critical factors that affect practical deployment. The benchmark dataset and evaluation framework introduced herein aim to provide a foundation for future research of more robust and effective data science agents.</abstract>
      <url hash="725972b0">2025.ijcnlp-long.181</url>
      <bibkey>kadiyala-etal-2025-dsbc</bibkey>
    </paper>
    <paper id="182">
      <title>Improving Document Retrieval Coherence for Semantically Equivalent Queries</title>
      <author><first>Stefano</first><last>Campese</last></author>
      <author id="alessandro-moschitti" orcid="0000-0003-2216-8034"><first>Alessandro</first><last>Moschitti</last><affiliation>Amazon AGI</affiliation></author>
      <author><first>Ivano</first><last>Lauriola</last><affiliation>Amazon AGI</affiliation></author>
      <pages>3425-3441</pages>
      <abstract>Dense Retrieval (DR) models have proven to be effective for Document Retrieval and Information Grounding tasks. Usually, these models are trained and optimized for improving the relevance of top-ranked documents for a given query. Previous work has shown that popular DR models are sensitive to the query and document lexicon: small variations of it may lead to a significant difference in the set of retrieved documents. In this paper, we propose a variation of the Multi-Negative Ranking loss for training DR that improves the coherence of models in retrieving the same documents with respect to semantically similar queries. The loss penalizes discrepancies between the top-k ranked documents retrieved for diverse but semantically equivalent queries. We conducted extensive experiments on various datasets, MS-MARCO, Natural Questions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes by our loss are subject to lower sensitivity, and, (ii) interestingly, higher accuracy.</abstract>
      <url hash="01a207bc">2025.ijcnlp-long.182</url>
      <bibkey>campese-etal-2025-improving</bibkey>
    </paper>
    <paper id="183">
      <title>Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models</title>
      <author><first>Akshar</first><last>Tumu</last></author>
      <author><first>Varad</first><last>Shinde</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>3442-3455</pages>
      <abstract>Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis papers often use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation (‘not’). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions.</abstract>
      <url hash="32b1b9c0">2025.ijcnlp-long.183</url>
      <bibkey>tumu-etal-2025-referring</bibkey>
    </paper>
    <paper id="184">
      <title><fixed-case>FINDR</fixed-case>: A Fast Influential Data Selector for <fixed-case>NL</fixed-case>2<fixed-case>C</fixed-case>ode Pretraining</title>
      <author><first>Xinliang Frederick</first><last>Zhang</last></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>3456-3476</pages>
      <abstract>Pretraining on massive corpora has given rise to large language models (LLMs) with multi-task capabilities. However, real-world applications often require more specialized training, as is the case of NL2Code. We approach this specialization through the lens of data selection, i.e., identifying a subset of a large corpus that aligns with a desired target distribution—a challenge that remains under-explored within NL2Code. Existing methods are typically designed for selecting instruction-tuning data, and might not easily scale to large-scale code repositories; while methods for NL2Code do exist, they primarily rely on coarse heuristics—–such as repo stars—–for filtering. To bridge this gap, we propose FINDR, an efficient data selection method that extends logistic regression with feature-wise importance reweighting—marking it, to our knowledge, the first fine-grained solution to NL2Code pretraining. Our method uses hashed n-grams and code-aware features to capture code-specific patterns, and then apply informative priors to reweight feature importance when computing influence scores. Extensive experiments on NL2Python and NL2SQL, with two model families, show that FINDR consistently outperforms strong baselines in both execution accuracy and token efficiency. Notably, pretraining on only 2% of FINDR-selected data boosts Gemma by over 29% in both domains, even surpassing CodeGemma (pretrained on 300x more examples) by 10% in Python.</abstract>
      <url hash="188dbc21">2025.ijcnlp-long.184</url>
      <bibkey>zhang-wang-2025-findr</bibkey>
    </paper>
    <paper id="185">
      <title>Found in Translation: Measuring Multilingual <fixed-case>LLM</fixed-case> Consistency as Simple as Translate then Evaluate</title>
      <author><first>Ashim</first><last>Gupta</last></author>
      <author><first>Maitrey</first><last>Mehta</last><affiliation>University of Utah</affiliation></author>
      <author id="zhichao-xu" orcid="0000-0002-2370-4487"><first>Zhichao</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Vivek</first><last>Srikumar</last><affiliation>University of Utah</affiliation></author>
      <pages>3477-3496</pages>
      <abstract>Large language models (LLMs) provide detailed and impressive responses to queries in English. However, are they really consistent at responding to the same query in other languages? The popular way of evaluating for multilingual performance of LLMs requires expensive-to-collect annotated datasets. Further, evaluating for tasks like open-ended generation, where multiple correct answers may exist, is nontrivial. Instead, we propose to evaluate the predictability of model response across different languages. In this work, we propose a framework to evaluate LLM’s cross-lingual consistency based on a simple Translate then Evaluate strategy. We instantiate this evaluation framework along two dimensions of consistency: information and empathy. Our results reveal pronounced inconsistencies in popular LLM responses across thirty languages, with severe performance deficits in certain language families and scripts, underscoring critical weaknesses in their multilingual capabilities. These findings necessitate cross-lingual evaluations that are consistent along multiple dimensions. We invite practitioners to use our framework for future multilingual LLM benchmarking.</abstract>
      <url hash="a02ef2b9">2025.ijcnlp-long.185</url>
      <bibkey>gupta-etal-2025-found</bibkey>
    </paper>
    <paper id="186">
      <title>Do Persona-Infused <fixed-case>LLM</fixed-case>s Affect Performance in a Strategic Reasoning Game?</title>
      <author id="john-licato" orcid="0000-0003-4700-9750"><first>John</first><last>Licato</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Stephen</first><last>Steinle</last><affiliation>University of South Florida</affiliation></author>
      <pages>3497-3528</pages>
      <abstract>Although the use of persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences. Furthermore, little work has studied whether these differences, when they do exist, can affect decision-making in an adversarial strategic environment. We investigate the impact of persona prompting on strategic performance in PERIL, a world domination board game. Specifically, we compare the effectiveness of persona-derived heuristics to those chosen manually. Our findings reveal that personality traits intuitively associated with strategic thinking do appear to improve game performance, but only when an additional mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into strategic heuristics. Results indicate our method enhances heuristic reliability and face validity when compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision-making behaviors. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a novel heuristic generation method that adds to the growing body of work applying psychometric principles to LLMs.</abstract>
      <url hash="8931da43">2025.ijcnlp-long.186</url>
      <bibkey>licato-steinle-2025-persona</bibkey>
    </paper>
    <paper id="187">
      <title><fixed-case>F</fixed-case>ar<fixed-case>S</fixed-case>ense: A Comprehensive Commonsense Benchmark and Evaluation Framework for the <fixed-case>F</fixed-case>arsi Language</title>
      <author><first>Kamyar</first><last>Zeinalipour</last></author>
      <author><first>Neda</first><last>Jamshidi</last></author>
      <author id="seyedehbahareh-hejazi" orcid="0000-0001-8316-5588"><first>Seyedehbahareh</first><last>Hejazi</last></author>
      <author id="marco-maggini" orcid="0000-0002-6428-1265"><first>Marco</first><last>Maggini</last></author>
      <author id="monica-bianchini" orcid="0000-0002-8206-8142"><first>Monica</first><last>Bianchini</last></author>
      <author><first>Simone</first><last>Paoletti</last><affiliation>University of Siena</affiliation></author>
      <author id="marco-gori" orcid="0000-0001-6337-5430"><first>Marco</first><last>Gori</last><affiliation>University of Siena</affiliation></author>
      <pages>3529-3599</pages>
      <abstract>Although Farsi is widely spoken, no comprehensive benchmark exists for assessing commonsense reasoning in language models. We therefore present <b>FarSense</b>, a 6‐task benchmark for Farsi covering True/False judgment, multiple-choice questions, Explanation, Cause‐Effect inference, Counterfactual reasoning, and Knowledge Completion. Starting from Farsi‐Wikipedia, we filtered noise and retained ~4,210 passages, rewrote them into realistic daily scenarios, and derived the above tasks from each scenario. Scenario and task generation quality was first judged via native‐speaker annotations on outputs from five major LLMs—GPT‐4o, Gemini-2.5-Flash, Mistral-Large, Qwen‐Plus, and DeepSeek‐Chat. Gemini-2.5-Flash demonstrated the highest performance, leading to its use in generating a large-scale dataset, subsequently finalized through meticulous two-step human validation. Using <b>FarSense</b>, we measured the commonsense ability of the same five flagship LLMs and also fine‐tuned six compact models (1B–24B parameters) before re‐evaluating them. To ensure broad applicability, task wording was designed to minimize dialectal, cultural, or religious bias. Experiments show that targeted fine‐tuning yields substantial gains, confirming <b>FarSense</b> as a reliable, openly licensed resource for advancing reproducible commonsense understanding research in Farsi NLP. We publicly release all code and data at https://github.com/KamyarZeinalipour/FarSense.</abstract>
      <url hash="b18c6c00">2025.ijcnlp-long.187</url>
      <bibkey>zeinalipour-etal-2025-farsense</bibkey>
    </paper>
    <paper id="188">
      <title><fixed-case>GL</fixed-case>-<fixed-case>CL</fixed-case>i<fixed-case>C</fixed-case>: Global-Local Coherence and Lexical Complexity for Sentence-Level <fixed-case>AI</fixed-case>-Generated Text Detection</title>
      <author id="rizky-adi" orcid="0009-0003-2673-1741"><first>Rizky</first><last>Adi</last><affiliation>Yamanashi University</affiliation></author>
      <author><first>Bassamtiano Renaufalgi</first><last>Irnawan</last></author>
      <author id="yoshimi-suzuki" orcid="0000-0001-5466-7351"><first>Yoshimi</first><last>Suzuki</last><affiliation>Yamanashi University</affiliation></author>
      <author id="fumiyo-fukumoto" orcid="0000-0001-7858-6206"><first>Fumiyo</first><last>Fukumoto</last><affiliation>Yamanashi University</affiliation></author>
      <pages>3600-3617</pages>
      <abstract>Unlike document-level AI-generated text (AIGT) detection, sentence-level AIGT detection remains underexplored, despite its importance for addressing collaborative writing scenarios where humans modify AIGT suggestions on a sentence-by-sentence basis. Prior sentence-level detectors often neglect the valuable context surrounding the target sentence, which may contain crucial linguistic artifacts that indicate a potential change in authorship. We propose **GL-CLiC**, a novel technique that leverages both **G**lobal and **L**ocal signals of **C**oherence and **L**ex**i**cal **C**omplexity, which we operationalize through discourse analysis and CEFR-based vocabulary sophistication. **GL-CLiC** models local coherence and lexical complexity by examining a sentence’s relationship with its neighbors or peers, complemented with its document-wide analysis. Our experimental results show that **GL-CLiC** achieves superior performance and better generalization across domains compared to existing methods.</abstract>
      <url hash="1931abcf">2025.ijcnlp-long.188</url>
      <bibkey>adi-etal-2025-gl</bibkey>
    </paper>
    <paper id="189">
      <title>Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance</title>
      <author id="ram-mohan-rao-kadiyala" orcid="0009-0008-3783-5550"><first>Ram Mohan Rao</first><last>Kadiyala</last></author>
      <author><first>Siddartha</first><last>Pullakhandam</last></author>
      <author id="siddhant-gupta" orcid="0009-0009-2311-8312"><first>Siddhant</first><last>Gupta</last></author>
      <author id="jebish-purbey" orcid="0009-0000-5620-3024"><first>Jebish</first><last>Purbey</last><affiliation>Tribhuvan University</affiliation></author>
      <author id="drishti-sharma" orcid="0009-0004-7007-6091"><first>Drishti</first><last>Sharma</last><affiliation>Cohere Labs</affiliation></author>
      <author><first>Kanwal</first><last>Mehreen</last><affiliation>Cohere Community</affiliation></author>
      <author id="muhammad-arham" orcid="0009-0003-3868-1120"><first>Muhammad</first><last>Arham</last></author>
      <author id="suman-debnath" orcid="0009-0009-5667-2432"><first>Suman</first><last>Debnath</last></author>
      <author><first>Hamza</first><last>Farooq</last><affiliation>University of California, Los Angeles and Stanford University</affiliation></author>
      <pages>3618-3641</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable capabilities, but their development has primarily focused on English and other high-resource languages, leaving many languages underserved. We present our latest Hindi-English bi-lingual LLM with ~3% average improvement in benchmark scores over both languages, outperforming models twice its size. Using a curated dataset composed of English and Hindi instruction data of 485K samples, we instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve performance over both English and Hindi. Our experiments encompassing seven different LLMs of varying parameter sizes and over 140 training attempts with varying English-Hindi training data ratios demonstrated that it is possible to significantly improve multilingual performance without compromising native performance. Further, our approach avoids resource-intensive techniques like vocabulary expansion or architectural modifications, thus keeping the model size small. Our results indicate that modest fine-tuning with culturally and locally informed data can bridge performance gaps without incurring significant computational overhead. We release our training code, datasets, and models under mit and apache licenses to aid further research towards under-represented and low-resource languages.</abstract>
      <url hash="1a243544">2025.ijcnlp-long.189</url>
      <bibkey>kadiyala-etal-2025-improving</bibkey>
    </paper>
    <paper id="190">
      <title><fixed-case>A</fixed-case>fri<fixed-case>S</fixed-case>peech-<fixed-case>M</fixed-case>ulti<fixed-case>B</fixed-case>ench: A Verticalized Multidomain Multicountry Benchmark Suite for <fixed-case>A</fixed-case>frican Accented <fixed-case>E</fixed-case>nglish <fixed-case>ASR</fixed-case></title>
      <author><first>Gabrial Zencha</first><last>Ashungafac</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Mardhiyah</first><last>Sanni</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Busayo</first><last>Awobade</last><affiliation>Federal University of Agriculture Abeokuta</affiliation></author>
      <author id="alex-gichamba" orcid="0000-0002-2226-9882"><first>Alex</first><last>Gichamba</last><affiliation>Intron Inc.</affiliation></author>
      <author id="tobi-olatunji" orcid="0009-0009-7214-2132"><first>Tobi</first><last>Olatunji</last></author>
      <pages>3642-3653</pages>
      <abstract>Recent advances in speech‐enabled AI, including Google’s NotebookLM and OpenAI’s speech-to-speech API, are driving widespread interest in voice interfaces across sectors such as finance, health, agritech, legal services, and call‐centers in the global north and south. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa’s linguistic diversity. We present <tex-math>\textbf{AfriSpeech‑MultiBench}</tex-math>, the first domain‐specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities, and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversations drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open‐source ASR excels in spontaneous speech contexts but degrades on noisy, non‐native dialogue; multimodal LLMs are more accent‐robust yet struggle with domain‐specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Smaller models fine‐tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment. By releasing this benchmark, we empower practitioners and researchers to select voice technologies suited to African use‐cases, fostering inclusive voice applications for undeserved communities.</abstract>
      <url hash="3613244b">2025.ijcnlp-long.190</url>
      <bibkey>ashungafac-etal-2025-afrispeech</bibkey>
    </paper>
    <paper id="191">
      <title>An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation</title>
      <author><first>Vimaleswar</first><last>A</last></author>
      <author><first>Prabhu Nandan</first><last>Sahu</last></author>
      <author id="nilesh-kumar-sahu" orcid="0000-0003-1675-7270"><first>Nilesh Kumar</first><last>Sahu</last></author>
      <author><first>Haroon R</first><last>Lone</last><affiliation>IISER Bhopal</affiliation></author>
      <pages>3654-3673</pages>
      <abstract>Mental health plays a crucial role in the overall well-being of an individual. In recent years, digital platforms have increasingly been used to expand mental health and emotional support. However, there are persistent challenges related to limited user accessibility, internet connectivity, and data privacy, which highlight the need for an offline, smartphone-based solutions. To address these challenges, we propose **EmoSApp (Emotional Support App)**: an entirely offline, smartphone-based conversational app designed to provide mental health and emotional support. EmoSApp leverages a language model, specifically the LLaMA-3.2-1B-Instruct, which is fine-tuned and quantized on a custom-curated “Knowledge Dataset” comprising 14,582 mental health QA pairs along with multi-turn conversational data, enabling robust domain expertise and fully on-device inference on resource-constrained smartphones.Through qualitative evaluation with students and mental health professionals, we demonstrate that EmoSApp has the ability to respond coherently and empathetically, provide relevant suggestions to user’s mental health problems, and maintain interactive dialogue. Additionally, quantitative evaluations on nine commonsense and reasoning benchmarks, along with two mental health specific datasets, demonstrate EmoSApp’s effectiveness in low-resource settings. By prioritizing on-device deployment and specialized domain-specific adaptation, EmoSApp serves as a blueprint for future innovations in portable, secure, and highly tailored AI-driven mental health support.</abstract>
      <url hash="6af7b694">2025.ijcnlp-long.191</url>
      <bibkey>a-etal-2025-offline</bibkey>
    </paper>
    <paper id="192">
      <title>Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective</title>
      <author id="tejas-anvekar" orcid="0000-0002-7417-8157"><first>Tejas</first><last>Anvekar</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Krishna Singh</first><last>Rajput</last></author>
      <author id="chitta-baral" orcid="0000-0002-7549-723X"><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>3674-3686</pages>
      <abstract>Recent advances in multimodal question answering have primarily focused on combining heterogeneous modalities or fine-tuning multimodal large language models. While these approaches have shown strong performance, they often rely on a single, generalized reasoning strategy, overlooking the unique characteristics of each modality ultimately limiting both accuracy and interpretability. To address these limitations, we propose MAMMQA, a multi-agent QA framework for multimodal inputs spanning text, tables, and images. Our system includes two Visual Language Model (VLM) agents and one text-based Large Language Model (LLM) agent. The first VLM decomposes the user query into sub-questions and sequentially retrieves partial answers from each modality. The second VLM synthesizes and refines these results through cross-modal reasoning. Finally, the LLM integrates the insights into a cohesive answer. This modular design enhances interpretability by making the reasoning process transparent and allows each agent to operate within its domain of expertise. Experiments on diverse multimodal QA benchmarks demonstrate that our cooperative, multi-agent framework consistently outperforms existing baselines in both accuracy and robustness.</abstract>
      <url hash="e6c5137a">2025.ijcnlp-long.192</url>
      <bibkey>anvekar-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="193">
      <title><fixed-case>S</fixed-case>urvey<fixed-case>G</fixed-case>en-<fixed-case>I</fixed-case>: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing</title>
      <author><first>Jing</first><last>Chen</last></author>
      <author><first>Zhiheng</first><last>Yang</last></author>
      <author><first>Yixian</first><last>Shen</last></author>
      <author><first>Jie</first><last>Liu</last></author>
      <author id="adam-belloum" orcid="0000-0001-6306-6937"><first>Adam</first><last>Belloum</last></author>
      <author><first>Paola</first><last>Grosso</last><affiliation>University of Amsterdam</affiliation></author>
      <author id="chrysa-papagianni" orcid="0000-0002-2665-6492"><first>Chrysa</first><last>Papagianni</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>3687-3714</pages>
      <abstract>Survey papers play a critical role in scientific communication by consolidating progress across a field. Recent advances in Large Language Models (LLMs) offer a promising solution by automating key steps in the survey-generation pipeline, such as retrieval, structuring, and summarization. However, existing LLM-based approaches often struggle with maintaining coherence across long, multi-section surveys and providing comprehensive citation coverage. To address these limitations, we introduce SurveyGen-I, an automatic survey generation framework that combines coarse-to-fine retrieval, adaptive planning, and memory-guided generation. SurveyGen-I performs survey-level retrieval to construct the initial outline and writing plan, then dynamically refines both during generation through a memory mechanism that stores previously written content and terminology, ensuring coherence across subsections. When the system detects insufficient context, it triggers fine-grained subsection-level retrieval. During generation, SurveyGen-I leverages this memory mechanism to maintain coherence across subsections. Experiments across six scientific domains demonstrate that SurveyGen-I consistently outperforms previous works in content quality, consistency, and citation coverage. The code is available at https://github.com/SurveyGens/SurveyGen-I.</abstract>
      <url hash="e723f87e">2025.ijcnlp-long.193</url>
      <bibkey>chen-etal-2025-surveygen</bibkey>
    </paper>
    <paper id="194">
      <title><fixed-case>VAGUE</fixed-case>‐<fixed-case>G</fixed-case>ate: <fixed-case>P</fixed-case>lug‐and‐<fixed-case>P</fixed-case>lay <fixed-case>L</fixed-case>ocal‐<fixed-case>P</fixed-case>rivacy Shield for <fixed-case>R</fixed-case>etrieval‐<fixed-case>A</fixed-case>ugmented Generation</title>
      <author><first>Arshia</first><last>Hemmat</last></author>
      <author id="matin-moqadas" orcid="0009-0006-7410-148X"><first>Matin</first><last>Moqadas</last><affiliation>Isfahan University, University of Tehran</affiliation></author>
      <author><first>Ali</first><last>Mamanpoosh</last></author>
      <author><first>Amirmasoud</first><last>Rismanchian</last></author>
      <author><first>Afsaneh</first><last>Fatemi</last></author>
      <pages>3715-3730</pages>
      <abstract>Retrieval-augmented generation (RAG) still *forwards* raw passages to large-language models, so private facts slip through. Prior defenses are either (i) **heavyweight**—full DP training that is impractical for today’s 70B-parameter models—or (ii) **over-zealous**—blanket redaction of every named entity, which slashes answer quality.We introduce **VAGUE-Gate**, a lightweight, *locally* differentially-private gate deployable in front of *any* RAG system. A precision pass drops low-utility tokens under a user budget ε, then up to k(ε) high-temperature paraphrase passes further cloud residual cues; post-processing guarantees preserve the same ε-LDP bound.To measure both privacy and utility, we release **BlendPriv** (3k blended-sensitivity QA pairs) and two new metrics: a lexical Information-Leakage Score and an LLM-as-Judge score. Across eight pipelines and four SOTA LLMs, **VAGUE-Gate** at ε = 0.3 lowers lexical leakage by **70%** and semantic leakage by **1.8** points (1–5 scale) while retaining **91%** of Plain-RAG faithfulness with only a **240 ms** latency overhead.All code, data, and prompts are publicly released:- Code: &lt; https://github.com/arshiahemmat/LDP_RAG &gt; - Dataset: &lt;https://huggingface.co/datasets/AliMnp/BlendPriv&gt;</abstract>
      <url hash="457e2a1f">2025.ijcnlp-long.194</url>
      <bibkey>hemmat-etal-2025-vague</bibkey>
    </paper>
    <paper id="195">
      <title><fixed-case>PII</fixed-case>-Scope: A Comprehensive Study on Training Data Privacy Leakage in Pretrained <fixed-case>LLM</fixed-case>s</title>
      <author><first>Krishna Kanth</first><last>Nakka</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ahmed</first><last>Frikha</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ricardo</first><last>Mendes</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author id="xue-jiang" orcid="0000-0002-4959-9910"><first>Xue</first><last>Jiang</last></author>
      <author><first>Xuebing</first><last>Zhou</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>3731-3765</pages>
      <abstract>In this work, we introduce PII-Scope, a comprehensive benchmark designed to evaluate state-of-the-art methodologies for PII extraction attacks targeting LLMs across diverse threat settings. Our study provides a deeper understanding of these attacks by uncovering several hyperparameters (e.g., demonstration selection) crucial to their effectiveness. Building on this understanding, we extend our study to more realistic attack scenarios, exploring PII attacks that employ advanced adversarial strategies, including repeated and diverse querying, and leveraging iterative learning for continual PII extraction. Through extensive experimentation, our results reveal a notable underestimation of PII leakage in existing single-query attacks. In fact, we show that with sophisticated adversarial capabilities and a limited query budget, PII extraction rates can increase by up to fivefold when targeting the pretrained model. Moreover, we evaluate PII leakage on finetuned models, showing that they are more vulnerable to leakage than pretrained models. Overall, our work establishes a rigorous empirical benchmark for PII extraction attacks in realistic threat scenarios and provides a strong foundation for developing effective mitigation strategies.</abstract>
      <url hash="e7b17b90">2025.ijcnlp-long.195</url>
      <bibkey>nakka-etal-2025-pii</bibkey>
    </paper>
    <paper id="196">
      <title><fixed-case>I</fixed-case>ndic-<fixed-case>S</fixed-case>2<fixed-case>ST</fixed-case>: a Multilingual and Multimodal Many-to-Many <fixed-case>I</fixed-case>ndic Speech-to-Speech Translation Dataset</title>
      <author id="nivedita-sethiya" orcid="0000-0002-8245-4924"><first>Nivedita</first><last>Sethiya</last></author>
      <author><first>Puneet</first><last>Walia</last><affiliation>Indian Institute of Technology, Indore and Guru Nanak Dev Engineering College, Ludhiana</affiliation></author>
      <author id="chandresh-kumar-maurya" orcid="0000-0003-3519-600X"><first>Chandresh Kumar</first><last>Maurya</last><affiliation>Indian Institute of Technology, Indore</affiliation></author>
      <pages>3766-3775</pages>
      <abstract>Speech-to-Speech Translation (S2ST) converts speech from one language to speech in a different language. While various S2ST models exist, none adequately support Indic languages, primarily due to the lack of a suitable dataset. We fill this gap by introducing Indic-S2ST, a multilingual and multimodal many-to-many S2ST data of approximately 600 hours in 14 Indic languages, including Indian-accented English. To the best of our knowledge, this is the largest data for the S2ST task with parallel speech and text in 14 scheduled Indic languages. Our data also supports Automatic Speech Recognition (ASR), Text-to-Speech (TTS) synthesis, Speech-to-Text translation (ST), and Machine Translation (MT) due to parallel speech and text alignment. Thus, our data may be useful to train a model likeMeta’s SeamlessM4T for Indic languages. We also propose Indic-S2UT, a discrete unit-based S2ST model for Indic languages. To showcase the utility of the data, we present baseline results on the Indic-S2ST data using the Indic-S2UT. The dataset and codes are available at https://github.com/Nivedita5/Indic-S2ST/blob/main/README.md.</abstract>
      <url hash="15449c7c">2025.ijcnlp-long.196</url>
      <bibkey>sethiya-etal-2025-indic</bibkey>
    </paper>
  </volume>
  <volume id="short" ingest-date="2026-01-12" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics</booktitle>
      <editor><first>Kentaro</first><last>Inui</last></editor>
      <editor><first>Sakriani</first><last>Sakti</last></editor>
      <editor><first>Haofen</first><last>Wang</last></editor>
      <editor><first>Derek F.</first><last>Wong</last></editor>
      <editor id="pushpak-bhattacharyya"><first>Pushpak</first><last>Bhattacharyya</last></editor>
      <editor><first>Biplab</first><last>Banerjee</last></editor>
      <editor><first>Asif</first><last>Ekbal</last></editor>
      <editor><first>Tanmoy</first><last>Chakraborty</last></editor>
      <editor><first>Dhirendra Pratap</first><last>Singh</last></editor>
      <publisher>The Asian Federation of Natural Language Processing and The Association for Computational Linguistics</publisher>
      <address>Mumbai, India</address>
      <month>December</month>
      <year>2025</year>
      <url hash="9e466995">2025.ijcnlp-short</url>
      <venue>ijcnlp</venue>
      <venue>aacl</venue>
      <isbn>979-8-89176-299-2</isbn>
    </meta>
    <frontmatter>
      <url hash="9c206fbe">2025.ijcnlp-short.0</url>
      <bibkey>ijcnlp-2025-short</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Is <fixed-case>O</fixed-case>pen<fixed-case>VLA</fixed-case> Truly Robust? A Systematic Evaluation of Positional Robustness</title>
      <author><first>Yiran</first><last>Pang</last><affiliation>Florida Atlantic University</affiliation></author>
      <author id="yiheng-zhao" orcid="0000-0002-5518-026X"><first>Yiheng</first><last>Zhao</last></author>
      <author><first>Zhuopu</first><last>Zhou</last><affiliation>Florida Atlantic University</affiliation></author>
      <author><first>Tingkai</first><last>Hu</last></author>
      <author><first>Ranxin</first><last>Hou</last></author>
      <pages>1-6</pages>
      <abstract>Pretrained language and vision-language models have become core components in building vision-language-action models (VLAs) due to their strong spatial reasoning capabilities. Evaluating the robustness of VLAs is crucial to ensuring their reliability in practical scenarios. Although prior work has focused on background and environment robustness, positional robustness remains underexplored. In this paper, we propose a comprehensive evaluation protocol to assess the positional robustness of VLAs and apply it to OpenVLA, an open-source, high-performing, and efficient model well suited for real-world deployment. We find that OpenVLA succeeds only when the target object is placed at one of the two positions encountered during training. Even in these cases, the success rate never exceeds 50% because it exhibits a memorized behavior that it randomly executes a grasping action toward one of the two fixed positions without relying on perception to localize the target object. This reveals that OpenVLA’s positional robustness is extremely weak.</abstract>
      <url hash="a5bed41f">2025.ijcnlp-short.1</url>
      <bibkey>pang-etal-2025-openvla</bibkey>
    </paper>
    <paper id="2">
      <title>Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing</title>
      <author><first>Jiabao</first><last>Ji</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Bairu</first><last>Hou</last></author>
      <author><first>Alexander</first><last>Robey</last></author>
      <author id="george-j-pappas" orcid="0000-0001-9081-0637"><first>George J.</first><last>Pappas</last><affiliation>School of Engineering and Applied Science, University of Pennsylvania</affiliation></author>
      <author><first>Hamed</first><last>Hassani</last><affiliation>University of Pennsylvania, University of Pennsylvania and University of Pennsylvania</affiliation></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Eric</first><last>Wong</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Shiyu</first><last>Chang</last></author>
      <pages>7-40</pages>
      <abstract>Aligned large language models (LLMs) are vulnerable to jailbreaks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based attacks, there are no defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SemanticSmooth, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SemanticSmooth achieves strong robustness against both manually constructed jailbreak prompts and automatic jailbreak attacks like GCG, PAIR, and PromptRS while maintaining strong nominal performance on standard LLM evaluation benchmarks such as AlpacaEval for the instruction-following tasks and PiQA for the question-answering tasks.</abstract>
      <url hash="6a4c492b">2025.ijcnlp-short.2</url>
      <bibkey>ji-etal-2025-defending</bibkey>
    </paper>
    <paper id="3">
      <title>Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning Large Language Models</title>
      <author><first>Nghia Tuan</first><last>Bui</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <author id="guergana-k-savova" orcid="0000-0002-5887-200X"><first>Guergana K</first><last>Savova</last><affiliation>Harvard University</affiliation></author>
      <author id="lijing-wang" orcid="0000-0002-0836-9190"><first>Lijing</first><last>Wang</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <pages>41-46</pages>
      <abstract>The impact of random seeds in fine-tuning large language models (LLMs) has been largely overlooked despite its potential influence on model performance. In this study, we systematically evaluate the effects of random seeds on LLMs using the GLUE and SuperGLUE benchmarks. We analyze the macro impact through traditional metrics like accuracy and F1, calculating their mean and variance to quantify performance fluctuations. To capture the micro effects, we introduce a novel metric, consistency, measuring the stability of individual predictions across runs. Our experiments reveal significant variance at both macro and micro levels, underscoring the need for careful consideration of random seeds in fine-tuning and evaluation.</abstract>
      <url hash="b20289ee">2025.ijcnlp-short.3</url>
      <bibkey>bui-etal-2025-assessing</bibkey>
    </paper>
    <paper id="4">
      <title>How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?</title>
      <author><first>Christian</first><last>Clark</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Byung-Doh</first><last>Oh</last><affiliation>New York University</affiliation></author>
      <author><first>William</first><last>Schuler</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>47-57</pages>
      <abstract>Contextual entropy is a psycholinguistic measure capturing the anticipated difficulty of processing a word just before it is encountered. Recent studies have tested for entropy-related effects as a potential complement to well-known effects from surprisal. For convenience, entropy is typically estimated based on a language model’s probability distribution over a word’s first subword token. However, this approximation results in underestimation and potential distortion of true word entropy. To address this, we generate Monte Carlo (MC) estimates of word entropy that allow words to span a variable number of tokens. Regression experiments on reading times show divergent results between first-token and MC word entropy, suggesting a need for caution in using first-token approximations of contextual entropy.</abstract>
      <url hash="073fa644">2025.ijcnlp-short.4</url>
      <bibkey>clark-etal-2025-well</bibkey>
    </paper>
    <paper id="5">
      <title>Speaking the Right Language: The Impact of Expertise (Mis)Alignment in User-<fixed-case>AI</fixed-case> Interactions</title>
      <author id="shramay-palta" orcid="0000-0002-8765-9734"><first>Shramay</first><last>Palta</last></author>
      <author><first>Nirupama</first><last>Chandrasekaran</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <author><first>Scott</first><last>Counts</last><affiliation>Research, Microsoft</affiliation></author>
      <pages>58-69</pages>
      <abstract>Using a sample of 25,000 Bing Copilot conversations, we study how the agent responds to users of varying levels of domain expertise and the resulting impact on user experience along multiple dimensions. Our findings show that across a variety of topical domains, the agent largely responds at proficient or expert levels of expertise (77% of conversations) which correlates with positive user experience regardless of the user’s level of expertise. Misalignment, such that the agent responds at a level of expertise below that of the user, has a negative impact on overall user experience, with the impact more profound for more complex tasks. We also show that users engage more, as measured by the number of words in the conversation, when the agent responds at a level of expertise commensurate with that of the user. Our findings underscore the importance of alignment between users and AI when designing human-centered AI systems, to ensure satisfactory and productive interactions.</abstract>
      <url hash="bc8f615e">2025.ijcnlp-short.5</url>
      <bibkey>palta-etal-2025-speaking</bibkey>
    </paper>
    <paper id="6">
      <title>To Labor is Not to Suffer: Exploration of Polarity Association Bias in <fixed-case>LLM</fixed-case>s for Sentiment Analysis</title>
      <author id="jiyu-chen" orcid="0000-0002-8057-3022"><first>Jiyu</first><last>Chen</last><affiliation>CSIRO</affiliation></author>
      <author id="sarvnaz-karimi" orcid="0000-0002-4927-3937"><first>Sarvnaz</first><last>Karimi</last><affiliation>CSIRO</affiliation></author>
      <author id="diego-molla" orcid="0000-0003-4973-0963"><first>Diego</first><last>Molla</last><affiliation>Macquarie University</affiliation></author>
      <author id="cecile-paris" orcid="0000-0003-3816-0176"><first>Cecile</first><last>Paris</last><affiliation>CSIRO</affiliation></author>
      <pages>70-78</pages>
      <abstract>Large language models (LLMs) are widely used for modeling sentiment trends on social media text. We examine whether LLMs have a polarity association bias—positive or negative—when encountering specific types of lexical word mentions. Such polarity association bias could lead to the wrong classification of neutral statements and thus a distorted estimation of sentiment trends. We estimate the severity of the polarity association bias across five widely used LLMs, identifying lexical word mentions spanning a diverse range of linguistic and psychological categories that correlate with this bias. Our results show a moderate to strong degree of polarity association bias in these LLMs.</abstract>
      <url hash="a5d1a1fd">2025.ijcnlp-short.6</url>
      <bibkey>chen-etal-2025-labor</bibkey>
    </paper>
    <paper id="7">
      <title>Seeing isn’t Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms</title>
      <author id="tyler-loakman" orcid="0000-0001-5333-7780"><first>Tyler</first><last>Loakman</last></author>
      <author><first>Joseph</first><last>James</last></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <pages>79-86</pages>
      <abstract>With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in different tasks that fuse both vision and language modalities. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the difficulty of this task stemming from the requirement for esoteric parametric knowledge of how to interpret such figures, rather than paired samples alone.</abstract>
      <url hash="6ce55a93">2025.ijcnlp-short.7</url>
      <bibkey>loakman-etal-2025-seeing</bibkey>
    </paper>
    <paper id="8">
      <title>A Formal Analysis of Chain-of-Thought Prompting via <fixed-case>T</fixed-case>uring Reductions</title>
      <author id="s-m-rafiuddin" orcid="0000-0001-9404-1556"><first>S M</first><last>Rafiuddin</last></author>
      <author><first>Muntaha Nujat</first><last>Khan</last></author>
      <pages>87-98</pages>
      <abstract>Chain-of-Thought (CoT) prompting has emerged as a powerful empirical technique for eliciting multi-step reasoning from large language models by decomposing complex tasks into sequential subprompts. However, the formal computational trade-offs between internal computation, query count, and space usage remain unexplored. We introduce the CoT-oracle Turing machine, a formal model in which each subprompt corresponds to an oracle query, and define three resource metrics: internal time T(n), query complexity Q(n), and prompt buffer space Sprompt(n). We prove that (T,Q)-bounded CoT machines exactly capture the class PO[Q(n)] of polynomial-time Turing reductions with Q(n) queries, derive upper bounds for P and NP-complete problems under linear and prefix-query budgets, and establish an Ω(n) query lower bound for SAT under P ≠ NP. Illustrative examples on integer factorization and SAT reconstruction, together with synthetic and LLM-based simulations, confirm our theoretical T–Q–S trade-off predictions. This framework provides principled guidelines for prompt design, noisy-oracle robustness, and cost-aware reasoning.</abstract>
      <url hash="8a738f2b">2025.ijcnlp-short.8</url>
      <bibkey>rafiuddin-khan-2025-formal</bibkey>
    </paper>
    <paper id="9">
      <title>Speak &amp; Spell: <fixed-case>LLM</fixed-case>-Driven Controllable Phonetic Error Augmentation for Robust Dialogue State Tracking</title>
      <author><first>Jihyun</first><last>Lee</last></author>
      <author><first>Solee</first><last>Im</last></author>
      <author><first>Wonjun</first><last>Lee</last></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>99-111</pages>
      <abstract>Dialogue State Tracking (DST) is a key part of task-oriented dialogue systems, identifying important information in conversations. However, its accuracy drops significantly in spoken dialogue environments due to named entity errors from Automatic Speech Recognition (ASR) systems. We introduce a simple yet effective data augmentation method that targets those entities to improve the robustness of DST model. Our novel method can control the placement of errors using keyword-highlighted prompts while introducing phonetically similar errors. As a result, our method generated sufficient error patterns on keywords, leading to improved accuracy in noised and low-accuracy ASR environments.</abstract>
      <url hash="4be5c61e">2025.ijcnlp-short.9</url>
      <bibkey>lee-etal-2025-speak</bibkey>
    </paper>
    <paper id="10">
      <title>Are Relational Triple Extraction Frameworks Sufficient for Hyper-relational Facts ?</title>
      <author><first>Pratik</first><last>Saini</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author id="chayan-sarkar" orcid="0000-0003-4777-2086"><first>Chayan</first><last>Sarkar</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author id="tapas-nayak"><first>Tapas</first><last>Nayak</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <pages>112-119</pages>
      <abstract>Hyper-relational fact extraction involves identifying relational triples along with additional contextual information—known as qualifiers—such as time, location, or quantity. These qualifiers enable models to represent complex real-world knowledge more accurately. While numerous end-to-end models have been developed for extracting relational triples, they are not designed to handle qualifiers directly. In this work, we propose a straightforward and effective approach to extend existing end-to-end triple extraction models to also capture qualifiers. Our method reformulates qualifiers as new relations by computing the Cartesian product between qualifiers and their associated relations. This transformation allows the model to extract qualifier information as additional triples, which can later be merged to form complete hyper-relational facts. We evaluate our approach using multiple end-to-end triple extraction models on the HyperRED dataset and demonstrate its effectiveness in extracting hyper-relational facts.</abstract>
      <url hash="a77cb0ae">2025.ijcnlp-short.10</url>
      <bibkey>saini-etal-2025-relational</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>IFE</fixed-case>val-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models</title>
      <author id="yiming-gao" orcid="0009-0003-6090-9929"><first>Yiming</first><last>Gao</last></author>
      <author id="bin-wang" orcid="0000-0001-9760-8343"><first>Bin</first><last>Wang</last></author>
      <author><first>Chengwei</first><last>Wei</last><affiliation>A*STAR</affiliation></author>
      <author><first>Shuo</first><last>Sun</last><affiliation>, A*STAR</affiliation></author>
      <author id="aiti-aw"><first>AiTi</first><last>Aw</last><affiliation>I2R</affiliation></author>
      <pages>120-127</pages>
      <abstract>Large language models (LLMs) have demonstrated strong instruction-following capabilities in text-based tasks. However, this ability often deteriorates in multimodal models after alignment with non-text modalities such as images or audio. While several recent efforts have investigated instruction-following performance in text and vision-language models, instruction-following in audio-based large language models remains largely unexplored. To bridge this gap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess the ability to follow instructions in an audio LLM. IFEval-Audio contains 280 audio–instruction–answer triples across six diverse dimensions: Content, Capitalization, Symbol, List Structure, Length, and Format. Each example pairs an audio input with a text instruction, requiring the model to generate an output that follows a specified structure. We benchmark state-of-the-art audio LLMs on their ability to follow audio-involved instructions. The dataset is released publicly to support future research in this emerging area.</abstract>
      <url hash="d37108f6">2025.ijcnlp-short.11</url>
      <bibkey>gao-etal-2025-ifeval</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>I</fixed-case>ndo<fixed-case>P</fixed-case>ref: A Multi-Domain Pairwise Preference Dataset for <fixed-case>I</fixed-case>ndonesian</title>
      <author><first>Vanessa Rebecca</first><last>Wiyono</last></author>
      <author><first>David</first><last>Anugraha</last><affiliation>Stanford University</affiliation></author>
      <author id="ayu-purwarianti" orcid="0000-0002-5016-3700"><first>Ayu</first><last>Purwarianti</last><affiliation>Institut Teknologi Bandung</affiliation></author>
      <author id="genta-indra-winata"><first>Genta Indra</first><last>Winata</last><affiliation>Capital One</affiliation></author>
      <pages>128-138</pages>
      <abstract>Over 200 million people speak Indonesian, yet the language remains significantly underrepresented in preference-based research for large language models (LLMs). Most existing multilingual datasets are derived from English translations, often resulting in content that lacks cultural and linguistic authenticity. To address this gap, we introduce IndoPref, the first fully human-authored and multi-domain Indonesian preference dataset designed to evaluate the naturalness and quality of LLM-generated text. The dataset contains 522 prompts and yields 4,099 human-annotated pairwise preferences from comparisons across five instruction-tuned LLMs. All annotations are natively written in Indonesian with strong inter-annotator agreement, measured by Krippendorff’s alpha. Our benchmark spans 10 diverse categories, enabling practitioners to identify LLMs’ fine-grained strengths and weaknesses.</abstract>
      <url hash="ac43e510">2025.ijcnlp-short.12</url>
      <bibkey>wiyono-etal-2025-indopref</bibkey>
    </paper>
    <paper id="13">
      <title>Large Language Models Exhibit Limited Reasoning Ability on Coding Problems</title>
      <author><first>Jinyoung</first><last>Jo</last><affiliation>Stanford University</affiliation></author>
      <author><first>Jonah</first><last>Engelmann</last></author>
      <author id="sean-choi" orcid="0000-0002-9823-8388"><first>Sean</first><last>Choi</last><affiliation>Santa Clara University</affiliation></author>
      <pages>139-146</pages>
      <abstract>Claims that large language models (LLMs) have complex reasoning ability have stirred broad interests, and controversies, of academics and non-academics alike. A popular basis for such claims comes from LLMs’ ability to solve coding problems, which involves understanding the problem statement and providing code that solves the problem. Although such abilities are remarkable feats worth praising, we argue that they come from memorization rather than reasoning. We first show that LLMs’ problem-solving ability degrades with increased recency of the problem, likely due to the reduced amount of training data for more recent problems, regardless of the problem difficulty labeled by human experts. Additionally, we show that an LLM often fails to solve the problem when presented with reworded but equivalent problem statements, further suggesting their limited reasoning ability.</abstract>
      <url hash="c06a4946">2025.ijcnlp-short.13</url>
      <bibkey>jo-etal-2025-large</bibkey>
    </paper>
    <paper id="14">
      <title>Documentation Retrieval Improves Planning Language Generation</title>
      <author><first>Renxiang</first><last>Wang</last></author>
      <author id="li-zhang-aws"><first>Li</first><last>Zhang</last><affiliation>Drexel University</affiliation></author>
      <pages>147-158</pages>
      <abstract>Certain strong LLMs have shown promise for zero-shot formal planning by generating planning languages like PDDL. Yet, performance of most open-source models under 50B parameters has been reported to be close to zero due to the low-resource nature of these languages. We significantly improve their performance via a series of lightweight pipelines that integrates documentation retrieval with modular code generation and error refinement. With models like Llama-4-Maverick, our best pipeline improves plan correctness from 0% to over 80% on the common BlocksWorld domain. However, while syntactic errors are substantially reduced, semantic errors persist in more challenging domains, revealing fundamental limitations in current models’ reasoning capabilities.</abstract>
      <url hash="d3fb42c4">2025.ijcnlp-short.14</url>
      <bibkey>wang-zhang-2025-documentation</bibkey>
    </paper>
    <paper id="15">
      <title>Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?</title>
      <author><first>Gaurav</first><last>Kamath</last><affiliation>McGill University</affiliation></author>
      <author id="sowmya-vajjala" orcid="0000-0002-4033-9936"><first>Sowmya</first><last>Vajjala</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>159-167</pages>
      <abstract>We explore whether synthetic datasets generated by large language models using a few high quality seed samples are useful for low-resource named entity recognition, considering 11 languages from three language families. Our results suggest that synthetic data created with such seed data is a reasonable choice when there is no available labeled data, and is better than using entirely automatically labeled data. However, a small amount of high-quality data, coupled with cross-lingual transfer from a related language, always offers better performance. Data and code available at: https://github.com/grvkamath/low-resource-syn-ner.</abstract>
      <url hash="d05856eb">2025.ijcnlp-short.15</url>
      <bibkey>kamath-vajjala-2025-synthetic</bibkey>
    </paper>
    <paper id="16">
      <title>From Facts to Folklore: Evaluating Large Language Models on <fixed-case>B</fixed-case>engali Cultural Knowledge</title>
      <author><first>Nafis</first><last>Chowdhury</last><affiliation>BRAC University</affiliation></author>
      <author><first>Moinul</first><last>Haque</last><affiliation>BRAC University and BRAC University</affiliation></author>
      <author><first>Anika</first><last>Ahmed</last><affiliation>BRAC University</affiliation></author>
      <author><first>Nazia</first><last>Tasnim</last><affiliation>Boston University, Boston University</affiliation></author>
      <author id="md-istiak-hossain-shihab" orcid="0009-0001-9362-2268"><first>Md. Istiak Hossain</first><last>Shihab</last><affiliation>Oregon State University</affiliation></author>
      <author id="sajjadur-rahman" orcid="0000-0003-4210-1582"><first>Sajjadur</first><last>Rahman</last><affiliation>Adobe Systems</affiliation></author>
      <author id="farig-sadeque" orcid="0000-0001-6797-7826"><first>Farig</first><last>Sadeque</last><affiliation>BRAC University</affiliation></author>
      <pages>168-177</pages>
      <abstract>Recent progress in NLP research has demonstrated remarkable capabilities of large language models (LLMs) across a wide range of tasks. While recent multilingual benchmarks have advanced cultural evaluation for LLMs, critical gaps remain in capturing the nuances of low-resource cultures. Our work addresses these limitations through a Bengali Language Cultural Knowledge (BLanCK) dataset including folk traditions, culinary arts, and regional dialects. Our investigation of several multilingual language models shows that while these models perform well in non-cultural categories, they struggle significantly with cultural knowledge and performance improves substantially across all models when context is provided, emphasizing context-aware architectures and culturally curated training data.</abstract>
      <url hash="e459f481">2025.ijcnlp-short.16</url>
      <bibkey>chowdhury-etal-2025-facts</bibkey>
    </paper>
    <paper id="17">
      <title>Are <fixed-case>ASR</fixed-case> foundation models generalized enough to capture features of regional dialects for low-resource languages?</title>
      <author><first>Tawsif Tashwar</first><last>Dipto</last></author>
      <author><first/><last>Azmol Hossain</last></author>
      <author><first>Rubayet Sabbir</first><last>Faruque</last></author>
      <author id="md-rezuwan-hassan" orcid="0009-0007-2731-5183"><first>Md. Rezuwan</first><last>Hassan</last></author>
      <author><first>Kanij</first><last>Fatema</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>Tanmoy</first><last>Shome</last><affiliation>Bengali.Ai</affiliation></author>
      <author><first>Ruwad</first><last>Naswan</last></author>
      <author><first>Md.Foriduzzaman</first><last>Zihad</last><affiliation>Bengali.Ai</affiliation></author>
      <author><first>Mohaymen Ul</first><last>Anam</last></author>
      <author><first>Nazia</first><last>Tasnim</last><affiliation>Boston University, Boston University</affiliation></author>
      <author id="hasan-mahmud" orcid="0000-0003-4375-6943"><first>Hasan</first><last>Mahmud</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Md Kamrul</first><last>Hasan</last></author>
      <author><first>Md. Mehedi Hasan</first><last>Shawon</last><affiliation>BRAC University</affiliation></author>
      <author id="farig-sadeque" orcid="0000-0001-6797-7826"><first>Farig</first><last>Sadeque</last><affiliation>BRAC University</affiliation></author>
      <author id="tahsin-reasat" orcid="0000-0001-9451-6814"><first>Tahsin</first><last>Reasat</last><affiliation>Bengali.AI</affiliation></author>
      <pages>178-188</pages>
      <abstract>Conventional research on speech recognition modeling relies on the canonical form for most low-resource languages while automatic speech recognition (ASR) for regional dialects is treated as a fine-tuning task. To investigate the effects of dialectal variations on ASR we develop a 78-hour annotated Bengali Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and data-driven perspectives shows that speech foundation models struggle heavily in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe that all deep learning methods struggle to model speech data under dialectal variations, but dialect specific model training alleviates the issue. Our dataset also serves as a out-of-distribution (OOD) resource for ASR modeling under constrained resources in ASR algorithms. The dataset and code developed for this project are publicly available.</abstract>
      <url hash="01852885">2025.ijcnlp-short.17</url>
      <bibkey>dipto-etal-2025-asr</bibkey>
    </paper>
    <paper id="18">
      <title>Gatsby without the ‘<fixed-case>E</fixed-case>’: Creating Lipograms with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Nitish</first><last>Gokulakrishnan</last></author>
      <author><first>Rohan</first><last>Balasubramanian</last></author>
      <author><first>Syeda Jannatus</first><last>Saba</last></author>
      <author id="steven-skiena"><first>Steven</first><last>Skiena</last><affiliation>State University of New York - Stony Brook, Stony Brook University, SUNY at Stony Brook, , State University of New York at Stony Brook and State University of New York at Stony Brook</affiliation></author>
      <pages>189-199</pages>
      <abstract>Lipograms are a unique form of constrained writing where all occurrences of a particular letter are excluded from the text, typified by the novel Gadsby (Wright, 1939), which daringly avoids all usage of the letter ‘e’. In this study, we explore the power of modern large language models (LLMs) by transforming the novel The Great Gatsby (Fitzgerald, 1925) into a fully ‘e’-less text. We experimented with a range of techniques, from baseline methods like synonym replacement to sophisticated generative models enhanced with beam search and named entity analysis. We show that excluding up to 3.6% of the most common letters (up to the letter ‘u’) had minimal impact on the text’s meaning, although translation fidelity rapidly and predictably decays with stronger lipogram constraints. Our work highlights the surprising flexibility of English under strict constraints.</abstract>
      <url hash="42d5ece3">2025.ijcnlp-short.18</url>
      <bibkey>gokulakrishnan-etal-2025-gatsby</bibkey>
    </paper>
    <paper id="19">
      <title>Hint-Augmented Re-ranking: Efficient Product Search using <fixed-case>LLM</fixed-case>-Based Query Decomposition</title>
      <author><first>Yilun</first><last>Zhu</last><affiliation>Amazon</affiliation></author>
      <author><first>Nikhita</first><last>Vedula</last><affiliation>Amazon</affiliation></author>
      <author id="shervin-malmasi"><first>Shervin</first><last>Malmasi</last><affiliation>Amazon</affiliation></author>
      <pages>200-216</pages>
      <abstract>Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or <i>hints</i>. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.</abstract>
      <url hash="feb16e7c">2025.ijcnlp-short.19</url>
      <bibkey>zhu-etal-2025-hint</bibkey>
    </paper>
    <paper id="20">
      <title>p²-<fixed-case>TQA</fixed-case>: A Process-based Preference Learning Framework for Self-Improving Table Question Answering Models</title>
      <author id="wei-zhou" orcid="0009-0006-8617-8337"><first>Wei</first><last>Zhou</last></author>
      <author><first>Mohsen</first><last>Mesgar</last><affiliation>Bosch</affiliation></author>
      <author><first>Heike</first><last>Adel</last><affiliation>Hochschule der Medien (University of Applied Sciences)</affiliation></author>
      <author id="annemarie-friedrich" orcid="0000-0001-8771-7634"><first>Annemarie</first><last>Friedrich</last><affiliation>University of Augsburg</affiliation></author>
      <pages>217-231</pages>
      <abstract>Table question answering (TQA) focuses on answering questions based on tabular data. Developing TQA systems targets effective interaction with tabular data for tasks such as cell retrieval and data analysis. While recent work has leveraged fine-tuning to improve TQA systems, existing approaches often under-utilize available data and neglect the potential of post-training for further gains. In this work, we introduce p²-TQA, a process-based preference learning framework for TQA post-training. p²-TQA automatically constructs process-based preference data via a table-specific pipeline, eliminating the need for manual or costly data collection. It then optimizes models through contrastive learning on the collected data. Experiments show that p²-TQA effectively improves TQA models by up to 5% on in-domain datasets and 2.4% on out-of-domain datasets with only 8,000 training instances. Furthermore, models enhanced with p²-TQA achieve competitive results against larger, more complex state-of-the-art TQA systems, while maintaining up to five times higher efficiency.</abstract>
      <url hash="caa35ce7">2025.ijcnlp-short.20</url>
      <bibkey>zhou-etal-2025-p2</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>P</fixed-case>er<fixed-case>M</fixed-case>ed-<fixed-case>MM</fixed-case>: A Multimodal, Multi-Specialty <fixed-case>P</fixed-case>ersian Medical Benchmark for Evaluating Vision Language Models</title>
      <author><first>Ali</first><last>Khoramfar</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Mohammad Javad</first><last>Dousti</last><affiliation>University of Tehran</affiliation></author>
      <author><first>Heshaam</first><last>Faili</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <pages>232-241</pages>
      <abstract>We present PerMed-MM, the first multimodal benchmark for Persian medical question answering. The dataset comprises 733 expert-authored multiple-choice questions from Iranian National Medical Board Exams, each paired with one to five clinically relevant images, spanning 46 medical specialties and diverse visual modalities. We evaluate five open-source and five proprietary vision language models, and find that reasoning supervision and domain-specific fine-tuning yield performance gains. Our cross-lingual analysis reveals significant unpredictability in translation-based pipelines, motivating the need for benchmarks that support direct, native-language evaluation. Additionally, domain- and modality-level analysis uncovers meaningful variation in model behavior often masked by aggregate metrics. PerMed-MM is publicly available on Hugging Face.</abstract>
      <url hash="5e660037">2025.ijcnlp-short.21</url>
      <bibkey>khoramfar-etal-2025-permed</bibkey>
    </paper>
    <paper id="22">
      <title>An Analysis of Large Language Models for Simulating User Responses in Surveys</title>
      <author><first>Ziyun</first><last>Yu</last><affiliation>NYU Shanghai</affiliation></author>
      <author><first>Yiru</first><last>Zhou</last></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>NYU Shanghai</affiliation></author>
      <author><first>Hongyi</first><last>Wen</last><affiliation>New York University, Shanghai</affiliation></author>
      <pages>242-259</pages>
      <abstract>Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions and propose two LLM-based approaches: chain-of-thought (COT) prompting and Diverse Claims Generation (CLAIMSIM), which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.</abstract>
      <url hash="b15a39b4">2025.ijcnlp-short.22</url>
      <bibkey>yu-etal-2025-analysis</bibkey>
    </paper>
    <paper id="23">
      <title>Enhancing <fixed-case>BERT</fixed-case> Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages</title>
      <author><first>Jozef</first><last>Kubík</last></author>
      <author><first>Marek</first><last>Suppa</last><affiliation>Comenius University in Bratislava</affiliation></author>
      <author id="martin-takac-962x" orcid="0000-0002-3956-962X"><first>Martin</first><last>Takac</last></author>
      <pages>260-272</pages>
      <abstract>Limited data for low-resource languages typically yields weaker language models (LMs). Since pre-training is compute-intensive, it is more pragmatic to target improvements during fine-tuning. In this work, we examine the use of Active Learning (AL) methods augmented by structured data selection strategies across epochs, which we term ‘Active Learning schedulers,’ to boost the fine-tuning process with a limited amount of training data. We connect the AL process to data clustering and propose an integrated fine-tuning pipeline that systematically combines AL, data clustering, and dynamic data selection schedulers to enhance models’ performance. Several experiments on the Slovak, Maltese, Icelandic, and Turkish languages show that the use of clustering during the fine-tuning phase together with novel AL scheduling can for models simultaneously yield annotation savings up to 30% and performance improvements up to four F1 score points, while also providing better fine-tuning stability.</abstract>
      <url hash="3d046672">2025.ijcnlp-short.23</url>
      <bibkey>kubik-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="24">
      <title>What am <fixed-case>I</fixed-case> missing here?: Evaluating Large Language Models for Masked Sentence Prediction</title>
      <author><first>Charlie</first><last>Wyatt</last></author>
      <author id="aditya-joshi" orcid="0000-0003-2200-9703"><first>Aditya</first><last>Joshi</last><affiliation>University of New South Wales</affiliation></author>
      <author id="flora-d-salim" orcid="0000-0002-1237-1664"><first>Flora D.</first><last>Salim</last><affiliation>University of New South Wales</affiliation></author>
      <pages>273-283</pages>
      <abstract>Transformer-based models primarily rely on Next Token Prediction (NTP), which predicts the next token in a sequence based on the preceding context. However, NTP’s focus on single-token prediction often limits a model’s ability to plan ahead or maintain long-range coherence, raising questions about how well LLMs can predict longer contexts, such as full sentences within structured documents. While NTP encourages local fluency, it provides no explicit incentive to ensure global coherence across sentence boundaries—an essential skill for reconstructive or discursive tasks. To investigate this, we evaluate three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on Masked Sentence Prediction (MSP) — the task of infilling a randomly removed sentence — from three domains: ROCStories (narrative), Recipe1M (procedural), and Wikipedia (expository). We assess both fidelity (similarity to the original sentence) and cohesiveness (fit within the surrounding context). Our key finding reveals that commercial LLMs, despite their superlative performance in other tasks, are poor at predicting masked sentences in low-structured domains, highlighting a gap in current model capabilities.</abstract>
      <url hash="72c2c978">2025.ijcnlp-short.24</url>
      <bibkey>wyatt-etal-2025-missing</bibkey>
    </paper>
    <paper id="25">
      <title>A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models</title>
      <author id="sadia-kamal" orcid="0000-0003-0353-0894"><first>Sadia</first><last>Kamal</last></author>
      <author><first>Lalu Prasad Yadav</first><last>Prakash</last></author>
      <author id="s-m-rafiuddin" orcid="0000-0001-9404-1556"><first>S M</first><last>Rafiuddin</last></author>
      <author id="mohammed-rakib" orcid="0000-0001-6201-3729"><first>Mohammed</first><last>Rakib</last></author>
      <author><first>Atriya</first><last>Sen</last><affiliation>Oklahoma State University</affiliation></author>
      <author><first>Sagnik Ray</first><last>Choudhury</last><affiliation>University of North Texas</affiliation></author>
      <pages>284-303</pages>
      <abstract>The Political Compass Test (PCT) and similar surveys are commonly used to assess political bias in auto-regressive LLMs. Our rigorous statistical experiments show that while changes to standard generation parameters have minimal effect on PCT scores, prompt phrasing and fine-tuning individually and together can significantly influence results. Interestingly, fine-tuning on politically rich vs. neutral datasets does not lead to different shifts in scores. We also generalize these findings to a similar popular test called 8 Values. Humans do not change their responses to questions when prompted differently (“answer this question” vs “state your opinion”), or after exposure to politically neutral text, such as mathematical formulae. But the fact that the models do so raises concerns about the validity of these tests for measuring model bias, and paves the way for deeper exploration into how political and social views are encoded in LLMs.</abstract>
      <url hash="98fe3043">2025.ijcnlp-short.25</url>
      <bibkey>kamal-etal-2025-detailed</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>E</fixed-case>dit<fixed-case>GRPO</fixed-case>: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest <fixed-case>X</fixed-case>-Ray Report Generation</title>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Christopher</first><last>Malon</last><affiliation>NEC Laboratories America</affiliation></author>
      <author><first>Lichao</first><last>Sun</last><affiliation>Lehigh University</affiliation></author>
      <author id="martin-renqiang-min" orcid="0000-0002-8563-6133"><first>Martin Renqiang</first><last>Min</last><affiliation>NEC Laboratories America and NEC Laboratories America</affiliation></author>
      <pages>304-316</pages>
      <abstract>Radiology report generation requires advanced medical image analysis, effective temporal reasoning, and accurate text generation. Although recent innovations, particularly multimodal large language models, have shown improved performance, their supervised fine-tuning (SFT) objective is not explicitly aligned with clinical efficacy. In this work, we introduce **EditGRPO**, a mixed-policy reinforcement learning algorithm designed specifically to optimize the generation through clinically motivated rewards. EditGRPO integrates on-policy exploration with off-policy guidance by injecting sentence-level detailed corrections during training rollouts. This mixed-policy approach addresses the exploration dilemma and sampling efficiency issues typically encountered in RL. Applied to a Qwen2.5-VL-3B, EditGRPO outperforms both SFT and vanilla GRPO baselines, achieving an average improvement of 3.4% in clinical metrics across four major datasets. Notably, EditGRPO also demonstrates superior out-of-domain generalization, with an average performance gain of 5.9% on unseen datasets.</abstract>
      <url hash="d0389910">2025.ijcnlp-short.26</url>
      <bibkey>zhang-etal-2025-editgrpo</bibkey>
    </paper>
    <paper id="27">
      <title>Enhancing Long Document Long Form Summarisation with Self-Planning</title>
      <author><first>Xiaotang</first><last>Du</last></author>
      <author><first>Rohit</first><last>Saxena</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Laura</first><last>Perez-Beltrachini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author id="pasquale-minervini" orcid="0000-0002-8442-602X"><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Ivan</first><last>Titov</last><affiliation>University of Edinburgh and University of Amsterdam</affiliation></author>
      <pages>317-332</pages>
      <abstract>We introduce a novel approach for long context summarisation, highlight-guided generation, that leverages sentence-level information as a content plan to improve the traceability and faithfulness of generated summaries. Our framework applies self-planning methods to identify important content and then generates a summary conditioned on the plan. We explore both an end-to-end and two-stage variants of the approach, finding that the two-stage pipeline performs better on long and information-dense documents. Experiments on long-form summarisation datasets demonstrate that our method consistently improves factual consistency while preserving relevance and overall quality. On GovReport, our best approach has improved ROUGE-L by 4.1 points and achieves about 35% gains in SummaC scores. Qualitative analysis shows that highlight-guided summarisation helps preserve important details, leading to more accurate and insightful summaries across domains.</abstract>
      <url hash="4d574f5a">2025.ijcnlp-short.27</url>
      <bibkey>du-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="28">
      <title>Faithful Transcription: Leveraging <fixed-case>B</fixed-case>ible Recordings to Improve <fixed-case>ASR</fixed-case> for Endangered Languages</title>
      <author id="eric-le-ferrand" orcid="0009-0001-0363-6771"><first>Eric</first><last>Le Ferrand</last><affiliation>Boston College</affiliation></author>
      <author><first>Cian Mohamed Bashar</first><last>Hauser</last></author>
      <author id="joshua-hartshorne" orcid="0000-0003-1240-3598"><first>Joshua</first><last>Hartshorne</last><affiliation>MGH Institute of Health Professions</affiliation></author>
      <author id="emily-prudhommeaux"><first>Emily</first><last>Prud’hommeaux</last><affiliation>Boston College</affiliation></author>
      <pages>333-342</pages>
      <abstract>While automatic speech recognition (ASR) now achieves human-level accuracy for a dozen or so languages, the majority of the world’s languages lack the resources needed to train robust ASR models. For many of these languages, the largest available source of transcribed speech data consists of recordings of the Bible. Bible recordings are appealingly large and well-structured resources, but they have notable limitations: the vocabulary and style are constrained, and the recordings are typically produced by a single speaker in a studio. These factors raise an important question: to what extent are Bible recordings useful for developing ASR models to transcribe contemporary naturalistic speech, the goal of most ASR applications? In this paper, we use Bible recordings alongside contemporary speech recordings to train ASR models in a selection of under-resourced and endangered languages. We find that models trained solely on Bible data yield shockingly weak performance when tested on contemporary everyday speech, even when compared to models trained on other (non-Bible) out-of-domain data. We identify one way of effectively leveraging Bible data in the ASR training pipeline via a two-stage training regime. Our results highlight the need to re-assess reported results relying exclusively on Bible data and to use Bible data carefully and judiciously.</abstract>
      <url hash="02694683">2025.ijcnlp-short.28</url>
      <bibkey>le-ferrand-etal-2025-faithful</bibkey>
    </paper>
    <paper id="29">
      <title>Still Not There: Can <fixed-case>LLM</fixed-case>s Outperform Smaller Task-Specific <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Models on the Poetry-to-Prose Conversion Task?</title>
      <author><first>Kunal Kingkar</first><last>Das</last></author>
      <author id="manoj-balaji-jagadeeshan" orcid="0000-0001-6452-4768"><first>Manoj Balaji</first><last>Jagadeeshan</last><affiliation>Indian Institute of Technology Kharagpur, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Nallani Chakravartula</first><last>Sahith</last></author>
      <author><first>Jivnesh</first><last>Sandhan</last></author>
      <author><first>Pawan</first><last>Goyal</last><affiliation>IIT Kharagpur</affiliation></author>
      <pages>343-357</pages>
      <abstract>Large Language Models (LLMs) are increasingly treated as universal, general-purpose solutions across NLP tasks, particularly in English. But does this assumption hold for low-resource, morphologically rich languages such as Sanskrit? We address this question by comparing instruction-tuned and in-context-prompted LLMs with smaller task-specific encoder–decoder models on the Sanskrit poetry-to-prose conversion task. This task is intrinsically challenging: Sanskrit verse exhibits free word order combined with rigid metrical constraints, and its conversion to canonical prose (anvaya) requires multi-step reasoning involving compound segmentation, dependency resolution, and syntactic linearisation. This makes it an ideal testbed to evaluate whether LLMs can surpass specialised models.For LLMs, we apply instruction fine-tuning on general-purpose models and design in-context learning templates grounded in Pāṇinian grammar and classical commentary heuristics. For task-specific modelling, we fully fine-tune a ByT5-Sanskrit Seq2Seq model. Our experiments show that domain-specific fine-tuning of ByT5-Sanskrit significantly outperforms all instruction-driven LLM approaches. Human evaluation strongly corroborates this result, with scores exhibiting high correlation with Kendall’s Tau scores.Additionally, our prompting strategies provide an alternative to fine-tuning when domain-specific verse corpora are unavailable, and the task-specific Seq2Seq model demonstrates robust generalisation on out-of-domain evaluations.Our code<tex-math>^1</tex-math> and dataset<tex-math>^2</tex-math> are publicly available.</abstract>
      <url hash="6df252da">2025.ijcnlp-short.29</url>
      <bibkey>das-etal-2025-still</bibkey>
    </paper>
    <paper id="30">
      <title>Exploring the Performance of Large Language Models on Subjective Span Identification Tasks</title>
      <author id="alphaeus-dmonte" orcid="0009-0009-7896-5834"><first>Alphaeus</first><last>Dmonte</last></author>
      <author id="roland-r-oruche" orcid="0000-0002-6631-9298"><first>Roland R</first><last>Oruche</last></author>
      <author id="tharindu-ranasinghe" orcid="0000-0003-3207-3821"><first>Tharindu</first><last>Ranasinghe</last><affiliation>Lancaster University</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <author><first>Prasad</first><last>Calyam</last></author>
      <pages>358-371</pages>
      <abstract>Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.</abstract>
      <url hash="377040ae">2025.ijcnlp-short.30</url>
      <bibkey>dmonte-etal-2025-exploring</bibkey>
    </paper>
    <paper id="31">
      <title>Broken Words, Broken Performance: Effect of Tokenization on Performance of <fixed-case>LLM</fixed-case>s</title>
      <author id="sachin-pawar" orcid="0000-0002-6531-7127"><first>Sachin</first><last>Pawar</last></author>
      <author id="manoj-apte" orcid="0000-0001-8602-369X"><first>Manoj</first><last>Apte</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author id="kshitij-jadhav" orcid="0009-0007-6286-9325"><first>Kshitij</first><last>Jadhav</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Girish Keshav</first><last>Palshikar</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Nitin</first><last>Ramrakhiyani</last><affiliation>International Institute of Information Technology Hyderabad and Tata Consultancy Services Limited, India</affiliation></author>
      <pages>372-385</pages>
      <abstract>Tokenization is the first step in training any Large Language Model (LLM), where the text is split into a sequence of tokens as per the model’s fixed vocabulary. This tokenization in LLMs is different from the traditional tokenization in NLP where the text is split into a sequence of “natural” words. In LLMs, a natural word may also be broken into multiple tokens due to limited vocabulary size of the LLMs (e.g., Mistral’s tokenizer splits “martial” into “mart” and “ial”). In this paper, we hypothesize that such breaking of natural words negatively impacts LLM performance on various NLP tasks. To quantify this effect, we propose a set of penalty functions that compute a tokenization penalty for a given text for a specific LLM, indicating how “bad” the tokenization is. We establish statistical significance of our hypothesis on multiple NLP tasks for a set of different LLMs.</abstract>
      <url hash="be44081f">2025.ijcnlp-short.31</url>
      <bibkey>pawar-etal-2025-broken</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>R</fixed-case>e<fixed-case>G</fixed-case>raph: Learning to Reformulate Graph Encodings with Large Language Models</title>
      <author><first>Amir</first><last>Hadifar</last></author>
      <author><first>Christopher</first><last>Ochs</last><affiliation>Nokia Bell Labs</affiliation></author>
      <author><first>Arjan</first><last>Van Ewijk</last></author>
      <pages>386-394</pages>
      <abstract>Large language models can rephrase and restructure natural language effectively, but their potential for reformulating graph encodings remains underexplored despite the significant impact of encoding choices on performance.In this work, we introduce ReGraph, a reinforcement learning-based approach that guides language models to reformulate graph encodings for improved task alignment.We demonstrate that reformulating graph encodings enhances reasoning and yields consistent performance gains on graph question answering tasks.Our results show that language models often prefer specific graph encodings, even if they are suboptimal for the task at hand.</abstract>
      <url hash="e7b83b00">2025.ijcnlp-short.32</url>
      <bibkey>hadifar-etal-2025-regraph</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>LLM</fixed-case>s Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring</title>
      <author><first>Chloe</first><last>Li</last><affiliation>University College London, University of London</affiliation></author>
      <author id="noah-y-siegel" orcid="0000-0002-5746-117X"><first>Noah Y.</first><last>Siegel</last><affiliation>University College London, University of London and Google DeepMind</affiliation></author>
      <pages>395-422</pages>
      <abstract>Trustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an AI system is safe to deploy. One empirically demonstrated threat is <tex-math>\textit{sandbagging}</tex-math>—the strategic underperformance on evaluations by AI models or their developers. A promising defense is to monitor a model’s chain-of-thought (CoT) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a CoT monitor by prompting them to sandbag while being either monitor-oblivious or monitor-aware. We show that both frontier models and small open-sourced models can <tex-math>\textit{covertly}</tex-math> sandbag against CoT monitoring 0-shot without hints. However, they cannot yet do so reliably: they bypass the monitor 16-36% of the time when monitor-aware, conditioned on sandbagging successfully. We qualitatively analyzed the uncaught CoTs to understand <tex-math>\textit{why}</tex-math> the monitor failed. We reveal a rich attack surface for CoT monitoring and contribute five covert sandbagging policies generated by models. These results inform potential failure modes of CoT monitoring and may help build more diverse sandbagging model organisms.</abstract>
      <url hash="ef911d79">2025.ijcnlp-short.33</url>
      <bibkey>li-siegel-2025-llms</bibkey>
    </paper>
    <paper id="34">
      <title>Meronymic Ontology Extraction via Large Language Models</title>
      <author id="dekai-zhang" orcid="0000-0001-5770-0903"><first>Dekai</first><last>Zhang</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Simone</first><last>Conia</last><affiliation>Sapienza University of Rome</affiliation></author>
      <author id="antonio-rago" orcid="0000-0001-5323-7739"><first>Antonio</first><last>Rago</last><affiliation>Imperial College London</affiliation></author>
      <pages>423-433</pages>
      <abstract>Ontologies have become essential in today’s digital age as a way of organising the vast amount of readily available unstructured text. In providing formal structure to this information, ontologies have immense value and application across various domains, e.g., e-commerce, where countless product listings necessitate proper product organisation. However, the manual construction of these ontologies is a time-consuming, expensive and laborious process. In this paper, we harness the recent advancements in large language models (LLMs) to develop a fully automated method of extracting product ontologies, in the form of meronymies, from raw review texts. We demonstrate that the ontologies produced by our method surpass an existing, BERT-based baseline when evaluating using an LLM-as-a-judge. Our investigation provides the groundwork for LLMs to be used more generally in (product or otherwise) ontology extraction.</abstract>
      <url hash="fb22b449">2025.ijcnlp-short.34</url>
      <bibkey>zhang-etal-2025-meronymic</bibkey>
    </paper>
    <paper id="35">
      <title>Compositional Phoneme Approximation for <fixed-case>L</fixed-case>1-Grounded <fixed-case>L</fixed-case>2 Pronunciation Training</title>
      <author><first>Jisang</first><last>Park</last></author>
      <author><first>Minu</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>DaYoung</first><last>Hong</last><affiliation>Brown University</affiliation></author>
      <author id="jongha-lee" orcid="0000-0003-1397-1642"><first>Jongha</first><last>Lee</last><affiliation>School of Engineering and Applied Sciences, Harvard University</affiliation></author>
      <pages>434-443</pages>
      <abstract>Learners of a second language (L2) often map non-native phonemes to similar native-language (L1) phonemes, making conventional L2-focused training slow and effortful. To address this, we propose an L1-grounded pronunciation training method based on compositional phoneme approximation (CPA), a feature-based representation technique that approximates L2 sounds with sequences of L1 phonemes.Evaluations with 20 Korean non-native English speakers show that CPA-based training achieves a 76% in-box formant rate in acoustic analysis, 17.6% relative improvement in phoneme recognition accuracy, and over 80% of speech being rated as more native-like, with minimal training. Project page: <url>https://gsanpark.github.io/CPA-Pronunciation</url>.</abstract>
      <url hash="a6c52f00">2025.ijcnlp-short.35</url>
      <bibkey>park-etal-2025-compositional</bibkey>
    </paper>
    <paper id="36">
      <title>Can Language Models Handle a Non-Gregorian Calendar? The Case of the <fixed-case>J</fixed-case>apanese wareki</title>
      <author><first>Mutsumi</first><last>Sasaki</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Go</first><last>Kamoda</last><affiliation>Graduate University for Advanced Studies and National Institute for Japanese Language and Linguistics</affiliation></author>
      <author><first>Ryosuke</first><last>Takahashi</last><affiliation>Tohoku University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Kosuke</first><last>Sato</last><affiliation>Tohoku University</affiliation></author>
      <author id="kentaro-inui" orcid="0000-0001-6510-604X"><first>Kentaro</first><last>Inui</last><affiliation>MBZUAI, RIKEN and Tohoku University</affiliation></author>
      <author><first>Keisuke</first><last>Sakaguchi</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Benjamin</first><last>Heinzerling</last><affiliation>Tohoku University and RIKEN</affiliation></author>
      <pages>444-463</pages>
      <abstract>Temporal reasoning and knowledge are essential capabilities for language models (LMs).While much prior work has analyzed and improved temporal reasoning in LMs, most studies have focused solely on the Gregorian calendar.However, many non-Gregorian systems, such as the Japanese, Hijri, and Hebrew calendars, are in active use and reflect culturally grounded conceptions of time.If and how well current LMs can accurately handle such non-Gregorian calendars has not been evaluated so far.Here, we present a systematic evaluation of how well language models handle one such non-Gregorian system: the Japanese *wareki*.We create datasets that require temporal knowledge and reasoning in using *wareki* dates. Evaluating open and closed LMs, we find that some models can perform calendar conversions, but GPT-4o, Deepseek V3, and even Japanese-centric models struggle with Japanese calendar arithmetic and knowledge involving *wareki* dates.Error analysis suggests corpus frequency of Japanese calendar expressions and a Gregorian bias in the model’s knowledge as possible explanations.Our results show the importance of developing LMs that are better equipped for culture-specific tasks such as calendar understanding.</abstract>
      <url hash="eccff751">2025.ijcnlp-short.36</url>
      <bibkey>sasaki-etal-2025-language</bibkey>
    </paper>
    <paper id="37">
      <title>Modeling Contextual Passage Utility for Multihop Question Answering</title>
      <author><first>Akriti</first><last>Jain</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Aparna</first><last>Garimella</last><affiliation>Adobe Research</affiliation></author>
      <pages>464-471</pages>
      <abstract>Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multi-hop reasoning, that the utility of a passage can be context-dependent, influenced by its relation to other passages—whether it provides complementary information, or forms a crucial link in conjunction with others. In this paper, we propose a light-weight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question, to obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to better reranking and downstream task performance compared to relevance-based reranking methods.</abstract>
      <url hash="1f01380f">2025.ijcnlp-short.37</url>
      <bibkey>jain-garimella-2025-modeling</bibkey>
    </paper>
    <paper id="38">
      <title>Improving <fixed-case>LLM</fixed-case>’s Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization</title>
      <author id="hadi-sheikhi" orcid="0000-0001-7088-2406"><first>Hadi</first><last>Sheikhi</last><affiliation>University of Alberta</affiliation></author>
      <author id="chenyang-huang" orcid="0000-0003-2811-6008"><first>Chenyang</first><last>Huang</last></author>
      <author id="osmar-r-zaiane" orcid="0000-0002-0060-5988"><first>Osmar</first><last>Zaiane</last><affiliation>University of Alberta</affiliation></author>
      <pages>472-483</pages>
      <abstract>Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs’ attachment on external knowledge.</abstract>
      <url hash="903325e4">2025.ijcnlp-short.38</url>
      <bibkey>sheikhi-etal-2025-improving</bibkey>
    </paper>
    <paper id="39">
      <title>Agreement-Constrained Probabilistic Minimum <fixed-case>B</fixed-case>ayes Risk Decoding</title>
      <author><first>Koki</first><last>Natsumi</last></author>
      <author id="hiroyuki-deguchi" orcid="0000-0003-2127-6607"><first>Hiroyuki</first><last>Deguchi</last><affiliation>NTT</affiliation></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author id="hidetaka-kamigaito" orcid="0000-0002-5249-5813"><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author id="taro-watanabe" orcid="0000-0001-8349-3522"><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>484-493</pages>
      <abstract>Minimum Bayes risk (MBR) decoding generates high-quality translations by maximizing the expected utility of output candidates, but it evaluates all pairwise scores over the candidate set; hence, it takes quadratic time with respect to the number of candidates. To reduce the number of utility function calls, probabilistic MBR (PMBR) decoding partially evaluates quality scores using sampled pairs of candidates and completes the missing scores with a matrix completion algorithm. Nevertheless, it degrades the translation quality as the number of utility function calls is reduced. Therefore, to improve the trade-off between quality and cost, we propose agreement-constrained PMBR (AC-PMBR) decoding, which leverages a knowledge distilled model to guide the completion of the score matrix. Our AC-PMBR decoding improved approximation errors of matrix completion by up to 3 times and achieved higher translation quality compared with PMBR decoding at a comparable computational cost on the WMT’23 En↔De translation tasks.</abstract>
      <url hash="a67b51e0">2025.ijcnlp-short.39</url>
      <bibkey>natsumi-etal-2025-agreement</bibkey>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2026-01-12" type="proceedings">
    <meta>
      <booktitle>The 14th International Joint Conference on Natural Language Processing and The 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics</booktitle>
      <editor><first>Santosh</first><last>T.y.s.s</last></editor>
      <editor><first>Shuichiro</first><last>Shimizu</last></editor>
      <editor><first>Yifan</first><last>Gong</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mumbai, India</address>
      <month>December</month>
      <year>2025</year>
      <url hash="9ae18b55">2025.ijcnlp-srw</url>
      <venue>ijcnlp</venue>
      <isbn>979-8-89176-304-3</isbn>
    </meta>
    <frontmatter>
      <url hash="2a921bdd">2025.ijcnlp-srw.0</url>
      <bibkey>ijcnlp-2025-srw</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Interpretable Sparse Features for Probing Self-Supervised Speech Models</title>
      <author id="inigo-parra" orcid="0009-0009-8686-9666"><first>Iñigo</first><last>Parra</last></author>
      <pages>1-9</pages>
      <abstract>Self-supervised speech models have demonstrated the ability to learn rich acoustic representations. However, interpreting which specific phonological or acoustic features these models leverage within their highly polysemantic activations remains challenging. In this paper, we propose a straightforward and unsupervised probing method for model interpretability. We extract the activations from the final MLP layer of a pretrained HuBERT model and train a sparse autoencoder (SAE) using dictionary learning techniques to generate an over-complete set of latent representations. Analyzing these latent codes, we observe that a small subset of high-variance units consistently aligns with phonetic events, suggesting their potential utility as interpretable acoustic detectors. Our proposed method does not require labeled data beyond raw audio, providing a lightweight and accessible tool to gain insights into the internal workings of self-supervised speech models.</abstract>
      <url hash="e4802558">2025.ijcnlp-srw.1</url>
      <bibkey>parra-2025-interpretable</bibkey>
    </paper>
    <paper id="2">
      <title>Learning Dynamics of Meta-Learning in Small Model Pretraining</title>
      <author id="david-demitri-africa" orcid="0009-0005-3543-8891"><first>David Demitri</first><last>Africa</last><affiliation>UK AI Security Institute</affiliation></author>
      <author><first>Yuval</first><last>Weiss</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Paula</first><last>Buttery</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Richard</first><last>Diehl Martinez</last></author>
      <pages>10-23</pages>
      <abstract>Large language models are powerful but costly. We ask whether meta-learning can make the pretraining of small language models not only faster but also more interpretable. We integrate first–order MAML with subset-masked LM pretraining, producing four LLama-style decoder-only models (11M–570M params), and evaluate on multilingual Universal NER. Compared with vanilla training, our hybrid setup (i) reaches the same loss up to 1.6× sooner, (ii) yields modest but consistent average gains on Universal NER at medium/large scales under equal compute (+2–3 percentage points), and (iii) and (iii) reveals phase-like learning dynamics: models first diversify their representations, then compress them in a pattern that aligns with improved episodic accuracy. These observations are correlational, not causal, and we do not claim generality beyond NER or across seeds. We also document a trade-off: perplexity on Paloma (a diverse language modeling benchmark spanning 18 domains) is worse at most scales. Code, checkpoints and analysis logs are released.</abstract>
      <url hash="39845b06">2025.ijcnlp-srw.2</url>
      <bibkey>africa-etal-2025-learning</bibkey>
    </paper>
    <paper id="3">
      <title>Efficient Environmental Claim Detection with Hyperbolic Graph Neural Networks</title>
      <author><first>Darpan</first><last>Aswal</last></author>
      <author id="manjira-sinha" orcid="0000-0002-2653-5092"><first>Manjira</first><last>Sinha</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <pages>24-35</pages>
      <abstract>Transformer based models, specially large language models (LLMs) dominate the field of NLP with their mass adoption in tasks such as text generation, summarization and fake news detection. These models offer ease of deployment and reliability for most applications, however, they require significant amounts of computational power for training as well as inference. This poses challenges in their adoption in resource-constrained applications, specially in the open-source community where compute availability is usually scarce. This work proposes a graph-based approach for Environmental Claim Detection, exploring Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as lightweight yet effective alternatives to transformer-based models. Re-framing the task as a graph classification problem, we transform claim sentences into dependency parsing graphs, utilizing a combination of word2vec &amp; learnable part-of-speech (POS) tag embeddings for the node features and encoding syntactic dependencies in the edge relations. Our results show that our graph-based models, particularly HGNNs in the poincaré space (P-HGNNs), achieve performance superior to the state-of-the-art on environmental claim detection while using up to **30x fewer parameters**. We also demonstrate that HGNNs benefit vastly from explicitly modeling data in hierarchical (tree-like) structures, enabling them to significantly improve over their euclidean counterparts.</abstract>
      <url hash="3fcdcd4e">2025.ijcnlp-srw.3</url>
      <bibkey>aswal-sinha-2025-efficient</bibkey>
    </paper>
    <paper id="4">
      <title>Stacked <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Isolated Low-Rank Adaptation for Lifelong Knowledge Management</title>
      <author><first>Heramb Vivek</first><last>Patil</last><affiliation>Birla Institute of Technology and Science, Pilani</affiliation></author>
      <author><first>Vaishnavee</first><last>Sanam</last></author>
      <author id="minakshi-pradeep-atre" orcid="0000-0002-5194-201X"><first>Minakshi Pradeep</first><last>Atre</last><affiliation>PVG’s COET &amp; GKPIM Pune</affiliation></author>
      <pages>36-46</pages>
      <abstract>Continual learning (CL) presents a significant challenge for large pre-trained models, primarily due to catastrophic forgetting and the high computational cost of sequential knowledge updating. Parameter-Efficient Transfer Learning (PETL) methods offer reduced computational burdens but often struggle to effectively mitigate forgetting. This paper introduces Stacked Low-Rank Adaptation (SLoRA), a novel parameter-efficient approach that leverages the additive composition of task-specific, frozen low-rank adapters to enable modular continual learning with inherent support for explicit knowledge modification. SLoRA was evaluated on vision benchmarks, BERT-base, and the 1-billion-parameter Llama-3.2-1B model. Experiments demonstrated that SLoRA almost completely eliminated catastrophic forgetting, achieving a final average accuracy of 92.75% on Llama-3.2-1B while perfectly preserving prior task performance. Furthermore, SLoRA is computationally efficient, enabling up to a 15x training speed-up over full fine-tuning with 99.7% fewer trainable parameters per update. SLoRA offers a compelling balance of forgetting mitigation, parameter efficiency, and modularity, representing a promising direction for developing adaptable and efficient lifelong knowledgeable foundation models.</abstract>
      <url hash="ea069369">2025.ijcnlp-srw.4</url>
      <bibkey>patil-etal-2025-stacked</bibkey>
    </paper>
    <paper id="5">
      <title>On Multilingual Encoder Language Model Compression for Low-Resource Languages</title>
      <author><first>Daniil</first><last>Gurgurov</last></author>
      <author id="michal-gregor" orcid="0000-0001-8332-6011"><first>Michal</first><last>Gregor</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Josef Van</first><last>Genabith</last></author>
      <author id="simon-ostermann" orcid="0000-0002-0899-0657"><first>Simon</first><last>Ostermann</last><affiliation>German Research Center for AI</affiliation></author>
      <pages>47-58</pages>
      <abstract>In this paper, we combine two-step knowledge distillation, structured pruning, truncation, and vocabulary trimming for extremely compressing multilingual encoder-only language models for low-resource languages. Our novel approach systematically combines existing techniques and takes them to the extreme, reducing layer depth, feed-forward hidden size, and intermediate layer embedding size to create significantly smaller monolingual models while retaining essential language-specific knowledge. We achieve compression rates of up to 92% while maintaining competitive performance, with average drops of 2–10% for moderate compression and 8–13% at maximum compression in four downstream tasks, including sentiment analysis, topic classification, named entity recognition, and part-of-speech tagging, across three low-resource languages. Notably, the performance degradation correlates with the amount of language-specific data in the teacher model, with larger datasets resulting in smaller performance losses. Additionally, we conduct ablation studies to identify the best practices for multilingual model compression using these techniques.</abstract>
      <url hash="04910e91">2025.ijcnlp-srw.5</url>
      <bibkey>gurgurov-etal-2025-multilingual-encoder</bibkey>
    </paper>
    <paper id="6">
      <title>Do We Need Large <fixed-case>VLM</fixed-case>s for Spotting Soccer Actions?</title>
      <author><first>Ritabrata</first><last>Chakraborty</last></author>
      <author><first>Rajatsubhra</first><last>Chakraborty</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <author id="avijit-dasgupta" orcid="0000-0001-5633-1843"><first>Avijit</first><last>Dasgupta</last></author>
      <author id="sandeep-chaurasia" orcid="0000-0002-0935-9795"><first>Sandeep</first><last>Chaurasia</last><affiliation>Manipal University Jaipur</affiliation></author>
      <pages>59-65</pages>
      <abstract>Traditional video-based tasks like soccer action spotting rely heavily on visual inputs, often requiring complex and computationally expensive models to process dense video data. We propose a shift from this video-centric approach to a text-based task, making it lightweight and scalable by utilizing Large Language Models (LLMs) instead of Vision-Language Models (VLMs). We posit that expert commentary, which provides rich descriptions and contextual cues contains sufficient information to reliably spot key actions in a match. To demonstrate this, we employ a system of three LLMs acting as judges specializing in outcome, excitement, and tactics for spotting actions in soccer matches. Our experiments show that this language-centric approach performs effectively in detecting critical match events coming close to state-of-the-art video-based spotters while using zero video processing compute and similar amount of time to process the entire match.</abstract>
      <url hash="90a53edc">2025.ijcnlp-srw.6</url>
      <bibkey>chakraborty-etal-2025-need</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>LRMGS</fixed-case>: A Language-Robust Metric for Evaluating Question Answering in Very Low-Resource <fixed-case>I</fixed-case>ndic Languages</title>
      <author id="anuj-kumar" orcid="0000-0002-1785-370X"><first>Anuj</first><last>Kumar</last></author>
      <author id="satyadev-ahlawat" orcid="0000-0003-0186-1446"><first>Satyadev</first><last>Ahlawat</last><affiliation>Indian Institute of Technology Jammu</affiliation></author>
      <author id="yamuna-prasad" orcid="0000-0002-3709-7956"><first>Yamuna</first><last>Prasad</last><affiliation>Indian Institute of Technology Jammu</affiliation></author>
      <author><first>Virendra</first><last>Singh</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <pages>66-77</pages>
      <abstract>Reliable evaluation of Question Answering (QA) systems in low-resource Indic languages presents a significant challenge due to limited annotated datasets, linguistic diversity, and suitable evaluation metrics. Languages such as Sindhi, Manipuri, Dogri, Konkani, and Maithili are particularly underrepresented, creating difficulty in assessing Large Language Models (LLMs) on QA tasks. Existing metrics, including BLEU, ROUGE-L, and BERTScore, are effective in machine translation and high-resource settings; however, they often fail in low-resource QA due to score compression, zero-inflation, and poor scale alignment. To overcome this, LRMGS (Language-Robust Metric for Generative QA) is introduced to capture semantic and lexical agreement while preserving the score scale across languages. LRMGS is evaluated across 8 Indic languages and multiple LLMs, demonstrating consistently higher concordance with reference-based chrF++ scores, measured using the Concordance Correlation Coefficient (CCC). Experimental results indicate that LRMGS provides more accurate discrimination of system performance in very low-resource languages compared to existing metrics. This work establishes a robust and interpretable framework for evaluating QA systems in low-resource Indic languages, supporting more reliable multilingual model assessment.</abstract>
      <url hash="43f73197">2025.ijcnlp-srw.7</url>
      <bibkey>kumar-etal-2025-lrmgs</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>N</fixed-case>um<fixed-case>P</fixed-case>ert: Numerical Perturbations to Probe Language Models for Veracity Prediction</title>
      <author id="peter-roysland-aarnes" orcid="0009-0002-3605-4847"><first>Peter Røysland</first><last>Aarnes</last></author>
      <author id="vinay-setty" orcid="0000-0002-9777-6758"><first>Vinay</first><last>Setty</last><affiliation>Factiverse and University of Stavanger</affiliation></author>
      <pages>78-95</pages>
      <abstract>Large language models show strong performance on knowledge intensive tasks such as fact checking and question answering, yet they often struggle with numerical reasoning. We present a systematic evaluation of state-of-the-art models for veracity prediction on numerical claims and evidence pairs using controlled perturbations, including label flipping probes, to test robustness. Our results indicate that even leading proprietary systems experience accuracy drops of up to 62% under certain perturbations. No model proves to be robust across all conditions. We further find that increasing context length generally reduces accuracy, but when extended context is enriched with perturbed demonstrations, most models substantially recover. These findings highlight critical limitations in numerical fact-checking and suggest that robustness remains an open challenge for current language models.</abstract>
      <url hash="62c245aa">2025.ijcnlp-srw.8</url>
      <bibkey>aarnes-setty-2025-numpert</bibkey>
    </paper>
    <paper id="9">
      <title>Testing Simulation Theory in <fixed-case>LLM</fixed-case>s’ Theory of Mind</title>
      <author><first>Koshiro</first><last>Aoki</last><affiliation>Waseda University</affiliation></author>
      <author id="daisuke-kawahara" orcid="0000-0002-3598-1027"><first>Daisuke</first><last>Kawahara</last><affiliation>Waseda University</affiliation></author>
      <pages>96-104</pages>
      <abstract>Theory of Mind (ToM) is the ability to understand others’ mental states, which is essential for human social interaction. Although recent studies suggest that large language models (LLMs) exhibit human-level ToM capabilities, the underlying mechanisms remain unclear. “Simulation Theory” posits that we infer others’ mental states by simulating their cognitive processes, which has been widely discussed in cognitive science. In this work, we propose a framework for investigating whether the ToM mechanism in LLMs is based on Simulation Theory by analyzing their internal representations. Following this framework, we successfully steered LLMs’ ToM reasoning through modeled perspective-taking and counterfactual interventions. Our results suggest that Simulation Theory may partially explain the ToM mechanism in state-of-the-art LLMs, indicating parallels between human and artificial social reasoning.</abstract>
      <url hash="afd11edd">2025.ijcnlp-srw.9</url>
      <bibkey>aoki-kawahara-2025-testing</bibkey>
    </paper>
    <paper id="10">
      <title>Turn-by-Turn Behavior Monitoring in <fixed-case>LM</fixed-case>-Guided Psychotherapy</title>
      <author><first>Anish Sai</first><last>Chedalla</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Samina</first><last>Ali</last></author>
      <author><first>Jiuming</first><last>Chen</last><affiliation>Algoverse AI Research</affiliation></author>
      <author><first>Starborn0128@gmail.com</first><last>Starborn0128@gmail.com</last><affiliation>NA</affiliation></author>
      <author><first>Eric</first><last>Xia</last></author>
      <pages>105-122</pages>
      <abstract>Large language models (LLMs) have the potential to be powerful instruments for psychotherapy. However, there is a shortage of practical tools to support their use in production. We develop a novel, iterative process of updating conversational context for tracking EIS (Emotional Intelligence Scale) instantaneously, and test Llama-70b. Through this, we show that (1) EIS varies more on psychotherapeutic (emotional support) conversations than control (emotionally unstimulating) conversations and (2) model responses can be systematically classified to identify consistent patterns. Thus, EIS is a valid indicator of empathetic model behavior. Rises in the EIS score correspond to prosocial behavior, and falls correspond to detached, unsocial behavior. These results suggest that psychometric questionnaires like EIS can provide a structured lens for observing empathetic stability of models and offer a foundation for future work on their role in psychotherapy.</abstract>
      <url hash="7846337c">2025.ijcnlp-srw.10</url>
      <bibkey>chedalla-etal-2025-turn</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>B</fixed-case>ook<fixed-case>A</fixed-case>s<fixed-case>S</fixed-case>um<fixed-case>QA</fixed-case>: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering</title>
      <author><first>Ryuhei</first><last>Miyazato</last></author>
      <author id="ting-ruen-wei" orcid="0009-0008-2649-4543"><first>Ting-Ruen</first><last>Wei</last><affiliation>Santa Clara University</affiliation></author>
      <author id="xuyang-wu" orcid="0000-0002-8807-0016"><first>Xuyang</first><last>Wu</last></author>
      <author><first>Hsin-Tai</first><last>Wu</last></author>
      <author><first>Kei</first><last>Harada</last><affiliation>University of Electro-Communications</affiliation></author>
      <pages>123-133</pages>
      <abstract>Aspect-based summarization aims to generate summaries that highlight specific aspects of a text, enabling more personalized and targeted summaries. However, its application to books remains unexplored due to the difficulty of constructing reference summaries for long text. To address this challenge, we propose BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization. BookAsSumQA automatically generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summary quality based on its question-answering performance. Our experiments using BookAsSumQA revealed that while LLM-based approaches showed higher accuracy on shorter texts, RAG-based methods become more effective as document length increases, making them more efficient and practical for aspect-based book summarization.</abstract>
      <url hash="a634ca2f">2025.ijcnlp-srw.11</url>
      <bibkey>miyazato-etal-2025-bookassumqa</bibkey>
    </paper>
    <paper id="12">
      <title>Thesis Proposal: Interpretable Reasoning Enhancement in Large Language Models through Puzzle and Ontological Task Analysis</title>
      <author id="mihir-panchal" orcid="0009-0001-6459-0439"><first>Mihir</first><last>Panchal</last></author>
      <pages>134-144</pages>
      <abstract>Large language models (LLMs) excel across diverse natural language processing tasks but remain opaque and unreliable. This thesis investigates how LLM reasoning can be made both interpretable and reliable through systematic analysis of internal dynamics and targeted interventions. Unlike prior work that examines reasoning broadly, this research focuses on two representative domains: puzzle solving, where reasoning steps can be precisely tracked, and ontological inference, where hierarchical structures constrain valid reasoning. The central questions are: (1) How can systematic error patterns in domain specific reasoning be detected through layer wise probing and mitigated through targeted interventions? (2) How can probing frameworks and middle layer analyses reveal and enhance the computational mechanisms underlying inference? By combining probing methods, middle layer investigations, and probe guided interventions, the work aims to uncover interpretable reasoning patterns, identify systematic failure modes, and develop adaptive enhancement strategies. The expected outcome is a domain grounded framework that advances both theoretical understanding of neural reasoning and the design of practical, trustworthy AI systems.</abstract>
      <url hash="9a13ee68">2025.ijcnlp-srw.12</url>
      <bibkey>panchal-2025-thesis</bibkey>
    </paper>
    <paper id="13">
      <title>Adaptive Coopetition: Leveraging Coarse Verifier Signals for Resilient Multi-Agent <fixed-case>LLM</fixed-case> Reasoning</title>
      <author><first>Wendy Yaqiao</first><last>Liu</last></author>
      <author><first>Rui Jerry</first><last>Huang</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Anastasia</first><last>Miin</last></author>
      <author id="lei-ding" orcid="0009-0002-2173-5666"><first>Lei</first><last>Ding</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <pages>145-155</pages>
      <abstract>Inference-time computation is a critical yet challenging paradigm for enhancing the reasoning performance of large language models (LLMs). While existing strategies improve reasoning stability and consistency, they suffer from notable limitations: self-correction often reinforces the model’s initial biases, and Multi-Agent Collaboration (MAC) often fails due to the lack of efficient coordination mechanisms, leading to collective errors. Although high-performing verifiers can detect reasoning errors, making them reliable requires substantial training. To address these challenges, we introduce a novel inference-time framework - **Adaptive Coopetition (AdCo)** - in which LLM agents utilize **an adaptive, UCB-based ‘coopetition’ mechanism**. At each round, agents leverage coarse verifier signals to determine whether to collaborate or compete, further iteratively refining their reasoning based on peer feedback. Without relying on high-performance verifiers, our adaptive strategy achieves significant performance gains on mathematical reasoning benchmarks, yielding **a 20% relative improvement** over baselines on the more challenging dataset. Our approach remains robust and consistent in terms of accuracy under different sample sizes and configurations. This adaptive, signal-guided ‘coopetition’ framework enhances reasoning robustness by leveraging bothmodel knowledge diversity and reasoning trace measure, while also promoting uncertainty-driven exploration, especially when participants have comparable capabilities. From this perspective, our work offers a fresh lens on inference-time computation and paves the way for more resilient multi-agent LLM systems.</abstract>
      <url hash="5c61947c">2025.ijcnlp-srw.13</url>
      <bibkey>liu-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>AI</fixed-case> Through the Human Lens: Investigating Cognitive Theories in Machine Psychology</title>
      <author><first>Akash</first><last>Kundu</last></author>
      <author><first>Rishika</first><last>Goswami</last></author>
      <pages>156-170</pages>
      <abstract>We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks frompsychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yetare shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and futurework that bridges cognitive psychology and AI safety.</abstract>
      <url hash="7455525e">2025.ijcnlp-srw.14</url>
      <bibkey>kundu-goswami-2025-ai</bibkey>
    </paper>
    <paper id="15">
      <title>Thesis Proposal: A <fixed-case>N</fixed-case>euro<fixed-case>S</fixed-case>ymbolic Approach to Control Task-Oriented Dialog Systems</title>
      <author><first>Anuja</first><last>Tayal</last></author>
      <author id="barbara-di-eugenio" orcid="0000-0003-1706-2577"><first>Barbara</first><last>Di Eugenio</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>171-183</pages>
      <abstract>Developing effective healthcare dialog systems requires controlling conversations to offer clear insight into the system’s understanding and to address the lack of patient-oriented conversational datasets. Moreover, evaluating these systems is equally challenging and requires user studies for robust evaluation. These challenges are even more pronounced when addressing the needs of minority populations with low health literacy and numeracy. This thesis proposal focuses on designing conversational architectures that deliver self-care information to African American patients with heart failure.Neuro-symbolic approaches provide a promising direction by integrating symbolic reasoning with the generative capabilities of Large Language Models (LLMs). In this proposal, we explore various approaches to creating a hybrid dialog model by combining the strengths of task-oriented dialog systems with the integration of neuro-symbolic rules into a Language Model (LM)/LLM-based dialog system, thereby controlling the dialog system. We propose a hybrid conversational system that uses schema graphs to control the flow of dialogue, while leveraging LLMs to generate responses grounded in these schemas. We will also conduct a user study to evaluate the system’s effectiveness.</abstract>
      <url hash="6d9473e2">2025.ijcnlp-srw.15</url>
      <bibkey>tayal-di-eugenio-2025-thesis</bibkey>
    </paper>
    <paper id="16">
      <title>Enriching the Low-Resource Neural Machine Translation with Large Language Model</title>
      <author><first>Sachin</first><last>Giri</last><affiliation>Ehime University</affiliation></author>
      <author><first>Takashi</first><last>Ninomiya</last><affiliation>Ehime University</affiliation></author>
      <author><first>Isao</first><last>Goto</last><affiliation>Ehime University</affiliation></author>
      <pages>184-192</pages>
      <abstract>Improving the performance of neural machine translation for low-resource languages is challenging due to the limited availability of parallel corpora. However, recently available Large Language Models (LLM) have demonstrated superior performance in various natural language processing tasks, including translation. In this work, we propose to incorporate an LLM into a Machine Translation (MT) model as a prior distribution to leverage its translation capabilities. The LLM acts as a teacher, instructing the student MT model about the target language. We conducted an experiment in four language pairs: English ⇔ German and English ⇔ Hindi. This resulted in improved BLEU and COMET scores in a low-resource setting.</abstract>
      <url hash="7b9be270">2025.ijcnlp-srw.16</url>
      <bibkey>giri-etal-2025-enriching</bibkey>
    </paper>
    <paper id="17">
      <title>Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models</title>
      <author><first>Tomoki</first><last>Doi</last></author>
      <author><first>Masaru</first><last>Isonuma</last><affiliation>National Institute of Informatics and Tohoku University</affiliation></author>
      <author id="hitomi-yanaka" orcid="0000-0003-0354-6116"><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <pages>193-208</pages>
      <abstract>Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models’ actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.</abstract>
      <url hash="ea4a2df3">2025.ijcnlp-srw.17</url>
      <bibkey>doi-etal-2025-investigating</bibkey>
    </paper>
    <paper id="18">
      <title>Thesis Proposal: Efficient Methods for Natural Language Generation/Understanding Systems</title>
      <author id="nalin-kumar" orcid="0000-0002-8948-3904"><first>Nalin</first><last>Kumar</last></author>
      <pages>209-217</pages>
      <abstract>While Large Language Models (LLMs) have shown remarkable performance in various Natural Language Processing (NLP) tasks, their effectiveness seem to be heavily biased toward high-resource languages. This proposal aims to address this gap by developing efficient training strategies for low-resource languages. We propose various techniques for efficient learning in simluated low-resource settings for English. We then plan to adapt these methods for low-resource languages. We plan to experiment with both natural language generation and understanding models. We evaluate the models on similar benchmarks as the BabyLM challenge for English. For other languages, we plan to use treebanks and translation techniques to create our own silver test set to evaluate the low-resource LMs.</abstract>
      <url hash="c0bacec3">2025.ijcnlp-srw.18</url>
      <bibkey>kumar-2025-thesis</bibkey>
    </paper>
    <paper id="19">
      <title>Two Step Automatic Post Editing of Patent Machine Translation based on Pre-trained Encoder Models and <fixed-case>LLM</fixed-case>s</title>
      <author><first>Kosei</first><last>Buma</last></author>
      <author><first>Takehito</first><last>Utsuro</last><affiliation>University of Tsukuba</affiliation></author>
      <author><first>Masaaki</first><last>Nagata</last><affiliation>NTT Corporation</affiliation></author>
      <pages>218-231</pages>
      <abstract>We study automatic post-editing for patent translation, where accuracy and traceability are critical, and propose a two-step pipeline that combines a multilingual encoder for token-level error detection with an LLM for targeted correction. As no word-level annotations exist for Japanese–English patents, we create supervised data by injecting synthetic errors into parallel patent sentences and fine-tune mBERT, XLM-RoBERTa, and mDeBERTa as detectors. In the second stage, GPT-4o is prompted to revise translations either freely or under a restricted policy that allows edits only on detector-marked spans. For error detection, evaluation on synthetic errors shows that encoder-based detectors outperform LLMs in both F1 and MCC. For error correction, tests on synthetic, repetition, and omission datasets demonstrate statistically significant BLEU gains over LLM methods for synthetic and repetition errors, while omission errors remain challenging. Overall, pairing compact encoders with an LLM enables more accurate and controllable post-editing for key patent error types, reducing unnecessary rewrites via restricted edits. Future work will focus on strengthening omission modeling to better detect and correct missing content.</abstract>
      <url hash="ba2c7c89">2025.ijcnlp-srw.19</url>
      <bibkey>buma-etal-2025-two</bibkey>
    </paper>
    <paper id="20">
      <title>Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over <fixed-case>BPE</fixed-case> and Morphological Alignment</title>
      <author id="saketh-reddy-vemula" orcid="0009-0002-0358-397X"><first>Saketh Reddy</first><last>Vemula</last><affiliation>International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad</affiliation></author>
      <author id="sandipan-dandapat"><first>Sandipan</first><last>Dandapat</last><affiliation>IIT Hyderabad</affiliation></author>
      <author id="dipti-misra-sharma"><first>Dipti</first><last>Sharma</last></author>
      <author><first>Parameswari</first><last>Krishnamurthy</last><affiliation>International Institute of Information Technology Hyderabad, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>232-252</pages>
      <abstract>The relationship between tokenizer algorithm (e.g., Byte-Pair Encoding (BPE), Unigram), morphological alignment, tokenization quality (e.g., compression efficiency), and downstream performance remains largely unclear, particularly for languages with complex morphology. In this paper, we conduct a comprehensive evaluation of tokenizers using small-sized BERT models—from pre-training through fine-tuning—for Telugu (agglutinative), along with preliminary evaluation in Hindi (primarily fusional with some agglutination) and English (fusional). To evaluate morphological alignment of tokenizers in Telugu, we create a dataset containing gold morpheme segmentations of 600 derivational and 7000 inflectional word forms.Our experiments reveal two key findings for Telugu. First, the choice of tokenizer algorithm is the most significant factor influencing performance, with Unigram-based tokenizers consistently outperforming BPE across most settings. Second, while better morphological alignment shows a moderate, positive correlation with performance on text classification and structure prediction tasks, its impact is secondary to the tokenizer algorithm. Notably, hybrid approaches that use morphological information for pre-segmentation significantly boost the performance of BPE, though not Unigram. Our results further showcase the need for comprehensive intrinsic evaluation metrics for tokenizers that could explain downstream performance trends consistently.</abstract>
      <url hash="0c61604f">2025.ijcnlp-srw.20</url>
      <bibkey>vemula-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="21">
      <title>Are <fixed-case>LLM</fixed-case>s Good for Semantic Role Labeling via Question Answering?: A Preliminary Analysis</title>
      <author id="ritwik-raghav" orcid="0009-0007-5947-1586"><first>Ritwik</first><last>Raghav</last></author>
      <author id="abhik-jana" orcid="0000-0002-4485-0002"><first>Abhik</first><last>Jana</last><affiliation>IIT Bhubaneswar</affiliation></author>
      <pages>253-258</pages>
      <abstract>Semantic role labeling (SRL) is a fundamental task in natural language processing that is crucial for achieving deep semantic understanding. Despite the success of large language models (LLMs) in several downstream NLP tasks, key tasks such as SRL remain a challenge for LLMs. Hence, in this study, we attempt to instantiate the efficacy of LLMs for the task of SRL via Question answering. Toward that goal, we investigate the effectiveness of five different LLMs (Llama, Mistral, Qwen, OpenChat, Gemini) using zero-shot and few-shot prompting. Our findings indicate that few-shot prompting enhances the performance of all models. Although Gemini outperformed others by a margin of 11%, Qwen and Llama are not too far behind. Additionally, we conduct a comprehensive error analysis to shed light on the cases where LLMs fail. This study offers valuable insights into the performance of LLMs for structured prediction and the effectiveness of simple prompting techniques in the Question-Answering framework for SRL.</abstract>
      <url hash="6443b1b0">2025.ijcnlp-srw.21</url>
      <bibkey>raghav-jana-2025-llms</bibkey>
    </paper>
    <paper id="22">
      <title>Could you <fixed-case>BE</fixed-case> more sarcastic? A Cognitive Approach to Bidirectional Sarcasm Understanding in Language Models</title>
      <author><first>Veer</first><last>Chheda</last><affiliation>Dwarkadas J. Sanghvi College Of Engineering</affiliation></author>
      <author><first>Avantika</first><last>Sankhe</last></author>
      <author><first>Atharva Vinay</first><last>Sankhe</last></author>
      <pages>259-276</pages>
      <abstract>Sarcasm is a specific form of ironic speech which can often be hard to understand for language models due to its nuanced nature. Recent improvements in the ability of such models to detect and generate sarcasm motivate us to try a new approach to help language models perceive sarcasm as a speech style, through a human cognitive perspective. In this work, we propose a multi-hop Chain of Thought (CoT) methodology to understand the context of an utterance that follows a dialogue and to perform bidirectional style transfer on that utterance, leveraging the Theory of Mind. We use small language models (SLMs) due to their cost-efficiency and fast response-time. The generated utterances are evaluated using both LLM-as-a-judge and human evaluation, suitable to the open-ended and stylistic nature of the generations. Along with these, we also evaluate scores of automated metrics such as DialogRPT, BLEU and SBERT; drawing valuable insights from them that support our evidence. Based on this, we find that our cognitive approach to sarcasm is an effective way for language models to stylistically understand and generate sarcasm with better authenticity.</abstract>
      <url hash="3e245b60">2025.ijcnlp-srw.22</url>
      <bibkey>chheda-etal-2025-sarcastic</bibkey>
    </paper>
    <paper id="23">
      <title>To What Extent Can In-Context Learning Solve Unseen Tasks?</title>
      <author><first>Ryoma</first><last>Shinto</last></author>
      <author id="masashi-takeshita" orcid="0000-0001-5853-3262"><first>Masashi</first><last>Takeshita</last><affiliation>Nagoya University</affiliation></author>
      <author id="rafal-rzepka" orcid="0000-0002-8274-0875"><first>Rafal</first><last>Rzepka</last><affiliation>Hokkaido University</affiliation></author>
      <author><first>Toshihiko</first><last>Itoh</last><affiliation>Hokkaido University, Tokyo Institute of Technology</affiliation></author>
      <pages>277-288</pages>
      <abstract>While Large Language Models (LLMs) are known for their In-Context Learning (ICL) capabilities, there is no consensus on the underlying mechanisms. A key point of debate is whether ICL allows models to adapt to unseen tasks without parameter updates—that is, whether they can extrapolate. In this study, we address this question by constructing an arithmetic dataset based on the bivariate linear function <tex-math>z=ax+by</tex-math> to train a model and quantitatively evaluate its interpolation and extrapolation abilities through ICL. Our results show that while extrapolation was not achieved within our experimental design, tasks that were partially learned could be solved. We also found that the model acquires internal representations that can distinguish unseen tasks, and that greater task diversity in the training dataset improves ICL capabilities.</abstract>
      <url hash="7e1cc29d">2025.ijcnlp-srw.23</url>
      <bibkey>shinto-etal-2025-extent</bibkey>
    </paper>
    <paper id="24">
      <title>Visualizing and Benchmarking <fixed-case>LLM</fixed-case> Factual Hallucination Tendencies via Internal State Analysis and Clustering</title>
      <author><first>Nathan</first><last>Mao</last><affiliation>The Harker School</affiliation></author>
      <author><first>Varun</first><last>Kaushik</last><affiliation>The Harker School</affiliation></author>
      <author><first>Shreya</first><last>Shivkumar</last></author>
      <author><first>Parham</first><last>Sharafoleslami</last></author>
      <author><first>Kevin</first><last>Zhu</last><affiliation>Algoverse AI Research</affiliation></author>
      <author><first>Sunishchal</first><last>Dev</last><affiliation>RAND Corporation and Algoverse Coding Academy LLC</affiliation></author>
      <pages>289-298</pages>
      <abstract>Large Language Models (LLMs) often hallucinate, generating nonsensical or false information that can be especially harmful in sensitive fields such as medicine or law. To study this phenomenon systematically, we introduce FalseCite, a curated dataset designed to capture and benchmark hallucinated responses induced by misleading or fabricated citations. Running GPT-4o-mini, Falcon-7B, and Mistral 7-B through FalseCite, we observed a noticeable increase in hallucination activity for false claims with deceptive citations, especially in GPT-4o-mini. Using the responses from FalseCite, we can also analyze the internal states of hallucinating models, visualizing and clustering the hidden state vectors. From this analysis, we noticed that the hidden state vectors, regardless of hallucination or non-hallucination, tend to trace out a distinct horn-like shape. Our work underscores FalseCite’s potential as a foundation for evaluating and mitigating hallucinations in future LLM research.</abstract>
      <url hash="7ecf6207">2025.ijcnlp-srw.24</url>
      <bibkey>mao-etal-2025-visualizing</bibkey>
    </paper>
    <paper id="25">
      <title>Mitigating Forgetting in Continual Learning with Selective Gradient Projection</title>
      <author><first>Anika</first><last>Singh</last><affiliation>Algoverse AI Research and Glenforest Secondary School</affiliation></author>
      <author id="david-martinez"><first>David</first><last>Martinez</last></author>
      <author><first>Aayush</first><last>Dhaulakhandi</last><affiliation>Algoverse AI Research</affiliation></author>
      <author><first>Varun</first><last>Chopade</last><affiliation>Old Scona Academic</affiliation></author>
      <author><first>Likhith</first><last>Malipati</last><affiliation>Algoverse AI Research</affiliation></author>
      <author><first>Vasu</first><last>Sharma</last><affiliation>Facebook</affiliation></author>
      <author><first>Kevin</first><last>Zhu</last><affiliation>Algoverse AI Research</affiliation></author>
      <author><first>Sunishchal</first><last>Dev</last><affiliation>RAND Corporation and Algoverse Coding Academy LLC</affiliation></author>
      <author id="ryan-lagasse" orcid="0009-0007-8821-4861"><first>Ryan</first><last>Lagasse</last><affiliation>Lockheed Martin Corp. and Algoverse AI Research</affiliation></author>
      <pages>299-313</pages>
      <abstract>As neural networks are increasingly deployed in dynamic environments, they face the challenge of catastrophic forgetting, the tendency to overwrite previously learned knowledge when adapting to new tasks, resulting in severe performance degradation on earlier tasks. We propose Selective Forgetting-Aware Optimization (SFAO), a dynamic method that regulates gradient directions via cosine similarity and per-layer gating, enabling controlled forgetting while balancing plasticity and stability. SFAO selectively projects, accepts, or discards updates using a tunable mechanism with efficient Monte Carlo approximation. Experiments on standard continual learning benchmarks show that SFAO achieves competitive accuracy with markedly lower memory cost, a 90% reduction, and improved forgetting on MNIST datasets, making it suitable for resource-constrained scenarios.</abstract>
      <url hash="bacfc426">2025.ijcnlp-srw.25</url>
      <bibkey>singh-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>V</fixed-case>ariant<fixed-case>B</fixed-case>ench: A Framework for Evaluating <fixed-case>LLM</fixed-case>s on Justifications for Genetic Variant Interpretation</title>
      <author><first>Humair</first><last>Basharat</last></author>
      <author><first>Simon</first><last>Plotkin</last></author>
      <author><first>Charlotte</first><last>Le</last><affiliation>Algoverse AI Research</affiliation></author>
      <author><first>Kevin</first><last>Zhu</last><affiliation>Algoverse AI Research</affiliation></author>
      <author><first>Michael</first><last>Pink</last></author>
      <author><first>Isabella</first><last>Alfaro</last></author>
      <pages>314-321</pages>
      <abstract>Accurate classification in high-stakes domains requires not only correct predictions but transparent, traceable reasoning. We instantiate this need in clinical genomics and present VariantBench, a reproducible benchmark and scoring harness that evaluates both the final American College of Medical Genetics and Genomics/Association for Molecular Pathology (ACMG/AMP) labels and criterion-level reasoning fidelity for missense single-nucleotide variants (SNVs). Each case pairs a variant with deterministic, machine-readable evidence aligned to five commonly used criteria (PM2, PP3, PS1, BS1, BA1), enabling consistent evaluation of large language models (LLMs). Unlike prior work that reports only final labels, our framework scores the correctness and faithfulness of per-criterion justifications against numeric evidence. On a balanced 100-variant freeze, Gemini 2.5 Flash and GPT-4o outperform Claude 3 Opus on label accuracy and criterion detection, and both improve materially when the decisive PS1 cue is provided explicitly. Error analyses show models master population-frequency cues yet underuse high-impact rules unless evidence is unambiguous. VariantBench delivers a substrate to track such improvements and compare prompting, calibration, and aggregation strategies in genomics and other rule-governed, safety-critical settings.</abstract>
      <url hash="3a6a3522">2025.ijcnlp-srw.26</url>
      <bibkey>basharat-etal-2025-variantbench</bibkey>
    </paper>
    <paper id="27">
      <title>The ‘aftermath’ of compounds: Investigating Compounds and their Semantic Representations</title>
      <author><first>Swarang</first><last>Joshi</last></author>
      <pages>322-328</pages>
      <abstract>This study investigated how well computational embeddings aligned with human semantic judgments in the processing of English compound words. We compared static word vectors (GloVe) and contextualized embeddings (BERT) against human ratings of lexeme meaning dominance (LMD) and semantic transparency (ST) drawn from a psycholinguistic dataset. Using measures of association strength (Edinburgh Associative Thesaurus), frequency (BNC), and predictability (LaDEC), we computed embedding-derived LMD and ST metrics and assessed their relationships with human judgments via Spearman’s correlation and regression analyses. Our results showed that BERT embeddings better captured compositional semantics than GloVe, and that predictability ratings were strong predictors of semantic transparency in both human and model data. These findings advanced computational psycholinguistics by clarifying the factors that drove compound word processing and offered insights into embedding-based semantic modeling.</abstract>
      <url hash="65b47b67">2025.ijcnlp-srw.27</url>
      <bibkey>joshi-2025-aftermath</bibkey>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2026-01-12" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: Tutorial Abstract</booktitle>
      <editor><first>Benjamin</first><last>Heinzerling</last></editor>
      <editor><first>Lun-Wei</first><last>Ku</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mumbai, India</address>
      <month>December</month>
      <year>2025</year>
      <url hash="e6193847">2025.ijcnlp-tutorials</url>
      <venue>ijcnlp</venue>
      <isbn>979-8-89176-302-9</isbn>
    </meta>
    <frontmatter>
      <url hash="a446a4a6">2025.ijcnlp-tutorials.0</url>
      <bibkey>ijcnlp-2025-tutorials</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Source Attribution for Large Language Models</title>
      <author><first>Vipula</first><last>Rawte</last></author>
      <author><first>Koustava</first><last>Goswami</last></author>
      <author><first>Puneet</first><last>Mathur</last></author>
      <author><first>Nedim</first><last>Lipka</last></author>
      <pages>1-5</pages>
      <abstract>As Large Language Models (LLMs) become more widely used for tasks like document summarization, question answering, and information extraction, improving their trustworthiness and interpretability has become increasingly important. One key strategy for achieving this is extbfattribution, a process that tracks the sources of the generated responses. This tutorial will explore various attribution techniques, including model-driven attribution, post-retrieval answering, and post-generation attribution. We will also discuss the challenges involved in implementing these approaches, and also look at the advanced topics such as model-based attribution for complex cases, table attribution, multimodal attribution, and multilingual attribution.</abstract>
      <url hash="731d1a48">2025.ijcnlp-tutorials.1</url>
      <bibkey>rawte-etal-2025-source</bibkey>
    </paper>
    <paper id="2">
      <title>Continual Learning in Large Language Models: Foundations to Frontiers</title>
      <author><first>P. K.</first><last>Srijith</last></author>
      <author><first>Shrey</first><last>Satapara</last></author>
      <author><first>Sarath</first><last>Chandar</last></author>
      <pages>6-17</pages>
      <abstract>Continual learning (CL) enables deep learning models to learn a sequence of tasks under resource constraint settings, without forgetting previously acquired knowledge. This is particularly useful for multilingual NLP for low-resource languages, where incremental data collection is common and the compute cost is crucial. This tutorial will introduce key CL methodologies and their applications in natural language processing (NLP), covering both foundational techniques and modern challenges posed by large language models (LLMs). This tutorial covers foundational CL strategies based on regularization, replay, and network architecture. We explore NLP-specific CL scenarios such as task-incremental, language-incremental, and joint task-language incremental setups, along with methodologies to address them. A major emphasize of the tutorial is on continual learning for large language models (LLMs), examining challenges in applying CL for LLMs and the benefits it can provide in LLM training and inference. We further explore the connection between several advances in LLM such as model merging and continual learning. This tutorial is suitable for NLP researchers, practitioners, and students interested in lifelong learning, multilingual NLP, or large language models. It is designed as a half-day tutorial at IJCNLP 2025 and fall under the category of Introduction to Non-CL/Non-NLP Topic.</abstract>
      <url hash="00c7a232">2025.ijcnlp-tutorials.2</url>
      <bibkey>srijith-etal-2025-continual</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>NLP</fixed-case> for Affective Science: Exploring Fundamental Questions on Emotions through Language and Computation</title>
      <author><first>Krishnapriya</first><last>Vishnubhotla</last></author>
      <author id="saif-mohammad"><first>Saif M.</first><last>Mohammad</last></author>
      <pages>18-19</pages>
      <abstract>Affect refers to the fundamental neural processes that generate and regulate emotions, moods, and feeling states. Affect and emotions are central to how we organize meaning, to our behaviour, to our health and well-being, and to our very survival. Despite this, and even though most of us are all intimately familiar with emotions in everyday life, there is much we do not know about how emotions work, and how they impact our lives. Affective Science is a broad interdisciplinary field that explores these and other related questions about affect and emotions.Since language is a powerful mechanism of emotion expression, there is great potential in using language data and computation to shed light on fundamental questions about emotions. However, even though much progress has been made in areas such as sentiment analysis and affective computing, much of the research focus is squarely on automatically classifying pieces of text. In this tutorial, we will present an introduction to Affective Science and argue that NLP is uniquely positioned to contribute to it: to boldly explore a new frontier — to use language and computation to ask fundamental questions about how emotions and affect work. We will cover the broad areas of research within this nascent field of study - Computational Affective Science (CAS).</abstract>
      <url hash="679842ad">2025.ijcnlp-tutorials.3</url>
      <bibkey>vishnubhotla-mohammad-2025-nlp</bibkey>
    </paper>
    <paper id="4">
      <title>Human–Agent Teaming for Higher-Order Thinking Augmentation</title>
      <author><first>Chung-Chi</first><last>Chen</last></author>
      <pages>20-24</pages>
      <abstract>Human-agent teaming refers to humans and artificial agents working together toward shared goals, and recent advances in artificial intelligence, including large language models and autonomous robots, have intensified interest in using these agents not only for automation but also to augment higher-order cognition. Higher-order thinking involves complex mental processes such as critical thinking, creative problem solving, abstract reasoning, and metacognition, and intelligent agents hold the potential to act as genuine teammates that complement human strengths and address cognitive limitations. This tutorial synthesizes emerging research on human-agent teaming for cognitive augmentation by outlining the foundations of higher-order thinking and the psychological frameworks that describe it, reviewing key concepts and interaction paradigms in human–AI collaboration, and examining applications across education, healthcare, military decision-making, scientific discovery, and creative industries, where systems such as language models, decision-support tools, multi-agent architectures, explainable AI, and hybrid human–AI methods are used to support complex reasoning and expert judgment. It also discusses the major challenges involved in achieving meaningful augmentation, including the calibration of trust, the need for transparency, the development of shared mental models, the role of human adaptability and training, and broader ethical concerns. The tutorial further identifies gaps such as limited evidence of long-term improvement in human cognitive abilities and insufficient co-adaptation between humans and agents. Finally, it outlines future directions involving real-time cognitive alignment, long-term studies of cognitive development, co-adaptive learning systems, ethics-aware AI teammates, and new benchmarks for evaluating collaborative cognition, offering a comprehensive overview of current progress and a roadmap for advancing human-agent teaming as a means of enhancing higher-order human thinking.</abstract>
      <url hash="45ccee9a">2025.ijcnlp-tutorials.4</url>
      <bibkey>chen-2025-human</bibkey>
    </paper>
    <paper id="5">
      <title>Beyond Guardrails: Advanced Safety for Large Language Models — Monolingual, Multilingual and Multimodal Frontiers</title>
      <author><first>Somnath</first><last>Banerjee</last></author>
      <author><first>Rima</first><last>Hazra</last></author>
      <author><first>Animesh</first><last>Mukherjee</last></author>
      <pages>25-33</pages>
      <abstract>LLMs are now embedded in workflows that span languages, modalities, and tools. This raises safety challenges that outpace conventional “guardrails”: jailbreaks and prompt injections, attributional safety failures under code-mixing, multimodal bypass via typography and icons, activation-level manipulation, and agentic risks from tool use. This tutorial synthesizes the newest advances (2023–2025) and lays out open research questions around (i) failure modes in monolingual / multilingual / multimodal settings, (ii) training-time and inference-time defenses (rejection SFT, RLHF/RLAIF, decoding-time safety, parameter/activation steering), and (iii) evaluation and red-teaming pipelines balancing safety and utility. We anchor the tutorial with recent results including our safety related papers published at top tier conferences, and connect them to emerging best practices from recent safety tutorials. The target audience is researchers/engineers with basic NLP knowledge who want the latest techniques and a research roadmap; format is half-day with short demos and Q&amp;A.</abstract>
      <url hash="1158a627">2025.ijcnlp-tutorials.5</url>
      <bibkey>banerjee-etal-2025-beyond</bibkey>
    </paper>
    <paper id="6">
      <title>Tutorial on Trustworthy Legal Text Processing with <fixed-case>LLM</fixed-case>s: Retrieval, Rhetorical Roles, Summarization, and Trustworthy Generation</title>
      <author id="anand-kumar-m"><first>Anand Kumar</first><last>M</last></author>
      <author><first>Sangeetha</first><last>S</last></author>
      <author><first>Manikandan</first><last>R</last></author>
      <author><first>Anjali</first><last>R</last></author>
      <pages>34-39</pages>
      <abstract>This half-day tutorial provides a comprehensive overview of Legal Natural Language Processing (NLP) with LLM for participants with a basic understanding of Computational Linguistics or NLP concepts. We introduce how NLP can help analyze and manage legal text by covering five key topics: legal text analysis with LLM insights, legal text retrieval, rhetorical role identification, legal text summarization, and addressing bias and hallucination in legal tasks. Our goals are to explain why these tasks matter for researchers in the legal domain, describe the challenges and open problems, and outline current solutions. This proposed tutorial blends lectures, live examples, and Q&amp;A to help researchers and students see how language technology and LLMs can make legal information more understandable and efficient.</abstract>
      <url hash="96b9e3a4">2025.ijcnlp-tutorials.6</url>
      <bibkey>m-etal-2025-tutorial</bibkey>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2026-01-12" type="proceedings">
    <meta>
      <booktitle>Proceedings of The 14th International Joint Conference on Natural Language Processing and The 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: System Demonstrations</booktitle>
      <editor><first>Xuebo</first><last>Liu</last></editor>
      <editor><first>Ayu</first><last>Purwarianti</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mumbai, India</address>
      <month>December</month>
      <year>2025</year>
      <url hash="2e2b3f5d">2025.ijcnlp-demo</url>
      <venue>ijcnlp</venue>
      <isbn>979-8-89176-301-2</isbn>
    </meta>
    <frontmatter>
      <url hash="1af5afa2">2025.ijcnlp-demo.0</url>
      <bibkey>ijcnlp-2025-demo</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>I</fixed-case>mage<fixed-case>T</fixed-case>ra: Real-Time Translation for Texts in Image and Video</title>
      <author id="hour-kaing" orcid="0000-0003-4389-2144"><first>Hour</first><last>Kaing</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author id="jiannan-mao" orcid="0009-0004-5787-1613"><first>Jiannan</first><last>Mao</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author id="haiyue-song" orcid="0000-0003-1159-0918"><first>Haiyue</first><last>Song</last><affiliation>National Institute of Information and Communications Technology (NICT)</affiliation></author>
      <author id="chenchen-ding" orcid="0000-0002-7523-208X"><first>Chenchen</first><last>Ding</last><affiliation>Nara Institute of Science and Technology and National Institute of Information and Communications Technology</affiliation></author>
      <author id="hideki-tanaka" orcid="0009-0001-4498-7017"><first>Hideki</first><last>Tanaka</last><affiliation>National Institute of Information and Communications Technology (NICT)</affiliation></author>
      <author><first>Masao</first><last>Utiyama</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>1-8</pages>
      <abstract>There has been a growing research interest in in-image machine translation, which involves translating texts in images from one language to another. Recent studies continue to explore pipeline-based systems due to its straightforward construction and the consistent improvement of its underlined components. However, the existing implementation for such pipeline often lack extensibility, composability, and support for real-time translation. Therefore, this work introduces <i>ImageTra</i>—an open-source toolkit designed to facilitate the development of the pipeline-based system of in-image machine translation. The toolkit integrates state-of-the-art open-source models and tools, and is designed with a focus on modularity and efficiency, making it particularly well-suited for real-time translation. The toolkit is released at <url>https://github.com/hour/imagetra</url>.</abstract>
      <url hash="bd45519b">2025.ijcnlp-demo.1</url>
      <bibkey>kaing-etal-2025-imagetra</bibkey>
    </paper>
    <paper id="2">
      <title>Human-in-the-Loop Generation of Adversarial Texts: A Case Study on <fixed-case>T</fixed-case>ibetan Script</title>
      <author id="xi-cao" orcid="0009-0003-4429-3327"><first>Xi</first><last>Cao</last></author>
      <author><first>Yuan</first><last>Sun</last><affiliation>Minzu University of China</affiliation></author>
      <author><first>Jiajun</first><last>Li</last></author>
      <author id="quzong-gesang" orcid="0009-0004-3638-6686"><first>Quzong</first><last>Gesang</last></author>
      <author><first>Nuo</first><last>Qun</last><affiliation>Xizang University</affiliation></author>
      <author><first>Nyima</first><last>Tashi</last><affiliation>Tibet University</affiliation></author>
      <pages>9-16</pages>
      <abstract>DNN-based language models excel across various NLP tasks but remain highly vulnerable to textual adversarial attacks. While adversarial text generation is crucial for NLP security, explainability, evaluation, and data augmentation, related work remains overwhelmingly English-centric, leaving the problem of constructing high-quality and sustainable adversarial robustness benchmarks for lower-resourced languages both difficult and understudied. First, method customization for lower-resourced languages is complicated due to linguistic differences and limited resources. Second, automated attacks are prone to generating invalid or ambiguous adversarial texts. Last but not least, language models continuously evolve and may be immune to parts of previously generated adversarial texts. To address these challenges, we introduce HITL-GAT, an interactive system based on a general approach to human-in-the-loop generation of adversarial texts. Additionally, we demonstrate the utility of HITL-GAT through a case study on Tibetan script, employing three customized adversarial text generation methods and establishing its first adversarial robustness benchmark, providing a valuable reference for other lower-resourced languages.</abstract>
      <url hash="e12d2b28">2025.ijcnlp-demo.2</url>
      <bibkey>cao-etal-2025-human</bibkey>
    </paper>
    <paper id="3">
      <title>Real-time Commentator Assistant for Photo Editing Live Streaming</title>
      <author id="matiss-rikters" orcid="0000-0002-3530-6873"><first>Matīss</first><last>Rikters</last><affiliation>National Institute of Advanced Industrial Science and Technology (AIST)</affiliation></author>
      <author id="goran-topic" orcid="0000-0002-4242-6881"><first>Goran</first><last>Topić</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>17-24</pages>
      <abstract>Live commentary has the potential of making specific broadcasts such as sports or video games more engaging and interesting to watch for spectators. With the recent popularity rise of online live streaming many new categories have entered the space, like art in its many forms or even software development, however, not all live streamers have the capability to be naturally engaging with the audience. We introduce a live commentator assistant system that can discuss what is visible on screen in real time. Our experimental setting is focused on the use-case of a photo editing live stream. We compare several recent vision language models for commentary generation and text to speech models for spoken output, all on relatively modest consumer hardware configurations.</abstract>
      <url hash="7b76e259">2025.ijcnlp-demo.3</url>
      <bibkey>rikters-topic-2025-real</bibkey>
    </paper>
    <paper id="4">
      <title>Supporting Plain Language Summarization of Psychological Meta-Analyses with Large Language Models</title>
      <author><first>Yarik</first><last>Menchaca Resendiz</last></author>
      <author><first>Martin</first><last>Kerwer</last></author>
      <author><first>Anita</first><last>Chasiotis</last></author>
      <author><first>Marlene</first><last>Bodemer</last></author>
      <author><first>Kai</first><last>Sassenberg</last></author>
      <author id="roman-klinger" orcid="0000-0002-2014-6619"><first>Roman</first><last>Klinger</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <pages>25-35</pages>
      <abstract>Communicating complex scientific findings to non-experts remains a major challenge in fields like psychology, where research is often presented in highly technical language. One effective way to improve accessibility, for non-experts, is through plain language summaries, which summarize key insights into simple and understandable terms. However, the limited number of institutions that produce lay summaries typically relies on psychology experts to create them manually – an approach that ensures high quality but requires significant expertise, time, and effort. In this paper, we introduce the KLARpsy App, a system designed to support psychology experts in creating plain language summaries of psychological meta-analyses using Large Language Models (LLM). Our system generates initial draft summaries based on a 37-criterion guideline developed to ensure clarity for non-experts. All summaries produced through the system are manually validated and edited by KLARpsy authors to ensure factual correctness and readability. We demonstrate how the system integrates LLM-generated content into an expert-in-the-loop workflow. The automatic evaluation showed a mean semantic-similarity score of 0.73 against expert-written summaries, and human evaluation on a 5-point Likert scale averaged above 3 (higher is better), indicate that the generated drafts are of high quality. The application and code are open source.</abstract>
      <url hash="5af1fd28">2025.ijcnlp-demo.4</url>
      <bibkey>menchaca-resendiz-etal-2025-supporting</bibkey>
    </paper>
    <paper id="5">
      <title>Standardizing the Measurement of Text Diversity: A Tool and Comparative Analysis</title>
      <author><first>Chantal</first><last>Shaib</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Venkata S</first><last>Govindarajan</last><affiliation>Ithaca College</affiliation></author>
      <author><first>Joe</first><last>Barrow</last><affiliation>Pattern Data</affiliation></author>
      <author><first>Jiuding</first><last>Sun</last><affiliation>Stanford University</affiliation></author>
      <author><first>Alexa</first><last>Siu</last><affiliation>Adobe</affiliation></author>
      <author><first>Byron C</first><last>Wallace</last><affiliation>Northeastern University, Brown University and Northeastern University</affiliation></author>
      <author><first>Ani</first><last>Nenkova</last><affiliation>Adobe Research</affiliation></author>
      <pages>36-46</pages>
      <abstract>The diversity across outputs generated by LLMs shapes perception of their quality and utility. High lexical diversity is often desirable, but there is no standard method to measure this property. Templated answer structures and “canned” responses across different documents are readily noticeable, but difficult to visualize across large corpora. This work aims to standardize measurement of text diversity. Specifically, we empirically investigate the convergent validity of existing scores across English texts, and release diversity, an open-source Python package (https://pypi.org/project/diversity/, https://github.com/cshaib/diversity) for measuring and extracting repetition in text. We also build a platform (https://ai-templates.app) based on diversity for users to interactively explore repetition in text. We find that fast compression algorithms capture information similar to what is measured by slow-to-compute <tex-math>n</tex-math>-gram overlap homogeneity scores. Further, a combination of measures—compression ratios, self-repetition of long <tex-math>n</tex-math>-grams, and Self-BLEU—are sufficient to report, as they have low mutual correlation with each other.</abstract>
      <url hash="39cefb50">2025.ijcnlp-demo.5</url>
      <bibkey>shaib-etal-2025-standardizing</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>LITMUS</fixed-case>++ : An Agentic System for Predictive Analysis of Low-Resource Languages Across Tasks and Models</title>
      <author><first>Avni</first><last>Mittal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Shanu</first><last>Kumar</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author id="sandipan-dandapat"><first>Sandipan</first><last>Dandapat</last><affiliation>IIT Hyderabad</affiliation></author>
      <author id="monojit-choudhury" orcid="0000-0001-7473-7839"><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>47-54</pages>
      <abstract>We present LITMUS++, an agentic system for predicting language-model performance for queries of the form “How will a Model perform on a Task in a Language?”, a persistent challenge in multilingual and low-resource settings, settings where benchmarks are incomplete or unavailable. Unlike static evaluation suites or opaque LLM-as-judge pipelines, LITMUS++ implements an agentic, auditable workflow: a Directed Acyclic Graph of specialized Thought Agents that generate hypotheses, retrieve multilingual evidence, select predictive features, and train lightweight regressors with calibrated uncertainty. The system supports interactive querying through a chat-style interface, enabling users to inspect reasoning traces and cited evidence. Experiments across six tasks and five multilingual scenarios show that LITMUS++ delivers accurate and interpretable performance predictions, including in low-resource and unseen conditions. Code is available at https://github.com/AvniMittal13/litmus_plus_plus.</abstract>
      <url hash="6bde0f20">2025.ijcnlp-demo.6</url>
      <bibkey>mittal-etal-2025-litmus</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>S</fixed-case>im<fixed-case>A</fixed-case>gents: Bridging Literature and the Universe Via A Multi-Agent Large Language Model System</title>
      <author id="xiaowen-zhang" orcid="0009-0008-0370-6021"><first>Xiaowen</first><last>Zhang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zhenyu</first><last>Bi</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author id="patrick-lachance" orcid="0009-0006-7511-0329"><first>Patrick</first><last>LaChance</last></author>
      <author id="xuan-wang" orcid="0000-0002-1381-8958"><first>Xuan</first><last>Wang</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Tiziana</first><last>Di Matteo</last><affiliation>Carnegie-Mellon University</affiliation></author>
      <author><first>Rupert</first><last>Croft</last></author>
      <pages>55-66</pages>
      <abstract>As cosmological simulations and their associated software become increasingly complex, physicists face the challenge of searching through vast amounts of literature and user manuals to extract simulation parameters from dense academic papers, each using different models, and formats. Translating these parameters into executable scripts remains a time-consuming and error-prone process. To improve efficiency in physics research and accelerate the cosmological simulation process, we introduce SimAgents, a multi-agent system designed to automate both parameter configuration from the literature and preliminary analysis for cosmology research. SimAgents is powered by specialized LLM agents capable of physics reasoning, simulation software validation, and tool execution. These agents collaborate through structured communication, ensuring that extracted parameters are physically meaningful, internally consistent, and software-compliant. We also construct a cosmological parameter extraction evaluation dataset by collecting over 40 simulations in published papers from Arxiv and leading journals that cover diverse simulation types. Experiments on the dataset demonstrate a strong performance of SimAgents, highlighting its effectiveness and potential to accelerate scientific research for physicists. Our demonstration video is available at: https://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly available at https://github.com/xwzhang98/SimAgents.</abstract>
      <url hash="2d2d09a6">2025.ijcnlp-demo.7</url>
      <bibkey>zhang-etal-2025-simagents</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>S</fixed-case>tance<fixed-case>M</fixed-case>ining: An open-source stance detection library supporting time-series and visualization</title>
      <author id="benjamin-steel" orcid="0009-0006-3845-1394"><first>Benjamin</first><last>Steel</last><affiliation>McGill University, McGill University</affiliation></author>
      <author><first>Derek</first><last>Ruths</last><affiliation>McGill University</affiliation></author>
      <pages>67-76</pages>
      <abstract>Despite the size of the field, stance detection has remained inaccessible to most researchers due to implementation barriers. Here we present a library that allows easy access to an end-to-end stance modelling solution. This library comes complete with everything needed to go from a corpus of documents, to exploring stance trends in a corpus through an interactive dashboard. To support this, we provide stance target extraction, stance detection, stance time-series trend inference, and an exploratory dashboard, all available in an easy-to-use library. We hope that this library can increase the accessibility of stance detection for the wider community of those who could benefit from this method.</abstract>
      <url hash="4e2a317c">2025.ijcnlp-demo.8</url>
      <bibkey>steel-ruths-2025-stancemining</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>S</fixed-case>hort<fixed-case>C</fixed-case>heck: Checkworthiness Detection of Multilingual <fixed-case>S</fixed-case>hort‐<fixed-case>F</fixed-case>orm Videos</title>
      <author><first>Henrik</first><last>Vatndal</last></author>
      <author id="vinay-setty" orcid="0000-0002-9777-6758"><first>Vinay</first><last>Setty</last><affiliation>Factiverse and University of Stavanger</affiliation></author>
      <pages>77-85</pages>
      <abstract>Short-form video platforms like TikTok present unique challenges for misinformation detection due to their multimodal, dynamic, and noisy content. We present ShortCheck, a modular, inference-only pipeline with a user-friendly pipeline that automatically identifies checkworthy short-form videos to help human fact-checkers. The system integrates speech transcription, OCR, object and deepfake detection, video-to-text summarization, and claim verification. ShortCheck is validated by evaluating it on two manually annotated datasets with TikTok videos in a multilingual setting. The pipeline achieves promising results with F1-weighted score over 70%. The demo can be accessed live at <url>http://shortcheck.factiverse.ai</url>.</abstract>
      <url hash="cded4aba">2025.ijcnlp-demo.9</url>
      <bibkey>vatndal-setty-2025-shortcheck</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>C</fixed-case>hart<fixed-case>E</fixed-case>val: <fixed-case>LLM</fixed-case>-Driven Chart Generation Evaluation Using Scene Graph Parsing</title>
      <author><first>Kanika</first><last>Goswami</last><affiliation>Indira Gandhi Delhi Technical University for Women</affiliation></author>
      <author><first>Puneet</first><last>Mathur</last><affiliation>Adobe Systems</affiliation></author>
      <author id="ryan-a-rossi" orcid="0000-0001-9758-0635"><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author id="franck-dernoncourt" orcid="0000-0002-1119-1346"><first>Franck</first><last>Dernoncourt</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <author id="dinesh-manocha" orcid="0000-0001-7047-9801"><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>86-93</pages>
      <abstract>Accurate assessment of generated chart quality is crucial for automated document creation and editing across diverse applications like finance, medicine, policy making, and education. Current evaluation approaches suffer from significant limitations: human evaluation is costly and difficult to scale, pixel-based metrics ignore data accuracy, while data-centric measures overlook design quality. Recent multimodal LLM evaluators show promise but exhibit concerning inconsistencies due to prompt sensitivity and subjective biases. Existing metrics fail to evaluate chart quality holistically across visual similarity, semantic alignment, and data fidelity, often producing misleading scores that unfairly penalize good charts while rewarding bad ones. We introduce ChartEval, a novel chart evaluation system that compares generated chart images with ground truth by leveraging scene graph parsing to decompose chart images into hierarchical scene graphs of chart objects, attributes, and relations. Subsequently, it applies graph-based similarity measures to compare candidate chart scene graphs against reference scene graphs for measuring chart quality. We demonstrate that our evaluation approach achieves significantly stronger correlation with human judgments compared to existing metrics like GPT-Score, SSIM, and SCRM using a comprehensive benchmark of 4K chart images paired with generation intents and human quality ratings. We demonstrate the utility of the ChartEval system as a reliable automatic chart quality metric on diverse tasks, including language-guided chart editing, chart reconstruction, and text-to-chart synthesis using both open-source and API-based LLMs.</abstract>
      <url hash="1325a3e3">2025.ijcnlp-demo.10</url>
      <bibkey>goswami-etal-2025-charteval</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>SPORTSQL</fixed-case>: An Interactive System for Real-Time Sports Reasoning and Visualization</title>
      <author><first>Sebastian</first><last>Martinez</last></author>
      <author><first>Naman</first><last>Ahuja</last></author>
      <author><first>Fenil</first><last>Bardoliya</last></author>
      <author><first>Suparno Roy</first><last>Chowdhury</last></author>
      <author><first>Chris</first><last>Bryan</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>94-101</pages>
      <abstract>We present a modular, interactive system, SPORTSQL, for natural language querying and visualization of dynamic sports data, with a focus on the English Premier League (EPL). The system translates user questions into executable SQL over a live, temporally indexeddatabase constructed from real-time Fantasy Premier League (FPL) data. It supports both tabular and visual outputs, leveraging symbolic reasoning capabilities of Large Language Models (LLMs) for query parsing, schema linking, and visualization selection. To evaluate system performance, we introduce the Dynamic Sport Question Answering Benchmark (DSQABENCH), comprising 1,700+ queries annotated with SQL programs, gold answers, and database snapshots. Our demo highlights how non-expert users can seamlessly explore evolving sports statistics through a natural, conversational interface.</abstract>
      <url hash="4b61ad0d">2025.ijcnlp-demo.11</url>
      <bibkey>martinez-etal-2025-sportsql</bibkey>
    </paper>
  </volume>
  <event id="ijcnlp-2025">
    <meta>
      <title>The 14th International Joint Conference on Natural Language Processing &amp; The 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics</title>
      <location>Mumbai, India</location>
      <dates>December 20–24, 2025</dates>
    </meta>
    <links>
      <url type="website">https://2025.aaclnet.org</url>
    </links>
    <colocated>
      <volume-id>2025.findings-ijcnlp</volume-id>
      <volume-id>2025.banglalp-1</volume-id>
      <volume-id>2025.bhasha-1</volume-id>
      <volume-id>2025.chomps-main</volume-id>
      <volume-id>2025.eval4nlp-1</volume-id>
      <volume-id>2025.justnlp-main</volume-id>
      <volume-id>2025.nlpai4health-main</volume-id>
      <volume-id>2025.quantumnlp-1</volume-id>
      <volume-id>2025.sciprodllm-1</volume-id>
      <volume-id>2025.wasp-main</volume-id>
      <volume-id>2025.wat-1</volume-id>
      <volume-id>2025.wslp-main</volume-id>
    </colocated>
  </event>
</collection>
