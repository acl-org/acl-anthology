<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.mmnlg">
  <volume id="1" ingest-date="2023-09-04" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop on Multimodal, Multilingual Natural Language Generation and Multilingual WebNLG Challenge (MM-NLG 2023)</booktitle>
      <editor><first>Albert</first><last>Gatt</last></editor>
      <editor><first>Claire</first><last>Gardent</last></editor>
      <editor><first>Liam</first><last>Cripwell</last></editor>
      <editor><first>Anya</first><last>Belz</last></editor>
      <editor><first>Claudia</first><last>Borg</last></editor>
      <editor><first>Aykut</first><last>Erdem</last></editor>
      <editor><first>Erkut</first><last>Erdem</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Prague, Czech Republic</address>
      <month>September</month>
      <year>2023</year>
      <venue>mmnlg</venue>
      <venue>ws</venue>
    </meta>
    <paper id="1">
      <title>Confidently Wrong: Exploring the Calibration and Expression of (Un)Certainty of Large Language Models in a Multilingual Setting</title>
      <author><first>Lea</first><last>Krause</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Wondimagegnhue</first><last>Tufa</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Selene</first><last>Baez Santamaria</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Angel</first><last>Daza</last><affiliation>Vrije Universiteit Amsterdam - Computational Linguistics &amp; Text Mining Lab</affiliation></author>
      <author><first>Urja</first><last>Khurana</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Piek</first><last>Vossen</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <pages>1-9</pages>
      <abstract>While the fluency and coherence of Large Language Models (LLMs) in text generation have seen significant improvements, their competency in generating appropriate expressions of uncertainty remains limited.Using a multilingual closed-book QA task and GPT-3.5, we explore how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings. Our results reveal strong performance in high-resource languages but a marked decline in performance in lower-resource languages. Across all, we observe an exaggerated expression of confidence in the model, which does not align with the correctness or likelihood of its responses. Our findings highlight the need for further research into accurate calibration of LLMs especially in a multilingual setting.</abstract>
      <url hash="ed8460ac">2023.mmnlg-1.1</url>
      <bibkey>krause-etal-2023-confidently</bibkey>
    </paper>
    <paper id="2">
      <title>Visual Question Generation in <fixed-case>B</fixed-case>engali</title>
      <author><first>Mahmud</first><last>Hasan</last><affiliation>North South University</affiliation></author>
      <author><first>Labiba</first><last>Islam</last><affiliation>North South University</affiliation></author>
      <author><first>Jannatul</first><last>Ruma</last><affiliation>North South University</affiliation></author>
      <author><first>Tasmiah</first><last>Mayeesha</last><affiliation>North South University</affiliation></author>
      <author><first>Rashedur</first><last>Rahman</last><affiliation>North South University</affiliation></author>
      <pages>10-19</pages>
      <abstract>The task of Visual Question Generation (VQG) is to generate human-like questions relevant to the given image. As VQG is an emerging research field, existing works tend to focus only on resource-rich language such as English due to the availability of datasets. In this paper, we propose the first Bengali Visual Question Generation task and develop a novel transformer-based encoder-decoder architecture that generates questions in Bengali when given an image. We propose multiple variants of models - (i) image-only: baseline model of generating questions from images without additional information, (ii) image-category and image-answer-category: guided VQG where we condition the model to generate questions based on the answer and the category of expected question. These models are trained and evaluated on the translated VQAv2.0 dataset. Our quantitative and qualitative results establish the first state of the art models for VQG task in Bengali and demonstrate that our models are capable of generating grammatically correct and relevant questions. Our quantitative results show that our image-cat model achieves a BLUE-1 score of 33.12 and BLEU-3 score of 7.56 which is the highest of the other two variants. We also perform a human evaluation to assess the quality of the generation tasks. Human evaluation suggests that image-cat model is capable of generating goal-driven and attribute-specific questions and also stays relevant to the corresponding image.</abstract>
      <url hash="6de9f157">2023.mmnlg-1.2</url>
      <bibkey>hasan-etal-2023-visual</bibkey>
    </paper>
    <paper id="3">
      <title>Keeping an Eye on Context: Attention Allocation over Input Partitions in Referring Expression Generation</title>
      <author><first>Simeon</first><last>Schüz</last><affiliation>Bielefeld University</affiliation></author>
      <author><first>Sina</first><last>Zarrieß</last><affiliation>University of Bielefeld</affiliation></author>
      <pages>20-27</pages>
      <abstract>In Referring Expression Generation, model inputs are often composed of different representations, including the visual properties of the intended referent, its relative position and size, and the visual context. Yet, the extent to which this information influences the generation process of black-box neural models is largely unclear. We investigate the relative weighting of target, location, and context information in the attention components of a Transformer-based generation model. Our results show a general target bias, which, however, depends on the content of the generated expressions, pointing to interesting directions for future research.</abstract>
      <url hash="3ee65552">2023.mmnlg-1.3</url>
      <bibkey>schuz-zarriess-2023-keeping</bibkey>
    </paper>
    <paper id="4">
      <title>Are Language-and-Vision Transformers Sensitive to Discourse? A Case Study of <fixed-case>V</fixed-case>i<fixed-case>LBERT</fixed-case></title>
      <author><first>Ekaterina</first><last>Voloshina</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Nikolai</first><last>Ilinykh</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Simon</first><last>Dobnik</last><affiliation>University of Gothenburg</affiliation></author>
      <pages>28-38</pages>
      <abstract>Language-and-vision models have shown good performance in tasks such as image-caption matching and caption generation. However, it is challenging for such models to generate pragmatically correct captions, which adequately reflect what is happening in one image or several images. It is crucial to evaluate this behaviour to understand underlying reasons behind it. Here we explore to what extent contextual language-and-vision models are sensitive to different discourse, both textual and visual. In particular, we employ one of the multi-modal transformers (ViLBERT) and test if it can match descriptions and images, differentiating them from distractors of different degree of similarity that are sampled from different visual and textual contexts. We place our evaluation in the multi-sentence and multi-image setup, where images and sentences are expected to form a single narrative structure. We show that the model can distinguish different situations but it is not sensitive to differences within one narrative structure. We also show that performance depends on the task itself, for example, what modality remains unchanged in non-matching pairs or how similar non-matching pairs are to original pairs.</abstract>
      <url hash="00a2d1d5">2023.mmnlg-1.4</url>
      <bibkey>voloshina-etal-2023-language</bibkey>
    </paper>
    <paper id="5">
      <title>Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs</title>
      <author><first>Agnes</first><last>Axelsson</last><affiliation>KTH Royal Institute of Technology</affiliation></author>
      <author><first>Gabriel</first><last>Skantze</last><affiliation>KTH Speech Music and Hearing</affiliation></author>
      <pages>39-54</pages>
      <abstract>In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans. Recent work has shown that models that make use of pretraining on large amounts of text data can perform well on the KG-to-text task, even with relatively little training data on the specific graph-to-text task. In this paper, we build on this concept by using large language models to perform zero-shot generation based on nothing but the model’s understanding of the triple structure from what it can read. We show that ChatGPT achieves near state-of-the-art performance on some measures of the WebNLG 2020 challenge, but falls behind on others. Additionally, we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows about the data it is parsing and the quality of the output text.</abstract>
      <url hash="815aa1af">2023.mmnlg-1.5</url>
      <bibkey>axelsson-skantze-2023-using</bibkey>
    </paper>
    <paper id="6">
      <title>The 2023 <fixed-case>W</fixed-case>eb<fixed-case>NLG</fixed-case> Shared Task on Low Resource Languages. Overview and Evaluation Results (<fixed-case>W</fixed-case>eb<fixed-case>NLG</fixed-case> 2023)</title>
      <author><first>Liam</first><last>Cripwell</last><affiliation>CNRS/LORIA and Université de Lorraine</affiliation></author>
      <author><first>Anya</first><last>Belz</last><affiliation>ADAPT Research Centre, Dublin City University</affiliation></author>
      <author><first>Claire</first><last>Gardent</last><affiliation>CNRS/LORIA</affiliation></author>
      <author><first>Albert</first><last>Gatt</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Claudia</first><last>Borg</last><affiliation>University of Malta</affiliation></author>
      <author><first>Marthese</first><last>Borg</last><affiliation>University of Malta</affiliation></author>
      <author><first>John</first><last>Judge</last><affiliation>ADAPT Centre</affiliation></author>
      <author><first>Michela</first><last>Lorandi</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Anna</first><last>Nikiforovskaya</last><affiliation>CNRS/LORIA and Université de Lorraine</affiliation></author>
      <author><first>William</first><last>Soto Martinez</last><affiliation>Loria</affiliation></author>
      <pages>55-66</pages>
      <abstract>The WebNLG task consists of mapping a knowledge graph to a text verbalising the con- tent of that graph. The 2017 WebNLG edi- tion required participating systems to gener- ate English text from a set of DBpedia triples, while the 2020 WebNLG+ challenge addition- ally included generation into Russian and se- mantic parsing of English and Russian texts. In contrast, WebNLG 2023 focuses on four under-resourced languages which are severely under-represented in research on text genera- tion, namely Breton, Irish, Maltese and Welsh. In addition, WebNLG 2023 once again includes Russian. In this paper, we present the organi- sation of the shared task (data, timeline, eval- uation), briefly describe the participating sys- tems and summarise results for participating systems.</abstract>
      <url hash="808dce99">2023.mmnlg-1.6</url>
      <bibkey>cripwell-etal-2023-2023</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>W</fixed-case>eb<fixed-case>NLG</fixed-case>-Interno: Utilizing <fixed-case>FRED</fixed-case>-T5 to address the <fixed-case>RDF</fixed-case>-to-text problem (<fixed-case>W</fixed-case>eb<fixed-case>NLG</fixed-case> 2023)</title>
      <author><first>Maxim</first><last>Kazakov</last><affiliation>Petal Cloud Technology Co.,Ltd; HSE University</affiliation></author>
      <author><first>Julia</first><last>Preobrazhenskaya</last><affiliation>Lobachevsky State University</affiliation></author>
      <author><first>Ivan</first><last>Bulychev</last><affiliation>Lobachevsky State University</affiliation></author>
      <author><first>Aleksandr</first><last>Shain</last><affiliation>Petal Cloud Technology Co.,Ltd; Pulkovo Observatory</affiliation></author>
      <pages>67-72</pages>
      <abstract>We present our solution for the Russian RDF002 to-text generation task of the WebNLG Challenge 2023. We use the pretrained large language model named FRED-T5 (Zmitrovich et al., 2023) to finetune on the train dataset. Also, we propose several types of prompt and run experiments to analyze their effectiveness. Our submission achieves 0.373 TER on the test dataset, taking the first place according to the results of the automatic evaluation and outperforming the best result of the previous challenge by 0.025. The code of our solution is available at the following link: https://github.com/Ivan30003/webnlg_interno</abstract>
      <url hash="0b590f70">2023.mmnlg-1.7</url>
      <bibkey>kazakov-etal-2023-webnlg</bibkey>
    </paper>
    <paper id="8">
      <title>Better Translation + Split and Generate for Multilingual <fixed-case>RDF</fixed-case>-to-Text (<fixed-case>W</fixed-case>eb<fixed-case>NLG</fixed-case> 2023)</title>
      <author><first>Nalin</first><last>Kumar</last><affiliation>Charles University in Prague</affiliation></author>
      <author><first>Saad</first><last>Obaid Ul Islam</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Ondrej</first><last>Dusek</last><affiliation>Charles University</affiliation></author>
      <pages>73-79</pages>
      <abstract>This paper presents system descriptions of our submitted outputs for WebNLG Challenge 2023. We use mT5 in multi-task and multilingual settings to generate more fluent and reliable verbalizations of the given RDF triples. Furthermore, we introduce a partial decoding technique to produce more elaborate yet simplified outputs. Additionally, we demonstrate the significance of employing better translation systems in creating training data.</abstract>
      <url hash="ff907e21">2023.mmnlg-1.8</url>
      <bibkey>kumar-etal-2023-better</bibkey>
    </paper>
    <paper id="9">
      <title>Data-to-text Generation for Severely Under-Resourced Languages with <fixed-case>GPT</fixed-case>-3.5: A Bit of Help Needed from <fixed-case>G</fixed-case>oogle <fixed-case>T</fixed-case>ranslate (<fixed-case>W</fixed-case>eb<fixed-case>NLG</fixed-case> 2023)</title>
      <author><first>Michela</first><last>Lorandi</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Anya</first><last>Belz</last><affiliation>ADAPT Research Centre, Dublin City University</affiliation></author>
      <pages>80-86</pages>
      <abstract>LLMs are great at tasks involving English which dominates in their training data. We explore their ability to address tasks involving languages that are severely under-represented in their training data. More specifically, we do this in the context of data-to-text generation for Irish, Maltese, Welsh and Breton. During the prompt-engineering phase we tested GPT-3.5 and~4 with a range of prompt types and formats on a small sample of example input/output pairs. We then fully evaluated the two most promising prompts in two scenarios: (i) direct generation into the under-resourced languages, and (ii) generation into English followed by translation into the under-resourced languages. We find that few-shot prompting works better for direct generation into under-resourced languages, but that the difference disappears when pivoting via English. The few-shot + translation system variants were submitted to the WebNLG 2023 shared task where they outperformed all other systems by substantial margins in all languages on all automatic metrics. We conclude that good performance can be achieved with state-of-the-art LLMs out-of-the box for under-resourced languages. However, best results (for Welsh) of BLEU 25.12, ChrF++ 0.55, and TER 0.64 are well below the lowest ranked English system at WebNLG’20 with BLEU 0.391, ChrF++ 0.579, and TER 0.564.</abstract>
      <url hash="fd39696d">2023.mmnlg-1.9</url>
      <bibkey>lorandi-belz-2023-data</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>DCU</fixed-case>/<fixed-case>TCD</fixed-case>-<fixed-case>FORG</fixed-case>e at <fixed-case>W</fixed-case>eb<fixed-case>NLG</fixed-case>’23: <fixed-case>I</fixed-case>rish rules! (<fixed-case>W</fixed-case>eg<fixed-case>NLG</fixed-case> 2023)</title>
      <author><first>Simon</first><last>Mille</last><affiliation>ADAPT Research Centre, Dublin City University</affiliation></author>
      <author><first>Elaine</first><last>Uí Dhonnchadha</last><affiliation>Trinity College Dublin</affiliation></author>
      <author><first>Stamatia</first><last>Dasiopoulou</last><affiliation>NTT DATA</affiliation></author>
      <author><first>Lauren</first><last>Cassidy</last><affiliation>ADAPT Research Centre, Dublin City University</affiliation></author>
      <author><first>Brian</first><last>Davis</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Anya</first><last>Belz</last><affiliation>ADAPT Research Centre, Dublin City University</affiliation></author>
      <pages>87-92</pages>
      <abstract>In this paper, we describe the submission of Dublin City University (DCU) and Trinity College Dublin (TCD) for the WebNLG 2023 shared task. We present a fully rule-based pipeline for generating Irish texts from DBpedia triple sets which comprises 4 components: triple lexicalisation, generation of noninflected Irish text, inflection generation, and post-processing.</abstract>
      <url hash="50975aa2">2023.mmnlg-1.10</url>
      <bibkey>mille-etal-2023-dcu</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>W</fixed-case>eb<fixed-case>NLG</fixed-case> Challenge 2023: Domain Adaptive Machine Translation for Low-Resource Multilingual <fixed-case>RDF</fixed-case>-to-Text Generation (<fixed-case>W</fixed-case>eb<fixed-case>NLG</fixed-case> 2023)</title>
      <author><first>Kancharla</first><last>Aditya Hari</last><affiliation>IIIT Hyderabad</affiliation></author>
      <author><first>Bhavyajeet</first><last>Singh</last><affiliation>IIIT Hyderabad</affiliation></author>
      <author><first>Anubhav</first><last>Sharma</last><affiliation>IIIT Hyderabad</affiliation></author>
      <author><first>Vasudeva</first><last>Varma</last><affiliation>IIIT Hyderabad</affiliation></author>
      <pages>93-94</pages>
      <abstract>This paper presents our submission to the WebNLG Challenge 2023 for generating text in several low-resource languages from RDF-triples. Our submission focuses on using machine translation for generating texts in Irish, Maltese, Welsh and Russian. While a simple and straightfoward approach, recent works have shown that using monolingual models for inference for multilingual tasks with the help of machine translation (translate-test) can out-perform multilingual models and training multilingual models on machine-translated data (translate-train) through careful tuning of the MT component. Our results show that this approach demonstrates competitive performance for this task even with limited data.</abstract>
      <url hash="f84436bb">2023.mmnlg-1.11</url>
      <bibkey>aditya-hari-etal-2023-webnlg</bibkey>
    </paper>
  </volume>
</collection>
