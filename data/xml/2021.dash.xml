<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.dash">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances</booktitle>
      <editor><first>Eduard</first><last>Dragut</last></editor>
      <editor><first>Yunyao</first><last>Li</last></editor>
      <editor><first>Lucian</first><last>Popa</last></editor>
      <editor><first>Slobodan</first><last>Vucetic</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.dash-1</url>
      <venue>dash</venue>
    </meta>
    <frontmatter>
      <url hash="0f20b02d">2021.dash-1.0</url>
      <bibkey>dash-2021-data</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Leveraging <fixed-case>W</fixed-case>ikipedia Navigational Templates for Curating Domain-Specific Fuzzy Conceptual Bases</title>
      <author><first>Krati</first><last>Saxena</last></author>
      <author><first>Tushita</first><last>Singh</last></author>
      <author><first>Ashwini</first><last>Patil</last></author>
      <author><first>Sagar</first><last>Sunkle</last></author>
      <author><first>Vinay</first><last>Kulkarni</last></author>
      <pages>1–7</pages>
      <abstract>Domain-specific conceptual bases use key concepts to capture domain scope and relevant information. Conceptual bases serve as a foundation for various downstream tasks, including ontology construction, information mapping, and analysis. However, building conceptual bases necessitates domain awareness and takes time. Wikipedia navigational templates offer multiple articles on the same/similar domain. It is possible to use the templates to recognize fundamental concepts that shape the domain. Earlier work in this domain used Wikipedia’s structured and unstructured data to construct open-domain ontologies, domain terminologies, and knowledge bases. We present a novel method for leveraging navigational templates to create domain-specific fuzzy conceptual bases in this work. Our system generates knowledge graphs from the articles mentioned in the template, which we then process using Wikidata and machine learning algorithms. We filter important concepts using fuzzy logic on network metrics to create a crude conceptual base. Finally, the expert helps by refining the conceptual base. We demonstrate our system using an example of RNA virus antiviral drugs.</abstract>
      <url hash="e0b5c3c3">2021.dash-1.1</url>
      <doi>10.18653/v1/2021.dash-1.1</doi>
      <bibkey>saxena-etal-2021-leveraging</bibkey>
    </paper>
    <paper id="2">
      <title>It is better to Verify: Semi-Supervised Learning with a human in the loop for large-scale <fixed-case>NLU</fixed-case> models</title>
      <author><first>Verena</first><last>Weber</last></author>
      <author><first>Enrico</first><last>Piovano</last></author>
      <author><first>Melanie</first><last>Bradford</last></author>
      <pages>8–15</pages>
      <abstract>When a NLU model is updated, new utter- ances must be annotated to be included for training. However, manual annotation is very costly. We evaluate a semi-supervised learning workflow with a human in the loop in a produc- tion environment. The previous NLU model predicts the annotation of the new utterances, a human then reviews the predicted annotation. Only when the NLU prediction is assessed as incorrect the utterance is sent for human anno- tation. Experimental results show that the pro- posed workflow boosts the performance of the NLU model while significantly reducing the annotation volume. Specifically, in our setup, we see improvements of up to 14.16% for a recall-based metric and up to 9.57% for a F1- score based metric, while reducing the annota- tion volume by 97% and overall cost by 60% for each iteration.</abstract>
      <url hash="648eb44a">2021.dash-1.2</url>
      <doi>10.18653/v1/2021.dash-1.2</doi>
      <bibkey>weber-etal-2021-better</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>V</fixed-case>izi<fixed-case>T</fixed-case>ex: Interactive Visual Sense-Making of Text Corpora</title>
      <author><first>Natraj</first><last>Raman</last></author>
      <author><first>Sameena</first><last>Shah</last></author>
      <author><first>Tucker</first><last>Balch</last></author>
      <author><first>Manuela</first><last>Veloso</last></author>
      <pages>16–23</pages>
      <abstract>Information visualization is critical to analytical reasoning and knowledge discovery. We present an interactive studio that integrates perceptive visualization techniques with powerful text analytics algorithms to assist humans in sense-making of large complex text corpora. The novel visual representations introduced here encode the features delivered by modern text mining models using advanced metaphors such as hypergraphs, nested topologies and tessellated planes. They enhance human-computer interaction experience for various tasks such as summarization, exploration, organization and labeling of documents. We demonstrate the ability of the visuals to surface the structure, relations and concepts from documents across different domains.</abstract>
      <url hash="0f1aede9">2021.dash-1.3</url>
      <doi>10.18653/v1/2021.dash-1.3</doi>
      <bibkey>raman-etal-2021-vizitex</bibkey>
    </paper>
    <paper id="4">
      <title>A Visualization Approach for Rapid Labeling of Clinical Notes for Smoking Status Extraction</title>
      <author><first>Saman</first><last>Enayati</last></author>
      <author><first>Ziyu</first><last>Yang</last></author>
      <author><first>Benjamin</first><last>Lu</last></author>
      <author><first>Slobodan</first><last>Vucetic</last></author>
      <pages>24–30</pages>
      <abstract>Labeling is typically the most human-intensive step during the development of supervised learning models. In this paper, we propose a simple and easy-to-implement visualization approach that reduces cognitive load and increases the speed of text labeling. The approach is fine-tuned for task of extraction of patient smoking status from clinical notes. The proposed approach consists of the ordering of sentences that mention smoking, centering them at smoking tokens, and annotating to enhance informative parts of the text. Our experiments on clinical notes from the MIMIC-III clinical database demonstrate that our visualization approach enables human annotators to label sentences up to 3 times faster than with a baseline approach.</abstract>
      <url hash="13287dcd">2021.dash-1.4</url>
      <doi>10.18653/v1/2021.dash-1.4</doi>
      <bibkey>enayati-etal-2021-visualization</bibkey>
    </paper>
    <paper id="5">
      <title>Semi-supervised Interactive Intent Labeling</title>
      <author><first>Saurav</first><last>Sahay</last></author>
      <author><first>Eda</first><last>Okur</last></author>
      <author><first>Nagib</first><last>Hakim</last></author>
      <author><first>Lama</first><last>Nachman</last></author>
      <pages>31–40</pages>
      <abstract>Building the Natural Language Understanding (NLU) modules of task-oriented Spoken Dialogue Systems (SDS) involves a definition of intents and entities, collection of task-relevant data, annotating the data with intents and entities, and then repeating the same process over and over again for adding any functionality/enhancement to the SDS. In this work, we showcase an Intent Bulk Labeling system where SDS developers can interactively label and augment training data from unlabeled utterance corpora using advanced clustering and visual labeling methods. We extend the Deep Aligned Clustering work with a better backbone BERT model, explore techniques to select the seed data for labeling, and develop a data balancing method using an oversampling technique that utilizes paraphrasing models. We also look at the effect of data augmentation on the clustering process. Our results show that we can achieve over 10% gain in clustering accuracy on some datasets using the combination of the above techniques. Finally, we extract utterance embeddings from the clustering model and plot the data to interactively bulk label the samples, reducing the time and effort for data labeling of the whole dataset significantly.</abstract>
      <url hash="cdbb589c">2021.dash-1.5</url>
      <doi>10.18653/v1/2021.dash-1.5</doi>
      <bibkey>sahay-etal-2021-semi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="6">
      <title>Human-In-The-<fixed-case>L</fixed-case>oop<fixed-case>E</fixed-case>ntity Linking for Low Resource Domains</title>
      <author><first>Jan-Christoph</first><last>Klie</last></author>
      <author><first>Richard</first><last>Eckart de Castilho</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>41–43</pages>
      <abstract>Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). To quickly annotate texts with EL even in low-resource domains and noisy text, we present a novel Human-In-The-Loop EL approach. We show that it greatly outperforms a strong baseline in simulation. In a user study, annotation time is reduced by 35 % compared to annotating without interactive support; users report that they strongly prefer our system over ones without. An open-source and ready-to-use implementation based on the text annotation platform is made available.</abstract>
      <url hash="aa13974a">2021.dash-1.6</url>
      <doi>10.18653/v1/2021.dash-1.6</doi>
      <bibkey>klie-etal-2021-human</bibkey>
    </paper>
    <paper id="7">
      <title>Bridging Multi-disciplinary Collaboration Challenges in <fixed-case>ML</fixed-case> Development via Domain Knowledge Elicitation</title>
      <author><first>Soya</first><last>Park</last></author>
      <pages>44–46</pages>
      <abstract>Building a machine learning model in a sophisticated domain is a time-consuming process, partially due to the steep learning curve of domain knowledge for data scientists. We introduce Ziva, an interface for supporting domain knowledge from domain experts to data scientists in two ways: (1) a concept creation interface where domain experts extract important concept of the domain and (2) five kinds of justification elicitation interfaces that solicit elicitation how the domain concept are expressed in data instances.</abstract>
      <url hash="e64d6360">2021.dash-1.7</url>
      <doi>10.18653/v1/2021.dash-1.7</doi>
      <bibkey>park-2021-bridging</bibkey>
    </paper>
    <paper id="8">
      <title>Active learning and negative evidence for language identification</title>
      <author><first>Thomas</first><last>Lippincott</last></author>
      <author><first>Ben</first><last>Van Durme</last></author>
      <pages>47–51</pages>
      <abstract>Language identification (LID), the task of determining the natural language of a given text, is an essential first step in most NLP pipelines. While generally a solved problem for documents of sufficient length and languages with ample training data, the proliferation of microblogs and other social media has made it increasingly common to encounter use-cases that *don’t* satisfy these conditions. In these situations, the fundamental difficulty is the lack of, and cost of gathering, labeled data: unlike some annotation tasks, no single “expert” can quickly and reliably identify more than a handful of languages. This leads to a natural question: can we gain useful information when annotators are only able to *rule out* languages for a given document, rather than supply a positive label? What are the optimal choices for gathering and representing such *negative evidence* as a model is trained? In this paper, we demonstrate that using negative evidence can improve the performance of a simple neural LID model. This improvement is sensitive to policies of how the evidence is represented in the loss function, and for deciding which annotators to employ given the instance and model state. We consider simple policies and report experimental results that indicate the optimal choices for this task. We conclude with a discussion of future work to determine if and how the results generalize to other classification tasks.</abstract>
      <url hash="6d9838aa">2021.dash-1.8</url>
      <doi>10.18653/v1/2021.dash-1.8</doi>
      <bibkey>lippincott-van-durme-2021-active</bibkey>
    </paper>
    <paper id="9">
      <title>Towards integrated, interactive, and extensible text data analytics with Leam</title>
      <author><first>Peter</first><last>Griggs</last></author>
      <author><first>Cagatay</first><last>Demiralp</last></author>
      <author><first>Sajjadur</first><last>Rahman</last></author>
      <pages>52–58</pages>
      <abstract>From tweets to product reviews, text is ubiquitous on the web and often contains valuable information for both enterprises and consumers. However, the online text is generally noisy and incomplete, requiring users to process and analyze the data to extract insights. While there are systems effective for different stages of text analysis, users lack extensible platforms to support interactive text analysis workflows end-to-end. To facilitate integrated text analytics, we introduce LEAM, which aims at combining the strengths of spreadsheets, computational notebooks, and interactive visualizations. LEAM supports interactive analysis via GUI-based interactions and provides a declarative specification language, implemented based on a visual text algebra, to enable user-guided analysis. We evaluate LEAM through two case studies using two popular Kaggle text analytics workflows to understand the strengths and weaknesses of the system.</abstract>
      <url hash="17014bf7">2021.dash-1.9</url>
      <doi>10.18653/v1/2021.dash-1.9</doi>
      <bibkey>griggs-etal-2021-towards</bibkey>
    </paper>
    <paper id="10">
      <title>Data Cleaning Tools for Token Classification Tasks</title>
      <author><first>Karthik</first><last>Muthuraman</last></author>
      <author><first>Frederick</first><last>Reiss</last></author>
      <author><first>Hong</first><last>Xu</last></author>
      <author><first>Bryan</first><last>Cutler</last></author>
      <author><first>Zachary</first><last>Eichenberger</last></author>
      <pages>59–61</pages>
      <abstract>Human-in-the-loop systems for cleaning NLP training data rely on automated sieves to isolate potentially-incorrect labels for manual review. We have developed a novel technique for flagging potentially-incorrect labels with high sensitivity in named entity recognition corpora. We incorporated our sieve into an end-to-end system for cleaning NLP corpora, implemented as a modular collection of Jupyter notebooks built on extensions to the Pandas DataFrame library. We used this system to identify incorrect labels in the CoNLL-2003 corpus for English-language named entity recognition (NER), one of the most influential corpora for NER model research. Unlike previous work that only looked at a subset of the corpus’s validation fold, our automated sieve enabled us to examine the entire corpus in depth. Across the entire CoNLL-2003 corpus, we identified over 1300 incorrect labels (out of 35089 in the corpus). We have published our corrections, along with the code we used in our experiments. We are developing a repeatable version of the process we used on the CoNLL-2003 corpus as an open-source library.</abstract>
      <url hash="3b9c164d">2021.dash-1.10</url>
      <doi>10.18653/v1/2021.dash-1.10</doi>
      <bibkey>muthuraman-etal-2021-data</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="11">
      <title>Building Low-Resource <fixed-case>NER</fixed-case> Models Using Non-Speaker Annotations</title>
      <author><first>Tatiana</first><last>Tsygankova</last></author>
      <author><first>Francesca</first><last>Marini</last></author>
      <author><first>Stephen</first><last>Mayhew</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>62–69</pages>
      <abstract>In low-resource natural language processing (NLP), the key problems are a lack of target language training data, and a lack of native speakers to create it. Cross-lingual methods have had notable success in addressing these concerns, but in certain common circumstances, such as insufficient pre-training corpora or languages far from the source language, their performance suffers. In this work we propose a complementary approach to building low-resource Named Entity Recognition (NER) models using “non-speaker” (NS) annotations, provided by annotators with no prior experience in the target language. We recruit 30 participants in a carefully controlled annotation experiment with Indonesian, Russian, and Hindi. We show that use of NS annotators produces results that are consistently on par or better than cross-lingual methods built on modern contextual representations, and have the potential to outperform with additional effort. We conclude with observations of common annotation patterns and recommended implementation practices, and motivate how NS annotations can be used in addition to prior methods for improved performance.</abstract>
      <url hash="b6fee22a">2021.dash-1.11</url>
      <doi>10.18653/v1/2021.dash-1.11</doi>
      <bibkey>tsygankova-etal-2021-building</bibkey>
    </paper>
    <paper id="12">
      <title>Evaluating and Explaining Natural Language Generation with <fixed-case>G</fixed-case>en<fixed-case>X</fixed-case></title>
      <author><first>Kayla</first><last>Duskin</last></author>
      <author><first>Shivam</first><last>Sharma</last></author>
      <author><first>Ji Young</first><last>Yun</last></author>
      <author><first>Emily</first><last>Saldanha</last></author>
      <author><first>Dustin</first><last>Arendt</last></author>
      <pages>70–78</pages>
      <abstract>Current methods for evaluation of natural language generation models focus on measuring text quality but fail to probe the model creativity, i.e., its ability to generate novel but coherent text sequences not seen in the training corpus. We present the GenX tool which is designed to enable interactive exploration and explanation of natural language generation outputs with a focus on the detection of memorization. We demonstrate the utility of the tool on two domain-conditioned generation use cases - phishing emails and ACL abstracts.</abstract>
      <url hash="a2f19a59">2021.dash-1.12</url>
      <doi>10.18653/v1/2021.dash-1.12</doi>
      <bibkey>duskin-etal-2021-evaluating</bibkey>
      <pwccode url="https://github.com/pnnl/genx" additional="false">pnnl/genx</pwccode>
    </paper>
    <paper id="13">
      <title><fixed-case>C</fixed-case>ross<fixed-case>C</fixed-case>heck: Rapid, Reproducible, and Interpretable Model Evaluation</title>
      <author><first>Dustin</first><last>Arendt</last></author>
      <author><first>Zhuanyi</first><last>Shaw</last></author>
      <author><first>Prasha</first><last>Shrestha</last></author>
      <author><first>Ellyn</first><last>Ayton</last></author>
      <author><first>Maria</first><last>Glenski</last></author>
      <author><first>Svitlana</first><last>Volkova</last></author>
      <pages>79–85</pages>
      <abstract>Evaluation beyond aggregate performance metrics, e.g. F1-score, is crucial to both establish an appropriate level of trust in machine learning models and identify avenues for future model improvements. In this paper we demonstrate CrossCheck, an interactive capability for rapid cross-model comparison and reproducible error analysis. We describe the tool, discuss design and implementation details, and present three NLP use cases – named entity recognition, reading comprehension, and clickbait detection that show the benefits of using the tool for model evaluation. CrossCheck enables users to make informed decisions when choosing between multiple models, identify when the models are correct and for which examples, investigate whether the models are making the same mistakes as humans, evaluate models’ generalizability and highlight models’ limitations, strengths and weaknesses. Furthermore, CrossCheck is implemented as a Jupyter widget, which allows for rapid and convenient integration into existing model development workflows.</abstract>
      <url hash="b2a2930a">2021.dash-1.13</url>
      <doi>10.18653/v1/2021.dash-1.13</doi>
      <bibkey>arendt-etal-2021-crosscheck</bibkey>
      <pwccode url="https://github.com/pnnl/crosscheck" additional="false">pnnl/crosscheck</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="14">
      <title><fixed-case>T</fixed-case>op<fixed-case>G</fixed-case>u<fixed-case>NN</fixed-case>: Fast <fixed-case>NLP</fixed-case> Training Data Augmentation using Large Corpora</title>
      <author><first>Rebecca</first><last>Iglesias-Flores</last></author>
      <author><first>Megha</first><last>Mishra</last></author>
      <author><first>Ajay</first><last>Patel</last></author>
      <author><first>Akanksha</first><last>Malhotra</last></author>
      <author><first>Reno</first><last>Kriz</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>86–101</pages>
      <abstract>Acquiring training data for natural language processing systems can be expensive and time-consuming. Given a few training examples crafted by experts, large corpora can be mined for thousands of semantically similar examples that provide useful variability to improve model generalization. We present TopGuNN, a fast contextualized k-NN retrieval system that can efficiently index and search over contextual embeddings generated from large corpora. TopGuNN is demonstrated for a training data augmentation use case over the Gigaword corpus. Using approximate k-NN and an efficient architecture, TopGuNN performs queries over an embedding space of 4.63TB (approximately 1.5B embeddings) in less than a day.</abstract>
      <url hash="5da64775">2021.dash-1.14</url>
      <doi>10.18653/v1/2021.dash-1.14</doi>
      <bibkey>iglesias-flores-etal-2021-topgunn</bibkey>
      <pwccode url="https://github.com/penn-topgunn/topgunn" additional="false">penn-topgunn/topgunn</pwccode>
    </paper>
    <paper id="15">
      <title>Everyday Living Artificial Intelligence Hub</title>
      <author><first>Raymond</first><last>Finzel</last></author>
      <author><first>Esha</first><last>Singh</last></author>
      <author><first>Martin</first><last>Michalowski</last></author>
      <author><first>Maria</first><last>Gini</last></author>
      <author><first>Serguei</first><last>Pakhomov</last></author>
      <pages>102–104</pages>
      <abstract>We present the Everyday Living Artificial Intelligence (AI) Hub, a novel proof-of-concept framework for enhancing human health and wellbeing via a combination of tailored wear-able and Conversational Agent (CA) solutions for non-invasive monitoring of physiological signals, assessment of behaviors through unobtrusive wearable devices, and the provision of personalized interventions to reduce stress and anxiety. We utilize recent advancements and industry standards in the Internet of Things (IoT)and AI technologies to develop this proof-of-concept framework.</abstract>
      <url hash="a37d2866">2021.dash-1.15</url>
      <doi>10.18653/v1/2021.dash-1.15</doi>
      <bibkey>finzel-etal-2021-everyday</bibkey>
    </paper>
    <paper id="16">
      <title>A Computational Model for Interactive Transcription</title>
      <author><first>William</first><last>Lane</last></author>
      <author><first>Mat</first><last>Bettinson</last></author>
      <author><first>Steven</first><last>Bird</last></author>
      <pages>105–111</pages>
      <abstract>Transcribing low resource languages can be challenging in the absence of a good lexicon and trained transcribers. Accordingly, we seek a way to enable interactive transcription whereby the machine amplifies human efforts. This paper presents a data model and a system architecture for interactive transcription, supporting multiple modes of interactivity, increasing the likelihood of finding tasks that engage local participation in language work. The approach also supports other applications which are useful in our context, including spoken document retrieval and language learning.</abstract>
      <url hash="f740e892">2021.dash-1.16</url>
      <doi>10.18653/v1/2021.dash-1.16</doi>
      <bibkey>lane-etal-2021-computational</bibkey>
    </paper>
  </volume>
</collection>
