<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.trustnlp">
  <volume id="1" ingest-date="2024-06-21" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th Workshop on Trustworthy Natural Language Processing (TrustNLP 2024)</booktitle>
      <editor><first>Kai-Wei</first><last>Chang</last></editor>
      <editor><first>Anaelia</first><last>Ovalle</last></editor>
      <editor><first>Jieyu</first><last>Zhao</last></editor>
      <editor><first>Yang Trista</first><last>Cao</last></editor>
      <editor><first>Ninareh</first><last>Mehrabi</last></editor>
      <editor><first>Aram</first><last>Galstyan</last></editor>
      <editor><first>Jwala</first><last>Dhamala</last></editor>
      <editor><first>Anoop</first><last>Kumar</last></editor>
      <editor><first>Rahul</first><last>Gupta</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2024</year>
      <url hash="bf64502e">2024.trustnlp-1</url>
      <venue>trustnlp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="510883ec">2024.trustnlp-1.0</url>
      <bibkey>trustnlp-2024-trustworthy</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Beyond <fixed-case>T</fixed-case>uring: A Comparative Analysis of Approaches for Detecting Machine-Generated Text</title>
      <author><first>Muhammad</first><last>Adilazuarda</last><affiliation>MBZUAI</affiliation></author>
      <pages>1-12</pages>
      <abstract>Significant progress has been made on text generation by pre-trained language models (PLMs), yet distinguishing between human and machine-generated text poses an escalating challenge. This paper offers an in-depth evaluation of three distinct methods used to address this task: traditional shallow learning, Language Model (LM) fine-tuning, and Multilingual Model fine-tuning. These approaches are rigorously tested on a wide range of machine-generated texts, providing a benchmark of their competence in distinguishing between human-authored and machine-authored linguistic constructs. The results reveal considerable differences in performance across methods, thus emphasizing the continued need for advancement in this crucial area of NLP. This study offers valuable insights and paves the way for future research aimed at creating robust and highly discriminative models.</abstract>
      <url hash="ea60ce6f">2024.trustnlp-1.1</url>
      <attachment type="SupplementaryMaterial" hash="9283f131">2024.trustnlp-1.1.SupplementaryMaterial.zip</attachment>
      <bibkey>adilazuarda-2024-beyond</bibkey>
    </paper>
    <paper id="2">
      <title>Automated Adversarial Discovery for Safety Classifiers</title>
      <author><first>Yash Kumar</first><last>Lal</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Preethi</first><last>Lahoti</last><affiliation>Google</affiliation></author>
      <author><first>Aradhana</first><last>Sinha</last><affiliation>Google</affiliation></author>
      <author><first>Yao</first><last>Qin</last><affiliation>Google</affiliation></author>
      <author><first>Ananth</first><last>Balashankar</last><affiliation>Google</affiliation></author>
      <pages>13-26</pages>
      <abstract>Safety classifiers are critical in mitigating toxicity on online forums such as social media and in chatbots. Still, they continue to be vulnerable to emergent, and often innumerable, adversarial attacks.Traditional automated adversarial data generation methods, however, tend to produce attacks that are not diverse, but variations of previously observed harm types.We formalize the task of automated adversarial discovery for safety classifiers - to find new attacks along previously unseen harm dimensions that expose new weaknesses in the classifier.We measure progress on this task along two key axes (1) adversarial success: does the attack fool the classifier? and (2) dimensional diversity: does the attack represent a previously unseen harm type?Our evaluation of existing attack generation methods on the CivilComments toxicity task reveals their limitations: Word perturbation attacks fail to fool classifiers, while prompt-based LLM attacks have more adversarial success, but lack dimensional diversity.Even our best-performing prompt-based method finds new successful attacks on unseen harm dimensions of attacks only 5% of the time.Automatically finding new harmful dimensions of attack is crucial and there is substantial headroom for future research on our new task.</abstract>
      <url hash="d1b8de68">2024.trustnlp-1.2</url>
      <bibkey>lal-etal-2024-automated</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>F</fixed-case>air<fixed-case>B</fixed-case>elief - Assessing Harmful Beliefs in Language Models</title>
      <author><first>Mattia</first><last>Setzu</last><affiliation>University of Pisa</affiliation></author>
      <author><first>Marta</first><last>Marchiori Manerba</last><affiliation>Università di Pisa</affiliation></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Debora</first><last>Nozza</last><affiliation>Bocconi University</affiliation></author>
      <pages>27-39</pages>
      <abstract>Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing.This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs’ outputs’ hurtfulness.Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models.We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing tasks, they show hurtful beliefs about specific genders. Interestingly, training procedure and dataset, model scale, and architecture induce beliefs of different degrees of hurtfulness.</abstract>
      <url hash="fd2f0fc2">2024.trustnlp-1.3</url>
      <bibkey>setzu-etal-2024-fairbelief</bibkey>
    </paper>
    <paper id="4">
      <title>The Trade-off between Performance, Efficiency, and Fairness in Adapter Modules for Text Classification</title>
      <author><first>Minh Duc</first><last>Bui</last><affiliation>University of Mainz</affiliation></author>
      <author><first>Katharina</first><last>Von Der Wense</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>40-50</pages>
      <abstract>Current natural language processing (NLP) research tends to focus on only one or, less frequently, two dimensions – e.g., performance, interpretability, or efficiency – at a time, which may lead to suboptimal conclusions. Work on adapter modulesfocuses on improving performance and efficiency, with no investigation of unintended consequences on other aspects such as fairness. To address this gap, we conduct experiments on three text classification datasets by either (1) finetuning all parameters or (2) using adapter modules. Regarding performance and efficiency, we confirm prior findings that the accuracy of adapter-enhanced models is roughly on par with that of fully finetuned models, while training time is substantially reduced. Regarding fairness, we show that adapter modules result in mixed fairness across sensitive groups. Further investigation reveals that, when the standard finetuned model exhibits limited biases, adapter modules typically do not introduce extra bias. On the other hand, when the finetuned model exhibits increased bias, the use of adapter modules poses the potential danger of amplifying these biases to a significant extent. Our findings highlight the need for a case-by-case evaluation rather than a one-size-fits-all judgment.</abstract>
      <url hash="644d6135">2024.trustnlp-1.4</url>
      <bibkey>bui-von-der-wense-2024-trade</bibkey>
    </paper>
    <paper id="5">
      <title>When <fixed-case>XGB</fixed-case>oost Outperforms <fixed-case>GPT</fixed-case>-4 on Text Classification: A Case Study</title>
      <author><first>Matyas</first><last>Bohacek</last><affiliation>Stanford University</affiliation></author>
      <author><first>Michal</first><last>Bravansky</last><affiliation>University College London</affiliation></author>
      <pages>51-60</pages>
      <abstract>Large language models (LLMs) are increasingly used for applications beyond text generation, ranging from text summarization to instruction following. One popular example of exploiting LLMs’ zero- and few-shot capabilities is the task of text classification. This short paper compares two popular LLM-based classification pipelines (GPT-4 and LLAMA 2) to a popular pre-LLM-era classification pipeline on the task of news trustworthiness classification, focusing on performance, training, and deployment requirements. We find that, in this case, the pre-LLM-era ensemble pipeline outperforms the two popular LLM pipelines while being orders of magnitude smaller in parameter size.</abstract>
      <url hash="329d4efd">2024.trustnlp-1.5</url>
      <attachment type="SupplementaryMaterial" hash="8bccc761">2024.trustnlp-1.5.SupplementaryMaterial.zip</attachment>
      <bibkey>bohacek-bravansky-2024-xgboost</bibkey>
    </paper>
    <paper id="6">
      <title>Towards Healthy <fixed-case>AI</fixed-case>: Large Language Models Need Therapists Too</title>
      <author><first>Baihan</first><last>Lin</last><affiliation>Columbia University</affiliation></author>
      <author><first>Djallel</first><last>Bouneffouf</last><affiliation>IBM</affiliation></author>
      <author><first>Guillermo</first><last>Cecchi</last><affiliation>IBM Research</affiliation></author>
      <author><first>Kush</first><last>Varshney</last><affiliation>IBM Research</affiliation></author>
      <pages>61-70</pages>
      <abstract>Recent advances in large language models (LLMs) have led to the development of powerful chatbots capable of engaging in fluent human-like conversations. However, these chatbots may be harmful, exhibiting manipulation, gaslighting, narcissism, and other toxicity. To work toward safer and more well-adjusted models, we propose a framework that uses psychotherapy to identify and mitigate harmful chatbot behaviors. The framework involves four different artificial intelligence (AI) agents: the Chatbot whose behavior is to be adjusted, a User, a Therapist, and a Critic that can be paired with reinforcement learning-based LLM tuning. We illustrate the framework with a working example of a social conversation involving four instances of ChatGPT, showing that the framework may mitigate the toxicity in conversations between LLM-driven chatbots and people. Although there are still several challenges and directions to be addressed in the future, the proposed framework is a promising approach to improving the alignment between LLMs and human values.</abstract>
      <url hash="b7ff3547">2024.trustnlp-1.6</url>
      <bibkey>lin-etal-2024-towards</bibkey>
    </paper>
    <paper id="7">
      <title>Exploring Causal Mechanisms for Machine Text Detection Methods</title>
      <author><first>Kiyoon</first><last>Yoo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Wonhyuk</first><last>Ahn</last><affiliation>Webtoon AI</affiliation></author>
      <author><first>Yeji</first><last>Song</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Nojun</first><last>Kwak</last><affiliation>Seoul National University</affiliation></author>
      <pages>71-78</pages>
      <abstract>The immense attraction towards text generation garnered by ChatGPT has spurred the need for discriminating machine-text from human text. In this work, we provide preliminary evidence that the scores computed by existing zero-shot and supervised machine-generated text detection methods are not solely determined by the generated texts, but are affected by prompts and real texts as well. Using techniques from causal inference, we show the existence of backdoor paths that confounds the relationships between text and its detection score and how the confounding bias can be partially mitigated. We open up new research directions in identifying other factors that may be interwoven in the detection of machine text. Our study calls for a deeper investigation into which kinds of prompts make the detection of machine text more difficult or easier</abstract>
      <url hash="09ef736d">2024.trustnlp-1.7</url>
      <bibkey>yoo-etal-2024-exploring</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>F</fixed-case>act<fixed-case>A</fixed-case>lign: Fact-Level Hallucination Detection and Classification Through Knowledge Graph Alignment</title>
      <author><first>Mohamed</first><last>Rashad</last><affiliation>Agolo</affiliation></author>
      <author><first>Ahmed</first><last>Zahran</last><affiliation>Agolo</affiliation></author>
      <author><first>Abanoub</first><last>Amin</last><affiliation>Agolo</affiliation></author>
      <author><first>Amr</first><last>Abdelaal</last><affiliation>Agolo</affiliation></author>
      <author><first>Mohamed</first><last>Altantawy</last><affiliation>Agolo</affiliation></author>
      <pages>79-84</pages>
      <abstract>This paper proposes a novel black-box approach for fact-level hallucination detection and classification by transforming the problem into a knowledge graph alignment task. This approach allows us to classify detected hallucinations as either intrinsic or extrinsic. The paper starts by discussing the field of hallucination detection and introducing several approaches to related work. Then, we introduce the proposed FactAlign approach for hallucination detection and discuss how we can use it to classify hallucinations as either intrinsic or extrinsic. Experiments are carried out to evaluate the proposed method against state-of-the-art methods on the hallucination detection task using the WikiBio GPT-3 hallucination dataset, and on the hallucination type classification task using the XSum hallucination annotations dataset. The experimental results show that our method achieves a 0.889 F1 score for the hallucination detection and 0.825 F1 for the hallucination type classification, without any further training, fine-tuning, or producing multiple samples of the LLM response.</abstract>
      <url hash="7b3974cd">2024.trustnlp-1.8</url>
      <attachment type="SupplementaryMaterial" hash="6a20315c">2024.trustnlp-1.8.SupplementaryMaterial.zip</attachment>
      <bibkey>rashad-etal-2024-factalign</bibkey>
    </paper>
    <paper id="9">
      <title>Cross-Task Defense: Instruction-Tuning <fixed-case>LLM</fixed-case>s for Content Safety</title>
      <author><first>Yu</first><last>Fu</last><affiliation>tianjin university</affiliation></author>
      <author><first>Wen</first><last>Xiao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jia</first><last>Chen</last><affiliation>University of California, Riverside</affiliation></author>
      <author><first>Jiachen</first><last>Li</last><affiliation>University of California, Riverside</affiliation></author>
      <author><first>Evangelos</first><last>Papalexakis</last><affiliation>University of California Riverside</affiliation></author>
      <author><first>Aichi</first><last>Chien</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Yue</first><last>Dong</last><affiliation>University of California Riverside</affiliation></author>
      <pages>85-93</pages>
      <abstract>Recent studies reveal that Large Language Models (LLMs) face challenges in balancing safety with utility, particularly when processing long texts for NLP tasks like summarization and translation. Despite defenses against malicious short questions, the ability of LLMs to safely handle dangerous long content, such as manuals teaching illicit activities, remains unclear. Our work aims to develop robust defenses for LLMs in processing malicious documents alongside benign NLP task queries. We introduce a defense dataset comprised of safety-related examples and propose single-task and mixed-task losses for instruction tuning. Our empirical results demonstrate that LLMs can significantly enhance their capacity to safely manage dangerous content with appropriate instruction tuning. Additionally, strengthening the defenses of tasks most susceptible to misuse is effective in protecting LLMs against processing harmful information. We also observe that trade-offs between utility and safety exist in defense strategies, where Llama2, utilizing our proposed approach, displays a significantly better balance compared to Llama1.</abstract>
      <url hash="0ce9826f">2024.trustnlp-1.9</url>
      <bibkey>fu-etal-2024-cross</bibkey>
    </paper>
    <paper id="10">
      <title>On the Interplay between Fairness and Explainability</title>
      <author><first>Stephanie</first><last>Brandl</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Emanuele</first><last>Bugliarello</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Ilias</first><last>Chalkidis</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>94-108</pages>
      <abstract>In order to build reliable and trustworthy NLP applications, models need to be both fair across different demographics and explainable. Usually these two objectives, fairness and explainability, are optimized and/or examined independently of each other. Instead, we argue that forthcoming, trustworthy NLP systems should consider both.In this work, we perform a first study to understand how they influence each other: do fair(er) models rely on more plausible explanations? and vice versa. To this end, we conduct experiments on two English multi-class text classification datasets, BIOS and ECtHR, that provide information on gender and nationality, respectively, as well as human-annotated rationales. We fine-tune pre-trained language models with several methods for (i) bias mitigation, which aims to improve fairness; (ii) rationale extraction, which aims to produce plausible explanations.We find that bias mitigation algorithms do not always lead to fairer models. Moreover, in our analysis, we see that empirical fairness and explainability are orthogonal.</abstract>
      <url hash="3f893467">2024.trustnlp-1.10</url>
      <bibkey>brandl-etal-2024-interplay</bibkey>
    </paper>
    <paper id="11">
      <title>Holistic Evaluation of Large Language Models: Assessing Robustness, Accuracy, and Toxicity for Real-World Applications</title>
      <author><first>David</first><last>Cecchini</last><affiliation>John Snow Labs</affiliation></author>
      <author><first>Arshaan</first><last>Nazir</last><affiliation>John Snow Labs</affiliation></author>
      <author><first>Kalyan</first><last>Chakravarthy</last><affiliation>John Snow Labs</affiliation></author>
      <author><first>Veysel</first><last>Kocaman</last><affiliation>John Snow Labs Inc.</affiliation></author>
      <pages>109-117</pages>
      <abstract>Large Language Models (LLMs) have been widely used in real-world applications. However, as LLMs evolve and new datasets are released, it becomes crucial to build processes to evaluate and control the models’ performance. In this paper, we describe how to add Robustness, Accuracy, and Toxicity scores to model comparison tables, or leaderboards. We discuss the evaluation metrics, the approaches considered, and present the results of the first evaluation round for model Robustness, Accuracy, and Toxicity scores. Our results show that GPT 4 achieves top performance on robustness and accuracy test, while Llama 2 achieves top performance on the toxicity test. We note that newer open-source models such as open chat 3.5 and neural chat 7B can perform well on these three test categories. Finally, domain-specific tests and models are also planned to be added to the leaderboard to allow for a more detailed evaluation of models in specific areas such as healthcare, legal, and finance.</abstract>
      <url hash="9b7d7b31">2024.trustnlp-1.11</url>
      <bibkey>cecchini-etal-2024-holistic</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>HGOT</fixed-case>: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation</title>
      <author><first>Yihao</first><last>Fang</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Stephen</first><last>Thomas</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Xiaodan</first><last>Zhu</last><affiliation>Queen’s University</affiliation></author>
      <pages>118-144</pages>
      <abstract>With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations has emerged as a significant concern. To address this issue, particularly in retrieval-augmented in-context learning, we introduce the hierarchical graph of thoughts (HGOT), a structured, multi-layered graph approach designed to enhance the retrieval of pertinent passages during in-context learning. The framework utilizes the emergent planning capabilities of LLMs, employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries. It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer’s credibility intrinsically to the thought’s quality. This methodology introduces a weighted system in majority voting, prioritizing answers based on the citation quality of their thoughts. Additionally, we propose a scoring mechanism for evaluating retrieved passages, considering factors such as citation frequency and quality, self-consistency confidence, and the retrieval module’s ranking. Experiments indicate that HGOT excels as a versatile approach, outperforming competing models in FEVER by up to 7% and matching leading models such as Retrieve-then-Read in Open-SQuAD, and DSP in HotPotQA, demonstrating its efficacy in enhancing LLMs’ factuality.</abstract>
      <url hash="7a4397c3">2024.trustnlp-1.12</url>
      <attachment type="SupplementaryMaterial" hash="5df5c0e0">2024.trustnlp-1.12.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="5df5c0e0">2024.trustnlp-1.12.SupplementaryMaterial.zip</attachment>
      <bibkey>fang-etal-2024-hgot</bibkey>
    </paper>
    <paper id="13">
      <title>Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language and Vision-Language Models</title>
      <author><first>Tobias</first><last>Groot</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Matias</first><last>Valdenegro - Toro</last><affiliation>University of Groningen</affiliation></author>
      <pages>145-171</pages>
      <abstract>Language and Vision-Language Models (LLMs/VLMs) have revolutionized the field of AI by their ability to generate human-like text and understand images, but ensuring their reliability is crucial. This paper aims to evaluate the ability of LLMs (GPT4, GPT-3.5, LLaMA2, and PaLM 2) and VLMs (GPT4V and Gemini Pro Vision) to estimate their verbalized uncertainty via prompting. We propose the new Japanese Uncertain Scenes (JUS) dataset, aimed at testing VLM capabilities via difficult queries and object counting, and the Net Calibration Error (NCE) to measure direction of miscalibration.Results show that both LLMs and VLMs have a high calibration error and are overconfident most of the time, indicating a poor capability for uncertainty estimation. Additionally we develop prompts for regression tasks, and we show that VLMs have poor calibration when producing mean/standard deviation and 95% confidence intervals.</abstract>
      <url hash="993207d7">2024.trustnlp-1.13</url>
      <attachment type="SupplementaryMaterial" hash="d34555c5">2024.trustnlp-1.13.SupplementaryMaterial.zip</attachment>
      <bibkey>groot-valdenegro-toro-2024-overconfidence</bibkey>
    </paper>
    <paper id="14">
      <title>Tweak to Trust: Assessing the Reliability of Summarization Metrics in Contact Centers via Perturbed Summaries</title>
      <author><first>Kevin</first><last>Patel</last><affiliation>Observe.AI</affiliation></author>
      <author><first>Suraj</first><last>Agrawal</last><affiliation>Observe.AI</affiliation></author>
      <author><first>Ayush</first><last>Kumar</last><affiliation>Observe.AI</affiliation></author>
      <pages>172-186</pages>
      <abstract>In the dynamic realm of call center communications, the potential of abstractive summarization to transform information condensation is evident. However, evaluating the performance of abstractive summarization systems within contact center domain poses a significant challenge. Traditional evaluation metrics prove inadequate in capturing the multifaceted nature of call center conversations, characterized by diverse topics, emotional nuances, and dynamic contexts. This paper uses domain-specific perturbed summaries to scrutinize the robustness of summarization metrics in the call center domain. Through extensive experiments on call center data, we illustrate how perturbed summaries uncover limitations in existing metrics. We additionally utilize perturbation as data augmentation strategy to train domain-specific metrics. Our findings underscore the potential of perturbed summaries to complement current evaluation techniques, advancing reliable and adaptable summarization solutions in the call center domain.</abstract>
      <url hash="9430c8a1">2024.trustnlp-1.14</url>
      <bibkey>patel-etal-2024-tweak</bibkey>
    </paper>
    <paper id="15">
      <title>Flatness-Aware Gradient Descent for Safe Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Leila</first><last>Khalatbari</last><affiliation>School of Electrical and Computer Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Saeid</first><last>Hosseini</last><affiliation>Sohar University</affiliation></author>
      <author><first>Hossein</first><last>Sameti</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Pascale</first><last>Fung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>187-195</pages>
      <abstract>As generative dialog models become ubiquitous in real-world applications, it is paramount to ensure a harmless generation. There are two major challenges when enforcing safety to open-domain chatbots. Firstly, it is impractical to provide training data reflecting the desired response to all emerging forms of toxicity (generalisation challenge). Secondly, implementing safety features may compromise the quality of the conversation (trade-off challenge). To tackle the challenges, this paper introduces a regularized fine-tuning approach called FlatGD. By employing a safety-tailored loss, we translate better optimization to more safety. To ensure better optimization, FlatGD penalizes sharp trajectories of loss curve, encouraging flatness of the converged local minima. Experimental results on datasets of “BAD” and “prosocial dialog” demonstrate that our model outperforms the current baselines in reducing toxicity while preserving the conversation quality. Moreover, compared to other baselines, FlatGD can better generalize to unseen toxic data.</abstract>
      <url hash="716dc6c3">2024.trustnlp-1.15</url>
      <attachment type="SupplementaryMaterial" hash="7779c860">2024.trustnlp-1.15.SupplementaryMaterial.zip</attachment>
      <bibkey>khalatbari-etal-2024-flatness</bibkey>
    </paper>
    <paper id="16">
      <title>Introducing <fixed-case>G</fixed-case>en<fixed-case>C</fixed-case>eption for Multimodal <fixed-case>LLM</fixed-case> Benchmarking: You May Bypass Annotations</title>
      <author><first>Lele</first><last>Cao</last><affiliation>EQT</affiliation></author>
      <author><first>Valentin</first><last>Buchner</last><affiliation>EQT</affiliation></author>
      <author><first>Zineb</first><last>Senane</last><affiliation>EQT</affiliation></author>
      <author><first>Fangkai</first><last>Yang</last><affiliation>KTH</affiliation></author>
      <pages>196-201</pages>
      <abstract>Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models’ inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption’s efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.</abstract>
      <url hash="63ebc731">2024.trustnlp-1.16</url>
      <attachment type="SupplementaryMaterial" hash="03d9dd29">2024.trustnlp-1.16.SupplementaryMaterial.zip</attachment>
      <bibkey>cao-etal-2024-introducing</bibkey>
    </paper>
    <paper id="17">
      <title>Semantic-Preserving Adversarial Example Attack against <fixed-case>BERT</fixed-case></title>
      <author><first>Chongyang</first><last>Gao</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Kang</first><last>Gu</last><affiliation>DartmouthCollege</affiliation></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth</affiliation></author>
      <author><first>Shagufta</first><last>Mehnaz</last><affiliation>Dartmouth College</affiliation></author>
      <pages>202-207</pages>
      <abstract>Adversarial example attacks against textual data have been drawing increasing attention in both the natural language processing (NLP) and security domains. However, most of the existing attacks overlook the importance of semantic similarity and yield easily recognizable adversarial samples. As a result, the defense methods developed in response to these attacks remain vulnerable and could be evaded by advanced adversarial examples that maintain high semantic similarity with the original, non-adversarial text. Hence, this paper aims to investigate the extent of textual adversarial examples in maintaining such high semantic similarity. We propose Reinforce attack, a reinforcement learning-based framework to generate adversarial text that preserves high semantic similarity with the original text. In particular, the attack process is controlled by a reward function rather than heuristics, as in previous methods, to encourage higher semantic similarity and lower query costs. Through automatic and human evaluations, we show that our generated adversarial texts preserve significantly higher semantic similarity than state-of-the-art attacks while achieving similar attack success rates (outperforming at times), thus uncovering novel challenges for effective defenses.</abstract>
      <url hash="ae28c9ff">2024.trustnlp-1.17</url>
      <bibkey>gao-etal-2024-semantic</bibkey>
    </paper>
    <paper id="18">
      <title>Sandwich attack: Multi-language Mixture Adaptive Attack on <fixed-case>LLM</fixed-case>s</title>
      <author><first>Bibek</first><last>Upadhayay</last><affiliation>University of new Haven</affiliation></author>
      <author><first>Vahid</first><last>Behzadan</last><affiliation>University of New Haven</affiliation></author>
      <pages>208-226</pages>
      <abstract>A significant challenge in reliable deployment of Large Language Models (LLMs) is malicious manipulation via adversarial prompting techniques such as jailbreaks. Employing mechanisms such as safety training have proven useful in addressing this challenge. However, in multilingual LLMs, adversaries can exploit the imbalanced representation of low-resource languages in datasets used for pretraining and safety training. In this paper, we introduce a new black-box attack vector called the Sandwich Attack: a multi-language mixture attack, which manipulates state-of-the-art LLMs into generating harmful and misaligned responses. Our experiments with five different models, namely Bard, Gemini Pro, LLaMA-2-70-B-Chat, GPT-3.5-Turbo, GPT-4, and Claude-3-OPUS, show that this attack vector can be used by adversaries to elicit harmful responses from these models. By detailing both the mechanism and impact of the Sandwich attack, this paper aims to guide future research and development towards more secure and resilient LLMs, ensuring they serve the public good while minimizing potential for misuse. Content Warning: This paper contains examples of harmful language.</abstract>
      <url hash="8fe57a19">2024.trustnlp-1.18</url>
      <bibkey>upadhayay-behzadan-2024-sandwich</bibkey>
    </paper>
    <paper id="19">
      <title>Masking Latent Gender Knowledge for Debiasing Image Captioning</title>
      <author><first>Fan</first><last>Yang</last><affiliation>Amazon</affiliation></author>
      <author><first>Shalini</first><last>Ghosh</last><affiliation>Amazon.com</affiliation></author>
      <author><first>Emre</first><last>Barut</last><affiliation>Alexa AI</affiliation></author>
      <author><first>Kechen</first><last>Qin</last><affiliation>Amazon</affiliation></author>
      <author><first>Prashan</first><last>Wanigasekara</last><affiliation>Amazon</affiliation></author>
      <author><first>Chengwei</first><last>Su</last><affiliation>Amazon</affiliation></author>
      <author><first>Weitong</first><last>Ruan</last><affiliation>Amazon Alexa</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last><affiliation>Amazon.com</affiliation></author>
      <pages>227-238</pages>
      <abstract>Large language models incorporate world knowledge and present breakthrough performances on zero-shot learning. However, these models capture societal bias (e.g., gender or racial bias) due to bias during the training process which raises ethical concerns or can even be potentially harmful. The issue is more pronounced in multi-modal settings, such as image captioning, as images can also add onto biases (e.g., due to historical non-equal representation of genders in different occupations). In this study, we investigate the removal of potentially problematic knowledge from multi-modal models used for image captioning. We relax the gender bias issue in captioning models by degenderizing generated captions through the use of a simple linear mask, trained via adversarial training. Our proposal makes no assumption on the architecture of the model and freezes the model weights during the procedure, which also enables the mask to be turned off. We conduct experiments on COCO caption datasets using our masking solution. The results suggest that the proposed mechanism can effectively mask the targeted biased knowledge, by replacing more than 99% gender words with neutral ones, and maintain a comparable captioning quality performance with minimal (e.g., -1.4 on BLEU4 and ROUGE) impact to accuracy metrics.</abstract>
      <url hash="293ee56f">2024.trustnlp-1.19</url>
      <bibkey>yang-etal-2024-masking</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>BELIEVE</fixed-case>: Belief-Enhanced Instruction Generation and Augmentation for Zero-Shot Bias Mitigation</title>
      <author><first>Lisa</first><last>Bauer</last><affiliation>Amazon Alexa</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Palash</first><last>Goyal</last><affiliation>Amazon Alexa AI-NU</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>UCLA</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>USC Information Sciences Institute</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last><affiliation>Amazon.com</affiliation></author>
      <pages>239-251</pages>
      <abstract>Language models, pre-trained on large amounts of unmoderated content, have been shown to contain societal biases. Mitigating such biases typically requires access to model parameters and training schemas. In this work, we address bias mitigation at inference time, such that it can be applied to any black-box model. To this end, we propose a belief generation and augmentation framework, BELIEVE, that demonstrates effective bias mitigation for natural language generation by augmenting input prompts with automatically generated instruction-based beliefs. Our framework eases the bottleneck required for manually crafting these instruction-based beliefs, by extending a recently proposed iterative in-context learning framework to automatically generate beliefs via a language model. We assess the impact of this system on fairness, and demonstrate effective bias mitigation on pretrained and instruction-tuned models for both sentiment and regard with respect to multiple protected classes including race, gender, and political ideology.</abstract>
      <url hash="449d60cd">2024.trustnlp-1.20</url>
      <bibkey>bauer-etal-2024-believe</bibkey>
    </paper>
    <paper id="21">
      <title>Tell Me Why: Explainable Public Health Fact-Checking with Large Language Models</title>
      <author><first>Majid</first><last>Zarharan</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Pascal</first><last>Wullschleger</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Babak</first><last>Behkam Kia</last><affiliation>Iran University of Science and Technology</affiliation></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last><affiliation>Tehran Institute for Advanced Studies</affiliation></author>
      <author><first>Jennifer</first><last>Foster</last><affiliation>Dublin City University</affiliation></author>
      <pages>252-278</pages>
      <abstract>This paper presents a comprehensive analysis of explainable fact-checking through a series of experiments, focusing on the ability of large language models to verify public health claims and provide explanations or justifications for their veracity assessments. We examine the effectiveness of zero/few-shot prompting and parameter-efficient fine-tuning across various open and closed-source models, examining their performance in both isolated and joint tasks of veracity prediction and explanation generation. Importantly, we employ a dual evaluation approach comprising previously established automatic metrics and a novel set of criteria through human evaluation. Our automatic evaluation indicates that, within the zero-shot scenario, GPT-4 emerges as the standout performer, but in few-shot and parameter-efficient fine-tuning contexts, open-source models demonstrate their capacity to not only bridge the performance gap but, in some instances, surpass GPT-4. Human evaluation reveals yet more nuance as well as indicating potential problems with the gold explanations.</abstract>
      <url hash="e341cdb3">2024.trustnlp-1.21</url>
      <bibkey>zarharan-etal-2024-tell</bibkey>
    </paper>
  </volume>
</collection>
