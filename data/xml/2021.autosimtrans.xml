<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.autosimtrans">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Automatic Simultaneous Translation</booktitle>
      <editor><first>Hua</first><last>Wu</last></editor>
      <editor><first>Colin</first><last>Cherry</last></editor>
      <editor><first>Liang</first><last>Huang</last></editor>
      <editor><first>Zhongjun</first><last>He</last></editor>
      <editor><first>Qun</first><last>Liu</last></editor>
      <editor><first>Maha</first><last>Elbayad</last></editor>
      <editor><first>Mark</first><last>Liberman</last></editor>
      <editor><first>Haifeng</first><last>Wang</last></editor>
      <editor><first>Mingbo</first><last>Ma</last></editor>
      <editor><first>Ruiqing</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.autosimtrans-1</url>
      <venue>autosimtrans</venue>
    </meta>
    <frontmatter>
      <url hash="a887cca4">2021.autosimtrans-1.0</url>
      <bibkey>autosimtrans-2021-automatic</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>ICT</fixed-case>’s System for <fixed-case>A</fixed-case>uto<fixed-case>S</fixed-case>im<fixed-case>T</fixed-case>rans 2021: Robust Char-Level Simultaneous Translation</title>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>1–11</pages>
      <abstract>Simultaneous translation (ST) outputs the translation simultaneously while reading the input sentence, which is an important component of simultaneous interpretation. In this paper, we describe our submitted ST system, which won the first place in the streaming transcription input track of the Chinese-English translation task of AutoSimTrans 2021. Aiming at the robustness of ST, we first propose char-level simultaneous translation and applied wait-k policy on it. Meanwhile, we apply two data processing methods and combine two training methods for domain adaptation. Our method enhance the ST model with stronger robustness and domain adaptability. Experiments on streaming transcription show that our method outperforms the baseline at all latency, especially at low latency, the proposed method improves about 6 BLEU. Besides, ablation studies we conduct verify the effectiveness of each module in the proposed method.</abstract>
      <url hash="2ee04c3d">2021.autosimtrans-1.1</url>
      <doi>10.18653/v1/2021.autosimtrans-1.1</doi>
      <bibkey>zhang-feng-2021-icts</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>BIT</fixed-case>’s system for <fixed-case>A</fixed-case>uto<fixed-case>S</fixed-case>imul<fixed-case>T</fixed-case>rans2021</title>
      <author><first>Mengge</first><last>Liu</last></author>
      <author><first>Shuoying</first><last>Chen</last></author>
      <author><first>Minqin</first><last>Li</last></author>
      <author><first>Zhipeng</first><last>Wang</last></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <pages>12–18</pages>
      <abstract>In this paper we introduce our Chinese-English simultaneous translation system participating in AutoSimulTrans2021. In simultaneous translation, translation quality and delay are both important. In order to reduce the translation delay, we cut the streaming-input source sentence into segments and translate the segments before the full sentence is received. In order to obtain high-quality translations, we pre-train a translation model with adequate corpus and fine-tune the model with domain adaptation and sentence length adaptation. The experimental results on the evaluation data show that our system performs better than the baseline system.</abstract>
      <url hash="af715e1a">2021.autosimtrans-1.2</url>
      <doi>10.18653/v1/2021.autosimtrans-1.2</doi>
      <bibkey>liu-etal-2021-bits</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>XMU</fixed-case>’s Simultaneous Translation System at <fixed-case>NAACL</fixed-case> 2021</title>
      <author><first>Shuangtao</first><last>Li</last></author>
      <author><first>Jinming</first><last>Hu</last></author>
      <author><first>Boli</first><last>Wang</last></author>
      <author><first>Xiaodong</first><last>Shi</last></author>
      <author><first>Yidong</first><last>Chen</last></author>
      <pages>19–23</pages>
      <abstract>This paper describes our two systems submitted to the simultaneous translation evaluation at the 2nd automatic simultaneous translation workshop.</abstract>
      <url hash="b22c6f61">2021.autosimtrans-1.3</url>
      <doi>10.18653/v1/2021.autosimtrans-1.3</doi>
      <bibkey>li-etal-2021-xmus</bibkey>
    </paper>
    <paper id="4">
      <title>System Description on Automatic Simultaneous Translation Workshop</title>
      <author><first>Linjie</first><last>Chen</last></author>
      <author><first>Jianzong</first><last>Wang</last></author>
      <author><first>Zhangcheng</first><last>Huang</last></author>
      <author><first>Xiongbin</first><last>Ding</last></author>
      <author><first>Jing</first><last>Xiao</last></author>
      <pages>24–27</pages>
      <abstract>This paper shows our submission on the second automatic simultaneous translation workshop at NAACL2021. We participate in all the two directions of Chinese-to-English translation, Chinese audio<tex-math>\rightarrow</tex-math>English text and Chinese text<tex-math>\rightarrow</tex-math>English text. We do data filtering and model training techniques to get the best BLEU score and reduce the average lagging. We propose a two-stage simultaneous translation pipeline system which is composed of Quartznet and BPE-based transformer. We propose a competitive simultaneous translation system and achieves a BLEU score of 24.39 in the audio input track.</abstract>
      <url hash="427d9f29">2021.autosimtrans-1.4</url>
      <doi>10.18653/v1/2021.autosimtrans-1.4</doi>
      <bibkey>chen-etal-2021-system</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aishell-1">AISHELL-1</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>BSTC</fixed-case>: A Large-Scale <fixed-case>C</fixed-case>hinese-<fixed-case>E</fixed-case>nglish Speech Translation Dataset</title>
      <author><first>Ruiqing</first><last>Zhang</last></author>
      <author><first>Xiyang</first><last>Wang</last></author>
      <author><first>Chuanqiang</first><last>Zhang</last></author>
      <author><first>Zhongjun</first><last>He</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Zhi</first><last>Li</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Ying</first><last>Chen</last></author>
      <author><first>Qinfei</first><last>Li</last></author>
      <pages>28–35</pages>
      <abstract>This paper presents BSTC (Baidu Speech Translation Corpus), a large-scale Chinese-English speech translation dataset. This dataset is constructed based on a collection of licensed videos of talks or lectures, including about 68 hours of Mandarin data, their manual transcripts and translations into English, as well as automated transcripts by an automatic speech recognition (ASR) model. We have further asked three experienced interpreters to simultaneously interpret the testing talks in a mock conference setting. This corpus is expected to promote the research of automatic simultaneous translation as well as the development of practical systems. We have organized simultaneous translation tasks and used this corpus to evaluate automatic simultaneous translation systems.</abstract>
      <url hash="9fe5c144">2021.autosimtrans-1.5</url>
      <doi>10.18653/v1/2021.autosimtrans-1.5</doi>
      <bibkey>zhang-etal-2021-bstc</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bstc">BSTC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/covost">CoVoST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/europarl-st">Europarl-ST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="6">
      <title>Findings of the Second Workshop on Automatic Simultaneous Translation</title>
      <author><first>Ruiqing</first><last>Zhang</last></author>
      <author><first>Chuanqiang</first><last>Zhang</last></author>
      <author><first>Zhongjun</first><last>He</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>36–44</pages>
      <abstract>This paper presents the results of the shared task of the 2nd Workshop on Automatic Simultaneous Translation (AutoSimTrans). The task includes two tracks, one for text-to-text translation and one for speech-to-text, requiring participants to build systems to translate from either the source text or speech into the target text. Different from traditional machine translation, the AutoSimTrans shared task evaluates not only translation quality but also latency. We propose a metric “Monotonic Optimal Sequence” (MOS) considering both quality and latency to rank the submissions. We also discuss some important open issues in simultaneous translation.</abstract>
      <url hash="9d8f46e9">2021.autosimtrans-1.6</url>
      <doi>10.18653/v1/2021.autosimtrans-1.6</doi>
      <bibkey>zhang-etal-2021-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bstc">BSTC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/covost">CoVoST</pwcdataset>
    </paper>
  </volume>
</collection>
