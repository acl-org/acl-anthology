<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.naloma">
  <volume id="1" ingest-date="2025-09-06" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 5th Workshop on Natural Logic Meets Machine Learning (NALOMA)</booktitle>
      <editor><first>Lasha</first><last>Abzianidze</last></editor>
      <editor><first>Valeria</first><last>de Paiva</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bochum, Germany</address>
      <month>August</month>
      <year>2025</year>
      <url hash="f69c9d00">2025.naloma-1</url>
      <venue>naloma</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-287-9</isbn>
    </meta>
    <frontmatter>
      <url hash="c210c41e">2025.naloma-1.0</url>
      <bibkey>naloma-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Unpacking Legal Reasoning in <fixed-case>LLM</fixed-case>s: Chain-of-Thought as a Key to Human-Machine Alignment in Essay-Based <fixed-case>NLU</fixed-case> Tasks</title>
      <author><first>Yu Ying</first><last>Chu</last><affiliation>National Taiwan University</affiliation></author>
      <author orcid="0000-0003-3571-5236"><first>Sieh-chuen</first><last>Huang</last><affiliation>National Taiwan University</affiliation></author>
      <author orcid="0000-0002-7101-5272"><first>Hsuan-Lei</first><last>Shao</last><affiliation>Taipei Medical University</affiliation></author>
      <pages>1-7</pages>
      <abstract>This study evaluates how Large Language Models (LLMs) perform deep legal reasoning on Taiwanese Status Law questions and investigates how Chain-of-Thought (CoT) prompting affects interpretability, alignment, and generalization. Using a two-stage evaluation framework, we first decomposed six real legal essay questions into 68 sub-questions covering issue spotting, statutory application, and inheritance computation. In Stage Two, full-length answers were collected under baseline and CoT-prompted conditions. Four LLMs—ChatGPT-4o, Gemini, Grok3, and Copilot—were tested. Results show CoT prompting significantly improved accuracy for Gemini (from 83.2% to 94.5%, p &lt; 0.05) and Grok3, with moderate but consistent gains for ChatGPT and Copilot. Human evaluation of full-length responses revealed CoT answers received notably higher scores in issue coverage and reasoning clarity, with ChatGPT and Gemini gaining +2.67 and +1.92 points respectively. Despite these gains, legal misclassifications persist, highlighting alignment gaps between surface-level fluency and expert legal reasoning. This work opens the black box of legal NLU by tracing LLM reasoning chains, quantifying performance shifts under structured prompting, and providing a diagnostic benchmark for complex, open-ended legal tasks beyond multiple-choice settings.</abstract>
      <url hash="034d88d7">2025.naloma-1.1</url>
      <bibkey>chu-etal-2025-unpacking</bibkey>
    </paper>
    <paper id="2">
      <title>Dataset Creation for Visual Entailment using Generative <fixed-case>AI</fixed-case></title>
      <author><first>Rob</first><last>Reijtenbach</last></author>
      <author orcid="0000-0002-9609-9505"><first>Suzan</first><last>Verberne</last><affiliation>Universiteit Leiden</affiliation></author>
      <author orcid="0000-0002-7198-1024"><first>Gijs</first><last>Wijnholds</last><affiliation>Leiden University, Leiden University</affiliation></author>
      <pages>8-17</pages>
      <abstract>In this paper we present and validate a new synthetic dataset for training visual entailment models. Existing datasets for visual entailment are small and sparse compared to datasets for textual entailment. Manually creating datasets is labor-intensive. We base our synthetic dataset on the SNLI dataset for textual entailment. We take the premise text from SNLI as input prompts in a generative image model, Stable Diffusion, creating an image to replace each textual premise. We evaluate our dataset both intrinsically and extrinsically. For extrinsic evaluation, we evaluate the validity of the generated images by using them as training data for a visual entailment classifier based on CLIP feature vectors. We find that synthetic training data only leads to a slight drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when trained on real data. We also compare the quality of our generated training data to original training data on another dataset: SICK-VTE. Again, there is only a slight drop in F-score: from 0.400 to 0.384. These results indicate that in settings with data sparsity, synthetic data can be a promising solution for training visual entailment models.</abstract>
      <url hash="2443f28b">2025.naloma-1.2</url>
      <bibkey>reijtenbach-etal-2025-dataset</bibkey>
    </paper>
    <paper id="3">
      <title>Implementing a Logical Inference System for <fixed-case>J</fixed-case>apanese Comparatives</title>
      <author><first>Yosuke</first><last>Mikami</last></author>
      <author><first>Daiki</first><last>Matsuoka</last></author>
      <author orcid="0000-0003-0354-6116"><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <pages>18-32</pages>
      <abstract>Natural Language Inference (NLI) involving comparatives is challenging because it requires understanding quantities and comparative relations expressed by sentences. While some approaches leverage Large Language Models (LLMs), we focus on logic-based approaches grounded in compositional semantics, which are promising for robust handling of numerical and logical expressions. Previous studies along these lines have proposed logical inference systems for English comparatives. However, it has been pointed out that there are several morphological and semantic differences between Japanese and English comparatives. These differences make it difficult to apply such systems directly to Japanese comparatives. To address this gap, this study proposes ccg-jcomp, a logical inference system for Japanese comparatives based on compositional semantics. We evaluate the proposed system on a Japanese NLI dataset containing comparative expressions. We demonstrate the effectiveness of our system by comparing its accuracy with that of existing LLMs.</abstract>
      <url hash="a0de0e3b">2025.naloma-1.3</url>
      <bibkey>mikami-etal-2025-implementing</bibkey>
    </paper>
    <paper id="4">
      <title>In the Mood for Inference: Logic-Based Natural Language Inference with Large Language Models</title>
      <author orcid="0000-0002-8323-4149"><first>Bill</first><last>Noble</last><affiliation>Göteborg University and University of Gothenburg</affiliation></author>
      <author orcid="0000-0002-4576-7817"><first>Rasmus</first><last>Blanck</last><affiliation>Göteborg University</affiliation></author>
      <author orcid="0000-0002-7198-1024"><first>Gijs</first><last>Wijnholds</last><affiliation>Leiden University, Leiden University</affiliation></author>
      <pages>33-47</pages>
      <abstract>In this paper we explore challenging Natural Language Inference datasets with a logic-based approach and Large Language Models (LLMs), in order to assess the validity of this hybrid strategy. We report on an experiment which combines an LLM meta-prompting strategy, eliciting logical representations, and Prover9, a first-order logic theorem prover. In addition, we experiment with the inclusion of (logical) world knowledge. Our findings suggest that (i) broad performance is sometimes on par, (ii) formula generation is rather brittle, and (iii) world knowledge aids performance relative to data annotation. We argue that these results explicate the weaknesses of both approaches. As such, we consider this study a source of inspiration for future work in the field of neuro-symbolic reasoning.</abstract>
      <url hash="a8de4179">2025.naloma-1.4</url>
      <bibkey>noble-etal-2025-mood</bibkey>
    </paper>
    <paper id="5">
      <title>Building a Compact Math Corpus</title>
      <author><first>Andrea</first><last>Ferreira</last><affiliation>Universität Konstanz</affiliation></author>
      <pages>48-55</pages>
      <abstract>This paper introduces the Compact Math Corpus (CMC), a preliminary resource for natural language processing in the mathematics domain. We process three open-access undergraduate textbooks from distinct mathematical areas and annotate them in the CoNLL-U format using a lightweight pipeline based on the spaCy Small model. The structured output enables the extraction of syntactic bigrams and TF-IDF scores, supporting a syntactic-semantic analysis of mathematical sentences.From the annotated data, we construct a classification dataset comprising bigrams potentially representing mathematical concepts, along with representative example sentences. We combine CMC with the conversational corpus UD English EWT and train a logistic regression model with K-fold cross-validation, achieving a minimum macro-F1 score of 0.989. These results indicate the feasibility of automatic concept identification in mathematical texts.The study is designed for easy replication in low-resource settings and to promote sustainable research practices. Our approach offers a viable path to tasks such as parser adaptation, terminology extraction, multiword expression modeling, and improved analysis of mathematical language structures.</abstract>
      <url hash="f029c2b8">2025.naloma-1.5</url>
      <bibkey>ferreira-2025-building</bibkey>
    </paper>
  </volume>
</collection>
