<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.restup">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the Workshop on Resources and Techniques for User and Author Profiling in Abusive Language</booktitle>
      <editor><first>Johanna</first><last>Monti</last></editor>
      <editor><first>Valerio</first><last>Basile</last></editor>
      <editor><first>Maria Pia Di</first><last>Buono</last></editor>
      <editor><first>Raffaele</first><last>Manna</last></editor>
      <editor><first>Antonio</first><last>Pascucci</last></editor>
      <editor><first>Sara</first><last>Tonelli</last></editor>
      <publisher>European Language Resources Association (ELRA)</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-49-8</isbn>
    </meta>
    <frontmatter>
      <url hash="e3c78bbf">2020.restup-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Profiling Bots, Fake News Spreaders and Haters</title>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>1</pages>
      <abstract>Author profiling studies how language is shared by people. Stylometry techniques help in identifying aspects such as gender, age, native language, or even personality. Author profiling is a problem of growing importance, not only in marketing and forensics, but also in cybersecurity. The aim is not only to identify users whose messages are potential threats from a terrorism viewpoint but also those whose messages are a threat from a social exclusion perspective because containing hate speech, cyberbullying etc. Bots often play a key role in spreading hate speech, as well as fake news, with the purpose of polarizing the public opinion with respect to controversial issues like Brexit or the Catalan referendum. For instance, the authors of a recent study about the 1 Oct 2017 Catalan referendum, showed that in a dataset with 3.6 million tweets, about 23.6% of tweets were produced by bots. The target of these bots were pro-independence influencers that were sent negative, emotional and aggressive hateful tweets with hashtags such as #sonunesbesties (i.e. #theyareanimals). Since 2013 at the PAN Lab at CLEF (https://pan.webis.de/) we have addressed several aspects of author profiling in social media. In 2019 we investigated the feasibility of distinguishing whether the author of a Twitter feed is a bot, while this year we are addressing the problem of profiling those authors that are more likely to spread fake news in Twitter because they did in the past. We aim at identifying possible fake news spreaders as a first step towards preventing fake news from being propagated among online users (fake news aim to polarize the public opinion and may contain hate speech). In 2021 we specifically aim at addressing the challenging problem of profiling haters in social media in order to monitor abusive language and prevent cases of social exclusion in order to combat, for instance, racism, xenophobia and misogyny. Although we already started addressing the problem of detecting hate speech when targets are immigrants or women at the HatEval shared task in SemEval-2019, and when targets are women also in the Automatic Misogyny Identification tasks at IberEval-2018, Evalita-2018 and Evalita-2020, it was not done from an author profiling perspective. At the end of the keynote, I will present some insights in order to stress the importance of monitoring abusive language in social media, for instance, in foreseeing sexual crimes. In fact, previous studies confirmed that a correlation might lay between the yearly per capita rate of rape and the misogynistic language used in Twitter.</abstract>
      <url hash="32e4815b">2020.restup-1.1</url>
      <language>eng</language>
    </paper>
    <paper id="2">
      <title>An <fixed-case>I</fixed-case>ndian Language Social Media Collection for Hate and Offensive Speech</title>
      <author><first>Anita</first><last>Saroj</last></author>
      <author><first>Sukomal</first><last>Pal</last></author>
      <pages>2–8</pages>
      <abstract>In social media, people express themselves every day on issues that affect their lives. During the parliamentary elections, people’s interaction with the candidates in social media posts reflects a lot of social trends in a charged atmosphere. People’s likes and dislikes on leaders, political parties and their stands often become subject of hate and offensive posts. We collected social media posts in Hindi and English from Facebook and Twitter during the run-up to the parliamentary election 2019 of India (PEI data-2019). We created a dataset for sentiment analysis into three categories: hate speech, offensive and not hate, or not offensive. We report here the initial results of sentiment classification for the dataset using different classifiers.</abstract>
      <url hash="c9930b33">2020.restup-1.2</url>
      <language>eng</language>
    </paper>
    <paper id="3">
      <title>Profiling <fixed-case>I</fixed-case>talian Misogynist: An Empirical Study</title>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Giulia</first><last>Boifava</last></author>
      <pages>9–13</pages>
      <abstract>Hate speech may take different forms in online social environments. In this paper, we address the problem of automatic detection of misogynous language on Italian tweets by focusing both on raw text and stylometric profiles. The proposed exploratory investigation about the adoption of stylometry for enhancing the recognition capabilities of machine learning models has demonstrated that profiling users can lead to good discrimination of misogynous and not misogynous contents.</abstract>
      <url hash="595f03c6">2020.restup-1.3</url>
      <language>eng</language>
    </paper>
    <paper id="4">
      <title>Lower Bias, Higher Density Abusive Language Datasets: A Recipe</title>
      <author><first>Juliet</first><last>van Rosendaal</last></author>
      <author><first>Tommaso</first><last>Caselli</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>14–19</pages>
      <abstract>Datasets to train models for abusive language detection are at the same time necessary and still scarce. One the reasons for their limited availability is the cost of their creation. It is not only that manual annotation is expensive, it is also the case that the phenomenon is sparse, causing human annotators having to go through a large number of irrelevant examples in order to obtain some significant data. Strategies used until now to increase density of abusive language and obtain more meaningful data overall, include data filtering on the basis of pre-selected keywords and hate-rich sources of data. We suggest a recipe that at the same time can provide meaningful data with possibly higher density of abusive language and also reduce top-down biases imposed by corpus creators in the selection of the data to annotate. More specifically, we exploit the controversy channel on Reddit to obtain keywords that are used to filter a Twitter dataset. While the method needs further validation and refinement, our preliminary experiments show a higher density of abusive tweets in the filtered vs unfiltered dataset, and a more meaningful topic distribution after filtering.</abstract>
      <url hash="1551ad51">2020.restup-1.4</url>
      <language>eng</language>
    </paper>
  </volume>
</collection>
