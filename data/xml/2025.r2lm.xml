<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.r2lm">
  <volume id="1" ingest-date="2026-01-07" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First Workshop on Comparative Performance Evaluation: From Rules to Language Models</booktitle>
      <editor><first>Alicia</first><last>Picazo-Izquierdo</last></editor>
      <editor><first>Ernesto Luis</first><last>Estevanell-Valladares</last></editor>
      <editor><first>Ruslan</first><last>Mitkov</last></editor>
      <editor><first>Rafael Muñoz</first><last>Guillena</last></editor>
      <editor><first>Raúl García</first><last>Cerdá</last></editor>
      <publisher>INCOMA Ltd., Shoumen, Bulgaria</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2025</year>
      <url hash="7f5a7d12">2025.r2lm-1</url>
      <venue>r2lm</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="da43bd49">2025.r2lm-1.0</url>
      <bibkey>r2lm-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>C</fixed-case>o<fixed-case>V</fixed-case>e<fixed-case>GAT</fixed-case>: A Hybrid <fixed-case>LLM</fixed-case> &amp; <fixed-case>G</fixed-case>raph‐<fixed-case>A</fixed-case>ttention Pipeline for Accurate <fixed-case>C</fixed-case>itation‐<fixed-case>A</fixed-case>ligned Claim Verification</title>
      <author><first>Max</first><last>Bader</last></author>
      <author><first>Akshatha</first><last>Arunkumar</last></author>
      <author><first>Ohan</first><last>Ahmad</last></author>
      <author><first>Maruf</first><last>Hassen</last></author>
      <author><first>Charles</first><last>Duong</last></author>
      <author><first>Vasu</first><last>Sharma</last></author>
      <author><first>Sean</first><last>O’Brien</last></author>
      <author><first>Kevin</first><last>Zhu</last></author>
      <pages>1–9</pages>
      <abstract>Modern LLMs often generate fluent text yet fabricate, misquote, or misattribute evidence. To quantify this flaw, we built a balanced Citation‐Alignment Dataset of 500 genuine, expert‐verified claim–quote pairs and 500 minimally perturbed false variants from news, legal, scientific, and literary sources. We then propose CoVeGAT, which converts claims and citations into SVO triplets (with trigram fallback), scores each pair via an LLM‐driven chain of verification, and embeds them in a weighted semantic graph. A Graph Attention Network over BERT embeddings issues strict pass/fail judgments on alignment. Zero‐shot evaluation of seven top LLMs (e.g., GPT‐4o, Gemini 1.5, Mistral 7B) reveals a trade‐off: decisive models reach 82.5 % accuracy but err confidently, while cautious ones fall below 50 %. A MiniLM + RBF kernel baseline, by contrast, achieves 96.4 % accuracy, underscoring the power of simple, interpretable methods.</abstract>
      <url hash="f94a9bbe">2025.r2lm-1.1</url>
      <bibkey>bader-etal-2025-covegat-hybrid</bibkey>
    </paper>
    <paper id="2">
      <title>A Comparative Study of Vision Transformers and Multimodal Language Models for Violence Detection in Videos</title>
      <author><first>Tomas</first><last>Ditchfield-Ogle</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>10–20</pages>
      <abstract>This project compares methods for de- tecting violent videos, which are crucial for ensuring real-time safety in surveil- lance and digital moderation. It evaluates four approaches: a random forest classi- fier, a transformer model, and two multi- modal vision-language models. The pro- cess involves preprocessing datasets, train- ing models, and assessing accuracy, inter- pretability, scalability, and real-time suit- ability. Results show that traditional meth- ods are simple but less effective. The trans- former model achieved high accuracy, and the multimodal models offered high vio- lence recall with descriptive justifications. The study highlights trade-offs and pro- vides practical insights for the deployment of automated violence detection.</abstract>
      <url hash="603ac928">2025.r2lm-1.2</url>
      <bibkey>ditchfield-ogle-mitkov-2025-comparative</bibkey>
    </paper>
    <paper id="3">
      <title>Detection of <fixed-case>AI</fixed-case>-generated Content in Scientific Abstracts</title>
      <author><first>Ernesto Luis</first><last>Estevanell-Valladares</last></author>
      <author><first>Alicia</first><last>Picazo-Izquierdo</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>21–29</pages>
      <abstract>The growing use of generative AI in academic writing raises urgent questions about authorship and the integrity of scientific communication. This study addresses the detection of AI-generated scientific abstracts by constructing a temporally anchored dataset of paired abstracts—each with a human-written version that contains scientific abstracts of works published before 2021 and a synthetic version generated using GPT-4.1. We evaluate three approaches to authorship classification: zero-shot large language models (LLMs), fine-tuned encoder-based transformers, and traditional machine learning classifiers. Results show that LLMs perform near chance level, while a LoRA-fine-tuned DistilBERT and a PassiveAggressive classifier achieve near-perfect performance. These findings suggest that shallow lexical or stylistic patterns still differentiate human and AI writing, and that supervised learning is key to capturing these signals.</abstract>
      <url hash="185f46e9">2025.r2lm-1.3</url>
      <bibkey>estevanell-valladares-etal-2025-detection</bibkey>
    </paper>
    <paper id="4">
      <title>A Comparative Study of Hyperbole Detection Methods: From Rule-Based Approaches through Deep Learning Models to Large Language Models</title>
      <author><first>Silvia</first><last>Gargova</last></author>
      <author><first>Nevena</first><last>Grigorova</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>30–38</pages>
      <abstract>We address hyperbole detection as a binary classification task, comparing rule-based methods, fine-tuned transformers (BERT, RoBERTa), and large language models (LLMs) in zero-shot and few-shot prompting (Gemini, LLaMA). Fine-tuned transformers achieved the best overall performance, with RoBERTa attaining an F1-score of 0.82. Rule-based methods performed lower (F1 = 0.58) but remain effective in constrained linguistic contexts. LLMs showed mixed results: zero-shot performance was variable, while few-shot prompting notably improved outcomes, reaching F1-scores up to 0.79 without task-specific training data. We discuss the trade-offs between interpretability, computational cost, and data requirements across methods. Our results highlight the promise of LLMs in low-resource scenarios and suggest future work on hybrid models and broader figurative language tasks.</abstract>
      <url hash="cb0af1a6">2025.r2lm-1.4</url>
      <bibkey>gargova-etal-2025-comparative</bibkey>
    </paper>
    <paper id="5">
      <title>Evaluating the Performance of Transformers in Translating Low-Resource Languages through <fixed-case>A</fixed-case>kkadian</title>
      <author><first>Daniel A.</first><last>Jones</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>39–47</pages>
      <abstract>In this paper, we evaluate the performance of various fine-tuned, transformer-based models in translating Akkadian into English. Using annotated Akkadian data, we seek to establish potential considerations when developing models for other low-resource languages, which do not yet have as robust data. The results of this study show the potency, but also cost inefficiency, of Large Language Models compared to smaller Neural Machine Translation models. Significant evidence was also found demonstrating the importance of fine-tuning machine translation models from related languages.</abstract>
      <url hash="f0f0ffe8">2025.r2lm-1.5</url>
      <bibkey>jones-mitkov-2025-evaluating</bibkey>
    </paper>
    <paper id="6">
      <title>United We Fine-Tune: Structurally Complementary Datasets for Hope Speech Detection</title>
      <author><first>Priya Dharshini</first><last>Krishnaraj</last></author>
      <author><first>Tulio</first><last>Ferreira Leite da Silva</last></author>
      <author><first>Gonzalo</first><last>Freijedo Aduna</last></author>
      <author><first>Samuel</first><last>Chen</last></author>
      <author><first>Farah</first><last>Benamara</last></author>
      <author><first>Alda</first><last>Mari</last></author>
      <pages>48–58</pages>
      <abstract>We propose a fine-tuning strategy for English Multi-class Hope Speech Detection using Mistral, leveraging two complementary datasets: PolyHope and CDB, a new unified framework for hope speech detection. While the former provides nuanced hope-related categories such as GENERALIZED, REALISTIC, and UNREALISTIC HOPE, the later introduces linguistically grounded dimensions including COUNTERFACTUAL, DESIRE, and BELIEF. By fine-tuning Mistral on both datasets, we enable the model to capture deeper semantic representations of hope. In addition to fine-tuning, we developed advanced prompting strategies which provide interpretable, zero-shot alternatives and further inform annotation and classification designs. Our approach achieved third place in the multi-class (Macro F1=71.77) and sixth in the binary (Macro F1=85.35) settings.</abstract>
      <url hash="7082bad3">2025.r2lm-1.6</url>
      <bibkey>krishnaraj-etal-2025-united</bibkey>
    </paper>
    <paper id="7">
      <title>Does Anaphora Resolution Improve <fixed-case>LLM</fixed-case> Fine-Tuning for Summarisation?</title>
      <author><first>Yi Chun</first><last>Lo</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>59–66</pages>
      <abstract>This study investigates whether adding anaphora resolution as a preprocessing step before fine-tuning the text summarisation application in LLM can improve the quality of summary output. Two sets of training with the T5-base model and BART-large model using the SAMSum dataset were conducted. One uses the original text and the other uses the text processed by a simplified version of MARS (Mitkov’s Anaphora Resolution System). The experiment reveals that when T5-base model is fine-tuned on the anaphora-resolved inputs, the ROUGE metrics are improved. In contrast, BART-large model only has a slight improvement after fine-tuning under the same conditions, which is not statistically significant. Further analysis of the generated summaries indicates that anaphora resolution is helpful in semantic alignment.</abstract>
      <url hash="2d4e258c">2025.r2lm-1.7</url>
      <bibkey>lo-mitkov-2025-anaphora</bibkey>
    </paper>
    <paper id="8">
      <title>Transformers and Large Language Models for Hope Speech Detection A Multilingual Approach</title>
      <author><first>Diana Patricia</first><last>Madera-Espíndola</last></author>
      <author><first>Zoe</first><last>Caballero-Domínguez</last></author>
      <author><first>Valeria J.</first><last>Ramírez-Macías</last></author>
      <author><first>Sabur</first><last>Butt</last></author>
      <author><first>Hector G.</first><last>Ceballos</last></author>
      <pages>67–76</pages>
      <abstract>With the rise of Generative AI (GenAI) models in recent years, it is necessary to understand how they performed compared with other Deep Learning techniques, across tasks and across different languages. In this study, we benchmark ChatGPT-4 and XML-RoBERTa, a multilingual transformer-based model, as part of the Multilingual Binary and Multiclass Hope Speech Detection within the PolyHope-M 2025 competition. Furthermore, we explored prompting techniques and data augmentation to determine which approach yields the best performance. In our experiments, XML-RoBERTa frequently outperformed ChatGPT-4. It also attained F1 scores of 0.86 for English, 0.83 for Spanish, 0.86 for German, and 0.94 for Urdu in Task 1, while achieving 0.73 for English, 0.70 for Spanish, 0.69 for German, and 0.60 for Urdu in Task 2.</abstract>
      <url hash="c04094a2">2025.r2lm-1.8</url>
      <bibkey>madera-espindola-etal-2025-transformers</bibkey>
    </paper>
    <paper id="9">
      <title>Beyond <fixed-case>BLEU</fixed-case>: Ethical Risks of Misleading Evaluation in Domain-Specific <fixed-case>QA</fixed-case> with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ayoub</first><last>Nainia</last></author>
      <author><first>Régine</first><last>Vignes-Lebbe</last></author>
      <author><first>Hajar</first><last>Mousannif</last></author>
      <author><first>Jihad</first><last>Zahir</last></author>
      <pages>77–86</pages>
      <abstract>Large Language Models (LLMs) are increasingly used in scientific question answering (QA), including high-stakes fields such as biodiversity informatics. However, standard evaluation metrics such as BLEU, ROUGE, Exact Match (EM), and BERTScore remain poorly aligned with the factual and domain-specific requirements of these tasks. In this work, we investigate the gap between automatic metrics and expert judgment in botanical QA by comparing metric scores with human ratings across five dimensions: accuracy, completeness, relevance, fluency, and terminology usage. Our results show that standard metrics often misrepresent response quality, particularly in the presence of paraphrasing, omission, or domain-specific language. Through both quantitative analysis and qualitative examples, we show that high-scoring responses may still exhibit critical factual errors or omissions. These findings highlight the need for domain-aware evaluation frameworks that incorporate expert feedback and raise important ethical concerns about the deployment of LLMs in scientific contexts.</abstract>
      <url hash="a8bfdf90">2025.r2lm-1.9</url>
      <bibkey>nainia-etal-2025-beyond</bibkey>
    </paper>
    <paper id="10">
      <title>From Zero to Hero: Building <fixed-case>S</fixed-case>erbian <fixed-case>NER</fixed-case> from Rules to <fixed-case>LLM</fixed-case>s</title>
      <author><first>Milica</first><last>Ikonić Nešić</last></author>
      <author><first>Sasa</first><last>Petalinkar</last></author>
      <author><first>Ranka</first><last>Stanković</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>87–96</pages>
      <abstract>Named Entity Recognition (NER) presents specific challenges in Serbian, a morphologically rich language. To address these challenges, a comparative evaluation of distinct model paradigms across diverse text genres was conducted. A rule-based system (SrpNER), a traditional deep learning model (Convolutional Neural Network – CNN), fine-tuned transformer architectures (Jerteh and Tesla), and Large Language Models (LLMs), specifically ChatGPT 4.0 Nano and 4.1 Mini, were evaluated and compared. For the LLMs, a one-shot prompt engineering approach was employed, using prompt instructions aligned with the entity type definitions used in the manual annotation guidelines. Evaluation was performed on three Serbian datasets representing varied domains: newspaper articles, history textbook excerpts, and a sample of literary texts from the srpELTeC collection. The highest performance was consistently achieved by the fine-tuned transformer models, with F1 scores ranging from 0.78 on newspaper articles to 0.96 on primary school history textbook sample.</abstract>
      <url hash="040f8218">2025.r2lm-1.10</url>
      <bibkey>ikonic-nesic-etal-2025-zero</bibkey>
    </paper>
    <paper id="11">
      <title>Enhancing the Performance of Spoiler Review Detection by a <fixed-case>LLM</fixed-case> with Hints</title>
      <author><first>Genta</first><last>Nishi</last></author>
      <author><first>Einoshin</first><last>Suzuki</last></author>
      <pages>97–112</pages>
      <abstract>We investigate the effects of various hints including an introduction text, a few examples, and prompting techniques to enhance the performance of a Large-Language Model (LLM) in detecting a spoiler review of a movie. Detecting a spoiler review of a movie represents an important Natural Language Processing (NLP) task which resists the Deep Learning (DL) approach due to its highly subjective nature and scarcity in data. The highly subjective nature is also the main reason of the poor performance of LLMs-based methods, which explains their scarcity for the target problem. We address this problem by providing the LLM with an introduction text of the movie and a few reviews with their class labels as well as equipping it with a prompt that selects and exploits spoiler types with reasoning. Experiments using 400 manually labeled reviews and about 3200 LLM-labeled reviews show that our CAST (Clue And Select Types prompting) outperforms (0.05 higher) or is on par with (only 0.01 lower) cutting-edge LLM-based methods in three out of four movies in ROC-AUC. We believe our study represents an evidence of a target problem in which the knowledge intensive approach outperforms the learning-based approach.</abstract>
      <url hash="856f0f19">2025.r2lm-1.11</url>
      <bibkey>nishi-suzuki-2025-enhancing</bibkey>
    </paper>
    <paper id="12">
      <title>Evaluating Structured Decoding for Text-to-Table Generation: Evidence from Three Datasets</title>
      <author><first>Julian</first><last>Oestreich</last></author>
      <author><first>Lydia</first><last>Müller</last></author>
      <pages>113–122</pages>
      <abstract>We present a comprehensive evaluation of structured decoding for text-to-table generation with large language models (LLMs). While previous work has primarily focused on unconstrained generation of tables, the impact of enforcing structural constraints during generation remains underexplored. We systematically compare schema-guided (structured) decoding to standard one-shot prompting across three diverse benchmarks - E2E, Rotowire, and Livesum - using open-source LLMs of up to 32B parameters, assessing the performance of table generation approaches in resource-constrained settings. Our experiments cover a wide range of evaluation metrics at cell, row, and table levels. Results demonstrate that structured decoding significantly enhances the validity and alignment of generated tables, particularly in scenarios demanding precise numerical alignment (Rotowire), but may degrade performance in contexts involving densely packed textual information (E2E) or extensive aggregation over lengthy texts (Livesum). We further analyze the suitability of different evaluation metrics and discuss the influence of model size.</abstract>
      <url hash="fedd29c9">2025.r2lm-1.12</url>
      <bibkey>oestreich-muller-2025-evaluating</bibkey>
    </paper>
    <paper id="13">
      <title>Evaluating the <fixed-case>LLM</fixed-case> and <fixed-case>NMT</fixed-case> Models in Translating Low-Resourced Languages</title>
      <author><first>Julita JP</first><last>Pucinskaite</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>123–133</pages>
      <abstract>Machine translation has significantly advanced due to the development of transformer architecture, which is utilised by many modern deep-learning models. However, low-resource languages, such as Lithuanian, still face challenges stemming from the limited availability of training data and resource constraints. This study examines the translation capabilities of Neural Machine Translation (NMT) models and Large Language Models (LLMs), comparing their performance in low-resource translation tasks. Furthermore, it assesses the impact of parameter scaling and fine-tuning on their effectiveness in enhancing model performance. The evaluation showed that while LLMs demonstrated proficiency in low-resource translation, their results were lower compared to NMT models, which remained consistent across smaller variants. However, as model size increased, the lead was not as prominent, correlating with automatic and human evaluations. The effort to enhance translation accuracy through fine-tuning proved to be an effective strategy, demonstrating improvements in vocabulary expansion and structural coherence in both architectures. These findings highlight the importance of diverse datasets, comprehensive model design, and fine-tuning techniques in addressing the challenges of low-resourced language translation. This project, one of the first studies to focus on the low-resourced Lithuanian language, aims to contribute to the broader discourse and ongoing efforts to enhance accessibility and inclusivity in Natural Language Processing.</abstract>
      <url hash="5facc516">2025.r2lm-1.13</url>
      <bibkey>pucinskaite-mitkov-2025-evaluating</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>KGEIR</fixed-case>: Knowledge Graph-Enhanced Iterative Reasoning for Multi-Hop Question Answering</title>
      <author><first>Tianda</first><last>Sun</last></author>
      <author><first>Dimitar</first><last>Kazakov</last></author>
      <pages>134–143</pages>
      <abstract>Multi-hop question answering (MHQA) requires systems to retrieve and connect information across multiple documents, a task where large language models often struggle. We introduce Knowledge Graph-Enhanced Iterative Reasoning (KGEIR), a framework that dynamically constructs and refines knowledge graphs during question answering to enhance multi-hop reasoning. KGEIR identifies key entities from questions, builds an initial graph from retrieved paragraphs, reasons over this structure, identifies information gaps, and iteratively retrieves additional context to refine the graph until sufficient information is gathered. Evaluations on HotpotQA, 2WikiMultiHopQA, and MuSiQue benchmarks show competitive or superior performance to state-of-the-art methods. Ablation studies confirm that structured knowledge representations significantly outperform traditional prompting approaches like Chain-of-Thought and Tree-of-Thought. KGEIR’s ability to explicitly model entity relationships while addressing information gaps through targeted retrieval offers a promising direction for integrating symbolic and neural approaches to complex reasoning tasks.</abstract>
      <url hash="ea6f705b">2025.r2lm-1.14</url>
      <bibkey>sun-kazakov-2025-kgeir</bibkey>
    </paper>
    <paper id="15">
      <title>From Handcrafted Features to <fixed-case>LLM</fixed-case>s: A Comparative Study in Native Language Identification</title>
      <author><first>Aliyah C.</first><last>Vanterpool</last></author>
      <author><first>Katsiaryna</first><last>Aharodnik</last></author>
      <pages>144–153</pages>
      <abstract>This study compares a traditional machine learning feature-engineering approach to a large language models (LLMs) fine-tuning method for Native Language Identification (NLI). We explored the COREFL corpus, which consists of L2 English narratives produced by Spanish and German L1 speakers with lower-advanced English proficiency (C1) (Lozano et al., 2020). For the feature-engineering approach, we extracted language productivity, linguistic diversity, and n-gram features for Support Vector Machine (SVM) classification. We also looked at sentence embeddings with SVM and logistic regression. For the LLM approach, we evaluated BERT-like models and GPT-4. The feature-engineering approach, particularly n-grams, outperformed the LLMs. Sentence-BERT embeddings with SVM achieved the second-highest accuracy (93%), while GPT-4 reached an average accuracy of 90.4% across three runs when prompted with labels. These findings suggest that feature engineering remains a robust method for NLI, especially for smaller datasets with subtle linguistic differences between classes. This study contributes to the comparative analysis of traditional machine learning and transformer-based LLMs, highlighting current LLM limitations in handling domain-specific data and their need for larger training resources.</abstract>
      <url hash="7e999ad2">2025.r2lm-1.15</url>
      <bibkey>vanterpool-aharodnik-2025-handcrafted</bibkey>
    </paper>
    <paper id="16">
      <title>Systematic Evaluation of Rule-Based Analytics for <fixed-case>LLM</fixed-case>-Driven Graph Data Modelling</title>
      <author><first>Fabio Antonio</first><last>Yanez</last></author>
      <author><first>Andrés</first><last>Montoyo</last></author>
      <author><first>Armando</first><last>Suárez</last></author>
      <author><first>Alejandro</first><last>Piad-Morffis</last></author>
      <author><first>Yudivián</first><last>Almeida Cruz</last></author>
      <pages>154–164</pages>
      <abstract>This paper presents a novel multi-agent system for automatically generating graph database schemas from tabular data, strategically integrating rule-based analytics with large language models (LLMs). The framework leverages a lightweight rule system to select the most suitable analytic methods based on column data types, providing targeted insights that guide schema generation.</abstract>
      <url hash="b74f82b6">2025.r2lm-1.16</url>
      <bibkey>yanez-etal-2025-systematic</bibkey>
    </paper>
    <paper id="17">
      <title>Improved Contrastive Learning over Commonsense Knowledge Graphs for Unsupervised Reasoning</title>
      <author><first>Rongwen</first><last>Zhao</last></author>
      <author><first>Jeffrey</first><last>Flanigan</last></author>
      <pages>165–178</pages>
      <abstract>Knowledge-augmented methods leverage external resources such as commonsense knowledge graphs (CSKGs) to improve downstream reasoning tasks. Recent work has explored contrastive learning over relation-aware sequence pairs derived from CSKG triples to inject commonsense knowledge into pre-trained language models (PLMs). However, existing approaches suffer from two key limitations: they rely solely on randomly sampled in-batch negatives, overlooking more informative hard negatives, and they ignore additional plausible positives that could strengthen training. Both factors limit the effectiveness of contrastive knowledge learning. In this paper, we propose an enhanced contrastive learning framework for CSKGs that integrates <b>hard negative sampling</b> and <b>positive set expansion</b>. Hard negatives are dynamically selected based on semantic similarity to ensure the model learns from challenging distinctions, while positive set expansion exploits the property that similar head entities often share overlapping tail entities, allowing the recovery of missing positives. We evaluate our method on unsupervised commonsense question answering and inductive CSKG completion using ConceptNet and ATOMIC. Experimental results demonstrate consistent improvements over strong baselines, confirming that our approach yields richer commonsense-aware representations and more effective knowledge injection into PLMs.</abstract>
      <url hash="9567260e">2025.r2lm-1.17</url>
      <bibkey>zhao-flanigan-2025-improved</bibkey>
    </paper>
  </volume>
</collection>
