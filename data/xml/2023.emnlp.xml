<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.emnlp">
  <volume id="tutorial" ingest-date="2023-11-25" type="proceedings">
    <meta>
      <booktitle>The 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</booktitle>
      <editor><first>Qi</first><last>Zhang</last></editor>
      <editor><first>Hassan</first><last>Sajjad</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore</address>
      <month>December</month>
      <year>2023</year>
      <url hash="db1fcf67">2023.emnlp-tutorial</url>
      <venue>emnlp</venue>
    </meta>
    <frontmatter>
      <url hash="713d6ee4">2023.emnlp-tutorial.0</url>
      <bibkey>emnlp-2023-tutorial</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>NLP</fixed-case>+<fixed-case>V</fixed-case>is: <fixed-case>NLP</fixed-case> Meets Visualization</title>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Enamul</first><last>Hoque</last></author>
      <author><first>Jesse</first><last>Vig</last></author>
      <pages>1-6</pages>
      <abstract>Natural language and visualization (Vis) are two powerful modalities of human communication. The goal of this tutorial is to push forward the agenda of tightly integrating these two modalities. To this end, the tutorial will introduce NLP+Vis with a focus on two main threads of work: <i>(i) NLP for Vis:</i> How to develop and adapt state-of-the-art NLP models for solving various visualization tasks? and <i>(ii) Vis for NLP:</i> How to leverage visualization techniques to interpret and explain complex NLP models effectively? The tutorial will first motivate why NLP+Vis is an important area of research and provide an overview of research topics on combining NLP and Vis techniques. Then an overview of state-of-the-art deep learning models for NLP will be covered. Next, we will provide an overview of applying visualization techniques to help make NLP models more interpretable and explainable. In the final part, we will focus on various application tasks at the intersection of NLP and Vis. We will conclude with an interactive discussion of future challenges for NLP+Vis applications. The audience will include researchers interested in applying NLP for visualizations as well as others who focus more generally at the intersection of machine learning and visualization.</abstract>
      <url hash="b4f4e1de">2023.emnlp-tutorial.1</url>
      <bibkey>joty-etal-2023-nlp</bibkey>
    </paper>
    <paper id="2">
      <title>Security Challenges in Natural Language Processing Models</title>
      <author><first>Qiongkai</first><last>Xu</last></author>
      <author><first>Xuanli</first><last>He</last></author>
      <pages>7-12</pages>
      <abstract>Large-scale natural language processing models have been developed and integrated into numerous applications, given the advantage of their remarkable performance. Nonetheless, the security concerns associated with these models prevent the widespread adoption of these black-box machine learning models. In this tutorial, we will dive into three emerging security issues in NLP research, i.e., backdoor attacks, private data leakage, and imitation attacks. These threats will be introduced in accordance with their threatening usage scenarios, attack methodologies, and defense technologies.</abstract>
      <url hash="49353684">2023.emnlp-tutorial.2</url>
      <bibkey>xu-he-2023-security</bibkey>
    </paper>
    <paper id="3">
      <title>Designing, Evaluating, and Learning from Humans Interacting with <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Tongshuang</first><last>Wu</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <author><first>Sebastin</first><last>Santy</last></author>
      <pages>13-18</pages>
      <abstract>The rapid advancement of natural language processing (NLP) research has led to various applications spanning a wide range of domains that require models to interact with humans – e.g., chatbots responding to human inquiries, machine translation systems assisting human translators, designers prompting Large Language Models for co-creation or prototyping AI-infused applications, etc. In these cases, humans interaction is key to the success of NLP applications; any potential misconceptions or differences might lead to error cascades at the subsequent stages. Such interaction involves a lot of design choices around models, e.g. the sensitivity of interfaces, the impact of design choice and evaluation questions, etc. This tutorial aims to provide a systematic and up-to-date overview of key considerations and effective approaches for studying human-NLP model interactions. Our tutorial will focus specifically on the scenario where end users – lay people and domain experts who have access to NLP models but are less familiar with NLP techniques – use or collaborate with deployed models. Throughout the tutorial, we will use five case studies (on classifier-assisted decision making, machine-aided translation, dialog systems, and prompting) to cover three major themes: (1) how to conduct human-in-the-loop usability evaluations to ensure that models are capable of interacting with humans; (2) how to design user interfaces (UIs) and interaction mechanisms that provide end users with easy access to NLP models; (3) how to learn and improve NLP models through the human interactions. We will use best practices from HCI to ground our discussion, and will highlight current challenges and future directions.</abstract>
      <url hash="64c118bf">2023.emnlp-tutorial.3</url>
      <bibkey>wu-etal-2023-designing</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>LLM</fixed-case>-driven Instruction Following: Progresses and Concerns</title>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Qinyuan</first><last>Ye</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>19-25</pages>
      <abstract>The progress of natural language processing (NLP) is primarily driven by machine learning that optimizes a system on a large-scale set of task-specific labeled examples. This learning paradigm limits the ability of machines to have the same capabilities as humans in handling new tasks since humans can often solve unseen tasks with a couple of examples accompanied by task instructions. In addition, we may not have a chance to prepare task-specific examples of large-volume for new tasks because we cannot foresee what task needs to be addressed next and how complex to annotate for it. Therefore, task instructions act as a novel and promising resource for supervision. This tutorial targets researchers and practitioners who are interested in AI and ML technologies for NLP generalization in a low-shot scenario. In particular, we will present a diverse thread of instruction-driven NLP studies that try to answer the following questions: (i) What is task instruction? (ii) How is the process of creating datasets and evaluating systems conducted? (iii) How to encode task instructions? (iv) When and why do some instructions work better? (v) What concerns remain in LLM-driven instruction following? We will discuss several lines of frontier research that tackle those challenges and will conclude the tutorial by outlining directions for further investigation.</abstract>
      <url hash="2d27f490">2023.emnlp-tutorial.4</url>
      <bibkey>yin-etal-2023-llm</bibkey>
    </paper>
    <paper id="5">
      <title>Mitigating Societal Harms in Large Language Models</title>
      <author><first>Sachin</first><last>Kumar</last></author>
      <author><first>Vidhisha</first><last>Balachandran</last></author>
      <author><first>Lucille</first><last>Njoo</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>26-33</pages>
      <abstract>Numerous recent studies have highlighted societal harms that can be caused by language technologies deployed in the wild. While several surveys, tutorials, and workshops have discussed the risks of harms in specific contexts – e.g., detecting and mitigating gender bias in NLP models – no prior work has developed a unified typology of technical approaches for mitigating harms of language generation models. Our tutorial is based on a survey we recently wrote that proposes such a typology. We will provide an overview of potential social issues in language generation, including toxicity, social biases, misinformation, factual inconsistency, and privacy violations. Our primary focus will be on how to systematically identify risks, and how eliminate them at various stages of model development, from data collection, to model development, to inference/language generation. Through this tutorial, we aim to equip NLP researchers and engineers with a suite of practical tools for mitigating safety risks from pretrained language generation models.</abstract>
      <url hash="fdbfec94">2023.emnlp-tutorial.5</url>
      <bibkey>kumar-etal-2023-mitigating</bibkey>
    </paper>
    <paper id="6">
      <title>Creative Natural Language Generation</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Vishakh</first><last>Padmakumar</last></author>
      <author><first>He</first><last>He</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>34-40</pages>
      <abstract>Large language models such as GPT-3, GPT4, Claude etc., have advanced the state of the art in several natural language generation tasks such as text summarization and machine translation. However when it comes to open-ended tasks with a focus on creativity such as generating stories, poetry, or various forms of figurative language, these state-of-the-art language models are often found to be inadequate. This tutorial aims to bring awareness of the important and emerging research area of open-domain creative generation, with a focus on language generation while also touching on multi-modal generation (e.g., image captioning, visual metaphors). It targets natural language processing (NLP) and artificial intelligence (AI) researchers as well as creative writing practitioners who are interested in building systems that are capable of emulating as well as augmenting human creativity. In particular, we will review recent studies on creative language generation both at the sentence level as well as longer forms of text. We will provide the audiences with a holistic view of 1) the importance and challenges of building creative language generation systems; 2) how we incorporate content planning, domain knowledge and creativity specific heuristics for different forms of creative language generation such as story, poetry, humor, metaphors etc 3) how can we build better evaluation methods for creative text generation? In particular, how could the recent advancement of AI shape the future workforce for creativity? We will conclude the tutorial by outlining future research directions in this area.</abstract>
      <url hash="6972fb41">2023.emnlp-tutorial.6</url>
      <bibkey>chakrabarty-etal-2023-creative</bibkey>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2023-11-26" type="proceedings">
    <meta>
      <booktitle>The 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</booktitle>
      <editor><first>Yansong</first><last>Feng</last></editor>
      <editor><first>Els</first><last>Lefever</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore</address>
      <month>December</month>
      <year>2023</year>
      <url hash="81910496">2023.emnlp-demo</url>
      <venue>emnlp</venue>
    </meta>
    <frontmatter>
      <url hash="fa4522ed">2023.emnlp-demo.0</url>
      <bibkey>emnlp-2023-demo</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jonas</first><last>Golde</last><affiliation>Humboldt-University of Berlin</affiliation></author>
      <author><first>Patrick</first><last>Haller</last><affiliation>Machine Learning Group - Humboldt University of Berlin</affiliation></author>
      <author><first>Felix</first><last>Hamborg</last><affiliation>University of Konstanz</affiliation></author>
      <author><first>Julian</first><last>Risch</last><affiliation>deepset</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt University of Berlin</affiliation></author>
      <pages>1-11</pages>
      <abstract>Most NLP tasks are modeled as supervised learning and thus require labeled training data to train effective models. However, manually producing such data at sufficient quality and quantity is known to be costly and time-intensive. Current research addresses this bottleneck by exploring a novel paradigm called zero-shot learning via dataset generation. Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model. For instance, an LLM might be prompted to “generate 500 movie reviews with positive overall sentiment, and another 500 with negative sentiment.” The generated data could then be used to train a binary sentiment classifier, effectively leveraging an LLM as a teacher to a smaller student model. With this demo, we introduce Fabricator, an open-source Python toolkit for dataset generation. Fabricator implements common dataset generation workflows, supports a wide range of downstream NLP tasks (such as text classification, question answering, and entity recognition), and is integrated with well-known libraries to facilitate quick experimentation. With Fabricator, we aim to support researchers in conducting reproducible dataset generation experiments using LLMs and help practitioners apply this approach to train models for downstream tasks.</abstract>
      <url hash="e398094a">2023.emnlp-demo.1</url>
      <bibkey>golde-etal-2023-fabricator</bibkey>
    </paper>
    <paper id="2">
      <title>End-to-End Evaluation for Low-Latency Simultaneous Speech Translation</title>
      <author><first>Christian</first><last>Huber</last><affiliation>Karlsruhe Institut of Technology</affiliation></author>
      <author><first>Tu Anh</first><last>Dinh</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Carlos</first><last>Mullov</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Ngoc-Quan</first><last>Pham</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Thai Binh</first><last>Nguyen</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Fabian</first><last>Retkowski</last><affiliation>Karlsruhe Institut of Technology</affiliation></author>
      <author><first>Stefan</first><last>Constantin</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Enes</first><last>Ugan</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Zhaolin</first><last>Li</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Sai</first><last>Koneru</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>Karlsruhe Institut of Technology</affiliation></author>
      <author><first>Alexander</first><last>Waibel</last><affiliation>Carnegie Mellon</affiliation></author>
      <pages>12-20</pages>
      <abstract>The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components. Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user.</abstract>
      <url hash="1208a807">2023.emnlp-demo.2</url>
      <bibkey>huber-etal-2023-end</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>CHATREPORT</fixed-case>: Democratizing Sustainability Disclosure Analysis through <fixed-case>LLM</fixed-case>-based Tools</title>
      <author><first>Jingwei</first><last>Ni</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Julia</first><last>Bingler</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Chiara</first><last>Colesanti-Senni</last><affiliation>University of Zürich</affiliation></author>
      <author><first>Mathias</first><last>Kraus</last><affiliation>FAU Erlangen-Nuremberg</affiliation></author>
      <author><first>Glen</first><last>Gostlow</last><affiliation>UniversityofZurich</affiliation></author>
      <author><first>Tobias</first><last>Schimanski</last><affiliation>UniversityofZurich,UniversityofOxford</affiliation></author>
      <author><first>Dominik</first><last>Stammbach</last><affiliation>ETH Zürich</affiliation></author>
      <author><first>Saeid</first><last>Ashraf Vaghefi</last><affiliation>University of Zürich</affiliation></author>
      <author><first>Qian</first><last>Wang</last><affiliation>University of Zurich, Inovest Partners AG</affiliation></author>
      <author><first>Nicolas</first><last>Webersinke</last><affiliation>FAU</affiliation></author>
      <author><first>Tobias</first><last>Wekhof</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Tingyu</first><last>Yu</last><affiliation>UniversityofZurich</affiliation></author>
      <author><first>Markus</first><last>Leippold</last><affiliation>University of Zürich</affiliation></author>
      <pages>21-51</pages>
      <abstract>In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we introduce ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) actively involving domain experts in the development loop. We make our methodology, annotated datasets, and generated analyses of 1015 reports publicly available. Video Introduction: <url>https://www.youtube.com/watch?v=Q5AzaKzPE4M</url> Github: <url>https://github.com/EdisonNi-hku/chatreport</url> Live web app: reports.chatclimate.ai</abstract>
      <url hash="b1ccd9fb">2023.emnlp-demo.3</url>
      <bibkey>ni-etal-2023-chatreport</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>R</fixed-case>a<fixed-case>LL</fixed-case>e: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models</title>
      <author><first>Yasuto</first><last>Hoshi</last><affiliation>Kioxia Corporation</affiliation></author>
      <author><first>Daisuke</first><last>Miyashita</last><affiliation>Kioxia Corporation</affiliation></author>
      <author><first>Youyang</first><last>Ng</last><affiliation>Kioxia Corporation</affiliation></author>
      <author><first>Kento</first><last>Tatsuno</last><affiliation>Kioxia Corporation</affiliation></author>
      <author><first>Yasuhiro</first><last>Morioka</last><affiliation>KIOXIA Corporation</affiliation></author>
      <author><first>Osamu</first><last>Torii</last><affiliation>KIOXIA Corporation</affiliation></author>
      <author><first>Jun</first><last>Deguchi</last><affiliation>Kioxia Corporation</affiliation></author>
      <pages>52-69</pages>
      <abstract>Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap, we present RaLLe, an open-source framework designed to facilitate the development, evaluation, and optimization of R-LLMs for knowledge-intensive tasks. With RaLLe, developers can easily develop and evaluate R-LLMs, improving hand-crafted prompts, assessing individual inference processes, and objectively measuring overall system performance quantitatively. By leveraging these features, developers can enhance the performance and accuracy of their R-LLMs in knowledge-intensive generation tasks.</abstract>
      <url hash="df8f5a82">2023.emnlp-demo.4</url>
      <bibkey>hoshi-etal-2023-ralle</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>VIST</fixed-case>5: An Adaptive, Retrieval-Augmented Language Model for Visualization-oriented Dialog</title>
      <author><first>Henrik</first><last>Voigt</last><affiliation>Friedrich-Schiller-University</affiliation></author>
      <author><first>Nuno</first><last>Carvalhais</last><affiliation>Max Planck Institute for Biogeochemistry</affiliation></author>
      <author><first>Monique</first><last>Meuschke</last><affiliation>University of Jena</affiliation></author>
      <author><first>Markus</first><last>Reichstein</last><affiliation>Max Planck Institute for Biogeochemistry</affiliation></author>
      <author><first>Sina</first><last>Zarrie</last><affiliation>University of Bielefeld</affiliation></author>
      <author><first>Kai</first><last>Lawonn</last><affiliation>University of Jena</affiliation></author>
      <pages>70-81</pages>
      <abstract>The advent of large language models has brought about new ways of interacting with data intuitively via natural language. In recent years, a variety of visualization systems have explored the use of natural language to create and modify visualizations through visualization-oriented dialog. However, the majority of these systems rely on tailored dialog agents to analyze domain-specific data and operate domain-specific visualization tools and libraries. This is a major challenge when trying to transfer functionalities between dialog interfaces of different visualization applications. To address this issue, we propose VIST5, a visualization-oriented dialog system that focuses on easy adaptability to an application domain as well as easy transferability of language-controllable visualization library functions between applications. Its architecture is based on a retrieval-augmented T5 language model that leverages few-shot learning capabilities to enable a rapid adaptation of the system.</abstract>
      <url hash="81b53d7d">2023.emnlp-demo.5</url>
      <bibkey>voigt-etal-2023-vist5</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>H</fixed-case>2<fixed-case>O</fixed-case> Open Ecosystem for State-of-the-art Large Language Models</title>
      <author><first>Arno</first><last>Candel</last><affiliation>H2O.ai</affiliation></author>
      <author><first>Jon</first><last>McKinney</last><affiliation>H2O.ai</affiliation></author>
      <author><first>Philipp</first><last>Singer</last><affiliation>H2O.ai</affiliation></author>
      <author><first>Pascal</first><last>Pfeiffer</last><affiliation>H2O.ai</affiliation></author>
      <author><first>Maximilian</first><last>Jeblick</last><affiliation>H2O.ai</affiliation></author>
      <author><first>Chun Ming</first><last>Lee</last><affiliation>H2O.ai</affiliation></author>
      <author><first>Marcos</first><last>Conde</last><affiliation>H2O.ai</affiliation></author>
      <pages>82-89</pages>
      <abstract>Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. Our demo is available at: https://gpt.h2o.ai/</abstract>
      <url hash="3b419b6a">2023.emnlp-demo.6</url>
      <bibkey>candel-etal-2023-h2o</bibkey>
    </paper>
    <paper id="7">
      <title>Koala: An Index for Quantifying Overlaps with Pre-training Corpora</title>
      <author><first>Thuy-Trang</first><last>Vu</last><affiliation>Monash University</affiliation></author>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University</affiliation></author>
      <author><first>Ehsan</first><last>Shareghi</last><affiliation>Monash University</affiliation></author>
      <pages>90-98</pages>
      <abstract>In very recent years more attention has been placed on probing the role of pre-training data in Large Language Models (LLMs) downstream behaviour. Despite the importance, there is no public tool that supports such analysis of pre-training corpora at large scale. To help research in this space, we launch Koala, a searchable index over large pre-training corpora using lossless compressed suffix arrays with highly efficient compression rate and search support. In its first release we index the public proportion of OPT 175B, GPT-3, GPT-Neo, GPT-Neo, LLaMA, BERT, ELECTRA, RoBERTA, XLNet pre-training corpora. Koala provides a framework to do forensic analysis on the current and future benchmarks as well as to assess the degree of memorization in the output from the LLMs. Koala is available for public use at https://koala-index.erc.monash.edu/.</abstract>
      <url hash="bac55941">2023.emnlp-demo.7</url>
      <bibkey>vu-etal-2023-koala</bibkey>
    </paper>
    <paper id="8">
      <title>Sudowoodo: A <fixed-case>C</fixed-case>hinese Lyric Imitation System with Source Lyrics</title>
      <author><first>Yongzhu</first><last>Chang</last><affiliation>Netease Fuxi AI Lab</affiliation></author>
      <author><first>Rongsheng</first><last>Zhang</last><affiliation>Netease Fuxi AI Lab</affiliation></author>
      <author><first>Lin</first><last>Jiang</last><affiliation>Netease Fuxi AI Lab</affiliation></author>
      <author><first>Qihang</first><last>Chen</last><affiliation>Netease Music AV Lab</affiliation></author>
      <author><first>Le</first><last>Zhang</last><affiliation>Fuxi AI Lab, NetEase Inc.</affiliation></author>
      <author><first>Jiashu</first><last>Pu</last><affiliation>NetEase Fuxi Lab</affiliation></author>
      <pages>99-105</pages>
      <abstract>Lyrics generation is a well-known application in natural language generation research, with several previous studies focusing on generating accurate lyrics using precise control such as keywords, rhymes, etc. However, lyrics imitation, which involves writing new lyrics by imitating the style and content of the source lyrics, remains a challenging task due to the lack of a parallel corpus. In this paper, we introduce Sudowoodo, a Chinese lyrics imitation system that can generate new lyrics based on the text of source lyrics. To address the issue of lacking a parallel training corpus for lyrics imitation, we propose a novel framework to construct a parallel corpus based on a keyword-based lyrics model from source lyrics. Then the pairs <i>(new lyrics, source lyrics)</i> are used to train the lyrics imitation model. During the inference process, we utilize a post-processing module to filter and rank the generated lyrics, selecting the highest-quality ones. We incorporated audio information and aligned the lyrics with the audio to form the songs as a bonus. The human evaluation results show that our framework can perform better lyric imitation. Meanwhile, the <i>Sudowoodo</i> system and demo video of the system is available at Sudowoodo and <url>https://youtu.be/u5BBT\_j1L5M</url>
      </abstract>
      <url hash="4c9150a7">2023.emnlp-demo.8</url>
      <bibkey>chang-etal-2023-sudowoodo</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>C</fixed-case>onv<fixed-case>L</fixed-case>ab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format</title>
      <author><first>Qi</first><last>Zhu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Christian</first><last>Geishauser</last><affiliation>Heinrich Heine University Duesseldorf</affiliation></author>
      <author><first>Hsien-chin</first><last>Lin</last><affiliation>Heinrich Heine University</affiliation></author>
      <author><first>Carel</first><last>van Niekerk</last><affiliation>Heinrich Heine University</affiliation></author>
      <author><first>Baolin</first><last>Peng</last><affiliation>Microsoft Research, Redmond, USA</affiliation></author>
      <author><first>Zheng</first><last>Zhang</last><affiliation>Tsinghua University, Beijing, China</affiliation></author>
      <author><first>Shutong</first><last>Feng</last><affiliation>Heinrich Heine University Duesseldorf</affiliation></author>
      <author><first>Michael</first><last>Heck</last><affiliation>Heinrich Heine University Duesseldorf</affiliation></author>
      <author><first>Nurul</first><last>Lubis</last><affiliation>Heinrich Heine University Duesseldorf</affiliation></author>
      <author><first>Dazhen</first><last>Wan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xiaochen</first><last>Zhu</last><affiliation>University of Cambridge, Cambridge, England</affiliation></author>
      <author><first>Jianfeng</first><last>Gao</last><affiliation>Microsoft Research, Redmond</affiliation></author>
      <author><first>Milica</first><last>Gasic</last><affiliation>Heinrich Heine University Duesseldorf</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>106-123</pages>
      <abstract>Task-oriented dialogue (TOD) systems function as digital assistants, guiding users through various tasks such as booking flights or finding restaurants. Existing toolkits for building TOD systems often fall short in delivering comprehensive arrays of data, model, and experimental environments with a user-friendly experience. We introduce ConvLab-3: a multifaceted dialogue system toolkit crafted to bridge this gap. Our unified data format simplifies the integration of diverse datasets and models, significantly reducing complexity and cost for studying generalization and transfer. Enhanced with robust reinforcement learning (RL) tools, featuring a streamlined training process, in-depth evaluation tools, and a selection of user simulators, ConvLab-3 supports the rapid development and evaluation of robust dialogue policies. Through an extensive study, we demonstrate the efficacy of transfer learning and RL and showcase that ConvLab-3 is not only a powerful tool for seasoned researchers but also an accessible platform for newcomers.</abstract>
      <url hash="16a231c0">2023.emnlp-demo.9</url>
      <bibkey>zhu-etal-2023-convlab</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>FLEEK</fixed-case>: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge</title>
      <author><first>Farima</first><last>Fatahi Bayat</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Kun</first><last>Qian</last><affiliation>Apple</affiliation></author>
      <author><first>Benjamin</first><last>Han</last><affiliation>Apple</affiliation></author>
      <author><first>Yisi</first><last>Sang</last><affiliation>Apple</affiliation></author>
      <author><first>Anton</first><last>Belyy</last><affiliation>Apple Inc.</affiliation></author>
      <author><first>Samira</first><last>Khorshidi</last><affiliation>Apple Inc.</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Apple Inc.</affiliation></author>
      <author><first>Ihab</first><last>Ilyas</last><affiliation>Apple</affiliation></author>
      <author><first>Yunyao</first><last>Li</last><affiliation>Apple</affiliation></author>
      <pages>124-130</pages>
      <abstract>Detecting factual errors of textual information, whether generated by large language models (LLM) or curated by humans, is crucial for making informed decisions. LLMs’ inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses. Humans, too, are prone to factual errors in their writing. Since manual detection and correction of factual er- rors is labor-intensive, developing an automatic approach can greatly reduce human effort. We present a prototype tool that automatically extracts factual claims from text, gathers evidence from external knowledge sources, evaluates the factuality of each claim, and suggests revisions for identified errors using the collected evidence. Initial empirical evaluation on fact error detection (77-85% F1) shows the potential of our tool.</abstract>
      <url hash="edaf2862">2023.emnlp-demo.10</url>
      <bibkey>fatahi-bayat-etal-2023-fleek</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>YATO</fixed-case>: Yet Another deep learning based Text analysis Open toolkit</title>
      <author><first>Zeqiang</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yile</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jiageng</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhiyang</first><last>Teng</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Jie</first><last>Yang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>131-139</pages>
      <abstract>We introduce YATO, an open-source, easy-to-use toolkit for text analysis with deep learning. Different from existing heavily engineered toolkits and platforms, YATO is lightweight and user-friendly for researchers from cross-disciplinary areas. Designed in a hierarchical structure, YATO supports free combinations of three types of widely used features including 1) traditional neural networks (CNN, RNN, etc.); 2) pre-trained language models (BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a simple configurable file. Benefiting from the advantages of flexibility and ease of use, YATO can facilitate fast reproduction and refinement of state-of-the-art NLP models, and promote the cross-disciplinary applications of NLP techniques. The code, examples, and documentation are publicly available at https://github.com/jiesutd/YATO. A demo video is also available at https://www.youtube.com/playlist?list=PLJ0mhzMcRuDUlTkzBfAftOqiJRxYTTjXH.</abstract>
      <url hash="1af9ce63">2023.emnlp-demo.11</url>
      <bibkey>wang-etal-2023-yato</bibkey>
    </paper>
    <paper id="12">
      <title>Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face</title>
      <author><first>Christopher</first><last>Akiki</last><affiliation>Leipzig University</affiliation></author>
      <author><first>Odunayo</first><last>Ogundepo</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Aleksandra</first><last>Piktus</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Xinyu</first><last>Zhang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Akintunde</first><last>Oladipo</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Martin</first><last>Potthast</last><affiliation>Leipzig University</affiliation></author>
      <pages>140-148</pages>
      <abstract>We present Spacerini, a tool that integrates the Pyserini toolkit for reproducible information retrieval research with Hugging Face to enable the seamless construction and deployment of interactive search engines. Spacerini makes state-of-the-art sparse and dense retrieval models more accessible to non-IR practitioners while minimizing deployment effort. This is useful for NLP researchers who want to better understand and validate their research by performing qualitative analyses of training corpora, for IR researchers who want to demonstrate new retrieval models integrated into the growing Pyserini ecosystem, and for third parties reproducing the work of other researchers. Spacerini is open source and includes utilities for loading, preprocessing, indexing, and deploying search engines locally and remotely. We demonstrate a portfolio of 13 search engines created with Spacerini for different use cases.</abstract>
      <url hash="b1f4d99e">2023.emnlp-demo.12</url>
      <bibkey>akiki-etal-2023-spacerini</bibkey>
    </paper>
    <paper id="13">
      <title>Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning</title>
      <author><first>Clifton</first><last>Poth</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <author><first>Hannah</first><last>Sterz</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Indraneil</first><last>Paul</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Sukannya</first><last>Purkayastha</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Leon</first><last>Engländer</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <author><first>Timo</first><last>Imhof</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <author><first>Ivan</first><last>Vuli</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Sebastian</first><last>Ruder</last><affiliation>Google</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <author><first>Jonas</first><last>Pfeiffer</last><affiliation>Google</affiliation></author>
      <pages>149-160</pages>
      <abstract>We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and practitioners to leverage adapter modularity through composition blocks, enabling the design of complex adapter setups. We demonstrate the library’s efficacy by evaluating its performance against full fine-tuning on various NLP tasks. Adapters provides a powerful tool for addressing the challenges of conventional fine-tuning paradigms and promoting more efficient and modular transfer learning. The library is available via https://adapterhub.ml/adapters.</abstract>
      <url hash="11850ef4">2023.emnlp-demo.13</url>
      <bibkey>poth-etal-2023-adapters</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>INTELMO</fixed-case>: Enhancing Models’ Adoption of Interactive Interfaces</title>
      <author><first>Chunxu</first><last>Yang</last><affiliation>UCLA HCI Research</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce</affiliation></author>
      <author><first>Lidiya</first><last>Murakhovs’ka</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Philippe</first><last>Laban</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Xiang</first><last>Chen</last><affiliation>UCLA HCI Research</affiliation></author>
      <pages>161-166</pages>
      <abstract>This paper presents INTELMO, an easy-to-use library to help model developers adopt user-faced interactive interfaces and articles from real-time RSS sources for their language models. The library categorizes common NLP tasks and provides default style patterns, streamlining the process of creating interfaces with minimal code modifications while ensuring an intuitive user experience. Moreover, INTELMO employs a multi-granular hierarchical abstraction to provide developers with fine-grained and flexible control over user interfaces. INTELMO is under active development, with document available at <url>https://intelmo.github.io</url>.</abstract>
      <url hash="e1d292a1">2023.emnlp-demo.14</url>
      <bibkey>yang-etal-2023-intelmo</bibkey>
    </paper>
    <paper id="15">
      <title>Humanoid Agents: Platform for Simulating Human-like Generative Agents</title>
      <author><first>Zhilin</first><last>Wang</last><affiliation>Nvidia</affiliation></author>
      <author><first>Yu Ying</first><last>Chiu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Yu Cheung</first><last>Chiu</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>167-176</pages>
      <abstract>Just as computational simulations of atoms, molecules and cells have shaped the way we study the sciences, true-to-life simulations of human-like agents can be valuable tools for studying human behavior. We propose Humanoid Agents, a system that guides Generative Agents to behave more like humans by introducing three elements of System 1 processing: Basic needs (e.g. hunger, health and energy), Emotion and Closeness in Relationships. Humanoid Agents are able to use these dynamic elements to adapt their daily activities and conversations with other agents, as supported with empirical experiments. Our system is designed to be extensible to various settings, three of which we demonstrate, as well as to other elements influencing human behavior (e.g. empathy, moral values and cultural background). Our platform also includes a Unity WebGL game interface for visualization and an interactive analytics dashboard to show agent statuses over time. Our platform is available on https://www.humanoidagents.com/ and code is on https://github.com/HumanoidAgents/HumanoidAgents</abstract>
      <url hash="283fa654">2023.emnlp-demo.15</url>
      <bibkey>wang-etal-2023-humanoid</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>TP</fixed-case>-Detector: Detecting Turning Points in the Engineering Process of Large-scale Projects</title>
      <author><first>Qi</first><last>Wu</last><affiliation>Beihang University</affiliation></author>
      <author><first>WenHan</first><last>Chao</last><affiliation>BeiHang University</affiliation></author>
      <author><first>Xian</first><last>Zhou</last><affiliation>Center for Information Research, Academy of Military Science</affiliation></author>
      <author><first>Zhunchen</first><last>Luo</last><affiliation>Center for Information Research, Academy of Military Science</affiliation></author>
      <pages>177-185</pages>
      <abstract>This paper introduces a novel task of detecting turning points in the engineering process of large-scale projects, wherein the turning points signify significant transitions occurring between phases. Given the complexities involving diverse critical events and limited comprehension in individual news reports, we approach the problem by treating the sequence of related news streams as a window with multiple instances. To capture the evolution of changes effectively, we adopt a deep Multiple Instance Learning (MIL) framework and employ the multiple instance ranking loss to discern the transition patterns exhibited in the turning point window. Extensive experiments consistently demonstrate the effectiveness of our proposed approach on the constructed dataset compared to baseline methods. We deployed the proposed mode and provided a demonstration video to illustrate its functionality. The code and dataset are available on GitHub.</abstract>
      <url hash="2633f509">2023.emnlp-demo.16</url>
      <bibkey>wu-etal-2023-tp</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>CLEVA</fixed-case>: <fixed-case>C</fixed-case>hinese Language Models <fixed-case>EVA</fixed-case>luation Platform</title>
      <author><first>Yanyang</first><last>Li</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jianqiao</first><last>Zhao</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Duo</first><last>Zheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zi-Yuan</first><last>Hu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zhi</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Xiaohui</first><last>Su</last><affiliation>18845752107</affiliation></author>
      <author><first>Yongfeng</first><last>Huang</last><affiliation>The Chinese University of Hong Kong (CUHK)</affiliation></author>
      <author><first>Shijia</first><last>Huang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Michael</first><last>Lyu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Liwei</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>186-217</pages>
      <abstract>With the continuous emergence of Chinese Large Language Models (LLMs), how to evaluate a model’s capabilities has become an increasingly significant issue. The absence of a comprehensive Chinese benchmark that thoroughly assesses a model’s performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted to holistically evaluate Chinese LLMs. Our platform employs a standardized workflow to assess LLMs’ performance across various dimensions, regularly updating a competitive leaderboard. To alleviate contamination, CLEVA curates a significant proportion of new data and develops a sampling strategy that guarantees a unique subset for each leaderboard round. Empowered by an easy-to-use interface that requires just a few mouse clicks and a model API, users can conduct a thorough evaluation with minimal coding. Large-scale experiments featuring 23 Chinese LLMs have validated CLEVA’s efficacy.</abstract>
      <url hash="1bb1274d">2023.emnlp-demo.17</url>
      <bibkey>li-etal-2023-cleva</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>DOPA</fixed-case> <fixed-case>METER</fixed-case> – A Tool Suite for Metrical Document Profiling and Aggregation</title>
      <author><first>Christina</first><last>Lohr</last><affiliation>Universität Leipzig</affiliation></author>
      <author><first>Udo</first><last>Hahn</last><affiliation>Friedrich-Schiller-Universitaet Jena</affiliation></author>
      <pages>218-228</pages>
      <abstract>We present DOPA METER, a tool suite for the metrical investigation of written language, that provides diagnostic means for its division into discourse categories, such as registers, genres, and style. The quantitative basis of our system are 120 metrics covering a wide range of lexical, syntactic, and semantic features relevant for language profiling. The scores can be summarized, compared, and aggregated using visualization tools that can be tailored according to the users’ needs. We also showcase an application scenario for DOPA METER.</abstract>
      <url hash="43961d05">2023.emnlp-demo.18</url>
      <bibkey>lohr-hahn-2023-dopa</bibkey>
    </paper>
    <paper id="19">
      <title>Muted: Multilingual Targeted Offensive Speech Identification and Visualization</title>
      <author><first>Christoph</first><last>Tillmann</last><affiliation>IBM Research</affiliation></author>
      <author><first>Aashka</first><last>Trivedi</last><affiliation>IBM Research</affiliation></author>
      <author><first>Sara</first><last>Rosenthal</last><affiliation>IBM Research</affiliation></author>
      <author><first>Santosh</first><last>Borse</last><affiliation>IBM Research</affiliation></author>
      <author><first>Rong</first><last>Zhang</last><affiliation>IBM.com</affiliation></author>
      <author><first>Avirup</first><last>Sil</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Bishwaranjan</first><last>Bhattacharjee</last><affiliation>IBM T.J.Watson Researcg</affiliation></author>
      <pages>229-236</pages>
      <abstract>Offensive language such as hate, abuse, and profanity (HAP) occurs in various content on the web. While previous work has mostly dealt with sentence level annotations, there have been a few recent attempts to identify offensive spans as well. We build upon this work and introduce MUTED, a system to identify multilingual HAP content by displaying offensive arguments and their targets using heat maps to indicate their intensity. MUTED can leverage any transformer-based HAP-classification model and its attention mechanism out-of-the-box to identify toxic spans, without further fine-tuning. In addition, we use the spaCy library to identify the specific targets and arguments for the words predicted by the attention heatmaps. We present the model’s performance on identifying offensive spans and their targets in existing datasets and present new annotations on German text. Finally, we demonstrate our proposed visualization tool on multilingual inputs.</abstract>
      <url hash="c5b2bac7">2023.emnlp-demo.19</url>
      <bibkey>tillmann-etal-2023-muted</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>G</fixed-case>entopia.<fixed-case>AI</fixed-case>: A Collaborative Platform for Tool-Augmented <fixed-case>LLM</fixed-case>s</title>
      <author><first>Binfeng</first><last>Xu</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Xukun</first><last>Liu</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Hua</first><last>Shen</last><affiliation>PennState University</affiliation></author>
      <author><first>Zeyu</first><last>Han</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Yuhan</first><last>Li</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Murong</first><last>Yue</last><affiliation>George Mason University</affiliation></author>
      <author><first>Zhiyuan</first><last>Peng</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Yuchen</first><last>Liu</last><affiliation>NC State University</affiliation></author>
      <author><first>Ziyu</first><last>Yao</last><affiliation>George Mason University</affiliation></author>
      <author><first>Dongkuan</first><last>Xu</last><affiliation>North Carolina State University</affiliation></author>
      <pages>237-245</pages>
      <abstract>Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. This paper proposes Gentopia, a lightweight and extensible framework for ALMs. Gentopia allows the flexible customization of agents through simple configurations, seamlessly integrating various language models, task formats, prompting modules, and plugins into a unified paradigm. Furthermore, we establish Gentpool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in Gentpool are composable such that they can be assembled together for agent collaboration, advancing the democratization of artificial intelligence. To ensure high-quality agents, Gentbench, an integral component of Gentpool, is designed to thoroughly evaluate user-customized agents across diverse aspects such as safety, robustness, efficiency, etc. We release Gentopia on Github and will continuously move forward.</abstract>
      <url hash="031fb1ac">2023.emnlp-demo.20</url>
      <bibkey>xu-etal-2023-gentopia</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>M</fixed-case>usic<fixed-case>A</fixed-case>gent: An <fixed-case>AI</fixed-case> Agent for Music Understanding and Generation with Large Language Models</title>
      <author><first>Dingyao</first><last>Yu</last><affiliation>School of Software and Microelectronics, Peking University</affiliation></author>
      <author><first>Kaitao</first><last>Song</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Peiling</first><last>Lu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tianyu</first><last>He</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Xu</first><last>Tan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Wei</first><last>Ye</last><affiliation>Peking University</affiliation></author>
      <author><first>Shikun</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Jiang</first><last>Bian</last><affiliation>Microsoft Research</affiliation></author>
      <pages>246-255</pages>
      <abstract>AI-empowered music processing is a diverse feld that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classifcation). For developers and amateurs, it is very diffcult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience. The code is available on GitHub along with a brief instructional video.</abstract>
      <url hash="038f758d">2023.emnlp-demo.21</url>
      <bibkey>yu-etal-2023-musicagent</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>S</fixed-case>ent<fixed-case>A</fixed-case>lign: Accurate and Scalable Sentence Alignment</title>
      <author><first>Steinthor</first><last>Steingrimsson</last><affiliation>The Arni Magnusson Institute for Icelandic Studies</affiliation></author>
      <author><first>Hrafn</first><last>Loftsson</last><affiliation>Reykjavik University</affiliation></author>
      <author><first>Andy</first><last>Way</last><affiliation>Lingo24</affiliation></author>
      <pages>256-263</pages>
      <abstract>We present SentAlign, an accurate sentence alignment tool designed to handle very large parallel document pairs. Given user-defined parameters, the alignment algorithm evaluates all possible alignment paths in fairly large documents of thousands of sentences and uses a divide-and-conquer approach to align documents containing tens of thousands of sentences. The scoring function is based on LaBSE bilingual sentence representations. SentAlign outperforms five other sentence alignment tools when evaluated on two different evaluation sets, German-French and English-Icelandic, and on a downstream machine translation task.</abstract>
      <url hash="51027d32">2023.emnlp-demo.22</url>
      <bibkey>steingrimsson-etal-2023-sentalign</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>QAC</fixed-case>heck: A Demonstration System for Question-Guided Multi-Hop Fact-Checking</title>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of California, Santa Barbara (UCSB)</affiliation></author>
      <author><first>Xinyuan</first><last>Lu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>264-273</pages>
      <abstract>Fact-checking real-world claims often requires intricate, multi-step reasoning due to the absence of direct evidence to support or refute them. However, existing fact-checking systems often lack transparency in their decision-making, making it challenging for users to comprehend their reasoning process. To address this, we propose the Question-guided Multi-hop Fact-Checking (QACheck) system, which guides the model’s reasoning process by asking a series of questions critical for verifying a claim. QACheck has five key modules: a claim verifier, a question generator, a question-answering module, a QA validator, and a reasoner. Users can input a claim into QACheck, which then predicts its veracity and provides a comprehensive report detailing its reasoning process, guided by a sequence of (question, answer) pairs. QACheck also provides the source of evidence supporting each question, fostering a transparent, explainable, and user-friendly fact-checking process.</abstract>
      <url hash="adb2406e">2023.emnlp-demo.23</url>
      <bibkey>pan-etal-2023-qacheck</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>R</fixed-case>obust<fixed-case>QA</fixed-case>: A Framework for Adversarial Text Generation Analysis on Question Answering Systems</title>
      <author><first>Yasaman</first><last>Boreshban</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Seyed Morteza</first><last>Mirbostani</last><affiliation>University of Guilan</affiliation></author>
      <author><first>Seyedeh Fatemeh</first><last>Ahmadi</last><affiliation>University of Guilan</affiliation></author>
      <author><first>Gita</first><last>Shojaee</last><affiliation>University of Guilan</affiliation></author>
      <author><first>Fatemeh</first><last>Kamani</last><affiliation>University of Guilan</affiliation></author>
      <author><first>Gholamreza</first><last>Ghassem-Sani</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Seyed Abolghasem</first><last>Mirroshandel</last><affiliation>Stony Brook University</affiliation></author>
      <pages>274-285</pages>
      <abstract>Question answering (QA) systems have reached human-level accuracy; however, these systems are not robust enough and are vulnerable to adversarial examples. Recently, adversarial attacks have been widely investigated in text classification. However, there have been few research efforts on this topic in QA. In this article, we have modified the attack algorithms widely used in text classification to fit those algorithms for QA systems. We have evaluated the impact of various attack methods on QA systems at character, word, and sentence levels. Furthermore, we have developed a new framework, named RobustQA, as the first open-source toolkit for investigating textual adversarial attacks in QA systems. RobustQA consists of seven modules: Tokenizer, Victim Model, Goals, Metrics, Attacker, Attack Selector, and Evaluator. It currently supports six different attack algorithms. Furthermore, the framework simplifies the development of new attack algorithms in QA. The source code and documentation of RobustQA are available at https://github.com/mirbostani/RobustQA.</abstract>
      <url hash="5dd43bdf">2023.emnlp-demo.24</url>
      <bibkey>boreshban-etal-2023-robustqa</bibkey>
    </paper>
    <paper id="25">
      <title>Kandinsky: An Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion</title>
      <author><first>Anton</first><last>Razzhigaev</last><affiliation>Skoltech</affiliation></author>
      <author><first>Arseniy</first><last>Shakhmatov</last><affiliation>Sber AI</affiliation></author>
      <author><first>Anastasia</first><last>Maltseva</last><affiliation>Sber AI</affiliation></author>
      <author><first>Vladimir</first><last>Arkhipkin</last><affiliation>Sber AI</affiliation></author>
      <author><first>Igor</first><last>Pavlov</last><affiliation>Sber AI</affiliation></author>
      <author><first>Ilya</first><last>Ryabov</last><affiliation>Sber AI</affiliation></author>
      <author><first>Angelina</first><last>Kuts</last><affiliation>Sber AI</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skolkovo Institue of Science and Technology</affiliation></author>
      <author><first>Andrey</first><last>Kuznetsov</last><affiliation>AIRI</affiliation></author>
      <author><first>Denis</first><last>Dimitrov</last><affiliation>Sber AI</affiliation></author>
      <pages>286-295</pages>
      <abstract>Text-to-image generation is a significant domain in modern computer vision and achieved substantial improvements through the evolution of generative architectures. Among these, diffusion-based models demonstrated essential quality enhancements. These models generally split into two categories: pixel-level and latent-level approaches. We present Kandinsky – a novel exploration of latent diffusion architecture, combining the principles of image prior models with latent diffusion techniques. The image prior model, is trained separately to map CLIP text and image embeddings. Another distinct feature of the proposed model is the modified MoVQ implementation, which serves as the image autoencoder component. Overall the designed model contains 3.3B parameters. We also deployed a user-friendly demo system that supports diverse generative modes such as text-to-image generation, image fusion, text and image fusion, image variations generation and text-guided inpainting/outpainting. Additionally we released the source code and checkpoints for Kandinsky models. Experimental evaluations demonstrate FID score of 8.03 on the COCO-30K dataset, marking our model as the top open source performer in terms of measurable image generation quality.</abstract>
      <url hash="1e2a80ca">2023.emnlp-demo.25</url>
      <bibkey>razzhigaev-etal-2023-kandinsky</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>N</fixed-case>ews<fixed-case>R</fixed-case>ec<fixed-case>L</fixed-case>ib: A <fixed-case>P</fixed-case>y<fixed-case>T</fixed-case>orch-Lightning Library for Neural News Recommendation</title>
      <author><first>Andreea</first><last>Iana</last><affiliation>University of Mannheim</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Center For Artificial Intelligence and Data Science, University of Würzburg</affiliation></author>
      <author><first>Heiko</first><last>Paulheim</last><affiliation>University of Mannheim</affiliation></author>
      <pages>296-310</pages>
      <abstract>NewsRecLib is an open-source library based on Pytorch-Lightning and Hydra developed for training and evaluating neural news recommendation models. The foremost goals of NewsRecLib are to promote reproducible research and rigorous experimental evaluation by (i) providing a unified and highly configurable framework for exhaustive experimental studies and (ii) enabling a thorough analysis of the performance contribution of different model architecture components and training regimes. NewsRecLib is highly modular, allows specifying experiments in a single configuration file, and includes extensive logging facilities. Moreover, NewsRecLib provides out-of-the-box implementations of several prominent neural models, training methods, standard evaluation benchmarks, and evaluation metrics for news recommendation.</abstract>
      <url hash="2a7b890d">2023.emnlp-demo.26</url>
      <bibkey>iana-etal-2023-newsreclib</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>M</fixed-case>ini<fixed-case>C</fixed-case>hain: A Small Library for Coding with Large Language Models</title>
      <author><first>Alexander</first><last>Rush</last><affiliation>Cornell University</affiliation></author>
      <pages>311-317</pages>
      <abstract>Programming augmented by large language models (LLMs) opens up many new application areas, but also requires care. LLMs are accurate enough, on average, to replace core functionality, yet make basic mistakes that demonstrate a lack of robustness. An ecosystem of prompting tools, from intelligent agents to new programming languages, have emerged with different solutions for patching LLMs with other tools. In this work, we introduce MiniChain, an opinionated tool for LLM augmented programming, with the design goals of ease-of-use of prototyping, transparency through automatic visualization, and a minimalistic approach to advanced features. The MiniChain library provides core primitives for coding LLM calls, separating out prompt templates, and capturing program structure. The library includes demo implementations of the main applications papers in the area, including chat-bots, code generation, retrieval-based question answering, and complex information extraction. The library is open-source and available at https://github.com/srush/MiniChain, with code demos available at https://srush-minichain.hf.space/, and video demo at https://www.youtube.com/watch?v=VszZ1VnO7sk.</abstract>
      <url hash="53c997f8">2023.emnlp-demo.27</url>
      <bibkey>rush-2023-minichain</bibkey>
    </paper>
    <paper id="28">
      <title>Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback</title>
      <author><first>Viet</first><last>Lai</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Chien</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Nghia</first><last>Ngo</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Thuat</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Ryan</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Thien</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <pages>318-327</pages>
      <abstract>A key technology for large language models (LLMs) involves instruction tuning that helps align the models’ responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are applied to produce the best commercial LLMs. To improve the accessibility of LLMs, various instruction-tuned open-source LLMs have also been introduced recently. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their accessibility to many other languages in the world. In addition, SFT has been used as the only approach to instruction-tune open-source LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework with created resources, fine-tuned LLMs, interaction scripts are released at https://github.com/nlp-uoregon/Okapi. A demo video to show our framework can also be found at: https://youtu.be/QFV2fkPwvi0.</abstract>
      <url hash="e14cbe66">2023.emnlp-demo.28</url>
      <bibkey>lai-etal-2023-okapi</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>SAGEV</fixed-case>iz: <fixed-case>S</fixed-case>chem<fixed-case>A</fixed-case> <fixed-case>GE</fixed-case>neration and Visualization</title>
      <author><first>Sugam</first><last>Devare</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Mahnaz</first><last>Koupaee</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Gautham</first><last>Gunapati</last><affiliation>Mr</affiliation></author>
      <author><first>Sayontan</first><last>Ghosh</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Sai</first><last>Vallurupalli</last><affiliation>University of Maryland at Baltimore County</affiliation></author>
      <author><first>Yash Kumar</first><last>Lal</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Francis</first><last>Ferraro</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author><first>Nathanael</first><last>Chambers</last><affiliation>US Naval Academy</affiliation></author>
      <author><first>Greg</first><last>Durrett</last><affiliation>UT Austin</affiliation></author>
      <author><first>Raymond</first><last>Mooney</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Katrin</first><last>Erk</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>Stony Brook University</affiliation></author>
      <pages>328-335</pages>
      <abstract>Schema induction involves creating a graph representation depicting how events unfold in a scenario. We present SAGEViz, an intuitive and modular tool that utilizes human-AI collaboration to create and update complex schema graphs efficiently, where multiple annotators (humans and models) can work simultaneously on a schema graph from any domain. The tool consists of two components: (1) a curation component powered by plug-and-play event language models to create and expand event sequences while human annotators validate and enrich the sequences to build complex hierarchical schemas, and (2) an easy-to-use visualization component to visualize schemas at varying levels of hierarchy. Using supervised and few-shot approaches, our event language models can continually predict relevant events starting from a seed event. We conduct a user study and show that users need less effort in terms of interaction steps with SAGEViz to generate schemas of better quality. We also include a video demonstrating the system.</abstract>
      <url hash="f3194bbf">2023.emnlp-demo.29</url>
      <bibkey>devare-etal-2023-sageviz</bibkey>
    </paper>
    <paper id="30">
      <title>Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation</title>
      <author><first>David</first><last>Heineman</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yao</first><last>Dou</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>336-345</pages>
      <abstract>Fine-grained, span-level human evaluation has emerged as a reliable and robust method for evaluating text generation tasks such as summarization, simplification, machine translation and news generation, and the derived annotations have been useful for training automatic metrics and improving language models. However, existing annotation tools implemented for these evaluation frameworks lack the adaptability to be extended to different domains or languages, or modify annotation settings according to user needs; and, the absence of a unified annotated data format inhibits the research in multi-task learning. In this paper, we introduce Thresh, a unified, customizable and deployable platform for fine-grained evaluation. With a single YAML configuration file, users can build and test an annotation interface for any framework within minutes – all in one web browser window. To facilitate collaboration and sharing, Thresh provides a community hub that hosts a collection of fine-grained frameworks and corresponding annotations made and collected by the community, covering a wide range of NLP tasks. For deployment, Thresh offers multiple options for any scale of annotation projects from small manual inspections to large crowdsourcing ones. Additionally, we introduce a Python library to streamline the entire process from typology design and deployment to annotation processing. Thresh is publicly accessible at https://thresh.tools.</abstract>
      <url hash="d57bf05d">2023.emnlp-demo.30</url>
      <bibkey>heineman-etal-2023-thresh</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>I</fixed-case>nsight<fixed-case>P</fixed-case>ilot: An <fixed-case>LLM</fixed-case>-Empowered Automated Data Exploration System</title>
      <author><first>Pingchuan</first><last>Ma</last><affiliation>HKUST</affiliation></author>
      <author><first>Rui</first><last>Ding</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Shuai</first><last>Wang</last><affiliation>HKUST</affiliation></author>
      <author><first>Shi</first><last>Han</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Dongmei</first><last>Zhang</last><affiliation>Microsoft Research</affiliation></author>
      <pages>346-352</pages>
      <abstract>Exploring data is crucial in data analysis, as it helps users understand and interpret the data more effectively. However, performing effective data exploration requires in-depth knowledge of the dataset, the user intent and expertise in data analysis techniques. Not being familiar with either can create obstacles that make the process time-consuming and overwhelming. To address this issue, we introduce InsightPilot, an LLM (Large Language Model)-based, automated data exploration system designed to simplify the data exploration process. InsightPilot features a set of carefully designed analysis actions that streamline the data exploration process. Given a natural language question, InsightPilot collaborates with the LLM to issue a sequence of analysis actions, explore the data and generate insights. We demonstrate the effectiveness of InsightPilot in a user study and a case study, showing how it can help users gain valuable insights from their datasets.</abstract>
      <url hash="9db8cc40">2023.emnlp-demo.31</url>
      <bibkey>ma-etal-2023-insightpilot</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>S</fixed-case>yn<fixed-case>J</fixed-case>ax: Structured Probability Distributions for <fixed-case>JAX</fixed-case></title>
      <author><first>Miloš</first><last>Stanojević</last><affiliation>DeepMind</affiliation></author>
      <author><first>Laurent</first><last>Sartran</last><affiliation>DeepMind</affiliation></author>
      <pages>353-364</pages>
      <abstract>The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form. SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. This is done by exploiting the connection between algorithms for automatic differentiation and probabilistic inference. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://github.com/google-deepmind/synjax</abstract>
      <url hash="68d895ff">2023.emnlp-demo.32</url>
      <bibkey>stanojevic-sartran-2023-synjax</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>RESIN</fixed-case>-<fixed-case>EDITOR</fixed-case>: A Schema-guided Hierarchical Event Graph Visualizer and Editor</title>
      <author><first>Khanh Duy</first><last>Nguyen</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Zixuan</first><last>Zhang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Reece</first><last>Suchocki</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Sha</first><last>Li</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado</affiliation></author>
      <author><first>Susan Windisch</first><last>Brown</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Jiawei</first><last>Han</last><affiliation>UIUC</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois at Urbana-Champaign and Amazon (Amazon Scholar)</affiliation></author>
      <pages>365-372</pages>
      <abstract>In this paper, we present RESIN-EDITOR, an interactive event graph visualizer and editor designed for analyzing complex events. Our RESIN-EDITOR system allows users to render and freely edit hierarchical event graphs extracted from multimedia and multi-document news clusters with guidance from human-curated event schemas. RESIN-EDITOR’s unique features include hierarchical graph visualization, comprehensive source tracing, and interactive user editing, which significantly outperforms existing Information Extraction (IE) visualization tools in both IE result analysis and general model improvements. In our evaluation of RESIN-EDITOR, we demonstrate ways in which our tool is effective in understanding complex events and enhancing system performances. The source code, a video demonstration, and a live website for RESIN-EDITOR have been made publicly available.</abstract>
      <url hash="150a9e85">2023.emnlp-demo.33</url>
      <bibkey>nguyen-etal-2023-resin</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>DRGC</fixed-case>oder: Explainable Clinical Coding for the Early Prediction of Diagnostic-Related Groups</title>
      <author><first>Daniel</first><last>Hajialigol</last><affiliation>VirginiaTech</affiliation></author>
      <author><first>Derek</first><last>Kaknes</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Tanner</first><last>Barbour</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Daphne</first><last>Yao</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Chris</first><last>North</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Jimeng</first><last>Sun</last><affiliation>University of Illinois</affiliation></author>
      <author><first>David</first><last>Liem</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Xuan</first><last>Wang</last><affiliation>Virginia Tech</affiliation></author>
      <pages>373-380</pages>
      <abstract>Medical claim coding is the process of transforming medical records, usually presented as free texts written by clinicians, or discharge summaries, into structured codes in a classification system such as ICD-10 (International Classification of Diseases, Tenth Revision) or DRG (Diagnosis-Related Group) codes. This process is essential for medical billing and transitional care; however, manual coding is time-consuming, error-prone, and expensive. To solve these issues, we propose DRGCoder, an explainability-enhanced clinical claim coding system for the early prediction of medical severity DRGs (MS-DRGs), a classification system that categorizes patients’ hospital stays into various DRG groups based on the severity of illness and mortality risk. The DRGCoder framework introduces a novel multi-task Transformer model for MS-DRG prediction, modeling both the DRG labels of the discharge summaries and the important, or salient words within he discharge summaries. We allow users to inspect DRGCoder’s reasoning by visualizing the weights for each word of the input. Additionally, DRGCoder allows users to identify diseases within discharge summaries and compare across multiple discharge summaries. Our demo is available at https://huggingface.co/spaces/danielhajialigol/DRGCoder. A video demonstrating the demo can be found at https://www.youtube.com/watch?v=pcdiG6VwqlA</abstract>
      <url hash="96e91bf4">2023.emnlp-demo.34</url>
      <bibkey>hajialigol-etal-2023-drgcoder</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>CAMRA</fixed-case>: Copilot for <fixed-case>AMR</fixed-case> Annotation</title>
      <author><first>Jon</first><last>Cai</last><affiliation>The University of Colorado</affiliation></author>
      <author><first>Shafiuddin Rehan</first><last>Ahmed</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Julia</first><last>Bonn</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Kristin</first><last>Wright-Bettner</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado</affiliation></author>
      <author><first>James H.</first><last>Martin</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>381-388</pages>
      <abstract>In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a cutting-edge web-based tool designed for constructing Abstract Meaning Representation (AMR) from natural language text. CAMRA offers a novel approach to deep lexical semantics annotation such as AMR, treating AMR annotation akin to coding in programming languages. Leveraging the familiarity of programming paradigms, CAMRA encompasses all essential features of existing AMR editors, including example lookup, while going a step further by integrating Propbank roleset lookup as an autocomplete feature within the tool. Notably, CAMRA incorporates AMR parser models as coding co-pilots, greatly enhancing the efficiency and accuracy of AMR annotators.</abstract>
      <url hash="edee925f">2023.emnlp-demo.35</url>
      <bibkey>cai-etal-2023-camra</bibkey>
    </paper>
    <paper id="36">
      <title>Reaction Miner: An Integrated System for Chemical Reaction Extraction from Textual Data</title>
      <author><first>Ming</first><last>Zhong</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Siru</first><last>Ouyang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Yizhu</first><last>Jiao</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Priyanka</first><last>Kargupta</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Leo</first><last>Luo</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Yanzhen</first><last>Shen</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Bobby</first><last>Zhou</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Xianrui</first><last>Zhong</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Xuan</first><last>Liu</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Hongxiang</first><last>Li</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Jinfeng</first><last>Xiao</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Minhao</first><last>Jiang</last><affiliation>University of Illinois at Urbana Champaign</affiliation></author>
      <author><first>Vivian</first><last>Hu</last><affiliation>UIUC</affiliation></author>
      <author><first>Xuan</first><last>Wang</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois at Urbana-Champaign and Amazon (Amazon Scholar)</affiliation></author>
      <author><first>Martin</first><last>Burke</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Huimin</first><last>Zhao</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Jiawei</first><last>Han</last><affiliation>UIUC</affiliation></author>
      <pages>389-402</pages>
      <abstract>Chemical reactions, as a core entity in the realm of chemistry, hold crucial implications in diverse areas ranging from hands-on laboratory research to advanced computational drug design. Despite a burgeoning interest in employing NLP techniques to extract these reactions, aligning this task with the real-world requirements of chemistry practitioners remains an ongoing challenge. In this paper, we present Reaction Miner, a system specifically designed to interact with raw scientific literature, delivering precise and more informative chemical reactions. Going beyond mere extraction, Reaction Miner integrates a holistic workflow: it accepts PDF files as input, bypassing the need for pre-processing and bolstering user accessibility. Subsequently, a text segmentation module ensures that the refined text encapsulates complete chemical reactions, augmenting the accuracy of extraction. Moreover, Reaction Miner broadens the scope of existing pre-defined reaction roles, including vital attributes previously neglected, thereby offering a more comprehensive depiction of chemical reactions. Evaluations conducted by chemistry domain users highlight the efficacy of each module in our system, demonstrating Reaction Miner as a powerful tool in this field.</abstract>
      <url hash="bf41a54b">2023.emnlp-demo.36</url>
      <bibkey>zhong-etal-2023-reaction</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>CHAMP</fixed-case>: Efficient Annotation and Consolidation of Cluster Hierarchies</title>
      <author><first>Arie</first><last>Cattan</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Tom</first><last>Hope</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Doug</first><last>Downey</last><affiliation>Allen Institute for AI, Northwestern University</affiliation></author>
      <author><first>Roy</first><last>Bar-Haim</last><affiliation>IBM Research</affiliation></author>
      <author><first>Lilach</first><last>Eden</last><affiliation>IBM Research</affiliation></author>
      <author><first>Yoav</first><last>Kantor</last><affiliation>IBM Research</affiliation></author>
      <author><first>Ido</first><last>Dagan</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>403-412</pages>
      <abstract>Various NLP tasks require a complex hierarchical structure over nodes, where each node is a cluster of items. Examples include generating entailment graphs, hierarchical cross-document coreference resolution, annotating event and subevent relations, etc. To enable efficient annotation of such hierarchical structures, we release CHAMP, an open source tool allowing to incrementally construct both clusters and hierarchy simultaneously over any type of texts. This incremental approach significantly reduces annotation time compared to the common pairwise annotation approach and also guarantees maintaining transitivity at the cluster and hierarchy levels. Furthermore, CHAMP includes a consolidation mode, where an adjudicator can easily compare multiple cluster hierarchy annotations and resolve disagreements.</abstract>
      <url hash="9715a0e8">2023.emnlp-demo.37</url>
      <bibkey>cattan-etal-2023-champ</bibkey>
    </paper>
    <paper id="38">
      <title><fixed-case>P</fixed-case>rompt2<fixed-case>M</fixed-case>odel: Generating Deployable Models from Natural Language Instructions</title>
      <author><first>Vijay</first><last>Viswanathan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Chenyang</first><last>Zhao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Amanda</first><last>Bertsch</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Tongshuang</first><last>Wu</last><affiliation>Carniege Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>413-421</pages>
      <abstract>Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model. Our demo video is posted at <url>youtu.be/LYYQ_EhGd-Q</url>.</abstract>
      <url hash="6adf42be">2023.emnlp-demo.38</url>
      <bibkey>viswanathan-etal-2023-prompt2model</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>N</fixed-case>ews<fixed-case>S</fixed-case>ense: Reference-free Verification via Cross-document Comparison</title>
      <author><first>Jeremiah</first><last>Milbauer</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Ziqi</first><last>Ding</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zhijin</first><last>Wu</last><affiliation>CarnegieMellonUniversity</affiliation></author>
      <author><first>Tongshuang</first><last>Wu</last><affiliation>Carniege Mellon University</affiliation></author>
      <pages>422-430</pages>
      <abstract>We present NewsSense, a novel sensemaking tool and reading interface designed to collect and integrate information from multiple news articles on a central topic. NewsSense provides “reference-free verification,” augmenting a central grounding article of the user’s choice by: (1) linking to related articles from different sources; and (2) providing inline highlights on how specific claims are either supported or contradicted by information from other articles. Using NewsSense, users can seamlessly digest and cross-check multiple information sources without disturbing their natural reading flow. Our pilot study shows that NewsSense has the potential to help users identify key information, verify the credibility of news articles, explore different perspectives, and understand what content is supported, contradicted, or missing.</abstract>
      <url hash="778aa619">2023.emnlp-demo.39</url>
      <bibkey>milbauer-etal-2023-newssense</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>N</fixed-case>e<fixed-case>M</fixed-case>o Guardrails: A Toolkit for Controllable and Safe <fixed-case>LLM</fixed-case> Applications with Programmable Rails</title>
      <author><first>Traian</first><last>Rebedea</last><affiliation>University Politehnica of Bucharest &amp; NVIDIA</affiliation></author>
      <author><first>Razvan</first><last>Dinu</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Makesh Narsimhan</first><last>Sreedhar</last><affiliation>University of Wisconsin-Madison</affiliation></author>
      <author><first>Christopher</first><last>Parisien</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Jonathan</first><last>Cohen</last><affiliation>NVIDIA</affiliation></author>
      <pages>431-445</pages>
      <abstract>NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Using a runtime inspired from dialogue management, NeMo Guardrails provides a different approach by allowing developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.</abstract>
      <url hash="e9bc0717">2023.emnlp-demo.40</url>
      <bibkey>rebedea-etal-2023-nemo</bibkey>
    </paper>
    <paper id="41">
      <title><fixed-case>LM</fixed-case>-Polygraph: Uncertainty Estimation for Language Models</title>
      <author><first>Ekaterina</first><last>Fadeeva</last><affiliation>Skoltech and HSE</affiliation></author>
      <author><first>Roman</first><last>Vashurin</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author><first>Akim</first><last>Tsvigun</last><affiliation>AIRI / Semrush / HSE</affiliation></author>
      <author><first>Artem</first><last>Vazhentsev</last><affiliation>AIRI, Skoltech</affiliation></author>
      <author><first>Sergey</first><last>Petrakov</last><affiliation>Skolkovo institute of science and technology</affiliation></author>
      <author><first>Kirill</first><last>Fedyanin</last><affiliation>skoltech.ru</affiliation></author>
      <author><first>Daniil</first><last>Vasilev</last><affiliation>HSE</affiliation></author>
      <author><first>Elizaveta</first><last>Goncharova</last><affiliation>NRU HSE</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skolkovo Institue of Science and Technology</affiliation></author>
      <author><first>Maxim</first><last>Panov</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>MBZUAI</affiliation></author>
      <author><first>Artem</first><last>Shelmanov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence: MBZUAI</affiliation></author>
      <pages>446-461</pages>
      <abstract>Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often “hallucinate”, i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.</abstract>
      <url hash="b7f7d010">2023.emnlp-demo.41</url>
      <attachment type="software" hash="95c9a77f">2023.emnlp-demo.41.software.zip</attachment>
      <bibkey>fadeeva-etal-2023-lm</bibkey>
    </paper>
    <paper id="42">
      <title>Descriptive Knowledge Graph in Biomedical Domain</title>
      <author><first>Kerui</first><last>Zhu</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Jie</first><last>Huang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Kevin Chen-Chuan</first><last>Chang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>462-470</pages>
      <abstract>We present a novel system that automatically extracts and generates informative and descriptive sentences from the biomedical corpus and facilitates the efficient search for relational knowledge. Unlike previous search engines or exploration systems that retrieve unconnected passages, our system organizes descriptive sentences as a relational graph, enabling researchers to explore closely related biomedical entities (e.g., diseases treated by a chemical) or indirectly connected entities (e.g., potential drugs for treating a disease). Our system also uses ChatGPT and a fine-tuned relation synthesis model to generate concise and reliable descriptive sentences from retrieved information, reducing the need for extensive human reading effort. With our system, researchers can easily obtain both high-level knowledge and detailed references and interactively steer to the information of interest. We spotlight the application of our system in COVID-19 research, illustrating its utility in areas such as drug repurposing and literature curation.</abstract>
      <url hash="68d20a53">2023.emnlp-demo.42</url>
      <bibkey>zhu-etal-2023-descriptive</bibkey>
    </paper>
    <paper id="43">
      <title>Prompterator: Iterate Efficiently towards More Effective Prompts</title>
      <author><first>Samuel</first><last>Sučik</last><affiliation>Slido</affiliation></author>
      <author><first>Daniel</first><last>Skala</last><affiliation>Slido</affiliation></author>
      <author><first>Andrej</first><last>Švec</last><affiliation>Slido</affiliation></author>
      <author><first>Peter</first><last>Hraška</last><affiliation>Slido</affiliation></author>
      <author><first>Marek</first><last>Šuppa</last><affiliation>Comenius University in Bratislava</affiliation></author>
      <pages>471-478</pages>
      <abstract>With the advent of Large Language Models (LLMs) the process known as prompting, which entices the LLM to solve an arbitrary language processing task without the need for finetuning, has risen to prominence. Finding well-performing prompts, however, is a non-trivial task which requires experimentation in order to arrive at a prompt that solves a specific task. When a given task does not readily reduce to one that can be easily measured with well established metrics, human evaluation of the results obtained by prompting is often necessary. In this work we present prompterator, a tool that helps the user interactively iterate over various potential prompts and choose the best performing one based on human feedback. It is distributed as an open source package with out-of-the-box support for various LLM providers and was designed to be easily extensible.</abstract>
      <url hash="8d83b790">2023.emnlp-demo.43</url>
      <bibkey>sucik-etal-2023-prompterator</bibkey>
    </paper>
    <paper id="44">
      <title><fixed-case>Z</fixed-case>hu<fixed-case>J</fixed-case>iu: A Multi-dimensional, Multi-faceted <fixed-case>C</fixed-case>hinese Benchmark for Large Language Models</title>
      <author><first>Baoli</first><last>Zhang</last><affiliation>National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Haining</first><last>Xie</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Pengfan</first><last>Du</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Junhao</first><last>Chen</last><affiliation>College of Computer Science and Technology, Harbin Engineering University</affiliation></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shengping</first><last>Liu</last><affiliation>Unisound AI Labs</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>NLPR, Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>479-494</pages>
      <abstract>The unprecedented performance of LLMs requires comprehensive and accurate evaluation. We argue that for LLMs evaluation, benchmarks need to be comprehensive and systematic. To this end, we propose the Zhujiu benchmark, which has the following strengths: (1) Multi-dimensional ability coverage: We comprehensively evaluate LLMs across 7 ability dimensions covering 51 tasks. Especially, we also propose a new benchmark that focus on knowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration: We use 3 different yet complementary evaluation methods to comprehensively evaluate LLMs, which can ensure the authority and accuracy of the evaluation results. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering benchmark that fully assesses LLMs in Chinese, while also providing equally robust evaluation abilities in English. (4) Avoiding potential data leakage: To avoid data leakage, we construct evaluation data specifically for 37 tasks. We evaluate 10 current mainstream LLMs, and conduct an in-depth discussion and analysis of their results. The ZhuJiu benchmark and open-participation leaderboard are publicly released at <url>http://www.zhujiu-benchmark.com</url> and we also provide a demo video at <url>https://youtu.be/qypkJ89L1Ic.</url>
      </abstract>
      <url hash="28e65a26">2023.emnlp-demo.44</url>
      <bibkey>zhang-etal-2023-zhujiu</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>P</fixed-case>aper<fixed-case>M</fixed-case>age: A Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents</title>
      <author><first>Kyle</first><last>Lo</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Zejiang</first><last>Shen</last><affiliation>MIT</affiliation></author>
      <author><first>Benjamin</first><last>Newman</last><affiliation>Stanford University</affiliation></author>
      <author><first>Joseph</first><last>Chang</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Russell</first><last>Authur</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Erin</first><last>Bransom</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Stefan</first><last>Candra</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Yoganand</first><last>Chandrasekhar</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Regan</first><last>Huff</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Bailey</first><last>Kuehl</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Amanpreet</first><last>Singh</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Chris</first><last>Wilhelm</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Angele</first><last>Zamarron</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Marti A.</first><last>Hearst</last><affiliation>UC Berkeley</affiliation></author>
      <author><first>Daniel</first><last>Weld</last><affiliation>University of Washington &amp; Allen Institute for Artificial Inelligence</affiliation></author>
      <author><first>Doug</first><last>Downey</last><affiliation>Allen Institute for AI, Northwestern University</affiliation></author>
      <author><first>Luca</first><last>Soldaini</last><affiliation>Allen Institute for AI</affiliation></author>
      <pages>495-507</pages>
      <abstract>Despite growing interest in applying natural language processing (NLP) and computer vision (CV) models to the scholarly domain, scientific documents remain challenging to work with. They’re often in difficult-to-use PDF formats, and the ecosystem of models to process them is fragmented and incomplete. We introduce PaperMage, an open-source Python toolkit for analyzing and processing visually-rich, structured scientific documents. PaperMage offers clean and intuitive abstractions for seamlessly representing and manipulating both textual and visual document elements. PaperMage achieves this by integrating disparate state-of-the-art NLP and CV models into a unified framework, and provides turn-key recipes for common scientific document processing use-cases. PaperMage has powered multiple research prototypes of AI applications over scientific documents, along with Semantic Scholar’s large-scale production system for processing millions of PDFs. GitHub: https://github.com/allenai/papermage</abstract>
      <url hash="33549adb">2023.emnlp-demo.45</url>
      <bibkey>lo-etal-2023-papermage</bibkey>
    </paper>
    <paper id="46">
      <title><fixed-case>O</fixed-case>mni<fixed-case>E</fixed-case>vent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding</title>
      <author><first>Hao</first><last>Peng</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xiaozhi</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Feng</first><last>Yao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zimu</first><last>Wang</last><affiliation>Xi’an Jiaotong-Liverpool University</affiliation></author>
      <author><first>Chuzhao</first><last>Zhu</last><affiliation>Department of Computer Science and Technology in Tsinghua University</affiliation></author>
      <author><first>Kaisheng</first><last>Zeng</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <pages>508-517</pages>
      <abstract>Event understanding aims at understanding the content and relationship of events within texts, which covers multiple complicated information extraction tasks: event detection, event argument extraction, and event relation extraction. To facilitate related research and application, we present an event understanding toolkit OmniEvent, which features three desiderata: (1) Comprehensive. OmniEvent supports mainstream modeling paradigms of all the event understanding tasks and the processing of 15 widely-used English and Chinese datasets. (2) Fair. OmniEvent carefully handles the inconspicuous evaluation pitfalls reported in Peng et al. (2023), which ensures fair comparisons between different models. (3) Easy-to-use. OmniEvent is designed to be easily used by users with varying needs. We provide off-the-shelf models that can be directly deployed as web services. The modular framework also enables users to easily implement and evaluate new event understanding models with OmniEvent. The toolkit is publicly released along with the demonstration website and video.</abstract>
      <url hash="44594f40">2023.emnlp-demo.46</url>
      <attachment type="software" hash="9183680e">2023.emnlp-demo.46.software.zip</attachment>
      <bibkey>peng-etal-2023-omnievent</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>C</fixed-case>oco<fixed-case>S</fixed-case>ci<fixed-case>S</fixed-case>um: A Scientific Summarization Toolkit with Compositional Controllability</title>
      <author><first>Yixi</first><last>Ding</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yanxia</first><last>Qin</last><affiliation>School of Computing, National University of Singapore</affiliation></author>
      <author><first>Qian</first><last>Liu</last><affiliation>Sea AI Lab</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <pages>518-526</pages>
      <abstract>We present a novel toolkit for controlled summarization of scientific documents, designed for the specific needs of the scientific community. Our system generates summaries based on user preferences, adjusting key attributes specifically of length and keyword inclusion. A distinguishing feature is its ability to manage multiple attributes concurrently, demonstrating Compositional Controllability for Scientific Summarization (CocoSciSum). Benchmarked against the strong Flan-T5 baseline, CocoSciSum exhibits superior performance on both the quality of summaries generated and the control over single and multiple attributes. Moreover, CocoSciSum is a user-centric toolkit, supporting user preferences expressed in natural language instructions, and accommodating diverse input document formats. CocoSciSum is available on GitHub (https://github.com/WING-NUS/SciAssist/tree/CocoSciSum) with an introduction video (https://youtu.be/YC1YDeEjAbQ).</abstract>
      <url hash="cd93a5ea">2023.emnlp-demo.47</url>
      <bibkey>ding-etal-2023-cocoscisum</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>C</fixed-case>o<fixed-case>LL</fixed-case>i<fixed-case>E</fixed-case>: Collaborative Training of Large Language Models in an Efficient Way</title>
      <author><first>Kai</first><last>Lv</last><affiliation>Fudan University</affiliation></author>
      <author><first>Shuo</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tianle</first><last>Gu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shuhao</first><last>Xing</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jiawei</first><last>Hong</last><affiliation>Fudan University</affiliation></author>
      <author><first>Keyu</first><last>Chen</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xiaoran</first><last>Liu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yuqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Honglin</first><last>Guo</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tengxiao</first><last>Liu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yu</first><last>Sun</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Amazon Shanghai AI Lab</affiliation></author>
      <author><first>Hang</first><last>Yan</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>527-542</pages>
      <abstract>Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, and LOMO. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie.</abstract>
      <url hash="660e64b4">2023.emnlp-demo.48</url>
      <bibkey>lv-etal-2023-collie</bibkey>
    </paper>
    <paper id="49">
      <title>Video-<fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>: An Instruction-tuned Audio-Visual Language Model for Video Understanding</title>
      <author><first>Hang</first><last>Zhang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Xin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Alibaba DAMO Academy</affiliation></author>
      <pages>543-553</pages>
      <abstract>We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual &amp; audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual &amp; audio encoders with LLM’s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.</abstract>
      <url hash="97612422">2023.emnlp-demo.49</url>
      <bibkey>zhang-etal-2023-video</bibkey>
    </paper>
    <paper id="50">
      <title><fixed-case>S</fixed-case>umm<fixed-case>H</fixed-case>elper: Collaborative Human-Computer Summarization</title>
      <author><first>Aviv</first><last>Slobodkin</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Niv</first><last>Nachum</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Shmuel</first><last>Amar</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Ori</first><last>Shapira</last><affiliation>Amazon</affiliation></author>
      <author><first>Ido</first><last>Dagan</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>554-565</pages>
      <abstract>Current approaches for text summarization are predominantly automatic, with rather limited space for human intervention and control over the process. In this paper, we introduce SummHelper, and screencast demo at <url>https://www.youtube.com/watch?v=nGcknJwGhxk</url> a 2-phase summarization assistant designed to foster human-machine collaboration. The initial phase involves content selection, where the system recommends potential content, allowing users to accept, modify, or introduce additional selections. The subsequent phase, content consolidation, involves SummHelper generating a coherent summary from these selections, which users can then refine using visual mappings between the summary and the source text. Small-scale user studies reveal the effectiveness of our application, with participants being especially appreciative of the balance between automated guidance and opportunities for personal input.</abstract>
      <url hash="c1e504e5">2023.emnlp-demo.50</url>
      <bibkey>slobodkin-etal-2023-summhelper</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>M</fixed-case>odel<fixed-case>S</fixed-case>cope-Agent: Building Your Customizable Agent System with Open-source Large Language Models</title>
      <author><first>Chenliang</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>He</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ming</first><last>Yan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weizhou</first><last>Shen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Haiyang</first><last>Xu</last><affiliation>Alibaba Damo Academy</affiliation></author>
      <author><first>Zhikai</first><last>Wu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhicheng</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wenmeng</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yingda</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chen</first><last>Cheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hongzhu</first><last>Shi</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba DAMO Academy</affiliation></author>
      <author><first>Jingren</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <pages>566-578</pages>
      <abstract>Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent frameworks that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with a customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent online demo, library are now publicly available.</abstract>
      <url hash="d80b637a">2023.emnlp-demo.51</url>
      <bibkey>li-etal-2023-modelscope</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>E</fixed-case>fficient<fixed-case>OCR</fixed-case>: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge</title>
      <author><first>Tom</first><last>Bryan</last><affiliation>Harvard University</affiliation></author>
      <author><first>Jacob</first><last>Carlson</last><affiliation>Harvard University</affiliation></author>
      <author><first>Abhishek</first><last>Arora</last><affiliation>Harvard University</affiliation></author>
      <author><first>Melissa</first><last>Dell</last><affiliation>Harvard University</affiliation></author>
      <pages>579-596</pages>
      <abstract>Billions of public domain documents remain trapped in hard copy or lack an accurate digitization. Modern natural language processing methods cannot be used to index, retrieve, and summarize their texts; conduct computational textual analyses; or extract information for statistical analyses, and these texts cannot be incorporated into language model training. Given the diversity and sheer quantity of public domain texts, liberating them at scale requires optical character recognition (OCR) that is accurate, extremely cheap to deploy, and sample-efficient to customize to novel collections, languages, and character sets. Existing OCR engines, largely designed for small-scale commercial applications in high resource languages, often fall short of these requirements. EffOCR (EfficientOCR), a novel open-source OCR package, meets both the computational and sample efficiency requirements for liberating texts at scale by abandoning the sequence-to-sequence architecture typically used for OCR, which takes representations from a learned vision model as inputs to a learned language model. Instead, EffOCR models OCR as a character or word-level image retrieval problem. EffOCR is cheap and sample efficient to train, as the model only needs to learn characters’ visual appearance and not how they are used in sequence to form language. Models in the EffOCR model zoo can be deployed off-the-shelf with only a few lines of code and include lightweight models designed for mobile phones that are extremely cheap to deploy. Importantly, EffOCR also allows for easy, sample efficient customization with a simple model training interface and minimal labeling requirements due to its sample efficiency. We illustrate the utility of EffOCR by cheaply and accurately digitizing 20 million historical U.S. newspaper scans, evaluating zero-shot performance on randomly selected documents from the U.S. National Archives, and accurately digitizing a Japanese document collection for which all other OCR solutions failed.</abstract>
      <url hash="5324a7b0">2023.emnlp-demo.52</url>
      <bibkey>bryan-etal-2023-efficientocr</bibkey>
    </paper>
  </volume>
  <event id="emnlp-2023">
    <meta>
      <title>The 2023 Conference on Empirical Methods in Natural Language Processing</title>
      <location>Singapore</location>
      <dates>December 6–10, 2023</dates>
    </meta>
    <colocated>
      <volume-id>2023.findings-emnlp</volume-id>
      <volume-id>2023.arabicnlp-1</volume-id>
      <volume-id>2023.argmining-1</volume-id>
      <volume-id>2023.lchange-1</volume-id>
      <volume-id>2023.mrl-1</volume-id>
      <volume-id>2023.newsum-1</volume-id>
      <volume-id>2023.nilli-1</volume-id>
      <volume-id>2023.nllp-1</volume-id>
      <volume-id>2023.splurobonlp-1</volume-id>
      <volume-id>2023.winlp-1</volume-id>
    </colocated>
  </event>
</collection>
