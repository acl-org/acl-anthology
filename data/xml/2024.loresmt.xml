<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.loresmt">
  <volume id="1" ingest-date="2024-07-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Seventh Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2024)</booktitle>
      <editor><first>Atul Kr.</first><last>Ojha</last></editor>
      <editor><first>Chao-hong</first><last>Liu</last></editor>
      <editor><first>Ekaterina</first><last>Vylomova</last></editor>
      <editor><first>Flammie</first><last>Pirinen</last></editor>
      <editor><first>Jade</first><last>Abbott</last></editor>
      <editor><first>Jonathan</first><last>Washington</last></editor>
      <editor><first>Nathaniel</first><last>Oco</last></editor>
      <editor><first>Valentin</first><last>Malykh</last></editor>
      <editor><first>Varvara</first><last>Logacheva</last></editor>
      <editor><first>Xiaobing</first><last>Zhao</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="21917577">2024.loresmt-1</url>
      <venue>loresmt</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="46d09781">2024.loresmt-1.0</url>
      <bibkey>loresmt-2024-technologies</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Tuning <fixed-case>LLM</fixed-case>s with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages</title>
      <author><first>Zhuoyuan</first><last>Mao</last><affiliation>Sony Group Corporation</affiliation></author>
      <author><first>Yen</first><last>Yu</last></author>
      <pages>1-25</pages>
      <abstract>This article introduces contrastive alignment instructions (AlignInstruct) to address two challenges in machine translation (MT) on large language models (LLMs). One is the expansion of supported languages to previously unseen ones. The second relates to the lack of data in low-resource languages. Model fine-tuning through MT instructions (MTInstruct) is a straightforward approach to the first challenge. However, MTInstruct is limited by weak cross-lingual signals inherent in the second challenge. AlignInstruct emphasizes cross-lingual supervision via a cross-lingual discriminator built using statistical word alignments. Our results based on fine-tuning the BLOOMZ models (1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can effectively translate unseen languages using MTInstruct; (2) AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English; (3) Discriminator-based instructions outperformed their generative counterparts as cross-lingual instructions; (4) AlignInstruct improved performance in 30 zero-shot directions.</abstract>
      <url hash="1e73f6a9">2024.loresmt-1.1</url>
      <bibkey>mao-yu-2024-tuning</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>H</fixed-case>e<fixed-case>S</fixed-case>um: a Novel Dataset for Abstractive Text Summarization in <fixed-case>H</fixed-case>ebrew</title>
      <author><first>Itai</first><last>Mondshine</last></author>
      <author><first>Tzuf</first><last>Paz-Argaman</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Asaf</first><last>Achi Mordechai</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <pages>26-36</pages>
      <abstract>While large language models (LLMs) excel in various natural language tasks in English, their performance in low-resource languages like Hebrew, especially for generative tasks such as abstractive summarization, remains unclear. The high morphological richness in Hebrew adds further challenges due to the ambiguity in sentence comprehension and the complexities in meaning construction. In this paper, we address this evaluation and resource gap by introducing HeSum, a novel benchmark dataset specifically designed for Hebrew abstractive text summarization. HeSum consists of 10,000 article-summary pairs sourced from Hebrew news websites written by professionals. Linguistic analysis confirms HeSum’s high abstractness and unique morphological challenges. We show that HeSum presents distinct difficulties even for state-of-the-art LLMs, establishing it as a valuable testbed for advancing generative language technology in Hebrew, and MRLs generative challenges in general.</abstract>
      <url hash="b0e7b353">2024.loresmt-1.2</url>
      <bibkey>mondshine-etal-2024-hesum</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>K</fixed-case>pop<fixed-case>MT</fixed-case>: Translation Dataset with Terminology for Kpop Fandom</title>
      <author><first>JiWoo</first><last>Kim</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>Yunsu</first><last>Kim</last><affiliation>aiXplain, Inc.</affiliation></author>
      <author><first>JinYeong</first><last>Bak</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>37-43</pages>
      <abstract>While machines learn from existing corpora, humans have the unique capability to establish and accept new language systems. This makes human form unique language systems within social groups. Aligning with this, we focus on a gap remaining in addressing translation challenges within social groups, where in-group members utilize unique terminologies. We propose KpopMT dataset, which aims to fill this gap by enabling precise terminology translation, choosing Kpop fandom as an initiative for social groups given its global popularity. Expert translators provide 1k English translations for Korean posts and comments, each annotated with specific terminology within social groups’ language systems. We evaluate existing translation systems including GPT models on KpopMT to identify their failure cases. Results show overall low scores, underscoring the challenges of reflecting group-specific terminologies and styles in translation. We make KpopMT publicly available.</abstract>
      <url hash="44d61731">2024.loresmt-1.3</url>
      <bibkey>kim-etal-2024-kpopmt</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.3</doi>
    </paper>
    <paper id="4">
      <title>Challenges in <fixed-case>U</fixed-case>rdu Machine Translation</title>
      <author><first>Abdul</first><last>Basit</last><affiliation>Lahore University of Management Sciences</affiliation></author>
      <author><first>Abdul Hameed</first><last>Azeemi</last><affiliation>Lahore University of Management Sciences</affiliation></author>
      <author><first>Agha Ali</first><last>Raza</last><affiliation>Lahore University of Management Sciences</affiliation></author>
      <pages>44-49</pages>
      <abstract>Recent advancements in Neural Machine Translation (NMT) systems have significantly improved model performance on various translation benchmarks. However, these systems still face numerous challenges when translating low-resource languages such as Urdu. In this work, we highlight the specific issues faced by machine translation systems when translating Urdu language. We first conduct a comprehensive evaluation of English to Urdu Machine Translation with four diverse models: GPT-3.5 (a large language model), opus-mt-en-ur (a bilingual translation model), NLLB (a model trained for translating 200 languages), and IndicTrans2 (a specialized model for translating low-resource Indic languages). The results demonstrate that IndicTrans2 significantly outperforms other models in Urdu Machine Translation. To understand the differences in the performance of these models, we analyze the Urdu word distribution in different training datasets and compare the training methodologies. Finally, we uncover the specific translation issues and provide suggestions for improvements in Urdu machine translation systems.</abstract>
      <url hash="e7838117">2024.loresmt-1.4</url>
      <bibkey>basit-etal-2024-challenges</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.4</doi>
    </paper>
    <paper id="5">
      <title>Linguistically Informed Transformers for Text to <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Translation</title>
      <author><first>Abhishek</first><last>Varanasi</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Manjira</first><last>Sinha</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Tirthankar</first><last>Dasgupta</last></author>
      <pages>50-56</pages>
      <abstract>In this paper we propose a framework for automatic translation of English text to American Sign Language (ASL) which leverages a linguistically informed transformer model to translate English sentences into ASL gloss sequences. These glosses are then associated with respective ASL videos, effectively representing English text in ASL. To facilitate experimentation, we create an English-ASL parallel dataset on banking domain.Our preliminary results demonstrated that the linguistically informed transformer model achieves a 97.83% ROUGE-L score for text-to-gloss translation on the ASLG-PC12 dataset. Furthermore, fine-tuning the transformer model on the banking domain dataset yields an 89.47% ROUGE-L score when fine-tuned on ASLG-PC12 + banking domain dataset. These results demonstrate the effectiveness of the linguistically informed model for both general and domain-specific translations. To facilitate parallel dataset generation in banking-domain, we choose ASL despite having limited benchmarks and data corpus compared to some of the other sign languages.</abstract>
      <url hash="1756348d">2024.loresmt-1.5</url>
      <bibkey>varanasi-etal-2024-linguistically</bibkey>
      <revision id="1" href="2024.loresmt-1.5v1" hash="93c7db9f"/>
      <revision id="2" href="2024.loresmt-1.5v2" hash="1756348d" date="2024-08-21">Minor updates.</revision>
      <doi>10.18653/v1/2024.loresmt-1.5</doi>
    </paper>
    <paper id="6">
      <title>Low-Resource Cross-Lingual Summarization through Few-Shot Learning with Large Language Models</title>
      <author><first>Gyutae</first><last>Park</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Seojin</first><last>Hwang</last></author>
      <author><first>Hwanhee</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>57-63</pages>
      <abstract>Cross-lingual summarization (XLS) aims to generate a summary in a target language different from the source language document. While large language models (LLMs) have shown promising zero-shot XLS performance, their few-shot capabilities on this task remain unexplored, especially for low-resource languages with limited parallel data. In this paper, we investigate the few-shot XLS performance of various models, including Mistral-7B-Instruct-v0.2, GPT-3.5, and GPT-4.Our experiments demonstrate that few-shot learning significantly improves the XLS performance of LLMs, particularly GPT-3.5 and GPT-4, in low-resource settings. However, the open-source model Mistral-7B-Instruct-v0.2 struggles to adapt effectively to the XLS task with limited examples. Our findings highlight the potential of few-shot learning for improving XLS performance and the need for further research in designing LLM architectures and pre-training objectives tailored for this task. We provide a future work direction to explore more effective few-shot learning strategies and to investigate the transfer learning capabilities of LLMs for cross-lingual summarization.</abstract>
      <url hash="67e04983">2024.loresmt-1.6</url>
      <bibkey>park-etal-2024-low</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.6</doi>
    </paper>
    <paper id="7">
      <title>Enhancing Low-Resource <fixed-case>NMT</fixed-case> with a Multilingual Encoder and Knowledge Distillation: A Case Study</title>
      <author><first>Aniruddha</first><last>Roy</last></author>
      <author><first>Pretam</first><last>Ray</last></author>
      <author><first>Ayush</first><last>Maheshwari</last></author>
      <author><first>Sudeshna</first><last>Sarkar</last><affiliation>Indian Institute of Technology Kharagpur, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Pawan</first><last>Goyal</last><affiliation>IIT Kharagpur</affiliation></author>
      <pages>64-73</pages>
      <abstract>Neural Machine Translation (NMT) remains a formidable challenge, especially when dealing with low-resource languages. Pre-trained sequence-to-sequence (seq2seq) multi-lingual models, such as mBART-50, have demonstrated impressive performance in various low-resource NMT tasks. However, their pre-training has been confined to 50 languages, leaving out support for numerous low-resource languages, particularly those spoken in the Indian subcontinent. Expanding mBART-50’s language support requires complex pre-training, risking performance decline due to catastrophic forgetting. Considering these expanding challenges, this paper explores a framework that leverages the benefits of a pre-trained language model along with knowledge distillation in a seq2seq architecture to facilitate translation for low-resource languages, including those not covered by mBART-50. The proposed framework employs a multilingual encoder-based seq2seq model as the foundational architecture and subsequently uses complementary knowledge distillation techniques to mitigate the impact of imbalanced training. Our framework is evaluated on three low-resource Indic languages in four Indic-to-Indic directions, yielding significant BLEU-4 and chrF improvements over baselines. Further, we conduct human evaluation to confirm effectiveness of our approach. Our code is publicly available at https://github.com/raypretam/Two-step-low-res-NMT.</abstract>
      <url hash="dff786cb">2024.loresmt-1.7</url>
      <bibkey>roy-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.7</doi>
    </paper>
    <paper id="8">
      <title>Leveraging <fixed-case>M</fixed-case>andarin as a Pivot Language for Low-Resource Machine Translation between <fixed-case>C</fixed-case>antonese and <fixed-case>E</fixed-case>nglish</title>
      <author><first>King Yiu</first><last>Suen</last><affiliation>Fano Labs</affiliation></author>
      <author><first>Rudolf</first><last>Chow</last><affiliation>Fano Labs</affiliation></author>
      <author><first>Albert Y.S.</first><last>Lam</last><affiliation>University of Hong Kong and Fano Labs</affiliation></author>
      <pages>74-84</pages>
      <abstract>Cantonese, the second most prevalent Chinese dialect after Mandarin, has been relatively overlooked in machine translation (MT) due to a scarcity of bilingual resources. In this paper, we propose to leverage Mandarin, a high-resource language, as a pivot language for translating between Cantonese and English. Our method utilizes transfer learning from pre-trained Bidirectional and Auto-Regressive Transformer (BART) models to initialize auxiliary source-pivot and pivot-target MT models. The parameters of the trained auxiliary models are then used to initialize the source-target model. Based on our experiments, our proposed method outperforms several baseline initialization strategies, naive pivot translation, and two commercial translation systems in both translation directions.</abstract>
      <url hash="79143cc5">2024.loresmt-1.8</url>
      <bibkey>suen-etal-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.8</doi>
    </paper>
    <paper id="9">
      <title>Enhancing <fixed-case>T</fixed-case>urkish Word Segmentation: A Focus on Borrowed Words and Invalid Morpheme</title>
      <author><first>Soheila</first><last>Behrooznia</last></author>
      <author><first>Ebrahim</first><last>Ansari</last><affiliation>Zanjan Institute for Advanced Studies in Basic Sciences</affiliation></author>
      <author><first>Zdenek</first><last>Zabokrtsky</last><affiliation>Faculty of Mathematics and Physics, Charles University Prague</affiliation></author>
      <pages>85-93</pages>
      <abstract>This study addresses a challenge in morphological segmentation: accurately segmenting words in languages with rich morphology. Current probabilistic methods, such as Morfessor, often produce results that lack consistency with human-segmented words. Our study adds some steps to the Morfessor segmentation process to consider invalid morphemes and borrowed words from other languages to improve morphological segmentation significantly. Comparing our idea to the results obtained from Morfessor demonstrates its efficiency, leading to more accurate morphology segmentation. This is particularly evident in the case of Turkish, highlighting the potential for further advancements in morpheme segmentation for morphologically rich languages.</abstract>
      <url hash="cf09e57e">2024.loresmt-1.9</url>
      <bibkey>behrooznia-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.9</doi>
    </paper>
    <paper id="10">
      <title>Super donors and super recipients: Studying cross-lingual transfer between high-resource and low-resource languages</title>
      <author><first>Vitaly</first><last>Protasov</last></author>
      <author><first>Elisei</first><last>Stakovskii</last></author>
      <author><first>Ekaterina</first><last>Voloshina</last><affiliation>Göteborg University and Chalmers University of Technology</affiliation></author>
      <author><first>Tatiana</first><last>Shavrina</last></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <pages>94-108</pages>
      <abstract>Despite the increasing popularity of multilingualism within the NLP community, numerous languages continue to be underrepresented due to the lack of available resources.Our work addresses this gap by introducing experiments on cross-lingual transfer between 158 high-resource (HR) and 31 low-resource (LR) languages.We mainly focus on extremely LR languages, some of which are first presented in research works.Across <tex-math>158*31</tex-math> HR–LR language pairs, we investigate how continued pretraining on different HR languages affects the mT5 model’s performance in representing LR languages in the LM setup.Our findings surprisingly reveal that the optimal language pairs with improved performance do not necessarily align with direct linguistic motivations, with subtoken overlap playing a more crucial role. Our investigation indicates that specific languages tend to be almost universally beneficial for pretraining (<i>super donors</i>), while others benefit from pretraining with almost any language (<i>super recipients</i>). This pattern recurs in various setups and is unrelated to the linguistic similarity of HR-LR pairs.Furthermore, we perform evaluation on two downstream tasks, part-of-speech (POS) tagging and machine translation (MT), showing how HR pretraining affects LR language performance.</abstract>
      <url hash="fb114b86">2024.loresmt-1.10</url>
      <bibkey>protasov-etal-2024-super</bibkey>
      <revision id="1" href="2024.loresmt-1.10v1" hash="50dffbf7"/>
      <revision id="2" href="2024.loresmt-1.10v2" hash="fb114b86" date="2024-08-17">Minor update.</revision>
      <doi>10.18653/v1/2024.loresmt-1.10</doi>
    </paper>
    <paper id="11">
      <title>Tokenisation in Machine Translation Does Matter: The impact of different tokenisation approaches for <fixed-case>M</fixed-case>altese</title>
      <author><first>Kurt</first><last>Abela</last></author>
      <author><first>Kurt</first><last>Micallef</last><affiliation>University of Malta</affiliation></author>
      <author><first>Marc</first><last>Tanti</last><affiliation>University of Malta</affiliation></author>
      <author><first>Claudia</first><last>Borg</last><affiliation>University of Malta</affiliation></author>
      <pages>109-120</pages>
      <abstract>In Machine Translation, various tokenisers are used to segment inputs before training a model. Despite tokenisation being mostly considered a solved problem for languages such as English, it is still unclear as to how effective different tokenisers are for morphologically rich languages. This study aims to explore how different approaches to tokenising Maltese impact machine translation results on the English-Maltese language pair.We observed that the OPUS-100 dataset has tokenisation inconsistencies in Maltese. We empirically found that training models on the original OPUS-100 dataset led to the generation of sentences with these issues.We therefore release an updated version of the OPUS-100 parallel English-Maltese dataset, referred to as OPUS-100-Fix, fixing these inconsistencies in Maltese by using the MLRS tokeniser. We show that after fixing the inconsistencies in the dataset, results on the fixed test set increase by 2.49 BLEU points over models trained on the original OPUS-100. We also experiment with different tokenisers, including BPE and SentencePiece to find the ideal tokeniser and vocabulary size for our setup, which was shown to be BPE with a vocabulary size of 8,000. Finally, we train different models in both directions for the ENG-MLT language pair using OPUS-100-Fix by training models from scratch as well as fine-tuning other pre-trained models, namely mBART-50 and NLLB, where a finetuned NLLB model performed the best.</abstract>
      <url hash="45a51726">2024.loresmt-1.11</url>
      <bibkey>abela-etal-2024-tokenisation</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.11</doi>
    </paper>
    <paper id="12">
      <title>Machine Translation Through Cultural Texts: Can Verses and Prose Help Low-Resource Indigenous Models?</title>
      <author><first>Antoine</first><last>Cadotte</last><affiliation>Université du Québec à Montréal</affiliation></author>
      <author><first>Nathalie</first><last>André</last></author>
      <author><first>Fatiha</first><last>Sadat</last><affiliation>université du Quebec à Montréal</affiliation></author>
      <pages>121-127</pages>
      <abstract>We propose the first MT models for Innu-Aimun, an Indigenous language in Eastern Canada, in an effort to provide assistance tools for translation and language learning. This project is carried out in collaboration with an Innu community school and involves the participation of students in Innu-Aimun translation, within the framework of a meaningful consideration of Indigenous perspectives.Our contributions in this paper result from the three initial stages of this project. First, we aim to align bilingual Innu-Aimun/French texts with collaboration and participation of Innu-Aimun locutors. Second, we present the training and evaluation results of the MT models (both statistical and neural) based on these aligned corpora. And third, we collaboratively analyze some of the translations resulting from the MT models.We also see these developments for Innu-Aimun as a useful case study for answering a larger question: in a context where few aligned bilingual sentences are available for an Indigenous language, can cultural texts such as literature and poetry be used in the development of MT models?</abstract>
      <url hash="f365e6a5">2024.loresmt-1.12</url>
      <bibkey>cadotte-etal-2024-machine</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.12</doi>
    </paper>
    <paper id="13">
      <title>Rule-Based, Neural and <fixed-case>LLM</fixed-case> Back-Translation: Comparative Insights from a Variant of <fixed-case>L</fixed-case>adin</title>
      <author><first>Samuel</first><last>Frontull</last><affiliation>Universität Innsbruck</affiliation></author>
      <author><first>Georg</first><last>Moser</last><affiliation>Universität Innsbruck</affiliation></author>
      <pages>128-138</pages>
      <abstract>This paper explores the impact of different back-translation approaches on machine translation for Ladin, specifically the Val Badia variant. Given the limited amount of parallel data available for this language (only 18k Ladin-Italian sentence pairs), we investigate the performance of a multilingual neural machine translation model fine-tuned for Ladin-Italian. In addition to the available authentic data, we synthesise further translations by using three different models: a fine-tuned neural model, a rule-based system developed specifically for this language pair, and a large language model. Our experiments show that all approaches achieve comparable translation quality in this low-resource scenario, yet round-trip translations highlight differences in model performance.</abstract>
      <url hash="31db597d">2024.loresmt-1.13</url>
      <bibkey>frontull-moser-2024-rule</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>AGE</fixed-case>: <fixed-case>A</fixed-case>mharic, <fixed-case>G</fixed-case>e’ez and <fixed-case>E</fixed-case>nglish Parallel Dataset</title>
      <author><first>Henok</first><last>Ademtew</last></author>
      <author><first>Mikiyas</first><last>Birbo</last></author>
      <pages>139-145</pages>
      <abstract>African languages are not well-represented in Natural Language Processing (NLP). The main reason is a lack of resources for training models. Low-resource languages, such as Amharic and Ge’ez, cannot benefit from modern NLP methods because of the lack of high-quality datasets. This paper presents AGE, an open-source tripartite alignment of Amharic, Ge’ez, and English parallel dataset. Additionally, we introduced a novel, 1,000 Ge’ez-centered sentences sourced from areas such as news and novels. Furthermore, we developed a model from a multilingual pre-trained language model, which brings 12.29 and 30.66 for English-Ge’ez and Ge’ez to English, respectively, and 9.39 and 12.29 for Amharic-Ge’ez and Ge’ez-Amharic respectively.</abstract>
      <url hash="a9b0f085">2024.loresmt-1.14</url>
      <bibkey>ademtew-birbo-2024-age</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.14</doi>
    </paper>
    <paper id="15">
      <title>Learning-From-Mistakes Prompting for Indigenous Language Translation</title>
      <author><first>You Cheng</first><last>Liao</last></author>
      <author><first>Chen-Jui</first><last>Yu</last></author>
      <author><first>Chi-Yi</first><last>Lin</last></author>
      <author><first>He-Feng</first><last>Yun</last></author>
      <author><first>Yen-Hsiang</first><last>Wang</last></author>
      <author><first>Hsiao-Min</first><last>Li</last></author>
      <author><first>Yao-Chung</first><last>Fan</last><affiliation>National Chung Hsing University</affiliation></author>
      <pages>146-158</pages>
      <abstract>Using large language models, this paper presents techniques to improve extremely low-resourced indigenous language translations. Our approaches are grounded in the use of (1) the presence of a datastore consisting of a limited number of parallel translation examples, (2) the inherent capabilities of LLMs like GPT-3.5, and (3) a word-level translation dictionary. We harness the potential of LLMs and in-context learning techniques in such a setting for using LLM as universal translators for extremely low-resourced languages. Our methodology hinges on utilizing LLMs as language compilers for selected language pairs, hypothesizing that they could internalize syntactic structures to facilitate accurate translation. We introduce three techniques: KNN-Prompting with Retrieved Prompting Context, Chain-of-Thought Prompting, and Learning-from-Mistakes Prompting, with the last method addressing past errors. The evaluation results suggest that, even with limited corpora, LLMs, when paired with proper prompting, can effectively translate extremely low-resource languages.</abstract>
      <url hash="db5ea23b">2024.loresmt-1.15</url>
      <bibkey>liao-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.15</doi>
    </paper>
    <paper id="16">
      <title>Adopting Ensemble Learning for Cross-lingual Classification of Crisis-related Text On Social Media</title>
      <author><first>Shareefa</first><last>Al Amer</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <author><first>Phillip</first><last>Smith</last><affiliation>University of Birmingham</affiliation></author>
      <pages>159-165</pages>
      <abstract>Cross-lingual classification poses a significant challenge in Natural Language Processing (NLP), especially when dealing with languages with scarce training data. This paper delves into the adaptation of ensemble learning to address this challenge, specifically for disaster-related social media texts. Initially, we employ Machine Translation to generate a parallel corpus in the target language to mitigate the issue of data scarcity and foster a robust training environment. Following this, we implement the bagging ensemble technique, integrating multiple classifiers into a cohesive model that demonstrates enhanced performance over individual classifiers. Our experimental results reveal significant improvements in adapting models for Arabic, utilising only English training data and markedly outperforming models intended for linguistically similar languages to English, with our ensemble model achieving an accuracy and F1 score of 0.78 when tested on original Arabic data. This research makes a substantial contribution to the field of cross-lingual classification, establishing a new benchmark for enhancing the effectiveness of language transfer in linguistically challenging scenarios.</abstract>
      <url hash="a6c40be5">2024.loresmt-1.16</url>
      <bibkey>al-amer-etal-2024-adopting</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.16</doi>
    </paper>
    <paper id="17">
      <title>Finetuning End-to-End Models for <fixed-case>E</fixed-case>stonian Conversational Spoken Language Translation</title>
      <author><first>Tiia</first><last>Sildam</last></author>
      <author><first>Andra</first><last>Velve</last></author>
      <author><first>Tanel</first><last>Alumäe</last><affiliation>Tallinn University of Technology</affiliation></author>
      <pages>166-174</pages>
      <abstract>This paper investigates the finetuning of end-to-end models for bidirectional Estonian-English and Estonian-Russian conversational speech-to-text translation. Due to the limited availability of speech translation data for Estonian, we created additional training data by web scraping and synthesizing data from speech recognition datasets using machine translation. We evaluated three publicly available end-to-end models: Whisper, OWSM 3.1, and SeamlessM4T. Our results indicate that fine-tuning with synthetic data enhances translation accuracy by a large margin, with SeamlessM4T matching or surpassing cascaded speech translation systems that use state-of-the-art speech recognition and machine translation models.</abstract>
      <url hash="9d162cf6">2024.loresmt-1.17</url>
      <bibkey>sildam-etal-2024-finetuning</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.17</doi>
    </paper>
    <paper id="18">
      <title>Benchmarking Low-Resource Machine Translation Systems</title>
      <author><first>Ana</first><last>Silva</last><affiliation>Universität Paderborn</affiliation></author>
      <author><first>Nikit</first><last>Srivastava</last><affiliation>Universität Paderborn</affiliation></author>
      <author><first>Tatiana</first><last>Moteu Ngoli</last></author>
      <author><first>Michael</first><last>Röder</last><affiliation>Paderborn University</affiliation></author>
      <author><first>Diego</first><last>Moussallem</last></author>
      <author><first>Axel-Cyrille</first><last>Ngonga Ngomo</last><affiliation>Universität Paderborn</affiliation></author>
      <pages>175-185</pages>
      <abstract>Assessing the performance of machine translation systems is of critical value, especially to languages with lower resource availability.Due to the large evaluation effort required by the translation task, studies often compare new systems against single systems or commercial solutions. Consequently, determining the best-performing system for specific languages is often unclear. This work benchmarks publicly available translation systems across 4 datasets and 26 languages, including low-resource languages. We consider both effectiveness and efficiency in our evaluation.Our results are made public through BENG—a FAIR benchmarking platform for Natural Language Generation tasks.</abstract>
      <url hash="353dfd01">2024.loresmt-1.18</url>
      <bibkey>silva-etal-2024-benchmarking</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.18</doi>
    </paper>
    <paper id="19">
      <title>Rosetta Balcanica: Deriving a “Gold Standard” Neural Machine Translation (<fixed-case>NMT</fixed-case>) Parallel Dataset from High-Fidelity Resources for <fixed-case>W</fixed-case>estern <fixed-case>B</fixed-case>alkan Languages</title>
      <author><first>Edmon</first><last>Begoli</last><affiliation>The University of Tennessee and Oak Ridge National Laboratory</affiliation></author>
      <author><first>Maria</first><last>Mahbub</last><affiliation>Oak Ridge National Laboratory</affiliation></author>
      <author><first>Sudarshan</first><last>Srinivasan</last><affiliation>Oak Ridge National Laboratory</affiliation></author>
      <pages>186-192</pages>
      <abstract>The Rosetta Balcanica is an ongoing effort in resource expansion for low-resource Western Balkans languages. This effort focuses on discovering and using accurately translated, officially mapped, and curated parallel language resources and their preparation and use as neural machine translation (NMT) datasets. Some of the guiding principles, practices, and methods employed by Rosetta Balcanica are generalizable and could apply to other low-resource language resource expansion efforts. With this goal in mind, we present our rationale and approach to discovering and using meticulously translated and officially curated low-resource language resources and our use of these resources to develop a parallel “gold standard” translation training resource. Secondly, we describe our specific methodology for NMT dataset development from these resources and its publication to a widely-used and accessible repository for natural language processing (<i>Hugging Face Hub</i>). Finally, we discuss the trade-offs and limitations of our current approach, and the roadmap for future development and the expansion of the current Rosetta Balcanica language resource.</abstract>
      <url hash="7b84b97d">2024.loresmt-1.19</url>
      <bibkey>begoli-etal-2024-rosetta</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>I</fixed-case>rish-based Large Language Model with Extreme Low-Resource Settings in Machine Translation</title>
      <author><first>Khanh-Tung</first><last>Tran</last><affiliation>University College Cork</affiliation></author>
      <author><first>Barry</first><last>O’Sullivan</last><affiliation>University College Cork</affiliation></author>
      <author><first>Hoang</first><last>Nguyen</last><affiliation>University College Cork</affiliation></author>
      <pages>193-202</pages>
      <abstract>Large Language Models (LLMs) have demonstrated exceptional performances in a wide range of natural language processing tasks. However, their success does not always extend to machine translation, particularly in challenging scenarios such as translating low-resource languages. This study investigates the multilingual capability of LLMs, with a case study on Irish, an extremely low-resource language, focusing on translation tasks between English and Irish. We propose a dynamic, efficient language adaptation framework for English-centric LLMs, which involves layer-specific adjustments and subsequent fine-tuning for machine translation. Our findings highlight several key insights: (1) different layers in the LLM serve distinct functions such as language understanding and task reasoning, (2) effective translation requires extensive pre-training on both source and target languages, and (3) targeted fine-tuning for machine translation leads to significant improvements of 36.7% for English to Irish and 133.4% for Irish to English compared to the previous state-of-the-art.</abstract>
      <url hash="23aadd73">2024.loresmt-1.20</url>
      <bibkey>tran-etal-2024-irish</bibkey>
      <doi>10.18653/v1/2024.loresmt-1.20</doi>
    </paper>
  </volume>
</collection>
