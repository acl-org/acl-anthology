<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.mrqa">
  <volume id="1" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</booktitle>
      <editor><first>Adam</first><last>Fisch</last></editor>
      <editor><first>Alon</first><last>Talmor</last></editor>
      <editor><first>Danqi</first><last>Chen</last></editor>
      <editor><first>Eunsol</first><last>Choi</last></editor>
      <editor><first>Minjoon</first><last>Seo</last></editor>
      <editor><first>Patrick</first><last>Lewis</last></editor>
      <editor><first>Robin</first><last>Jia</last></editor>
      <editor><first>Sewon</first><last>Min</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
      <venue>mrqa</venue>
    </meta>
    <frontmatter>
      <url hash="13500ebb">2021.mrqa-1.0</url>
      <bibkey>mrqa-2021-machine</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>MFAQ</fixed-case>: a Multilingual <fixed-case>FAQ</fixed-case> Dataset</title>
      <author><first>Maxime</first><last>De Bruyn</last></author>
      <author><first>Ehsan</first><last>Lotfi</last></author>
      <author><first>Jeska</first><last>Buhmann</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>1–13</pages>
      <abstract>In this paper, we present the first multilingual FAQ dataset publicly available. We collected around 6M FAQ pairs from the web, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges: duplication of content and uneven distribution of topics. We adopt a similar setup as Dense Passage Retrieval (DPR) and test various bi-encoders on this dataset. Our experiments reveal that a multilingual model based on XLM-RoBERTa achieves the best results, except for English. Lower resources languages seem to learn from one another as a multilingual model achieves a higher MRR than language-specific ones. Our qualitative analysis reveals the brittleness of the model on simple word changes. We publicly release our dataset, model, and training script.</abstract>
      <url hash="e9c21c43">2021.mrqa-1.1</url>
      <bibkey>de-bruyn-etal-2021-mfaq</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.1</doi>
      <pwccode url="https://github.com/clips/mfaq" additional="false">clips/mfaq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mfaq">MFAQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paq">PAQ</pwcdataset>
    </paper>
    <paper id="2">
      <title>Rethinking the Objectives of Extractive Question Answering</title>
      <author><first>Martin</first><last>Fajcik</last></author>
      <author><first>Josef</first><last>Jon</last></author>
      <author><first>Pavel</first><last>Smrz</last></author>
      <pages>14–27</pages>
      <abstract>This work demonstrates that using the objective with independence assumption for modelling the span probability P (a_s , a_e ) = P (a_s )P (a_e) of span starting at position a_s and ending at position a_e has adverse effects. Therefore we propose multiple approaches to modelling joint probability P (a_s , a_e) directly. Among those, we propose a compound objective, composed from the joint probability while still keeping the objective with independence assumption as an auxiliary objective. We find that the compound objective is consistently superior or equal to other assumptions in exact match. Additionally, we identified common errors caused by the assumption of independence and manually checked the counterpart predictions, demonstrating the impact of the compound objective on the real examples. Our findings are supported via experiments with three extractive QA models (BIDAF, BERT, ALBERT) over six datasets and our code, individual results and manual analysis are available online.</abstract>
      <url hash="fc859555">2021.mrqa-1.2</url>
      <bibkey>fajcik-etal-2021-rethinking</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.2</doi>
      <pwccode url="https://github.com/KNOT-FIT-BUT/JointSpanExtraction" additional="false">KNOT-FIT-BUT/JointSpanExtraction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="3">
      <title>What Would it Take to get Biomedical <fixed-case>QA</fixed-case> Systems into Practice?</title>
      <author><first>Gregory</first><last>Kell</last></author>
      <author><first>Iain</first><last>Marshall</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <author><first>Andre</first><last>Jaun</last></author>
      <pages>28–41</pages>
      <abstract>Medical question answering (QA) systems have the potential to answer clinicians’ uncertainties about treatment and diagnosis on-demand, informed by the latest evidence. However, despite the significant progress in general QA made by the NLP community, medical QA systems are still not widely used in clinical environments. One likely reason for this is that clinicians may not readily trust QA system outputs, in part because transparency, trustworthiness, and provenance have not been key considerations in the design of such models. In this paper we discuss a set of criteria that, if met, we argue would likely increase the utility of biomedical QA systems, which may in turn lead to adoption of such systems in practice. We assess existing models, tasks, and datasets with respect to these criteria, highlighting shortcomings of previously proposed approaches and pointing toward what might be more usable QA systems.</abstract>
      <url hash="1295194c">2021.mrqa-1.3</url>
      <bibkey>kell-etal-2021-take</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.3</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mediqa-ans">MEDIQA-AnS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmedqa">PubMedQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emrqa">emrQA</pwcdataset>
    </paper>
    <paper id="4">
      <title><fixed-case>G</fixed-case>erman<fixed-case>Q</fixed-case>u<fixed-case>AD</fixed-case> and <fixed-case>G</fixed-case>erman<fixed-case>DPR</fixed-case>: Improving Non-<fixed-case>E</fixed-case>nglish Question Answering and Passage Retrieval</title>
      <author><first>Timo</first><last>Möller</last></author>
      <author><first>Julian</first><last>Risch</last></author>
      <author><first>Malte</first><last>Pietsch</last></author>
      <pages>42–50</pages>
      <abstract>A major challenge of research on non-English machine reading for question answering (QA) is the lack of annotated datasets. In this paper, we present GermanQuAD, a dataset of 13,722 extractive question/answer pairs. To improve the reproducibility of the dataset creation approach and foster QA research on other languages, we summarize lessons learned and evaluate reformulation of question/answer pairs as a way to speed up the annotation process. An extractive QA model trained on GermanQuAD significantly outperforms multilingual models and also shows that machine-translated training data cannot fully substitute hand-annotated training data in the target language. Finally, we demonstrate the wide range of applications of GermanQuAD by adapting it to GermanDPR, a training dataset for dense passage retrieval (DPR), and train and evaluate one of the first non-English DPR models.</abstract>
      <url hash="ec432b47">2021.mrqa-1.4</url>
      <bibkey>moller-etal-2021-germanquad</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.4</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/germandpr">GermanDPR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/germanquad">GermanQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="5">
      <title>Zero-Shot Clinical Questionnaire Filling From Human-Machine Interactions</title>
      <author><first>Farnaz</first><last>Ghassemi Toudeshki</last></author>
      <author><first>Philippe</first><last>Jolivet</last></author>
      <author><first>Alexandre</first><last>Durand-Salmon</last></author>
      <author><first>Anna</first><last>Liednikova</last></author>
      <pages>51–62</pages>
      <abstract>In clinical studies, chatbots mimicking doctor-patient interactions are used for collecting information about the patient’s health state. Later, this information needs to be processed and structured for the doctor. One way to organize it is by automatically filling the questionnaires from the human-bot conversation. It would help the doctor to spot the possible issues. Since there is no such dataset available for this task and its collection is costly and sensitive, we explore the capacities of state-of-the-art zero-shot models for question answering, textual inference, and text classification. We provide a detailed analysis of the results and propose further directions for clinical questionnaire filling.</abstract>
      <url hash="18d508db">2021.mrqa-1.5</url>
      <bibkey>ghassemi-toudeshki-etal-2021-zero</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.5</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
    </paper>
    <paper id="6">
      <title>Can Question Generation Debias Question Answering Models? A Case Study on Question–Context Lexical Overlap</title>
      <author><first>Kazutoshi</first><last>Shinoda</last></author>
      <author><first>Saku</first><last>Sugawara</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>63–72</pages>
      <abstract>Question answering (QA) models for reading comprehension have been demonstrated to exploit unintended dataset biases such as question–context lexical overlap. This hinders QA models from generalizing to under-represented samples such as questions with low lexical overlap. Question generation (QG), a method for augmenting QA datasets, can be a solution for such performance degradation if QG can properly debias QA datasets. However, we discover that recent neural QG models are biased towards generating questions with high lexical overlap, which can amplify the dataset bias. Moreover, our analysis reveals that data augmentation with these QG models frequently impairs the performance on questions with low lexical overlap, while improving that on questions with high lexical overlap. To address this problem, we use a synonym replacement-based approach to augment questions with low lexical overlap. We demonstrate that the proposed data augmentation approach is simple yet effective to mitigate the degradation problem with only 70k synthetic examples.</abstract>
      <url hash="361d679b">2021.mrqa-1.6</url>
      <bibkey>shinoda-etal-2021-question</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.6</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="7">
      <title>What Can a Generative Language Model Answer About a Passage?</title>
      <author><first>Douglas</first><last>Summers-Stay</last></author>
      <author><first>Claire</first><last>Bonial</last></author>
      <author><first>Clare</first><last>Voss</last></author>
      <pages>73–81</pages>
      <abstract>Generative language models trained on large, diverse corpora can answer questions about a passage by generating the most likely continuation of the passage followed by a question/answer pair. However, accuracy rates vary depending on the type of question asked. In this paper we keep the passage fixed, and test with a wide variety of question types, exploring the strengths and weaknesses of the GPT-3 language model. We provide the passage and test questions as a challenge set for other language models.</abstract>
      <url hash="f415bd6d">2021.mrqa-1.7</url>
      <bibkey>summers-stay-etal-2021-generative</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.7</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="8">
      <title>Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models</title>
      <author><first>Bogdan</first><last>Kostić</last></author>
      <author><first>Julian</first><last>Risch</last></author>
      <author><first>Timo</first><last>Möller</last></author>
      <pages>82–91</pages>
      <abstract>Open-domain extractive question answering works well on textual data by first retrieving candidate texts and then extracting the answer from those candidates. However, some questions cannot be answered by text alone but require information stored in tables. In this paper, we present an approach for retrieving both texts and tables relevant to a question by jointly encoding texts, tables and questions into a single vector space. To this end, we create a new multi-modal dataset based on text and table datasets from related work and compare the retrieval performance of different encoding schemata. We find that dense vector embeddings of transformer models outperform sparse embeddings on four out of six evaluation datasets. Comparing different dense embedding models, tri-encoders with one encoder for each question, text and table increase retrieval performance compared to bi-encoders with one encoder for the question and one for both text and tables. We release the newly created multi-modal dataset to the community so that it can be used for training and evaluation.</abstract>
      <url hash="fcc109a1">2021.mrqa-1.8</url>
      <bibkey>kostic-etal-2021-multi</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.8</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/hybridqa">HybridQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ott-qa">OTT-QA</pwcdataset>
    </paper>
    <paper id="9">
      <title>Eliciting Bias in Question Answering Models through Ambiguity</title>
      <author><first>Andrew</first><last>Mao</last></author>
      <author><first>Naveen</first><last>Raman</last></author>
      <author><first>Matthew</first><last>Shu</last></author>
      <author><first>Eric</first><last>Li</last></author>
      <author><first>Franklin</first><last>Yang</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>92–99</pages>
      <abstract>Question answering (QA) models use retriever and reader systems to answer questions. Reliance on training data by QA systems can amplify or reflect inequity through their responses. Many QA models, such as those for the SQuAD dataset, are trained and tested on a subset of Wikipedia articles which encode their own biases and also reproduce real-world inequality. Understanding how training data affects bias in QA systems can inform methods to mitigate inequity. We develop two sets of questions for closed and open domain questions respectively, which use ambiguous questions to probe QA models for bias. We feed three deep-learning-based QA systems with our question sets and evaluate responses for bias via the metrics. Using our metrics, we find that open-domain QA models amplify biases more than their closed-domain counterparts and propose that biases in the retriever surface more readily due to greater freedom of choice.</abstract>
      <url hash="66250172">2021.mrqa-1.9</url>
      <bibkey>mao-etal-2021-eliciting</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.9</doi>
      <pwccode url="https://github.com/axz5fy3e6fq07q13/emnlp_bias" additional="false">axz5fy3e6fq07q13/emnlp_bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="10">
      <title>Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer</title>
      <author><first>Ziqing</first><last>Yang</last></author>
      <author><first>Wentao</first><last>Ma</last></author>
      <author><first>Yiming</first><last>Cui</last></author>
      <author><first>Jiani</first><last>Ye</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Shijin</first><last>Wang</last></author>
      <pages>100–105</pages>
      <abstract>Multilingual pre-trained models have achieved remarkable performance on cross-lingual transfer learning. Some multilingual models such as mBERT, have been pre-trained on unlabeled corpora, therefore the embeddings of different languages in the models may not be aligned very well. In this paper, we aim to improve the zero-shot cross-lingual transfer performance by proposing a pre-training task named Word-Exchange Aligning Model (WEAM), which uses the statistical alignment information as the prior knowledge to guide cross-lingual word prediction. We evaluate our model on multilingual machine reading comprehension task MLQA and natural language interface task XNLI. The results show that WEAM can significantly improve the zero-shot performance.</abstract>
      <url hash="649f1b24">2021.mrqa-1.10</url>
      <bibkey>yang-etal-2021-bilingual</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>P</fixed-case>ara<fixed-case>S</fixed-case>hoot: A <fixed-case>H</fixed-case>ebrew Question Answering Dataset</title>
      <author><first>Omri</first><last>Keren</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>106–112</pages>
      <abstract>NLP research in Hebrew has largely focused on morphology and syntax, where rich annotated datasets in the spirit of Universal Dependencies are available. Semantic datasets, however, are in short supply, hindering crucial advances in the development of NLP technology in Hebrew. In this work, we present ParaShoot, the first question answering dataset in modern Hebrew. The dataset follows the format and crowdsourcing methodology of SQuAD, and contains approximately 3000 annotated examples, similar to other question-answering datasets in low-resource languages. We provide the first baseline results using recently-released BERT-style models for Hebrew, showing that there is significant room for improvement on this task.</abstract>
      <url hash="3446cc8d">2021.mrqa-1.11</url>
      <bibkey>keren-levy-2021-parashoot</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.11</doi>
      <pwccode url="https://github.com/omrikeren/parashoot" additional="false">omrikeren/parashoot</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/parashoot">ParaShoot</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/oscar">OSCAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="12">
      <title>Unsupervised Multiple Choices Question Answering: Start Learning from Basic Knowledge</title>
      <author><first>Chi-Liang</first><last>Liu</last></author>
      <author><first>Hung-yi</first><last>Lee</last></author>
      <pages>113–118</pages>
      <abstract>In this paper, we study the possibility of unsupervised Multiple Choices Question Answering (MCQA). From very basic knowledge, the MCQA model knows that some choices have higher probabilities of being correct than others. The information, though very noisy, guides the training of an MCQA model. The proposed method is shown to outperform the baseline approaches on RACE and is even comparable with some supervised learning approaches on MC500.</abstract>
      <url hash="49280a44">2021.mrqa-1.12</url>
      <bibkey>liu-lee-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.12</doi>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mctest">MCTest</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="13">
      <title><fixed-case>GANDALF</fixed-case>: a General Character Name Description Dataset for Long Fiction</title>
      <author><first>Fredrik</first><last>Carlsson</last></author>
      <author><first>Magnus</first><last>Sahlgren</last></author>
      <author><first>Fredrik</first><last>Olsson</last></author>
      <author><first>Amaru</first><last>Cuba Gyllensten</last></author>
      <pages>119–132</pages>
      <abstract>This paper introduces a long-range multiple-choice Question Answering (QA) dataset, based on full-length fiction book texts. The questions are formulated as 10-way multiple-choice questions, where the task is to select the correct character name given a character description, or vice-versa. Each character description is formulated in natural text and often contains information from several sections throughout the book. We provide 20,000 questions created from 10,000 manually annotated descriptions of characters from 177 books containing 152,917 words on average. We address the current discourse regarding dataset bias and leakage by a simple anonymization procedure, which in turn enables interesting probing possibilities. Finally, we show that suitable baseline algorithms perform very poorly on this task, with the book size itself making it non-trivial to attempt a Transformer-based QA solution. This leaves ample room for future improvement, and hints at the need for a completely different type of solution.</abstract>
      <url hash="affbd65f">2021.mrqa-1.13</url>
      <bibkey>carlsson-etal-2021-gandalf</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.13</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/booktest">BookTest</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="14">
      <title>Investigating Post-pretraining Representation Alignment for Cross-Lingual Question Answering</title>
      <author><first>Fahim</first><last>Faisal</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>133–148</pages>
      <abstract>Human knowledge is collectively encoded in the roughly 6500 languages spoken around the world, but it is not distributed equally across languages. Hence, for information-seeking question answering (QA) systems to adequately serve speakers of all languages, they need to operate cross-lingually. In this work we investigate the capabilities of multilingually pretrained language models on cross-lingual QA. We find that explicitly aligning the representations across languages with a post-hoc finetuning step generally leads to improved performance. We additionally investigate the effect of data size as well as the language choice in this fine-tuning step, also releasing a dataset for evaluating cross-lingual QA systems.</abstract>
      <url hash="b36f7425">2021.mrqa-1.14</url>
      <bibkey>faisal-anastasopoulos-2021-investigating</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.14</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="15">
      <title>Semantic Answer Similarity for Evaluating Question Answering Models</title>
      <author><first>Julian</first><last>Risch</last></author>
      <author><first>Timo</first><last>Möller</last></author>
      <author><first>Julian</first><last>Gutsch</last></author>
      <author><first>Malte</first><last>Pietsch</last></author>
      <pages>149–157</pages>
      <abstract>The evaluation of question answering models compares ground-truth annotations with model predictions. However, as of today, this comparison is mostly lexical-based and therefore misses out on answers that have no lexical overlap but are still semantically similar, thus treating correct answers as false. This underestimation of the true performance of models hinders user acceptance in applications and complicates a fair comparison of different models. Therefore, there is a need for an evaluation metric that is based on semantics instead of pure string similarity. In this short paper, we present SAS, a cross-encoder-based metric for the estimation of semantic answer similarity, and compare it to seven existing metrics. To this end, we create an English and a German three-way annotated evaluation dataset containing pairs of answers along with human judgment of their semantic similarity, which we release along with an implementation of the SAS metric and the experiments. We find that semantic similarity metrics based on recent transformer models correlate much better with human judgment than traditional lexical similarity metrics on our two newly created datasets and one dataset from related work.</abstract>
      <url hash="af261719">2021.mrqa-1.15</url>
      <bibkey>risch-etal-2021-semantic</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.15</doi>
    </paper>
    <paper id="16">
      <title>Simple and Efficient ways to Improve <fixed-case>REALM</fixed-case></title>
      <author><first>Vidhisha</first><last>Balachandran</last></author>
      <author><first>Ashish</first><last>Vaswani</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <author><first>Niki</first><last>Parmar</last></author>
      <pages>158–164</pages>
      <abstract>Dense retrieval has been shown to be effective for Open Domain Question Answering, surpassing sparse retrieval methods like BM25. One such model, REALM, (Guu et al., 2020) is an end-to-end dense retrieval system that uses MLM based pretraining for improved downstream QA performance. However, the current REALM setup uses limited resources and is not comparable in scale to more recent systems, contributing to its lower performance. Additionally, it relies on noisy supervision for retrieval during fine-tuning. We propose REALM++, where we improve upon the training and inference setups and introduce better supervision signal for improving performance, without any architectural changes. REALM++ achieves ~5.5% absolute accuracy gains over the baseline while being faster to train. It also matches the performance of large models which have 3x more parameters demonstrating the efficiency of our setup.</abstract>
      <url hash="456a27ee">2021.mrqa-1.16</url>
      <bibkey>balachandran-etal-2021-simple</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.16</doi>
      <video href="2021.mrqa-1.16.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
  </volume>
</collection>
